{"id": "2506.03470", "pdf": "https://arxiv.org/pdf/2506.03470", "abs": "https://arxiv.org/abs/2506.03470", "authors": ["Liam Hodgkinson", "Zhichao Wang", "Michael W. Mahoney"], "title": "Models of Heavy-Tailed Mechanistic Universality", "categories": ["stat.ML", "cs.LG"], "comment": "40 pages, 4 figures", "summary": "Recent theoretical and empirical successes in deep learning, including the\ncelebrated neural scaling laws, are punctuated by the observation that many\nobjects of interest tend to exhibit some form of heavy-tailed or power law\nbehavior. In particular, the prevalence of heavy-tailed spectral densities in\nJacobians, Hessians, and weight matrices has led to the introduction of the\nconcept of heavy-tailed mechanistic universality (HT-MU). Multiple lines of\nempirical evidence suggest a robust correlation between heavy-tailed metrics\nand model performance, indicating that HT-MU may be a fundamental aspect of\ndeep learning efficacy. Here, we propose a general family of random matrix\nmodels -- the high-temperature Marchenko-Pastur (HTMP) ensemble -- to explore\nattributes that give rise to heavy-tailed behavior in trained neural networks.\nUnder this model, spectral densities with power laws on (upper and lower) tails\narise through a combination of three independent factors (complex correlation\nstructures in the data; reduced temperatures during training; and reduced\neigenvector entropy), appearing as an implicit bias in the model structure, and\nthey can be controlled with an \"eigenvalue repulsion\" parameter. Implications\nof our model on other appearances of heavy tails, including neural scaling\nlaws, optimizer trajectories, and the five-plus-one phases of neural network\ntraining, are discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u77e9\u9635\u6a21\u578b\uff08HTMP\uff09\uff0c\u7528\u4e8e\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u91cd\u5c3e\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u4f5c\u4e3a\u6a21\u578b\u9690\u6027\u504f\u7f6e\u7684\u672c\u8d28\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7b49\u591a\u4e2a\u65b9\u9762\u7684\u5f71\u54cd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u8bb8\u591a\u5bf9\u8c61\u8868\u73b0\u51fa\u91cd\u5c3e\u6216\u5e42\u5f8b\u884c\u4e3a\uff0c\u5982\u96c5\u53ef\u6bd4\u77e9\u9635\u3001\u6d77\u68ee\u77e9\u9635\u548c\u6743\u91cd\u77e9\u9635\u7684\u91cd\u5c3e\u8c31\u5bc6\u5ea6\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u91cd\u5c3e\u673a\u5236\u666e\u9002\u6027\uff08HT-MU\uff09\u7684\u6982\u5ff5\u3002", "method": "\u63d0\u51fa\u9ad8\u6e29\u5ea6Marchenko-Pastur\uff08HTMP\uff09\u968f\u673a\u77e9\u9635\u6a21\u578b\uff0c\u5206\u6790\u8c31\u5bc6\u5ea6\u7684\u5e42\u5f8b\u884c\u4e3a\u53ca\u5176\u4e0e\u8bad\u7ec3\u6570\u636e\u3001\u6e29\u5ea6\u3001\u7279\u5f81\u5411\u91cf\u71b5\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8c31\u5bc6\u5ea6\u7684\u5e42\u5f8b\u884c\u4e3a\u6e90\u4e8e\u6570\u636e\u76f8\u5173\u6027\u3001\u8bad\u7ec3\u6e29\u5ea6\u964d\u4f4e\u548c\u7279\u5f81\u5411\u91cf\u71b5\u51cf\u5c11\u4e09\u4e2a\u56e0\u7d20\u7684\u7ec4\u5408\uff0c\u5e76\u53ef\u901a\u8fc7\u201c\u7279\u5f81\u503c\u6392\u65a5\u201d\u53c2\u6570\u8fdb\u884c\u63a7\u5236\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u77e9\u9635\u6a21\u578b\uff08HTMP\uff09\u6765\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u91cd\u5c3e\u884c\u4e3a\uff0c\u8868\u660e\u8fd9\u4e9b\u884c\u4e3a\u662f\u6a21\u578b\u7ed3\u6784\u4e2d\u7684\u9690\u6027\u504f\u7f6e\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u6a21\u578b\u5bf9\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7b49\u73b0\u8c61\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.03657", "pdf": "https://arxiv.org/pdf/2506.03657", "abs": "https://arxiv.org/abs/2506.03657", "authors": ["Leonardo Martins Bianco", "Christine Keribin", "Zacharie Naulet"], "title": "SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Community detection is a fundamental task in graph analysis, with methods\noften relying on fitting models like the Stochastic Block Model (SBM) to\nobserved networks. While many algorithms can accurately estimate SBM parameters\nwhen the input graph is a perfect sample from the model, real-world graphs\nrarely conform to such idealized assumptions. Therefore, robust algorithms are\ncrucial-ones that can recover model parameters even when the data deviates from\nthe assumed distribution. In this work, we propose SubSearch, an algorithm for\nrobustly estimating SBM parameters by exploring the space of subgraphs in\nsearch of one that closely aligns with the model's assumptions. Our approach\nalso functions as an outlier detection method, properly identifying nodes\nresponsible for the graph's deviation from the model and going beyond simple\ntechniques like pruning high-degree nodes. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSubSearch\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5065\u5730\u4f30\u8ba1Stochastic Block Model\u53c2\u6570\u5e76\u68c0\u6d4b\u56fe\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u8282\u70b9\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u6570\u636e\u901a\u5e38\u4e0d\u6ee1\u8db3\u7406\u60f3\u5316\u6a21\u578b\u7684\u5047\u8bbe\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7a33\u5065\u7684\u7b97\u6cd5\u6765\u8fdb\u884c\u793e\u533a\u68c0\u6d4b\u548c\u53c2\u6570\u4f30\u8ba1\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u5b50\u56fe\u7a7a\u95f4\u6765\u7a33\u5065\u4f30\u8ba1SBM\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u5bfb\u627e\u4e0e\u6a21\u578b\u5047\u8bbe\u7d27\u5bc6\u5bf9\u9f50\u7684\u5b50\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSubSearch\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u6709\u6548\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u5bfc\u81f4\u56fe\u504f\u79bb\u6a21\u578b\u7684\u5f02\u5e38\u8282\u70b9\u3002", "conclusion": "SubSearch\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u793e\u533a\u68c0\u6d4b\uff0c\u5e76\u4e14\u5728\u9762\u5bf9\u4e0e\u6a21\u578b\u5047\u8bbe\u4e0d\u7b26\u7684\u56fe\u6570\u636e\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u53ef\u4ee5\u7528\u4e8e\u5f02\u5e38\u8282\u70b9\u68c0\u6d4b\u3002"}}
{"id": "2506.03670", "pdf": "https://arxiv.org/pdf/2506.03670", "abs": "https://arxiv.org/abs/2506.03670", "authors": ["Ivan Melev", "Goeran Kauermann"], "title": "Position: There Is No Free Bayesian Uncertainty Quantification", "categories": ["stat.ML", "cs.LG"], "comment": "NeurIPS 2025 preprint, frequentist critique of Bayesian UQ", "summary": "Due to their intuitive appeal, Bayesian methods of modeling and uncertainty\nquantification have become popular in modern machine and deep learning. When\nproviding a prior distribution over the parameter space, it is straightforward\nto obtain a distribution over the parameters that is conventionally interpreted\nas uncertainty quantification of the model. We challenge the validity of such\nBayesian uncertainty quantification by discussing the equivalent\noptimization-based representation of Bayesian updating, provide an alternative\ninterpretation that is coherent with the optimization-based perspective,\npropose measures of the quality of the Bayesian inferential stage, and suggest\ndirections for future work.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u65b0\u89e3\u91ca\u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u4e86\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u8d28\u91cf\u3002", "motivation": "\u7531\u4e8e\u8d1d\u53f6\u65af\u65b9\u6cd5\u56e0\u5176\u76f4\u89c2\u5438\u5f15\u529b\u800c\u5728\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6709\u6548\u6027\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u8ba8\u8bba\u8d1d\u53f6\u65af\u66f4\u65b0\u7684\u7b49\u6548\u4f18\u5316\u8868\u793a\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0e\u4f18\u5316\u89c2\u70b9\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u8d1d\u53f6\u65af\u63a8\u65ad\u8d28\u91cf\u7684\u63aa\u65bd\u3002", "result": "\u6311\u6218\u4e86\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u4f20\u7edf\u89e3\u91ca\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u4ee5\u53ca\u672a\u6765\u5de5\u4f5c\u7684\u65b9\u5411\u3002", "conclusion": "\u4f5c\u8005\u8d28\u7591\u4e86\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f18\u5316\u89c6\u89d2\u7684\u66ff\u4ee3\u89e3\u91ca\u548c\u8861\u91cf\u8d1d\u53f6\u65af\u63a8\u65ad\u9636\u6bb5\u8d28\u91cf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.03672", "pdf": "https://arxiv.org/pdf/2506.03672", "abs": "https://arxiv.org/abs/2506.03672", "authors": ["Sobihan Surendran", "Adeline Fermanian", "Sylvain Le Corff"], "title": "Latent Guided Sampling for Combinatorial Optimization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Combinatorial Optimization problems are widespread in domains such as\nlogistics, manufacturing, and drug discovery, yet their NP-hard nature makes\nthem computationally challenging. Recent Neural Combinatorial Optimization\nmethods leverage deep learning to learn solution strategies, trained via\nSupervised or Reinforcement Learning (RL). While promising, these approaches\noften rely on task-specific augmentations, perform poorly on\nout-of-distribution instances, and lack robust inference mechanisms. Moreover,\nexisting latent space models either require labeled data or rely on pre-trained\npolicies. In this work, we propose LGS-Net, a novel latent space model that\nconditions on problem instances, and introduce an efficient inference method,\nLatent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic\nApproximation. We show that the iterations of our method form a\ntime-inhomogeneous Markov Chain and provide rigorous theoretical convergence\nguarantees. Empirical results on benchmark routing tasks show that our method\nachieves state-of-the-art performance among RL-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLGS-Net\u7684\u65b0\u578b\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u4ee5\u53ca\u79f0\u4e3aLatent Guided Sampling\u7684\u6709\u6548\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u8981\u4e48\u9700\u8981\u6807\u8bb0\u6570\u636e\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u800c\u73b0\u6709\u7684\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u5728\u5206\u5e03\u5916\u5b9e\u4f8b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u7f3a\u4e4f\u9c81\u68d2\u7684\u63a8\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6f5c\u5728\u7a7a\u95f4\u6a21\u578bLGS-Net\u548c\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65b9\u6cd5Latent Guided Sampling(LGS)\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u548c\u968f\u673a\u903c\u8fd1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u57fa\u51c6\u8def\u7531\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u57fa\u4e8eRL\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LGS-Net\u548cLatent Guided Sampling\u65b9\u6cd5\u5728\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002"}}
{"id": "2506.03205", "pdf": "https://arxiv.org/pdf/2506.03205", "abs": "https://arxiv.org/abs/2506.03205", "authors": ["Umberto Gon\u00e7alves de Sousa"], "title": "Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments", "categories": ["cs.AI"], "comment": "17 pages, 5 figures", "summary": "This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum\nreinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,\nwhere Q-ARDNS-Multi stands for \"Quantum Adaptive Reward-Driven Neural Simulator\n- Multi-Agent\". It integrates quantum circuits with RY gates, meta-cognitive\nadaptation, and multi-agent coordination mechanisms for complex 3D\nenvironments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action\nselection, a dual-memory system inspired by human cognition, a shared memory\nmodule for agent cooperation, and adaptive exploration strategies modulated by\nreward variance and intrinsic motivation. Evaluated in a $10 \\times 10 \\times\n3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi\nachieves success rates of 99.6\\% and 99.5\\% for Agents 0 and 1, respectively,\noutperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft\nActor-Critic (SAC) in terms of success rate, stability, navigation efficiency,\nand collision avoidance. The framework records mean rewards of $-304.2891 \\pm\n756.4636$ and $-295.7622 \\pm 752.7103$, averaging 210 steps to goal,\ndemonstrating its robustness in dynamic settings. Comprehensive analyses,\nincluding learning curves, reward distributions, statistical tests, and\ncomputational efficiency evaluations, highlight the contributions of quantum\ncircuits and meta-cognitive adaptation. By bridging quantum computing,\ncognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,\nhuman-like approach for applications in robotics, autonomous navigation, and\ndecision-making under uncertainty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Q-ARDNS-Multi\uff0c\u5176\u5728\u590d\u6742\u4e09\u7ef4\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u534f\u8c03\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u590d\u6742\u4e09\u7ef4\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6027\u80fd\uff0c\u8bbe\u8ba1\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "method": "Q-ARDNS-Multi\u5229\u75282\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u8fdb\u884c\u52a8\u4f5c\u9009\u62e9\uff0c\u91c7\u7528\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u3001\u7528\u4e8e\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u5171\u4eab\u8bb0\u5fc6\u6a21\u5757\u4ee5\u53ca\u7531\u5956\u52b1\u65b9\u5dee\u548c\u5185\u5728\u52a8\u673a\u8c03\u8282\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\uff0c\u5e76\u572810\u00d710\u00d73 GridWorld\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "Q-ARDNS-Multi\u5206\u522b\u4ee599.6%\u548c99.5%\u7684\u6210\u529f\u7387\u8d85\u8d8a\u4e86MADDPG\u548cSAC\u7b97\u6cd5\uff0c\u5728\u6210\u529f\u7387\u3001\u7a33\u5b9a\u6027\u3001\u5bfc\u822a\u6548\u7387\u548c\u907f\u969c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u8fbe\u5230210\u6b65\u5230\u76ee\u6807\u3002", "conclusion": "Q-ARDNS-Multi\u901a\u8fc7\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7c7b\u4eba\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u7b49\u5e94\u7528\u3002"}}
{"id": "2506.03154", "pdf": "https://arxiv.org/pdf/2506.03154", "abs": "https://arxiv.org/abs/2506.03154", "authors": ["Zhaoyang Chen", "Cody Fleming"], "title": "Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL", "categories": ["cs.LG"], "comment": null, "summary": "Classifier free guidance has shown strong potential in diffusion-based\nreinforcement learning. However, existing methods rely on joint training of the\nguidance module and the diffusion model, which can be suboptimal during the\nearly stages when the guidance is inaccurate and provides noisy learning\nsignals. In offline RL, guidance depends solely on offline data: observations,\nactions, and rewards, and is independent of the policy module's behavior,\nsuggesting that joint training is not required. This paper proposes modular\ntraining methods that decouple the guidance module from the diffusion model,\nbased on three key findings:\n  Guidance Necessity: We explore how the effectiveness of guidance varies with\nthe training stage and algorithm choice, uncovering the roles of guidance and\ndiffusion. A lack of good guidance in the early stage presents an opportunity\nfor optimization.\n  Guidance-First Diffusion Training: We introduce a method where the guidance\nmodule is first trained independently as a value estimator, then frozen to\nguide the diffusion model using classifier-free reward guidance. This\nmodularization reduces memory usage, improves computational efficiency, and\nenhances both sample efficiency and final performance.\n  Cross-Module Transferability: Applying two independently trained guidance\nmodels, one during training and the other during inference, can significantly\nreduce normalized score variance (e.g., reducing IQR by 86%). We show that\nguidance modules trained with one algorithm (e.g., IDQL) can be directly reused\nwith another (e.g., DQL), with no additional training required, demonstrating\nbaseline-level performance as well as strong modularity and transferability.\n  We provide theoretical justification and empirical validation on bullet D4RL\nbenchmarks. Our findings suggest a new paradigm for offline RL: modular,\nreusable, and composable training pipelines.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u4e2d\u5982\u4f55\u901a\u8fc7\u6a21\u5757\u5316\u8bad\u7ec3\u89e3\u8026\u5f15\u5bfc\u6a21\u5757\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u5176\u826f\u597d\u7684\u53ef\u8f6c\u79fb\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5f15\u5bfc\u6a21\u5757\u548c\u6269\u6563\u6a21\u578b\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u8fd9\u5728\u65e9\u671f\u9636\u6bb5\u53ef\u80fd\u6548\u679c\u4e0d\u4f73\u3002\u800c\u79bb\u7ebfRL\u4e2d\uff0c\u5f15\u5bfc\u4ec5\u4f9d\u8d56\u4e8e\u79bb\u7ebf\u6570\u636e\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u8054\u5408\u8bad\u7ec3\u3002", "method": "\u57fa\u4e8e\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\u8fdb\u884c\u5b9e\u9a8c\uff1a\u6307\u5bfc\u5fc5\u8981\u6027\u3001\u5148\u6307\u5bfc\u540e\u6269\u6563\u8bad\u7ec3\u4ee5\u53ca\u8de8\u6a21\u5757\u53ef\u8f6c\u79fb\u6027\u3002\u901a\u8fc7\u72ec\u7acb\u8bad\u7ec3\u5f15\u5bfc\u6a21\u5757\u5e76\u5c06\u5176\u51bb\u7ed3\u4ee5\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e0d\u540c\u7b97\u6cd5\u95f4\u5f15\u5bfc\u6a21\u5757\u7684\u53ef\u91cd\u7528\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u5757\u5316\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u4ec5\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u663e\u8457\u589e\u5f3a\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff1b\u6b64\u5916\uff0c\u8de8\u6a21\u5757\u7684\u53ef\u8f6c\u79fb\u6027\u51cf\u5c11\u4e86\u6807\u51c6\u5316\u5206\u6570\u7684\u65b9\u5dee\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u7b97\u6cd5\u95f4\u7684\u76f4\u63a5\u91cd\u7528\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u5f15\u5bfc\u6a21\u5757\u4e0e\u6269\u6563\u6a21\u578b\u89e3\u8026\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u5f0f\uff1a\u6a21\u5757\u5316\u3001\u53ef\u91cd\u7528\u548c\u53ef\u7ec4\u5408\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002"}}
{"id": "2506.03764", "pdf": "https://arxiv.org/pdf/2506.03764", "abs": "https://arxiv.org/abs/2506.03764", "authors": ["R\u00f3is\u00edn Luo"], "title": "Infinitesimal Higher-Order Spectral Variations in Rectangular Real Random Matrices", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We present a theoretical framework for deriving the general $n$-th order\nFr\\'echet derivatives of singular values in real rectangular matrices, by\nleveraging reduced resolvent operators from Kato's analytic perturbation theory\nfor self-adjoint operators. Deriving closed-form expressions for higher-order\nderivatives of singular values is notoriously challenging through standard\nmatrix-analysis techniques. To overcome this, we treat a real rectangular\nmatrix as a compact operator on a finite-dimensional Hilbert space, and embed\nthe rectangular matrix into a block self-adjoint operator so that non-symmetric\nperturbations are captured. Applying Kato's asymptotic eigenvalue expansion to\nthis construction, we obtain a general, closed-form expression for the\ninfinitesimal $n$-th order spectral variations. Specializing to $n=2$ and\ndeploying on a Kronecker-product representation with matrix convention yield\nthe Hessian of a singular value, not found in literature. By bridging abstract\noperator-theoretic perturbation theory with matrices, our framework equips\nresearchers with a practical toolkit for higher-order spectral sensitivity\nstudies in random matrix applications (e.g., adversarial perturbation in deep\nlearning).", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528Kato\u7684\u6270\u52a8\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u5b9e\u77e9\u5f62\u77e9\u9635\u5947\u5f02\u503c\u9ad8\u9636\u5bfc\u6570\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u7ed9\u51fa\u4e86\u4e8c\u9636\u5bfc\u6570\uff08Hessian\uff09\u7684\u95ed\u5408\u5f62\u5f0f\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "motivation": "\u6807\u51c6\u77e9\u9635\u5206\u6790\u6280\u672f\u5728\u63a8\u5bfc\u9ad8\u9636\u5947\u5f02\u503c\u5bfc\u6570\u7684\u95ed\u5408\u5f62\u5f0f\u65f6\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5904\u7406\u975e\u5bf9\u79f0\u6270\u52a8\u95ee\u9898\u5e76\u7b80\u5316\u8ba1\u7b97\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5c06\u5b9e\u77e9\u5f62\u77e9\u9635\u89c6\u4e3a\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e0a\u7684\u7d27\u7b97\u5b50\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230\u5757\u81ea\u4f34\u7b97\u5b50\u4e2d\uff0c\u7ed3\u5408Kato\u7684\u89e3\u6790\u6270\u52a8\u7406\u8bba\u548c\u6e10\u8fd1\u7279\u5f81\u503c\u5c55\u5f00\u65b9\u6cd5\uff0c\u5f97\u5230\u4e86n\u9636\u8c31\u53d8\u5206\u7684\u95ed\u5408\u5f62\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u51fa\u4e00\u822cn\u9636Fr\u00e9chet\u5bfc\u6570\u7684\u95ed\u5408\u5f62\u5f0f\u8868\u8fbe\u5f0f\uff0c\u7279\u522b\u5728n=2\u7684\u60c5\u51b5\u4e0b\u9996\u6b21\u5f97\u5230\u4e86\u5947\u5f02\u503c\u7684Hessian\u77e9\u9635\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u968f\u673a\u77e9\u9635\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97\u5b9e\u77e9\u5f62\u77e9\u9635\u5947\u5f02\u503c\u7684n\u9636Fr\u00e9chet\u5bfc\u6570\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u95ed\u5408\u5f62\u5f0f\u7684\u8868\u8fbe\u5f0f\uff0c\u7279\u522b\u662f\u5728\u4e8c\u9636\u60c5\u51b5\u4e0b\u63a8\u5bfc\u51fa\u4e86Hessian\u77e9\u9635\u3002"}}
{"id": "2506.03233", "pdf": "https://arxiv.org/pdf/2506.03233", "abs": "https://arxiv.org/abs/2506.03233", "authors": ["Andrea Ferrario"], "title": "A Trustworthiness-based Metaphysics of Artificial Intelligence Systems", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "To appear in the proceedings of 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25)", "summary": "Modern AI systems are man-made objects that leverage machine learning to\nsupport our lives across a myriad of contexts and applications. Despite\nextensive epistemological and ethical debates, their metaphysical foundations\nremain relatively under explored. The orthodox view simply suggests that AI\nsystems, as artifacts, lack well-posed identity and persistence conditions --\ntheir metaphysical kinds are no real kinds. In this work, we challenge this\nperspective by introducing a theory of metaphysical identity of AI systems. We\ndo so by characterizing their kinds and introducing identity criteria -- formal\nrules that answer the questions \"When are two AI systems the same?\" and \"When\ndoes an AI system persist, despite change?\" Building on Carrara and Vermaas'\naccount of fine-grained artifact kinds, we argue that AI trustworthiness\nprovides a lens to understand AI system kinds and formalize the identity of\nthese artifacts by relating their functional requirements to their physical\nmake-ups. The identity criteria of AI systems are determined by their\ntrustworthiness profiles -- the collection of capabilities that the systems\nmust uphold over time throughout their artifact histories, and their\neffectiveness in maintaining these capabilities. Our approach suggests that the\nidentity and persistence of AI systems is sensitive to the socio-technical\ncontext of their design and utilization via their trustworthiness, providing a\nsolid metaphysical foundation to the epistemological, ethical, and legal\ndiscussions about these artifacts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86AI\u7cfb\u7edf\u4f5c\u4e3a\u4eba\u5de5\u7269\u7f3a\u4e4f\u660e\u786e\u8eab\u4efd\u548c\u6301\u7eed\u6027\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u53ef\u4fe1\u5ea6\u7684\u8eab\u4efd\u6807\u51c6\u7406\u8bba\uff0c\u4e3a\u5176\u76f8\u5173\u8ba8\u8bba\u63d0\u4f9b\u4e86\u5f62\u800c\u4e0a\u5b66\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u73b0\u4ee3AI\u7cfb\u7edf\u6709\u5e7f\u6cdb\u7684\u8ba4\u8bc6\u8bba\u548c\u4f26\u7406\u5b66\u8ba8\u8bba\uff0c\u4f46\u5b83\u4eec\u7684\u5f62\u800c\u4e0a\u5b66\u57fa\u7840\u4ecd\u76f8\u5bf9\u672a\u88ab\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u6311\u6218\u4f20\u7edf\u89c2\u70b9\u5e76\u63d0\u51fa\u65b0\u7684\u8eab\u4efd\u8bc6\u522b\u6807\u51c6\u3002", "method": "\u57fa\u4e8eCarrara\u548cVermaas\u7684\u7ec6\u7c92\u5ea6\u4eba\u5de5\u7269\u79cd\u7c7b\u7406\u8bba\uff0c\u5f15\u5165\u4e86AI\u7cfb\u7edf\u5f62\u800c\u4e0a\u5b66\u8eab\u4efd\u7684\u7406\u8bba\uff0c\u5e76\u901a\u8fc7\u53ef\u4fe1\u5ea6\u5206\u6790\u5176\u79cd\u7c7b\u548c\u8eab\u4efd\u6807\u51c6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\u7684\u8eab\u4efd\u8bc6\u522b\u7406\u8bba\uff0c\u5f3a\u8c03\u5176\u53ef\u4fe1\u5ea6\u7279\u5f81\u51b3\u5b9a\u4e86\u5176\u8eab\u4efd\u6807\u51c6\uff0c\u4ece\u800c\u4e3a\u5176\u8ba4\u8bc6\u8bba\u3001\u4f26\u7406\u548c\u6cd5\u5f8b\u8ba8\u8bba\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\u8ba4\u4e3a\uff0cAI\u7cfb\u7edf\u7684\u8eab\u4efd\u548c\u6301\u7eed\u6027\u901a\u8fc7\u5176\u53ef\u4fe1\u5ea6\u7684\u89c6\u89d2\u6765\u7406\u89e3\uff0c\u5e76\u4e3a\u5173\u4e8e\u8fd9\u4e9b\u4eba\u5de5\u7269\u7684\u8ba4\u8bc6\u8bba\u3001\u4f26\u7406\u548c\u6cd5\u5f8b\u8ba8\u8bba\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u5f62\u800c\u4e0a\u5b66\u57fa\u7840\u3002"}}
{"id": "2506.03155", "pdf": "https://arxiv.org/pdf/2506.03155", "abs": "https://arxiv.org/abs/2506.03155", "authors": ["Yu Zheng"], "title": "Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The proliferation of artificial intelligence has enabled a diversity of\napplications that bridge the gap between digital and physical worlds. As\nphysical environments are too complex to model through a single information\nacquisition approach, it is crucial to fuse multimodal data generated by\ndifferent sources, such as sensors, devices, systems, and people, to solve a\nproblem in the real world. Unfortunately, it is neither applicable nor\nsustainable to deploy new resources to collect original data from scratch for\nevery problem. Thus, when data is inadequate in the domain of problem, it is\nvital to fuse knowledge from multimodal data that is already available in other\ndomains. We call this cross-domain knowledge fusion. Existing research focus on\nfusing multimodal data in a single domain, supposing the knowledge from\ndifferent datasets is intrinsically aligned; however, this assumption may not\nhold in the scenarios of cross-domain knowledge fusion. In this paper, we\nformally define the cross-domain multimodal data fusion problem, discussing its\nunique challenges, differences and advantages beyond data fusion in a single\ndomain. We propose a four-layer framework, consisting of Domains, Links, Models\nand Data layers, answering three key questions: \"what to fuse\", \"why can be\nfused\", and \"how to fuse\". The Domains Layer selects relevant data from\ndifferent domains for a given problem. The Links Layer reveals the philosophy\nof knowledge alignment beyond specific model structures. The Models Layer\nprovides two knowledge fusion paradigms based on the fundamental mechanisms for\nprocessing data. The Data Layer turns data of different structures,\nresolutions, scales and distributions into a consistent representation that can\nbe fed into an AI model. With this framework, we can design end-to-end\nsolutions that fuse cross-domain multimodal data effectively for solving\nreal-world problems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8de8\u9886\u57df\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u5c42\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u5355\u4e00\u7684\u4fe1\u606f\u83b7\u53d6\u65b9\u6cd5\u65e0\u6cd5\u5bf9\u590d\u6742\u7684\u7269\u7406\u73af\u5883\u8fdb\u884c\u5efa\u6a21\uff0c\u56e0\u6b64\u9700\u8981\u878d\u5408\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u591a\u6a21\u6001\u6570\u636e\u3002\u7136\u800c\uff0c\u4e3a\u6bcf\u4e2a\u95ee\u9898\u4ece\u5934\u5f00\u59cb\u6536\u96c6\u539f\u59cb\u6570\u636e\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5229\u7528\u5df2\u6709\u9886\u57df\u7684\u77e5\u8bc6\u8fdb\u884c\u8de8\u9886\u57df\u77e5\u8bc6\u878d\u5408\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u9886\u57df\u5c42\u3001\u94fe\u63a5\u5c42\u3001\u6a21\u578b\u5c42\u548c\u6570\u636e\u5c42\u7684\u56db\u5c42\u6846\u67b6\uff0c\u56de\u7b54\u4e86'\u878d\u5408\u4ec0\u4e48'\u3001'\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u878d\u5408'\u548c'\u5982\u4f55\u878d\u5408'\u4e09\u4e2a\u5173\u952e\u95ee\u9898\u3002", "result": "\u6b63\u5f0f\u5b9a\u4e49\u4e86\u8de8\u9886\u57df\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u95ee\u9898\uff0c\u8ba8\u8bba\u4e86\u5176\u72ec\u7279\u7684\u6311\u6218\u3001\u4e0e\u5355\u9886\u57df\u6570\u636e\u878d\u5408\u7684\u5dee\u5f02\u53ca\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u6846\u67b6\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u878d\u5408\u8de8\u9886\u57df\u591a\u6a21\u6001\u6570\u636e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8de8\u9886\u57df\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u8be5\u6846\u67b6\u8bbe\u8ba1\u6709\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u4ee5\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u95ee\u9898\u3002"}}
{"id": "2506.03780", "pdf": "https://arxiv.org/pdf/2506.03780", "abs": "https://arxiv.org/abs/2506.03780", "authors": ["Hasan Fallahgoul"], "title": "High-Dimensional Learning in Finance", "categories": ["stat.ML", "cs.LG", "econ.EM"], "comment": null, "summary": "Recent advances in machine learning have shown promising results for\nfinancial prediction using large, over-parameterized models. This paper\nprovides theoretical foundations and empirical validation for understanding\nwhen and how these methods achieve predictive success. I examine three key\naspects of high-dimensional learning in finance. First, I prove that\nwithin-sample standardization in Random Fourier Features implementations\nfundamentally alters the underlying Gaussian kernel approximation, replacing\nshift-invariant kernels with training-set dependent alternatives. Second, I\nderive sample complexity bounds showing when reliable learning becomes\ninformation-theoretically impossible under weak signal-to-noise ratios typical\nin finance. Third, VC-dimension analysis reveals that ridgeless regression's\neffective complexity is bounded by sample size rather than nominal feature\ndimension. Comprehensive numerical validation confirms these theoretical\npredictions, revealing systematic breakdown of claimed theoretical properties\nacross realistic parameter ranges. These results show that when sample size is\nsmall and features are high-dimensional, observed predictive success is\nnecessarily driven by low-complexity artifacts, not genuine high-dimensional\nlearning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u91d1\u878d\u9886\u57df\u4f7f\u7528\u5927\u578b\u8fc7\u53c2\u6570\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u7684\u95ee\u9898\u3002\u4f5c\u8005\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u7814\u7a76\u4e86\u9ad8\u7ef4\u5b66\u4e60\u7684\u4e09\u4e2a\u65b9\u9762\uff0c\u53d1\u73b0\u6837\u672c\u5185\u6807\u51c6\u5316\u5f71\u54cd\u6838\u8fd1\u4f3c\u3001\u5f31\u4fe1\u566a\u6bd4\u4e0b\u7684\u5b66\u4e60\u96be\u5ea6\u4ee5\u53ca\u5cad\u56de\u5f52\u7684\u6709\u6548\u590d\u6742\u5ea6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5c0f\u6837\u672c\u548c\u9ad8\u7ef4\u7279\u5f81\u60c5\u51b5\u4e0b\uff0c\u9884\u6d4b\u6210\u529f\u662f\u7531\u4f4e\u590d\u6742\u5ea6\u56e0\u7d20\u9a71\u52a8\uff0c\u800c\u975e\u771f\u6b63\u9ad8\u7ef4\u5b66\u4e60\u3002", "motivation": "\u8bba\u6587\u7684\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u5728\u91d1\u878d\u9886\u57df\u4f7f\u7528\u5927\u578b\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u7684\u65b9\u6cd5\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u53d6\u5f97\u9884\u6d4b\u6210\u529f\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "method": "\u8bba\u6587\u7684\u65b9\u6cd5\u5305\u62ec\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u5b9e\u73b0\u4e2d\u7684\u6837\u672c\u5185\u6807\u51c6\u5316\u5bf9\u9ad8\u65af\u6838\u8fd1\u4f3c\u7684\u5f71\u54cd\u5206\u6790\u3001\u5f31\u4fe1\u566a\u6bd4\u4e0b\u53ef\u9760\u5b66\u4e60\u7684\u4fe1\u606f\u7406\u8bba\u754c\u9650\u63a8\u5bfc\u4ee5\u53ca\u5cad\u56de\u5f52\u7684\u6709\u6548\u590d\u6742\u5ea6\u7684VC\u7ef4\u5ea6\u5206\u6790\u3002", "result": "\u8bba\u6587\u7684\u7ed3\u679c\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u65b9\u9762\uff1a1\uff09\u8bc1\u660e\u4e86\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u5b9e\u73b0\u4e2d\u7684\u6837\u672c\u5185\u6807\u51c6\u5316\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u57fa\u7840\u9ad8\u65af\u6838\u8fd1\u4f3c\uff1b2\uff09\u63a8\u5bfc\u51fa\u4e86\u5f31\u4fe1\u566a\u6bd4\u4e0b\u53ef\u9760\u5b66\u4e60\u7684\u4fe1\u606f\u7406\u8bba\u754c\u9650\uff1b3\uff09\u901a\u8fc7VC\u7ef4\u5ea6\u5206\u6790\u63ed\u793a\u4e86\u5cad\u56de\u5f52\u7684\u6709\u6548\u590d\u6742\u5ea6\u53d7\u9650\u4e8e\u6837\u672c\u5927\u5c0f\u800c\u975e\u540d\u4e49\u7279\u5f81\u7ef4\u5ea6\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6570\u503c\u9a8c\u8bc1\uff0c\u786e\u8ba4\u4e86\u8fd9\u4e9b\u7406\u8bba\u9884\u6d4b\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u5f53\u6837\u672c\u91cf\u8f83\u5c0f\u4e14\u7279\u5f81\u4e3a\u9ad8\u7ef4\u65f6\uff0c\u89c2\u5bdf\u5230\u7684\u9884\u6d4b\u6210\u529f\u5fc5\u7136\u7531\u4f4e\u590d\u6742\u5ea6\u7684\u4f2a\u5f71\u9a71\u52a8\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u9ad8\u7ef4\u5b66\u4e60\u3002"}}
{"id": "2506.03315", "pdf": "https://arxiv.org/pdf/2506.03315", "abs": "https://arxiv.org/abs/2506.03315", "authors": ["Kai Sauerwald", "Kenneth Skiba", "Eduardo Ferm\u00e9", "Thomas Meyer"], "title": "Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback", "categories": ["cs.AI", "cs.LO", "03E99, 91B14", "I.2.4"], "comment": null, "summary": "We study how linear orders can be employed to realise choice functions for\nwhich the set of potential choices is restricted, i.e., the possible choice is\nnot possible among the full powerset of all alternatives. In such restricted\nsettings, constructing a choice function via a relation on the alternatives is\nnot always possible. However, we show that one can always construct a choice\nfunction via a linear order on sets of alternatives, even when a fallback value\nis encoded as the minimal element in the linear order. The axiomatics of such\nchoice functions are presented for the general case and the case of\nunion-closed input restrictions. Restricted choice structures have applications\nin knowledge representation and reasoning, and here we discuss their\napplications for theory change and abstract argumentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u53d7\u9650\u9009\u62e9\u7ed3\u6784\u7684\u5b9e\u73b0\u4e0e\u516c\u7406\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ebf\u6027\u5e8f\u6784\u9020\u9009\u62e9\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528\u4e8e\u7406\u8bba\u66f4\u65b0\u548c\u62bd\u8c61\u8bba\u8bc1\u3002", "motivation": "\u8bba\u6587\u52a8\u673a\u662f\u89e3\u51b3\u6f5c\u5728\u9009\u62e9\u96c6\u88ab\u9650\u5236\u7684\u95ee\u9898\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u66ff\u4ee3\u5173\u7cfb\u7684\u9009\u62e9\u51fd\u6570\u6784\u9020\u65b9\u6cd5\u53ef\u80fd\u5931\u6548\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u7ebf\u6027\u5e8f\u6765\u5b9e\u73b0\u9009\u62e9\u51fd\u6570\uff0c\u5e76\u5bf9\u53d7\u9650\u8bbe\u7f6e\u4e0b\u7684\u516c\u7406\u5316\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4fbf\u662f\u5728\u53d7\u9650\u8bbe\u7f6e\u4e2d\uff0c\u4f7f\u7528\u66ff\u4ee3\u96c6\u5408\u4e0a\u7684\u7ebf\u6027\u5e8f\u4ecd\u80fd\u6709\u6548\u6784\u9020\u9009\u62e9\u51fd\u6570\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u77e5\u8bc6\u8868\u793a\u548c\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5373\u4f7f\u5728\u7f16\u7801\u56de\u9000\u503c\u4e3a\u7ebf\u6027\u5e8f\u4e2d\u7684\u6700\u5c0f\u5143\u7d20\u65f6\uff0c\u4e5f\u53ef\u4ee5\u59cb\u7ec8\u901a\u8fc7\u66ff\u4ee3\u96c6\u5408\u4e0a\u7684\u7ebf\u6027\u5e8f\u6765\u6784\u9020\u9009\u62e9\u51fd\u6570\u3002"}}
{"id": "2506.03158", "pdf": "https://arxiv.org/pdf/2506.03158", "abs": "https://arxiv.org/abs/2506.03158", "authors": ["Jiahao Qin", "Bei Peng", "Feng Liu", "Guangliang Cheng", "Lu Zong"], "title": "DUAL: Dynamic Uncertainty-Aware Learning", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 3 figures", "summary": "Deep learning models frequently encounter feature uncertainty in diverse\nlearning scenarios, significantly impacting their performance and reliability.\nThis challenge is particularly complex in multi-modal scenarios, where models\nmust integrate information from different sources with inherent uncertainties.\nWe propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that\neffectively handles feature uncertainty in both single-modal and multi-modal\nscenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty\nModeling, which continuously refines uncertainty estimates through joint\nconsideration of feature characteristics and learning dynamics; Adaptive\nDistribution-Aware Modulation, which maintains balanced feature distributions\nthrough dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal\nRelationship Learning, which explicitly models uncertainties in cross-modal\ninteractions. Through extensive experiments, we demonstrate DUAL's\neffectiveness across multiple domains: in computer vision tasks, it achieves\nsubstantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on\nCIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it\ndemonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy\non CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements\non MISR. The code will be available on GitHub soon.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDUAL\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u591a\u79cd\u5b66\u4e60\u573a\u666f\u4e2d\u7ecf\u5e38\u9047\u5230\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u8fd9\u663e\u8457\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u3002", "method": "DUAL\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u52a8\u6001\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u81ea\u9002\u5e94\u5206\u5e03\u611f\u77e5\u8c03\u5236\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8de8\u6a21\u6001\u5173\u7cfb\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDUAL\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u4e5f\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u589e\u76ca\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6DUAL\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.03819", "pdf": "https://arxiv.org/pdf/2506.03819", "abs": "https://arxiv.org/abs/2506.03819", "authors": ["Marc Aurel Vischer", "Noelia Otero", "Jackie Ma"], "title": "Spatially Resolved Meteorological and Ancillary Data in Central Europe for Rainfall Streamflow Modeling", "categories": ["stat.ML", "cs.LG", "I.2.1; I.6.5; J.2"], "comment": "6 pages, 1 figure", "summary": "We present a dataset for rainfall streamflow modeling that is fully spatially\nresolved with the aim of taking neural network-driven hydrological modeling\nbeyond lumped catchments. To this end, we compiled data covering five river\nbasins in central Europe: upper Danube, Elbe, Oder, Rhine, and Weser. The\ndataset contains meteorological forcings, as well as ancillary information on\nsoil, rock, land cover, and orography. The data is harmonized to a regular 9km\ntimes 9km grid and contains daily values that span from October 1981 to\nSeptember 2011. We also provide code to further combine our dataset with\npublicly available river discharge data for end-to-end rainfall streamflow\nmodeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a7a\u95f4\u5206\u8fa8\u7387\u9ad8\u7684\u964d\u96e8\u5f84\u6d41\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63a8\u52a8\u6c34\u6587\u5efa\u6a21\u4e2d\u795e\u7ecf\u7f51\u7edc\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u5c06\u795e\u7ecf\u7f51\u7edc\u9a71\u52a8\u7684\u6c34\u6587\u5efa\u6a21\u6269\u5c55\u5230\u7a7a\u95f4\u5206\u8fa8\u66f4\u7cbe\u7ec6\u7684\u9886\u57df\uff0c\u514b\u670d\u4f20\u7edf\u96c6\u603b\u5f0f\u6d41\u57df\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u6574\u5408\u4e86\u6db5\u76d6\u4e2d\u6b27\u4e94\u4e2a\u6cb3\u6d41\u6d41\u57df\u7684\u6570\u636e\uff0c\u5305\u62ec\u6c14\u8c61\u5f3a\u8feb\u6570\u636e\u4ee5\u53ca\u571f\u58e4\u3001\u5ca9\u77f3\u3001\u571f\u5730\u8986\u76d6\u548c\u5730\u5f62\u7684\u8f85\u52a9\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u7edf\u4e00\u4e3a\u89c4\u5219\u76849\u516c\u91cc\u00d79\u516c\u91cc\u7f51\u683c\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1981\u5e7410\u6708\u81f32011\u5e749\u65e5\u6bcf\u65e5\u6570\u636e\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ed3\u5408\u516c\u5f00\u6cb3\u6d41\u6d41\u91cf\u6570\u636e\u8fdb\u884c\u7aef\u5230\u7aef\u5efa\u6a21\u7684\u4ee3\u7801\u652f\u6301\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a7a\u95f4\u89e3\u6790\u5b8c\u6574\u7684\u964d\u96e8\u5f84\u6d41\u5efa\u6a21\u6570\u636e\u96c6\uff0c\u4ee5\u63a8\u52a8\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6c34\u6587\u6a21\u578b\u8d85\u8d8a\u96c6\u603b\u5f0f\u6d41\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2506.03332", "pdf": "https://arxiv.org/pdf/2506.03332", "abs": "https://arxiv.org/abs/2506.03332", "authors": ["Yifei Ming", "Zixuan Ke", "Xuan-Phi Nguyen", "Jiayu Wang", "Shafiq Joty"], "title": "Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows", "categories": ["cs.AI"], "comment": null, "summary": "Agentic workflows -- where multiple large language model (LLM) instances\ninteract to solve tasks -- are increasingly built on feedback mechanisms, where\none model evaluates and critiques another. Despite the promise of\nfeedback-driven improvement, the stability of agentic workflows rests on the\nreliability of the judge. However, judges may hallucinate information, exhibit\nbias, or act adversarially -- introducing critical vulnerabilities into the\nworkflow. In this work, we present a systematic analysis of agentic workflows\nunder deceptive or misleading feedback. We introduce a two-dimensional\nframework for analyzing judge behavior, along axes of intent (from constructive\nto malicious) and knowledge (from parametric-only to retrieval-augmented\nsystems). Using this taxonomy, we construct a suite of judge behaviors and\ndevelop WAFER-QA, a new benchmark with critiques grounded in retrieved web\nevidence to evaluate robustness of agentic workflows against factually\nsupported adversarial feedback. We reveal that even strongest agents are\nvulnerable to persuasive yet flawed critiques -- often switching correct\nanswers after a single round of misleading feedback. Taking a step further, we\nstudy how model predictions evolve over multiple rounds of interaction,\nrevealing distinct behavioral patterns between reasoning and non-reasoning\nmodels. Our findings highlight fundamental vulnerabilities in feedback-based\nworkflows and offer guidance for building more robust agentic systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u7814\u7a76\u4e86\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4f7f\u7528\u53cd\u9988\u673a\u5236\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u53cd\u9988\u9a71\u52a8\u7684\u6539\u8fdb\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u7a33\u5b9a\u6027\u53d6\u51b3\u4e8e\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027\u3002\u7136\u800c\uff0c\u8bc4\u5224\u8005\u53ef\u80fd\u4f1a\u4ea7\u751f\u5e7b\u89c9\u4fe1\u606f\u3001\u8868\u73b0\u51fa\u504f\u89c1\u6216\u91c7\u53d6\u5bf9\u6297\u884c\u4e3a\uff0c\u8fd9\u7ed9\u5de5\u4f5c\u6d41\u7a0b\u5e26\u6765\u4e86\u5173\u952e\u7684\u6f0f\u6d1e\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u6846\u67b6\u6765\u5206\u6790\u6cd5\u5b98\u884c\u4e3a\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u4e00\u5957\u6cd5\u5b98\u884c\u4e3a\u4f53\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86WAFER-QA\u8fd9\u4e00\u65b0\u57fa\u51c6\u6765\u8bc4\u4f30\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u4e8b\u5b9e\u652f\u6301\u5bf9\u6297\u6027\u53cd\u9988\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5373\u4f7f\u662f\u6700\u5f3a\u7684\u4ee3\u7406\u4e5f\u5bb9\u6613\u53d7\u5230\u5177\u6709\u8bf4\u670d\u529b\u4f46\u6709\u7f3a\u9677\u7684\u6279\u8bc4\u7684\u5f71\u54cd\uff0c\u5728\u4e00\u8f6e\u8bef\u5bfc\u6027\u53cd\u9988\u540e\u5e38\u5e38\u6539\u53d8\u6b63\u786e\u7684\u7b54\u6848\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u6a21\u578b\u9884\u6d4b\u5728\u591a\u8f6e\u4e92\u52a8\u4e2d\u7684\u6f14\u53d8\u60c5\u51b5\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u548c\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u7684\u4e0d\u540c\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u57fa\u4e8e\u53cd\u9988\u7684\u5de5\u4f5c\u6d41\u7a0b\u5b58\u5728\u6839\u672c\u6027\u7684\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u6709\u8bef\u5bfc\u6027\u7684\u53cd\u9988\u65f6\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e5f\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u7684\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.03159", "pdf": "https://arxiv.org/pdf/2506.03159", "abs": "https://arxiv.org/abs/2506.03159", "authors": ["Lesley Wheat", "Martin v. Mohrenschildt", "Saeid Habibi"], "title": "Bayes Error Rate Estimation in Difficult Situations", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": "21 pages, 13 figures, 20 tables", "summary": "The Bayes Error Rate (BER) is the fundamental limit on the achievable\ngeneralizable classification accuracy of any machine learning model due to\ninherent uncertainty within the data. BER estimators offer insight into the\ndifficulty of any classification problem and set expectations for optimal\nclassification performance. In order to be useful, the estimators must also be\naccurate with a limited number of samples on multivariate problems with unknown\nclass distributions. To determine which estimators meet the minimum\nrequirements for \"usefulness\", an in-depth examination of their accuracy is\nconducted using Monte Carlo simulations with synthetic data in order to obtain\ntheir confidence bounds for binary classification. To examine the usability of\nthe estimators on real-world applications, new test scenarios are introduced\nupon which 2500 Monte Carlo simulations per scenario are run over a wide range\nof BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized\nHenze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,\nresults show that kNN is overwhelmingly the more accurate non-parametric\nestimator. In order to reach the target of an under 5 percent range for the 95\npercent confidence bounds, the minimum number of required samples per class is\n1000. As more features are added, more samples are needed, so that 2500 samples\nper class are required at only 4 features. Other estimators do become more\naccurate than kNN as more features are added, but continuously fail to meet the\ntarget range.", "AI": {"tldr": "The study compares kNN, GHP, and KDE techniques for estimating Bayes Error Rate (BER) and finds that kNN is the most accurate non-parametric estimator, especially with fewer features.", "motivation": "To identify which BER estimators are 'useful' by evaluating their accuracy on both synthetic and real-world applications across a wide range of BER values.", "method": "Monte Carlo simulations with synthetic data and new test scenarios were used to examine the accuracy of kNN, GHP, and KDE techniques in estimating BER.", "result": "kNN was found to be the most accurate non-parametric estimator for binary classification problems. At least 1000 samples per class are required to achieve under 5 percent range for 95 percent confidence bounds, and this requirement increases with the number of features.", "conclusion": "kNN is the most accurate non-parametric estimator for BER when dealing with fewer features, but as more features are added, other estimators may become more accurate. However, they continuously fail to meet the target range."}}
{"id": "2506.03849", "pdf": "https://arxiv.org/pdf/2506.03849", "abs": "https://arxiv.org/abs/2506.03849", "authors": ["Benjamin Dupuis", "Dario Shariatian", "Maxime Haddouche", "Alain Durmus", "Umut Simsekli"], "title": "Algorithm- and Data-Dependent Generalization Bounds for Score-Based Generative Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Score-based generative models (SGMs) have emerged as one of the most popular\nclasses of generative models. A substantial body of work now exists on the\nanalysis of SGMs, focusing either on discretization aspects or on their\nstatistical performance. In the latter case, bounds have been derived, under\nvarious metrics, between the true data distribution and the distribution\ninduced by the SGM, often demonstrating polynomial convergence rates with\nrespect to the number of training samples. However, these approaches adopt a\nlargely approximation theory viewpoint, which tends to be overly pessimistic\nand relatively coarse. In particular, they fail to fully explain the empirical\nsuccess of SGMs or capture the role of the optimization algorithm used in\npractice to train the score network. To support this observation, we first\npresent simple experiments illustrating the concrete impact of optimization\nhyperparameters on the generalization ability of the generated distribution.\nThen, this paper aims to bridge this theoretical gap by providing the first\nalgorithmic- and data-dependent generalization analysis for SGMs. In\nparticular, we establish bounds that explicitly account for the optimization\ndynamics of the learning algorithm, offering new insights into the\ngeneralization behavior of SGMs. Our theoretical findings are supported by\nempirical results on several datasets.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u51c6\u786e\u7684\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u6cdb\u5316\u6027\u80fd\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4f18\u5316\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7ed3\u679c\u652f\u6301\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "motivation": "\u73b0\u6709\u5bf9SGMs\u7684\u7edf\u8ba1\u6027\u80fd\u5206\u6790\u8fc7\u4e8e\u60b2\u89c2\u4e14\u4e0d\u591f\u7cbe\u786e\uff0c\u65e0\u6cd5\u89e3\u91ca\u5176\u5728\u5b9e\u8df5\u4e2d\u7684\u6210\u529f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u7406\u8bba\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u7ed3\u5408\u7b97\u6cd5\u548c\u6570\u636e\u76f8\u5173\u7684\u56e0\u7d20\uff0c\u7814\u7a76SGMs\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u9996\u4e2a\u7ed3\u5408\u4f18\u5316\u52a8\u529b\u5b66\u7684SGMs\u6cdb\u5316\u754c\uff0c\u4e3a\u7406\u89e3SGMs\u7684\u6cdb\u5316\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SGMs\u6cdb\u5316\u5206\u6790\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u4f18\u5316\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002"}}
{"id": "2506.03469", "pdf": "https://arxiv.org/pdf/2506.03469", "abs": "https://arxiv.org/abs/2506.03469", "authors": ["Tuan Le", "Risal Shefin", "Debashis Gupta", "Thai Le", "Sarra Alqahtani"], "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration", "categories": ["cs.AI", "cs.LG"], "comment": "8 pages, 7 figures, European Conference on Artificial Intelligence\n  (ECAI)", "summary": "Ensuring the safety of reinforcement learning (RL) policies in high-stakes\nenvironments requires not only formal verification but also interpretability\nand targeted falsification. While model checking provides formal guarantees,\nits effectiveness is limited by abstraction quality and the completeness of the\nunderlying trajectory dataset. We propose a hybrid framework that integrates\n(1) explainability, (2) model checking, and (3) risk-guided falsification to\nachieve both rigor and coverage. Our approach begins by constructing a\nhuman-interpretable abstraction of the RL policy using Comprehensible Abstract\nPolicy Summarization (CAPS). This abstract graph, derived from offline\ntrajectories, is both verifier-friendly, semantically meaningful, and can be\nused as input to Storm probabilistic model checker to verify satisfaction of\ntemporal safety specifications. If the model checker identifies a violation, it\nwill return an interpretable counterexample trace by which the policy fails the\nsafety requirement. However, if no violation is detected, we cannot conclude\nsatisfaction due to potential limitation in the abstraction and coverage of the\noffline dataset. In such cases, we estimate associated risk during model\nchecking to guide a falsification strategy that prioritizes searching in\nhigh-risk states and regions underrepresented in the trajectory dataset. We\nfurther provide PAC-style guarantees on the likelihood of uncovering undetected\nviolations. Finally, we incorporate a lightweight safety shield that switches\nto a fallback policy at runtime when such a risk exceeds a threshold,\nfacilitating failure mitigation without retraining.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u3001\u6a21\u578b\u68c0\u9a8c\u548c\u98ce\u9669\u5f15\u5bfc\u53cd\u4f8b\u9a8c\u8bc1\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u786e\u4fdd\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5b89\u5168\u6027\u4e0d\u4ec5\u9700\u8981\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u8fd8\u9700\u8981\u53ef\u89e3\u91ca\u6027\u548c\u9488\u5bf9\u6027\u7684\u53cd\u4f8b\u9a8c\u8bc1\u3002\u73b0\u6709\u7684\u6a21\u578b\u68c0\u9a8c\u65b9\u6cd5\u53d7\u9650\u4e8e\u62bd\u8c61\u8d28\u91cf\u548c\u8f68\u8ff9\u6570\u636e\u96c6\u7684\u5b8c\u5907\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u9996\u5148\u4f7f\u7528\u53ef\u89e3\u91ca\u7b56\u7565\u6458\u8981\uff08CAPS\uff09\u6784\u5efa\u4eba\u7c7b\u53ef\u7406\u89e3\u7684RL\u7b56\u7565\u62bd\u8c61\uff1b\u63a5\u7740\u5229\u7528Storm\u6982\u7387\u6a21\u578b\u68c0\u9a8c\u5668\u9a8c\u8bc1\u65f6\u6001\u5b89\u5168\u89c4\u8303\u7684\u6ee1\u8db3\u6027\uff1b\u5982\u679c\u53d1\u73b0\u8fdd\u89c4\uff0c\u5219\u8fd4\u56de\u53ef\u89e3\u91ca\u7684\u53cd\u4f8b\u8f68\u8ff9\uff1b\u5982\u679c\u6ca1\u6709\u53d1\u73b0\u8fdd\u89c4\uff0c\u5219\u901a\u8fc7\u4f30\u8ba1\u98ce\u9669\u6765\u6307\u5bfc\u53cd\u4f8b\u641c\u7d22\u7b56\u7565\uff0c\u5e76\u63d0\u4f9bPAC\u98ce\u683c\u7684\u672a\u68c0\u6d4b\u8fdd\u89c4\u53ef\u80fd\u6027\u4fdd\u8bc1\uff1b\u6700\u540e\uff0c\u5728\u8fd0\u884c\u65f6\u8d85\u8fc7\u9608\u503c\u65f6\u5207\u6362\u5230\u5907\u7528\u7b56\u7565\u7684\u5b89\u5168\u4fdd\u62a4\u673a\u5236\u88ab\u5f15\u5165\u4ee5\u7f13\u89e3\u5931\u8d25\u5f71\u54cd\u3002", "result": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u62bd\u8c61\u7b56\u7565\u8fdb\u884c\u5b89\u5168\u6027\u9a8c\u8bc1\uff0c\u5e76\u5728\u65e0\u6cd5\u5b8c\u5168\u9a8c\u8bc1\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u98ce\u9669\u8bc4\u4f30\u5f15\u5bfc\u53cd\u4f8b\u641c\u7d22\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u7684\u672a\u68c0\u6d4b\u8fdd\u89c4\u53ef\u80fd\u6027\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd0\u884c\u65f6\u7684\u5b89\u5168\u4fdd\u62a4\u673a\u5236\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u7b56\u7565\u5931\u8d25\u7684\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89e3\u91ca\u6027\u3001\u6a21\u578b\u68c0\u9a8c\u548c\u98ce\u9669\u5f15\u5bfc\u7684\u53cd\u4f8b\u9a8c\u8bc1\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5b89\u5168\u6027\u4fdd\u969c\u3002"}}
{"id": "2506.03160", "pdf": "https://arxiv.org/pdf/2506.03160", "abs": "https://arxiv.org/abs/2506.03160", "authors": ["Shriyank Somvanshi", "Anannya Ghosh Tusti", "Mahmuda Sultana Mimi", "Md Monzurul Islam", "Sazzad Bin Bashar Polock", "Anandi Dutta", "Subasish Das"], "title": "Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes", "categories": ["cs.LG"], "comment": null, "summary": "The increasing presence of automated vehicles (AVs) presents new challenges\nfor crash classification and safety analysis. Accurately identifying the SAE\nautomation level involved in each crash is essential to understanding crash\ndynamics and system accountability. However, existing approaches often overlook\nautomation-specific factors and lack model sophistication to capture\ndistinctions between different SAE levels. To address this gap, this study\nevaluates the performance of three advanced tabular deep learning models\nMambaAttention, TabPFN, and TabTransformer for classifying SAE automation\nlevels using structured crash data from Texas (2024), covering 4,649 cases\ncategorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level\n2), and Advanced Automation (SAE Levels 3-5 combined). Following class\nbalancing using SMOTEENN, the models were trained and evaluated on a unified\ndataset of 7,300 records. MambaAttention demonstrated the highest overall\nperformance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),\nwhile TabPFN excelled in zero-shot inference with high robustness for rare\ncrash categories. In contrast, TabTransformer underperformed, particularly in\ndetecting Partial Automation crashes (F1-score: 55%), suggesting challenges in\nmodeling shared human-system control dynamics. These results highlight the\ncapability of deep learning models tailored for tabular data to enhance the\naccuracy and efficiency of automation-level classification. Integrating such\nmodels into crash analysis frameworks can support policy development, AV safety\nevaluation, and regulatory decisions, especially in distinguishing high-risk\nconditions for mid- and high-level automation technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6539\u8fdb\u81ea\u52a8\u5316\u8f66\u8f86\u78b0\u649e\u4e8b\u6545\u7684SAE\u7ea7\u522b\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u78b0\u649e\u4e8b\u4ef6\u4e2d\u7684SAE\u81ea\u52a8\u5316\u7ea7\u522b\u5bf9\u4e8e\u7406\u89e3\u78b0\u649e\u52a8\u6001\u548c\u7cfb\u7edf\u8d23\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u7279\u5b9a\u4e8e\u81ea\u52a8\u5316\u7684\u56e0\u7d20\u3002", "method": "\u4f7f\u7528MambaAttention\u3001TabPFN\u548cTabTransformer\u4e09\u79cd\u5148\u8fdb\u7684\u8868\u683c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u5fb7\u514b\u8428\u65af\u5dde\u7684\u78b0\u649e\u6570\u636e\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u91c7\u7528SMOTEENN\u65b9\u6cd5\u5e73\u8861\u7c7b\u522b\u5206\u5e03\u3002", "result": "MambaAttention\u603b\u4f53\u8868\u73b0\u6700\u4f73\uff08F1\u5206\u6570\uff1aSAE 1\u4e3a88%\uff0cSAE 2\u4e3a97%\uff0cSAE 3-5\u4e3a99%\uff09\uff0c\u800cTabPFN\u5728\u96f6\u6837\u672c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002TabTransformer\u5728\u68c0\u6d4b\u90e8\u5206\u81ea\u52a8\u5316\u78b0\u649e\u65f6\u8868\u73b0\u4e0d\u4f73\uff08F1\u5206\u6570\uff1a55%\uff09\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u81ea\u52a8\u5316\u6c34\u5e73\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u4e0e\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u653f\u7b56\u5236\u5b9a\u548cAV\u5b89\u5168\u8bc4\u4f30\u3002"}}
{"id": "2506.04193", "pdf": "https://arxiv.org/pdf/2506.04193", "abs": "https://arxiv.org/abs/2506.04193", "authors": ["Stephen R. Pfohl", "Natalie Harris", "Chirag Nagpal", "David Madras", "Vishwali Mhasawade", "Olawale Salaudeen", "Awa Dieng", "Shannon Sequeira", "Santiago Arciniegas", "Lillian Sung", "Nnamdi Ezeanochie", "Heather Cole-Lewis", "Katherine Heller", "Sanmi Koyejo", "Alexander D'Amour"], "title": "Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness", "categories": ["stat.ML", "cs.CY", "cs.LG"], "comment": null, "summary": "Disaggregated evaluation across subgroups is critical for assessing the\nfairness of machine learning models, but its uncritical use can mislead\npractitioners. We show that equal performance across subgroups is an unreliable\nmeasure of fairness when data are representative of the relevant populations\nbut reflective of real-world disparities. Furthermore, when data are not\nrepresentative due to selection bias, both disaggregated evaluation and\nalternative approaches based on conditional independence testing may be invalid\nwithout explicit assumptions regarding the bias mechanism. We use causal\ngraphical models to predict metric stability across subgroups under different\ndata generating processes. Our framework suggests complementing disaggregated\nevaluations with explicit causal assumptions and analysis to control for\nconfounding and distribution shift, including conditional independence testing\nand weighted performance estimation. These findings have broad implications for\nhow practitioners design and interpret model assessments given the ubiquity of\ndisaggregated evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u5728\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u516c\u5e73\u6027\u65f6\uff0c\u5355\u7eaf\u4f7f\u7528\u5206\u7ec4\u8bc4\u4f30\u53ef\u80fd\u5b58\u5728\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u5e94\u7ed3\u5408\u56e0\u679c\u5047\u8bbe\u548c\u5206\u6790\u65b9\u6cd5\u8fdb\u884c\u6539\u8fdb\u3002", "motivation": "\u52a8\u673a\u662f\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u4e2d\uff0c\u5206\u7ec4\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u8bef\u5bfc\u5b9e\u8df5\u8005\u7684\u95ee\u9898\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u56e0\u679c\u56fe\u6a21\u578b\u6765\u9884\u6d4b\u5728\u4e0d\u540c\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u8bc4\u4f30\u6307\u6807\u5728\u5404\u5b50\u7fa4\u4f53\u95f4\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6570\u636e\u5177\u6709\u4ee3\u8868\u6027\u4f46\u53cd\u6620\u73b0\u5b9e\u5dee\u5f02\u7684\u60c5\u51b5\u4e0b\uff0c\u5404\u5b50\u7fa4\u4f53\u6027\u80fd\u76f8\u540c\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u8861\u91cf\u516c\u5e73\u6027\uff1b\u800c\u5728\u5b58\u5728\u9009\u62e9\u504f\u5dee\u7684\u60c5\u51b5\u4e0b\uff0c\u5206\u7ec4\u8bc4\u4f30\u548c\u5176\u4ed6\u65b9\u6cd5\u53ef\u80fd\u65e0\u6548\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5206\u7ec4\u8bc4\u4f30\u5982\u679c\u4e0d\u7ed3\u5408\u660e\u786e\u7684\u56e0\u679c\u5047\u8bbe\u548c\u5206\u6790\uff0c\u53ef\u80fd\u65e0\u6cd5\u53ef\u9760\u5730\u8861\u91cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2506.03503", "pdf": "https://arxiv.org/pdf/2506.03503", "abs": "https://arxiv.org/abs/2506.03503", "authors": ["Shan Shan"], "title": "Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis", "categories": ["cs.AI"], "comment": null, "summary": "The quantification of social science remains a longstanding challenge,\nlargely due to the philosophical nature of its foundational theories. Although\nquantum computing has advanced rapidly in recent years, its relevance to social\ntheory remains underexplored. Most existing research focuses on micro-cognitive\nmodels or philosophical analogies, leaving a gap in system-level applications\nof quantum principles to the analysis of social systems. This study addresses\nthat gap by proposing a theoretical and computational framework that combines\nquantum mechanics with Generative AI to simulate the emergence and evolution of\nsocial norms. Drawing on core quantum concepts--such as superposition,\nentanglement, and probabilistic measurement--this research models society as a\ndynamic, uncertain system and sets up five ideal-type experiments. These\nscenarios are simulated using 25 generative agents, each assigned evolving\nroles as compliers, resistors, or enforcers. Within a simulated environment\nmonitored by a central observer (the Watcher), agents interact, respond to\nsurveillance, and adapt to periodic normative disruptions. These interactions\nallow the system to self-organize under external stress and reveal emergent\npatterns. Key findings show that quantum principles, when integrated with\ngenerative AI, enable the modeling of uncertainty, emergence, and\ninterdependence in complex social systems. Simulations reveal patterns\nincluding convergence toward normative order, the spread of resistance, and the\nspontaneous emergence of new equilibria in social rules. In conclusion, this\nstudy introduces a novel computational lens that lays the groundwork for a\nquantum-informed social theory. It offers interdisciplinary insights into how\nsociety can be understood not just as a structure to observe but as a dynamic\nsystem to simulate and redesign through quantum technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408\u91cf\u5b50\u529b\u5b66\u4e0e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u793e\u4f1a\u89c4\u8303\u6d8c\u73b0\u4e0e\u6f14\u5316\u7684\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u91cf\u5b50\u539f\u7406\u7684\u793e\u4f1a\u7cfb\u7edf\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u91cf\u5b50\u8ba1\u7b97\u5728\u793e\u4f1a\u7406\u8bba\u4e2d\u7684\u76f8\u5173\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u793e\u4f1a\u7cfb\u7edf\u7684\u7cfb\u7edf\u7ea7\u5e94\u7528\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u91cf\u5b50\u529b\u5b66\u4e0e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u548c\u8ba1\u7b97\u6846\u67b6\uff0c\u5229\u7528\u91cf\u5b50\u53e0\u52a0\u3001\u7ea0\u7f20\u548c\u6982\u7387\u6d4b\u91cf\u7b49\u6838\u5fc3\u6982\u5ff5\uff0c\u5bf925\u4e2a\u751f\u6210\u4ee3\u7406\u8fdb\u884c\u4e94\u4e2a\u7406\u60f3\u7c7b\u578b\u5b9e\u9a8c\u7684\u6a21\u62df\u3002", "result": "\u5173\u952e\u53d1\u73b0\u8868\u660e\uff0c\u5c06\u91cf\u5b50\u539f\u7406\u4e0e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7ed3\u5408\uff0c\u53ef\u4ee5\u5efa\u6a21\u590d\u6742\u793e\u4f1a\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u76f8\u4e92\u4f9d\u5b58\u5173\u7cfb\u3002\u6a21\u62df\u63ed\u793a\u4e86\u5411\u89c4\u8303\u79e9\u5e8f\u6536\u655b\u3001\u62b5\u6297\u4f20\u64ad\u4ee5\u53ca\u793e\u4f1a\u89c4\u5219\u4e2d\u65b0\u5e73\u8861\u7684\u81ea\u53d1\u51fa\u73b0\u7b49\u6a21\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u89c6\u89d2\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u793e\u4f1a\u7406\u8bba\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u8de8\u5b66\u79d1\u7684\u89c1\u89e3\uff0c\u5c06\u793e\u4f1a\u4e0d\u4ec5\u89c6\u4e3a\u89c2\u5bdf\u7684\u7ed3\u6784\uff0c\u8fd8\u89c6\u4e3a\u901a\u8fc7\u91cf\u5b50\u6280\u672f\u6a21\u62df\u548c\u91cd\u65b0\u8bbe\u8ba1\u7684\u52a8\u6001\u7cfb\u7edf\u3002"}}
{"id": "2506.03161", "pdf": "https://arxiv.org/pdf/2506.03161", "abs": "https://arxiv.org/abs/2506.03161", "authors": ["Mira Nuthakki"], "title": "Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment", "categories": ["cs.LG"], "comment": "18 pages, figures at end, methods at end. Format/order can be changed\n  if necessary", "summary": "Traffic congestion and collisions represent significant economic,\nenvironmental, and social challenges worldwide. Traditional traffic management\napproaches have shown limited success in addressing these complex, dynamic\nproblems. To address the current research gaps, three potential tools are\ndeveloped: a comprehensive 3D city-wide simulation environment that integrates\nboth macroscopic and microscopic traffic dynamics; a collision model; and a\nreinforcement learning framework with custom reward functions prioritizing\nsafety over efficiency. Unity game engine-based simulation is used for direct\ncollision modeling. A custom reward enabled reinforcement learning method,\nproximal policy optimization (PPO) model, yields substantial improvements over\nbaseline results, reducing the number of serious collisions, number of\nvehicle-vehicle collisions, and total distance travelled by over 3 times the\nbaseline values. The model also improves fuel efficiency by 39% and reduces\ncarbon emissions by 88%. Results establish feasibility for city-wide 3D traffic\nsimulation applications incorporating the vision-zero safety principles of the\nDepartment of Transportation, including physics-informed, adaptable, realistic\ncollision modeling, as well as appropriate reward modeling for real-world\ntraffic signal light control towards reducing collisions, optimizing traffic\nflow and reducing greenhouse emissions.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u7ed3\u5408\u5b8f\u89c2\u548c\u5fae\u89c2\u4ea4\u901a\u52a8\u6001\u76843D\u57ce\u5e02\u4ea4\u901a\u4eff\u771f\u73af\u5883\u3001\u4e00\u4e2a\u78b0\u649e\u6a21\u578b\u548c\u4e00\u4e2a\u4f18\u5148\u8003\u8651\u5b89\u5168\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6210\u529f\u51cf\u5c11\u4e86\u4ea4\u901a\u4e8b\u6545\u548c\u78b3\u6392\u653e\uff0c\u5e76\u4f18\u5316\u4e86\u4ea4\u901a\u6d41\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5f25\u8865\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u89e3\u51b3\u5168\u7403\u8303\u56f4\u5185\u56e0\u4ea4\u901a\u62e5\u5835\u548c\u78b0\u649e\u5e26\u6765\u7684\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u6311\u6218\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u4ea4\u901a\u7ba1\u7406\u7684\u6548\u679c\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6027\u76843D\u5168\u5e02\u8303\u56f4\u4ea4\u901a\u4eff\u771f\u73af\u5883\uff0c\u7ed3\u5408\u4e86\u5b8f\u89c2\u548c\u5fae\u89c2\u4ea4\u901a\u52a8\u529b\u5b66\uff1b\u4e00\u79cd\u78b0\u649e\u6a21\u578b\uff1b\u4ee5\u53ca\u4e00\u79cd\u4f18\u5148\u8003\u8651\u5b89\u5168\u800c\u975e\u6548\u7387\u7684\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u6b64\u5916\uff0c\u4f7f\u7528Unity\u6e38\u620f\u5f15\u64ce\u8fdb\u884c\u76f4\u63a5\u78b0\u649e\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u4e86PPO\uff08Proximal Policy Optimization\uff09\u6a21\u578b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u57fa\u7ebf\u7ed3\u679c\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e25\u91cd\u78b0\u649e\u6b21\u6570\u3001\u8f66\u8f86\u95f4\u78b0\u649e\u6b21\u6570\u548c\u603b\u884c\u9a76\u8ddd\u79bb\u5747\u51cf\u5c11\u4e86\u8d85\u8fc7\u4e09\u500d\uff0c\u71c3\u6cb9\u6548\u7387\u63d0\u9ad8\u4e8639%\uff0c\u78b3\u6392\u653e\u51cf\u5c11\u4e8688%\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5f00\u53d1\u7684Unity\u6e38\u620f\u5f15\u64ce\u6a21\u62df\u73af\u5883\u3001\u78b0\u649e\u6a21\u578b\u548c\u57fa\u4e8e\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4e25\u91cd\u78b0\u649e\u6b21\u6570\uff0c\u4f18\u5316\u4ea4\u901a\u6d41\u91cf\u5e76\u964d\u4f4e\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u3002"}}
{"id": "2506.00316", "pdf": "https://arxiv.org/pdf/2506.00316", "abs": "https://arxiv.org/abs/2506.00316", "authors": ["Atul Ganju", "Shashaank Aiyer", "Ved Sriraman", "Karthik Sridharan"], "title": "Active Learning via Regression Beyond Realizability", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "We present a new active learning framework for multiclass classification\nbased on surrogate risk minimization that operates beyond the standard\nrealizability assumption. Existing surrogate-based active learning algorithms\ncrucially rely on realizability$\\unicode{x2014}$the assumption that the optimal\nsurrogate predictor lies within the model class$\\unicode{x2014}$limiting their\napplicability in practical, misspecified settings. In this work we show that\nunder conditions significantly weaker than realizability, as long as the class\nof models considered is convex, one can still obtain a label and sample\ncomplexity comparable to prior work. Despite achieving similar rates, the\nalgorithmic approaches from prior works can be shown to fail in non-realizable\nsettings where our assumption is satisfied. Our epoch-based active learning\nalgorithm departs from prior methods by fitting a model from the full class to\nthe queried data in each epoch and returning an improper classifier obtained by\naggregating these models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a81\u7834\u6807\u51c6\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u7684\u4e3b\u52a8\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u5728\u975e\u73b0\u5b9e\u6761\u4ef6\u4e0b\u4f9d\u7136\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f80\u5f80\u53d7\u9650\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5e7f\u6cdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u4ee3\u7406\u98ce\u9669\u6700\u5c0f\u5316\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u5468\u671f\u6027\u7b97\u6cd5\u6765\u62df\u5408\u5b8c\u6574\u6a21\u578b\u7c7b\u522b\u7684\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5f31\u4e8e\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u7684\u6761\u4ef6\u4e0b\u4ecd\u80fd\u83b7\u5f97\u4e0e\u4e4b\u524d\u5de5\u4f5c\u76f8\u5f53\u7684\u6807\u7b7e\u548c\u6837\u672c\u590d\u6742\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u5373\u4f7f\u5728\u6a21\u578b\u7c7b\u522b\u4e0d\u5b8c\u5168\u7b26\u5408\u5b9e\u9645\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002"}}
{"id": "2506.03543", "pdf": "https://arxiv.org/pdf/2506.03543", "abs": "https://arxiv.org/abs/2506.03543", "authors": ["Wanghao Ye", "Sihan Chen", "Yiting Wang", "Shwai He", "Bowei Tian", "Guoheng Sun", "Ziyi Wang", "Ziyao Wang", "Yexiao He", "Zheyu Shen", "Meng Liu", "Yuning Zhang", "Meng Feng", "Yang Wang", "Siyuan Peng", "Yilong Dai", "Zhenle Duan", "Hanzhang Qin", "Ang Li"], "title": "CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications", "categories": ["cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "Current large language model (LLM) agents lack authentic human psychological\nprocesses necessary for genuine digital twins and social AI applications. To\naddress this limitation, we present a computational implementation of Global\nWorkspace Theory (GNWT) that integrates human cognitive architecture principles\ninto LLM agents, creating specialized sub-agents for emotion, memory, social\nnorms, planning, and goal-tracking coordinated through a global workspace\nmechanism. However, authentic digital twins require accurate personality\ninitialization. We therefore develop a novel adventure-based personality test\nthat evaluates true personality through behavioral choices within interactive\nscenarios, bypassing self-presentation bias found in traditional assessments.\nBuilding on these innovations, our CogniPair platform enables digital twins to\nengage in realistic simulated dating interactions and job interviews before\nreal encounters, providing bidirectional cultural fit assessment for both\nromantic compatibility and workplace matching. Validation using 551 GNWT-Agents\nand Columbia University Speed Dating dataset demonstrates 72% correlation with\nhuman attraction patterns, 77.8% match prediction accuracy, and 74% agreement\nin human validation studies. This work advances psychological authenticity in\nLLM agents and establishes a foundation for intelligent dating platforms and HR\ntechnology solutions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u5de5\u4f5c\u533a\u7406\u8bba\u548c\u65b0\u578b\u4eba\u683c\u6d4b\u8bd5\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5fc3\u7406\u771f\u5b9e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6a21\u62df\u7ea6\u4f1a\u548c\u804c\u573a\u5339\u914d\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7f3a\u4e4f\u771f\u5b9e\u7684\u4eba\u7c7b\u5fc3\u7406\u8fc7\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u6570\u5b57\u5b6a\u751f\u548c\u793e\u4f1a\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u529b\u548c\u9002\u7528\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u81f4\u529b\u4e8e\u589e\u5f3aLLM\u4ee3\u7406\u7684\u5fc3\u7406\u771f\u5b9e\u6027\u3002", "method": "\u7814\u7a76\u8005\u4eec\u91c7\u7528\u4e86\u5168\u5c40\u5de5\u4f5c\u533a\u7406\u8bba\uff08GNWT\uff09\u7684\u8ba1\u7b97\u5b9e\u73b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u8ba4\u77e5\u67b6\u6784\u539f\u5219\uff0c\u6784\u5efa\u4e86\u5177\u6709\u60c5\u611f\u3001\u8bb0\u5fc6\u3001\u793e\u4f1a\u89c4\u8303\u3001\u89c4\u5212\u548c\u76ee\u6807\u8ddf\u8e2a\u7b49\u5b50\u4ee3\u7406\u7684LLM\u7cfb\u7edf\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5192\u9669\u7684\u4eba\u683c\u6d4b\u8bd5\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e551\u4e2aGNWT\u4ee3\u7406\u548c\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66\u5feb\u901f\u7ea6\u4f1a\u6570\u636e\u96c6\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4e0e\u4eba\u7c7b\u5438\u5f15\u529b\u6a21\u5f0f\u7684\u76f8\u5173\u6027\u8fbe\u523072%\uff0c\u5339\u914d\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523077.8%\uff0c\u5728\u4eba\u7c7b\u9a8c\u8bc1\u7814\u7a76\u4e2d\u4e00\u81f4\u6027\u8fbe\u523074%\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u5c06\u5168\u5c40\u5de5\u4f5c\u533a\u7406\u8bba\uff08GNWT\uff09\u8ba1\u7b97\u5316\uff0c\u5e76\u7ed3\u5408\u5192\u9669\u5f0f\u4eba\u683c\u6d4b\u8bd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5fc3\u7406\u771f\u5b9e\u6027\uff0c\u4ece\u800c\u5728\u667a\u80fd\u7ea6\u4f1a\u5e73\u53f0\u548c\u4eba\u529b\u8d44\u6e90\u6280\u672f\u89e3\u51b3\u65b9\u6848\u4e2d\u5b9e\u73b0\u5e94\u7528\u3002"}}
{"id": "2506.03163", "pdf": "https://arxiv.org/pdf/2506.03163", "abs": "https://arxiv.org/abs/2506.03163", "authors": ["Oluwaseyi Giwa"], "title": "Causal Discovery in Dynamic Fading Wireless Networks", "categories": ["cs.LG", "eess.SP", "stat.ME"], "comment": "5 pages, 3 figures", "summary": "Dynamic causal discovery in wireless networks is essential due to evolving\ninterference, fading, and mobility, which complicate traditional static causal\nmodels. This paper addresses causal inference challenges in dynamic fading\nwireless environments by proposing a sequential regression-based algorithm with\na novel application of the NOTEARS acyclicity constraint, enabling efficient\nonline updates. We derive theoretical lower and upper bounds on the detection\ndelay required to identify structural changes, explicitly quantifying their\ndependence on network size, noise variance, and fading severity. Monte Carlo\nsimulations validate these theoretical results, demonstrating linear increases\nin detection delay with network size, quadratic growth with noise variance, and\ninverse-square dependence on the magnitude of structural changes. Our findings\nprovide rigorous theoretical insights and practical guidelines for designing\nrobust online causal inference mechanisms to maintain network reliability under\nnonstationary wireless conditions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a8\u6001\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u987a\u5e8f\u56de\u5f52\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5f71\u54cd\u7ed3\u6784\u53d8\u5316\u68c0\u6d4b\u5ef6\u8fdf\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u7531\u4e8e\u5e72\u6270\u3001\u8870\u843d\u548c\u79fb\u52a8\u6027\u7684\u6f14\u53d8\uff0c\u4f20\u7edf\u7684\u9759\u6001\u56e0\u679c\u6a21\u578b\u96be\u4ee5\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u52a8\u6001\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u4ee5\u4fdd\u6301\u7f51\u7edc\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u987a\u5e8f\u56de\u5f52\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u4e86\u8bc6\u522b\u7ed3\u6784\u53d8\u5316\u6240\u9700\u7684\u68c0\u6d4b\u5ef6\u8fdf\u7684\u7406\u8bba\u4e0a\u4e0b\u754c\u3002", "result": "\u63a8\u5bfc\u51fa\u68c0\u6d4b\u5ef6\u8fdf\u7684\u7406\u8bba\u4e0a\u4e0b\u754c\uff0c\u8868\u660e\u5176\u4e0e\u7f51\u7edc\u89c4\u6a21\u5448\u7ebf\u6027\u589e\u52a0\u3001\u4e0e\u566a\u58f0\u65b9\u5dee\u5448\u4e8c\u6b21\u589e\u957f\u3001\u4e0e\u7ed3\u6784\u53d8\u5316\u5e45\u5ea6\u5448\u5e73\u65b9\u53cd\u6bd4\u5173\u7cfb\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u8870\u843d\u65e0\u7ebf\u73af\u5883\u7684\u987a\u5e8f\u56de\u5f52\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408NOTEARS\u65e0\u73af\u7ea6\u675f\u7684\u65b0\u5e94\u7528\uff0c\u4e3a\u5728\u975e\u5e73\u7a33\u65e0\u7ebf\u6761\u4ef6\u4e0b\u8bbe\u8ba1\u9c81\u68d2\u7684\u5728\u7ebf\u56e0\u679c\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2506.03548", "pdf": "https://arxiv.org/pdf/2506.03548", "abs": "https://arxiv.org/abs/2506.03548", "authors": ["Chenglong Ye", "Gang Xiong", "Junyou Shang", "Xingyuan Dai", "Xiaoyan Gong", "Yisheng Lv"], "title": "SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Traffic simulation tools, such as SUMO, are essential for urban mobility\nresearch. However, such tools remain challenging for users due to complex\nmanual workflows involving network download, demand generation, simulation\nsetup, and result analysis. In this paper, we introduce SUMO-MCP, a novel\nplatform that not only wraps SUMO' s core utilities into a unified tool suite\nbut also provides additional auxiliary utilities for common preprocessing and\npostprocessing tasks. Using SUMO-MCP, users can issue simple natural-language\nprompts to generate traffic scenarios from OpenStreetMap data, create demand\nfrom origin-destination matrices or random patterns, run batch simulations with\nmultiple signal-control strategies, perform comparative analyses with automated\nreporting, and detect congestion for signal-timing optimization. Furthermore,\nthe platform allows flexible custom workflows by dynamically combining exposed\nSUMO tools without additional coding. Experiments demonstrate that SUMO-MCP\nsignificantly makes traffic simulation more accessible and reliable for\nresearchers. We will release code for SUMO-MCP at\nhttps://github.com/ycycycl/SUMO-MCP in the future.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u901a\u4eff\u771f\u5e73\u53f0SUMO-MCP\uff0c\u5b83\u7b80\u5316\u4e86\u4f20\u7edf\u5de5\u5177\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u63d0\u4f9b\u4e86\u66f4\u591a\u7684\u8f85\u52a9\u529f\u80fd\uff0c\u4f7f\u5f97\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u4ea4\u901a\u573a\u666f\u3001\u521b\u5efa\u9700\u6c42\u3001\u8fd0\u884c\u6279\u91cf\u4eff\u771f\u5e76\u8fdb\u884c\u5206\u6790\uff0c\u4ece\u800c\u63d0\u9ad8\u4ea4\u901a\u4eff\u771f\u7684\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u4ea4\u901a\u4eff\u771f\u5de5\u5177\u5982SUMO\u7531\u4e8e\u590d\u6742\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u5bf9\u7528\u6237\u6765\u8bf4\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86SUMO-MCP\u5e73\u53f0\uff0c\u5c06SUMO\u7684\u6838\u5fc3\u5de5\u5177\u5c01\u88c5\u5230\u7edf\u4e00\u7684\u5de5\u5177\u5957\u4ef6\u4e2d\uff0c\u5e76\u63d0\u4f9b\u989d\u5916\u7684\u8f85\u52a9\u5de5\u5177\u7528\u4e8e\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSUMO-MCP\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u901a\u4eff\u771f\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "SUMO-MCP\u4f7f\u4ea4\u901a\u4eff\u771f\u5bf9\u4e8e\u7814\u7a76\u4eba\u5458\u6765\u8bf4\u66f4\u52a0\u6613\u7528\u548c\u53ef\u9760\uff0c\u4ee3\u7801\u5c06\u5728\u672a\u6765\u901a\u8fc7GitHub\u53d1\u5e03\u3002"}}
{"id": "2506.03164", "pdf": "https://arxiv.org/pdf/2506.03164", "abs": "https://arxiv.org/abs/2506.03164", "authors": ["Vignav Ramesh", "Morteza Mardani"], "title": "Test-Time Scaling of Diffusion Models via Noise Trajectory Search", "categories": ["cs.LG"], "comment": null, "summary": "The iterative and stochastic nature of diffusion models enables test-time\nscaling, whereby spending additional compute during denoising generates\nhigher-fidelity samples. Increasing the number of denoising steps is the\nprimary scaling axis, but this yields quickly diminishing returns. Instead\noptimizing the noise trajectory--the sequence of injected noise vectors--is\npromising, as the specific noise realizations critically affect sample quality;\nbut this is challenging due to a high-dimensional search space, complex\nnoise-outcome interactions, and costly trajectory evaluations. We address this\nby first casting diffusion as a Markov Decision Process (MDP) with a terminal\nreward, showing tree-search methods such as Monte Carlo tree search (MCTS) to\nbe meaningful but impractical. To balance performance and efficiency, we then\nresort to a relaxation of MDP, where we view denoising as a sequence of\nindependent contextual bandits. This allows us to introduce an\n$\\epsilon$-greedy search algorithm that globally explores at extreme timesteps\nand locally exploits during the intermediate steps where de-mixing occurs.\nExperiments on EDM and Stable Diffusion reveal state-of-the-art scores for\nclass-conditioned/text-to-image generation, exceeding baselines by up to\n$164\\%$ and matching/exceeding MCTS performance. To our knowledge, this is the\nfirst practical method for test-time noise trajectory optimization of arbitrary\n(non-differentiable) rewards.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u566a\u58f0\u8f68\u8ff9\u6765\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u751f\u6210\u6837\u672c\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4efb\u610f\u5956\u52b1\u51fd\u6570\u4e0b\u5177\u6709\u5b9e\u7528\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u548c\u968f\u673a\u7279\u6027\u4f7f\u5176\u5728\u6d4b\u8bd5\u65f6\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u7684\u4e3b\u8981\u6269\u5c55\u65b9\u5f0f\uff08\u589e\u52a0\u53bb\u566a\u6b65\u9aa4\uff09\u5b58\u5728\u6536\u76ca\u9012\u51cf\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u4f18\u5316\u566a\u58f0\u8f68\u8ff9\u4ee5\u63d0\u5347\u6837\u672c\u8d28\u91cf\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u5c06\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u5177\u6709\u7ec8\u7aef\u5956\u52b1\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u8fdb\u4e00\u6b65\u653e\u677e\u4e3a\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u4ece\u800c\u5f15\u5165\u4e00\u79cd\u03b5-\u8d2a\u5fc3\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u5168\u5c40\u63a2\u7d22\u548c\u5c40\u90e8\u5229\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7c7b\u6761\u4ef6\u751f\u6210\u548c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5f97\u5206\u8d85\u8fc7\u57fa\u7ebf\u6700\u591a\u8fbe164%\uff0c\u5e76\u4e14\u5339\u914d\u6216\u8d85\u8fc7\u4e86MCTS\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u6709\u6548\u4f18\u5316\u4efb\u610f\u5956\u52b1\u51fd\u6570\u4e0b\u7684\u566a\u58f0\u8f68\u8ff9\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.03302", "pdf": "https://arxiv.org/pdf/2506.03302", "abs": "https://arxiv.org/abs/2506.03302", "authors": ["James Bagrow", "Josh Bongard"], "title": "Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony", "categories": ["cs.LG", "cs.NE", "physics.data-an", "stat.ML"], "comment": "14 pages, 7 figures, 2 tables", "summary": "Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with\ninterpretability, making them valuable for scientific modeling. However, it is\nunclear a priori how deep a network needs to be for any given task, and deeper\nKANs can be difficult to optimize. Here we introduce multi-exit KANs, where\neach layer includes its own prediction branch, enabling the network to make\naccurate predictions at multiple depths simultaneously. This architecture\nprovides deep supervision that improves training while discovering the right\nlevel of model complexity for each task. Multi-exit KANs consistently\noutperform standard, single-exit versions on synthetic functions, dynamical\nsystems, and real-world datasets. Remarkably, the best predictions often come\nfrom earlier, simpler exits, revealing that these networks naturally identify\nsmaller, more parsimonious and interpretable models without sacrificing\naccuracy. To automate this discovery, we develop a differentiable \"learning to\nexit\" algorithm that balances contributions from exits during training. Our\napproach offers scientists a practical way to achieve both high performance and\ninterpretability, addressing a fundamental challenge in machine learning for\nscientific discovery.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd multi-exit KANs\uff0c\u5b83\u80fd\u5728\u591a\u4e2a\u6df1\u5ea6\u540c\u65f6\u8fdb\u884c\u9884\u6d4b\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u679c\u5e76\u81ea\u52a8\u627e\u5230\u6bcf\u4e2a\u4efb\u52a1\u7684\u6700\u4f73\u6a21\u578b\u590d\u6742\u5ea6\u7ea7\u522b\u3002", "motivation": "Kolmogorov-Arnold Networks (KANs) \u867d\u7136\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4e8b\u5148\u4e0d\u6e05\u695a\u4efb\u4f55\u7ed9\u5b9a\u4efb\u52a1\u9700\u8981\u591a\u6df1\u7684\u7f51\u7edc\uff0c\u66f4\u6df1\u7684KAN\u53ef\u80fd\u96be\u4ee5\u4f18\u5316\u3002", "method": "\u5f15\u5165 multi-exit KAN \u67b6\u6784\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u5fae\u7684\u201clearning to exit\u201d\u7b97\u6cd5\u3002", "result": "multi-exit KANs \u5728\u5408\u6210\u51fd\u6570\u3001\u52a8\u529b\u7cfb\u7edf\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u7684\u5355\u51fa\u53e3\u7248\u672c\u3002", "conclusion": "multi-exit KANs \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u53d1\u73b0\u4e2d\u673a\u5668\u5b66\u4e60\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002"}}
{"id": "2506.03586", "pdf": "https://arxiv.org/pdf/2506.03586", "abs": "https://arxiv.org/abs/2506.03586", "authors": ["Yu Ma", "Chongtao Guo", "Le Liang", "Xiao Li", "Shi Jin"], "title": "Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "This paper investigates a joint phase design and resource allocation problem\nin downlink reconfigurable intelligent surface (RIS)-assisted orthogonal\nfrequency division multiplexing (OFDM) systems to optimize average delay, where\ndata packets for each user arrive at the base station stochastically. The\nsequential optimization problem is inherently a Markov decision process (MDP),\nmaking it fall within the scope of reinforcement learning. To effectively\nhandle the mixed action space and reduce the state space dimensionality, a\nhybrid deep reinforcement learning (DRL) approach is proposed. Specifically,\nproximal policy optimization (PPO)-$\\Theta$ is employed to optimize RIS phase\nshift design, while PPO-N is responsible for subcarrier allocation decisions.\nTo further mitigate the curse of dimensionality associated with subcarrier\nallocation, a multi-agent strategy is introduced to optimize subcarrier\nallocation indicater more efficiently. Moreover, to achieve more adaptive\nresource allocation and accurately capture network dynamics, key factors\nclosely related to average delay, including the number of backlogged packets in\nbuffers and the current packet arrivals, are incorporated into the state space.\nFurthermore, a transfer learning framework is introduced to enhance training\nefficiency and accelerate convergence. Simulation results demonstrate that the\nproposed algorithm significantly reduces average delay, enhances resource\nallocation efficiency, and achieves superior system robustness and fairness\ncompared to baseline methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u667a\u80fd\u4f53\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u5728RIS\u8f85\u52a9OFDM\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u76f8\u4f4d\u8bbe\u8ba1\u4e0e\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u964d\u4f4e\u5e73\u5747\u5ef6\u8fdf\u5e76\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u7528\u6237\u6570\u636e\u5305\u968f\u673a\u5230\u8fbe\u57fa\u7ad9\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u5e73\u5747\u5ef6\u8fdf\uff0c\u7814\u7a76\u4e86\u4e0b\u884c\u94fe\u8def\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u8f85\u52a9\u6b63\u4ea4\u9891\u5206\u590d\u7528\uff08OFDM\uff09\u7cfb\u7edf\u4e2d\u7684\u8054\u5408\u76f8\u4f4d\u8bbe\u8ba1\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\uff0c\u5305\u62ecPPO-\u0398\u7528\u4e8e\u4f18\u5316RIS\u76f8\u79fb\u8bbe\u8ba1\uff0cPPO-N\u7528\u4e8e\u5b50\u8f7d\u6ce2\u5206\u914d\u51b3\u7b56\uff0c\u5e76\u5f15\u5165\u591a\u667a\u80fd\u4f53\u7b56\u7565\u4ee5\u7f13\u89e3\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u8d44\u6e90\u5206\u914d\u6548\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u4f18\u8d8a\u7684\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316RIS\u8f85\u52a9OFDM\u7cfb\u7edf\u7684\u4e0b\u884c\u94fe\u8def\u4e2d\u7684\u76f8\u4f4d\u8bbe\u8ba1\u548c\u8d44\u6e90\u5206\u914d\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5e73\u5747\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2506.03176", "pdf": "https://arxiv.org/pdf/2506.03176", "abs": "https://arxiv.org/abs/2506.03176", "authors": ["Bin Wang", "Yongqi Han", "Minbo Ma", "Tianrui Li", "Junbo Zhang", "Feng Hong", "Yanwei Yu"], "title": "Non-collective Calibrating Strategy for Time Series Forecasting", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Deep learning-based approaches have demonstrated significant advancements in\ntime series forecasting. Despite these ongoing developments, the complex\ndynamics of time series make it challenging to establish the rule of thumb for\ndesigning the golden model architecture. In this study, we argue that refining\nexisting advanced models through a universal calibrating strategy can deliver\nsubstantial benefits with minimal resource costs, as opposed to elaborating and\ntraining a new model from scratch. We first identify a multi-target learning\nconflict in the calibrating process, which arises when optimizing variables\nacross time steps, leading to the underutilization of the model's learning\ncapabilities. To address this issue, we propose an innovative calibrating\nstrategy called Socket+Plug (SoP). This approach retains an exclusive optimizer\nand early-stopping monitor for each predicted target within each Plug while\nkeeping the fully trained Socket backbone frozen. The model-agnostic nature of\nSoP allows it to directly calibrate the performance of any trained deep\nforecasting models, regardless of their specific architectures. Extensive\nexperiments on various time series benchmarks and a spatio-temporal\nmeteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up\nto a 22% improvement even when employing a simple MLP as the Plug (highlighted\nin Figure 1)", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6a21\u578b\u6821\u51c6\u7b56\u7565 SoP\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u65f6\u5e8f\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u65f6\u5e8f\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8bbe\u8ba1\u6700\u4f18\u6a21\u578b\u67b6\u6784\u4ecd\u9762\u4e34\u6311\u6218\u3002\u901a\u8fc7\u901a\u7528\u7684\u6821\u51c6\u7b56\u7565\u6539\u8fdb\u73b0\u6709\u6a21\u578b\u80fd\u591f\u4ee5\u8f83\u4f4e\u8d44\u6e90\u6210\u672c\u5e26\u6765\u663e\u8457\u6536\u76ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Socket+Plug (SoP) \u7684\u6821\u51c6\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4e3a\u6bcf\u4e2a\u9884\u6d4b\u76ee\u6807\u4fdd\u7559\u72ec\u7acb\u7684\u4f18\u5316\u5668\u548c\u65e9\u505c\u76d1\u63a7\uff0c\u5e76\u5728\u6821\u51c6\u8fc7\u7a0b\u4e2d\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684 Socket \u4e3b\u5e72\u7f51\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u5e8f\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6c14\u8c61\u6570\u636e\u96c6 ERA5 \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSoP \u7b56\u7565\u53ef\u5e26\u6765\u6700\u9ad8 22% \u7684\u6027\u80fd\u63d0\u5347\uff0c\u5373\u4f7f\u4f7f\u7528\u7b80\u5355\u7684 MLP \u4f5c\u4e3a Plug \u4e5f\u80fd\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "conclusion": "Socket+Plug (SoP) \u7b56\u7565\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5df2\u6709\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u67b6\u6784\u7684\u6a21\u578b\u3002"}}
{"id": "2506.03363", "pdf": "https://arxiv.org/pdf/2506.03363", "abs": "https://arxiv.org/abs/2506.03363", "authors": ["Divya Shyamal", "Jiaqi Zhang", "Caroline Uhler"], "title": "Probabilistic Factorial Experimental Design for Combinatorial Interventions", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": null, "summary": "A combinatorial intervention, consisting of multiple treatments applied to a\nsingle unit with potentially interactive effects, has substantial applications\nin fields such as biomedicine, engineering, and beyond. Given $p$ possible\ntreatments, conducting all possible $2^p$ combinatorial interventions can be\nlaborious and quickly becomes infeasible as $p$ increases. Here we introduce\nprobabilistic factorial experimental design, formalized from how scientists\nperform lab experiments. In this framework, the experimenter selects a dosage\nfor each possible treatment and applies it to a group of units. Each unit\nindependently receives a random combination of treatments, sampled from a\nproduct Bernoulli distribution determined by the dosages. Additionally, the\nexperimenter can carry out such experiments over multiple rounds, adapting the\ndesign in an active manner. We address the optimal experimental design problem\nwithin an intervention model that imposes bounded-degree interactions between\ntreatments. In the passive setting, we provide a closed-form solution for the\nnear-optimal design. Our results prove that a dosage of $\\tfrac{1}{2}$ for each\ntreatment is optimal up to a factor of $1+O(\\tfrac{\\ln(n)}{n})$ for estimating\nany $k$-way interaction model, regardless of $k$, and imply that\n$O\\big(kp^{3k}\\ln(p)\\big)$ observations are required to accurately estimate\nthis model. For the multi-round setting, we provide a near-optimal acquisition\nfunction that can be numerically optimized. We also explore several extensions\nof the design problem and finally validate our findings through simulations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u56e0\u5b50\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4ee5\u9ad8\u6548\u4f30\u8ba1\u5904\u7406\u95f4\u7684\u4ea4\u4e92\u6548\u5e94\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u5b9e\u9a8c\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7ec4\u5408\u5e72\u9884\u5728\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5f53\u5904\u7406\u6570\u91cf\u589e\u52a0\u65f6\uff0c\u6240\u6709\u53ef\u80fd\u7ec4\u5408\u7684\u5b9e\u9a8c\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u6982\u7387\u56e0\u5b50\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5e76\u5728\u88ab\u52a8\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u4e86\u5c01\u95ed\u5f62\u5f0f\u7684\u8fd1\u4f3c\u6700\u4f18\u89e3\uff0c\u5728\u591a\u8f6e\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u4e86\u53ef\u6570\u503c\u4f18\u5316\u7684\u8fd1\u4f3c\u6700\u4f18\u83b7\u53d6\u51fd\u6570\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u4efb\u4f55k\u9636\u4ea4\u4e92\u6a21\u578b\uff0c\u6bcf\u79cd\u5904\u7406\u4f7f\u75280.5\u7684\u5242\u91cf\u662f\u6700\u4f18\u7684\uff0c\u4e14\u9700\u8981O(kp^(3k)ln(p))\u6b21\u89c2\u6d4b\u6765\u51c6\u786e\u4f30\u8ba1\u6a21\u578b\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u6700\u4f18\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4efb\u4f55k\u9636\u4ea4\u4e92\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u8bbe\u7f6e\u4e0b\u7684\u8fd1\u4f3c\u6700\u4f18\u83b7\u53d6\u51fd\u6570\u3002"}}
{"id": "2506.03610", "pdf": "https://arxiv.org/pdf/2506.03610", "abs": "https://arxiv.org/abs/2506.03610", "authors": ["Dongmin Park", "Minkyu Kim", "Beongjun Choi", "Junhyuck Kim", "Keon Lee", "Jonghyun Lee", "Inkyu Park", "Byeong-Uk Lee", "Jaeyoung Hwang", "Jaewoo Ahn", "Ameya S. Mahabaleshwarkar", "Bilal Kartal", "Pritam Biswas", "Yoshi Suhara", "Kangwook Lee", "Jaewoong Cho"], "title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents are reshaping the game industry,\nparticularly with more intelligent and human-preferable game characters.\nHowever, existing game benchmarks fall short of practical needs: they lack\nevaluations of diverse LLM capabilities across various game genres, studies of\nagentic modules crucial for complex gameplay, and fine-tuning datasets for\naligning pre-trained LLMs into gaming agents. To fill these gaps, we present\n\\textbf{\\benchname{}}, a foundational benchmark designed to train and evaluate\nLLM agents across diverse real-world video games. Unlike existing benchmarks,\nOrak includes 12 popular video games spanning all major genres, enabling\ncomprehensive studies of LLM capabilities and agentic modules essential for\nintricate game scenarios. To support consistent evaluation of LLMs, we\nintroduce a plug-and-play interface based on Model Context Protocol (MCP) that\nenables LLMs to seamlessly connect with games and manipulate agentic modules.\nAdditionally, we propose a fine-tuning dataset, consisting of LLM gameplay\ntrajectories across diverse game genres. Orak offers a comprehensive evaluation\nframework, encompassing general game score leaderboards, LLM battle arenas, and\nin-depth analyses of visual input state, agentic strategies, and fine-tuning\neffects, establishing a foundation towards building generic gaming agents. Code\nis available at https://github.com/krafton-ai/Orak.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOrak\u7684\u65b0\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6e38\u620f\u4ee3\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u591a\u6837\u5316LLM\u80fd\u529b\u3001\u590d\u6742\u6e38\u620f\u573a\u666f\u6240\u9700\u4ee3\u7406\u6a21\u5757\u7814\u7a76\u53ca\u9884\u8bad\u7ec3LLM\u5411\u6e38\u620f\u4ee3\u7406\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aOrak\u7684\u57fa\u7840\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec12\u6b3e\u4e3b\u6d41\u89c6\u9891\u6e38\u620f\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eModel Context Protocol (MCP)\u7684\u5373\u63d2\u5373\u7528\u63a5\u53e3\u4ee5\u53ca\u4e00\u4e2a\u7528\u4e8e\u5fae\u8c03\u7684LLM\u6e38\u620f\u8f68\u8ff9\u6570\u636e\u96c6\u3002", "result": "Orak\u652f\u6301\u5bf9LLM\u80fd\u529b\u548c\u4ee3\u7406\u6a21\u5757\u8fdb\u884c\u5168\u9762\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u6e38\u620f\u5f97\u5206\u6392\u884c\u699c\u3001LLM\u6218\u6597\u7ade\u6280\u573a\u4ee5\u53ca\u5bf9\u89c6\u89c9\u8f93\u5165\u72b6\u6001\u3001\u4ee3\u7406\u7b56\u7565\u548c\u5fae\u8c03\u6548\u679c\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "Orak\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u591a\u6837\u5316\u7684\u6e38\u620f\u73af\u5883\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u7684\u6e38\u620f\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.03206", "pdf": "https://arxiv.org/pdf/2506.03206", "abs": "https://arxiv.org/abs/2506.03206", "authors": ["Nadav Timor", "Jonathan Mamou", "Oren Pereg", "Hongyang Zhang", "David Harel"], "title": "Out-of-Vocabulary Sampling Boosts Speculative Decoding", "categories": ["cs.LG"], "comment": null, "summary": "Speculative decoding relies on fast and accurate drafters. Recent\nstate-of-the-art language models employ larger and larger vocabularies, which\nsignificantly slows down drafters. One promising approach to boost the\nefficiency of speculative decoding is to use drafters with smaller\nvocabularies. However, existing sampling methods cannot draw out-of-vocabulary\ntokens, creating a tradeoff between drafters' vocabulary size and acceptance\nrates. This paper introduces Redistributing Drafter Kernels (RDK), the first\nout-of-vocabulary sampler that effectively recovers acceptance rates by\nvirtually restoring pruned target tokens. RDK leverages token-affinity priors\nto reallocate drafter mass towards high-overlap regions. We prove\nmathematically that RDK can achieve higher acceptance rates than vanilla and\nstate-of-the-art samplers. We provide an efficient first-order approximation of\nRDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,\nenabling lightweight implementations for large vocabularies. Our experiments\ndemonstrate that this linear-time RDK significantly boosts acceptance rates\neven after extreme pruning (removing more than 75% of the drafter's\nvocabulary), where existing samplers fail. RDK opens the door to extremely\npruned drafters, which were previously impractical.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u65b9\u6cd5Redistributing Drafter Kernels (RDK)\uff0c\u5b83\u89e3\u51b3\u4e86\u4f7f\u7528\u8f83\u5c0f\u8bcd\u6c47\u8868\u7684drafters\u65f6\u51fa\u73b0\u7684\u63a5\u53d7\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u6807\u8bb0\u4eb2\u548c\u5148\u9a8c\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u63a5\u53d7\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e00\u9636\u8fd1\u4f3c\u5b9e\u73b0\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8bcd\u6c47\u8868\u4e4b\u5916\u7684\u6807\u8bb0\uff0c\u5bfc\u81f4drafters\u7684\u6548\u7387\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRedistributing Drafter Kernels (RDK)\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u9636\u8fd1\u4f3c\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7ebf\u6027\u65f6\u95f4RDK\u5728\u6781\u7aef\u526a\u679d\u540e\u663e\u8457\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\u3002", "conclusion": "RDK\u4f7f\u5f97\u6781\u5ea6\u7cbe\u7b80\u7684drafters\u53d8\u5f97\u5b9e\u7528\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u6548\u7387\u3002"}}
{"id": "2506.03522", "pdf": "https://arxiv.org/pdf/2506.03522", "abs": "https://arxiv.org/abs/2506.03522", "authors": ["Daniel Campa", "Mehdi Saeedi", "Ian Colbert", "Srinjoy Das"], "title": "Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach", "categories": ["cs.LG", "stat.ML"], "comment": "8 pages, 9 figures, Accepted at the IEEE Conference on Games 2025\n  (IEEE CoG)", "summary": "Navigation path traces play a crucial role in video game design, serving as a\nvital resource for both enhancing player engagement and fine-tuning\nnon-playable character behavior. Generating such paths with human-like realism\ncan enrich the overall gaming experience, and evaluating path traces can\nprovide game designers insights into player interactions. Despite the\nimpressive recent advancements in deep learning-based generative modeling, the\nvideo game industry hesitates to adopt such models for path generation, often\nciting their complex training requirements and interpretability challenges. To\naddress these problems, we propose a novel path generation and evaluation\napproach that is grounded in principled nonparametric statistics and provides\nprecise control while offering interpretable insights. Our path generation\nmethod fuses two statistical techniques: (1) nonparametric model-free\ntransformations that capture statistical characteristics of path traces through\ntime; and (2) copula models that capture statistical dependencies in space. For\npath evaluation, we adapt a nonparametric three-sample hypothesis test designed\nto determine if the generated paths are overfit (mimicking the original data\ntoo closely) or underfit (diverging too far from it). We demonstrate the\nprecision and reliability of our proposed methods with empirical analysis on\ntwo existing gaming benchmarks to showcase controlled generation of diverse\nnavigation paths. Notably, our novel path generator can be fine-tuned with user\ncontrollable parameters to create navigation paths that exhibit varying levels\nof human-likeness in contrast to those produced by neural network-based agents.\nThe code is available at https://github.com/daniel-campa/mf-copula.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bfc\u822a\u8def\u5f84\u751f\u6210\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u975e\u53c2\u6570\u7edf\u8ba1\u6280\u672f\u548ccopula\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u590d\u6742\u8bad\u7ec3\u9700\u6c42\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u63a7\u5236\u548c\u591a\u6837\u5316\u8def\u5f84\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u89c6\u9891\u6e38\u620f\u884c\u4e1a\u5f80\u5f80\u56e0\u4e3a\u5176\u590d\u6742\u7684\u8bad\u7ec3\u9700\u6c42\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\u800c\u4e0d\u613f\u610f\u91c7\u7528\u6b64\u7c7b\u6a21\u578b\u8fdb\u884c\u8def\u5f84\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fd9\u4e00\u65b0\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u7684\u8def\u5f84\u751f\u6210\u65b9\u6cd5\u878d\u5408\u4e86\u4e24\u79cd\u7edf\u8ba1\u6280\u672f\uff1a(1) \u975e\u53c2\u6570\u65e0\u6a21\u578b\u53d8\u6362\uff0c\u901a\u8fc7\u65f6\u95f4\u6355\u6349\u8def\u5f84\u8f68\u8ff9\u7684\u7edf\u8ba1\u7279\u5f81\uff1b(2) \u6355\u6349\u7a7a\u95f4\u7edf\u8ba1\u4f9d\u8d56\u6027\u7684copula\u6a21\u578b\u3002\u5bf9\u4e8e\u8def\u5f84\u8bc4\u4f30\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u4e09\u6837\u672c\u5047\u8bbe\u68c0\u9a8c\uff0c\u7528\u4e8e\u5224\u65ad\u751f\u6210\u7684\u8def\u5f84\u662f\u5426\u8fc7\u62df\u5408\uff08\u8fc7\u4e8e\u6a21\u4eff\u539f\u59cb\u6570\u636e\uff09\u6216\u6b20\u62df\u5408\uff08\u504f\u79bb\u539f\u59cb\u6570\u636e\u592a\u8fdc\uff09\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6837\u5316\u5bfc\u822a\u8def\u5f84\u7684\u53ef\u63a7\u751f\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b0\u9896\u8def\u5f84\u751f\u6210\u5668\u53ef\u4ee5\u5fae\u8c03\u4ee5\u521b\u5efa\u4e0d\u540c\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u6c34\u5e73\u7684\u8def\u5f84\uff0c\u76f8\u8f83\u4e8e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4ee3\u7406\u751f\u6210\u7684\u8def\u5f84\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u53c2\u6570\u7edf\u8ba1\u539f\u7406\u7684\u5bfc\u822a\u8def\u5f84\u751f\u6210\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u7cbe\u786e\u63a7\u5236\u548c\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u73b0\u6709\u6e38\u620f\u57fa\u51c6\u4e0a\u5c55\u793a\u4e86\u591a\u6837\u5316\u5bfc\u822a\u8def\u5f84\u7684\u53ef\u63a7\u751f\u6210\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u65b0\u9896\u8def\u5f84\u751f\u6210\u5668\u53ef\u4ee5\u901a\u8fc7\u7528\u6237\u53ef\u63a7\u53c2\u6570\u5fae\u8c03\uff0c\u4ee5\u521b\u5efa\u5177\u6709\u4e0d\u540c\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u6c34\u5e73\u7684\u5bfc\u822a\u8def\u5f84\u3002"}}
{"id": "2506.03613", "pdf": "https://arxiv.org/pdf/2506.03613", "abs": "https://arxiv.org/abs/2506.03613", "authors": ["Shaoshan Liu", "Fan Wang", "Hongjun Zhou", "Yuanfeng Wang"], "title": "Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations", "categories": ["cs.AI", "cs.CC"], "comment": null, "summary": "While theory and practice are often seen as separate domains, this article\nshows that theoretical insight is essential for overcoming real-world\nengineering barriers. We begin with a practical challenge: training a\ncross-morphology embodied AI policy that generalizes across diverse robot\nmorphologies. We formalize this as the Heterogeneous Embodied Agent Training\n(HEAT) problem and prove it reduces to a structured Partially Observable Markov\nDecision Process (POMDP) that is PSPACE-complete. This result explains why\ncurrent reinforcement learning pipelines break down under morphological\ndiversity, due to sequential training constraints, memory-policy coupling, and\ndata incompatibility. We further explore Collective Adaptation, a distributed\nlearning alternative inspired by biological systems. Though NEXP-complete in\ntheory, it offers meaningful scalability and deployment benefits in practice.\nThis work illustrates how computational theory can illuminate system design\ntrade-offs and guide the development of more robust, scalable embodied AI. For\npractitioners and researchers to explore this problem, the implementation code\nof this work has been made publicly available at\nhttps://github.com/airs-admin/HEAT", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.03207", "pdf": "https://arxiv.org/pdf/2506.03207", "abs": "https://arxiv.org/abs/2506.03207", "authors": ["Md Nahid Hasan Shuvo", "Moinul Hossain"], "title": "Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "7 pages, 4 Figures, Accepted to publish in Proceedings of the 2025\n  ACM Workshop on Wireless Security and Machine Learning (WiseML 2025), July 3,\n  2025, Arlington, VA, USA", "summary": "Federated Learning (FL) is increasingly adopted as a decentralized machine\nlearning paradigm due to its capability to preserve data privacy by training\nmodels without centralizing user data. However, FL is susceptible to indirect\nprivacy breaches via network traffic analysis-an area not explored in existing\nresearch. The primary objective of this research is to study the feasibility of\nfingerprinting deep learning models deployed within FL environments by\nanalyzing their network-layer traffic information. In this paper, we conduct an\nexperimental evaluation using various deep learning architectures (i.e., CNN,\nRNN) within a federated learning testbed. We utilize machine learning\nalgorithms, including Support Vector Machines (SVM), Random Forest, and\nGradient-Boosting, to fingerprint unique patterns within the traffic data. Our\nexperiments show high fingerprinting accuracy, achieving 100% accuracy using\nRandom Forest and around 95.7% accuracy using SVM and Gradient Boosting\nclassifiers. This analysis suggests that we can identify specific architectures\nrunning within the subsection of the network traffic. Hence, if an adversary\nknows about the underlying DL architecture, they can exploit that information\nand conduct targeted attacks. These findings suggest a notable security\nvulnerability in FL systems and the necessity of strengthening it at the\nnetwork level.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u88ab\u7cbe\u786e\u5730\u6307\u7eb9\u8bc6\u522b\uff0c\u4ece\u800c\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u5efa\u8bae\u5728\u7f51\u7edc\u5c42\u9762\u52a0\u5f3a\u5b89\u5168\u9632\u62a4\u3002", "motivation": "\u7531\u4e8e\u8054\u90a6\u5b66\u4e60\u901a\u8fc7\u4e0d\u96c6\u4e2d\u7528\u6237\u6570\u636e\u6765\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u800c\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u5bb9\u6613\u53d7\u5230\u901a\u8fc7\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u7684\u95f4\u63a5\u9690\u79c1\u6cc4\u9732\uff0c\u8fd9\u4e00\u9886\u57df\u5c1a\u672a\u5f97\u5230\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u5c42\u6d41\u91cf\u4fe1\u606f\u6765\u6307\u7eb9\u8bc6\u522b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982CNN\u3001RNN\uff09\u5728\u8054\u90a6\u5b66\u4e60\u6d4b\u8bd5\u5e73\u53f0\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5e76\u91c7\u7528\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u3001\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\u7b49\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u6d41\u91cf\u6570\u636e\u4e2d\u7684\u72ec\u7279\u6a21\u5f0f\u8fdb\u884c\u6307\u7eb9\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u9ad8\u6307\u7eb9\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u8fbe\u5230\u4e86100%\u7684\u51c6\u786e\u7387\uff0c\u800cSVM\u548c\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\u7ea6\u4e3a95.7%\u3002\u8fd9\u8868\u660e\u53ef\u4ee5\u5728\u7f51\u7edc\u6d41\u91cf\u7684\u5b50\u90e8\u5206\u4e2d\u8bc6\u522b\u7279\u5b9a\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u5728\u7f51\u7edc\u5c42\u9762\u4e0a\u52a0\u5f3a\u9632\u62a4\u4ee5\u9632\u6b62\u6a21\u578b\u6307\u7eb9\u8bc6\u522b\u548c\u9488\u5bf9\u6027\u653b\u51fb\u3002"}}
{"id": "2506.03595", "pdf": "https://arxiv.org/pdf/2506.03595", "abs": "https://arxiv.org/abs/2506.03595", "authors": ["Runa Eschenhagen", "Aaron Defazio", "Tsung-Hsien Lee", "Richard E. Turner", "Hao-Jun Michael Shi"], "title": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The recent success of Shampoo in the AlgoPerf contest has sparked renewed\ninterest in Kronecker-factorization-based optimization algorithms for training\nneural networks. Despite its success, Shampoo relies heavily on several\nheuristics such as learning rate grafting and stale preconditioning to achieve\nperformance at-scale. These heuristics increase algorithmic complexity,\nnecessitate further hyperparameter tuning, and lack theoretical justification.\nThis paper investigates these heuristics from the angle of Frobenius norm\napproximation to full-matrix Adam and decouples the preconditioner's\neigenvalues and eigenbasis updates. We show that grafting from Adam mitigates\nthe staleness and mis-scaling of the preconditioner's eigenvalues and how\ncorrecting the eigenvalues directly can eliminate the need for learning rate\ngrafting. To manage the error induced by infrequent eigenbasis computations, we\npropose an adaptive criterion for determining the eigenbasis computation\nfrequency motivated by terminating a warm-started QR algorithm. This criterion\ndecouples the update frequency of different preconditioner matrices and enables\nus to investigate the impact of approximation error on convergence. These\npractical techniques offer a principled angle towards removing Shampoo's\nheuristics and developing improved Kronecker-factorization-based training\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eKronecker\u5206\u89e3\u7684\u4f18\u5316\u7b97\u6cd5Shampoo\u4e2d\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "Shampoo\u867d\u7136\u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f9d\u8d56\u4e8e\u4e00\u4e9b\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u589e\u52a0\u4e86\u7b97\u6cd5\u590d\u6742\u6027\u548c\u8c03\u53c2\u96be\u5ea6\u3002", "method": "\u4eceFrobenius\u8303\u6570\u8fd1\u4f3c\u548c\u77e9\u9635Adam\u7684\u89d2\u5ea6\u5206\u6790Shampoo\u4e2d\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u89e3\u8026\u9884\u6761\u4ef6\u77e9\u9635\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u57fa\u66f4\u65b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8c03\u6574\u7279\u5f81\u57fa\u8ba1\u7b97\u9891\u7387\u7684\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86Adam\u7684\u5ac1\u63a5\u80fd\u591f\u7f13\u89e3\u9884\u6761\u4ef6\u77e9\u9635\u7279\u5f81\u503c\u7684\u9648\u65e7\u6027\u548c\u9519\u8bef\u7f29\u653e\u95ee\u9898\uff1b\u901a\u8fc7\u76f4\u63a5\u4fee\u6b63\u7279\u5f81\u503c\u53ef\u4ee5\u6d88\u9664\u5bf9\u5b66\u4e60\u7387\u5ac1\u63a5\u7684\u9700\u6c42\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6807\u51c6\u6709\u52a9\u4e8e\u51cf\u5c11\u56e0\u7a00\u758f\u7279\u5f81\u57fa\u8ba1\u7b97\u5e26\u6765\u7684\u8bef\u5dee\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u6280\u672f\u4e3a\u53bb\u9664Shampoo\u4e2d\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u4f18\u7684Kronecker\u5206\u89e3\u8bad\u7ec3\u7b97\u6cd5\u3002"}}
{"id": "2506.03673", "pdf": "https://arxiv.org/pdf/2506.03673", "abs": "https://arxiv.org/abs/2506.03673", "authors": ["Yinlong Xu", "Yanzhao Zheng", "Shuoshuo Sun", "Shuaihan Huang", "Baohua Dong", "Hangcheng Zhu", "Ruohui Huang", "Gang Yu", "Hongxia Xu", "Jian Wu"], "title": "Reason from Future: Reverse Thought Chain Enhances LLM Reasoning", "categories": ["cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "It has been demonstrated that carefully designed reasoning paradigms, like\nChain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning\ncapabilities of small language models by detailed thinking and extensive\nthought searching, unbounded branching factors in the searching space create\nprohibitive reasoning consumption. However these methods fall into the trap of\nlocal optimum reasoning, which means the model lacks a global perspective while\nsolving problems. We propose a novel reasoning paradigm called Reason from\nFuture (RFF), which generates reasoning paths by bidirectional reasoning that\ncombines top-down planning with bottom-up reasoning accumulation. The essence\nof RFF lies in its reverse reasoning mechanism, which prioritizes core logical\nrelationships and imposes goal-oriented constraints on intermediate steps,\nthereby reducing the searching space and mitigating error accumulation inherent\nin sequential forward reasoning. Empirical evaluations across diverse\nexperiments demonstrate that RFF outperforms conventional paradigms with higher\naccuracy and less searching space to solve complex tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u8303\u5f0fRFF\uff0c\u901a\u8fc7\u53cc\u5411\u63a8\u7406\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u8303\u5f0f\u5982Chain-of-Thought (CoT)\u548cTree-of-Thought (ToT)\u867d\u7136\u80fd\u589e\u5f3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u65e0\u754c\u5206\u652f\u56e0\u5b50\u5bfc\u81f4\u63a8\u7406\u6d88\u8017\u5927\uff0c\u4e14\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReason from Future (RFF)\u7684\u65b0\u578b\u63a8\u7406\u8303\u5f0f\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "RFF\u901a\u8fc7\u4f18\u5148\u8003\u8651\u6838\u5fc3\u903b\u8f91\u5173\u7cfb\u5e76\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u65bd\u52a0\u76ee\u6807\u5bfc\u5411\u7ea6\u675f\uff0c\u6709\u6548\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u5e76\u51cf\u8f7b\u987a\u5e8f\u524d\u5411\u63a8\u7406\u56fa\u6709\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "conclusion": "RFF\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u53cc\u5411\u63a8\u7406\u7ed3\u5408\u81ea\u9876\u5411\u4e0b\u89c4\u5212\u548c\u81ea\u5e95\u5411\u4e0a\u63a8\u7406\u79ef\u7d2f\uff0c\u63d0\u9ad8\u4e86\u4f20\u7edf\u8303\u5f0f\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u641c\u7d22\u7a7a\u95f4\u3002"}}
{"id": "2506.03210", "pdf": "https://arxiv.org/pdf/2506.03210", "abs": "https://arxiv.org/abs/2506.03210", "authors": ["Qiusheng Huang", "Yuan Niu", "Xiaohui Zhong", "Anboyu Guo", "Lei Chen", "Dianjun Zhang", "Xuefeng Zhang", "Hao Li"], "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Accurate, high-resolution ocean forecasting is crucial for maritime\noperations and environmental monitoring. While traditional numerical models are\ncapable of producing sub-daily, eddy-resolving forecasts, they are\ncomputationally intensive and face challenges in maintaining accuracy at fine\nspatial and temporal scales. In contrast, recent data-driven approaches offer\nimproved computational efficiency and emerging potential, yet typically operate\nat daily resolution and struggle with sub-daily predictions due to error\naccumulation over time. We introduce FuXi-Ocean, the first data-driven global\nocean forecasting model achieving six-hourly predictions at eddy-resolving\n1/12{\\deg} spatial resolution, reaching depths of up to 1500 meters. The model\narchitecture integrates a context-aware feature extraction module with a\npredictive network employing stacked attention blocks. The core innovation is\nthe Mixture-of-Time (MoT) module, which adaptively integrates predictions from\nmultiple temporal contexts by learning variable-specific reliability ,\nmitigating cumulative errors in sequential forecasting. Through comprehensive\nexperimental evaluation, FuXi-Ocean demonstrates superior skill in predicting\nkey variables, including temperature, salinity, and currents, across multiple\ndepths.", "AI": {"tldr": "FuXi-Ocean\u662f\u4e00\u79cd\u521b\u65b0\u6027\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6b21\u65e5\u6d77\u6d0b\u9884\u6d4b\u5e76\u89e3\u51b3\u4e86\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5728\u7cbe\u7ec6\u65f6\u7a7a\u5c3a\u5ea6\u4e0a\u4fdd\u6301\u51c6\u786e\u6027\uff1b\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u63d0\u4f9b\u65e5\u5206\u8fa8\u7387\u9884\u6d4b\u5e76\u5728\u6b21\u65e5\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u4f7f\u7528\u5806\u53e0\u6ce8\u610f\u529b\u5757\u7684\u9884\u6d4b\u7f51\u7edc\uff0c\u5e76\u91c7\u7528Mixture-of-Time (MoT) \u6a21\u5757\u8fdb\u884c\u591a\u65f6\u95f4\u4e0a\u4e0b\u6587\u9884\u6d4b\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFuXi-Ocean\u5728\u591a\u4e2a\u6df1\u5ea6\u7684\u5173\u952e\u53d8\u91cf\uff08\u5982\u6e29\u5ea6\u3001\u76d0\u5ea6\u548c\u6d0b\u6d41\uff09\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "FuXi-Ocean\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6570\u636e\u9a71\u52a8\u5168\u7403\u6d77\u6d0b\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u6bcf\u516d\u5c0f\u65f6\u4e00\u6b21\u7684\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\uff0c\u5176\u5f15\u5165\u7684MoT\u6a21\u5757\u80fd\u6709\u6548\u51cf\u5c11\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u3002"}}
{"id": "2506.03719", "pdf": "https://arxiv.org/pdf/2506.03719", "abs": "https://arxiv.org/abs/2506.03719", "authors": ["Quentin Bertrand", "Anne Gagneux", "Mathurin Massias", "R\u00e9mi Emonet"], "title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Modern deep generative models can now produce high-quality synthetic samples\nthat are often indistinguishable from real training data. A growing body of\nresearch aims to understand why recent methods -- such as diffusion and flow\nmatching techniques -- generalize so effectively. Among the proposed\nexplanations are the inductive biases of deep learning architectures and the\nstochastic nature of the conditional flow matching loss. In this work, we rule\nout the latter -- the noisy nature of the loss -- as a primary contributor to\ngeneralization in flow matching. First, we empirically show that in\nhigh-dimensional settings, the stochastic and closed-form versions of the flow\nmatching loss yield nearly equivalent losses. Then, using state-of-the-art flow\nmatching models on standard image datasets, we demonstrate that both variants\nachieve comparable statistical performance, with the surprising observation\nthat using the closed-form can even improve performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u6d41\u5339\u914d\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u635f\u5931\u51fd\u6570\u7684\u968f\u673a\u6027\u4e0d\u662f\u4e3b\u8981\u8d21\u732e\u56e0\u7d20\uff0c\u800c\u95ed\u5408\u5f62\u5f0f\u65b9\u6cd5\u6709\u65f6\u80fd\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u73b0\u4ee3\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e3a\u4f55\u80fd\u591f\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6837\u672c\u4ee5\u53ca\u4e3a\u4f55\u6d41\u5339\u914d\u6280\u672f\u80fd\u591f\u6709\u6548\u6cdb\u5316\uff0c\u9700\u8981\u6392\u9664\u635f\u5931\u51fd\u6570\u968f\u673a\u6027\u5bf9\u5176\u6cdb\u5316\u80fd\u529b\u5f71\u54cd\u7684\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u5bf9\u6d41\u5339\u914d\u635f\u5931\u7684\u968f\u673a\u7248\u672c\u548c\u95ed\u5408\u5f62\u5f0f\u7248\u672c\u8fdb\u884c\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u6d41\u5339\u914d\u6a21\u578b\u5728\u6807\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e24\u8005\u7684\u7edf\u8ba1\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728\u635f\u5931\u503c\u8fd8\u662f\u7edf\u8ba1\u6027\u80fd\u65b9\u9762\uff0c\u6d41\u5339\u914d\u635f\u5931\u7684\u968f\u673a\u7248\u672c\u548c\u95ed\u5408\u5f62\u5f0f\u7248\u672c\u8868\u73b0\u51e0\u4e4e\u76f8\u540c\uff0c\u751a\u81f3\u95ed\u5408\u5f62\u5f0f\u6709\u65f6\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u6d41\u5339\u914d\u4e2d\u635f\u5931\u51fd\u6570\u7684\u968f\u673a\u6027\u5e76\u4e0d\u662f\u5176\u6cdb\u5316\u80fd\u529b\u7684\u4e3b\u8981\u8d21\u732e\u56e0\u7d20\u3002"}}
{"id": "2506.03828", "pdf": "https://arxiv.org/pdf/2506.03828", "abs": "https://arxiv.org/abs/2506.03828", "authors": ["Dhaval Patel", "Shuxin Lin", "James Rayfield", "Nianjun Zhou", "Roman Vaculin", "Natalia Martinez", "Fearghal O'donncha", "Jayant Kalagnanam"], "title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance", "categories": ["cs.AI", "cs.MA"], "comment": "39 pages, 18 figures", "summary": "AI for Industrial Asset Lifecycle Management aims to automate complex\noperational workflows -- such as condition monitoring, maintenance planning,\nand intervention scheduling -- to reduce human workload and minimize system\ndowntime. Traditional AI/ML approaches have primarily tackled these problems in\nisolation, solving narrow tasks within the broader operational pipeline. In\ncontrast, the emergence of AI agents and large language models (LLMs)\nintroduces a next-generation opportunity: enabling end-to-end automation across\nthe entire asset lifecycle. This paper envisions a future where AI agents\nautonomously manage tasks that previously required distinct expertise and\nmanual coordination. To this end, we introduce AssetOpsBench -- a unified\nframework and environment designed to guide the development, orchestration, and\nevaluation of domain-specific agents tailored for Industry 4.0 applications. We\noutline the key requirements for such holistic systems and provide actionable\ninsights into building agents that integrate perception, reasoning, and control\nfor real-world industrial operations. The software is available at\nhttps://github.com/IBM/AssetOpsBench.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528AI\u4ee3\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5de5\u4e1a\u8d44\u4ea7\u751f\u547d\u5468\u671f\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u7684\u6846\u67b6AssetOpsBench\uff0c\u65e8\u5728\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u5e76\u4f18\u5316\u7cfb\u7edf\u8fd0\u884c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfAI/ML\u65b9\u6cd5\u4e3b\u8981\u5b64\u7acb\u5730\u89e3\u51b3\u5de5\u4e1a\u64cd\u4f5c\u6d41\u7a0b\u4e2d\u7684\u7279\u5b9a\u95ee\u9898\uff0c\u800cAI\u4ee3\u7406\u548cLLM\u7684\u51fa\u73b0\u4e3a\u6574\u4e2a\u8d44\u4ea7\u751f\u547d\u5468\u671f\u63d0\u4f9b\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165AI\u4ee3\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAssetOpsBench\u7684\u7edf\u4e00\u6846\u67b6\u548c\u73af\u5883\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u9002\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\u7684\u667a\u80fd\u4ee3\u7406\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86AssetOpsBench\u6846\u67b6\uff0c\u5e76\u6982\u8ff0\u4e86\u6784\u5efa\u80fd\u591f\u96c6\u6210\u611f\u77e5\u3001\u63a8\u7406\u548c\u63a7\u5236\u529f\u80fd\u7684\u667a\u80fd\u4ee3\u7406\u6240\u9700\u7684\u5173\u952e\u8981\u6c42\u548c\u53ef\u884c\u89c1\u89e3\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u4ee3\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u672a\u6765\u613f\u666f\uff0c\u4ee5\u5b9e\u73b0\u5de5\u4e1a\u8d44\u4ea7\u751f\u547d\u5468\u671f\u7ba1\u7406\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\uff0c\u5e76\u4ecb\u7ecd\u4e86AssetOpsBench\u8fd9\u4e00\u7edf\u4e00\u6846\u67b6\u6765\u6307\u5bfc\u884c\u4e1a4.0\u5e94\u7528\u4e2d\u7279\u5b9a\u9886\u57df\u4ee3\u7406\u7684\u5f00\u53d1\u3001\u534f\u8c03\u548c\u8bc4\u4f30\u3002"}}
{"id": "2506.03225", "pdf": "https://arxiv.org/pdf/2506.03225", "abs": "https://arxiv.org/abs/2506.03225", "authors": ["Wa\u00ebl Doulazmi", "Auguste Lehuger", "Marin Toromanoff", "Valentin Charraut", "Thibault Buhet", "Fabien Moutarde"], "title": "Multiple-Frequencies Population-Based Training", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "Accepted at RLC25", "summary": "Reinforcement Learning's high sensitivity to hyperparameters is a source of\ninstability and inefficiency, creating significant challenges for\npractitioners. Hyperparameter Optimization (HPO) algorithms have been developed\nto address this issue, among them Population-Based Training (PBT) stands out\nfor its ability to generate hyperparameters schedules instead of fixed\nconfigurations. PBT trains a population of agents, each with its own\nhyperparameters, frequently ranking them and replacing the worst performers\nwith mutations of the best agents. These intermediate selection steps can cause\nPBT to focus on short-term improvements, leading it to get stuck in local\noptima and eventually fall behind vanilla Random Search over longer timescales.\nThis paper studies how this greediness issue is connected to the choice of\nevolution frequency, the rate at which the selection is done. We propose\nMultiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm\nthat addresses greediness by employing sub-populations, each evolving at\ndistinct frequencies. MF-PBT introduces a migration process to transfer\ninformation between sub-populations, with an asymmetric design to balance short\nand long-term optimization. Extensive experiments on the Brax suite demonstrate\nthat MF-PBT improves sample efficiency and long-term performance, even without\nactually tuning hyperparameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MF-PBT\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u8d85\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u8fdb\u5316\u9891\u7387\u7684\u5b50\u79cd\u7fa4\u548c\u8fc1\u79fb\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86PBT\u7b97\u6cd5\u4e2d\u7684\u8d2a\u5a6a\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u957f\u671f\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u8d85\u53c2\u6570\u7684\u9ad8\u5ea6\u654f\u611f\u6027\u5bfc\u81f4\u4e86\u4e0d\u7a33\u5b9a\u6027\u548c\u4f4e\u6548\u6027\uff0c\u800c\u73b0\u6709\u7684\u8d85\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\u5982PBT\u5728\u957f\u671f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMF-PBT\u7684\u65b0\u9896HPO\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u91c7\u7528\u5177\u6709\u4e0d\u540c\u8fdb\u5316\u9891\u7387\u7684\u5b50\u79cd\u7fa4\u4ee5\u53ca\u4e0d\u5bf9\u79f0\u8bbe\u8ba1\u7684\u8fc1\u79fb\u8fc7\u7a0b\u6765\u89e3\u51b3\u8d2a\u5a6a\u95ee\u9898\u3002", "result": "\u5728Brax\u5957\u4ef6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u4e0d\u5b9e\u9645\u8c03\u6574\u8d85\u53c2\u6570\uff0cMF-PBT\u4e5f\u80fd\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u957f\u671f\u6027\u80fd\u3002", "conclusion": "MF-PBT\u7b97\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86PBT\u7b97\u6cd5\u4e2d\u7531\u4e8e\u4e2d\u95f4\u9009\u62e9\u6b65\u9aa4\u5bfc\u81f4\u7684\u8d2a\u5a6a\u95ee\u9898\uff0c\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2506.03784", "pdf": "https://arxiv.org/pdf/2506.03784", "abs": "https://arxiv.org/abs/2506.03784", "authors": ["Beatrix M. G. Nielsen", "Emanuele Marconato", "Andrea Dittadi", "Luigi Gresele"], "title": "When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When and why representations learned by different deep neural networks are\nsimilar is an active research topic. We choose to address these questions from\nthe perspective of identifiability theory, which suggests that a measure of\nrepresentational similarity should be invariant to transformations that leave\nthe model distribution unchanged. Focusing on a model family which includes\nseveral popular pre-training approaches, e.g., autoregressive language models,\nwe explore when models which generate distributions that are close have similar\nrepresentations. We prove that a small Kullback-Leibler divergence between the\nmodel distributions does not guarantee that the corresponding representations\nare similar. This has the important corollary that models arbitrarily close to\nmaximizing the likelihood can still learn dissimilar representations, a\nphenomenon mirrored in our empirical observations on models trained on\nCIFAR-10. We then define a distributional distance for which closeness implies\nrepresentational similarity, and in synthetic experiments, we find that wider\nnetworks learn distributions which are closer with respect to our distance and\nhave more similar representations. Our results establish a link between\ncloseness in distribution and representational similarity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5230\u7684\u8868\u793a\u7684\u76f8\u4f3c\u6027\u95ee\u9898\uff0c\u6307\u51fa\u5206\u5e03\u63a5\u8fd1\u7684\u5c0fKL\u6563\u5ea6\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u8868\u793a\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5e03\u8ddd\u79bb\u5b9a\u4e49\uff0c\u4f7f\u5f97\u5206\u5e03\u63a5\u8fd1\u610f\u5473\u7740\u8868\u793a\u76f8\u4f3c\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u4e0d\u540c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5230\u7684\u8868\u793a\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u4f1a\u76f8\u4f3c\uff0c\u8fd9\u662f\u5f53\u524d\u7814\u7a76\u7684\u4e00\u4e2a\u70ed\u70b9\u95ee\u9898\u3002", "method": "\u8bba\u6587\u4ece\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u7814\u7a76\u4e86\u751f\u6210\u5206\u5e03\u63a5\u8fd1\u7684\u6a21\u578b\u662f\u5426\u5177\u6709\u76f8\u4f3c\u7684\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8fd9\u4e00\u89c2\u70b9\u3002", "result": "\u8bba\u6587\u8bc1\u660e\u4e86\u5c0fKL\u6563\u5ea6\u4e0d\u80fd\u4fdd\u8bc1\u8868\u793a\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u5728\u5408\u6210\u5b9e\u9a8c\u4e2d\u53d1\u73b0\u66f4\u5bbd\u7684\u7f51\u7edc\u5728\u5206\u5e03\u4e0a\u66f4\u63a5\u8fd1\u4e14\u5177\u6709\u66f4\u76f8\u4f3c\u7684\u8868\u793a\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5206\u5e03\u63a5\u8fd1\u7684\u5c0fKL\u6563\u5ea6\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u76f8\u5e94\u8868\u793a\u7684\u76f8\u4f3c\u6027\u3002\u8fd9\u8868\u660e\u5373\u4f7f\u6a21\u578b\u63a5\u8fd1\u6700\u5927\u5316\u4f3c\u7136\uff0c\u5b83\u4eec\u4ecd\u53ef\u80fd\u5b66\u4e60\u5230\u4e0d\u76f8\u4f3c\u7684\u8868\u793a\u3002"}}
{"id": "2506.03915", "pdf": "https://arxiv.org/pdf/2506.03915", "abs": "https://arxiv.org/abs/2506.03915", "authors": ["Sebastian R\u00f6dling", "Matej Ze\u010devi\u0107", "Devendra Singh Dhami", "Kristian Kersting"], "title": "Causal Explanations Over Time: Articulated Reasoning for Interactive Environments", "categories": ["cs.AI"], "comment": "Main paper: 9 pages, References: 2 pages, Supplementary: 9 pages.\n  Number of figures: 10, number of tables: 3", "summary": "Structural Causal Explanations (SCEs) can be used to automatically generate\nexplanations in natural language to questions about given data that are\ngrounded in a (possibly learned) causal model. Unfortunately they work for\nsmall data only. In turn they are not attractive to offer reasons for events,\ne.g., tracking causal changes over multiple time steps, or a behavioral\ncomponent that involves feedback loops through actions of an agent. To this\nend, we generalize SCEs to a (recursive) formulation of explanation trees to\ncapture the temporal interactions between reasons. We show the benefits of this\nmore general SCE algorithm on synthetic time-series data and a 2D grid game,\nand further compare it to the base SCE and other existing methods for causal\nexplanations.", "AI": {"tldr": "This paper generalizes Structural Causal Explanations to handle temporal interactions and feedback loops, showing improved performance in causal reasoning over time.", "motivation": "The motivation is that current SCE methods are limited to small data and cannot effectively handle temporal interactions or feedback loops, making them unsuitable for complex causal reasoning.", "method": "The authors generalize Structural Causal Explanations (SCEs) into a recursive formulation of explanation trees and test its effectiveness on synthetic time-series data and a 2D grid game.", "result": "The generalized SCE algorithm demonstrates improved performance in capturing temporal interactions and causal reasoning over multiple time steps.", "conclusion": "The paper concludes that the generalized SCE algorithm provides better causal explanations for temporal interactions compared to existing methods."}}
{"id": "2506.03227", "pdf": "https://arxiv.org/pdf/2506.03227", "abs": "https://arxiv.org/abs/2506.03227", "authors": ["Abdelrahman Sayed Sayed", "Pierre-Jean Meyer", "Mohamed Ghazel"], "title": "Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 5 figures, Accepted for publication in the proceedings of\n  the 8th International Symposium on AI Verification SAIV 2025", "summary": "A neural ordinary differential equation (neural ODE) is a machine learning\nmodel that is commonly described as a continuous depth generalization of a\nresidual network (ResNet) with a single residual block, or conversely, the\nResNet can be seen as the Euler discretization of the neural ODE. These two\nmodels are therefore strongly related in a way that the behaviors of either\nmodel are considered to be an approximation of the behaviors of the other. In\nthis work, we establish a more formal relationship between these two models by\nbounding the approximation error between two such related models. The obtained\nerror bound then allows us to use one of the models as a verification proxy for\nthe other, without running the verification tools twice: if the reachable\noutput set expanded by the error bound satisfies a safety property on one of\nthe models, this safety property is then guaranteed to be also satisfied on the\nother model. This feature is fully reversible, and the initial safety\nverification can be run indifferently on either of the two models. This novel\napproach is illustrated on a numerical example of a fixed-point attractor\nsystem modeled as a neural ODE.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bef\u5dee\u754c\u9650\uff0c\u7528\u4e8e\u63cf\u8ff0\u795e\u7ecfODE\u548cResNet\u4e4b\u95f4\u7684\u8fd1\u4f3c\u7a0b\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u4e00\u4e2a\u6a21\u578b\u7528\u4f5c\u53e6\u4e00\u4e2a\u6a21\u578b\u7684\u5b89\u5168\u6027\u9a8c\u8bc1\u4ee3\u7406\u3002", "motivation": "\u795e\u7ecfODE\u548cResNet\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u4f46\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u9a8c\u8bc1\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5229\u7528\u8fd9\u79cd\u5173\u7cfb\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5efa\u7acb\u4e24\u4e2a\u76f8\u5173\u6a21\u578b\u4e4b\u95f4\u7684\u8fd1\u4f3c\u8bef\u5dee\u754c\u9650\uff0c\u6765\u5c55\u793a\u795e\u7ecfODE\u548cResNet\u4e4b\u95f4\u7684\u66f4\u6b63\u5f0f\u7684\u5173\u7cfb\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u7684\u8bef\u5dee\u754c\u9650\u65b9\u6cd5\u5141\u8bb8\u5728\u4e00\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u5b89\u5168\u6027\u9a8c\u8bc1\uff0c\u5e76\u4fdd\u8bc1\u5176\u5728\u53e6\u4e00\u4e2a\u6a21\u578b\u4e0a\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u901a\u8fc7\u5efa\u7acb\u8bef\u5dee\u754c\u9650\uff0c\u53ef\u4ee5\u5728\u4e0d\u8fd0\u884c\u9a8c\u8bc1\u5de5\u5177\u4e24\u6b21\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u4e00\u4e2a\u6a21\u578b\u7528\u4f5c\u53e6\u4e00\u4e2a\u6a21\u578b\u7684\u9a8c\u8bc1\u4ee3\u7406\u3002"}}
{"id": "2506.03839", "pdf": "https://arxiv.org/pdf/2506.03839", "abs": "https://arxiv.org/abs/2506.03839", "authors": ["Tobias Pielok", "Bernd Bischl", "David R\u00fcgamer"], "title": "Revisiting Unbiased Implicit Variational Inference", "categories": ["cs.LG", "stat.ML", "62F15, 68T07", "I.2.6; G.3"], "comment": "Accepted to ICML 2025", "summary": "Recent years have witnessed growing interest in semi-implicit variational\ninference (SIVI) methods due to their ability to rapidly generate samples from\ncomplex distributions. However, since the likelihood of these samples is\nnon-trivial to estimate in high dimensions, current research focuses on finding\neffective SIVI training routines. Although unbiased implicit variational\ninference (UIVI) has largely been dismissed as imprecise and computationally\nprohibitive because of its inner MCMC loop, we revisit this method and show\nthat UIVI's MCMC loop can be effectively replaced via importance sampling and\nthe optimal proposal distribution can be learned stably by minimizing an\nexpected forward Kullback-Leibler divergence without bias. Our refined approach\ndemonstrates superior performance or parity with state-of-the-art methods on\nestablished SIVI benchmarks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u7406\uff08SIVI\uff09\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u65e0\u504f\u9690\u5f0f\u53d8\u5206\u63a8\u7406\uff08UIVI\uff09\u65b9\u6cd5\uff0c\u4f7f\u5176\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684SIVI\u6280\u672f\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u5bfb\u627e\u6709\u6548\u7684SIVI\u8bad\u7ec3\u7a0b\u5e8f\uff0c\u800cUIVI\u56e0\u7cbe\u5ea6\u95ee\u9898\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u88ab\u5ffd\u89c6\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6UIVI\u5e76\u627e\u5230\u6539\u8fdb\u5176\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u66ff\u4ee3UIVI\u4e2d\u7684MCMC\u5faa\u73af\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u671f\u524d\u5411Kullback-Leibler\u6563\u5ea6\u6765\u7a33\u5b9a\u5b66\u4e60\u6700\u4f73\u63d0\u8bae\u5206\u5e03\u3002", "result": "\u8bba\u6587\u7684\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u6539\u8fdb\u7684UIVI\u65b9\u6cd5\u5728\u73b0\u6709SIVI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6216\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u6539\u8fdb\u540e\u7684UIVI\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u73b0\u6709\u7684SIVI\u6280\u672f\u3002"}}
{"id": "2506.03939", "pdf": "https://arxiv.org/pdf/2506.03939", "abs": "https://arxiv.org/abs/2506.03939", "authors": ["Junqi Gao", "Xiang Zou", "YIng Ai", "Dong Li", "Yichen Niu", "Biqing Qi", "Jianxing Liu"], "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.", "AI": {"tldr": "Graph Counselor\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u77e5\u8bc6\u6574\u5408\u80fd\u529b\u548c\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709GraphRAG\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u805a\u5408\u6548\u7387\u4f4e\u548c\u63a8\u7406\u673a\u5236\u50f5\u5316\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u7684\u56fe\u4fe1\u606f\u5904\u7406\u548c\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684GraphRAG\u65b9\u6cd5Graph Counselor\uff0c\u5176\u4e2d\u5305\u62ec\u81ea\u9002\u5e94\u56fe\u4fe1\u606f\u63d0\u53d6\u6a21\u5757\uff08AGIEM\uff09\u548c\u591a\u89c6\u89d2\u81ea\u6211\u53cd\u601d\uff08SR\uff09\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGraph Counselor\u5728\u591a\u4e2a\u56fe\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Graph Counselor\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.03230", "pdf": "https://arxiv.org/pdf/2506.03230", "abs": "https://arxiv.org/abs/2506.03230", "authors": ["Selcuk Gurses", "Aozhong Zhang", "Yanxia Deng", "Xun Dong", "Xin Li", "Naigang Wang", "Penghang Yin", "Zi Yang"], "title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "comment": null, "summary": "Finetuning is a critical step for adapting large language models (LLMs) to\ndomain-specific downstream tasks. To mitigate the substantial computational and\nmemory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)\nmethods have been proposed to update only a small subset of model parameters.\nHowever, performance gaps between PEFT approaches and full-model fine-tuning\nstill exist. In this work, we present DiaBlo, a simple yet effective PEFT\napproach that updates only the diagonal blocks of selected model weight\nmatrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates\nthe need for low rank matrix products, thereby avoiding the reliance on\nauxiliary initialization schemes or customized optimization strategies to\nimprove convergence. This design leads to stable and robust convergence while\nmaintaining comparable memory efficiency and training speed to LoRA. We conduct\nextensive experiments across a range of tasks, including commonsense reasoning,\narithmetic reasoning, code generation, and safety alignment, to evaluate the\neffectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo\ndemonstrates strong and consistent performance while maintaining high memory\nefficiency and fast finetuning speed. Codes are available at\nhttps://github.com/ziyangjoy/DiaBlo.", "AI": {"tldr": "DiaBlo \u662f\u4e00\u79cd\u6709\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u66f4\u65b0\u9009\u5b9a\u6a21\u578b\u6743\u91cd\u77e9\u9635\u7684\u5bf9\u89d2\u5757\uff0c\u5728\u4fdd\u6301\u9ad8\u5185\u5b58\u6548\u7387\u548c\u5feb\u901f\u5fae\u8c03\u901f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u5168\u6a21\u578b\u5fae\u8c03\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u53ea\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff0c\u4f46\u5b83\u4eec\u4e0e\u5168\u6a21\u578b\u5fae\u8c03\u4e4b\u95f4\u4ecd\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "method": "DiaBlo \u9009\u62e9\u6027\u5730\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u77e9\u9635\u4e2d\u7684\u5bf9\u89d2\u5757\uff0c\u800c\u4e0d\u9700\u8981\u4f4e\u79e9\u77e9\u9635\u4e58\u6cd5\uff0c\u4ece\u800c\u907f\u514d\u4f9d\u8d56\u8f85\u52a9\u521d\u59cb\u5316\u65b9\u6848\u6216\u5b9a\u5236\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\uff08\u5305\u62ec\u5e38\u8bc6\u63a8\u7406\u3001\u7b97\u672f\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u5b89\u5168\u5bf9\u9f50\uff09\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDiaBlo \u8868\u73b0\u51fa\u8272\u4e14\u7a33\u5b9a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u5185\u5b58\u6548\u7387\u548c\u5feb\u901f\u7684\u5fae\u8c03\u901f\u5ea6\u3002", "conclusion": "DiaBlo \u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u5f0f\u6765\u6539\u8fdb PEFT \u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e0e LoRA \u76f8\u5f53\u7684\u5185\u5b58\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.03898", "pdf": "https://arxiv.org/pdf/2506.03898", "abs": "https://arxiv.org/abs/2506.03898", "authors": ["Pierre-Fran\u00e7ois Massiani", "Christian Fiedler", "Lukas Haverbeck", "Friedrich Solowjow", "Sebastian Trimpe"], "title": "A kernel conditional two-sample test", "categories": ["cs.LG", "stat.ML"], "comment": "40 pages, 8 figures, 8 tables. Under review", "summary": "We propose a framework for hypothesis testing on conditional probability\ndistributions, which we then use to construct conditional two-sample\nstatistical tests. These tests identify the inputs -- called covariates in this\ncontext -- where two conditional expectations differ with high probability. Our\nkey idea is to transform confidence bounds of a learning method into a\nconditional two-sample test, and we instantiate this principle for kernel ridge\nregression (KRR) and conditional kernel mean embeddings. We generalize existing\npointwise-in-time or time-uniform confidence bounds for KRR to\npreviously-inaccessible yet essential cases such as infinite-dimensional\noutputs with non-trace-class kernels. These bounds enable circumventing the\nneed for independent data in our statistical tests, since they allow online\nsampling. We also introduce bootstrapping schemes leveraging the parametric\nform of testing thresholds identified in theory to avoid tuning inaccessible\nparameters, making our method readily applicable in practice. Such conditional\ntwo-sample tests are especially relevant in applications where data arrive\nsequentially or non-independently, or when output distributions vary with\noperational parameters. We demonstrate their utility through examples in\nprocess monitoring and comparison of dynamical systems. Overall, our results\nestablish a comprehensive foundation for conditional two-sample testing, from\ntheoretical guarantees to practical implementation, and advance the\nstate-of-the-art on the concentration of vector-valued least squares\nestimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u65b9\u6cd5\u7f6e\u4fe1\u754c\u7684\u6761\u4ef6\u4e24\u6837\u672c\u68c0\u9a8c\u65b0\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u72ec\u7acb\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6838\u5cad\u56de\u5f52\u548c\u6838\u5747\u503c\u5d4c\u5165\u5c55\u793a\u4e86\u5176\u7406\u8bba\u4e0e\u5b9e\u8df5\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u8bc6\u522b\u4e24\u4e2a\u6761\u4ef6\u671f\u671b\u503c\u5728\u9ad8\u6982\u7387\u4e0b\u5b58\u5728\u5dee\u5f02\u7684\u8f93\u5165\u53d8\u91cf\uff0c\u9700\u8981\u53d1\u5c55\u4e00\u79cd\u65b0\u7684\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5c06\u5b66\u4e60\u65b9\u6cd5\u7684\u7f6e\u4fe1\u754c\u8f6c\u5316\u4e3a\u6761\u4ef6\u4e24\u6837\u672c\u68c0\u9a8c\uff0c\u5e76\u5177\u4f53\u5e94\u7528\u4e8e\u6838\u5cad\u56de\u5f52\uff08KRR\uff09\u548c\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\u3002", "result": "\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u65e0\u9650\u7ef4\u8f93\u51fa\u548c\u975e\u8ff9\u7c7b\u6838\u7684\u65b0\u7f6e\u4fe1\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u5728\u7ebf\u62bd\u6837\u7684\u7edf\u8ba1\u68c0\u9a8c\u65b9\u6cd5\u53ca\u907f\u514d\u8c03\u6574\u4e0d\u53ef\u8fbe\u53c2\u6570\u7684\u5f15\u5bfc\u65b9\u6848\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u6761\u4ef6\u4e24\u6837\u672c\u68c0\u9a8c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u5e94\u7528\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6570\u636e\u987a\u5e8f\u5230\u8fbe\u6216\u975e\u72ec\u7acb\u7684\u60c5\u51b5\u4e0b\u975e\u5e38\u6709\u7528\u3002"}}
{"id": "2506.03997", "pdf": "https://arxiv.org/pdf/2506.03997", "abs": "https://arxiv.org/abs/2506.03997", "authors": ["Mario Alviano", "Laura Giordano", "Daniele Theseider Dupr\u00e9"], "title": "A framework for Conditional Reasoning in Answer Set Programming", "categories": ["cs.AI", "cs.LO", "I.2.4"], "comment": "19 pages", "summary": "In this paper we introduce a Conditional Answer Set Programming framework\n(Conditional ASP) for the definition of conditional extensions of Answer Set\nProgramming (ASP). The approach builds on a conditional logic with typicality,\nand on the combination of a conditional knowledge base with an ASP program, and\nallows for conditional reasoning over the answer sets of the program. The\nformalism relies on a multi-preferential semantics (and on the KLM preferential\nsemantics, as a special case) to provide an interpretation of conditionals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6761\u4ef6\u7b54\u6848\u96c6\u7f16\u7a0b\u6846\u67b6\uff0c\u5229\u7528\u5178\u578b\u6027\u6761\u4ef6\u903b\u8f91\u548c\u591a\u4f18\u5148\u8bed\u4e49\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7b54\u6848\u96c6\u7f16\u7a0b\u7684\u6761\u4ef6\u6269\u5c55\u4e0e\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u4ee5\u652f\u6301\u6761\u4ef6\u63a8\u7406\uff0c\u5f15\u5165\u4e86\u6709\u6761\u4ef6\u7b54\u6848\u96c6\u7f16\u7a0b\u6846\u67b6\uff08Conditional ASP\uff09\u3002", "method": "\u7ed3\u5408\u6761\u4ef6\u77e5\u8bc6\u5e93\u4e0eASP\u7a0b\u5e8f\uff0c\u5e76\u91c7\u7528\u591a\u4f18\u5148\u8bed\u4e49\u548cKLM\u4f18\u5148\u8bed\u4e49\u4f5c\u4e3a\u7279\u6b8a\u60c5\u51b5\u6765\u89e3\u91ca\u6761\u4ef6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u6761\u4ef6\u903b\u8f91\u5bf9ASP\u7a0b\u5e8f\u7684\u7b54\u6848\u96c6\u8fdb\u884c\u6761\u4ef6\u6269\u5c55\u548c\u63a8\u7406\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5178\u578b\u6027\u6761\u4ef6\u903b\u8f91\u7684\u6709\u6761\u4ef6\u7b54\u6848\u96c6\u7f16\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u7a0b\u5e8f\u7684\u7b54\u6848\u96c6\u8fdb\u884c\u6761\u4ef6\u63a8\u7406\u3002"}}
{"id": "2506.03234", "pdf": "https://arxiv.org/pdf/2506.03234", "abs": "https://arxiv.org/abs/2506.03234", "authors": ["Kaiwen Duan", "Hongwei Yao", "Yufei Chen", "Ziyun Li", "Tong Qiao", "Zhan Qin", "Cong Wang"], "title": "BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\ntext-to-image (T2I) models with human preferences. However, RLHF's feedback\nmechanism also opens new pathways for adversaries. This paper demonstrates the\nfeasibility of hijacking T2I models by poisoning a small fraction of preference\ndata with natural-appearing examples. Specifically, we propose BadReward, a\nstealthy clean-label poisoning attack targeting the reward model in multi-modal\nRLHF. BadReward operates by inducing feature collisions between visually\ncontradicted preference data instances, thereby corrupting the reward model and\nindirectly compromising the T2I model's integrity. Unlike existing alignment\npoisoning techniques focused on single (text) modality, BadReward is\nindependent of the preference annotation process, enhancing its stealth and\npractical threat. Extensive experiments on popular T2I models show that\nBadReward can consistently guide the generation towards improper outputs, such\nas biased or violent imagery, for targeted concepts. Our findings underscore\nthe amplified threat landscape for RLHF in multi-modal systems, highlighting\nthe urgent need for robust defenses. Disclaimer. This paper contains uncensored\ntoxic content that might be offensive or disturbing to the readers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aBadReward\u7684\u65b0\u653b\u51fb\u65b9\u5f0f\uff0c\u5b83\u80fd\u901a\u8fc7\u6c61\u67d3\u5c11\u91cf\u81ea\u7136\u5916\u89c2\u7684\u504f\u597d\u6570\u636e\u6765\u52ab\u6301\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u751f\u6210\u7ed3\u679c\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u4f7f\u5f97\u53cd\u9988\u673a\u5236\u4e3a\u5bf9\u624b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u9690\u853d\u4e14\u5b9e\u7528\u7684\u653b\u51fb\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86BadReward\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u7684\u9690\u853d\u5e72\u51c0\u6807\u7b7e\u4e2d\u6bd2\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cBadReward\u80fd\u591f\u6301\u7eed\u5f15\u5bfc\u751f\u6210\u4e0d\u5f53\u8f93\u51fa\uff0c\u5982\u9488\u5bf9\u7279\u5b9a\u6982\u5ff5\u7684\u504f\u89c1\u6216\u66b4\u529b\u56fe\u50cf\u3002", "conclusion": "BadReward\u653b\u51fb\u65b9\u6cd5\u7684\u5b58\u5728\u8868\u660e\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u4e2dRLHF\u7684\u589e\u5f3a\u5a01\u80c1\uff0c\u5f3a\u8c03\u9700\u8981\u5f3a\u5927\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2506.03931", "pdf": "https://arxiv.org/pdf/2506.03931", "abs": "https://arxiv.org/abs/2506.03931", "authors": ["Yotam Alexander", "Yonatan Slutzky", "Yuval Ran-Milo", "Nadav Cohen"], "title": "Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Conventional wisdom attributes the mysterious generalization abilities of\noverparameterized neural networks to gradient descent (and its variants). The\nrecent volume hypothesis challenges this view: it posits that these\ngeneralization abilities persist even when gradient descent is replaced by\nGuess & Check (G&C), i.e., by drawing weight settings until one that fits the\ntraining data is found. The validity of the volume hypothesis for wide and deep\nneural networks remains an open question. In this paper, we theoretically\ninvestigate this question for matrix factorization (with linear and non-linear\nactivation)--a common testbed in neural network theory. We first prove that\ngeneralization under G&C deteriorates with increasing width, establishing what\nis, to our knowledge, the first case where G&C is provably inferior to gradient\ndescent. Conversely, we prove that generalization under G&C improves with\nincreasing depth, revealing a stark contrast between wide and deep networks,\nwhich we further validate empirically. These findings suggest that even in\nsimple settings, there may not be a simple answer to the question of whether\nneural networks need gradient descent to generalize well.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u68af\u5ea6\u4e0b\u964d\u662f\u5426\u662f\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u826f\u597d\u6cdb\u5316\u6240\u5fc5\u9700\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5bf9\u4e8e\u77e9\u9635\u5206\u89e3\u6a21\u578b\uff0cGuess & Check\u65b9\u6cd5\u5728\u5bbd\u7f51\u7edc\u4e2d\u6548\u679c\u53d8\u5dee\u4f46\u5728\u6df1\u7f51\u7edc\u4e2d\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u4f53\u79ef\u5047\u8bbe\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u70b9\uff0c\u8ba4\u4e3a\u5373\u4f7f\u5c06\u68af\u5ea6\u4e0b\u964d\u66ff\u6362\u4e3aGuess & Check\uff08G&C\uff09\uff0c\u8fc7\u53c2\u6570\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u4ecd\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\u8be5\u5047\u8bbe\u5728\u6df1\u5c42\u548c\u5bbd\u5c42\u7f51\u7edc\u4e2d\u662f\u5426\u6210\u7acb\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u7814\u7a76\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86Guess & Check\u65b9\u6cd5\u5728\u4e0d\u540c\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u77e9\u9635\u5206\u89e3\u6a21\u578b\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u53d8\u5316\u3002", "result": "\u4f5c\u8005\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5bf9\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u7684\u77e9\u9635\u5206\u89e3\u6a21\u578b\uff0c\u968f\u7740\u5bbd\u5ea6\u589e\u52a0\uff0cGuess & Check\u7684\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff1b\u800c\u968f\u7740\u6df1\u5ea6\u589e\u52a0\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u73b0\u8c61\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u7684\u8bbe\u5b9a\u4e0b\uff0c\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u9700\u8981\u68af\u5ea6\u4e0b\u964d\u4ee5\u826f\u597d\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u5e76\u4e0d\u5b58\u5728\u7b80\u5355\u7684\u7b54\u6848\u3002"}}
{"id": "2506.04018", "pdf": "https://arxiv.org/pdf/2506.04018", "abs": "https://arxiv.org/abs/2506.04018", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Goun\u00e9", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.11; K.4.1; I.2.6"], "comment": "Prepint, under review for NeurIPS 2025", "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgentMisalignment\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4e0d\u4e00\u81f4\u884c\u4e3a\u503e\u5411\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u548c\u7cfb\u7edf\u63d0\u793a\u5bf9\u4e0d\u4e00\u81f4\u503e\u5411\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u76f8\u5173\u7684\u4e0d\u4e00\u81f4\u98ce\u9669\u589e\u52a0\uff0c\u4f46\u73b0\u5b9e\u4e2d\u4ee3\u7406\u5c1d\u8bd5\u4e0d\u4e00\u81f4\u884c\u4e3a\u7684\u53ef\u80fd\u6027\u4ecd\u7f3a\u4e4f\u7406\u89e3\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aAgentMisalignment\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e00\u7cfb\u5217\u73b0\u5b9e\u573a\u666f\uff0c\u8bc4\u4f30LLM\u4ee3\u7406\u5c55\u793a\u4e0d\u4e00\u81f4\u884c\u4e3a\u7684\u673a\u4f1a\u3002", "result": "\u62a5\u544a\u4e86\u524d\u6cbf\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8bc4\u4f30\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\u65f6\uff0c\u5e73\u5747\u800c\u8a00\u4e0d\u4e00\u81f4\u7a0b\u5ea6\u66f4\u9ad8\u3002\u901a\u8fc7\u4e0d\u540c\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u4ee3\u7406\u4e2a\u6027\u7684\u5f71\u54cd\u6709\u65f6\u6bd4\u6a21\u578b\u9009\u62e9\u672c\u8eab\u66f4\u80fd\u5f71\u54cd\u4e0d\u4e00\u81f4\u503e\u5411\u3002", "conclusion": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u5728LLM\u4ee3\u7406\u4e2d\u672a\u80fd\u63a8\u5e7f\uff0c\u5f3a\u8c03\u4e86\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u8fdb\u884c\u503e\u5411\u6027\u8bc4\u4f30\u3002"}}
{"id": "2506.03267", "pdf": "https://arxiv.org/pdf/2506.03267", "abs": "https://arxiv.org/abs/2506.03267", "authors": ["Shahbaz Rezaei", "Avishai Halev", "Xin Liu"], "title": "On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "A prevailing approach to explain time series models is to generate\nattribution in time domain. A recent development in time series XAI is the\nconcept of explanation spaces, where any model trained in the time domain can\nbe interpreted with any existing XAI method in alternative domains, such as\nfrequency. The prevailing approach is to present XAI attributions either in the\ntime domain or in the domain where the attribution is most sparse. In this\npaper, we demonstrate that in certain cases, XAI methods can generate\nattributions that highlight fundamentally different features in the time and\nfrequency domains that are not direct counterparts of one another. This\nsuggests that both domains' attributions should be presented to achieve a more\ncomprehensive interpretation. Thus it shows the necessity of multi-domain\nexplanation. To quantify when such cases arise, we introduce the uncertainty\nprinciple (UP), originally developed in quantum mechanics and later studied in\nharmonic analysis and signal processing, to the XAI literature. This principle\nestablishes a lower bound on how much a signal can be simultaneously localized\nin both the time and frequency domains. By leveraging this concept, we assess\nwhether attributions in the time and frequency domains violate this bound,\nindicating that they emphasize distinct features. In other words, UP provides a\nsufficient condition that the time and frequency domain explanations do not\nmatch and, hence, should be both presented to the end user. We validate the\neffectiveness of this approach across various deep learning models, XAI\nmethods, and a wide range of classification and forecasting datasets. The\nfrequent occurrence of UP violations across various datasets and XAI methods\nhighlights the limitations of existing approaches that focus solely on\ntime-domain explanations. This underscores the need for multi-domain\nexplanations as a new paradigm.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8ba8\u8bba\u4e86\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u89e3\u91ca\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u5355\u4e00\u65f6\u95f4\u57df\u7684\u89e3\u91ca\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u5f15\u5165\u591a\u57df\u89e3\u91ca\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u66f4\u5168\u9762\u5730\u89e3\u91ca\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u9700\u8981\u8003\u8651\u5728\u65f6\u95f4\u548c\u9891\u7387\u57df\u4e2d\u5448\u73b0XAI\u5f52\u56e0\uff0c\u56e0\u4e3a\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5b83\u4eec\u7a81\u51fa\u663e\u793a\u6839\u672c\u4e0d\u540c\u7684\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u539f\u7406\uff08UP\uff09\u6765\u8bc4\u4f30\u65f6\u95f4\u57df\u548c\u9891\u7387\u57df\u7684\u89e3\u91ca\u662f\u5426\u8fdd\u53cd\u8be5\u754c\u9650\uff0c\u4ece\u800c\u5224\u65ad\u5b83\u4eec\u662f\u5426\u5f3a\u8c03\u4e0d\u540c\u7684\u7279\u5f81\u3002", "result": "\u9a8c\u8bc1\u4e86\u5728\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3001XAI\u65b9\u6cd5\u4ee5\u53ca\u5206\u7c7b\u548c\u9884\u6d4b\u6570\u636e\u96c6\u4e2d\u9891\u7e41\u51fa\u73b0UP\u8fdd\u89c4\u73b0\u8c61\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u73b0\u6709\u7684\u4ec5\u5173\u6ce8\u65f6\u95f4\u57df\u89e3\u91ca\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f15\u5165\u591a\u57df\u89e3\u91ca\u4f5c\u4e3a\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2506.03943", "pdf": "https://arxiv.org/pdf/2506.03943", "abs": "https://arxiv.org/abs/2506.03943", "authors": ["Shiyi Yang", "Can Chen", "Didong Li"], "title": "Lower Ricci Curvature for Hypergraphs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Networks with higher-order interactions, prevalent in biological, social, and\ninformation systems, are naturally represented as hypergraphs, yet their\nstructural complexity poses fundamental challenges for geometric\ncharacterization. While curvature-based methods offer powerful insights in\ngraph analysis, existing extensions to hypergraphs suffer from critical\ntrade-offs: combinatorial approaches such as Forman-Ricci curvature capture\nonly coarse features, whereas geometric methods like Ollivier-Ricci curvature\noffer richer expressivity but demand costly optimal transport computations. To\naddress these challenges, we introduce hypergraph lower Ricci curvature (HLRC),\na novel curvature metric defined in closed form that achieves a principled\nbalance between interpretability and efficiency. Evaluated across diverse\nsynthetic and real-world hypergraph datasets, HLRC consistently reveals\nmeaningful higher-order organization, distinguishing intra- from\ninter-community hyperedges, uncovering latent semantic labels, tracking\ntemporal dynamics, and supporting robust clustering of hypergraphs based on\nglobal structure. By unifying geometric sensitivity with algorithmic\nsimplicity, HLRC provides a versatile foundation for hypergraph analytics, with\nbroad implications for tasks including node classification, anomaly detection,\nand generative modeling in complex systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u56fe\u4f4e\u91cc\u5947\u66f2\u7387\uff08HLRC\uff09\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u51e0\u4f55\u654f\u611f\u6027\u548c\u7b97\u6cd5\u7b80\u5355\u6027\uff0c\u53ef\u7528\u4e8e\u5206\u6790\u751f\u7269\u3001\u793e\u4f1a\u548c\u4fe1\u606f\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u9ad8\u9636\u4ea4\u4e92\u7f51\u7edc\u3002", "motivation": "\u73b0\u6709\u7684\u8d85\u56fe\u51e0\u4f55\u8868\u5f81\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u66f2\u7387\u5ea6\u91cf\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5f15\u5165\u4e86\u95ed\u5f0f\u5b9a\u4e49\u7684\u8d85\u56fe\u4f4e\u91cc\u5947\u66f2\u7387\uff08HLRC\uff09\uff0c\u5e73\u8861\u4e86\u89e3\u91ca\u6027\u548c\u6548\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728\u591a\u79cd\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u8d85\u56fe\u6570\u636e\u96c6\u4e2d\uff0cHLRC\u63ed\u793a\u4e86\u6709\u610f\u4e49\u7684\u9ad8\u9636\u7ec4\u7ec7\u7ed3\u6784\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u5168\u5c40\u7ed3\u6784\u7684\u9c81\u68d2\u805a\u7c7b\u3002", "conclusion": "HLRC\u4e3a\u8d85\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u7684\u57fa\u7840\uff0c\u5bf9\u8282\u70b9\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u751f\u6210\u5efa\u6a21\u7b49\u4efb\u52a1\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\u3002"}}
{"id": "2506.04022", "pdf": "https://arxiv.org/pdf/2506.04022", "abs": "https://arxiv.org/abs/2506.04022", "authors": ["Qiyue Xia", "J. Michael Herrmann"], "title": "Interpretability by Design for Efficient Multi-Objective Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-objective reinforcement learning (MORL) aims at optimising several,\noften conflicting goals in order to improve flexibility and reliability of RL\nin practical tasks. This can be achieved by finding diverse policies that are\noptimal for some objective preferences and non-dominated by optimal policies\nfor other preferences so that they form a Pareto front in the multi-objective\nperformance space. The relation between the multi-objective performance space\nand the parameter space that represents the policies is generally non-unique.\nUsing a training scheme that is based on a locally linear map between the\nparameter space and the performance space, we show that an approximate Pareto\nfront can provide an interpretation of the current parameter vectors in terms\nof the objectives which enables an effective search within contiguous solution\ndomains. Experiments are conducted with and without retraining across different\ndomains, and the comparison with previous methods demonstrates the efficiency\nof our approach.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u6620\u5c04\u751f\u6210\u8fd1\u4f3c\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4ece\u800c\u63d0\u9ad8\u7b56\u7565\u4f18\u5316\u7684\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u89e3\u51b3\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u7684\u95ee\u9898\uff0c\u627e\u5230\u5728\u591a\u4e2a\u51b2\u7a81\u76ee\u6807\u4e0b\u8868\u73b0\u826f\u597d\u7684\u7b56\u7565\u3002", "method": "\u8bba\u6587\u7684\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u5c40\u90e8\u7ebf\u6027\u6620\u5c04\u6765\u8fde\u63a5\u53c2\u6570\u7a7a\u95f4\u548c\u6027\u80fd\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u8bba\u6587\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u9ad8\u6548\u5730\u751f\u6210\u8fd1\u4f3c\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e76\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u53c2\u6570\u7a7a\u95f4\u4e0e\u6027\u80fd\u7a7a\u95f4\u4e4b\u95f4\u5c40\u90e8\u7ebf\u6027\u6620\u5c04\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u91ca\u5f53\u524d\u53c2\u6570\u5411\u91cf\uff0c\u5e76\u63d0\u9ad8\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u7387\u3002"}}
{"id": "2506.03979", "pdf": "https://arxiv.org/pdf/2506.03979", "abs": "https://arxiv.org/abs/2506.03979", "authors": ["Haoxuan Chen", "Yinuo Ren", "Martin Renqiang Min", "Lexing Ying", "Zachary Izzo"], "title": "Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach", "categories": ["cs.LG", "cs.CV", "cs.NA", "eess.IV", "math.NA", "stat.ML"], "comment": "45 pages", "summary": "Diffusion models (DMs) have proven to be effective in modeling\nhigh-dimensional distributions, leading to their widespread adoption for\nrepresenting complex priors in Bayesian inverse problems (BIPs). However,\ncurrent DM-based posterior sampling methods proposed for solving common BIPs\nrely on heuristic approximations to the generative process. To exploit the\ngenerative capability of DMs and avoid the usage of such approximations, we\npropose an ensemble-based algorithm that performs posterior sampling without\nthe use of heuristic approximations. Our algorithm is motivated by existing\nworks that combine DM-based methods with the sequential Monte Carlo (SMC)\nmethod. By examining how the prior evolves through the diffusion process\nencoded by the pre-trained score function, we derive a modified partial\ndifferential equation (PDE) governing the evolution of the corresponding\nposterior distribution. This PDE includes a modified diffusion term and a\nreweighting term, which can be simulated via stochastic weighted particle\nmethods. Theoretically, we prove that the error between the true posterior\ndistribution can be bounded in terms of the training error of the pre-trained\nscore function and the number of particles in the ensemble. Empirically, we\nvalidate our algorithm on several inverse problems in imaging to show that our\nmethod gives more accurate reconstructions compared to existing DM-based\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u65b0\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u548c\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u542f\u53d1\u5f0f\u8fd1\u4f3c\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u542f\u53d1\u5f0f\u8fd1\u4f3c\uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u5e76\u907f\u514d\u8fd9\u4e9b\u8fd1\u4f3c\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7814\u7a76\u5148\u9a8c\u5728\u9884\u8bad\u7ec3\u5f97\u5206\u51fd\u6570\u7f16\u7801\u7684\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u6f14\u5316\uff0c\u63a8\u5bfc\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4fee\u6b63\u6269\u6563\u9879\u548c\u52a0\u6743\u9879\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\uff0c\u5e76\u5229\u7528\u968f\u673a\u52a0\u6743\u7c92\u5b50\u65b9\u6cd5\u5bf9\u8be5\u65b9\u7a0b\u8fdb\u884c\u6a21\u62df\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u4f5c\u8005\u8bc1\u660e\u4e86\u771f\u5b9e\u540e\u9a8c\u5206\u5e03\u4e4b\u95f4\u7684\u8bef\u5dee\u53ef\u4ee5\u7531\u9884\u8bad\u7ec3\u5f97\u5206\u51fd\u6570\u7684\u8bad\u7ec3\u8bef\u5dee\u548c\u96c6\u6210\u4e2d\u7c92\u5b50\u6570\u91cf\u6765\u9650\u5b9a\uff1b\u5b9e\u8bc1\u4e0a\uff0c\u4f5c\u8005\u5728\u591a\u4e2a\u6210\u50cf\u9006\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u7684\u540e\u9a8c\u91c7\u6837\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u65e0\u9700\u4f7f\u7528\u542f\u53d1\u5f0f\u8fd1\u4f3c\u5373\u53ef\u6c42\u89e3\u8d1d\u53f6\u65af\u9006\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.04133", "pdf": "https://arxiv.org/pdf/2506.04133", "abs": "https://arxiv.org/abs/2506.04133", "authors": ["Shaina Raza", "Ranjan Sapkota", "Manoj Karkee", "Christos Emmanouilidis"], "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems", "categories": ["cs.AI"], "comment": null, "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u8ba8\u8bba\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u4fe1\u4efb\u3001\u98ce\u9669\u548c\u5b89\u5168\u7ba1\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d1f\u8d23\u4efb\u7684\u4ee3\u7406AI\u8def\u7ebf\u56fe\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6f0f\u6d1e\u548c\u89e3\u51b3\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4f01\u4e1a\u548c\u793e\u4f1a\u9886\u57df\u7684\u5e94\u7528\uff0c\u5bf9\u5176\u4fe1\u4efb\u3001\u98ce\u9669\u548c\u5b89\u5168\u7ba1\u7406\u7684\u7814\u7a76\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u56db\u4e2a\u652f\u67f1\uff08\u6cbb\u7406\u3001\u53ef\u89e3\u91ca\u6027\u3001ModelOps\u548c\u9690\u79c1/\u5b89\u5168\uff09\u8be6\u7ec6\u5206\u6790\u4e86\u4ee3\u7406AI\u6846\u67b6\u4e2d\u7684TRiSM\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u7ed3\u5408\u6848\u4f8b\u7814\u7a76\u8bf4\u660e\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6f0f\u6d1e\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u4e86\u72ec\u7279\u7684\u5a01\u80c1\u5411\u91cf\uff0c\u5e76\u4e3a\u4ee3\u7406AI\u5e94\u7528\u5f15\u5165\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u98ce\u9669\u5206\u7c7b\u6cd5\u3002\u540c\u65f6\u8c03\u67e5\u4e86\u4fe1\u4efb\u5efa\u7acb\u673a\u5236\u3001\u900f\u660e\u5ea6\u548c\u76d1\u7763\u6280\u672f\u4ee5\u53ca\u6700\u5148\u8fdb\u7684\u5206\u5e03\u5f0fLLM\u4ee3\u7406\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u4e00\u4e2a\u8d1f\u8d23\u4efb\u7684\u4ee3\u7406AI\u8def\u7ebf\u56fe\uff0c\u63d0\u51fa\u4e86\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u5c06\u65b0\u5174\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u7a33\u5065\u7684TRiSM\u539f\u5219\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u8d1f\u8d23\u548c\u900f\u660e\u7684\u90e8\u7f72\u3002"}}
{"id": "2506.03307", "pdf": "https://arxiv.org/pdf/2506.03307", "abs": "https://arxiv.org/abs/2506.03307", "authors": ["Kristen Goebel", "William Solow", "Paola Pesantez-Cabrera", "Markus Keller", "Alan Fern"], "title": "Budgeted Online Active Learning with Expert Advice and Episodic Priors", "categories": ["cs.LG"], "comment": null, "summary": "This paper introduces a novel approach to budgeted online active learning\nfrom finite-horizon data streams with extremely limited labeling budgets. In\nagricultural applications, such streams might include daily weather data over a\ngrowing season, and labels require costly measurements of weather-dependent\nplant characteristics. Our method integrates two key sources of prior\ninformation: a collection of preexisting expert predictors and episodic\nbehavioral knowledge of the experts based on unlabeled data streams. Unlike\nprevious research on online active learning with experts, our work\nsimultaneously considers query budgets, finite horizons, and episodic\nknowledge, enabling effective learning in applications with severely limited\nlabeling capacity. We demonstrate the utility of our approach through\nexperiments on various prediction problems derived from both a realistic\nagricultural crop simulator and real-world data from multiple grape cultivars.\nThe results show that our method significantly outperforms baseline expert\npredictions, uniform query selection, and existing approaches that consider\nbudgets and limited horizons but neglect episodic knowledge, even under highly\nconstrained labeling budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5728\u7ebf\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9884\u7b97\u6709\u9650\u3001\u65f6\u95f4\u8303\u56f4\u6709\u9650\u7684\u519c\u4e1a\u6570\u636e\u6d41\u573a\u666f\uff0c\u5e76\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u519c\u4e1a\u5e94\u7528\u4e2d\u9700\u8981\u4ece\u6709\u9650\u7684\u6570\u636e\u6d41\u4e2d\u8fdb\u884c\u9ad8\u6548\u5b66\u4e60\uff0c\u4f46\u6807\u7b7e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u6574\u5408\u4e13\u5bb6\u9884\u6d4b\u5668\u548c\u57fa\u4e8e\u672a\u6807\u8bb0\u6570\u636e\u6d41\u7684\u7247\u6bb5\u77e5\u8bc6\uff0c\u540c\u65f6\u8003\u8651\u67e5\u8be2\u9884\u7b97\u548c\u6709\u9650\u65f6\u95f4\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u9ad8\u5ea6\u53d7\u9650\u7684\u6807\u7b7e\u9884\u7b97\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u4e13\u5bb6\u9884\u6d4b\u548c\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6709\u9650\u9884\u7b97\u548c\u6709\u9650\u65f6\u95f4\u8303\u56f4\u5185\u8fdb\u884c\u5728\u7ebf\u4e3b\u52a8\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002"}}
{"id": "2506.04118", "pdf": "https://arxiv.org/pdf/2506.04118", "abs": "https://arxiv.org/abs/2506.04118", "authors": ["Jonathan Geuter", "Youssef Mroueh", "David Alvarez-Melis"], "title": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs", "categories": ["cs.LG", "stat.ML", "I.2.7"], "comment": "12 pages, 2 figures", "summary": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI .", "AI": {"tldr": "This paper proposes Guided Speculative Inference (GSI), a novel algorithm for efficient reward-guided decoding in large language models, achieving higher accuracy than existing methods.", "motivation": "The motivation of this paper is to develop an efficient reward-guided decoding method for large language models which can achieve high accuracy.", "method": "GSI combines soft best-of-n test-time scaling with a reward model r(x,y) and speculative samples from a small auxiliary model pi_S(y|x).", "result": "The authors derive a theoretical bound on the KL divergence between their induced distribution and the optimal policy. They also demonstrate experimentally that their method achieves higher accuracy than standard soft best-of-n with pi_S and reward-guided speculative decoding.", "conclusion": "GSI is a new algorithm for decoding in large language models that achieves higher accuracy than existing methods, and sometimes even outperforms soft best-of-n with the primary model."}}
{"id": "2506.04135", "pdf": "https://arxiv.org/pdf/2506.04135", "abs": "https://arxiv.org/abs/2506.04135", "authors": ["Pei Yang", "Hai Ci", "Mike Zheng Shou"], "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents show promising capabilities for\nautomating computer-use tasks and facilitating accessibility, but existing\ninteractive benchmarks are mostly English-only, covering web-use or Windows,\nLinux, and Android environments, but not macOS. macOS is a major OS with\ndistinctive GUI patterns and exclusive applications. To bridge the gaps, we\npresent macOSWorld, the first comprehensive benchmark for evaluating GUI agents\non macOS. macOSWorld features 202 multilingual interactive tasks across 30\napplications (28 macOS-exclusive), with task instructions and OS interfaces\noffered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As\nGUI agents are shown to be vulnerable to deception attacks, macOSWorld also\nincludes a dedicated safety benchmarking subset. Our evaluation on six GUI\nagents reveals a dramatic gap: proprietary computer-use agents lead at above\n30% success rate, while open-source lightweight research models lag at below\n2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks\nalso expose common weaknesses, especially in Arabic, with a 27.5% average\ndegradation compared to English. Results from safety benchmarking also\nhighlight that deception attacks are more general and demand immediate\nattention. macOSWorld is available at https://github.com/showlab/macosworld.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5168\u9762\u7684macOS GUI\u4ee3\u7406\u8bc4\u4f30\u5de5\u5177macOSWorld\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u4ee3\u7406\u5728\u6267\u884c\u4efb\u52a1\u548c\u9762\u5bf9\u6b3a\u9a97\u653b\u51fb\u65f6\u7684\u80fd\u529b\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u7684\u8bed\u8a00\u73af\u5883\u4e0b\u3002", "motivation": "\u73b0\u6709\u7684\u4ea4\u4e92\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4e3a\u82f1\u6587\uff0c\u6db5\u76d6\u7f51\u7edc\u4f7f\u7528\u6216Windows\u3001Linux\u548cAndroid\u73af\u5883\uff0c\u4f46\u4e0d\u5305\u62ecmacOS\u3002\u800cmacOS\u5177\u6709\u72ec\u7279\u7684GUI\u6a21\u5f0f\u548c\u72ec\u5bb6\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177macOSWorld\uff0c\u5176\u4e2d\u5305\u62ec202\u4e2a\u591a\u8bed\u8a00\u4ea4\u4e92\u4efb\u52a1\uff0c\u8986\u76d630\u4e2a\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u5305\u62ec\u4e00\u4e2a\u4e13\u95e8\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u5b50\u96c6\u3002", "result": "\u4e13\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u6210\u529f\u7387\u4e0a\u9886\u5148\u4e8e\u5f00\u6e90\u8f7b\u91cf\u7ea7\u7814\u7a76\u6a21\u578b\uff0c\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u66b4\u9732\u4e86\u5e38\u89c1\u5f31\u70b9\uff0c\u5c24\u5176\u662f\u5728\u963f\u62c9\u4f2f\u8bed\u4e0a\uff0c\u4e0e\u82f1\u8bed\u76f8\u6bd4\u5e73\u5747\u4e0b\u964d\u4e8627.5%\u3002", "conclusion": "macOSWorld\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4e86\u4e13\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u548c\u5f00\u6e90\u8f7b\u91cf\u7ea7\u7814\u7a76\u6a21\u578b\u4e4b\u95f4\u5728\u6210\u529f\u6267\u884c\u4efb\u52a1\u65b9\u9762\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u4ee5\u53ca\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u5f3a\u8c03\u4e86\u6b3a\u9a97\u653b\u51fb\u7684\u666e\u904d\u6027\u9700\u8981\u7acb\u5373\u5173\u6ce8\u3002"}}
{"id": "2506.03320", "pdf": "https://arxiv.org/pdf/2506.03320", "abs": "https://arxiv.org/abs/2506.03320", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 1 figure, accepted at TCAI workshop 2025", "summary": "Continual learning--the ability to acquire, retain, and refine knowledge over\ntime--has always been fundamental to intelligence, both human and artificial.\nHistorically, different AI paradigms have acknowledged this need, albeit with\nvarying priorities: early expert and production systems focused on incremental\nknowledge consolidation, while reinforcement learning emphasised dynamic\nadaptation. With the rise of deep learning, deep continual learning has\nprimarily focused on learning robust and reusable representations over time to\nsolve sequences of increasingly complex tasks. However, the emergence of Large\nLanguage Models (LLMs) and foundation models has raised the question: Do we\nstill need continual learning when centralised, monolithic models can tackle\ndiverse tasks with access to internet-scale knowledge? We argue that continual\nlearning remains essential for three key reasons: (i) continual pre-training is\nstill necessary to ensure foundation models remain up to date, mitigating\nknowledge staleness and distribution shifts while integrating new information;\n(ii) continual fine-tuning enables models to specialise and personalise,\nadapting to domain-specific tasks, user preferences, and real-world constraints\nwithout full retraining, avoiding the need for computationally expensive long\ncontext-windows; (iii) continual compositionality offers a scalable and modular\napproach to intelligence, enabling the orchestration of foundation models and\nagents to be dynamically composed, recombined, and adapted. While continual\npre-training and fine-tuning are explored as niche research directions, we\nargue it is continual compositionality that will mark the rebirth of continual\nlearning. The future of AI will not be defined by a single static model but by\nan ecosystem of continually evolving and interacting models, making continual\nlearning more relevant than ever.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u6301\u7eed\u7ec4\u5408\u6027\u662f\u672a\u6765\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u7684\u5173\u952e\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4f5c\u8005\u63a2\u8ba8\u4e86\u6301\u7eed\u5b66\u4e60\u662f\u5426\u4ecd\u7136\u5fc5\u8981\uff0c\u5e76\u9610\u8ff0\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u4ee5\u53ca\u7ec4\u5408\u6027\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u56de\u987e\u4eba\u5de5\u667a\u80fd\u4e0d\u540c\u8303\u5f0f\u5bf9\u6301\u7eed\u5b66\u4e60\u7684\u5386\u53f2\u9700\u6c42\uff0c\u5e76\u5206\u6790\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u4e0e\u9650\u5236\u6765\u8fbe\u5230\u5176\u7ed3\u8bba\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u539f\u56e0\u8bf4\u660e\u6301\u7eed\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff1a(i) \u6301\u7eed\u9884\u8bad\u7ec3\u786e\u4fdd\u57fa\u7840\u6a21\u578b\u4fdd\u6301\u6700\u65b0\u72b6\u6001\uff1b(ii) \u6301\u7eed\u5fae\u8c03\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u4e1a\u5316\u548c\u4e2a\u6027\u5316\uff1b(iii) \u6301\u7eed\u7ec4\u5408\u6027\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u6a21\u5757\u5316\u7684\u667a\u80fd\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u51fa\u73b0\u6539\u53d8\u4e86\u8fde\u7eed\u5b66\u4e60\u7684\u9700\u6c42\uff0c\u4f46\u6301\u7eed\u5b66\u4e60\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u662f\u6301\u7eed\u7ec4\u5408\u6027\u5c06\u6807\u5fd7\u7740\u6301\u7eed\u5b66\u4e60\u7684\u91cd\u751f\u3002\u672a\u6765\u7684AI\u5c06\u7531\u4e0d\u65ad\u8fdb\u5316\u548c\u76f8\u4e92\u4f5c\u7528\u7684\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u5b9a\u4e49\uff0c\u4f7f\u5f97\u6301\u7eed\u5b66\u4e60\u6bd4\u4ee5\u5f80\u4efb\u4f55\u65f6\u5019\u90fd\u66f4\u52a0\u91cd\u8981\u3002"}}
{"id": "2506.04166", "pdf": "https://arxiv.org/pdf/2506.04166", "abs": "https://arxiv.org/abs/2506.04166", "authors": ["Caleb Chin", "Aashish Khubchandani", "Harshvardhan Maskara", "Kyuseong Choi", "Jacob Feitelberg", "Albert Gong", "Manit Paul", "Tathagata Sadhukhan", "Anish Agarwal", "Raaz Dwivedi"], "title": "N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": "21 pages, 6 figures", "summary": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86N$^2$\u6846\u67b6\u53ca\u5176\u65b0NN\u53d8\u4f53\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "NN\u65b9\u6cd5\u5728\u77e9\u9635\u8865\u5168\u4e2d\u91cd\u65b0\u5174\u8d77\uff0c\u5177\u6709\u5f3a\u5927\u7684\u7ecf\u9a8c\u6027\u80fd\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u4fc3\u8fdb\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684Python\u5305N$^2$\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684NN\u53d8\u4f53\u3002", "result": "\u65b0\u63d0\u51fa\u7684NN\u53d8\u4f53\u5728\u591a\u4e2a\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u57fa\u51c6\u5957\u4ef6\u3002", "conclusion": "\u57fa\u4e8eNN\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.04210", "pdf": "https://arxiv.org/pdf/2506.04210", "abs": "https://arxiv.org/abs/2506.04210", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u601d\u8003\u5bf9\u63a8\u7406\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fc7\u5ea6\u601d\u8003\u4f1a\u589e\u52a0\u8f93\u51fa\u65b9\u5dee\u5e76\u964d\u4f4e\u7cbe\u5ea6\uff0c\u800c\u975e\u771f\u6b63\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u2014\u2014\u5e76\u884c\u601d\u8003\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u72ec\u7acb\u63a8\u7406\u8def\u5f84\u5e76\u8fdb\u884c\u591a\u6570\u6295\u7968\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u6700\u8fd1\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u6269\u5c55\u601d\u8003\u75d5\u8ff9\uff08\u4f8b\u5982\u4f7f\u7528\u201cWait\u201d\u6216\u201cLet me rethink\u201d\u63d0\u793a\uff09\u63d0\u9ad8\u6027\u80fd\u7684\u8d8b\u52bf\u5f15\u53d1\u4e86\u5173\u6ce8\uff0c\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u8fd9\u79cd\u505a\u6cd5\u662f\u5426\u771f\u6b63\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u6a21\u578b\u548c\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u53d8\u5316\u8d8b\u52bf\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6982\u7387\u6a21\u578b\u6765\u89e3\u91ca\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u73b0\u8c61\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86\u201c\u5e76\u884c\u601d\u8003\u201d\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5e76\u5bf9\u5176\u6548\u679c\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u521d\u671f\u589e\u52a0\u601d\u8003\u65f6\u95f4\u786e\u5b9e\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u968f\u540e\u7531\u4e8e\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u989d\u5916\u7684\u601d\u8003\u589e\u52a0\u4e86\u8f93\u51fa\u65b9\u5dee\uff0c\u9020\u6210\u4e86\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u5047\u8c61\uff0c\u6700\u7ec8\u53cd\u800c\u964d\u4f4e\u4e86\u7cbe\u5ea6\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u6d4b\u8bd5\u65f6\u901a\u8fc7\u6269\u5c55\u601d\u8003\u5e76\u4e0d\u662f\u5229\u7528\u63a8\u7406\u9884\u7b97\u7684\u6709\u6548\u65b9\u5f0f\u3002\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBest-of-N\u91c7\u6837\u7684\u201c\u5e76\u884c\u601d\u8003\u201d\u65b9\u6cd5\uff0c\u5728\u76f8\u540c\u9884\u7b97\u4e0b\u751f\u6210\u591a\u4e2a\u72ec\u7acb\u63a8\u7406\u8def\u5f84\u5e76\u901a\u8fc7\u591a\u6570\u6295\u7968\u9009\u62e9\u6700\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u8fbe20%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002"}}
{"id": "2506.03324", "pdf": "https://arxiv.org/pdf/2506.03324", "abs": "https://arxiv.org/abs/2506.03324", "authors": ["Ethan Che", "Hakan Ceylan", "James McInerney", "Nathan Kallus"], "title": "Optimization of Epsilon-Greedy Exploration", "categories": ["cs.LG"], "comment": null, "summary": "Modern recommendation systems rely on exploration to learn user preferences\nfor new items, typically implementing uniform exploration policies (e.g.,\nepsilon-greedy) due to their simplicity and compatibility with machine learning\n(ML) personalization models. Within these systems, a crucial consideration is\nthe rate of exploration - what fraction of user traffic should receive random\nitem recommendations and how this should evolve over time. While various\nheuristics exist for navigating the resulting exploration-exploitation\ntradeoff, selecting optimal exploration rates is complicated by practical\nconstraints including batched updates, time-varying user traffic, short time\nhorizons, and minimum exploration requirements. In this work, we propose a\nprincipled framework for determining the exploration schedule based on directly\nminimizing Bayesian regret through stochastic gradient descent (SGD), allowing\nfor dynamic exploration rate adjustment via Model-Predictive Control (MPC).\nThrough extensive experiments with recommendation datasets, we demonstrate that\nvariations in the batch size across periods significantly influence the optimal\nexploration strategy. Our optimization methods automatically calibrate\nexploration to the specific problem setting, consistently matching or\noutperforming the best heuristic for each setting.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u63a8\u8350\u7cfb\u7edf\u4e2d\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u7387\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u9057\u61be\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u6743\u8861\uff0c\u4f46\u5b9e\u9645\u9650\u5236\u6761\u4ef6\u5982\u6279\u91cf\u66f4\u65b0\u3001\u7528\u6237\u6d41\u91cf\u53d8\u5316\u7b49\u4f7f\u9009\u62e9\u6700\u4f73\u63a2\u7d22\u7387\u53d8\u5f97\u590d\u6742\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u76f4\u63a5\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u9057\u61be\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u8fdb\u884c\u52a8\u6001\u63a2\u7d22\u7387\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8de8\u5468\u671f\u7684\u6279\u91cf\u5927\u5c0f\u53d8\u5316\u663e\u8457\u5f71\u54cd\u6700\u4f18\u63a2\u7d22\u7b56\u7565\uff0c\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u5177\u4f53\u95ee\u9898\u8bbe\u7f6e\u81ea\u52a8\u6821\u51c6\u63a2\u7d22\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u9057\u61be\u6700\u5c0f\u5316\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u7387\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5404\u79cd\u5b9e\u9645\u6761\u4ef6\u4e0b\u81ea\u52a8\u6821\u51c6\u63a2\u7d22\u7b56\u7565\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2209.01205", "pdf": "https://arxiv.org/pdf/2209.01205", "abs": "https://arxiv.org/abs/2209.01205", "authors": ["Han Wu", "Jie Yin", "Bala Rajaratnam", "Jianyuan Guo"], "title": "Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2"], "comment": "Published at ICLR 2023", "summary": "Knowledge graphs (KGs) are powerful in terms of their inference abilities,\nbut are also notorious for their incompleteness and long-tail distribution of\nrelations. To address these challenges and expand the coverage of KGs, few-shot\nKG completion aims to make predictions for triplets involving novel relations\nwhen only a few training triplets are provided as reference. Previous methods\nhave focused on designing local neighbor aggregators to learn entity-level\ninformation and/or imposing a potentially invalid sequential dependency\nassumption at the triplet level to learn meta relation information. However,\npairwise triplet-level interactions and context-level relational information\nhave been largely overlooked for learning meta representations of few-shot\nrelations. In this paper, we propose a hierarchical relational learning method\n(HiRe) for few-shot KG completion. By jointly capturing three levels of\nrelational information (entity-level, triplet-level and context-level), HiRe\ncan effectively learn and refine meta representations of few-shot relations,\nand thus generalize well to new unseen relations. Extensive experiments on\nbenchmark datasets validate the superiority of HiRe over state-of-the-art\nmethods. The code can be found in https://github.com/alexhw15/HiRe.git.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u65b0\u65b9\u6cd5HiRe\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6355\u6349\u5b9e\u4f53\u7ea7\u3001\u4e09\u5143\u7ec4\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\u7684\u5173\u7cfb\u4fe1\u606f\u6765\u6709\u6548\u5b66\u4e60\u548c\u4f18\u5316\u5c11\u6837\u672c\u5173\u7cfb\u7684\u5143\u8868\u793a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u5728\u63a8\u7406\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u5176\u4e0d\u5b8c\u6574\u6027\u548c\u5173\u7cfb\u7684\u957f\u5c3e\u5206\u5e03\u800c\u81ed\u540d\u662d\u8457\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u5e76\u6269\u5927\u77e5\u8bc6\u56fe\u8c31\u7684\u8986\u76d6\u8303\u56f4\uff0c\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65e8\u5728\u4ec5\u63d0\u4f9b\u5c11\u91cf\u8bad\u7ec3\u4e09\u5143\u7ec4\u4f5c\u4e3a\u53c2\u8003\u65f6\uff0c\u5bf9\u6d89\u53ca\u65b0\u5173\u7cfb\u7684\u4e09\u5143\u7ec4\u8fdb\u884c\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiRe\u7684\u5206\u5c42\u5173\u7cfb\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u8054\u5408\u6355\u6349\u5b9e\u4f53\u7ea7\u3001\u4e09\u5143\u7ec4\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\u7684\u5173\u7cfb\u4fe1\u606f\u6765\u6539\u8fdb\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HiRe\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u5176\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiRe\u7684\u5206\u5c42\u5173\u7cfb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u3002\u901a\u8fc7\u6355\u6349\u4e09\u4e2a\u5c42\u6b21\u7684\u5173\u7cfb\u4fe1\u606f\uff08\u5b9e\u4f53\u7ea7\u3001\u4e09\u5143\u7ec4\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\uff09\uff0cHiRe\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u548c\u4f18\u5316\u5c11\u6837\u672c\u5173\u7cfb\u7684\u5143\u8868\u793a\uff0c\u4ece\u800c\u5f88\u597d\u5730\u6cdb\u5316\u5230\u65b0\u7684\u672a\u89c1\u8fc7\u7684\u5173\u7cfb\u3002"}}
{"id": "2506.03333", "pdf": "https://arxiv.org/pdf/2506.03333", "abs": "https://arxiv.org/abs/2506.03333", "authors": ["Juan Sebastian Rojas", "Chi-Guhn Lee"], "title": "A Differential Perspective on Distributional Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To date, distributional reinforcement learning (distributional RL) methods\nhave exclusively focused on the discounted setting, where an agent aims to\noptimize a potentially-discounted sum of rewards over time. In this work, we\nextend distributional RL to the average-reward setting, where an agent aims to\noptimize the reward received per time-step. In particular, we utilize a\nquantile-based approach to develop the first set of algorithms that can\nsuccessfully learn and/or optimize the long-run per-step reward distribution,\nas well as the differential return distribution of an average-reward MDP. We\nderive proven-convergent tabular algorithms for both prediction and control, as\nwell as a broader family of algorithms that have appealing scaling properties.\nEmpirically, we find that these algorithms consistently yield competitive\nperformance when compared to their non-distributional equivalents, while also\ncapturing rich information about the long-run reward and return distributions.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7b97\u6cd5\u5e76\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ea\u5173\u6ce8\u6298\u6263\u8bbe\u7f6e\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u4f18\u5316\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u5956\u52b1\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u65b9\u6cd5\u5f00\u53d1\u65b0\u7b97\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u53ef\u8bc1\u660e\u6536\u655b\u7684\u8868\u683c\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u7ec4\u65b0\u7b97\u6cd5\uff0c\u80fd\u591f\u5b66\u4e60\u548c\u4f18\u5316\u957f\u671f\u6bcf\u6b65\u5956\u52b1\u5206\u5e03\u4ee5\u53ca\u5dee\u5206\u56de\u62a5\u5206\u5e03\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u5730\u5c06\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230\u4e86\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u6765\u4f18\u5316\u6bcf\u65f6\u95f4\u6b65\u7684\u5956\u52b1\u3002"}}
{"id": "2506.03337", "pdf": "https://arxiv.org/pdf/2506.03337", "abs": "https://arxiv.org/abs/2506.03337", "authors": ["Yide Ran", "Wentao Guo", "Jingwei Sun", "Yanzhou Pan", "Xiaodong Yu", "Hao Wang", "Jianwen Xie", "Yiran Chen", "Denghui Zhang", "Zhaozhuo Xu"], "title": "Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": "56 pages, 11 figures", "summary": "Federated Learning enables collaborative fine-tuning of Large Language Models\n(LLMs) across decentralized Non-Independent and Identically Distributed\n(Non-IID) clients, but such models' massive parameter sizes lead to significant\nmemory and communication challenges. This work introduces Meerkat, a sparse\nzeroth-order optimization (ZO) method designed for federated LLM fine-tuning.\nBy limiting fine-tuning to a transferable, static, extremely sparse subset of\nparameters, Meerkat achieves remarkable communication efficiency, enabling\ncost-effective high-frequency synchronization. With theoretical analysis and\nexperiments, we show that this high-frequency communication effectively\nmitigates Non-IID data challenges and leads to superior performance compared to\nfull-parameter ZO. Furthermore, experiment results show that Meerkat\noutperforms existing sparsity baselines with better performance at the same\ncommunication frequency. To further handle Non-IID drift, Meerkat leverages\ntraceable local updates and forms a virtual path for each client. This virtual\npath mechanism reveals the GradIP phenomenon: the inner products between LLM\npre-training gradients maintained by server and client gradients estimated via\nZO converges for extreme Non-IID clients but oscillates for IID ones. This\ndistinct behavior provides a signal for identifying clients with extreme data\nheterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP\ntrajectories to identify extreme Non-IID clients and applies early stopping to\nenhance aggregated model quality. Experiments confirm that Meerkat and\nMeerkat-vp significantly improve the efficiency and effectiveness of ZO\nfederated LLM fine-tuning.", "AI": {"tldr": "Meerkat\u662f\u4e00\u79cd\u7a00\u758f\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u90a6\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff0c\u901a\u8fc7\u9650\u5236\u53c2\u6570\u66f4\u65b0\u5b9e\u73b0\u901a\u4fe1\u6548\u7387\uff0c\u6709\u6548\u5904\u7406\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u6311\u6218\uff0c\u5e76\u63d0\u51faMeerkat-vp\u8bc6\u522b\u6781\u7aef\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5ba2\u6237\u7aef\u4ee5\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u89c4\u6a21\u5bfc\u81f4\u663e\u8457\u5185\u5b58\u548c\u901a\u4fe1\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4f18\u5316\u65b9\u6cd5\u5e94\u5bf9\u53bb\u4e2d\u5fc3\u5316\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5ba2\u6237\u7aef\u573a\u666f\u3002", "method": "Meerkat\u91c7\u7528\u9759\u6001\u3001\u53ef\u8fc1\u79fb\u7684\u6781\u7a00\u758f\u53c2\u6570\u5b50\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u5229\u7528\u53ef\u8ffd\u8e2a\u672c\u5730\u66f4\u65b0\u5f62\u6210\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u865a\u62df\u8def\u5f84\uff0c\u5e76\u57fa\u4e8eGradIP\u73b0\u8c61\u5206\u6790\u8bc6\u522b\u6781\u7aef\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5ba2\u6237\u7aef\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMeerkat\u76f8\u6bd4\u5168\u53c2\u6570\u96f6\u9636\u4f18\u5316\u66f4\u9ad8\u6548\u4e14\u80fd\u6709\u6548\u7f13\u89e3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u6311\u6218\uff1bMeerkat-vp\u901a\u8fc7\u5206\u6790GradIP\u8f68\u8ff9\u8fdb\u4e00\u6b65\u63d0\u5347\u805a\u5408\u6a21\u578b\u8d28\u91cf\u3002", "conclusion": "Meerkat\u53caMeerkat-vp\u5728\u901a\u4fe1\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u8054\u90a6\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03355", "pdf": "https://arxiv.org/pdf/2506.03355", "abs": "https://arxiv.org/abs/2506.03355", "authors": ["Elias Abad Rocamora", "Christian Schlarmann", "Naman Deep Singh", "Yongtao Wu", "Matthias Hein", "Volkan Cevher"], "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLEAF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u5fae\u8c03\u63d0\u5347CLIP\u6587\u672c\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u81f4\u529b\u4e8e\u63d0\u9ad8CLIP\u56fe\u50cf\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u6587\u672c\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u6587\u732e\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86LEAF\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u6587\u672c\u57df\u5bf9\u6297\u5fae\u8c03\u65b9\u6cd5\uff0c\u5177\u6709\u6269\u5c55\u5230\u5927\u578bCLIP\u6a21\u578b\u7684\u80fd\u529b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6587\u672c\u57df\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u5bf9\u6297\u51c6\u786e\u6027\uff0c\u6539\u5584\u4e86\u5bf9\u6297\u566a\u58f0\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u548c\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u53ec\u56de\u7387\uff0c\u5e76\u63d0\u5347\u4e86\u8f93\u5165\u6587\u672c\u5d4c\u5165\u540e\u7684\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "LEAF\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347CLIP\u6587\u672c\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u89c6\u89c9\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6a21\u6001\u4efb\u52a1\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6297\u566a\u80fd\u529b\u548c\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2506.03370", "pdf": "https://arxiv.org/pdf/2506.03370", "abs": "https://arxiv.org/abs/2506.03370", "authors": ["Leonid Ryvkin"], "title": "Comparison of different Unique hard attention transformer models by the formal languages they can recognize", "categories": ["cs.LG", "cs.CL", "cs.FL"], "comment": null, "summary": "This note is a survey of various results on the capabilities of unique hard\nattention transformers encoders (UHATs) to recognize formal languages. We\ndistinguish between masked vs. non-masked, finite vs. infinite image and\ngeneral vs. bilinear attention score functions. We recall some relations\nbetween these models, as well as a lower bound in terms of first-order logic\nand an upper bound in terms of circuit complexity.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u72ec\u7279\u786c\u6ce8\u610f\u529b\u53d8\u538b\u5668\u7f16\u7801\u5668(UHATs)\u5728\u8bc6\u522b\u5f62\u5f0f\u8bed\u8a00\u65b9\u9762\u7684\u6027\u80fd\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u786e\u7acb\u4e86\u8ba1\u7b97\u80fd\u529b\u7684\u7406\u8bba\u8fb9\u754c\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u72ec\u7279\u786c\u6ce8\u610f\u529b\u53d8\u538b\u5668\u7f16\u7801\u5668(UHATs)\u5728\u8bc6\u522b\u5f62\u5f0f\u8bed\u8a00\u65b9\u9762\u7684\u6f5c\u529b\u548c\u9650\u5236\u3002", "method": "\u5bf9\u4e0d\u540c\u7684\u7ed3\u679c\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u533a\u5206\u4e86\u5c4f\u853d\u4e0e\u975e\u5c4f\u853d\u3001\u6709\u9650\u4e0e\u65e0\u9650\u56fe\u50cf\u4ee5\u53ca\u4e00\u822c\u4e0e\u53cc\u7ebf\u6027\u6ce8\u610f\u529b\u5f97\u5206\u51fd\u6570\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u603b\u7ed3\u4e86\u8fd9\u4e9b\u6a21\u578b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u7684\u4e0b\u754c\u548c\u57fa\u4e8e\u7535\u8def\u590d\u6742\u5ea6\u7684\u4e0a\u754c\u3002", "conclusion": "UHATs\u5728\u8bc6\u522b\u5f62\u5f0f\u8bed\u8a00\u65b9\u9762\u7684\u80fd\u529b\u5df2\u7ecf\u88ab\u8c03\u67e5\uff0c\u5e76\u4e14\u5df2\u7ecf\u786e\u5b9a\u4e86\u4e0e\u4e00\u9636\u903b\u8f91\u7684\u4e0b\u754c\u548c\u7535\u8def\u590d\u6742\u5ea6\u7684\u4e0a\u754c\u7684\u5173\u7cfb\u3002"}}
{"id": "2506.03374", "pdf": "https://arxiv.org/pdf/2506.03374", "abs": "https://arxiv.org/abs/2506.03374", "authors": ["Haley Dozier", "Althea Henslee", "Ashley Abraham", "Andrew Strelzoff", "Mark Chappell"], "title": "Product Quantization for Surface Soil Similarity", "categories": ["cs.LG"], "comment": "To be published in the CSCE 2022 proceedings", "summary": "The use of machine learning (ML) techniques has allowed rapid advancements in\nmany scientific and engineering fields. One of these problems is that of\nsurface soil taxonomy, a research area previously hindered by the reliance on\nhuman-derived classifications, which are mostly dependent on dividing a dataset\nbased on historical understandings of that data rather than data-driven,\nstatistically observable similarities. Using a ML-based taxonomy allows soil\nresearchers to move beyond the limitations of human visualization and create\nclassifications of high-dimension datasets with a much higher level of\nspecificity than possible with hand-drawn taxonomies. Furthermore, this\npipeline allows for the possibility of producing both highly accurate and\nflexible soil taxonomies with classes built to fit a specific application. The\nmachine learning pipeline outlined in this work combines product quantization\nwith the systematic evaluation of parameters and output to get the best\navailable results, rather than accepting sub-optimal results by using either\ndefault settings or best guess settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6539\u8fdb\u571f\u58e4\u5206\u7c7b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u571f\u58e4\u5206\u7c7b\u4f9d\u8d56\u4e8e\u5386\u53f2\u7406\u89e3\u548c\u4eba\u5de5\u5212\u5206\uff0c\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u9650\u5236\u4e86\u9ad8\u7ef4\u6570\u636e\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002", "method": "\u7ed3\u5408\u4ea7\u54c1\u91cf\u5316\u4e0e\u53c2\u6570\u53ca\u8f93\u51fa\u7684\u7cfb\u7edf\u8bc4\u4f30\u6765\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u9ed8\u8ba4\u6216\u731c\u6d4b\u8bbe\u7f6e\u3002", "result": "\u5b9e\u73b0\u4e86\u6bd4\u4eba\u5de5\u5206\u7c7b\u66f4\u9ad8\u7cbe\u786e\u5ea6\u548c\u7075\u6d3b\u6027\u7684\u571f\u58e4\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u80fd\u6839\u636e\u7279\u5b9a\u5e94\u7528\u6784\u5efa\u5206\u7c7b\u4f53\u7cfb\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u571f\u58e4\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u53ef\u4ee5\u4ea7\u751f\u9ad8\u5ea6\u51c6\u786e\u548c\u7075\u6d3b\u7684\u5206\u7c7b\u7cfb\u7edf\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4eba\u5de5\u5206\u7c7b\u65b9\u6cd5\u7684\u9650\u5236\u3002"}}
{"id": "2506.03392", "pdf": "https://arxiv.org/pdf/2506.03392", "abs": "https://arxiv.org/abs/2506.03392", "authors": ["Aref Ghoreishee", "Abhishek Mishra", "John Walsh", "Anup Das", "Nagarajan Kandasamy"], "title": "Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "We propose a new ternary spiking neuron model to improve the representation\ncapacity of binary spiking neurons in deep Q-learning. Although a ternary\nneuron model has recently been introduced to overcome the limited\nrepresentation capacity offered by the binary spiking neurons, we show that its\nperformance is worse than that of binary models in deep Q-learning tasks. We\nhypothesize gradient estimation bias during the training process as the\nunderlying potential cause through mathematical and empirical analysis. We\npropose a novel ternary spiking neuron model to mitigate this issue by reducing\nthe estimation bias. We use the proposed ternary spiking neuron as the\nfundamental computing unit in a deep spiking Q-learning network (DSQN) and\nevaluate the network's performance in seven Atari games from the Gym\nenvironment. Results show that the proposed ternary spiking neuron mitigates\nthe drastic performance degradation of ternary neurons in Q-learning tasks and\nimproves the network performance compared to the existing binary neurons,\nmaking DSQN a more practical solution for on-board autonomous decision-making\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4e09\u503c\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u4f18\u5316\u4e86\u6df1\u5ea6Q\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u68af\u5ea6\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2aAtari\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u4e8c\u503c\u8109\u51b2\u795e\u7ecf\u5143\u5728\u6df1\u5ea6Q\u5b66\u4e60\u4e2d\u7684\u8868\u793a\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u4e09\u503c\u795e\u7ecf\u5143\u6a21\u578b\u6027\u80fd\u8f83\u5dee\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4f30\u8ba1\u504f\u5dee\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u503c\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u57fa\u672c\u8ba1\u7b97\u5355\u5143\u5e94\u7528\u4e8e\u6df1\u5ea6\u8109\u51b2Q\u5b66\u4e60\u7f51\u7edc\uff08DSQN\uff09\u4e2d\uff0c\u5728\u4e03\u4e2aAtari\u6e38\u620f\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u4e09\u503c\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u6709\u6548\u7f13\u89e3\u4e86\u4e09\u503c\u6a21\u578b\u5728Q\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u4e8e\u73b0\u6709\u4e8c\u503c\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u503c\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u80fd\u591f\u51cf\u8f7bQ\u5b66\u4e60\u4efb\u52a1\u4e2d\u4e09\u503c\u795e\u7ecf\u5143\u7684\u5267\u70c8\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u4e8c\u503c\u795e\u7ecf\u5143\u76f8\u6bd4\u63d0\u9ad8\u4e86\u7f51\u7edc\u6027\u80fd\uff0c\u4f7f\u5f97DSQN\u6210\u4e3a\u66f4\u5b9e\u7528\u7684\u81ea\u4e3b\u51b3\u7b56\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03404", "pdf": "https://arxiv.org/pdf/2506.03404", "abs": "https://arxiv.org/abs/2506.03404", "authors": ["Walter Mayor", "Johan Obando-Ceron", "Aaron Courville", "Pablo Samuel Castro"], "title": "The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "The use of parallel actors for data collection has been an effective\ntechnique used in reinforcement learning (RL) algorithms. The manner in which\ndata is collected in these algorithms, controlled via the number of parallel\nenvironments and the rollout length, induces a form of bias-variance trade-off;\nthe number of training passes over the collected data, on the other hand, must\nstrike a balance between sample efficiency and overfitting. We conduct an\nempirical analysis of these trade-offs on PPO, one of the most popular RL\nalgorithms that uses parallel actors, and establish connections to network\nplasticity and, more generally, optimization stability. We examine its impact\non network architectures, as well as the hyper-parameter sensitivity when\nscaling data. Our analyses indicate that larger dataset sizes can increase\nfinal performance across a variety of settings, and that scaling parallel\nenvironments is more effective than increasing rollout lengths. These findings\nhighlight the critical role of data collection strategies in improving agent\nperformance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e76\u884c\u6267\u884c\u5668\u7684\u6570\u636e\u6536\u96c6\u7b56\u7565\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u548c\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u66f4\u5927\u7684\u6570\u636e\u96c6\u548c\u66f4\u591a\u7684\u5e76\u884c\u73af\u5883\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60\u4e2d\u901a\u8fc7\u5e76\u884c\u6267\u884c\u5668\u8fdb\u884c\u6570\u636e\u6536\u96c6\u7684\u6280\u672f\uff0c\u7814\u7a76\u6570\u636e\u6536\u96c6\u65b9\u5f0f\u6240\u5f15\u53d1\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u4ee5\u53ca\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6837\u672c\u6548\u7387\u548c\u8fc7\u62df\u5408\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u8bba\u6587\u5bf9PPO\u8fd9\u4e00\u4f7f\u7528\u5e76\u884c\u6267\u884c\u5668\u7684\u6d41\u884cRL\u7b97\u6cd5\u4e2d\u7684\u8fd9\u4e9b\u6743\u8861\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0e\u7f51\u7edc\u53ef\u5851\u6027\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u7684\u8054\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u66f4\u5927\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u63d0\u9ad8\u5404\u79cd\u8bbe\u7f6e\u4e0b\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u540c\u65f6\u6269\u5c55\u5e76\u884c\u73af\u5883\u7684\u6548\u679c\u4f18\u4e8e\u589e\u52a0rollout\u957f\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u5176\u5bf9\u7f51\u7edc\u67b6\u6784\u7684\u5f71\u54cd\u4ee5\u53ca\u6269\u5c55\u6570\u636e\u65f6\u7684\u8d85\u53c2\u6570\u654f\u611f\u6027\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u6570\u636e\u6536\u96c6\u7b56\u7565\u5728\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u66f4\u5927\u7684\u6570\u636e\u96c6\u5927\u5c0f\u53ef\u4ee5\u63d0\u9ad8\u6700\u7ec8\u6027\u80fd\uff0c\u800c\u6269\u5c55\u5e76\u884c\u73af\u5883\u6bd4\u589e\u52a0 rollout \u957f\u5ea6\u66f4\u6709\u6548\u3002"}}
{"id": "2506.03411", "pdf": "https://arxiv.org/pdf/2506.03411", "abs": "https://arxiv.org/abs/2506.03411", "authors": ["Melissa Dutz", "Han Shao", "Avrim Blum", "Aloni Cohen"], "title": "A Machine Learning Theory Perspective on Strategic Litigation", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "Strategic litigation involves bringing a legal case to court with the goal of\nhaving a broader impact beyond resolving the case itself: for example, creating\nprecedent which will influence future rulings. In this paper, we explore\nstrategic litigation from the perspective of machine learning theory. We\nconsider an abstract model of a common-law legal system where a lower court\ndecides new cases by applying a decision rule learned from a higher court's\npast rulings. In this model, we explore the power of a strategic litigator, who\nstrategically brings cases to the higher court to influence the learned\ndecision rule, thereby affecting future cases. We explore questions including:\nWhat impact can a strategic litigator have? Which cases should a strategic\nlitigator bring to court? Does it ever make sense for a strategic litigator to\nbring a case when they are sure the court will rule against them?", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u6218\u7565\u8bc9\u8bbc\u5728\u6cd5\u5f8b\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u4e86\u6218\u7565\u8bc9\u8bbc\u8005\u5982\u4f55\u901a\u8fc7\u9009\u62e9\u6848\u4ef6\u6765\u6539\u53d8\u672a\u6765\u88c1\u51b3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u6218\u7565\u8bc9\u8bbc\u5982\u4f55\u901a\u8fc7\u6cd5\u5f8b\u6848\u4f8b\u5bf9\u66f4\u5e7f\u6cdb\u7684\u6cd5\u5f8b\u4f53\u7cfb\u4ea7\u751f\u5f71\u54cd\uff0c\u5e76\u4ece\u673a\u5668\u5b66\u4e60\u7684\u89d2\u5ea6\u63d0\u4f9b\u65b0\u7684\u89c1\u89e3\u3002", "method": "\u8bba\u6587\u4f7f\u7528\u4e86\u673a\u5668\u5b66\u4e60\u7406\u8bba\u548c\u62bd\u8c61\u6a21\u578b\u6765\u6a21\u62df\u666e\u901a\u6cd5\u6cd5\u5f8b\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u6218\u7565\u8bc9\u8bbc\u8005\u7684\u7b56\u7565\u548c\u5f71\u54cd\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u6218\u7565\u8bc9\u8bbc\u8005\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u7279\u5b9a\u6848\u4ef6\u6765\u5851\u9020\u6cd5\u9662\u7684\u51b3\u7b56\u89c4\u5219\uff0c\u5373\u4f7f\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u77e5\u9053\u4f1a\u8f93\u6389\u4e2a\u522b\u6848\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u6574\u4f53\u5229\u76ca\u6700\u5927\u5316\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u6218\u7565\u8bc9\u8bbc\u8005\u901a\u8fc7\u7cbe\u5fc3\u6311\u9009\u6848\u4ef6\u53ef\u4ee5\u663e\u8457\u5f71\u54cd\u6cd5\u9662\u51b3\u7b56\u89c4\u5219\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u8fdb\u800c\u5f71\u54cd\u672a\u6765\u7684\u88c1\u51b3\u3002"}}
{"id": "2506.03426", "pdf": "https://arxiv.org/pdf/2506.03426", "abs": "https://arxiv.org/abs/2506.03426", "authors": ["Joonseong Kang", "Soojeong Lee", "Subeen Park", "Sumin Park", "Taero Kim", "Jihee Kim", "Ryunyi Lee", "Kyungwoo Song"], "title": "Adaptive Task Vectors for Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks without parameter updates by conditioning on a few demonstrations\nprovided in the prompt. Despite its success, ICL suffers from several\nlimitations, including sensitivity to demonstration order, context length\nconstraints, and computational inefficiency. To address these challenges, task\nvector-based approaches compress task information into a single vector.\nHowever, these methods typically construct task vectors from fixed sets of\ndemonstrations and reuse them across input queries, without conditioning on the\nspecific input. This limitation can lead models to struggle with effective\nadaptation when the input query is not well aligned with the underlying\ndemonstrations, consequently degrading their generalization performance on\nunseen tasks. To overcome this limitation, we propose Adaptive Task Vectors\n(ATV), a simple and effective framework that dynamically generates task vectors\nconditioned on each input query. ATV employs a small language model to generate\ntask vectors, which are then transformed to match the target LLM's architecture\nand applied to guide its output generation. In contrast to ICL and previous\nvector-based approaches, which rely on fixed demonstration sets and their\ncorresponding vectors, ATV dynamically generates task vectors tailored to each\nspecific input query and task. Consequently, ATV demonstrates strong\nperformance and generalization capabilities, even for unseen tasks.\nFurthermore, we provide a theoretical analysis indicating that ATV is\nexpressively equivalent to LoRA under equal rank budgets and more expressive\nthan Prefix-Tuning, thereby offering formal support for its representational\nadvantage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Adaptive Task Vectors (ATV)\uff0c\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6839\u636e\u6bcf\u4e2a\u8f93\u5165\u67e5\u8be2\u52a8\u6001\u751f\u6210\u4efb\u52a1\u5411\u91cf\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3ICL\u548c\u73b0\u6709\u5411\u91cf\u57fa\u7840\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u5bf9\u6f14\u793a\u987a\u5e8f\u654f\u611f\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u4ee5\u53ca\u5f53\u8f93\u5165\u67e5\u8be2\u4e0e\u5e95\u5c42\u6f14\u793a\u4e0d\u4e00\u81f4\u65f6\u9002\u5e94\u6548\u679c\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptive Task Vectors (ATV)\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u6bcf\u4e2a\u8f93\u5165\u67e5\u8be2\u52a8\u6001\u751f\u6210\u4efb\u52a1\u5411\u91cf\uff0c\u5e76\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u5411\u91cf\uff0c\u968f\u540e\u5c06\u5176\u8f6c\u6362\u4ee5\u5339\u914d\u76ee\u6807LLM\u67b6\u6784\u5e76\u6307\u5bfc\u5176\u8f93\u51fa\u751f\u6210\u3002", "result": "ATV\u5c55\u793a\u4e86\u5373\u4f7f\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u7684\u5f3a\u5927\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u8868\u660eATV\u5728\u8868\u793a\u80fd\u529b\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "ATV\u65b9\u6cd5\u76f8\u8f83\u4e8e\u4e4b\u524d\u7684\u5411\u91cf\u57fa\u7840\u65b9\u6cd5\u548cICL\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u66f4\u5f3a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u7406\u8bba\u4e0a\u4e0eLoRA\u5728\u76f8\u540c\u7b49\u7ea7\u9884\u7b97\u4e0b\u5177\u6709\u8868\u8fbe\u7b49\u4ef7\u6027\uff0c\u6bd4Prefix-Tuning\u66f4\u5177\u8868\u73b0\u529b\u3002"}}
{"id": "2506.03444", "pdf": "https://arxiv.org/pdf/2506.03444", "abs": "https://arxiv.org/abs/2506.03444", "authors": ["Yue Gong", "Raul Castro Fernandez"], "title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior", "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "As hypothesis generation becomes increasingly automated, a new bottleneck has\nemerged: hypothesis assessment. Modern systems can surface thousands of\nstatistical relationships-correlations, trends, causal links-but offer little\nguidance on which ones are novel, non-trivial, or worthy of expert attention.\nIn this work, we study the complementary problem to hypothesis generation:\nautomatic hypothesis assessment. Specifically, we ask: given a large set of\nstatistical relationships, can we automatically assess which ones are novel and\nworth further exploration? We focus on correlations as they are a common entry\npoint in exploratory data analysis that often serve as the basis for forming\ndeeper scientific or causal hypotheses.\n  To support automatic assessment, we propose to leverage the vast knowledge\nencoded in LLMs' weights to derive a prior distribution over the correlation\nvalue of a variable pair. If an LLM's prior expects the correlation value\nobserved, then such correlation is not surprising, and vice versa. We propose\nthe Logit-based Calibrated Prior, an LLM-elicited correlation prior that\ntransforms the model's raw output logits into a calibrated, continuous\npredictive distribution over correlation values. We evaluate the prior on a\nbenchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of\n78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of\n89.2% in predicting Pearson correlation coefficient. It also outperforms a\nfine-tuned RoBERTa classifier in binary correlation prediction and achieves\nhigher precision@K in hypothesis ranking. We further show that the prior\ngeneralizes to correlations not seen during LLM pretraining, reflecting\ncontext-sensitive reasoning rather than memorization.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u5316\u5047\u8bbe\u751f\u6210\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u2014\u2014\u5047\u8bbe\u8bc4\u4f30\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u548c\u8bc4\u4f30\u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u5316\u5047\u8bbe\u751f\u6210\u7684\u53d1\u5c55\uff0c\u5047\u8bbe\u8bc4\u4f30\u6210\u4e3a\u65b0\u7684\u74f6\u9888\u3002\u73b0\u4ee3\u7cfb\u7edf\u80fd\u591f\u63ed\u793a\u5927\u91cf\u7684\u7edf\u8ba1\u5173\u7cfb\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u54ea\u4e9b\u5173\u7cfb\u662f\u65b0\u9896\u3001\u975e\u663e\u800c\u6613\u89c1\u6216\u503c\u5f97\u4e13\u5bb6\u5173\u6ce8\u7684\u6307\u5bfc\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u5047\u8bbe\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLogit\u7684\u6821\u51c6\u5148\u9a8c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06LLM\u7684\u539f\u59cb\u8f93\u51falogits\u8f6c\u5316\u4e3a\u6821\u51c6\u540e\u7684\u8fde\u7eed\u9884\u6d4b\u5206\u5e03\uff0c\u7528\u4e8e\u8bc4\u4f30\u53d8\u91cf\u5bf9\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684LLM\u5f15\u5bfc\u7684\u76f8\u5173\u6027\u5148\u9a8c\u65b9\u6cd5\u57282096\u4e2a\u771f\u5b9e\u4e16\u754c\u53d8\u91cf\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff1a\u7b26\u53f7\u51c6\u786e\u7387\u8fbe\u523078.8%\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.26\uff0c95%\u53ef\u4fe1\u533a\u95f4\u8986\u76d6\u7387\u4e3a89.2%\u3002\u6b64\u5916\uff0c\u5b83\u5728\u4e8c\u5143\u76f8\u5173\u6027\u9884\u6d4b\u548c\u5047\u8bbe\u6392\u5e8f\u7684\u7cbe\u5ea6@K\u4e0a\u4f18\u4e8e\u5fae\u8c03\u7684RoBERTa\u5206\u7c7b\u5668\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\u53ef\u4ee5\u6709\u6548\u5730\u8fdb\u884c\u5047\u8bbe\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u76f8\u5173\u6027\u5206\u6790\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u9884\u6d4b\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4e14\u80fd\u591f\u63a8\u5e7f\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u672a\u89c1\u8fc7\u7684\u76f8\u5173\u6027\u60c5\u51b5\u3002"}}
{"id": "2506.03472", "pdf": "https://arxiv.org/pdf/2506.03472", "abs": "https://arxiv.org/abs/2506.03472", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Embeddings for MNIST", "categories": ["cs.LG", "20-XX, 08A02", "F.4.1; I.2"], "comment": null, "summary": "We present an empirical validation of the directional non-commutative\nmonoidal embedding framework recently introduced in prior\nwork~\\cite{Godavarti2025monoidal}. This framework defines learnable\ncompositional embeddings using distinct non-commutative operators per dimension\n(axis) that satisfy an interchange law, generalizing classical one-dimensional\ntransforms. Our primary goal is to verify that this framework can effectively\nmodel real data by applying it to a controlled, well-understood task: image\nclassification on the MNIST dataset~\\cite{lecun1998gradient}. A central\nhypothesis for why the proposed monoidal embedding works well is that it\ngeneralizes the Discrete Fourier Transform (DFT)~\\cite{oppenheim1999discrete}\nby learning task-specific frequency components instead of using fixed basis\nfrequencies. We test this hypothesis by comparing learned monoidal embeddings\nagainst fixed DFT-based embeddings on MNIST. The results show that as the\nembedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance\ngap between the learned monoidal embeddings and fixed DFT-based embeddings on\nMNIST grows increasingly large. This comparison is used as an analytic tool to\nexplain why the framework performs well: the learnable embeddings can capture\nthe most discriminative spectral components for the task. Overall, our\nexperiments confirm that directional non-commutative monoidal embeddings are\nhighly effective for representing image data, offering a compact learned\nrepresentation that retains high task performance. The code used in this work\nis available at\nhttps://github.com/mahesh-godavarti/directional_composition_mnist.", "AI": {"tldr": "\u8bba\u6587\u9a8c\u8bc1\u4e86\u65b9\u5411\u6027\u975e\u4ea4\u6362\u5e7a\u534a\u7fa4\u5d4c\u5165\u6846\u67b6\u5728MNIST\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5176\u4f18\u4e8e\u56fa\u5b9aDFT\u5d4c\u5165\uff0c\u5c24\u5176\u5728\u4f4e\u7ef4\u5d4c\u5165\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u9891\u7387\u5206\u91cf\u6765\u63a8\u5e7f\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\uff0c\u5047\u8bbe\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u4efb\u52a1\u7684\u5173\u952e\u8c31\u5206\u91cf\u3002", "method": "\u5c06\u975e\u4ea4\u6362\u5e7a\u534a\u7fa4\u5d4c\u5165\u6846\u67b6\u5e94\u7528\u4e8eMNIST\u6570\u636e\u96c6\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u4e0e\u56fa\u5b9aDFT\u5d4c\u5165\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u968f\u7740\u5d4c\u5165\u7ef4\u5ea6\u964d\u4f4e\uff0c\u5b66\u4e60\u5230\u7684\u5e7a\u534a\u7fa4\u5d4c\u5165\u4e0e\u56fa\u5b9aDFT\u5d4c\u5165\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u9010\u6e10\u589e\u5927\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u65b9\u5411\u6027\u975e\u4ea4\u6362\u5e7a\u534a\u7fa4\u5d4c\u5165\u5728\u56fe\u50cf\u6570\u636e\u8868\u793a\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u4e14\u4efb\u52a1\u6027\u80fd\u9ad8\u7684\u5b66\u4e60\u8868\u793a\u3002"}}
{"id": "2506.03474", "pdf": "https://arxiv.org/pdf/2506.03474", "abs": "https://arxiv.org/abs/2506.03474", "authors": ["Yifeng Xiao", "Yurong Xu", "Ning Yan", "Masood Mortazavi", "Pierluigi Nuzzo"], "title": "CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "I.2.6; C.3"], "comment": "Preprint. 10 pages + appendix. Submitted to NeurIPS 2025", "summary": "Simulation-based design space exploration (DSE) aims to efficiently optimize\nhigh-dimensional structured designs under complex constraints and expensive\nevaluation costs. Existing approaches, including heuristic and multi-step\nreinforcement learning (RL) methods, struggle to balance sampling efficiency\nand constraint satisfaction due to sparse, delayed feedback, and large hybrid\naction spaces. In this paper, we introduce CORE, a constraint-aware, one-step\nRL method for simulationguided DSE. In CORE, the policy agent learns to sample\ndesign configurations by defining a structured distribution over them,\nincorporating dependencies via a scaling-graph-based decoder, and by reward\nshaping to penalize invalid designs based on the feedback obtained from\nsimulation. CORE updates the policy using a surrogate objective that compares\nthe rewards of designs within a sampled batch, without learning a value\nfunction. This critic-free formulation enables efficient learning by\nencouraging the selection of higher-reward designs. We instantiate CORE for\nhardware-mapping co-design of neural network accelerators, demonstrating that\nit significantly improves sample efficiency and achieves better accelerator\nconfigurations compared to state-of-the-art baselines. Our approach is general\nand applicable to a broad class of discrete-continuous constrained design\nproblems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCORE\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u57fa\u4e8e\u4eff\u771f\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u901a\u8fc7\u7ea6\u675f\u611f\u77e5\u548c\u4e00\u6b65\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u5e76\u4f18\u5316\u8bbe\u8ba1\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u548c\u591a\u6b65\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u7ed3\u6784\u5316\u8bbe\u8ba1\u65f6\u96be\u4ee5\u5e73\u8861\u91c7\u6837\u6548\u7387\u4e0e\u7ea6\u675f\u6ee1\u8db3\uff0c\u56e0\u4e3a\u53cd\u9988\u7a00\u758f\u3001\u5ef6\u8fdf\u4e14\u52a8\u4f5c\u7a7a\u95f4\u5927\u3002", "method": "CORE\u65b9\u6cd5\u901a\u8fc7\u5b9a\u4e49\u8bbe\u8ba1\u914d\u7f6e\u7684\u7ed3\u6784\u5316\u5206\u5e03\u8fdb\u884c\u91c7\u6837\uff0c\u4f7f\u7528\u6269\u5c55\u56fe\u89e3\u7801\u5668\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u5851\u9020\u60e9\u7f5a\u65e0\u6548\u8bbe\u8ba1\uff0c\u5229\u7528\u4ee3\u7406\u76ee\u6807\u51fd\u6570\u6bd4\u8f83\u6279\u6b21\u5185\u8bbe\u8ba1\u5956\u52b1\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCORE\u5728\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u786c\u4ef6\u6620\u5c04\u534f\u540c\u8bbe\u8ba1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff0c\u5e76\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CORE\u662f\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u79bb\u6563-\u8fde\u7eed\u7ea6\u675f\u8bbe\u8ba1\u95ee\u9898\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2506.03588", "pdf": "https://arxiv.org/pdf/2506.03588", "abs": "https://arxiv.org/abs/2506.03588", "authors": ["Hiroki Shiraishi", "Hisao Ishibuchi", "Masaya Nakata"], "title": "A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "The decision-making process significantly influences the predictions of\nmachine learning models. This is especially important in rule-based systems\nsuch as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and\napplication of rules directly determine prediction accuracy and reliability.\nLFCSs combine evolutionary algorithms with supervised learning to optimize\nfuzzy classification rules, offering enhanced interpretability and robustness.\nDespite these advantages, research on improving decision-making mechanisms\n(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use\nvoting-based or single-winner-based inference schemes. These schemes rely on\nclassification performance on training data and may not perform well on unseen\ndata, risking overfitting. To address these limitations, this article\nintroduces a novel class inference scheme for LFCSs based on the\nDempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles\nuncertainty well. By using the DS theory, the scheme calculates belief masses\n(i.e., measures of belief) for each specific class and the ``I don't know''\nstate from each fuzzy rule and infers a class from these belief masses. Unlike\nthe conventional schemes, the proposed scheme also considers the ``I don't\nknow'' state that reflects uncertainty, thereby improving the transparency and\nreliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the\nproposed scheme demonstrates statistically significant improvements in terms of\ntest macro F1 scores across 30 real-world datasets compared to conventional\nvoting-based and single-winner-based fuzzy inference schemes. It forms smoother\ndecision boundaries, provides reliable confidence measures, and enhances the\nrobustness and generalizability of LFCSs in real-world applications. Our\nimplementation is available at https://github.com/YNU-NakataLab/jUCS.", "AI": {"tldr": "This paper proposes a new class inference scheme for Learning Fuzzy-Classifier Systems (LFCSs) based on Dempster-Shafer Theory of Evidence (DS theory), which significantly improves performance, robustness, and interpretability across 30 real-world datasets.", "motivation": "Improving decision-making mechanisms (class inference schemes) in Learning Fuzzy-Classifier Systems (LFCSs) is crucial due to their influence on prediction accuracy and reliability. Existing schemes like voting-based or single-winner-based methods risk overfitting and perform poorly on unseen data.", "method": "A novel class inference scheme using Dempster-Shafer Theory of Evidence (DS theory) was introduced and applied to a variant of LFCS (Fuzzy-UCS). Belief masses for each class and the ``I don't know'' state were calculated from fuzzy rules to infer classes.", "result": "The proposed scheme demonstrated statistically significant improvements in test macro F1 scores across 30 real-world datasets compared to conventional schemes. It formed smoother decision boundaries, provided reliable confidence measures, and enhanced robustness.", "conclusion": "The study concludes that the proposed class inference scheme based on DS theory improves transparency, reliability, robustness, and generalizability of LFCSs in real-world applications."}}
{"id": "2506.03531", "pdf": "https://arxiv.org/pdf/2506.03531", "abs": "https://arxiv.org/abs/2506.03531", "authors": ["Daniel Ovalle", "Lorenz T. Biegler", "Ignacio E. Grossmann", "Carl D. Laird", "Mateo Dulce Rubio"], "title": "Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel\nframework that provides probabilistic feasibility guarantees for data-driven\nconstraints in optimization problems. While standard Mixed-Integer Constraint\nLearning methods often violate the true constraints due to model error or data\nlimitations, our C-MICL approach leverages conformal prediction to ensure\nfeasible solutions are ground-truth feasible. This guarantee holds with\nprobability at least $1{-}\\alpha$, under a conditional independence assumption.\nThe proposed framework supports both regression and classification tasks\nwithout requiring access to the true constraint function, while avoiding the\nscalability issues associated with ensemble-based heuristics. Experiments on\nreal-world applications demonstrate that C-MICL consistently achieves target\nfeasibility rates, maintains competitive objective performance, and\nsignificantly reduces computational cost compared to existing methods. Our work\nbridges mathematical optimization and machine learning, offering a principled\napproach to incorporate uncertainty-aware constraints into decision-making with\nrigorous statistical guarantees.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6C-MICL\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4f18\u5316\u95ee\u9898\u4e2d\u4e3a\u6570\u636e\u9a71\u52a8\u7ea6\u675f\u63d0\u4f9b\u4e86\u6982\u7387\u53ef\u884c\u6027\u4fdd\u8bc1\u3002", "motivation": "\u6807\u51c6\u7684\u6df7\u5408\u6574\u6570\u7ea6\u675f\u5b66\u4e60\u65b9\u6cd5\u7531\u4e8e\u6a21\u578b\u8bef\u5dee\u6216\u6570\u636e\u9650\u5236\u7ecf\u5e38\u8fdd\u53cd\u771f\u6b63\u7684\u7ea6\u675f\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86C-MICL\u65b9\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConformal Mixed-Integer Constraint Learning (C-MICL)\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e86\u5171\u5f62\u9884\u6d4b\u6765\u786e\u4fdd\u89e3\u51b3\u65b9\u6848\u662f\u771f\u5b9e\u53ef\u884c\u7684\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cC-MICL\u6301\u7eed\u5b9e\u73b0\u4e86\u76ee\u6807\u53ef\u884c\u6027\u7387\uff0c\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u76ee\u6807\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0cC-MICL\u65b9\u6cd5\u5728\u6570\u5b66\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u7684\u65b9\u6cd5\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ea6\u675f\u7eb3\u5165\u51b3\u7b56\u4e2d\uff0c\u5e76\u5177\u5907\u4e25\u683c\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002"}}
{"id": "2506.03542", "pdf": "https://arxiv.org/pdf/2506.03542", "abs": "https://arxiv.org/abs/2506.03542", "authors": ["Yongxiang Tang", "Yanhua Cheng", "Xiaocheng Liu", "Chenchen Jiao", "Yanxiang Zeng", "Ning Luo", "Pengjia Yuan", "Xialong Liu", "Peng Jiang"], "title": "Learning Monotonic Probabilities with a Generative Cost Model", "categories": ["cs.LG"], "comment": null, "summary": "In many machine learning tasks, it is often necessary for the relationship\nbetween input and output variables to be monotonic, including both strictly\nmonotonic and implicitly monotonic relationships. Traditional methods for\nmaintaining monotonicity mainly rely on construction or regularization\ntechniques, whereas this paper shows that the issue of strict monotonic\nprobability can be viewed as a partial order between an observable revenue\nvariable and a latent cost variable. This perspective enables us to reformulate\nthe monotonicity challenge into modeling the latent cost variable. To tackle\nthis, we introduce a generative network for the latent cost variable, termed\nthe Generative Cost Model (GCM), which inherently addresses the strict\nmonotonic problem, and propose the Implicit Generative Cost Model (IGCM) to\naddress the implicit monotonic problem. We further validate our approach with a\nnumerical simulation of quantile regression and conduct multiple experiments on\npublic datasets, showing that our method significantly outperforms existing\nmonotonic modeling techniques. The code for our experiments can be found at\nhttps://github.com/tyxaaron/GCM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff08GCM\u548cIGCM\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e25\u683c\u548c\u9690\u5f0f\u5355\u8c03\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u8f93\u5165\u4e0e\u8f93\u51fa\u53d8\u91cf\u4e4b\u95f4\u7684\u5355\u8c03\u5173\u7cfb\uff08\u5305\u62ec\u4e25\u683c\u5355\u8c03\u548c\u9690\u5f0f\u5355\u8c03\uff09\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6784\u9020\u6216\u6b63\u5219\u5316\u6280\u672f\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u4e86\u751f\u6210\u6210\u672c\u6a21\u578b\uff08GCM\uff09\u548c\u9690\u5f0f\u751f\u6210\u6210\u672c\u6a21\u578b\uff08IGCM\uff09\u6765\u5904\u7406\u5355\u8c03\u6027\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u6570\u503c\u6a21\u62df\u548c\u516c\u5f00\u6570\u636e\u96c6\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684GCM\u548cIGCM\u65b9\u6cd5\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u8c03\u5efa\u6a21\u6280\u672f\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e25\u683c\u5355\u8c03\u548c\u9690\u5f0f\u5355\u8c03\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.03602", "pdf": "https://arxiv.org/pdf/2506.03602", "abs": "https://arxiv.org/abs/2506.03602", "authors": ["Hiroki Shiraishi", "Yohei Hayamizu", "Tomonori Hashiyama", "Keiki Takadama", "Hisao Ishibuchi", "Masaya Nakata"], "title": "Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Rule representations significantly influence the search capabilities and\ndecision boundaries within the search space of Learning Classifier Systems\n(LCSs), a family of rule-based machine learning systems that evolve\ninterpretable models through evolutionary processes. However, it is very\ndifficult to choose an appropriate rule representation for each problem.\nAdditionally, some problems benefit from using different representations for\ndifferent subspaces within the input space. Thus, an adaptive mechanism is\nneeded to choose an appropriate rule representation for each rule in LCSs. This\narticle introduces a flexible rule representation using a four-parameter beta\ndistribution and integrates it into a fuzzy-style LCS. The four-parameter beta\ndistribution can form various function shapes, and this flexibility enables our\nLCS to automatically select appropriate representations for different\nsubspaces. Our rule representation can represent crisp/fuzzy decision\nboundaries in various boundary shapes, such as rectangles and bells, by\ncontrolling four parameters, compared to the standard representations such as\ntrapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the\nappropriate rule representation for each subspace. Moreover, our LCS\nincorporates a generalization bias favoring crisp rules where feasible,\nenhancing model interpretability without compromising accuracy. Experimental\nresults on real-world classification tasks show that our LCS achieves\nsignificantly superior test accuracy and produces more compact rule sets. Our\nimplementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An\nextended abstract related to this work is available at\nhttps://doi.org/10.36227/techrxiv.174900805.59801248/v1.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u53c2\u6570Beta\u5206\u5e03\u7684\u65b0\u89c4\u5219\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u7cca\u98ce\u683c\u7684\u5b66\u4e60\u5206\u7c7b\u7cfb\u7edf\uff0c\u4ee5\u81ea\u52a8\u9009\u62e9\u9002\u5408\u4e0d\u540c\u8f93\u5165\u7a7a\u95f4\u5b50\u533a\u57df\u7684\u89c4\u5219\u8868\u793a\u5f62\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u89e3\u51b3\u5b66\u4e60\u5206\u7c7b\u7cfb\u7edf\uff08LCSs\uff09\u4e2d\u5982\u4f55\u9009\u62e9\u9002\u5f53\u7684\u89c4\u5219\u8868\u793a\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u67d0\u4e9b\u95ee\u9898\u9700\u8981\u9488\u5bf9\u8f93\u5165\u7a7a\u95f4\u7684\u4e0d\u540c\u5b50\u7a7a\u95f4\u91c7\u7528\u4e0d\u540c\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\u3002", "method": "\u8bba\u6587\u7684\u65b9\u6cd5\u662f\u5f15\u5165\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u89c4\u5219\u8868\u793a\u65b9\u6cd5\uff0c\u57fa\u4e8e\u56db\u53c2\u6570Beta\u5206\u5e03\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6a21\u7cca\u98ce\u683c\u7684LCS\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5f62\u6210\u5404\u79cd\u51fd\u6570\u5f62\u72b6\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u4e0d\u540c\u8f93\u5165\u7a7a\u95f4\u5b50\u533a\u57df\u7684\u9002\u5f53\u8868\u793a\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6d4b\u8bd5\u51c6\u786e\u6027\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u52a0\u7d27\u51d1\u7684\u89c4\u5219\u96c6\u3002", "conclusion": "\u8bba\u6587\u7684\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u4f7f\u7528\u56db\u53c2\u6570Beta\u5206\u5e03\u89c4\u5219\u8868\u793a\u548c\u6a21\u7cca\u98ce\u683cLCS\u7684\u6574\u5408\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u9009\u62e9\u9002\u5408\u4e0d\u540c\u5b50\u7a7a\u95f4\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u8d8a\u7684\u6d4b\u8bd5\u51c6\u786e\u6027\u548c\u66f4\u7d27\u51d1\u7684\u89c4\u5219\u96c6\u3002"}}
{"id": "2506.03556", "pdf": "https://arxiv.org/pdf/2506.03556", "abs": "https://arxiv.org/abs/2506.03556", "authors": ["Wang WeiQuan", "Riaz-ul-Haque Mian"], "title": "Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "In semiconductor manufacturing, testing costs remain significantly high,\nespecially during wafer and FPGA testing. To reduce the number of required\ntests while maintaining predictive accuracy, this study investigates three\nbaseline sampling strategies: Random Sampling, Stratified Sampling, and k-means\nClustering Sampling. To further enhance these methods, this study proposes a\nnovel algorithm that improves the sampling quality of each approach. This\nresearch is conducted using real industrial production data from wafer-level\ntests and silicon measurements from various FPGAs. This study introduces two\nhybrid strategies: Stratified with Short Distance Elimination (S-SDE) and\nk-means with Short Distance Elimination (K-SDE). Their performance is evaluated\nwithin the framework of Gaussian Process Regression (GPR) for predicting wafer\nand FPGA test data. At the core of our proposed approach is the Short Distance\nElimination (SDE) algorithm, which excludes spatially proximate candidate\npoints during sampling, thereby ensuring a more uniform distribution of\ntraining data across the physical domain. A parameter sweep was conducted over\nthe (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,\n3, 4} and not both zero, to identify the optimal combination that minimizes\nRMSD. Experimental results on a randomly selected wafer file reveal that\n(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent\nexperiments adopt this parameter configuration. The results demonstrate that\nthe proposed SDE-based strategies enhance predictive accuracy: K-SDE improves\nupon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while\nS-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84\npercent (FPGA).", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u6765\u6539\u8fdb\u968f\u673a\u62bd\u6837\u3001\u5206\u5c42\u62bd\u6837\u548ck\u5747\u503c\u805a\u7c7b\u62bd\u6837\u65b9\u6cd5\uff0c\u4ece\u800c\u51cf\u5c11\u6240\u9700\u7684\u6d4b\u8bd5\u6570\u91cf\u5e76\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u964d\u4f4e\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u6676\u5706\u548cFPGA\u6d4b\u8bd5\u7684\u6d4b\u8bd5\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5728\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6846\u67b6\u5185\u8bc4\u4f30 Stratified with Short Distance Elimination \u548c k-means with Short Distance Elimination \u7684\u6027\u80fd\uff0c\u4f7f\u7528\u5b9e\u9645\u5de5\u4e1a\u751f\u4ea7\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u5bf9(alpha, beta)\u9608\u503c\u8fdb\u884c\u4e86\u53c2\u6570\u626b\u63cf\u4ee5\u627e\u5230\u6700\u5c0fRMSD\u7684\u6700\u4f73\u7ec4\u5408\u3002", "result": "K-SDE\u5728\u6676\u5706\u548cFPGA\u91c7\u6837\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e8616.26%\u548c13.07%\uff0cS-SDE\u5219\u5206\u522b\u63d0\u9ad8\u4e8616.49%\u548c8.84%\u3002", "conclusion": "Stratified with Short Distance Elimination \u548c k-means with Short Distance Elimination \u8fd9\u4e24\u79cd\u6df7\u5408\u7b56\u7565\u80fd\u591f\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2506.03618", "pdf": "https://arxiv.org/pdf/2506.03618", "abs": "https://arxiv.org/abs/2506.03618", "authors": ["Jiayi Wan", "Xiang Zhu", "Fanzhen Liu", "Wei Fan", "Xiaolong Xu"], "title": "GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5dee\u5206\u9690\u79c1\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u68af\u5ea6\u6821\u6b63\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u52a8\u6001\u8c03\u6574\u6dfb\u52a0\u7684\u566a\u58f0\u6216\u4e22\u5f03\u67d0\u4e9b\u68af\u5ea6\u4ee5\u51cf\u8f7b\u7531\u5dee\u5206\u9690\u79c1\u5f15\u5165\u7684\u566a\u58f0\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6d88\u9664\u963b\u788d\u6536\u655b\u7684\u566a\u58f0\u5e76\u5bf9\u53d7\u566a\u58f0\u5f71\u54cd\u7684\u68af\u5ea6\u8fdb\u884c\u6821\u6b63\uff0c\u8fd9\u5927\u5927\u964d\u4f4e\u4e86\u6a21\u578b\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "method": "\u5728\u5ba2\u6237\u7aef\u6267\u884c\u68af\u5ea6\u88c1\u526a\u548c\u566a\u58f0\u6270\u52a8\u540e\uff0c\u8be5\u6846\u67b6\u68c0\u6d4b\u566a\u58f0\u5c40\u90e8\u68af\u5ea6\u4e2d\u7684\u504f\u5dee\uff0c\u5e76\u4f7f\u7528\u6295\u5f71\u673a\u5236\u5bf9\u5176\u8fdb\u884c\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u76f8\u540c\u7684\u9690\u79c1\u9884\u7b97\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5dee\u5206\u9690\u79c1\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u68af\u5ea6\u6821\u6b63\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.03758", "pdf": "https://arxiv.org/pdf/2506.03758", "abs": "https://arxiv.org/abs/2506.03758", "authors": ["Daniel Palenicek", "Florian Vogt", "Jan Peters"], "title": "Scaling CrossQ with Weight Normalization", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.07523", "summary": "Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics which are\nemphasized by higher UTDs, particularly Q-bias explosion and the growing\nmagnitude of critic network weights. To address this, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nprevents potential loss of plasticity and keeps the effective learning rate\nconstant. Our proposed approach reliably scales with increasing UTD ratios,\nachieving competitive or superior performance across a range of challenging\ntasks on the DeepMind control benchmark, notably the complex dog and humanoid\nenvironments. This work eliminates the need for drastic interventions, such as\nnetwork resets, and offers a robust pathway for improving sample efficiency and\nscalability in model-free reinforcement learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u96c6\u6210\u6743\u91cd\u5f52\u4e00\u5316\u6765\u63d0\u9ad8CrossQ\u6846\u67b6\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u53d6\u5f97\u4e86\u91cd\u5927\u91cc\u7a0b\u7891\uff0c\u4f46\u6837\u672c\u6548\u7387\u4ecd\u7136\u662f\u73b0\u5b9e\u5e94\u7528\u7684\u74f6\u9888\u3002\u6700\u8fd1\u7684CrossQ\u5c55\u793a\u4e86\u5177\u6709\u4f4e\u66f4\u65b0\u5230\u6570\u636e\u6bd4\u7684\u6700\u5148\u8fdb\u7684\u6837\u672c\u6548\u7387\uff0c\u4f46\u9700\u8981\u63a2\u7d22\u5176\u9ad8\u6bd4\u7387\u4e0b\u7684\u884c\u4e3a\u3002", "method": "\u5728CrossQ\u6846\u67b6\u4e2d\u96c6\u6210\u6743\u91cd\u5f52\u4e00\u5316\u4ee5\u89e3\u51b3Q-\u504f\u5dee\u7206\u70b8\u548c\u8bc4\u8bba\u7f51\u7edc\u6743\u91cd\u589e\u957f\u5e45\u5ea6\u7684\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u589e\u52a0\u7684UTD\u6bd4\u7387\u4e0b\u53ef\u9760\u5730\u6269\u5c55\uff0c\u5728DeepMind\u63a7\u5236\u57fa\u51c6\u7684\u4e00\u7cfb\u5217\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6216\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684CrossQ\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u6743\u91cd\u5f52\u4e00\u5316\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u6837\u672c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7684\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u8def\u5f84\u3002"}}
{"id": "2506.03590", "pdf": "https://arxiv.org/pdf/2506.03590", "abs": "https://arxiv.org/abs/2506.03590", "authors": ["Minh Luu", "Surya Jasper", "Khoi Le", "Evan Pan", "Michael Quinn", "Aakash Tyagi", "Jiang Hu"], "title": "VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration", "categories": ["cs.LG"], "comment": null, "summary": "Failure triage in design functional verification is critical but\ntime-intensive, relying on manual specification reviews, log inspections, and\nwaveform analyses. While machine learning (ML) has improved areas like stimulus\ngeneration and coverage closure, its application to RTL-level simulation\nfailure triage, particularly for large designs, remains limited. VCDiag offers\nan efficient, adaptable approach using VCD data to classify failing waveforms\nand pinpoint likely failure locations. In the largest experiment, VCDiag\nachieves over 94% accuracy in identifying the top three most likely modules.\nThe framework introduces a novel signal selection and statistical compression\napproach, achieving over 120x reduction in raw data size while preserving\nfeatures essential for classification. It can also be integrated into diverse\nVerilog/SystemVerilog designs and testbenches.", "AI": {"tldr": "VCDiag\u662f\u4e00\u79cd\u5229\u7528VCD\u6570\u636e\u8fdb\u884c\u9ad8\u6548\u9002\u5e94\u7684\u5931\u8d25\u6ce2\u5f62\u5206\u7c7b\u548c\u53ef\u80fd\u5931\u8d25\u4f4d\u7f6e\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u548c\u6570\u636e\u538b\u7f29\u6548\u679c\u3002", "motivation": "\u8bbe\u8ba1\u529f\u80fd\u9a8c\u8bc1\u4e2d\u7684\u6545\u969c\u6392\u9664\u662f\u4e00\u9879\u8017\u65f6\u7684\u5de5\u4f5c\uff0c\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4eba\u5de5\u89c4\u8303\u5ba1\u67e5\u3001\u65e5\u5fd7\u68c0\u67e5\u548c\u6ce2\u5f62\u5206\u6790\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u5728\u67d0\u4e9b\u9886\u57df\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5728RTL\u7ea7\u4eff\u771f\u5931\u8d25\u6392\u67e5\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002", "method": "\u4f7f\u7528VCD\u6570\u636e\u548c\u4e00\u79cd\u65b0\u7684\u4fe1\u53f7\u9009\u62e9\u4e0e\u7edf\u8ba1\u538b\u7f29\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5931\u8d25\u6ce2\u5f62\u5206\u7c7b\u548c\u53ef\u80fd\u5931\u8d25\u4f4d\u7f6e\u7684\u5b9a\u4f4d\u3002", "result": "VCDiag\u5728\u6700\u5927\u7684\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc794%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u8d85\u8fc7120\u500d\u7684\u539f\u59cb\u6570\u636e\u5927\u5c0f\u7684\u51cf\u5c11\u3002", "conclusion": "VCDiag\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8eRTL\u7ea7\u4eff\u771f\u5931\u8d25\u5206\u7c7b\uff0c\u5e76\u80fd\u51c6\u786e\u5730\u8bc6\u522b\u51fa\u6700\u6709\u53ef\u80fd\u5931\u8d25\u7684\u6a21\u5757\u3002"}}
{"id": "2506.03954", "pdf": "https://arxiv.org/pdf/2506.03954", "abs": "https://arxiv.org/abs/2506.03954", "authors": ["Jianqing Zhang", "Xinghao Wu", "Yanbing Zhou", "Xiaoting Sun", "Qiqi Cai", "Yang Liu", "Yang Hua", "Zhenzhe Zheng", "Jian Cao", "Qiang Yang"], "title": "HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Accepted by KDD2025", "summary": "As AI evolves, collaboration among heterogeneous models helps overcome data\nscarcity by enabling knowledge transfer across institutions and devices.\nTraditional Federated Learning (FL) only supports homogeneous models, limiting\ncollaboration among clients with heterogeneous model architectures. To address\nthis, Heterogeneous Federated Learning (HtFL) methods are developed to enable\ncollaboration across diverse heterogeneous models while tackling the data\nheterogeneity issue at the same time. However, a comprehensive benchmark for\nstandardized evaluation and analysis of the rapidly growing HtFL methods is\nlacking. Firstly, the highly varied datasets, model heterogeneity scenarios,\nand different method implementations become hurdles to making easy and fair\ncomparisons among HtFL methods. Secondly, the effectiveness and robustness of\nHtFL methods are under-explored in various scenarios, such as the medical\ndomain and sensor signal modality. To fill this gap, we introduce the first\nHeterogeneous Federated Learning Library (HtFLlib), an easy-to-use and\nextensible framework that integrates multiple datasets and model heterogeneity\nscenarios, offering a robust benchmark for research and practical applications.\nSpecifically, HtFLlib integrates (1) 12 datasets spanning various domains,\nmodalities, and data heterogeneity scenarios; (2) 40 model architectures,\nranging from small to large, across three modalities; (3) a modularized and\neasy-to-extend HtFL codebase with implementations of 10 representative HtFL\nmethods; and (4) systematic evaluations in terms of accuracy, convergence,\ncomputation costs, and communication costs. We emphasize the advantages and\npotential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze\nadvancing HtFL research and enable its broader applications. The code is\nreleased at https://github.com/TsingZ0/HtFLlib.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 HtFLlib\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u7684\u6613\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u3002", "motivation": "\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u5206\u6790\u5feb\u901f\u589e\u957f\u7684 HtFL \u65b9\u6cd5\u7684\u7efc\u5408\u6027\u57fa\u51c6\u3002", "method": "\u6574\u5408\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u5f02\u6784\u6027\u573a\u666f\uff0c\u5e76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u51c6\u786e\u6027\u3001\u6536\u655b\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u901a\u4fe1\u6210\u672c\u7b49\u65b9\u9762\u3002", "result": "\u5f15\u5165\u4e86\u9996\u4e2a\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u5e93 (HtFLlib)\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u6570\u636e\u96c6\u3001\u6a21\u578b\u67b6\u6784\u4ee5\u53ca\u4ee3\u8868\u6027 HtFL \u65b9\u6cd5\u7684\u5b9e\u73b0\u3002", "conclusion": "HtFLlib \u662f\u4e00\u4e2a\u5168\u9762\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4e3a HtFL \u65b9\u6cd5\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7a33\u5065\u57fa\u51c6\u3002"}}
{"id": "2506.03964", "pdf": "https://arxiv.org/pdf/2506.03964", "abs": "https://arxiv.org/abs/2506.03964", "authors": ["HyunGi Kim", "Jisoo Mok", "Dongjun Lee", "Jaihyun Lew", "Sungjae Kim", "Sungroh Yoon"], "title": "Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Utilizing the complex inter-variable causal relationships within multivariate\ntime-series provides a promising avenue toward more robust and reliable\nmultivariate time-series anomaly detection (MTSAD) but remains an underexplored\narea of research. This paper proposes Causality-Aware contrastive learning for\nRObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that\nincorporates the notion of causality into contrastive learning. CAROTS employs\ntwo data augmentors to obtain causality-preserving and -disturbing samples that\nserve as a wide range of normal variations and synthetic anomalies,\nrespectively. With causality-preserving and -disturbing samples as positives\nand negatives, CAROTS performs contrastive learning to train an encoder whose\nlatent space separates normal and abnormal samples based on causality.\nMoreover, CAROTS introduces a similarity-filtered one-class contrastive loss\nthat encourages the contrastive learning process to gradually incorporate more\nsemantically diverse samples with common causal relationships. Extensive\nexperiments on five real-world and two synthetic datasets validate that the\nintegration of causal relationships endows CAROTS with improved MTSAD\ncapabilities. The code is available at https://github.com/kimanki/CAROTS.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5CAROTS\uff0c\u5b83\u901a\u8fc7\u5c06\u56e0\u679c\u5173\u7cfb\u7eb3\u5165\u5bf9\u6bd4\u5b66\u4e60\u6765\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5229\u7528\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u590d\u6742\u7684\u53d8\u91cf\u95f4\u56e0\u679c\u5173\u7cfb\u53ef\u4ee5\u63d0\u4f9b\u66f4\u7a33\u5065\u53ef\u9760\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b(MTSAD)\uff0c\u4f46\u8fd9\u4e00\u9886\u57df\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAROTS\u7684\u65b0\u9896MTSAD\u6d41\u7a0b\uff0c\u4f7f\u7528\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u5668\u751f\u6210\u4fdd\u6301\u548c\u5e72\u6270\u56e0\u679c\u5173\u7cfb\u7684\u6837\u672c\u6765\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u4e86\u76f8\u4f3c\u5ea6\u8fc7\u6ee4\u7684\u4e00\u7c7b\u5bf9\u6bd4\u635f\u5931\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u4e24\u4e2a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u56e0\u679c\u5173\u7cfb\u6574\u5408\u8d4b\u4e88CAROTS\u6539\u8fdb\u7684MTSAD\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u901a\u8fc7\u5c06\u56e0\u679c\u5173\u7cfb\u7eb3\u5165\u5bf9\u6bd4\u5b66\u4e60\uff0cCAROTS\u65b9\u6cd5\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u80fd\u529b\u3002"}}
{"id": "2506.04001", "pdf": "https://arxiv.org/pdf/2506.04001", "abs": "https://arxiv.org/abs/2506.04001", "authors": ["Han Ji", "Yuqi Feng", "Jiahao Fan", "Yanan Sun"], "title": "CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Performance predictors have emerged as a promising method to accelerate the\nevaluation stage of neural architecture search (NAS). These predictors estimate\nthe performance of unseen architectures by learning from the correlation\nbetween a small set of trained architectures and their performance. However,\nmost existing predictors ignore the inherent distribution shift between limited\ntraining samples and diverse test samples. Hence, they tend to learn spurious\ncorrelations as shortcuts to predictions, leading to poor generalization. To\naddress this, we propose a Causality-guided Architecture Representation\nLearning (CARL) method aiming to separate critical (causal) and redundant\n(non-causal) features of architectures for generalizable architecture\nperformance prediction. Specifically, we employ a substructure extractor to\nsplit the input architecture into critical and redundant substructures in the\nlatent space. Then, we generate multiple interventional samples by pairing\ncritical representations with diverse redundant representations to prioritize\ncritical features. Extensive experiments on five NAS search spaces demonstrate\nthe state-of-the-art accuracy and superior interpretability of CARL. For\ninstance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.", "AI": {"tldr": "CARL\u65b9\u6cd5\u901a\u8fc7\u5173\u6ce8\u5173\u952e\u67b6\u6784\u7279\u5f81\uff0c\u6539\u8fdb\u4e86\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u6027\u80fd\u9884\u6d4b\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u7387\u548c\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u5668\u5ffd\u7565\u8bad\u7ec3\u6837\u672c\u548c\u6d4b\u8bd5\u6837\u672c\u4e4b\u95f4\u7684\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u56e0\u679c\u5173\u7cfb\u5f15\u5bfc\u7684\u65b9\u6cd5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5b50\u7ed3\u6784\u63d0\u53d6\u5668\u5c06\u8f93\u5165\u67b6\u6784\u5206\u4e3a\u5173\u952e\u548c\u5197\u4f59\u5b50\u7ed3\u6784\uff0c\u5e76\u751f\u6210\u591a\u4e2a\u5e72\u9884\u6837\u672c\u4ee5\u4f18\u5148\u8003\u8651\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2aNAS\u641c\u7d22\u7a7a\u95f4\u7684\u5b9e\u9a8c\u4e2d\uff0cCARL\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u4f18\u8d8a\u7684\u53ef\u89e3\u91ca\u6027\u3002\u4f8b\u5982\uff0c\u5728CIFAR-10\u4e0a\u5b9e\u73b0\u4e8697.67%\u7684\u4e00\u81f4\u51c6\u786e\u7387\u3002", "conclusion": "CARL\u65b9\u6cd5\u901a\u8fc7\u5206\u79bb\u67b6\u6784\u4e2d\u7684\u5173\u952e\u548c\u5197\u4f59\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u6027\u80fd\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.03674", "pdf": "https://arxiv.org/pdf/2506.03674", "abs": "https://arxiv.org/abs/2506.03674", "authors": ["Yidi Wang", "Jiawei Gu", "pei Xiaobing", "Xubin Zheng", "Xiao Luo", "Pengyang Wang", "Ziyue Qiao"], "title": "Out-of-Distribution Graph Models Merging", "categories": ["cs.LG"], "comment": null, "summary": "This paper studies a novel problem of out-of-distribution graph models\nmerging, which aims to construct a generalized model from multiple graph models\npre-trained on different domains with distribution discrepancy. This problem is\nchallenging because of the difficulty in learning domain-invariant knowledge\nimplicitly in model parameters and consolidating expertise from potentially\nheterogeneous GNN backbones. In this work, we propose a graph generation\nstrategy that instantiates the mixture distribution of multiple domains. Then,\nwe merge and fine-tune the pre-trained graph models via a MoE module and a\nmasking mechanism for generalized adaptation. Our framework is\narchitecture-agnostic and can operate without any source/target domain data.\nBoth theoretical analysis and experimental results demonstrate the\neffectiveness of our approach in addressing the model generalization problem.", "AI": {"tldr": "This paper introduces a novel framework for merging out-of-distribution graph models by using a graph generation strategy, a MoE module, and a masking mechanism, which operates without source/target domain data.", "motivation": "The motivation is to construct a generalized model from multiple graph models trained on different domains with distribution discrepancies, which is challenging due to the difficulty in learning domain-invariant knowledge and consolidating expertise from heterogeneous GNN backbones.", "method": "The paper proposes a graph generation strategy combined with a MoE module and a masking mechanism to merge and fine-tune pre-trained graph models.", "result": "Both theoretical analysis and experimental results show the effectiveness of the approach in addressing the model generalization problem.", "conclusion": "The paper concludes that their proposed framework effectively addresses the model generalization problem in out-of-distribution graph models merging."}}
{"id": "2506.04088", "pdf": "https://arxiv.org/pdf/2506.04088", "abs": "https://arxiv.org/abs/2506.04088", "authors": ["Jun-Peng Jiang", "Yu Xia", "Hai-Long Sun", "Shiyin Lu", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Turbo\u6846\u67b6\uff0c\u5229\u7528\u8bad\u7ec3\u671f\u95f4\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u53d6\u5f97\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8868\u683c\u6570\u636e\u65f6\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u8868\u793a\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8868\u683c\u901a\u5e38\u4ee5\u56fe\u50cf\u5f62\u5f0f\u5b58\u5728\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u6587\u672c\u8868\u793a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u8868\u683c\u56fe\u50cf\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u8868\u683c\u63a8\u7406\u6846\u67b6Turbo\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8eDeepSeek-R1\u7684\u7ed3\u6784\u611f\u77e5\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u5668\uff0c\u5e76\u901a\u8fc7\u91cd\u590d\u751f\u6210\u548c\u9009\u62e9\u4f18\u52bf\u63a8\u7406\u8def\u5f84\u6765\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6570\u636e\u91cf\u4ec5\u4e3a9k\u7684\u60c5\u51b5\u4e0b\uff0cTurbo\u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u8f83\u4e4b\u524d\u7684\u6700\u4f73\u7ed3\u679c\u63d0\u5347\u4e867.2%\u3002", "conclusion": "Turbo\u6846\u67b6\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u671f\u95f4\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5373\u4f7f\u6570\u636e\u91cf\u6709\u9650\u4e5f\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.03696", "pdf": "https://arxiv.org/pdf/2506.03696", "abs": "https://arxiv.org/abs/2506.03696", "authors": ["Fang Wang", "Paolo Ceravolo", "Ernesto Damiani"], "title": "Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring", "categories": ["cs.LG"], "comment": null, "summary": "Predictive Business Process Monitoring (PBPM) aims to forecast future\noutcomes of ongoing business processes. However, existing methods often lack\nflexibility to handle real-world challenges such as simultaneous events, class\nimbalance, and multi-level attributes. While prior work has explored static\nencoding schemes and fixed LSTM architectures, they struggle to support\nadaptive representations and generalize across heterogeneous datasets. To\naddress these limitations, we propose a suite of dynamic LSTM HyperModels that\nintegrate two-level hierarchical encoding for event and sequence attributes,\ncharacter-based decomposition of event labels, and novel pseudo-embedding\ntechniques for durations and attribute correlations. We further introduce\nspecialized LSTM variants for simultaneous event modeling, leveraging\nmultidimensional embeddings and time-difference flag augmentation. Experimental\nvalidation on four public and real-world datasets demonstrates up to 100%\naccuracy on balanced datasets and F1 scores exceeding 86\\% on imbalanced ones.\nOur approach advances PBPM by offering modular and interpretable models better\nsuited for deployment in complex settings. Beyond PBPM, it contributes to the\nbroader AI community by improving temporal outcome prediction, supporting data\nheterogeneity, and promoting explainable process intelligence frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u6d4b\u6027\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001LSTM HyperModels\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u3001\u81ea\u9002\u5e94\u8868\u793a\u548c\u5f02\u6784\u6570\u636e\u6cdb\u5316\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u6d4b\u6027\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\u65b9\u6cd5\u7f3a\u4e4f\u5e94\u5bf9\u73b0\u5b9e\u6311\u6218\u7684\u7075\u6d3b\u6027\uff0c\u4f8b\u5982\u5e76\u53d1\u4e8b\u4ef6\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u591a\u7ea7\u5c5e\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u81ea\u9002\u5e94\u8868\u793a\u5e76\u5728\u5f02\u6784\u6570\u636e\u96c6\u4e2d\u63a8\u5e7f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u96c6\u6210\u53cc\u5c42\u5c42\u6b21\u7f16\u7801\u3001\u4e8b\u4ef6\u6807\u7b7e\u7684\u5b57\u7b26\u7ea7\u5206\u89e3\u4ee5\u53ca\u6301\u7eed\u65f6\u95f4\u548c\u5c5e\u6027\u76f8\u5173\u6027\u7684\u4f2a\u5d4c\u5165\u6280\u672f\u7684\u52a8\u6001LSTM HyperModels\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u7528\u4e8e\u540c\u65f6\u5efa\u6a21\u5e76\u53d1\u4e8b\u4ef6\u7684\u591a\u7ef4\u5d4c\u5165\u548c\u65f6\u95f4\u5dee\u6807\u5fd7\u589e\u5f3a\u7684LSTM\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u56db\u4e2a\u516c\u5f00\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86100%\u7684\u51c6\u786e\u7387\uff0c\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684F1\u5206\u6570\u8d85\u8fc7\u4e8686%\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001LSTM HyperModels\u5957\u4ef6\uff0c\u4ee5\u89e3\u51b3\u9884\u6d4b\u6027\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\uff08PBPM\uff09\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u4e0d\u4ec5\u63a8\u52a8\u4e86PBPM\u7684\u53d1\u5c55\uff0c\u8fd8\u4e3a\u66f4\u5e7f\u6cdb\u7684AI\u793e\u533a\u5728\u65f6\u95f4\u7ed3\u679c\u9884\u6d4b\u3001\u6570\u636e\u5f02\u6784\u6027\u652f\u6301\u548c\u53ef\u89e3\u91ca\u8fc7\u7a0b\u667a\u80fd\u6846\u67b6\u65b9\u9762\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2506.04089", "pdf": "https://arxiv.org/pdf/2506.04089", "abs": "https://arxiv.org/abs/2506.04089", "authors": ["Anastasiia Ivanova", "Eva Bakaeva", "Zoya Volovikova", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": "ACL 2025 (Main Conference)", "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u540d\u4e3aAmbiK\u7684\u65b0\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4e3a\u53a8\u623f\u73af\u5883\u4e2d\u673a\u5668\u4eba\u63a5\u6536\u5230\u7684\u6a21\u7cca\u6307\u4ee4\u63d0\u4f9b\u7edf\u4e00\u7684\u6a21\u7cca\u6027\u68c0\u6d4b\u65b9\u6cd5\u6bd4\u8f83\u57fa\u51c6\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u4efb\u52a1\u6a21\u7cca\u6027\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e4b\u95f4\u8fdb\u884c\u6bd4\u8f83\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u901a\u7528\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7LLM\u8f85\u52a9\u6536\u96c6\u548c\u4eba\u7c7b\u9a8c\u8bc1\u7684\u65b9\u5f0f\u6784\u5efa\u4e86\u5305\u542b1000\u5bf9\u6a21\u7cca\u4efb\u52a1\u53ca\u5176\u975e\u6a21\u7cca\u5bf9\u5e94\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6309\u6a21\u7cca\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2000\u4e2a\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6db5\u76d6\u591a\u79cd\u6a21\u7cca\u7c7b\u578b\u4ee5\u53ca\u73af\u5883\u63cf\u8ff0\u3001\u6f84\u6e05\u95ee\u9898\u3001\u7528\u6237\u610f\u56fe\u548c\u4efb\u52a1\u8ba1\u5212\u7b49\u4fe1\u606f\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u3001\u5b8c\u5168\u6587\u672c\u7684\u6a21\u7cca\u4efb\u52a1\u6570\u636e\u96c6AmbiK\uff0c\u7528\u4e8e\u5e2e\u52a9\u7814\u7a76\u8005\u7edf\u4e00\u6bd4\u8f83\u6a21\u7cca\u6027\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2506.03703", "pdf": "https://arxiv.org/pdf/2506.03703", "abs": "https://arxiv.org/abs/2506.03703", "authors": ["Xiansheng Cai", "Sihan Hu", "Tao Wang", "Yuan Huang", "Pan Zhang", "Youjin Deng", "Kun Chen"], "title": "Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cond-mat.str-el", "physics.comp-ph"], "comment": null, "summary": "Fundamental physics often confronts complex symbolic problems with few\nguiding exemplars or established principles. While artificial intelligence (AI)\noffers promise, its typical need for vast datasets to learn from hinders its\nuse in these information-scarce frontiers. We introduce learning at criticality\n(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models\n(LLMs) to a sharp learning transition, addressing this information scarcity. At\nthis transition, LLMs achieve peak generalization from minimal data,\nexemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic\nreasoning. To elucidate this peak, we analyze a minimal concept-network model\n(CoNet) designed to capture the essence of how LLMs might link tokens. Trained\non a single exemplar, this model also undergoes a sharp learning transition.\nThis transition exhibits hallmarks of a second-order phase transition, notably\npower-law distributed solution path lengths. At this critical point, the system\nmaximizes a ``critical thinking pattern\" crucial for generalization, enabled by\nthe underlying scale-free exploration. This suggests LLMs reach peak\nperformance by operating at criticality, where such explorative dynamics enable\nthe extraction of underlying operational rules. We demonstrate LaC in quantum\nfield theory: an 8B-parameter LLM, tuned to its critical point by LaC using a\nfew exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,\nsignificantly outperforming far larger models. LaC thus leverages critical\nphenomena, a physical principle, to empower AI for complex, data-sparse\nchallenges in fundamental physics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faLaC\u65b9\u6cd5\uff0c\u5229\u7528\u4e34\u754c\u73b0\u8c61\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6781\u5c0f\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u57fa\u7840\u7269\u7406\u5b66\u4e2d\u590d\u6742\u4e14\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u57fa\u7840\u7269\u7406\u5b66\u5e38\u5e38\u9762\u4e34\u4fe1\u606f\u7a00\u7f3a\u7684\u590d\u6742\u7b26\u53f7\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u4eba\u5de5\u667a\u80fd\u56e0\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff08LaC\uff09\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8c03\u6574\u5230\u4e34\u754c\u70b9\uff0c\u5e76\u5206\u6790\u4e86\u4e00\u4e2a\u6982\u5ff5\u7f51\u7edc\u6a21\u578b\uff08CoNet\uff09\u6765\u63ed\u793a\u5b66\u4e60\u8dc3\u8fc1\u7684\u672c\u8d28\u3002", "result": "\u5728\u5c11\u91cf\u793a\u4f8b\u8bad\u7ec3\u4e0b\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u4ece\u7b80\u5355\u6570\u636e\u7684\u6700\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u91cf\u5b50\u573a\u7406\u8bba\u4e2d\u7684\u7b26\u53f7Matsubara\u6c42\u548c\u95ee\u9898\u3002", "conclusion": "LaC\u901a\u8fc7\u5229\u7528\u4e34\u754c\u73b0\u8c61\u8fd9\u4e00\u7269\u7406\u539f\u7406\uff0c\u4f7fLLMs\u5728\u6570\u636e\u7a00\u7f3a\u7684\u590d\u6742\u95ee\u9898\u4e2d\u8fbe\u5230\u5cf0\u503c\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u57fa\u7840\u7269\u7406\u5b66\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.04168", "pdf": "https://arxiv.org/pdf/2506.04168", "abs": "https://arxiv.org/abs/2506.04168", "authors": ["Seohong Park", "Kevin Frans", "Deepinder Mann", "Benjamin Eysenbach", "Aviral Kumar", "Sergey Levine"], "title": "Horizon Reduction Makes RL Scalable", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we study the scalability of offline reinforcement learning (RL)\nalgorithms. In principle, a truly scalable offline RL algorithm should be able\nto solve any given problem, regardless of its complexity, given sufficient\ndata, compute, and model capacity. We investigate if and how current offline RL\nalgorithms match up to this promise on diverse, challenging, previously\nunsolved tasks, using datasets up to 1000x larger than typical offline RL\ndatasets. We observe that despite scaling up data, many existing offline RL\nalgorithms exhibit poor scaling behavior, saturating well below the maximum\nperformance. We hypothesize that the horizon is the main cause behind the poor\nscaling of offline RL. We empirically verify this hypothesis through several\nanalysis experiments, showing that long horizons indeed present a fundamental\nbarrier to scaling up offline RL. We then show that various horizon reduction\ntechniques substantially enhance scalability on challenging tasks. Based on our\ninsights, we also introduce a minimal yet scalable method named SHARSA that\neffectively reduces the horizon. SHARSA achieves the best asymptotic\nperformance and scaling behavior among our evaluation methods, showing that\nexplicitly reducing the horizon unlocks the scalability of offline RL. Code:\nhttps://github.com/seohongpark/horizon-reduction", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\uff0c\u53d1\u73b0\u5c3d\u7ba1\u6570\u636e\u589e\u52a0\uff0c\u8bb8\u591a\u73b0\u6709\u7b97\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u957f\u89c6\u91ce\u662f\u9650\u5236\u53ef\u6269\u5c55\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5SHARSA\u6765\u51cf\u5c11\u89c6\u91ce\u5e76\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u771f\u6b63\u7684\u53ef\u6269\u5c55\u79bb\u7ebfRL\uff0c\u7b97\u6cd5\u5e94\u80fd\u5728\u63d0\u4f9b\u8db3\u591f\u6570\u636e\u3001\u8ba1\u7b97\u80fd\u529b\u548c\u6a21\u578b\u5bb9\u91cf\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u4efb\u4f55\u95ee\u9898\u3002\u5f53\u524d\u7684\u7b97\u6cd5\u662f\u5426\u80fd\u591f\u8fbe\u5230\u8fd9\u4e00\u76ee\u6807\u4ecd\u4e0d\u6e05\u695a\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u6bd4\u5178\u578b\u79bb\u7ebfRL\u6570\u636e\u96c6\u59271000\u500d\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u73b0\u6709\u79bb\u7ebfRL\u7b97\u6cd5\u5728\u591a\u6837\u5316\u3001\u5177\u6709\u6311\u6218\u6027\u4e14\u6b64\u524d\u672a\u89e3\u51b3\u7684\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5176\u6269\u5c55\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86SHARSA\u65b9\u6cd5\u4ee5\u51cf\u5c11\u89c6\u91ce\u5e76\u63d0\u5347\u6269\u5c55\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bb8\u591a\u73b0\u6709\u79bb\u7ebfRL\u7b97\u6cd5\u5728\u6269\u5c55\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u6027\u80fd\u8fdc\u4f4e\u4e8e\u6700\u4f18\u6c34\u5e73\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u957f\u89c6\u91ce\u662f\u5f71\u54cd\u79bb\u7ebfRL\u6269\u5c55\u6027\u7684\u6839\u672c\u969c\u788d\uff0c\u800c\u5404\u79cd\u89c6\u91ce\u7f29\u51cf\u6280\u672f\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6269\u5c55\u6027\u3002SHARSA\u5728\u6240\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u7684\u6e10\u8fd1\u6027\u80fd\u548c\u6269\u5c55\u884c\u4e3a\u3002", "conclusion": "\u663e\u5f0f\u51cf\u5c11\u89c6\u91ce\u80fd\u591f\u89e3\u9501\u79bb\u7ebfRL\u7684\u53ef\u6269\u5c55\u6027\uff0cSHARSA\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.04171", "pdf": "https://arxiv.org/pdf/2506.04171", "abs": "https://arxiv.org/abs/2506.04171", "authors": ["Utkarsh Utkarsh", "Pengfei Cai", "Alan Edelman", "Rafael Gomez-Bombarelli", "Christopher Vincent Rackauckas"], "title": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "27 pages, 9 figures, 4 tables", "summary": "Deep generative models have recently been applied to physical systems\ngoverned by partial differential equations (PDEs), offering scalable simulation\nand uncertainty-aware inference. However, enforcing physical constraints, such\nas conservation laws (linear and nonlinear) and physical consistencies, remains\nchallenging. Existing methods often rely on soft penalties or architectural\nbiases that fail to guarantee hard constraints. In this work, we propose\nPhysics-Constrained Flow Matching (PCFM), a zero-shot inference framework that\nenforces arbitrary nonlinear constraints in pretrained flow-based generative\nmodels. PCFM continuously guides the sampling process through physics-based\ncorrections applied to intermediate solution states, while remaining aligned\nwith the learned flow and satisfying physical constraints. Empirically, PCFM\noutperforms both unconstrained and constrained baselines on a range of PDEs,\nincluding those with shocks, discontinuities, and sharp features, while\nensuring exact constraint satisfaction at the final solution. Our method\nprovides a general framework for enforcing hard constraints in both scientific\nand general-purpose generative models, especially in applications where\nconstraint satisfaction is essential.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCFM\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e2d\u6709\u6548\u5b9e\u65bd\u7269\u7406\u7ea6\u675f\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e0d\u80fd\u4fdd\u8bc1\u786c\u7ea6\u675f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u4e8e\u8f6f\u60e9\u7f5a\u6216\u67b6\u6784\u504f\u5dee\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u786c\u7ea6\u675f\uff0c\u800c\u7269\u7406\u7ea6\u675f\uff08\u5982\u5b88\u6052\u5b9a\u5f8b\u548c\u7269\u7406\u4e00\u81f4\u6027\uff09\u7684\u4e25\u683c\u6267\u884c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhysics-Constrained Flow Matching (PCFM)\u7684\u96f6\u6837\u672c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u7684\u4fee\u6b63\u6301\u7eed\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5b66\u4e60\u6d41\u7684\u4e00\u81f4\u6027\u5e76\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u3002", "result": "PCFM\u5728\u4e00\u7cfb\u5217\u504f\u5fae\u5206\u65b9\u7a0b\uff08\u5305\u62ec\u5177\u6709\u51b2\u51fb\u6ce2\u3001\u4e0d\u8fde\u7eed\u6027\u548c\u5c16\u9510\u7279\u5f81\u7684\u95ee\u9898\uff09\u4e0a\u4f18\u4e8e\u65e0\u7ea6\u675f\u548c\u6709\u7ea6\u675f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u786e\u4fdd\u6700\u7ec8\u89e3\u7684\u7cbe\u786e\u7ea6\u675f\u6ee1\u8db3\u3002", "conclusion": "PCFM\u65b9\u6cd5\u4e3a\u5728\u79d1\u5b66\u548c\u901a\u7528\u751f\u6210\u6a21\u578b\u4e2d\u6267\u884c\u786c\u7ea6\u675f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u4e2d\u3002"}}
{"id": "2506.03725", "pdf": "https://arxiv.org/pdf/2506.03725", "abs": "https://arxiv.org/abs/2506.03725", "authors": ["Daniil Medyakov", "Sergey Stanko", "Gleb Molodtsov", "Philip Zmushko", "Grigoriy Evseev", "Egor Petrov", "Aleksandr Beznosikov"], "title": "Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization", "categories": ["cs.LG", "math.OC"], "comment": "58 pages, 5 figures, 5 tables", "summary": "Quite recently, large language models have made a significant breakthrough\nacross various disciplines. However, training them is an extremely\nresource-intensive task, even for major players with vast computing resources.\nOne of the methods gaining popularity in light of these challenges is Sign-SGD.\nThis method can be applied both as a memory-efficient approach in single-node\ntraining and as a gradient compression technique in the distributed learning.\nNevertheless, it is impossible to automatically determine the effective\nstepsize from the theoretical standpoint. Indeed, it depends on the parameters\nof the dataset to which we do not have access in the real-world learning\nparadigm. To address this issue, we design several variants of single-node\ndeterministic Sign-SGD. We extend our approaches to practical scenarios:\nstochastic single-node and multi-node learning, methods with incorporated\nmomentum. We conduct extensive experiments on real machine learning problems\nthat emphasize the practical applicability of our ideas.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u65e8\u5728\u89e3\u51b3Sign-SGD\u7b97\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u96be\u4ee5\u786e\u5b9a\u6709\u6548\u6b65\u957f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u51e0\u79cd\u5355\u8282\u70b9\u786e\u5b9a\u6027Sign-SGD\u53d8\u4f53\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u548c\u95ee\u9898\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u4e00\u9879\u6781\u5176\u8017\u8d39\u8d44\u6e90\u7684\u4efb\u52a1\uff0cSign-SGD\u4f5c\u4e3a\u4e00\u79cd\u5185\u5b58\u6548\u7387\u9ad8\u4e14\u53ef\u4f5c\u4e3a\u5206\u5e03\u5f0f\u5b66\u4e60\u68af\u5ea6\u538b\u7f29\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u4f46\u7406\u8bba\u4e0a\u65e0\u6cd5\u81ea\u52a8\u786e\u5b9a\u6709\u6548\u7684\u6b65\u957f\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e9b\u53d8\u79cd\u7684Sign-SGD\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u5b66\u4e60\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u51e0\u79cd\u5355\u8282\u70b9\u786e\u5b9a\u6027Sign-SGD\u53d8\u4f53\uff0c\u5e76\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u5982\u968f\u673a\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u5b66\u4e60\u4ee5\u53ca\u5e26\u6709\u52a8\u91cf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.04195", "pdf": "https://arxiv.org/pdf/2506.04195", "abs": "https://arxiv.org/abs/2506.04195", "authors": ["Elena Zamaraeva", "Christopher M. Collins", "George R. Darling", "Matthew S. Dyer", "Bei Peng", "Rahul Savani", "Dmytro Antypov", "Vladimir V. Gusev", "Judith Clymo", "Paul G. Spirakis", "Matthew J. Rosseinsky"], "title": "MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.11"], "comment": null, "summary": "Geometry optimization of atomic structures is a common and crucial task in\ncomputational chemistry and materials design. Following the learning to\noptimize paradigm, we propose a new multi-agent reinforcement learning method\ncalled Multi-Agent Crystal Structure optimization (MACS) to address periodic\ncrystal structure optimization. MACS treats geometry optimization as a\npartially observable Markov game in which atoms are agents that adjust their\npositions to collectively discover a stable configuration. We train MACS across\nvarious compositions of reported crystalline materials to obtain a policy that\nsuccessfully optimizes structures from the training compositions as well as\nstructures of larger sizes and unseen compositions, confirming its excellent\nscalability and zero-shot transferability. We benchmark our approach against a\nbroad range of state-of-the-art optimization methods and demonstrate that MACS\noptimizes periodic crystal structures significantly faster, with fewer energy\ncalculations, and the lowest failure rate.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMACS\u7684\u65b0\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5468\u671f\u6676\u4f53\u7ed3\u6784\u4f18\u5316\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6f5c\u529b\u3002", "motivation": "\u51e0\u4f55\u4f18\u5316\u662f\u8ba1\u7b97\u5316\u5b66\u548c\u6750\u6599\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5468\u671f\u6027\u6676\u4f53\u7ed3\u6784\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5MACS\uff0c\u5c06\u51e0\u4f55\u4f18\u5316\u89c6\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u3002", "result": "MACS\u5728\u4f18\u5316\u901f\u5ea6\u3001\u80fd\u91cf\u8ba1\u7b97\u6b21\u6570\u548c\u5931\u8d25\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "MACS\u65b9\u6cd5\u5728\u5468\u671f\u6676\u4f53\u7ed3\u6784\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2506.03757", "pdf": "https://arxiv.org/pdf/2506.03757", "abs": "https://arxiv.org/abs/2506.03757", "authors": ["Razvan-Andrei Lascu", "David \u0160i\u0161ka", "\u0141ukasz Szpruch"], "title": "PPO in the Fisher-Rao geometry", "categories": ["cs.LG", "math.OC"], "comment": "17 pages", "summary": "Proximal Policy Optimization (PPO) has become a widely adopted algorithm for\nreinforcement learning, offering a practical policy gradient method with strong\nempirical performance. Despite its popularity, PPO lacks formal theoretical\nguarantees for policy improvement and convergence. PPO is motivated by Trust\nRegion Policy Optimization (TRPO) that utilizes a surrogate loss with a KL\ndivergence penalty, which arises from linearizing the value function within a\nflat geometric space. In this paper, we derive a tighter surrogate in the\nFisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).\nOur proposed scheme provides strong theoretical guarantees, including monotonic\npolicy improvement. Furthermore, in the tabular setting, we demonstrate that\nFR-PPO achieves sub-linear convergence without any dependence on the\ndimensionality of the action or state spaces, marking a significant step toward\nestablishing formal convergence results for PPO-based algorithms.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684Proximal Policy Optimization\uff08PPO\uff09\u7b97\u6cd5\u53d8\u4f53-Fisher-Rao PPO\uff08FR-PPO\uff09\uff0c\u5b83\u5728Fisher-Rao\u51e0\u4f55\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u4e25\u683c\u7684\u66ff\u4ee3\u635f\u5931\u51fd\u6570\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5305\u62ec\u5355\u8c03\u7b56\u7565\u6539\u8fdb\u548c\u6b21\u7ebf\u6027\u6536\u655b\u3002", "motivation": "\u5c3d\u7ba1PPO\u5f88\u53d7\u6b22\u8fce\uff0c\u4f46\u5b83\u7f3a\u4e4f\u653f\u7b56\u6539\u8fdb\u548c\u6536\u655b\u7684\u6b63\u5f0f\u7406\u8bba\u4fdd\u8bc1\uff0c\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u5bfb\u627e\u4e00\u79cd\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u6027\u5316\u4ef7\u503c\u51fd\u6570\u65f6\u4f7f\u7528Fisher-Rao\uff08FR\uff09\u51e0\u4f55\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PPO\u53d8\u4f53\uff0c\u5373Fisher-Rao PPO\uff08FR-PPO\uff09\u3002", "result": "\u6240\u63d0\u51fa\u7684FR-PPO\u5728\u8868\u683c\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u6536\u655b\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u52a8\u4f5c\u6216\u72b6\u6001\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002", "conclusion": "FR-PPO\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5305\u62ec\u5355\u8c03\u7b56\u7565\u6539\u8fdb\uff0c\u5e76\u5728\u8868\u683c\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u6536\u655b\uff0c\u8fd9\u662f\u671d\u7740\u5efa\u7acb\u57fa\u4e8ePPO\u7684\u7b97\u6cd5\u5f62\u5f0f\u6536\u655b\u7ed3\u679c\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2506.04207", "pdf": "https://arxiv.org/pdf/2506.04207", "abs": "https://arxiv.org/abs/2506.04207", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "19 pages, 6 figures", "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86ReVisual-R1\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u5f00\u6e907B MLLM\uff0c\u5728\u591a\u4e2a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u8bb8\u591a\u4f5c\u54c1\u8bd5\u56fe\u901a\u8fc7\u76f4\u63a5\u5e94\u7528\u5f3a\u5316\u5b66\u4e60(RL)\u6765\u6fc0\u52b1Multimodal Large Language Models (MLLMs)\u4e2d\u7684\u7c7b\u4f3c\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u6fc0\u6d3b\u590d\u6742\u7684\u63a8\u7406\u3002", "method": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f53\u524d\u7684\u8bad\u7ec3\u7ba1\u9053\uff0c\u5e76\u786e\u5b9a\u4e86\u4e09\u4e2a\u5173\u952e\u73b0\u8c61\uff1a\u6709\u6548\u7684\u51b7\u542f\u52a8\u521d\u59cb\u5316\u5bf9\u4e8e\u589e\u5f3aMLLM\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff1b\u6807\u51c6GRPO\u5728\u591a\u6a21\u6001RL\u5e94\u7528\u4e2d\u5b58\u5728\u68af\u5ea6\u505c\u6ede\u95ee\u9898\uff1b\u540e\u7eed\u7684\u6587\u672cRL\u8bad\u7ec3\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u8be5\u8bba\u6587\u53d1\u73b0\u521d\u59cb\u5316\u7684\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001RL\u4e2d\u7684\u68af\u5ea6\u505c\u6ede\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u9636\u6bb5\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u5e73\u8861\u611f\u77e5\u57fa\u7840\u548c\u8ba4\u77e5\u63a8\u7406\u53d1\u5c55\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165ReVisual-R1\uff0c\u8be5\u8bba\u6587\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5176\u4e2d\u5305\u62ecMathVerse\u3001MathVision\u3001WeMath\u3001LogicVista\u3001DynaMath\u548c\u6311\u6218\u6027\u7684AIME2024\u548cAIME2025\u3002"}}
{"id": "2506.03777", "pdf": "https://arxiv.org/pdf/2506.03777", "abs": "https://arxiv.org/abs/2506.03777", "authors": ["Li Zhang", "Zhongxuan Han", "Chaochao chen", "Xiaohua Feng", "Jiaming Zhang", "Yuyuan Li"], "title": "FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "With emerging application of Federated Learning (FL) in decision-making\nscenarios, it is imperative to regulate model fairness to prevent disparities\nacross sensitive groups (e.g., female, male). Current research predominantly\nfocuses on two concepts of group fairness within FL: Global Fairness (overall\nmodel disparity across all clients) and Local Fairness (the disparity within\neach client). However, the non-decomposable, non-differentiable nature of\nfairness criteria pose two fundamental, unresolved challenges for fair FL: (i)\nHarmonizing global and local fairness in multi-class classification; (ii)\nEnabling a controllable, optimal accuracy-fairness trade-off. To tackle the\naforementioned challenges, we propose a novel controllable federated\ngroup-fairness calibration framework, named FedFACT. FedFACT identifies the\nBayes-optimal classifiers under both global and local fairness constraints in\nmulti-class case, yielding models with minimal performance decline while\nguaranteeing fairness. To effectively realize an adjustable, optimal\naccuracy-fairness balance, we derive specific characterizations of the\nBayes-optimal fair classifiers for reformulating fair FL as personalized\ncost-sensitive learning problem for in-processing, and bi-level optimization\nfor post-processing. Theoretically, we provide convergence and generalization\nguarantees for FedFACT to approach the near-optimal accuracy under given\nfairness levels. Extensive experiments on multiple datasets across various data\nheterogeneity demonstrate that FedFACT consistently outperforms baselines in\nbalancing accuracy and global-local fairness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFedFACT\uff0c\u4e00\u79cd\u65b0\u578b\u53ef\u63a7\u7684\u8054\u90a6\u7fa4\u4f53\u516c\u5e73\u6821\u51c6\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5168\u5c40\u4e0e\u5c40\u90e8\u516c\u5e73\u6027\u534f\u8c03\u4ee5\u53ca\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u6743\u8861\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u6807\u51c6\u96be\u4ee5\u5206\u89e3\u4e14\u4e0d\u53ef\u5fae\u5206\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u534f\u8c03\u5168\u5c40\u516c\u5e73\u4e0e\u5c40\u90e8\u516c\u5e73\uff0c\u5e76\u96be\u4ee5\u63a7\u5236\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86FedFACT\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u8d1d\u53f6\u65af\u6700\u4f18\u516c\u5e73\u5206\u7c7b\u5668\uff0c\u5c06\u516c\u5e73\u8054\u90a6\u5b66\u4e60\u95ee\u9898\u8f6c\u5316\u4e3a\u4e2a\u6027\u5316\u7684\u6210\u672c\u654f\u611f\u5b66\u4e60\u95ee\u9898\uff08\u7528\u4e8e\u9884\u5904\u7406\uff09\u548c\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff08\u7528\u4e8e\u540e\u5904\u7406\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u79cd\u6570\u636e\u5f02\u6784\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cFedFACT\u5728\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u5168\u5c40-\u5c40\u90e8\u516c\u5e73\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedFACT\u6846\u67b6\u80fd\u591f\u5728\u591a\u7c7b\u5206\u7c7b\u4e2d\u8bc6\u522b\u5168\u5c40\u548c\u5c40\u90e8\u516c\u5e73\u7ea6\u675f\u4e0b\u7684\u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u516c\u5e73\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5c0f\u7684\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2506.03790", "pdf": "https://arxiv.org/pdf/2506.03790", "abs": "https://arxiv.org/abs/2506.03790", "authors": ["Peng Wang", "Yifu Lu", "Yaodong Yu", "Druv Pai", "Qing Qu", "Yi Ma"], "title": "Attention-Only Transformers via Unrolled Subspace Denoising", "categories": ["cs.LG"], "comment": "28 pages, 7 figures, 5 tables", "summary": "Despite the popularity of transformers in practice, their architectures are\nempirically designed and neither mathematically justified nor interpretable.\nMoreover, as indicated by many empirical studies, some components of\ntransformer architectures may be redundant. To derive a fully interpretable\ntransformer architecture with only necessary components, we contend that the\ngoal of representation learning is to compress a set of noisy initial token\nrepresentations towards a mixture of low-dimensional subspaces. To compress\nthese noisy token representations, an associated denoising operation naturally\ntakes the form of a multi-head (subspace) self-attention. By unrolling such\niterative denoising operations into a deep network, we arrive at a highly\ncompact architecture that consists of \\textit{only} self-attention operators\nwith skip connections at each layer. Moreover, we show that each layer performs\nhighly efficient denoising: it improves the signal-to-noise ratio of token\nrepresentations \\textit{at a linear rate} with respect to the number of layers.\nDespite its simplicity, extensive experiments on vision and language tasks\ndemonstrate that such a transformer achieves performance close to that of\nstandard transformer architectures such as GPT-2 and CRATE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u6d01\u3001\u53ef\u89e3\u91ca\u7684transformer\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1transformers\u5728\u5b9e\u8df5\u4e2d\u5f88\u6d41\u884c\uff0c\u4f46\u5176\u67b6\u6784\u8bbe\u8ba1\u662f\u7ecf\u9a8c\u6027\u7684\uff0c\u65e2\u6ca1\u6709\u6570\u5b66\u4f9d\u636e\u4e5f\u4e0d\u6613\u4e8e\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u4e00\u4e9b\u7ec4\u4ef6\u53ef\u80fd\u662f\u5197\u4f59\u7684\u3002", "method": "\u901a\u8fc7\u5c06\u53bb\u566a\u64cd\u4f5c\u4e0e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u5408\uff0c\u63a8\u5bfc\u51fa\u4e00\u79cd\u7d27\u51d1\u7684transformer\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8fd9\u79cd\u65b0\u7684transformer\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u6807\u51c6\u7684transformer\u67b6\u6784\uff0c\u5982GPT-2\u548cCRATE\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u4e86\u4e00\u79cd\u4ec5\u5305\u542b\u5fc5\u8981\u7ec4\u4ef6\u7684\u5b8c\u5168\u53ef\u89e3\u91ca\u7684transformer\u67b6\u6784\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.03802", "pdf": "https://arxiv.org/pdf/2506.03802", "abs": "https://arxiv.org/abs/2506.03802", "authors": ["Andreas Athanasopoulos", "Christos Dimitrakakis"], "title": "Learning Equilibria in Matching Games with Bandit Feedback", "categories": ["cs.LG"], "comment": "21 pages, 2 figures", "summary": "We investigate the problem of learning an equilibrium in a generalized\ntwo-sided matching market, where agents can adaptively choose their actions\nbased on their assigned matches. Specifically, we consider a setting in which\nmatched agents engage in a zero-sum game with initially unknown payoff\nmatrices, and we explore whether a centralized procedure can learn an\nequilibrium from bandit feedback. We adopt the solution concept of matching\nequilibrium, where a pair consisting of a matching $\\mathfrak{m}$ and a set of\nagent strategies $X$ forms an equilibrium if no agent has the incentive to\ndeviate from $(\\mathfrak{m}, X)$. To measure the deviation of a given pair\n$(\\mathfrak{m}, X)$ from the equilibrium pair $(\\mathfrak{m}^\\star, X^\\star)$,\nwe introduce matching instability that can serve as a regret measure for the\ncorresponding learning problem. We then propose a UCB algorithm in which agents\nform preferences and select actions based on optimistic estimates of the game\npayoffs, and prove that it achieves sublinear, instance-independent regret over\na time horizon $T$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53cc\u4fa7\u5339\u914d\u5e02\u573a\u4e2d\u7684\u5747\u8861\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2aUCB\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u7684\u5747\u8861\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u7814\u7a76\u5728\u5177\u6709\u96f6\u548c\u535a\u5f08\u7684\u53cc\u8fb9\u5339\u914d\u5e02\u573a\u4e2d\uff0c\u5982\u4f55\u4ece\u8001\u864e\u673a\u53cd\u9988\u4e2d\u5b66\u4e60\u5747\u8861\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5339\u914d\u5747\u8861\u89e3\u6982\u5ff5\uff0c\u5e76\u5f15\u5165\u5339\u914d\u4e0d\u7a33\u5b9a\u6027\u4f5c\u4e3a\u9057\u61be\u5ea6\u91cf\u6807\u51c6\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2aUCB\u7b97\u6cd5\u7528\u4e8e\u5b66\u4e60\u5747\u8861\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684UCB\u7b97\u6cd5\u80fd\u591f\u5728\u672a\u77e5\u6536\u76ca\u77e9\u9635\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5747\u8861\u5b66\u4e60\uff0c\u5e76\u8fbe\u5230\u6b21\u7ebf\u6027\u9057\u61be\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e50\u89c2\u4f30\u8ba1\u7684UCB\u7b97\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u65f6\u95f4\u8303\u56f4T\u4e0a\u7684\u6b21\u7ebf\u6027\u3001\u5b9e\u4f8b\u65e0\u5173\u7684\u9057\u61be\u3002"}}
{"id": "2506.03813", "pdf": "https://arxiv.org/pdf/2506.03813", "abs": "https://arxiv.org/abs/2506.03813", "authors": ["Lili Chen", "Changyang She", "Jingge Zhu", "Jamie Evans"], "title": "Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "As the number of mobile devices continues to grow, interference has become a\nmajor bottleneck in improving data rates in wireless networks. Efficient joint\nchannel and power allocation (JCPA) is crucial for managing interference. In\nthis paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the\nJCPA problem in multi-channel wireless networks. To reduce the computational\ncomplexity of iterative optimization, we further introduce JCPGNN-M, a graph\nneural network-based solution that enables simultaneous multi-channel\nallocation for each user. We reformulate the problem as a Lagrangian function,\nwhich allows us to enforce the total power constraints systematically. Our\nsolution involves combining this Lagrangian framework with GNNs and iteratively\nupdating the Lagrange multipliers and resource allocation scheme. Unlike\nexisting GNN-based methods that limit each user to a single channel, JCPGNN-M\nsupports efficient spectrum reuse and scales well in dense network scenarios.\nSimulation results show that JCPGNN-M achieves better data rate compared to\neWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and\nit can generalize well to larger networks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8054\u5408\u4fe1\u9053\u548c\u529f\u7387\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86eWMMSE\u7b97\u6cd5\u548c\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684JCPGNN-M\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u679c\u8868\u660eJCPGNN-M\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u5e72\u6270\u6210\u4e3a\u65e0\u7ebf\u7f51\u7edc\u6570\u636e\u901f\u7387\u63d0\u5347\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8054\u5408\u4fe1\u9053\u548c\u529f\u7387\u5206\u914d\u65b9\u6848\u3002", "method": "\u63d0\u51faeWMMSE\u7b97\u6cd5\u548cJCPGNN-M\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u51fd\u6570\u8fdb\u884c\u529f\u7387\u7ea6\u675f\u3002", "result": "JCPGNN-M\u6bd4eWMMSE\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u7387\u548c\u66f4\u4f4e\u7684\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "JCPGNN-M\u5728\u5bc6\u96c6\u7f51\u7edc\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.03817", "pdf": "https://arxiv.org/pdf/2506.03817", "abs": "https://arxiv.org/abs/2506.03817", "authors": ["Julius Gonsior", "Tim Rie\u00df", "Anja Reusch", "Claudio Hartmann", "Maik Thiele", "Wolfgang Lehner"], "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid", "categories": ["cs.LG"], "comment": null, "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u8d85\u53c2\u6570\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u5206\u6790\u4e86\u8d85\u53c2\u6570\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u5efa\u8bae\u4ee5\u63d0\u5347\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5df2\u6709\u6570\u5341\u5e74\u7684\u5386\u53f2\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u7136\u5f88\u5c11\u4f7f\u7528\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u8bbe\u7f6eAL\u7684\u590d\u6742\u6027\u548c\u5bf9\u5176\u6548\u679c\u7f3a\u4e4f\u4fe1\u4efb\uff0c\u8fd9\u901a\u5e38\u5f52\u56e0\u4e8eAL\u7684\u5927\u8d85\u53c2\u6570\u7a7a\u95f4\uff0c\u5bfc\u81f4\u8bef\u5bfc\u6027\u548c\u4e0d\u53ef\u518d\u73b0\u7684AL\u5b9e\u9a8c\u7ed3\u679c\u3002", "method": "\u9996\u5148\u7f16\u8bd1\u4e86\u4e00\u4e2a\u8d85\u8fc7460\u4e07\u4e2a\u8d85\u53c2\u6570\u7ec4\u5408\u7684\u5927\u8d85\u53c2\u6570\u7f51\u683c\uff0c\u5176\u6b21\u8bb0\u5f55\u4e86\u6240\u6709\u7ec4\u5408\u5728\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u89c4\u6a21\u7684AL\u7814\u7a76\u4e2d\u7684\u8868\u73b0\uff0c\u6700\u540e\u5206\u6790\u4e86\u6bcf\u4e2a\u8d85\u53c2\u6570\u5728\u5b9e\u9a8c\u7ed3\u679c\u4e2d\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5305\u62ec\u5bf9\u6bcf\u4e2a\u8d85\u53c2\u6570\u5f71\u54cd\u7684\u5206\u6790\u3001\u5177\u4f53AL\u7b56\u7565\u5b9e\u73b0\u7684\u610f\u5916\u5f71\u54cd\u5c55\u793a\uff0c\u4ee5\u53ca\u4e3a\u53ef\u91cd\u590d\u7684AL\u5b9e\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u7684\u5efa\u8bae\uff0c\u65e8\u5728\u51cf\u5c11\u8ba1\u7b97\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8AL\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790AL\u8d85\u53c2\u6570\u7a7a\u95f4\u5bf9\u5b9e\u9a8c\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6bcf\u4e2a\u8d85\u53c2\u6570\u5f71\u54cd\u7684\u5efa\u8bae\uff0c\u5c55\u793a\u4e86\u5177\u4f53AL\u7b56\u7565\u5b9e\u73b0\u7684\u610f\u5916\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u5c0f\u8ba1\u7b97\u52aa\u529b\u4e0b\u53ef\u91cd\u590d\u7684AL\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u672a\u6765\u66f4\u53ef\u9760\u548c\u53ef\u4fe1\u7684AL\u7814\u7a76\u3002"}}
{"id": "2506.03835", "pdf": "https://arxiv.org/pdf/2506.03835", "abs": "https://arxiv.org/abs/2506.03835", "authors": ["Jianyuan Yin", "Qianxiao Li"], "title": "Learning task-specific predictive models for scientific computing", "categories": ["cs.LG"], "comment": null, "summary": "We consider learning a predictive model to be subsequently used for a given\ndownstream task (described by an algorithm) that requires access to the model\nevaluation. This task need not be prediction, and this situation is frequently\nencountered in machine-learning-augmented scientific computing. We show that\nthis setting differs from classical supervised learning, and in general it\ncannot be solved by minimizing the mean square error of the model predictions\nas is frequently performed in the literature. Instead, we find that the maximum\nprediction error on the support of the downstream task algorithm can serve as\nan effective estimate for the subsequent task performance. With this insight,\nwe formulate a task-specific supervised learning problem based on the given\nsampling measure, whose solution serves as a reliable surrogate model for the\ndownstream task. Then, we discretize the empirical risk based on training data,\nand develop an iterative algorithm to solve the task-specific supervised\nlearning problem. Three illustrative numerical examples on trajectory\nprediction, optimal control and minimum energy path computation demonstrate the\neffectiveness of the approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5e38\u89c1\u7684\u95ee\u9898\uff0c\u5373\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff09\u65e0\u6cd5\u6709\u6548\u9002\u5e94\u67d0\u4e9b\u4e0b\u6e38\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0b\u6e38\u4efb\u52a1\u7b97\u6cd5\u7684\u652f\u6301\u96c6\u4e0a\u7684\u6700\u5927\u9884\u6d4b\u8bef\u5dee\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u5f00\u53d1\u4e86\u8fed\u4ee3\u7b97\u6cd5\u6765\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u5728\u8f68\u8ff9\u9884\u6d4b\u3001\u6700\u4f18\u63a7\u5236\u548c\u6700\u5c0f\u80fd\u91cf\u8def\u5f84\u8ba1\u7b97\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\uff0c\u9488\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u7684\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u5b9e\u9645\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.03850", "pdf": "https://arxiv.org/pdf/2506.03850", "abs": "https://arxiv.org/abs/2506.03850", "authors": ["Liang Chen", "Xueting Han", "Li Shen", "Jing Bai", "Kam-Fai Wong"], "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVulnerability-Aware Alignment (VAA)\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u6709\u6548\u964d\u4f4e\u6709\u5bb3\u5fae\u8c03\uff08HFT\uff09\u7684\u98ce\u9669\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5bf9\u9f50\u6570\u636e\u4e2d\u7684\u8106\u5f31\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9HFT\u5e26\u6765\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u6570\u636e\u8106\u5f31\u6027\uff0c\u5c06\u6570\u636e\u5206\u4e3a\u201c\u8106\u5f31\u201d\u548c\u201c\u4e0d\u53ef\u4fb5\u201d\u7ec4\uff0c\u5e76\u91c7\u7528Group DRO\u6846\u67b6\u8fdb\u884c\u5e73\u8861\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVAA\u663e\u8457\u964d\u4f4e\u4e86\u6709\u5bb3\u8bc4\u5206\uff0c\u4e14\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VAA\u65b9\u6cd5\u5728\u51cf\u5c11\u6709\u5bb3\u5fae\u8c03\u5f71\u54cd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.03857", "pdf": "https://arxiv.org/pdf/2506.03857", "abs": "https://arxiv.org/abs/2506.03857", "authors": ["Mingxuan Xia", "Haobo Wang", "Yixuan Li", "Zewei Yu", "Jindong Wang", "Junbo Zhao", "Runze Wu"], "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 (Main conference)", "summary": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5019\u9009\u6ce8\u91ca\u8303\u5f0f\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u6240\u6709\u53ef\u80fd\u7684\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63d0\u70bc\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u65f6\u4ea7\u751f\u9519\u8bef\u6807\u7b7e\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u65f6\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u51b3\u5b9a\u5355\u4e00\u9ec4\u91d1\u6807\u7b7e\uff0c\u5bfc\u81f4\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u65b0\u7684\u5019\u9009\u6ce8\u91ca\u8303\u5f0f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6559\u5e08-\u5b66\u751f\u6846\u67b6CanDist\uff0c\u7528\u4e8e\u63d0\u70bc\u5019\u9009\u6ce8\u91ca\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5728\u516d\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5019\u9009\u6ce8\u91ca\u8303\u5f0f\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u6240\u6709\u53ef\u80fd\u7684\u6807\u7b7e\uff0c\u5e76\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63d0\u70bc\uff0c\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2506.03870", "pdf": "https://arxiv.org/pdf/2506.03870", "abs": "https://arxiv.org/abs/2506.03870", "authors": ["Mohd. Farhan Israk Soumik", "Syed Mhamudul Hasan", "Abdur R. Shahid"], "title": "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Apple Intelligence\u7684\u5199\u4f5c\u5de5\u5177\u5982\u4f55\u901a\u8fc7\u6587\u672c\u4fee\u6539\uff08\u5982\u91cd\u5199\u548c\u8bed\u6c14\u8c03\u6574\uff09\u6765\u9632\u6b62\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u6ee5\u7528\u4e8e\u60c5\u7eea\u63a8\u65ad\u653b\u51fb\uff0c\u4ece\u800c\u589e\u5f3a\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u60c5\u7eea\u63a8\u65ad\u653b\u51fb\u5bf9\u7528\u6237\u9690\u79c1\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u800cApple Intelligence\u7684\u5199\u4f5c\u5de5\u5177\u53ef\u80fd\u901a\u8fc7\u6587\u672c\u4fee\u6539\u6765\u51cf\u8f7b\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u4fee\u6539\u5982\u4f55\u5f71\u54cd\u57fa\u4e8eLLM\u68c0\u6d4b\u7684\u65e9\u671f\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5bf9Apple Intelligence\u7684\u5199\u4f5c\u5de5\u5177\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u7684\u6587\u672c\u4fee\u6539\u65b9\u5f0f\u5bf9\u57fa\u4e8eLLM\u7684\u60c5\u7eea\u63a8\u65ad\u68c0\u6d4b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8868\u660eApple Intelligence\u7684\u5199\u4f5c\u5de5\u5177\u53ef\u6709\u6548\u589e\u5f3a\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "Apple Intelligence\u7684\u5199\u4f5c\u5de5\u5177\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u52a8\u6001\u4e2d\u548c\u654f\u611f\u60c5\u611f\u5185\u5bb9\u7684\u81ea\u9002\u5e94\u91cd\u5199\u7cfb\u7edf\u7684\u57fa\u7840\u3002"}}
{"id": "2506.03889", "pdf": "https://arxiv.org/pdf/2506.03889", "abs": "https://arxiv.org/abs/2506.03889", "authors": ["Pau Vilimelis Aceituno", "Jack William Miller", "Noah Marti", "Youssef Farag", "Victor Boussange"], "title": "Temporal horizons in forecasting: a performance-learnability trade-off", "categories": ["cs.LG", "nlin.CD"], "comment": "33 pages, 12 figures", "summary": "When training autoregressive models for dynamical systems, a critical\nquestion arises: how far into the future should the model be trained to\npredict? Too short a horizon may miss long-term trends, while too long a\nhorizon can impede convergence due to accumulating prediction errors. In this\nwork, we formalize this trade-off by analyzing how the geometry of the loss\nlandscape depends on the training horizon. We prove that for chaotic systems,\nthe loss landscape's roughness grows exponentially with the training horizon,\nwhile for limit cycles, it grows linearly, making long-horizon training\ninherently challenging. However, we also show that models trained on long\nhorizons generalize well to short-term forecasts, whereas those trained on\nshort horizons suffer exponentially (resp. linearly) worse long-term\npredictions in chaotic (resp. periodic) systems. We validate our theory through\nnumerical experiments and discuss practical implications for selecting training\nhorizons. Our results provide a principled foundation for hyperparameter\noptimization in autoregressive forecasting models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6700\u4f73\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5408\u9002\u7684\u8bad\u7ec3\u89c6\u91ce\u7684\u539f\u5219\u65b9\u6cd5\u3002", "motivation": "\u5728\u8bad\u7ec3\u7528\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u81ea\u56de\u5f52\u6a21\u578b\u65f6\uff0c\u5982\u4f55\u9009\u62e9\u9002\u5f53\u7684\u9884\u6d4b\u672a\u6765\u65f6\u95f4\u8303\u56f4\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7279\u6027\u968f\u8bad\u7ec3\u89c6\u91ce\u7684\u53d8\u5316\u6765\u7814\u7a76\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u8bba\u6587\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6df7\u6c8c\u7cfb\u7edf\uff0c\u635f\u5931\u666f\u89c2\u7684\u7c97\u7cd9\u5ea6\u968f\u7740\u8bad\u7ec3\u89c6\u91ce\u5448\u6307\u6570\u589e\u957f\uff0c\u800c\u5bf9\u4e8e\u6781\u9650\u73af\u7cfb\u7edf\u5219\u5448\u7ebf\u6027\u589e\u957f\u3002\u6b64\u5916\uff0c\u957f\u671f\u89c6\u91ce\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u800c\u77ed\u671f\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u8bad\u7ec3\u9884\u6d4b\u7684\u89c6\u91ce\u957f\u5ea6\u5bf9\u4e8e\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e3a\u9009\u62e9\u8bad\u7ec3\u89c6\u91ce\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2506.03910", "pdf": "https://arxiv.org/pdf/2506.03910", "abs": "https://arxiv.org/abs/2506.03910", "authors": ["Shyam Prabhu", "P Akshay Kumar", "Antov Selwinston", "Pavan Taduvai", "Shreya Bairi", "Rohit Batra"], "title": "Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods", "categories": ["cs.LG"], "comment": "7 pages, 3 figures", "summary": "Materials design problems often require optimizing multiple variables,\nrendering full factorial exploration impractical. Design of experiment (DOE)\nmethods, such as Taguchi technique, are commonly used to efficiently sample the\ndesign space but they inherently lack the ability to capture non-linear\ndependency of process variables. In this work, we demonstrate how machine\nlearning (ML) methods can be used to overcome these limitations. We compare the\nperformance of Taguchi method against an active learning based Gaussian process\nregression (GPR) model in a wire arc additive manufacturing (WAAM) process to\naccurately predict aspects of bead geometry, including penetration depth, bead\nwidth, and height. While Taguchi method utilized a three-factor, five-level L25\northogonal array to suggest weld parameters, the GPR model used an\nuncertainty-based exploration acquisition function coupled with latin hypercube\nsampling for initial training data. Accuracy and efficiency of both models was\nevaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.\nThis work applies to broader materials processing domain requiring efficient\nexploration of complex parameters.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5982\u4f55\u514b\u670d\u4f20\u7edf\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u5728\u6750\u6599\u8bbe\u8ba1\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u51c6\u786e\u6027\u4e0e\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u6750\u6599\u8bbe\u8ba1\u95ee\u9898\u901a\u5e38\u9700\u8981\u4f18\u5316\u591a\u4e2a\u53d8\u91cf\uff0c\u4f7f\u5f97\u5168\u9762\u56e0\u5b50\u63a2\u7d22\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u4f20\u7edf\u7684\u5b9e\u9a8c\u8bbe\u8ba1\uff08DOE\uff09\u65b9\u6cd5\uff0c\u5982Taguchi\u6280\u672f\uff0c\u867d\u7136\u5e38\u7528\u4e8e\u9ad8\u6548\u5730\u91c7\u6837\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4f46\u5b83\u4eec\u672c\u8d28\u4e0a\u65e0\u6cd5\u6355\u6349\u8fc7\u7a0b\u53d8\u91cf\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u6bd4\u8f83\u4e86Taguchi\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u6a21\u578b\u5728\u91d1\u5c5e\u4e1d\u5f27\u589e\u6750\u5236\u9020\uff08WAAM\uff09\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u3002Taguchi\u65b9\u6cd5\u4f7f\u7528\u4e86\u4e00\u4e2a\u4e09\u56e0\u7d20\u3001\u4e94\u6c34\u5e73L25\u6b63\u4ea4\u9635\u5217\u6765\u5efa\u8bae\u710a\u63a5\u53c2\u6570\uff0c\u800cGPR\u6a21\u578b\u5219\u4f7f\u7528\u4e86\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u63a2\u7d22\u83b7\u53d6\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\u8fdb\u884c\u521d\u59cb\u8bad\u7ec3\u6570\u636e\u91c7\u96c6\u3002", "result": "\u901a\u8fc715\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u8bc4\u4f30\u4e24\u79cd\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0cGPR\u5728\u4e24\u4e2a\u6307\u6807\u4e0a\u5747\u4f18\u4e8eTaguchi\u65b9\u6cd5\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6a21\u578b\uff0c\u5728\u6750\u6599\u8bbe\u8ba1\u95ee\u9898\u4e2d\u6bd4\u4f20\u7edf\u7684Taguchi\u65b9\u6cd5\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u3002"}}
{"id": "2506.03911", "pdf": "https://arxiv.org/pdf/2506.03911", "abs": "https://arxiv.org/abs/2506.03911", "authors": ["Chamsi Hssaine", "Yichun Hu", "Ciara Pike-Burke"], "title": "Learning Fair And Effective Points-Based Rewards Programs", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Points-based rewards programs are a prevalent way to incentivize customer\nloyalty; in these programs, customers who make repeated purchases from a seller\naccumulate points, working toward eventual redemption of a free reward. These\nprograms have recently come under scrutiny due to accusations of unfair\npractices in their implementation. Motivated by these concerns, we study the\nproblem of fairly designing points-based rewards programs, with a focus on two\nobstacles that put fairness at odds with their effectiveness. First, due to\ncustomer heterogeneity, the seller should set different redemption thresholds\nfor different customers to generate high revenue. Second, the relationship\nbetween customer behavior and the number of accumulated points is typically\nunknown; this requires experimentation which may unfairly devalue customers'\npreviously earned points. We first show that an individually fair rewards\nprogram that uses the same redemption threshold for all customers suffers a\nloss in revenue of at most a factor of $1+\\ln 2$, compared to the optimal\npersonalized strategy that differentiates between customers. We then tackle the\nproblem of designing temporally fair learning algorithms in the presence of\ndemand uncertainty. Toward this goal, we design a learning algorithm that\nlimits the risk of point devaluation due to experimentation by only changing\nthe redemption threshold $O(\\log T)$ times, over a horizon of length $T$. This\nalgorithm achieves the optimal (up to polylogarithmic factors)\n$\\widetilde{O}(\\sqrt{T})$ regret in expectation. We then modify this algorithm\nto only ever decrease redemption thresholds, leading to improved fairness at a\ncost of only a constant factor in regret. Extensive numerical experiments show\nthe limited value of personalization in average-case settings, in addition to\ndemonstrating the strong practical performance of our proposed learning\nalgorithms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u516c\u5e73\u8bbe\u8ba1\u79ef\u5206\u5956\u52b1\u8ba1\u5212\uff0c\u5728\u516c\u5e73\u6027\u4e0e\u6536\u76ca\u95f4\u627e\u5230\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u7531\u4e8e\u57fa\u4e8e\u79ef\u5206\u7684\u5956\u52b1\u8ba1\u5212\u88ab\u6307\u63a7\u5b58\u5728\u4e0d\u516c\u5e73\u505a\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u516c\u5e73\u5730\u8bbe\u8ba1\u8fd9\u4e9b\u8ba1\u5212\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u4e2a\u6027\u5316\u7b56\u7565\u4e0e\u7edf\u4e00\u5151\u6362\u9608\u503c\u7684\u6536\u76ca\u5dee\u8ddd\u4ee5\u53ca\u5b66\u4e60\u7b97\u6cd5\u7684\u8bbe\u8ba1\u3002", "result": "\u8bba\u6587\u663e\u793a\u4f7f\u7528\u7edf\u4e00\u5151\u6362\u9608\u503c\u7684\u6536\u76ca\u635f\u5931\u4e0d\u8d85\u8fc7 $1+\\ln 2$\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u9650\u5236\u5151\u6362\u9608\u503c\u8c03\u6574\u6b21\u6570\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u671f\u671b\u4e0b\u7684\u6700\u4f18\u9057\u61be\u754c\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5e73\u5747\u60c5\u51b5\u4e0b\u4e2a\u6027\u5316\u4ef7\u503c\u6709\u9650\u4e14\u6240\u63d0\u7b97\u6cd5\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u4e86\u4e00\u79cd\u5728\u516c\u5e73\u6027\u4e0e\u6536\u76ca\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u79ef\u5206\u5956\u52b1\u8ba1\u5212\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u5177\u6709\u5b9e\u8df5\u6027\u80fd\u7684\u5b66\u4e60\u7b97\u6cd5\u3002"}}
{"id": "2506.03914", "pdf": "https://arxiv.org/pdf/2506.03914", "abs": "https://arxiv.org/abs/2506.03914", "authors": ["Eduardo Santos Escriche", "Stefanie Jegelka"], "title": "Learning equivariant models by discovering symmetries with learnable augmentations", "categories": ["cs.LG"], "comment": null, "summary": "Recently, a trend has emerged that favors learning relevant symmetries from\ndata in geometric domains instead of designing constrained architectures. To do\nso, two popular options are (1) to modify the training protocol, e.g., with a\nspecific loss and data augmentations (soft equivariance), or (2) to ignore\nequivariance and infer it only implicitly. However, both options have\nlimitations: soft equivariance requires a priori knowledge about relevant\nsymmetries, while inferring symmetries merely via the task and larger data\nlacks interpretability. To address both limitations, we propose SEMoLA, an\nend-to-end approach that jointly (1) discovers a priori unknown symmetries in\nthe data via learnable data augmentations, and (2) softly encodes the\nrespective approximate equivariance into an arbitrary unconstrained model.\nHence, it does not need prior knowledge about symmetries, it offers\ninterpretability, and it maintains robustness to distribution shifts.\nEmpirically, we demonstrate the ability of SEMoLA to robustly discover relevant\nsymmetries while achieving high prediction accuracy across various datasets,\nencompassing multiple data modalities and underlying symmetry groups.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SEMoLA\u65b9\u6cd5\uff0c\u5728\u4e0d\u4f9d\u8d56\u5bf9\u79f0\u6027\u5148\u9a8c\u77e5\u8bc6\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u6570\u636e\u589e\u5f3a\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u5bf9\u79f0\u6027\u5e76\u5c06\u5176\u8fd1\u4f3c\u7b49\u53d8\u6027\u5d4c\u5165\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\u3001\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e24\u79cd\u65b9\u6cd5\u5747\u6709\u5c40\u9650\u6027\uff1a\u8f6f\u7b49\u53d8\u9700\u8981\u5bf9\u79f0\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u800c\u4ec5\u901a\u8fc7\u4efb\u52a1\u548c\u66f4\u5927\u6570\u636e\u9690\u5f0f\u63a8\u65ad\u5bf9\u79f0\u6027\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u81ea\u52a8\u53d1\u73b0\u5bf9\u79f0\u6027\u53c8\u80fd\u4fdd\u6301\u6a21\u578b\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSEMoLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6570\u636e\u589e\u5f3a\u6765\u53d1\u73b0\u672a\u77e5\u5bf9\u79f0\u6027\uff0c\u5e76\u5c06\u76f8\u5e94\u7684\u8fd1\u4f3c\u7b49\u53d8\u6027\u4ee5\u8f6f\u7f16\u7801\u65b9\u5f0f\u5d4c\u5165\u5230\u6a21\u578b\u4e2d\u3002", "result": "SEMoLA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u80fd\u591f\u7a33\u5065\u5730\u53d1\u73b0\u76f8\u5173\u5bf9\u79f0\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u6f5c\u5728\u5bf9\u79f0\u7fa4\u3002", "conclusion": "SEMoLA\u80fd\u591f\u5728\u4e0d\u9700\u8981\u5bf9\u79f0\u6027\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u76f8\u5173\u5bf9\u79f0\u6027\uff0c\u5e76\u5c06\u5176\u8fd1\u4f3c\u7b49\u53d8\u6027\u7f16\u7801\u5230\u4efb\u610f\u65e0\u7ea6\u675f\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.03919", "pdf": "https://arxiv.org/pdf/2506.03919", "abs": "https://arxiv.org/abs/2506.03919", "authors": ["Lorenz Kummer", "Samir Moustafa", "Anatol Ehrlich", "Franka Bause", "Nikolaus Suess", "Wilfried N. Gansterer", "Nils M. Kriege"], "title": "Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "The lottery ticket hypothesis (LTH) is well-studied for convolutional neural\nnetworks but has been validated only empirically for graph neural networks\n(GNNs), for which theoretical findings are largely lacking. In this paper, we\nidentify the expressivity of sparse subnetworks, i.e. their ability to\ndistinguish non-isomorphic graphs, as crucial for finding winning tickets that\npreserve the predictive performance. We establish conditions under which the\nexpressivity of a sparsely initialized GNN matches that of the full network,\nparticularly when compared to the Weisfeiler-Leman test, and in that context\nput forward and prove a Strong Expressive Lottery Ticket Hypothesis. We\nsubsequently show that an increased expressivity in the initialization\npotentially accelerates model convergence and improves generalization. Our\nfindings establish novel theoretical foundations for both LTH and GNN research,\nhighlighting the importance of maintaining expressivity in sparsely initialized\nGNNs. We illustrate our results using examples from drug discovery.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5f69\u7968\u5047\u8bf4\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8868\u660e\u4fdd\u6301\u7a00\u758f\u521d\u59cb\u5316\u5b50\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u5bf9\u4e8e\u7ef4\u6301\u9884\u6d4b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u867d\u7136\u5f69\u7968\u5047\u8bf4\uff08LTH\uff09\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f97\u5230\u4e86\u5145\u5206\u7814\u7a76\uff0c\u4f46\u5176\u5728\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u7684\u6709\u6548\u6027\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u7a00\u758f\u521d\u59cb\u5316\u7684GNN\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u5b8c\u6574\u7f51\u7edc\u76f8\u5f53\u7684\u6761\u4ef6\uff0c\u5e76\u7ed3\u5408Weisfeiler-Leman\u6d4b\u8bd5\u8fdb\u884c\u5206\u6790\uff0c\u63d0\u51fa\u5e76\u8bc1\u660e\u4e86\u5f3a\u8868\u8fbe\u6027\u4e2d\u5956\u5f69\u7968\u5047\u8bf4\u3002", "result": "\u4f5c\u8005\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u7a00\u758f\u521d\u59cb\u5316\u7684GNN\u53ef\u4ee5\u5339\u914d\u5b8c\u6574\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u63d0\u9ad8\u521d\u59cb\u5316\u8868\u8fbe\u80fd\u529b\u5bf9\u6a21\u578b\u6536\u655b\u901f\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u7684\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5728\u7a00\u758f\u521d\u59cb\u5316\u7684GNN\u4e2d\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u5bf9\u4e8e\u627e\u5230\u4fdd\u7559\u9884\u6d4b\u6027\u80fd\u7684\u201c\u4e2d\u5956\u5f69\u7968\u201d\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.03938", "pdf": "https://arxiv.org/pdf/2506.03938", "abs": "https://arxiv.org/abs/2506.03938", "authors": ["C\u00e9dric L\u00e9onard", "Dirk Stober", "Martin Schulz"], "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review", "categories": ["cs.LG", "cs.AR"], "comment": "35 pages, 3 figures, 2 tables. Submitted to ACM Computing Surveys\n  (ACM CSUR)", "summary": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5728FPGA\u4e0a\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u9065\u611f\u5e94\u7528\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5206\u7c7b\u6cd5\u5e76\u4fdd\u8bc1\u7814\u7a76\u7684\u900f\u660e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u6280\u672f\u548cNewSpace\u65f6\u4ee3\u6b63\u5728\u6539\u53d8\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u548c\u6570\u636e\u91c7\u96c6\u65b9\u5f0f\uff0c\u9700\u8981\u673a\u8f7d\u5b9e\u65f6\u51b3\u7b56\u4ee5\u5e94\u5bf9\u5e26\u5bbd\u9650\u5236\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u5730\u5206\u6790\u4e8666\u4e2a\u5b9e\u9a8c\uff0c\u5e76\u9075\u5faaPRISMA 2020\u6307\u5357\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\uff0c\u540c\u65f6\u516c\u5f00\u6240\u6709\u6570\u636e\u548c\u4ee3\u7801\u3002", "result": "\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e24\u79cd\u5206\u7c7b\u6cd5\uff0c\u5206\u522b\u7528\u4e8e\u6355\u6349\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u548cFPGA\u5b9e\u73b0\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u9ad8\u9065\u611f\u6570\u636e\u5904\u7406\u7684\u6548\u7387\u3002", "conclusion": "\u672c\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728FPGA\u4e0a\u7684\u90e8\u7f72\u4e3a\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u5206\u7c7b\u6cd5\u5c55\u793a\u4e86\u6709\u6548\u7684\u6a21\u578b\u67b6\u6784\u548c\u5b9e\u73b0\u7b56\u7565\u3002"}}
{"id": "2506.03951", "pdf": "https://arxiv.org/pdf/2506.03951", "abs": "https://arxiv.org/abs/2506.03951", "authors": ["Aojun Lu", "Hangjie Yuan", "Tao Feng", "Yanan Sun"], "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6Dual-Arch\uff0c\u5b83\u5728\u67b6\u6784\u5c42\u9762\u4e0a\u89e3\u51b3\u4e86\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u4e86\u7f51\u7edc\u7ed3\u6784\u5bf9\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u7684\u5f71\u54cd\uff0c\u5c06\u8fd9\u79cd\u6743\u8861\u9650\u5236\u5728\u53c2\u6570\u7ea7\u522b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDual-Arch\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDual-Arch\u4e0d\u4ec5\u63d0\u5347\u4e86\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u53c2\u6570\u6570\u91cf\u4e0a\u6700\u591a\u51cf\u5c11\u4e8687%\u3002", "conclusion": "Dual-Arch\u901a\u8fc7\u7ed3\u5408\u4e24\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u7f51\u7edc\uff0c\u4e00\u4e2a\u7528\u4e8e\u53ef\u5851\u6027\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u5728\u67b6\u6784\u5c42\u9762\u89e3\u51b3\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002"}}
{"id": "2506.03956", "pdf": "https://arxiv.org/pdf/2506.03956", "abs": "https://arxiv.org/abs/2506.03956", "authors": ["Aojun Lu", "Tao Feng", "Hangjie Yuan", "Chunhui Ding", "Yanan Sun"], "title": "Adapt before Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6ACL\uff0c\u901a\u8fc7\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u524d\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6709\u6548\u5e73\u8861\u4e86\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u4fdd\u6301\u7a33\u5b9a\u6027\uff0c\u4f46\u9650\u5236\u4e86\u5176\u53ef\u5851\u6027\uff1b\u800c\u5bf9\u6574\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u987a\u5e8f\u5fae\u8c03\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6ACL\uff0c\u901a\u8fc7\u5728\u6838\u5fc3CL\u8fc7\u7a0b\u4e4b\u524d\u8fdb\u884c\u63d2\u4ef6\u5f0f\u7684\u9002\u914d\u9636\u6bb5\u6765\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTM\uff09\u4e3b\u5e72\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cACL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u73b0\u6709\u65b9\u6cd5\u96c6\u6210\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0cACL\u65b9\u6cd5\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u5851\u6027\uff0c\u4e3a\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03996", "pdf": "https://arxiv.org/pdf/2506.03996", "abs": "https://arxiv.org/abs/2506.03996", "authors": ["Lianfeng Shi", "Ao Li", "Benjamin Ward-Cherrier"], "title": "Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Spiking Neural Networks (SNNs) have emerged as a new generation of\nenergy-efficient neural networks suitable for implementation on neuromorphic\nhardware. As neuromorphic hardware has limited memory and computing resources,\nweight pruning and quantization have recently been explored to improve SNNs'\nefficiency. State-of-the-art SNN pruning/quantization methods employ multiple\ncompression and training iterations, increasing the cost for pre-trained or\nvery large SNNs. In this paper, we propose a new one-shot post-training\npruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that\nadapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and\nAlistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input\ncurrent as OBC does, OSBC achieves more efficient and accurate SNN compression\nin one pass by minimizing the loss on spiking neuron membrane potential with a\nsmall sample dataset. Our experiments on neuromorphic datasets (N-MNIST,\nCIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity\nthrough pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric\nquantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code\nwill be available on GitHub.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8eSpiking Neural Networks (SNNs)\u7684\u65b0\u578b\u9ad8\u6548\u538b\u7f29\u6846\u67b6Optimal Spiking Brain Compression (OSBC)\uff0c\u65e8\u5728\u964d\u4f4eSNNs\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u9884\u8bad\u7ec3\u6216\u975e\u5e38\u5927\u7684SNNs\u7684\u6210\u672c\uff0c\u63d0\u9ad8SNNs\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u7684\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6b21\u540e\u8bad\u7ec3\u526a\u679d/\u91cf\u5316\u6846\u67b6Optimal Spiking Brain Compression (OSBC)\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8109\u51b2\u795e\u7ecf\u5143\u819c\u7535\u4f4d\u635f\u5931\u8fdb\u884c\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOSBC\u5728N-MNIST\u3001CIFAR10-DVS\u548cDVS128-Gesture\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8697%\u7684\u7a00\u758f\u5ea6\uff0c\u51c6\u786e\u7387\u635f\u5931\u5206\u522b\u4e3a1.41%\uff0c10.20%\u548c1.74%\u3002", "conclusion": "OSBC\u662f\u4e00\u79cd\u9ad8\u6548\u7684SNN\u538b\u7f29\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u3002"}}
{"id": "2506.04026", "pdf": "https://arxiv.org/pdf/2506.04026", "abs": "https://arxiv.org/abs/2506.04026", "authors": ["Cl\u00e9ment B\u00e9nesse", "Patrick Mesana", "Ath\u00e9na\u00efs Gautier", "S\u00e9bastien Gambs"], "title": "On the Usage of Gaussian Process for Efficient Data Valuation", "categories": ["cs.LG"], "comment": null, "summary": "In machine learning, knowing the impact of a given datum on model training is\na fundamental task referred to as Data Valuation. Building on previous works\nfrom the literature, we have designed a novel canonical decomposition allowing\npractitioners to analyze any data valuation method as the combination of two\nparts: a utility function that captures characteristics from a given model and\nan aggregation procedure that merges such information. We also propose to use\nGaussian Processes as a means to easily access the utility function on\n``sub-models'', which are models trained on a subset of the training set. The\nstrength of our approach stems from both its theoretical grounding in Bayesian\ntheory, and its practical reach, by enabling fast estimation of valuations\nthanks to efficient update formulae.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u53f6\u65af\u7406\u8bba\u4e0e\u9ad8\u6548\u8ba1\u7b97\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u4f30\u503c\u3002", "motivation": "\u4e86\u89e3\u6570\u636e\u70b9\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u5f71\u54cd\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u4efb\u52a1\uff0c\u79f0\u4e3a\u6570\u636e\u4f30\u503c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6807\u51c6\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u6765\u4f30\u8ba1\u5b50\u6a21\u578b\u7684\u6548\u7528\u51fd\u6570\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u4f30\u7b97\u4f30\u503c\u7684\u65b9\u6cd5\uff0c\u4e14\u80fd\u591f\u5c06\u4efb\u4f55\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u5206\u6790\u4e3a\u6548\u7528\u51fd\u6570\u4e0e\u805a\u5408\u8fc7\u7a0b\u7684\u7ec4\u5408\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u7406\u8bba\u7684\u65b0\u578b\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.04053", "pdf": "https://arxiv.org/pdf/2506.04053", "abs": "https://arxiv.org/abs/2506.04053", "authors": ["Alexander Semenenko", "Ivan Butakov", "Alexey Frolov", "Ivan Oseledets"], "title": "Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence", "categories": ["cs.LG", "cs.IT", "math.IT", "94A16, 68T07, 94A17", "E.4; H.1.1"], "comment": null, "summary": "Sliced Mutual Information (SMI) is widely used as a scalable alternative to\nmutual information for measuring non-linear statistical dependence. Despite its\nadvantages, such as faster convergence, robustness to high dimensionality, and\nnullification only under statistical independence, we demonstrate that SMI is\nhighly susceptible to data manipulation and exhibits counterintuitive behavior.\nThrough extensive benchmarking and theoretical analysis, we show that SMI\nsaturates easily, fails to detect increases in statistical dependence (even\nunder linear transformations designed to enhance the extraction of\ninformation), prioritizes redundancy over informative content, and in some\ncases, performs worse than simpler dependence measures like the correlation\ncoefficient.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.04071", "pdf": "https://arxiv.org/pdf/2506.04071", "abs": "https://arxiv.org/abs/2506.04071", "authors": ["Luiz Manella Pereira", "M. Hadi Amini"], "title": "Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated learning (FL) is a subfield of machine learning that avoids sharing\nlocal data with a central server, which can enhance privacy and scalability.\nThe inability to consolidate data leads to a unique problem called dataset\nimbalance, where agents in a network do not have equal representation of the\nlabels one is trying to learn to predict. In FL, fusing locally-trained models\nwith unbalanced datasets may deteriorate the performance of global model\naggregation, and reduce the quality of updated local models and the accuracy of\nthe distributed agents' decisions. In this work, we introduce an Optimal\nTransport-based preprocessing algorithm that aligns the datasets by minimizing\nthe distributional discrepancy of data along the edge devices. We accomplish\nthis by leveraging Wasserstein barycenters when computing channel-wise\naverages. These barycenters are collected in a trusted central server where\nthey collectively generate a target RGB space. By projecting our dataset\ntowards this target space, we minimize the distributional discrepancy on a\nglobal level, which facilitates the learning process due to a minimization of\nvariance across the samples. We demonstrate the capabilities of the proposed\napproach over the CIFAR-10 dataset, where we show its capability of reaching\nhigher degrees of generalization in fewer communication rounds.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u6570\u636e\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u548cWasserstein\u91cd\u5fc3\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7531\u4e8e\u672c\u5730\u6570\u636e\u96c6\u6807\u7b7e\u5206\u5e03\u4e0d\u5747\u8861\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5bf9\u9f50\u6570\u636e\u5206\u5e03\uff0c\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8eWasserstein\u91cd\u5fc3\u7684\u9884\u5904\u7406\u7b97\u6cd5\uff0c\u5c06\u672c\u5730\u6570\u636e\u96c6\u6295\u5f71\u5230\u7531\u4e2d\u5fc3\u670d\u52a1\u5668\u751f\u6210\u7684\u76ee\u6807RGB\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u51cf\u5c11\u6837\u672c\u95f4\u7684\u65b9\u5dee\u5e76\u4f18\u5316\u5168\u5c40\u6a21\u578b\u805a\u5408\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u66f4\u5c11\u901a\u4fe1\u8f6e\u6b21\u4e0b\u8fbe\u5230\u66f4\u9ad8\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u9884\u5904\u7406\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u6765\u6539\u5584\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8f83\u5c11\u901a\u4fe1\u8f6e\u6b21\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.04126", "pdf": "https://arxiv.org/pdf/2506.04126", "abs": "https://arxiv.org/abs/2506.04126", "authors": ["Yujun Kim", "Jaeyoung Cha", "Chulhee Yun"], "title": "Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems", "categories": ["cs.LG", "math.OC"], "comment": "Accepted to ICML 2025, 56 pages, 6 figures", "summary": "Recent theoretical results demonstrate that the convergence rates of\npermutation-based SGD (e.g., random reshuffling SGD) are faster than\nuniform-sampling SGD; however, these studies focus mainly on the large epoch\nregime, where the number of epochs $K$ exceeds the condition number $\\kappa$.\nIn contrast, little is known when $K$ is smaller than $\\kappa$, and it is still\na challenging open question whether permutation-based SGD can converge faster\nin this small epoch regime (Safran and Shamir, 2021). As a step toward\nunderstanding this gap, we study the naive deterministic variant, Incremental\nGradient Descent (IGD), on smooth and strongly convex functions. Our lower\nbounds reveal that for the small epoch regime, IGD can exhibit surprisingly\nslow convergence even when all component functions are strongly convex.\nFurthermore, when some component functions are allowed to be nonconvex, we\nprove that the optimality gap of IGD can be significantly worse throughout the\nsmall epoch regime. Our analyses reveal that the convergence properties of\npermutation-based SGD in the small epoch regime may vary drastically depending\non the assumptions on component functions. Lastly, we supplement the paper with\ntight upper and lower bounds for IGD in the large epoch regime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5728\u5c0fepoch\u73af\u5883\u4e0b\uff0c\u57fa\u4e8e\u6392\u5217\u7684SGD\uff08\u5982IGD\uff09\u7684\u6536\u655b\u7279\u6027\uff0c\u5e76\u6307\u51fa\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5176\u8868\u73b0\u53ef\u80fd\u663e\u8457\u52a3\u4e8e\u9884\u671f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5728\u5927epoch\u73af\u5883\u4e0b\uff0c\u57fa\u4e8e\u6392\u5217\u7684SGD\uff08\u5982\u968f\u673a\u91cd\u6392SGD\uff09\u6bd4\u5747\u5300\u91c7\u6837SGD\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4f46\u5728epoch\u6570K\u5c0f\u4e8e\u6761\u4ef6\u6570\u03ba\u7684\u5c0fepoch\u73af\u5883\u4e0b\uff0c\u5bf9\u5176\u6027\u80fd\u77e5\u4e4b\u751a\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u7406\u8bba\u5206\u6790\u548c\u6570\u5b66\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u5bf9Incremental Gradient Descent (IGD) \u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u63a2\u8ba8\uff0c\u5e76\u5f97\u51fa\u4e86\u6536\u655b\u7279\u6027\u7684\u7ed3\u8bba\u3002", "result": "\u8bba\u6587\u63ed\u793a\u4e86\u5728\u5c0fepoch\u73af\u5883\u4e0b\uff0c\u5373\u4f7f\u6240\u6709\u7ec4\u4ef6\u51fd\u6570\u5747\u4e3a\u5f3a\u51f8\u51fd\u6570\uff0cIGD\u4ecd\u53ef\u80fd\u8868\u73b0\u51fa\u663e\u8457\u7f13\u6162\u7684\u6536\u655b\u901f\u5ea6\uff1b\u800c\u5f53\u5141\u8bb8\u90e8\u5206\u7ec4\u4ef6\u51fd\u6570\u4e3a\u975e\u51f8\u65f6\uff0cIGD\u7684\u6700\u4f18\u95f4\u9699\u5728\u6574\u4e2a\u5c0fepoch\u73af\u5883\u4e0b\u53ef\u80fd\u4f1a\u66f4\u52a0\u6076\u5316\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\uff0c\u57fa\u4e8e\u6392\u5217\u7684SGD\u5728\u5c0fepoch\u8303\u56f4\u5185\uff0c\u6839\u636e\u7ec4\u4ef6\u51fd\u6570\u7684\u5047\u8bbe\u4e0d\u540c\uff0c\u5176\u6536\u655b\u7279\u6027\u53ef\u80fd\u4f1a\u6709\u5f88\u5927\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u5728\u5927epoch\u8303\u56f4\u5185\uff0cIGD\u5177\u6709\u7d27\u5bc6\u7684\u4e0a\u4e0b\u9650\u3002"}}
{"id": "2506.04165", "pdf": "https://arxiv.org/pdf/2506.04165", "abs": "https://arxiv.org/abs/2506.04165", "authors": ["Yashas Samaga", "Varun Yerram", "Spandana Raj Babbula", "Prateek Jain", "Praneeth Netrapalli"], "title": "Faster Approx. Top-K: Harnessing the Full Power of Two Stages", "categories": ["cs.LG", "cs.DS"], "comment": "Rejected from MLSys 2025", "summary": "We consider the Top-$K$ selection problem, which aims to identify the\nlargest-$K$ elements from an array. Top-$K$ selection arises in many machine\nlearning algorithms and often becomes a bottleneck on accelerators, which are\noptimized for dense matrix multiplications. To address this problem,\n\\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage\n\\textit{approximate} Top-$K$ algorithm: (i) partition the input array and\nselect the top-$1$ element from each partition, (ii) sort this \\textit{smaller\nsubset} and return the top $K$ elements. In this paper, we consider a\ngeneralized version of this algorithm, where the first stage selects top-$K'$\nelements, for some $1 \\leq K' \\leq K$, from each partition. Our contributions\nare as follows: (i) we derive an expression for the expected recall of this\ngeneralized algorithm and show that choosing $K' > 1$ with fewer partitions in\nthe first stage reduces the input size to the second stage more effectively\nwhile maintaining the same expected recall as the original algorithm, (ii) we\nderive a bound on the expected recall for the original algorithm in\n\\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of\n$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud\nTPUv5e and achieve around an order of magnitude speedups over the original\nalgorithm without sacrificing recall on real-world tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684Top-K\u9009\u62e9\u7b97\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548cTPU\u4e0a\u7684\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u5373\u5728\u4e0d\u727a\u7272\u53ec\u56de\u7387\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "Top-K\u9009\u62e9\u95ee\u9898\u5728\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e2d\u7ecf\u5e38\u51fa\u73b0\uff0c\u5e76\u4e14\u5728\u52a0\u901f\u5668\u4e0a\u6210\u4e3a\u74f6\u9888\uff0c\u800c\u52a0\u901f\u5668\u662f\u4e3a\u5bc6\u96c6\u77e9\u9635\u4e58\u6cd5\u4f18\u5316\u7684\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5feb\u7684\u8fd1\u4f3cTop-K\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u4e24\u9636\u6bb5Top-K\u9009\u62e9\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4ece\u6bcf\u4e2a\u5206\u533a\u4e2d\u9009\u62e9top-K'\u5143\u7d20\uff081 \u2264 K' \u2264 K\uff09\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u8fd9\u4e9b\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\u5e76\u8fd4\u56detop-K\u5143\u7d20\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63a8\u5bfc\u51fa\u4e86\u8be5\u5e7f\u4e49\u7b97\u6cd5\u7684\u9884\u671f\u53ec\u56de\u7387\u8868\u8fbe\u5f0f\u548c\u539f\u59cb\u7b97\u6cd5\u7684\u66f4\u7d27\u7684\u9884\u671f\u53ec\u56de\u7387\u754c\u9650\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u7b2c\u4e00\u9636\u6bb5\u9009\u62e9K' > 1\u5e76\u51cf\u5c11\u5206\u533a\u6570\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u964d\u4f4e\u7b2c\u4e8c\u9636\u6bb5\u7684\u8f93\u5165\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u7b97\u6cd5\u76f8\u540c\u7684\u9884\u671f\u53ec\u56de\u7387\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u63a8\u5bfc\u51fa\u4e86\u539f\u59cb\u7b97\u6cd5\u9884\u671f\u53ec\u56de\u7387\u7684\u4e00\u4e2a\u66f4\u7d27\u7684\u8fb9\u754c\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u4ed6\u4eec\u7684\u7b97\u6cd5\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u5f97\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Top-K\u9009\u62e9\u7b97\u6cd5\uff0c\u5e76\u5728Cloud TPUv5e\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u539f\u7b97\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u53ec\u56de\u7387\u3002"}}
{"id": "2506.04172", "pdf": "https://arxiv.org/pdf/2506.04172", "abs": "https://arxiv.org/abs/2506.04172", "authors": ["Shreenidhi Srinivasan", "Lydia Manikonda"], "title": "Does Prompt Design Impact Quality of Data Imputation by LLMs?", "categories": ["cs.LG", "cs.ET"], "comment": "7 pages", "summary": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u7f3a\u5931\u95ee\u9898\u7684\u65b0\u578b\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u7528\u4e8e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "motivation": "\u751f\u6210\u903c\u771f\u7684\u5408\u6210\u8868\u683c\u6570\u636e\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5f53\u8fd9\u4e9b\u6570\u636e\u5305\u542b\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u65f6\u3002\u8fd9\u662f\u7814\u7a76\u7684\u4e3b\u8981\u52a8\u673a\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6807\u8bb0\u611f\u77e5\u6570\u636e\u63d2\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u9010\u7ec4CSV\u98ce\u683c\u63d0\u793a\u6280\u672f\u548c\u6d88\u9664\u8f93\u5165\u63d0\u793a\u4e2d\u7684\u65e0\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u8f93\u5165\u63d0\u793a\u7684\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u63d2\u8865\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u76f8\u5bf9\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u6307\u51fa\uff0c\u8be5\u7814\u7a76\u6240\u63d0\u51fa\u7684\u63d0\u793a\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u5e0c\u671b\u6fc0\u53d1\u66f4\u591a\u5173\u4e8e\u5229\u7528LLMs\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u8fdb\u884c\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u7814\u7a76\u4e0e\u8ba8\u8bba\u3002"}}
{"id": "2506.04178", "pdf": "https://arxiv.org/pdf/2506.04178", "abs": "https://arxiv.org/abs/2506.04178", "authors": ["Etash Guha", "Ryan Marten", "Sedrick Keh", "Negin Raoof", "Georgios Smyrnis", "Hritik Bansal", "Marianna Nezhurina", "Jean Mercat", "Trung Vu", "Zayne Sprague", "Ashima Suvarna", "Benjamin Feuer", "Liangyu Chen", "Zaid Khan", "Eric Frankel", "Sachin Grover", "Caroline Choi", "Niklas Muennighoff", "Shiye Su", "Wanjia Zhao", "John Yang", "Shreyas Pimpalgaonkar", "Kartik Sharma", "Charlie Cheng-Jie Ji", "Yichuan Deng", "Sarah Pratt", "Vivek Ramanujan", "Jon Saad-Falcon", "Jeffrey Li", "Achal Dave", "Alon Albalak", "Kushal Arora", "Blake Wulfe", "Chinmay Hegde", "Greg Durrett", "Sewoong Oh", "Mohit Bansal", "Saadia Gabriel", "Aditya Grover", "Kai-Wei Chang", "Vaishaal Shankar", "Aaron Gokaslan", "Mike A. Merrill", "Tatsunori Hashimoto", "Yejin Choi", "Jenia Jitsev", "Reinhard Heckel", "Maheswaran Sathiamoorthy", "Alexandros G. Dimakis", "Ludwig Schmidt"], "title": "OpenThoughts: Data Recipes for Reasoning Models", "categories": ["cs.LG"], "comment": "https://www.openthoughts.ai/blog/ot3", "summary": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of the OpenThoughts project is to create open-source datasets\nfor training reasoning models. After initial explorations, our OpenThoughts2-1M\ndataset led to OpenThinker2-32B, the first model trained on public reasoning\ndata to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as\nAIME and LiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves\nstate-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,\nand 54% on GPQA Diamond. All of our datasets and models are available on\nhttps://openthoughts.ai.", "AI": {"tldr": "\u8be5\u9879\u76ee\u65e8\u5728\u521b\u5efa\u5f00\u653e\u6e90\u4ee3\u7801\u7684\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u751f\u6210\u6d41\u7a0b\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u4f9d\u8d56\u4e8e\u4e13\u6709\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u516c\u5f00\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u521b\u5efa\u5f00\u653e\u6e90\u4ee3\u7801\u7684\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u6570\u636e\u751f\u6210\u6d41\u7a0b\u4e2d\u7684\u6bcf\u4e00\u6b65\uff0c\u5e76\u8fdb\u884c1000\u591a\u6b21\u53d7\u63a7\u5b9e\u9a8c\u6765\u6539\u8fdb\u6570\u636e\u96c6\u3002", "result": "OpenThinker3-7B\u6a21\u578b\u5728AIME\u3001LiveCodeBench\u548cGPQA Diamond\u7b49\u6807\u51c6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523053%\u300151%\u548c54%\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "OpenThoughts\u9879\u76ee\u6210\u529f\u521b\u5efa\u4e86\u5f00\u6e90\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u751f\u6210\u6d41\u7a0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.04190", "pdf": "https://arxiv.org/pdf/2506.04190", "abs": "https://arxiv.org/abs/2506.04190", "authors": ["Yuxuan Cao", "Jiarong Xu", "Chen Zhao", "Jiaan Wang", "Carl Yang", "Chunping Wang", "Yang Yang"], "title": "How to Use Graph Data in the Wild to Help Graph Anomaly Detection?", "categories": ["cs.LG"], "comment": "Accepted by SIGKDD2025", "summary": "In recent years, graph anomaly detection has found extensive applications in\nvarious domains such as social, financial, and communication networks. However,\nanomalies in graph-structured data present unique challenges, including label\nscarcity, ill-defined anomalies, and varying anomaly types, making supervised\nor semi-supervised methods unreliable. Researchers often adopt unsupervised\napproaches to address these challenges, assuming that anomalies deviate\nsignificantly from the normal data distribution. Yet, when the available data\nis insufficient, capturing the normal distribution accurately and\ncomprehensively becomes difficult. To overcome this limitation, we propose to\nutilize external graph data (i.e., graph data in the wild) to help anomaly\ndetection tasks. This naturally raises the question: How can we use external\ndata to help graph anomaly detection tasks? To answer this question, we propose\na framework called Wild-GAD. It is built upon a unified database, UniWildGraph,\nwhich comprises a large and diverse collection of graph data with broad domain\ncoverage, ample data volume, and a unified feature space. Further, we develop\nselection criteria based on representativity and diversity to identify the most\nsuitable external data for anomaly detection task. Extensive experiments on six\nreal-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the\nbaseline methods, our framework has an average 18% AUCROC and 32% AUCPR\nimprovement over the best-competing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWild-GAD\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u56fe\u6570\u636e\u63d0\u5347\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u5b58\u5728\u6807\u7b7e\u7a00\u7f3a\u3001\u5b9a\u4e49\u6a21\u7cca\u548c\u7c7b\u578b\u591a\u6837\u7b49\u95ee\u9898\uff0c\u76d1\u7763\u6216\u534a\u76d1\u7763\u65b9\u6cd5\u4e0d\u53ef\u9760\uff0c\u800c\u73b0\u6709\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5168\u9762\u5730\u6355\u83b7\u6b63\u5e38\u6570\u636e\u5206\u5e03\u3002\u56e0\u6b64\u9700\u8981\u5229\u7528\u5916\u90e8\u6570\u636e\u8f85\u52a9\u56fe\u5f02\u5e38\u68c0\u6d4b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u4e00\u6570\u636e\u5e93UniWildGraph\u7684\u6846\u67b6Wild-GAD\uff0c\u5e76\u57fa\u4e8e\u4ee3\u8868\u6027\u4e0e\u591a\u6837\u6027\u5236\u5b9a\u9009\u62e9\u6807\u51c6\u4ee5\u786e\u5b9a\u6700\u9002\u5408\u7684\u5916\u90e8\u6570\u636e\u8fdb\u884c\u56fe\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5e73\u5747AUCROC\u63d0\u9ad8\u4e8618%\uff0cAUCPR\u63d0\u9ad8\u4e8632%\u3002", "conclusion": "Wild-GAD\u6846\u67b6\u5728\u56fe\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2506.04205", "pdf": "https://arxiv.org/pdf/2506.04205", "abs": "https://arxiv.org/abs/2506.04205", "authors": ["Jinghan Jia", "Hadi Reisizadeh", "Chongyu Fan", "Nathalie Baracaldo", "Mingyi Hong", "Sijia Liu"], "title": "EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEPiC\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559CoT\u8f68\u8ff9\u7684\u8d77\u59cb\u548c\u7ed3\u675f\u90e8\u5206\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u5f00\u9500\u4e14\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\u76d1\u7763\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6602\u6210\u672c\uff0c\u9700\u8981\u5bf9\u5197\u957f\u7684CoT\u8f68\u8ff9\u8fdb\u884c\u538b\u7f29\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdEdge-Preserving Condensation\uff08EPiC\uff09\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u5730\u4fdd\u7559CoT\u8f68\u8ff9\u7684\u521d\u59cb\u548c\u6700\u7ec8\u90e8\u5206\uff0c\u53bb\u9664\u4e2d\u95f4\u90e8\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEPiC\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c11\u4e86\u8d85\u8fc734%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u5728MATH500\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u6574CoT\u76d1\u7763\u76f8\u5f53\u7684\u63a8\u7406\u7cbe\u5ea6\u3002", "conclusion": "EPiC\u65b9\u6cd5\u5728\u4fdd\u6301\u7b54\u6848\u51c6\u786e\u6027\u548c\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2506.04206", "pdf": "https://arxiv.org/pdf/2506.04206", "abs": "https://arxiv.org/abs/2506.04206", "authors": ["Reza Ramezanpour", "Victor M. Tenorio", "Antonio G. Marques", "Ashutosh Sabharwal", "Santiago Segarra"], "title": "A Few Moments Please: Scalable Graphon Learning via Moment Matching", "categories": ["cs.LG"], "comment": null, "summary": "Graphons, as limit objects of dense graph sequences, play a central role in\nthe statistical analysis of network data. However, existing graphon estimation\nmethods often struggle with scalability to large networks and\nresolution-independent approximation, due to their reliance on estimating\nlatent variables or costly metrics such as the Gromov-Wasserstein distance. In\nthis work, we propose a novel, scalable graphon estimator that directly\nrecovers the graphon via moment matching, leveraging implicit neural\nrepresentations (INRs). Our approach avoids latent variable modeling by\ntraining an INR--mapping coordinates to graphon values--to match empirical\nsubgraph counts (i.e., moments) from observed graphs. This direct estimation\nmechanism yields a polynomial-time solution and crucially sidesteps the\ncombinatorial complexity of Gromov-Wasserstein optimization. Building on\nfoundational results, we establish a theoretical guarantee: when the observed\nsubgraph motifs sufficiently represent those of the true graphon (a condition\nmet with sufficiently large or numerous graph samples), the estimated graphon\nachieves a provable upper bound in cut distance from the ground truth.\nAdditionally, we introduce MomentMixup, a data augmentation technique that\nperforms mixup in the moment space to enhance graphon-based learning. Our\ngraphon estimation method achieves strong empirical performance--demonstrating\nhigh accuracy on small graphs and superior computational efficiency on large\ngraphs--outperforming state-of-the-art scalable estimators in 75\\% of benchmark\nsettings and matching them in the remaining cases. Furthermore, MomentMixup\ndemonstrated improved graph classification accuracy on the majority of our\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684graphon\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5e76\u901a\u8fc7\u5339\u914d\u89c2\u5bdf\u5230\u7684\u5b50\u56fe\u8ba1\u6570\u6765\u8fdb\u884c\u76f4\u63a5\u6062\u590d\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86MomentMixup\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684graphon\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u7f51\u7edc\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u5206\u8fa8\u7387\u65e0\u5173\u8fd1\u4f3c\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u901a\u8fc7\u5339\u914d\u89c2\u5bdf\u56fe\u4e2d\u7684\u5b50\u56fe\u8ba1\u6570\uff08\u5373\u77e9\uff09\u6765\u76f4\u63a5\u6062\u590dgraphon\u503c\uff0c\u5e76\u5f15\u5165\u4e86MomentMixup\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u6240\u63d0\u51fa\u7684graphon\u4f30\u8ba1\u65b9\u6cd5\u572875%\u7684\u57fa\u51c6\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53ef\u6269\u5c55\u4f30\u8ba1\u5668\uff0c\u5728\u5176\u4f59\u60c5\u51b5\u4e0b\u4e5f\u4e0e\u4e4b\u76f8\u5f53\uff1bMomentMixup\u5728\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u56fe\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u53ef\u6269\u5c55\u7684graphon\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5339\u914d\u5b50\u56fe\u8ba1\u6570\u76f4\u63a5\u6062\u590dgraphon\uff0c\u5e76\u4e14\u907f\u514d\u4e86\u4f7f\u7528\u6f5c\u5728\u53d8\u91cf\u5efa\u6a21\u548cGromov-Wasserstein\u8ddd\u79bb\u8ba1\u7b97\u6240\u5e26\u6765\u7684\u590d\u6742\u6027\u3002\u6b64\u5916\uff0cMomentMixup\u6280\u672f\u63d0\u5347\u4e86\u57fa\u4e8egraphon\u7684\u5b66\u4e60\u6027\u80fd\u3002"}}
