<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 86]
- [cs.AI](#cs.AI) [总数: 11]
- [stat.ML](#stat.ML) [总数: 4]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag, Q. M. Jonathan Wu, Farhad Pourpanah*

**主要类别:** cs.LG

**AI概要:** 提出FSIGenZ框架，通过Model-Specific Attribute Scoring（MSAS）和Dual-Purpose Semantic Regularization（DPSR）策略减少对大规模特征合成的依赖，在使用更少合成特征的情况下实现具有竞争力的性能。


<details>
  <summary>更多</summary>
  
**动机:** 生成式零样本学习（ZSL）方法通常需要大量计算资源和广泛的合成数据，这偏离了原始ZSL假设。此外，传统方法将类级属性视为均匀存在，而实际上这些属性在实例级别上可能存在变化。

**方法:** 提出FSIGenZ框架，包含Model-Specific Attribute Scoring（MSAS），用于动态重新评分类别属性以反映实例级别的变异性；以及Dual-Purpose Semantic Regularization（DPSR），用于在训练语义感知对比分类器（SCC）时缓解数据不平衡问题。

**结果:** 在SUN、AwA2和CUB基准测试中，FSIGenZ使用远 fewer 合成特征实现了具有竞争力的性能。

**结论:** FSIGenZ通过减少对大规模特征合成的依赖，提供了一种更高效的生成式零样本学习方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Few-Shot+Inspired+Generative+Zero-Shot+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01026&send_immediately=true&force_search=false)

**原文摘要:** Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [2] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye, Wei Huang, Yifei Yu, Tianhe Ren, Zhongrui Wang, Xiaojuan Qi*

**主要类别:** cs.LG

**AI概要:** 大型语言模型（LLMs）尽管表现出色，但面临着巨大的计算和内存挑战。量化被提出作为一种有前途的解决方案，然而由于权重分布不友好和激活异常值的存在，其效果常受到量化误差的限制。为了解决这些问题，我们引入了DBellQuant，一种创新的训练后量化（PTQ）框架，可实现接近1位权重压缩和6位激活量化，同时性能下降最小。DBellQuant使用Learnable Transformation for Dual-Bell (LTDB)算法，将单峰权重分布转换为双峰形式以减少二值化误差，并应用逆变换来平滑激活。DBellQuant在激进的权重和激活量化下保持卓越的模型性能，树立了新的行业标杆。例如，在Wikitext2数据集上，DBellQuant在LLaMA2-13B模型上实现了6位激活量化的困惑度为14.39，显著优于BiLLM的21.35（无激活量化），突显了其在实际应用中压缩LLMs的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型尽管表现出色，但其部署面临计算和内存方面的巨大挑战。现有的量化方法虽然有助于缓解这些挑战，但因权重分布不友好及激活异常值导致的量化误差限制了其有效性。因此，需要一种更有效的量化方法来降低计算和内存需求，同时尽量减少性能损失。

**方法:** 提出了DBellQuant这一创新的训练后量化框架，采用Learnable Transformation for Dual-Bell (LTDB)算法，该算法通过将单峰权重分布转化为双峰形式以减少二值化误差，并利用逆变换来平滑激活。这种方法实现了接近1位的权重压缩和6位的激活量化。

**结果:** DBellQuant在多个测试中表现优异，尤其在Wikitext2数据集上，使用LLaMA2-13B模型时，6位激活量化的困惑度达到14.39，明显优于未进行激活量化的BiLLM的21.35。这表明DBellQuant能够在大幅度量化的同时保持模型性能。

**结论:** DBellQuant作为一种高效的训练后量化方法，不仅显著降低了大型语言模型的计算和内存需求，还通过最小化性能损失展示了其优越性。它为实际应用中的模型压缩提供了新的可能性，并树立了行业新标杆。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DBellQuant%3A+Breaking+the+Bell+with+Double-Bell+Transformation+for+LLMs+Post+Training+Binarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01027，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01027&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [3] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce, Martial Hebert, Basile Terver*

**主要类别:** cs.LG

**AI概要:** The paper explores non-contrastive self-supervised learning methods focusing on stop gradient and exponential moving average procedures, analyzing them from optimization and dynamical systems perspectives. It demonstrates that these techniques prevent representation collapse, contrasts their performance with direct objective minimization, and shows their equilibria are stable without trivial solutions.


<details>
  <summary>更多</summary>
  
**动机:** Non-contrastive approaches to self-supervised learning aim to minimize the discrepancy between embeddings of different views of data. Stop gradient and exponential moving average procedures are widely used to avoid representation collapse, but a deeper theoretical understanding of why they work is needed.

**方法:** The authors analyze stop gradient and exponential moving average procedures using optimization theory and dynamical systems theory. They compare these methods to directly minimizing the original objective function in both general and linear cases.

**结果:** In the general case, stop gradient and exponential moving average do not optimize the original objective but still avoid collapse. In the linear case, directly minimizing the original objective always leads to collapse. The limit points of these procedures are asymptotically stable equilibria.

**结论:** Stop gradient and exponential moving average procedures are effective in avoiding representation collapse in non-contrastive self-supervised learning. Their associated dynamical systems have stable equilibria, ensuring meaningful representations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dual+Perspectives+on+Non-Contrastive+Self-Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01028&send_immediately=true&force_search=false)

**原文摘要:** The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [4] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou, Yingli Zuo, Shichang Feng, Peng Wan, Qi Zhu, Daoqiang Zhang, Wei Shao*

**主要类别:** cs.LG

**AI概要:** PathCoT是一种新的零样本链式思维提示方法，将病理学专家知识融入多模态大语言模型的推理过程，并通过自评估缓解答案偏差。在PathMMU数据集上的实验表明了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态大语言模型在处理病理视觉推理任务时存在两个主要问题：缺乏领域特定信息导致模型表现不佳，以及链式思维推理中的额外步骤可能引入错误，导致答案偏离。

**方法:** 提出了一种名为PathCoT的新方法，它将病理学专家知识整合到多模态大语言模型的推理过程中，并包含自评估步骤以减少答案偏差。具体来说，PathCoT利用先验知识引导模型像病理学专家一样进行推理，并使用领域特定知识对图像进行全面分析。

**结果:** 在PathMMU数据集上的实验结果表明，PathCoT在病理视觉理解和推理方面具有有效性。

**结论:** PathCoT能够有效地解决当前多模态大语言模型在病理视觉推理任务中的不足，通过引入专家知识和自评估机制提高了模型的准确性和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PathCoT%3A+Chain-of-Thought+Prompting+for+Zero-shot+Pathology+Visual+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01029，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01029&send_immediately=true&force_search=false)

**原文摘要:** With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [5] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei, Mohammad Safarzadeh, Seyed Mohammad Jafar Sobhani*

**主要类别:** cs.LG

**AI概要:** 本研究利用四种机器学习算法（多层感知机、随机森林、线性回归和支持向量机）生成适用于甲烷燃料燃烧模拟的层流火焰面生成流形（FGM）库。通过对比评估，选择了经过超参数优化的多层感知机方法作为最佳模型，其准确率达到99.81%。


<details>
  <summary>更多</summary>
  
**动机:** 在化学表和火焰燃烧模型中，火焰面生成流形（FGM）因其精确性和物理表示而受到认可。然而，FGM的实际应用需要大量的内存资源，并且通常为特定燃料开发FGM库，然后在所有数值问题中使用这些库。因此，研究者希望利用机器学习技术来生成适用于甲烷燃料燃烧模拟的层流FGM库，以提高效率和准确性。

**方法:** 研究采用了四种机器学习算法：多层感知机（MLP）、随机森林（RF）、线性回归（LR）和支持向量机（SVM），基于对数据源、技术和数据驱动概念的理解，重新生成火焰面库。识别出七个合适的库用于构建训练机器学习模型的数据库。评估了每种方法的默认架构，最终选择MLP方法作为主要选择，并通过超参数调整进一步提升其性能。

**结果:** 通过对不同模型的评估，发现隐藏层和神经元的数量显著影响方法性能。经过超参数优化后的MLP模型包含四个隐藏层，分别具有10、15、20和25个神经元，该模型达到了99.81%的高准确率。

**结论:** 使用机器学习算法生成FGM库是可行且高效的。其中，多层感知机方法在经过超参数优化后表现最佳，可为甲烷燃料的燃烧模拟提供高度精确的FGM库。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Flamelet+Generated+Manifold+Models%3A+A+Machine+Learning+Performance+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01030，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01030&send_immediately=true&force_search=false)

**原文摘要:** In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [6] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu, Kijung Shin*

**主要类别:** cs.LG

**AI概要:** 将PyTorch几何学习框架移植到Gaudi-v2 HPUs的经验分享，包括核心工具集、教程和实例。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Nvidia的CUDA GPU在硬件领域占据主导地位，但像Intel的Gaudi Habana Processing Units (HPUs)这样的新兴加速器提供了具有竞争力的性能和能效。然而，使用非CUDA处理单元需要大量的工程工作和新颖的软件适配。

**方法:** 引入了一系列核心工具，恢复了Gaudi-v2 HPUs上的关键操作（如scatter、稀疏索引、k-最近邻等），并整理了16个指导教程和11个现实世界示例，包含故障诊断分析和详细解决方法。

**结果:** 这些贡献降低了研究者在非CUDA硬件上实验几何学习算法和模型的门槛，为后续优化和跨平台移植提供了基础。

**结论:** 本研究通过提供实用工具和示例，促进了在非CUDA硬件上进行几何学习的研究和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PyTorch-based+Geometric+Learning+with+Non-CUDA+Processing+Units%3A+Experiences+from+Intel+Gaudi-v2+HPUs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01031，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01031&send_immediately=true&force_search=false)

**原文摘要:** Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [7] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu, Hongbo Yang, Chen Zhao*

**主要类别:** cs.LG

**AI概要:** 提出了一种不确定性感知的多视角动态决策框架，用于组学数据分类，减少测试成本同时保持高诊断准确性。


<details>
  <summary>更多</summary>
  
**动机:** 高通量多组学技术虽然对阐明疾病机制和早期诊断非常有价值，但其高昂的成本带来了显著的经济负担，并可能导致不必要的资源消耗。

**方法:** 在单组学层面，改进神经网络的激活函数以生成狄利克雷分布参数，使用主观逻辑量化分类结果的信任质量和不确定性质量；在多组学层面，采用基于Dempster-Shafer理论的融合策略整合异构模态，并应用动态决策机制逐步引入组学数据，直到达到预定义的置信度阈值或用尽所有数据源。

**结果:** 在四个基准多组学数据集（ROSMAP、LGG、BRCA和KIPAN）上进行评估，其中三个数据集中超过50%的病例仅使用单一组学模态即可实现准确分类，有效减少了冗余测试。

**结论:** 该方法在减少测试成本的同时，维持了与全组学模型相当的诊断性能，并保留了重要的生物见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Uncertainty-Aware+Dynamic+Decision+Framework+for+Progressive+Multi-Omics+Integration+in+Classification+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01032&send_immediately=true&force_search=false)

**原文摘要:** Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [8] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal, Mansour Essgaer, Hend M. Farkash, Zulaiha Ali Othman*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种基于历史数据的电力负荷、发电量和赤字预测方法，并通过多种时间序列模型比较，发现LSTM表现最佳。优化后的LSTM框架整合了温度和湿度等外生变量，为政策制定者和电网运营商提供了实用见解。


<details>
  <summary>更多</summary>
  
**动机:** 准确的电力预测对电网稳定和能源规划至关重要，特别是在利比亚班加西等经常发生负荷削减、发电赤字和基础设施限制的地方。

**方法:** 利用2019年（不稳定的一年）和2023年（较为稳定的一年）的历史数据，应用多种时间序列模型（如ARIMA、季节性ARIMA、动态回归ARIMA、指数平滑、极端梯度提升和LSTM神经网络）进行2025年的电力负荷、发电量和赤字预测。通过缺失值填补、异常值平滑和对数转换增强数据集，并使用均方误差、均方根误差、平均绝对误差和平均绝对百分比误差评估性能。

**结果:** LSTM在所有模型中表现最佳，能够很好地模拟非平稳和季节性模式。优化后的LSTM框架整合了温度和湿度等外生因素，表现出强大的多指标预测能力。

**结论:** 本研究提供了一个优化的LSTM框架，该框架在数据稀缺和波动较大的地区为政策制定者和电网运营商提供了实用的见解，有助于主动负荷管理和资源规划。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-driven+Insights+for+Informed+Decision-Making%3A+Applying+LSTM+Networks+for+Robust+Electricity+Forecasting+in+Libya，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01034&send_immediately=true&force_search=false)

**原文摘要:** Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [9] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin, Isaac Chuang*

**主要类别:** cs.LG

**AI概要:** 这篇论文详细阐述了Ziyin等人（2025）给出的关于嵌入式深度线性网络模型（EDLN）的“完美”柏拉图表示假设（PRH）的证明。研究发现，使用SGD训练的不同宽度和深度的两个EDLN，即使在不同的数据上训练，也会变得完全柏拉图化，即每一对可能的层都会学到相同的表示，最多相差一个旋转。由于损失函数的大多数全局最小值都不是柏拉图式的，因此SGD只找到完美的柏拉图解是相当非凡的。该证明还提出了至少六种破坏PRH的方式，并表明在EDLN模型中，柏拉图表示的出现与渐进锐化现象的原因相同。这暗示着这两个看似不相关的深度学习现象可以有共同的原因。总体而言，理论和证明强调了理解由SGD训练不可逆性产生的“熵力”及其在表征学习中的作用的重要性。本文的目标是具有指导意义并避免冗长的技术细节。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望深入解释Ziyin等人提出的关于嵌入式深度线性网络模型（EDLN）的“完美”柏拉图表示假设（PRH）的证明，以帮助理解为什么使用随机梯度下降（SGD）训练的EDLN会表现出特定的表征学习特性，以及这些特性背后更深层次的原因。此外，研究者希望通过分析PRH的成立条件和潜在破坏方式，揭示表征学习过程中“熵力”的重要性。

**方法:** 作者通过数学推导和理论分析，详细阐述了Ziyin等人的证明过程，展示了不同宽度和深度的EDLN在SGD训练下如何达到柏拉图式的表示一致性。他们还探讨了导致这种一致性的原因，并将这一现象与渐进锐化现象联系起来。此外，作者指出了至少六种可能导致PRH失效的情况。

**结果:** 研究表明，在使用SGD训练EDLN时，不同网络结构和数据集条件下仍能实现柏拉图式的表示一致性。然而，这种一致性并非必然发生，其形成依赖于特定的训练机制。同时，研究揭示了柏拉图表示和渐进锐化的共同成因，为理解深度学习中的表征学习提供了新的视角。

**结论:** 本研究不仅详细解释了EDLN模型中“完美”柏拉图表示假设的证明，还揭示了SGD训练过程中不可逆性所产生的“熵力”对表征学习的重要影响。这些发现有助于进一步理解深度学习模型的训练动态和表征学习的本质。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Proof+of+a+perfect+platonic+representation+hypothesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01098，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01098&send_immediately=true&force_search=false)

**原文摘要:** In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [10] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao, Haotian Lyu, Yike Peng, Aijia Sun, Feng Jiang, Xinyue Han*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了在混合图神经网络(GNN)和大语言模型(LLM)的推荐系统(ReS)中优化推理延迟和训练效率的方法。实验表明，通过硬件加速(FPGA, DeepSpeed)和参数高效调优(LoRA)，可以显著提高准确率并降低训练时间。未来工作将涉及联邦学习和高级融合架构以提升可扩展性和隐私保护。


<details>
  <summary>更多</summary>
  
**动机:** 在线服务的不断涌现要求推荐系统具备高速度和高效率，同时能够处理复杂的用户-项目交互。这促使研究者关注如何优化混合GNN-LLM推荐系统的推理延迟和训练效率。

**方法:** 采用混合GNN-LLM架构，并应用多种优化策略（量化、LoRA、蒸馏）以及硬件加速技术（FPGA, DeepSpeed）。

**结果:** 最优配置（Hybrid + FPGA + DeepSpeed）提高了13.6%的准确率（NDCG@10: 0.75），且延迟保持在40-60ms；LoRA使训练时间减少了66%（从基线的约11.4小时降至3.8小时）。

**结论:** 软硬件协同设计和参数高效调优使得混合模型优于单独实施的GNN或LLM方法。推荐使用FPGA和LoRA进行实时部署，未来研究应考虑联邦学习和更先进的融合架构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Research+on+Low-Latency+Inference+and+Training+Efficiency+Optimization+for+Graph+Neural+Network+and+Large+Language+Model-Based+Recommendation+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01035，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01035&send_immediately=true&force_search=false)

**原文摘要:** The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [11] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy, Hugues Talbot, Bertrand Thirion*

**主要类别:** cs.LG

**AI概要:** 尽管生成模型近年来取得了显著进展，但由于无法可靠评估样本质量，其在关键应用中的使用受到阻碍。本文提出了两个新指标：Clipped Density 和 Clipped Coverage，通过裁剪单个样本贡献和最近邻球体半径来防止分布外样本对聚合值产生偏差，并表现出线性评分退化特性，可直观解释为优质样本的比例。实验表明，这两个指标在稳健性、敏感性和可解释性方面优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 生成模型在关键应用中的推广受限于其无法可靠地评估样本质量，而当前的质量度量标准往往缺乏可靠性、可解释性以及对外部异常点的鲁棒性。

**方法:** 提出两个新的度量指标：Clipped Density 和 Clipped Coverage。通过裁剪单个样本贡献及最近邻球体半径，防止分布外样本影响聚合值；并进行解析和经验校准以确保线性评分退化特性。

**结果:** 在合成和真实数据集上的广泛实验表明，Clipped Density 和 Clipped Coverage 在稳健性、敏感性和可解释性方面优于现有方法。

**结论:** Clipped Density 和 Clipped Coverage 是用于评估生成模型样本质量的有效且可靠的指标，具有更高的鲁棒性和可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhanced+Generative+Model+Evaluation+with+Clipped+Density+and+Coverage，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01761&send_immediately=true&force_search=false)

**原文摘要:** Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [12] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang, Sirui Li, Yining Ma, Cathy Wu*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种名为First-Segment-Then-Aggregate (FSTA) 的分解技术，结合Learning-to-Segment (L2Seg) 神经框架，以加速车辆路径问题（VRP）的迭代求解器。实验结果表明，L2Seg能将现有最先进的迭代求解器提速高达7倍。


<details>
  <summary>更多</summary>
  
**动机:** 现有的迭代搜索启发式算法在解决车辆路径问题（VRPs）方面表现出色，但存在冗余计算的问题，尤其是在大规模VRP中，因为解决方案的大部分在搜索迭代过程中保持稳定不变。

**方法:** 论文引入了First-Segment-Then-Aggregate (FSTA) 技术来加速迭代求解器。该技术通过保留稳定解段、将节点聚合成超节点，并仅对不稳定部分进行搜索来减少冗余计算。为了解决如何识别应聚合的段的问题，论文提出了Learning-to-Segment (L2Seg) 神经框架，用于智能区分潜在的稳定和不稳定部分。此外，还介绍了三种L2Seg变体：非自回归（全局全面但局部不加区分）、自回归（局部精细但全局不足）及其协同作用。

**结果:** 在CVRP和VRPTW上的实证结果表明，L2Seg可以将最先进的迭代求解器的速度提升至原来的7倍。深入分析显示，非自回归和自回归的协同作用通过结合其互补优势实现了最佳性能。

**结论:** L2Seg是一个灵活的框架，与传统、基于学习和混合求解器兼容，并支持广泛的VRP类别，显著提升了求解效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Segment+for+Vehicle+Routing+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01037&send_immediately=true&force_search=false)

**原文摘要:** Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [13] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios*

**主要类别:** cs.LG

**AI概要:** Low-Rank Adapters (LoRAs) allow efficient updates of Large Language Models (LLMs). This paper proposes a method for LoRA fine-tuning on CPUs, making it accessible for users with limited computational resources. By learning a meta-operator that combines pre-trained adapters, this approach offers a practical alternative to GPU-based fine-tuning.


<details>
  <summary>更多</summary>
  
**动机:** 尽管LoRAs在大型语言模型的微调中实现了参数高效的更新，但其广泛应用仍受限于对GPU训练的依赖。为了使计算资源有限的用户（尤其是仅使用标准笔记本电脑CPU的用户）能够进行LoRA微调，本文提出了一个基于理论的方法。

**方法:** 该方法通过利用Mistral-7B-Instruct-v0.2模型的大规模预训练适配器库，学习一个元算子，将任何输入数据集（表示为概率分布）映射到一组LoRA权重。与传统的梯度更新不同，此方法通过轻量级组合现有的LoRAs直接在CPU上构建适配器。

**结果:** 虽然生成的适配器性能不如GPU训练的对应物，但它们在下游任务中始终优于基础Mistral模型，提供了一个实用且易于获取的传统GPU微调替代方案。

**结论:** 本研究提出了一种针对计算资源有限用户的LoRA微调方法，使得仅使用CPU的用户也可以进行高效、可行的模型微调，推动了LoRA技术的更广泛应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LoRA+Fine-Tuning+Without+GPUs%3A+A+CPU-Efficient+Meta-Generation+Framework+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01806，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01806&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [14] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar, Wilhelm Louw, Kelly Cohen*

**主要类别:** cs.LG

**AI概要:** We propose a reinforcement learning (RL) approach for training neuro-fuzzy controllers using Proximal Policy Optimization (PPO). It was found that PPO-trained fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000 updates, showcasing less variance than prior DQN-based methods during training and overall faster convergence.


<details>
  <summary>更多</summary>
  
**动机:** To explore the potential of using Proximal Policy Optimization (PPO) in training neuro-fuzzy controllers as an alternative to Deep Q-Learning applied to Adaptive Neuro-Fuzzy Inference Systems (ANFIS).

**方法:** Replacing the off-policy value-based framework with a stable on-policy actor-critic loop using PPO to train neuro-fuzzy controllers. Evaluated in the CartPole-v1 environment with multiple random seeds.

**结果:** PPO-trained fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000 updates, showing less variance and faster convergence compared to DQN-based methods.

**结论:** PPO offers a promising pathway for training explainable neuro-fuzzy controllers in reinforcement learning tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On-Policy+Optimization+of+ANFIS+Policies+Using+Proximal+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01039&send_immediately=true&force_search=false)

**原文摘要:** We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [15] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li, Daohan Lu, Polina Kirichenko, Shikai Qiu, Tim G. J. Rudner, C. Bayan Bruss, Andrew Gordon Wilson*

**主要类别:** cs.LG

**AI概要:** 这篇论文重新审视了依赖预测不确定性或监督模型特征的OOD检测方法，指出这些方法根本上回答了错误的问题，并且展示了基于不确定性和特征的方法在OOD检测中的不可约误差和无效场景。此外，一些改进OOD检测的干预措施也无法解决目标上的基本错位问题，无监督密度估计和生成模型也有其根本限制。


<details>
  <summary>更多</summary>
  
**动机:** 作者观察到现有的OOD检测方法存在根本性问题，即这些方法错误地将高不确定性或特征空间距离与OOD等同起来，而实际上分类器只能对已知类别的数据进行有效分类，对于未知类别可能产生错误分类。

**方法:** 作者通过分析基于不确定性和特征的方法在OOD检测中的表现，揭示了它们的病理表现（如高不确定性等同于OOD、远特征空间距离等同于OOD），并探讨了各种改进方法（如特征-logit混合方法、模型和数据规模扩展、表征认知不确定性、异常值暴露）以及无监督密度估计和生成模型在OOD检测中的局限性。

**结果:** 研究发现基于不确定性和特征的方法存在不可约误差，并在常见设置中无效；同时，尝试改进OOD检测的各种干预措施也未能解决目标上的基本错位问题，无监督密度估计和生成模型同样有其根本限制。

**结论:** 作者得出结论：当前流行的OOD检测方法存在根本性缺陷，无法通过简单修复解决，需要重新思考OOD检测的目标和方法设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Out-of-Distribution+Detection+Methods+Answer+the+Wrong+Questions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01831，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01831&send_immediately=true&force_search=false)

**原文摘要:** To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [16] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia, Max Neuwinger, Lin Xiao*

**主要类别:** cs.LG

**AI概要:** Clifford Neural Layers通过在神经网络中引入Clifford代数来改进PDE建模。本项目专注于优化适用于单核CPU性能的2/3D Clifford卷积层和多向量激活层的推理。测试结果显示，对于相对较大的数据+网络规模（>L2缓存），我们的实现比标准PyTorch实现快30%。我们在https://github.com/egretwAlker/c-opt-clifford-layers开源了代码库。


<details>
  <summary>更多</summary>
  
**动机:** 将Clifford代数引入神经网络以改进偏微分方程（PDE）建模，并且为了提高Clifford卷积层和多向量激活层在单核CPU上的性能，需要对这些层进行优化。

**方法:** 聚焦于优化2/3D Clifford卷积层和多向量激活层的推理过程，特别是针对单核CPU的性能表现。通过实际的网络块测试，涉及Clifford卷积层和多向量激活层，比较自定义实现与标准PyTorch实现的性能差异。

**结果:** 对于较大规模的数据和网络（超过L2缓存大小），所提出的实现方法比标准PyTorch实现快30%。

**结论:** 通过优化Clifford卷积层和多向量激活层，可以在单核CPU上获得显著的性能提升，并且相关代码已开源供社区使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast+Clifford+Neural+Layers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01040&send_immediately=true&force_search=false)

**原文摘要:** Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [17] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li, Wen Wu, Shaohua Wu, Songge Zhang, Ye Wang, Xuemin, Shen*

**主要类别:** cs.LG

**AI概要:** Split learning (SL) 是一种计算高效的AI模型训练方法，可以减轻设备端的计算工作量。然而，复杂的AI模型架构需要高计算复杂度来找到最优模型分割。本文将任意AI模型表示为有向无环图(DAG)，并重新定义最优模型分割问题为最小s-t切割搜索问题。提出了一种快速DAG-based模型分割算法，通过最大流方法确定最优模型分割，并针对具有块结构的AI模型提出了块级模型分割算法以简化计算。实验结果表明，所提出的算法可以在毫秒内确定最优模型分割，并在动态边缘网络中比现有基准减少24.62%-38.95%的训练延迟。


<details>
  <summary>更多</summary>
  
**动机:** 复杂AI模型架构的最优分割面临高计算复杂度挑战，需要更高效的算法解决这一问题。

**方法:** 1. 将AI模型表示为DAG。
2. 重新定义最优模型分割问题为最小s-t切割搜索问题。
3. 提出基于DAG的快速模型分割算法，利用最大流方法。
4. 针对块结构AI模型，提出块级模型分割算法，简化计算。

**结果:** - 算法可以在毫秒内确定最优模型分割。
- 在动态边缘网络中，相比现有基准，训练延迟减少了24.62%-38.95%。

**结论:** 所提出的DAG-based和块级模型分割算法能够高效地确定最优模型分割，显著降低训练延迟，理论分析表明算法是最佳的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast+AI+Model+Splitting+over+Edge+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01041，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01041&send_immediately=true&force_search=false)

**原文摘要:** Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [18] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski, Agnieszka Jastrzębska*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的方法，通过蒙特卡洛树搜索实现神经网络的动态扩展和收缩，验证了其在视觉和时间序列数据集上的有效性，特别是在多元时间序列分类任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 构建数据驱动的神经网络模型是人工智能领域中的核心问题之一。传统方法通常假设固定的架构并训练权重，而更先进的方法需要同时优化权重和模型架构。

**方法:** 提出了一种基于蒙特卡洛树搜索的方法，该方法允许在训练过程中动态地扩展和收缩神经网络架构。决策机制通过模拟网络行为来比较多个候选架构更改，并选择最佳方案。

**结果:** 实验结果表明，该方法在视觉模式识别和多元时间序列分类任务中表现优异，特别是对于多元时间序列分类任务，其动态适应能力显著提高了性能。

**结论:** 所提出的方法通过动态调整神经网络架构，展现了强大的鲁棒性和适应性，为数据驱动的神经网络模型构建提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Classification+with+Dynamically+Growing+and+Shrinking+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01043&send_immediately=true&force_search=false)

**原文摘要:** The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [19] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu, Wei Tang, Jinpei Han, Veer Sangha, Fenglin Liu, Shreyank N Gowda, Antonio H. Ribeiro, Patrick Schwab, Kim Branson, Lei Clifton, Antonio Luiz P. Ribeiro, Zhangdaihong Liu, David A. Clifton*

**主要类别:** cs.LG

**AI概要:** 提出了一种心脏感知基础模型（CSFM），利用先进的变压器架构和生成性掩码预训练策略，从大量的异构健康记录中学习统一的表示。该模型在多模态数据集上进行预训练，包括来自约170万个体的心脏信号及其对应的临床或机器生成的文本报告。结果表明，CSFM在诊断任务、人口统计信息识别、生命体征测量、临床结果预测和ECG问答等方面的表现优于传统的单一模式单一任务方法。此外，CSFM在不同的心电图导联配置以及仅有心电图、仅有PPG或两者结合的情况下表现出稳健的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统深度学习方法分析心脏生物信号时依赖于同质数据集和静态定制模型，限制了其在不同临床环境和采集协议中的鲁棒性和泛化能力。

**方法:** 构建了一个心脏感知基础模型（CSFM），使用先进的变压器架构和生成性掩码预训练策略，从大量异构健康记录中学习统一表示。模型在多模态数据集上进行预训练，这些数据集整合了多个大规模数据集，包含心脏信号及相应的临床或机器生成的文本报告。

**结果:** CSFM在多种任务中表现出色，包括诊断、人口统计信息识别、生命体征测量、临床结果预测和ECG问答等，并且在不同的心电图导联配置以及仅有心电图、仅有PPG或两者结合的情况下表现出稳健的性能。

**结论:** CSFM作为一种多功能和可扩展的解决方案，具有全面心脏监测的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sensing+Cardiac+Health+Across+Scenarios+and+Devices%3A+A+Multi-Modal+Foundation+Model+Pretrained+on+Heterogeneous+Data+from+1.7+Million+Individuals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01045，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01045&send_immediately=true&force_search=false)

**原文摘要:** Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [20] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett, Umme Mahbuba Nabila, Majdi I. Radaideh*

**主要类别:** cs.LG

**AI概要:** 提出了一种变分数字孪生(VDT)框架，通过增加单一贝叶斯输出层来增强标准神经架构，并结合高效的更新算法，使模型在提供实时洞察的同时具备校准的不确定性估计。该框架在能源领域的多个问题中表现出色，显著减少了实验数量、训练时间和误差，同时提高了模型的可靠性和鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于数字孪生(DT)的研究存在一些不足：信息交换框架不清晰、缺乏实时实现的关键特性以及对模型不确定性关注有限。为了解决这些差距，提出了变分数字孪生(VDT)框架。

**方法:** VDT框架通过在标准神经架构中添加单一贝叶斯输出层，结合一种新颖的VDT更新算法，能够在普通GPU上快速更新模型，同时生成校准的不确定性边界。

**结果:** VDT框架在四个能源领域问题中的表现如下：1) 关键热通量预测中，不确定性驱动的主动学习方法比随机采样减少了47%的实验和三分之一的训练时间；2) 三年期可再生能源生成双胞胎保持太阳能输出R2 > 0.95，并通过每月更新控制风力预测误差增长；3) 核反应堆瞬态冷却双胞胎在50%传感器丢失的情况下仍能准确重建热电偶信号；4) 物理信息锂离子电池双胞胎在接近寿命终点时降低了一个数量级的电压均方误差并调整其置信区间。

**结论:** 将适度的贝叶斯增强与高效的更新方案相结合，可以使传统的代理模型转变为具有不确定性感知、数据高效且计算可行的数字孪生，为工业和科学能源系统中的可靠模型铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Digital+Twins，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01047，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01047&send_immediately=true&force_search=false)

**原文摘要:** While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [21] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas, Afrânio José de Melo Junior, Celso José Munaro, Cláudio Benevenuto de Campos Lima, Eduardo Toledo de Lima Junior, Felipe Muntzberg Barrocas, Flávio Miguel Varejão, Guilherme Fidelis Peixer, Igor de Melo Nery Oliveira, Jader Riso Barbosa Jr., Jaime Andrés Lozano Cadena, Jean Carlos Dias de Araújo, João Neuenschwander Escosteguy Carneiro, Lucas Gouveia Omena Lopes, Lucas Pereira de Gouveia, Mateus de Araujo Fernandes, Matheus Lima Scramignon, Patrick Marques Ciarelli, Rodrigo Castello Branco, Rogério Leite Alves Pinto*

**主要类别:** cs.LG

**AI概要:** 论文介绍了当前公开可用版本的3W数据集，该数据集包含结构修改和额外标注的数据，旨在鼓励和支持3W社区及新用户改进先前已发表的结果，并开发新的稳健方法、数字产品和服务，以提前检测油井中的不良事件。


<details>
  <summary>更多</summary>
  
**动机:** 在石油行业中，油井中的不良事件可能引发经济损失、环境事故和人员伤亡。基于人工智能和机器学习的早期检测解决方案在多个行业中被证明具有重要价值。由于缺乏与油井不良事件相关的公开数据集，2019年Petrobras开发并发布了第一个版本的3W数据集。

**方法:** 3W数据集是一组由专家标注的多元时间序列数据，自发布以来，该数据集通过协作开发得到了改进，成为许多相关研究的基础参考。本文描述了当前公开版本的3W数据集，包括其结构修改和新增加的标注数据。

**结果:** 该数据集的更新版本提供了更详细的数据描述，有助于社区用户改进先前的研究成果，并开发新的方法和技术来提前检测油井中的不良事件。

**结论:** 作者希望通过对最新版3W数据集的详细介绍，能够促进社区内更广泛的研究活动，并推动开发出更为精确和及时的不良事件检测技术，从而为石油行业提供更好的支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是3W+Dataset+2.0.0%3A+a+realistic+and+public+dataset+with+rare+undesirable+real+events+in+oil+wells，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01048&send_immediately=true&force_search=false)

**原文摘要:** In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [22] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu, Yibo Zhao, Jiapeng Zhu, Wenming Shao, Bo Pang, Zhao Zhang, Xiang Li*

**主要类别:** cs.LG

**AI概要:** This paper proposes a two-stage training framework to optimize detoxification methods for toxic content on social media, improving generalization, reducing reliance on annotated data, and demonstrating state-of-the-art performance.


<details>
  <summary>更多</summary>
  
**动机:** The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics.

**方法:** A two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. First perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization.

**结果:** Experimental results demonstrate that the method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data.

**结论:** Proposed a two-stage training framework which addresses the challenges in detoxification methods, leading to state-of-the-art performance with better generalization and less reliance on annotated data. Code is available at: https://anonymous.4open.science/r/Detoxification-of-Text-725F/

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Text+Detoxification%3A+Data+Efficiency%2C+Semantic+Preservation+and+Model+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01050&send_immediately=true&force_search=false)

**原文摘要:** The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [23] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的能量函数，基于密集的Hopfield网络框架，通过高阶交互实现指数级存储容量，适用于长序列记忆。引入时间核$K(m, k)$以结合时间依赖性，从而能够有效地顺序检索扩展序列中的模式。该技术在电影帧的存储和顺序检索中取得成功，并在现代Transformer架构中有广泛的应用前景。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Hopfield网络模型在处理长序列记忆时存在局限性，特别是在高效存储和检索长时间序列数据方面。为了克服这些限制并增强对长时间依赖性的处理能力，需要一种新的方法来改进存储和检索机制。

**方法:** 作者提出了一个新颖的能量函数，利用密集Hopfield网络的框架，通过高阶相互作用实现指数级存储容量。进一步地，他们引入了一个时间核$K(m, k)$，用于结合时间依赖性，使得模型能够更有效地进行序列模式的检索。此外，还展示了该技术在存储和检索电影帧方面的应用效果。

**结果:** 该方法成功实现了电影帧的存储和顺序检索，验证了其在高维数据上的有效性。同时，该技术可以应用于现代Transformer架构中，包括提高长序列建模效率、增强记忆能力、改进带时间偏差的注意力机制以及更好地处理时间序列数据中的长期依赖性。

**结论:** 所提出的模型为解决Transformer在长上下文任务中的局限性提供了一个有希望的方向，具有在自然语言处理、预测等领域的潜在应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Long-Sequence+Memory+with+Temporal+Kernels+and+Dense+Hopfield+Functionals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01052&send_immediately=true&force_search=false)

**原文摘要:** In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [24] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian, Linda Hung, Daniel Schweigert, Santosh Suram, Weike Ye*

**主要类别:** cs.LG

**AI概要:** 近期材料发现的进步主要由基于结构的模型驱动，特别是使用晶体图的模型。然而，这些模型在实际应用中并不实用，因为原子结构通常未知或难以获取。本文提出了一种可扩展的多模态框架，直接从元素组成和X射线衍射（XRD）数据中学习，而无需晶体结构输入。该框架集成了特定模态编码器与交叉注意力融合模块，并在包含500万样本的Alexandria数据集上进行训练。提出了掩码XRD建模（MXM），并将其与对比对齐作为自监督预训练策略。预训练带来了更快的收敛速度（最高加速4.2倍），提高了准确性和表示质量。此外，多模态性能随着数据集规模的增加比单模态基线更有利，特别是在大规模数据情况下效果更显著。本文结果为材料科学提供了无结构、实验基础的模型路径。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于结构的模型在计算数据集上虽然有效，但在实际应用中，由于原子结构常常未知或难以获得，导致这些模型不切实际。因此需要一种新的方法，可以不依赖于晶体结构输入而进行材料发现。

**方法:** 提出了一种可扩展的多模态框架，利用元素组成和X射线衍射（XRD）数据进行学习，无需晶体结构输入。该框架包括特定模态编码器和交叉注意力融合模块，并在Alexandria数据集上进行训练。同时，引入了掩码XRD建模（MXM）和对比对齐作为自监督预训练策略。

**结果:** 预训练使模型收敛速度提升高达4.2倍，并且提高了准确性和表示质量。多模态模型的性能随着数据集规模的增长优于单模态基线模型，特别是在大数据规模下表现更佳。

**结论:** 本研究为材料科学提供了一条无结构、实验基础的模型路径，展示了多模态模型在材料发现中的潜力和优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是XxaCT-NN%3A+Structure+Agnostic+Multimodal+Learning+for+Materials+Science，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01054&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [25] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng, Lu Gao, Feng Hong, Jingran Sun*

**主要类别:** cs.LG

**AI概要:** 洪水对道路铺装的损害显著，研究通过分析20年的数据和使用XAI技术揭示了洪水影响铺装退化的机制，强调需要采取积极的洪水缓解策略。


<details>
  <summary>更多</summary>
  
**动机:** 探究洪水事件如何影响铺装退化，特别是通过国际平整度指数（IRI）来衡量铺装粗糙度的变化。

**方法:** 利用TxDOT的PMIS数据库中20年的铺装状况数据与洪水事件数据结合，进行统计分析比较洪水前后IRI值，并计算受洪水影响的退化率；同时应用可解释的人工智能（XAI）技术如SHAP和LIME评估洪水对铺装性能的影响。

**结果:** 受洪水影响的铺装比未受洪水影响的部分表现出更快的粗糙度增加。

**结论:** 需要包括改进排水系统、抗洪材料和预防性维护在内的主动洪水缓解策略，以增强易受洪水影响区域的铺装弹性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Pavement+Deterioration+Rates+Due+to+Flooding+Events+Using+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01056，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01056&send_immediately=true&force_search=false)

**原文摘要:** Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [26] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan, Yuqin Xia, Jun Li, Karl Jenkins*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于深度卷积神经网络的智能优化系统，用于根据机翼坐标生成高质量的网格。通过Loop2Net生成器和损失函数，结合惩罚项，实现了高效的网格生成和优化。


<details>
  <summary>更多</summary>
  
**动机:** 提高网格质量对于许多工程应用至关重要，传统的网格生成方法可能效率较低或不够精确，因此需要一种创新的方法来实现高效的网格生成和优化。

**方法:** 使用深度卷积神经网络架构构建智能优化系统，核心是Loop2Net生成器和损失函数，该方法根据给定的机翼坐标预测网格，并通过两个关键损失函数不断优化模型性能。

**结果:** 模型的性能在训练过程中通过两个关键损失函数得到了持续优化，并通过添加惩罚项最终达到了网格生成的目标。

**结论:** 提出了一种基于深度卷积神经网络架构的创新智能优化系统，用于网格生成和优化。通过Loop2Net生成器和损失函数，根据给定的机翼坐标预测网格，并通过添加惩罚项最终实现了网格生成的目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Loop2Net%3A+Data-Driven+Generation+and+Optimization+of+Airfoil+CFD+Meshes+from+Sparse+Boundary+Coordinates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01057&send_immediately=true&force_search=false)

**原文摘要:** In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [27] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

**主要类别:** cs.LG

**AI概要:** 研究优化了最先进的基础模型以预测高性能机器学习服务中的稀疏或尖峰生产停机事件，通过与经典随机预测模型对比分析得出误差，并使用优化参数估计特定根本原因的全年停机统计，误差小于6%。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列预测模型在各种实际应用中表现出色，但尚未被用于预测稀少、尖峰事件（例如高性能量子学习服务的生产停机）。这些事件由于属于极端情况，因此具有挑战性。

**方法:** 优化一个最先进的基础模型来预测稀疏或尖峰生产停机事件；将该基础模型与经典随机预测模型（如移动平均和自回归模型）进行比较评估预测误差；识别目标数据中的关键模式并分析每种模型对这些事件的表现。

**结果:** 基础模型能够更好地跟踪目标数据中的关键模式；通过对优化参数模型的使用，可以估计特定根本原因全年停机统计数据，且值误差小于6%。

**结论:** 优化后的基础模型在预测稀少、尖峰事件方面表现优于经典随机预测模型，误差较小，适合用于此类极端事件的预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluation+of+a+Foundational+Model+and+Stochastic+Models+for+Forecasting+Sporadic+or+Spiky+Production+Outages+of+High-Performance+Machine+Learning+Services，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01067，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01067&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [28] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

**主要类别:** cs.LG

**AI概要:** 本研究利用惯性测量单元（IMU）数据集开发了可解释的人工智能方法，用于帕金森病中步态冻结（FOG）的早期检测和预测。通过使用多种机器学习模型和联邦学习框架，实现了高达99%的分类准确率，并发现时间是区分步态模式的最关键因素。


<details>
  <summary>更多</summary>
  
**动机:** 步态冻结（FOG）是帕金森病患者的常见症状，早期检测和预测FOG对于改善患者生活质量至关重要。然而，传统的检测方法可能存在不足，因此需要开发更高效、可解释的AI方法来解决这一问题。

**方法:** 研究采用了IMU数据集并使用多种机器学习模型（如CatBoost、XGBoost、Extra Trees等）进行FOG事件的分类。其中，Stacking集成模型表现最佳，超越了混合双向GRU模型。此外，研究还引入了联邦学习技术，通过在本地设备上训练模型并在中央服务器上聚合，采用Conv1D + LSTM混合架构提升预测能力。

**结果:** Stacking集成模型达到了接近99%的分类准确率，显著优于其他模型。SHAP分析表明，时间是影响步态模式分类的最重要特征。同时，联邦学习框架提高了模型的预测性能并保护了用户隐私。

**结论:** 本研究表明，结合可解释AI方法与联邦学习可以有效实现FOG的早期检测和预测，为帕金森病患者提供了潜在的临床应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prediction+of+Freezing+of+Gait+in+Parkinsons+Disease+using+Explainable+AI+and+Federated+Deep+Learning+for+Wearable+Sensors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01068，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01068&send_immediately=true&force_search=false)

**原文摘要:** This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [29] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的基于旋转采样的3D编码模块，通过计算SO(3)旋转群的期望值实现近似旋转不变性，并通过后对齐策略实现严格不变性。该方法在QM9和C10数据集上表现出更高的预测准确性、鲁棒性和泛化性能，同时保持较低的计算复杂度和增强的可解释性，为药物发现和材料设计中的3D分子信息处理提供了有希望的方向。


<details>
  <summary>更多</summary>
  
**动机:** 传统的图表示方法难以有效编码分子的固有3D空间结构，因为分子在3D空间中的方向引入了显著的变化，限制了模型的泛化能力和鲁棒性。现有的方法主要集中在旋转不变和旋转等变方法上，但这些方法要么依赖于先验知识且泛化能力不足，要么计算成本过高。

**方法:** 提出了一种新的插件式3D编码模块，利用旋转采样技术。通过计算SO(3)旋转群的期望值，该方法自然实现了近似的旋转不变性。此外，还引入了一种精心设计的后对齐策略，可以在不损害性能的情况下实现严格的不变性。

**结果:** 在QM9和C10数据集上的实验评估表明，与现有方法相比，该方法具有更高的预测准确性、鲁棒性和泛化性能。同时，该方法保持了低计算复杂度和增强的可解释性。

**结论:** 所提出的基于旋转采样的3D编码模块提供了一种高效且有效的处理3D分子信息的方法，适用于药物发现和材料设计领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rotational+Sampling%3A+A+Plug-and-Play+Encoder+for+Rotation-Invariant+3D+Molecular+GNNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01073，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01073&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [30] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani, Valentine Anantharaj, Sandro Fiore*

**主要类别:** cs.LG

**AI概要:** 随着对大规模AI模型需求的增长，优化其训练以平衡计算效率、执行时间、准确性和能耗成为了一个关键的多维挑战。通过利用数据来源信息（provenance），研究人员和工程师可以深入了解资源使用模式、识别低效之处，并确保AI开发工作流的可重复性和责任性。为此，我们引入了yProv4ML库，该工具以JSON格式收集符合W3C PROV和ProvML标准的数据来源信息。yProv4ML注重灵活性和可扩展性，允许用户通过插件集成额外的数据收集工具，并且完全集成了yProv框架，支持与工作流管理系统一起运行的任务的高层配对。


<details>
  <summary>更多</summary>
  
**动机:** 为了应对在分布式资源上高效扩展大型AI模型的挑战，特别是以节能的方式优化模型训练过程中的计算效率、执行时间、准确性和能耗。同时，通过数据来源信息（provenance）提高资源使用透明度、识别低效点并确保AI开发流程的可重复性和责任性。

**方法:** 引入yProv4ML库，该工具以JSON格式收集符合W3C PROV和ProvML标准的数据来源信息。yProv4ML具有灵活性和可扩展性，允许用户通过插件集成额外的数据收集工具，并与yProv框架完全集成，支持与工作流管理系统协同工作的任务。

**结果:** yProv4ML能够成功地以标准化格式收集数据来源信息，提供关于资源使用模式的深入见解，帮助识别低效点，并确保AI开发流程的可重复性和责任性。

**结论:** yProv4ML为优化大规模AI模型训练提供了一种灵活且可扩展的解决方案，有助于实现更高效的资源利用和节能的模型扩展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provenance+Tracking+in+Large-Scale+Machine+Learning+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01075，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01075&send_immediately=true&force_search=false)

**原文摘要:** As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [31] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan, Arina Cazacu, Laura Vasilie*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的仅解码器的大型语言模型（LLM）用于检测电子控制单元（ECU）通信日志中的异常，解决了现有方法在专业化领域如汽车通信系统中可扩展性不足的问题。通过引入熵正则化技术，该方法提高了对已知异常的不确定性，并保持了对相似场景的一致性。此外，该架构还提供了解决标签不一致问题的方法，以及适应不同ECU通信用例的能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的异常检测方法依赖于监督或聚类方法，在专业领域（如汽车通信系统）中成功有限，缺乏针对ECU通信优化的大型语言模型，且存在标注数据不一致和复杂性的问题。

**方法:** 提出一种基于UDP通信日志学习的仅解码器大型语言模型，将异常检测定义为识别与正常行为的时间偏差。同时引入熵正则化技术，以增加模型对已知异常的不确定性，同时保持对类似场景的一致性。

**结果:** 该方法提供了三个创新点：1）仅解码器的异常检测架构；2）处理不一致标注的方法；3）适应不同ECU通信用例的可扩展LLM。通过生成式能力减少了手动标注的成本和错误率，提高了复杂通信环境中的检测准确性。

**结论:** 该研究展示了一种新的仅解码器LLM架构，能够有效解决ECU通信日志中的异常检测问题，并为复杂通信环境提供了一种更可扩展和准确的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Good+Enough+to+Learn%3A+LLM-based+Anomaly+Detection+in+ECU+Logs+without+Reliable+Labels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01077&send_immediately=true&force_search=false)

**原文摘要:** Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [32] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani, Valentine Anantharaj, Sandro Fiore*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种名为yProv4ML的框架，用于在机器学习过程中以PROV-JSON格式捕获溯源信息，并且只需要对代码进行最少的修改。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）的快速发展突显了其灵活性和泛化能力，但同时也暴露了开发过程中的不透明性和缺乏严谨性的问题。特别是无法提前确定训练轮数和其他超参数，这给识别最佳模型带来了挑战。现有的机器学习框架如MLFlow虽然可以自动化收集相关信息，但使用专有格式存储数据并且对数据谱系关注较少。

**方法:** 提出了一个名为yProv4ML的框架，该框架能够在机器学习过程中以PROV-JSON格式捕获溯源信息，并且只需要对现有代码进行最少的修改。

**结果:** 通过yProv4ML框架，研究人员可以更透明地记录和追踪机器学习过程中的各种信息，解决了现有工具在数据格式专有性和谱系关注度上的不足。

**结论:** yProv4ML为机器学习过程提供了一种新的、更加透明和注重谱系的方法来捕获溯源信息，有助于提高模型开发的透明性和严谨性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是yProv4ML%3A+Effortless+Provenance+Tracking+for+Machine+Learning+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01078，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01078&send_immediately=true&force_search=false)

**原文摘要:** The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [33] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux, Ramy Azzouz, Emmanuel Chazard, Amélie Vromant, Eric Wiel*

**主要类别:** cs.LG

**AI概要:** 在急诊科中，分类错误是一个持续存在的挑战。本研究比较了三种AI模型（NLP、LLM和JEPA）在预测分类结果方面的表现，并发现LLM模型URGENTIAPARSE的表现优于其他模型和护士分类，能够更准确地预测住院需求并处理结构化数据。将AI整合到急诊工作流程中可能提高患者安全性和运营效率，但需要解决模型局限性并确保伦理透明度。


<details>
  <summary>更多</summary>
  
**动机:** 急诊科面临的患者涌入增加和人员短缺问题促使人们关注将人工智能集成到分类协议中，以减少分类错误。

**方法:** 回顾性分析了法国里尔Roger Salengro医院急诊科7个月期间招募的成人患者分类数据。训练和验证了三种AI模型：TRIAGEMASTER（NLP）、URGENTIAPARSE（LLM）和EMERGINET（JEPA）。使用F1分数、加权Kappa、Spearman、MAE和RMSE等指标评估模型与FRENCH标准分类的一致性。

**结果:** LLM模型URGENTIAPARSE表现出更高的准确性（综合得分：2.514），优于JEPA（0.438）和NLP（-3.511），甚至优于护士分类（-4.343）。此外，URGENTIAPARSE在预测住院需求方面也表现出色，并且在处理结构化数据时比原始转录数据更稳健。

**结论:** LLM架构通过抽象患者表征，在测试模型中提供了最准确的分类预测。将AI整合到急诊科工作流程中可以提高患者安全性和运营效率，但需要解决模型限制并确保伦理透明度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Development+and+Comparative+Evaluation+of+Three+Artificial+Intelligence+Models+%28NLP%2C+LLM%2C+JEPA%29+for+Predicting+Triage+in+Emergency+Departments%3A+A+7-Month+Retrospective+Proof-of-Concept，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01080&send_immediately=true&force_search=false)

**原文摘要:** Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [34] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee, Konstantinos Barmpas, Yannis Panagakis, Dimitrios Adamos, Nikolaos Laskaris, Stefanos Zafeiriou*

**主要类别:** cs.LG

**AI概要:** 评估了大型脑电基础模型（LBMs）在脑机接口任务中的表现，发现其相较于传统深度架构仅有边际改进但参数量显著增加，并通过低秩适应（LoRA）等方法揭示了模型架构和训练效率的问题，提出需要领域特定的发展策略来优化LBMs。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基础模型在人工智能多个领域取得了成功，但它们在脑电波建模方面的能力尚不明确，因此需要全面评估这些模型在脑机接口任务中的性能及适用性。

**方法:** 通过对多个脑机接口基准任务（如记忆任务和睡眠阶段分类）进行系统性的微调实验，评估当前的大型脑电基础模型；同时采用详细的消融研究和低秩适应（LoRA）技术，探索减少可训练参数的方法并分析模型架构和训练效率问题。

**结果:** 最先进的LBMs相较于传统深度架构仅实现0.9%-1.2%的性能提升，但需要数百万的参数（而传统模型只需数千参数）。使用LoRA可以减少可训练参数而不降低性能，表明当前LBMs存在架构和训练效率的限制。

**结论:** 当前的LBMs需要领域特定的发展策略以进一步改进，可能需要重新设计模型架构才能充分发挥基础模型在脑电波分析中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+Large+Brainwave+Foundation+Models+Capable+Yet%3F+Insights+from+Fine-tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01196&send_immediately=true&force_search=false)

**原文摘要:** Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [35] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang, Yihang Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的随机共轭次梯度方法及自适应采样技术，用于改进大语言模型训练中的优化速度和准确性。相比传统SGD，新方法在收敛性和可扩展性上均有提升。


<details>
  <summary>更多</summary>
  
**动机:** 随机梯度下降（SGD）长期以来是训练大语言模型的核心方法，但在大规模应用中其有效性受到质疑，存在性能限制问题。因此需要一种更高效的方法来解决非凸性和非光滑性问题，同时保持一阶方法的优势。

**方法:** 提出了一种结合自适应采样的随机共轭次梯度方法，通过样本复杂度分析选择样本大小、利用随机共轭次梯度确定搜索方向，并采用类似AdamW的算法自适应调整步长。

**结果:** 实验结果表明，该方法不仅保持了传统SGD的可扩展性，在许多情况下还超越了它，显著提高了优化过程的速度和准确性。

**结论:** 所提出的随机共轭次梯度方法及其自适应采样技术为大语言模型的训练提供了一种更快、更准确的优化方案，有效解决了传统SGD的性能瓶颈。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+First-Order%3A+Training+LLMs+with+Stochastic+Conjugate+Subgradients+and+AdamW，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01241，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01241&send_immediately=true&force_search=false)

**原文摘要:** Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [36] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich, Dmitry Aksenov, Ekaterina Pleshakova, Sergey Gataullin*

**主要类别:** cs.LG

**AI概要:** The paper presents a neural operator based on the dynamic mode decomposition (DMD) algorithm that combines DMD and deep learning for efficient modeling of spatiotemporal processes. It automatically extracts key modes and system dynamics, reducing computational costs compared to traditional methods while achieving high reconstruction accuracy.


<details>
  <summary>更多</summary>
  
**动机:** Finding a balance between lightweight and accurate computations in scientific computation methods development with artificial intelligence technologies is crucial. This study aims to address this by proposing an innovative approach.

**方法:** The method uses a neural operator based on the dynamic mode decomposition algorithm (DMD), which maps functional spaces and combines DMD and deep learning for efficient modeling of spatiotemporal processes.

**结果:** The approach has demonstrated its efficiency through comparative analysis with DeepONet and FNO in approximating solutions to the heat equation, Laplaces equation, and Burgers equation, achieving high reconstruction accuracy.

**结论:** This novel method provides a balance between computational efficiency and accuracy, making it a promising direction in the field of scientific computation combined with AI.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Neural+Operator+based+on+Dynamic+Mode+Decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01117&send_immediately=true&force_search=false)

**原文摘要:** The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [37] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa*

**主要类别:** cs.LG

**AI概要:** 近年来，忘却技术作为一种解决大语言模型（LLMs）和大型多模态模型（LMMs）中隐私和版权问题的方法受到了广泛关注。然而，对于LMMs的忘却评估框架尚未得到充分探索。本研究提出了PULSE协议，通过两个关键视角：预训练知识忘却和长期可持续性评估，来分析不同知识获取阶段的效果及处理顺序请求的能力。研究发现，虽然一些技术能够成功忘却微调获得的知识，但在消除预训练阶段学到的信息方面表现不佳。此外，单次操作中有效忘却目标数据批次的方法在分批顺序忘却相同数据时会导致显著性能下降。


<details>
  <summary>更多</summary>
  
**动机:** 当前针对LMMs的忘却基准仅考虑通过单一忘却操作使模型忘却微调知识的情景，缺乏对不同知识获取阶段和顺序请求处理能力的评估。

**方法:** 提出PULSE协议，包含两个关键视角：(i) 预训练知识忘却，用于分析不同知识获取阶段的效果；(ii) 长期可持续性评估，用于处理顺序请求。然后根据这些维度评估现有的忘却方法。

**结果:** 结果表明，部分技术可以成功忘却微调知识，但难以消除预训练阶段学到的信息。此外，单次操作中有效忘却目标数据的方法在分批顺序忘却时会导致显著性能下降。

**结论:** 现有的忘却技术在处理LMMs预训练知识忘却和顺序请求方面存在挑战，需要进一步改进以实现更全面的忘却效果和更好的性能稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PULSE%3A+Practical+Evaluation+Scenarios+for+Large+Multimodal+Model+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01271&send_immediately=true&force_search=false)

**原文摘要:** In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [38] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh, Brendan McMahan, Abhradeep Thakurta*

**主要类别:** cs.LG

**AI概要:** 在差分隐私（DP）训练中，添加到梯度的球形噪声会削弱自适应优化器（如AdaGrad和Adam）的性能。本文调查了几个解决此问题的算法变体，通过理论分析和实证研究发现，名为scale-then-privatize的简单技术比其他方法表现更好。该技术虽然无法实现无偏的梯度二阶矩估计，但在小规模语言模型训练任务上表现出更理想的理论特性和实际效果。此外，该技术使得噪声添加更好地匹配实际应用中的相关噪声机制。


<details>
  <summary>更多</summary>
  
**动机:** 差分隐私训练中，添加到梯度的球形噪声削弱了自适应优化器（如AdaGrad和Adam）的性能，因此需要开发更好的算法来应对这一挑战。然而，现有工作的经验结果主要集中在简单的任务和模型上，结论可能无法推广到实际的模型训练中。

**方法:** 调查了几种针对差分隐私训练中梯度噪声问题的算法变体，并对其进行了更好的理论直觉分析以及实证研究比较。具体来说，研究了旨在实现无偏梯度二阶矩估计的方法，并与一种名为scale-then-privatize的技术进行了对比。后者并不追求无偏二阶矩估计，但具有更理想的理论行为。

**结果:** 发现scale-then-privatize技术在小规模语言模型训练任务中优于其他所有变体。尽管它不实现无偏的梯度二阶矩估计，但它具有更好的理论行为和实际性能。此外，该技术使噪声添加更好地匹配实际应用中的相关噪声机制。

**结论:** scale-then-privatize是一种简单而有效的技术，在差分隐私训练中，相较于追求无偏梯度二阶矩估计的方法，它提供了更好的理论和实证性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Design+Principles+for+Private+Adaptive+Optimizers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01129&send_immediately=true&force_search=false)

**原文摘要:** The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [39] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

**主要类别:** cs.LG

**AI概要:** 通过引入Neural Hamiltonian Operator (NHO)，利用深度学习解决高维随机控制问题，将问题转化为前向-后向随机微分方程系统，并通过训练神经网络来满足Pontryagin最大值原理的约束条件。


<details>
  <summary>更多</summary>
  
**动机:** 高维随机控制问题由于维度灾难难以求解，传统的动态规划方法在高维空间中效率低下，因此需要一种新的方法来处理这类问题。

**方法:** 提出了一种基于深度学习的形式化框架，使用Neural Hamiltonian Operator (NHO) 来参数化前向-后向随机微分方程（FBSDE）的动力学过程，其中神经网络用于表示反馈控制和价值函数的空间梯度的假设形式。通过训练这些神经网络以满足Pontryagin最大值原理所规定的相容性条件，找到最优的NHO。

**结果:** 该方法将深度FBSDE方法置于统计推断的严格语言中，将其视为从模拟数据中学习未知算子的问题。证明了NHO在一般鞅驱动下的通用逼近能力，并为分析此类模型中的优化挑战提供了清晰的视角。

**结论:** Neural Hamiltonian Operator提供了一个理论严谨的框架，用于解决高维随机控制问题，同时揭示了该方法在优化方面的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Hamiltonian+Operator，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01313，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01313&send_immediately=true&force_search=false)

**原文摘要:** Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [40] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin, Cong Fu, Zachary Krueger, Haiyang Yu, Maho Nakata, Jianwen Xie, Emine Kucukbenli, Xiaofeng Qian, Shuiwang Ji*

**主要类别:** cs.LG

**AI概要:** 本研究开发了张量分解网络（TDNs），通过低秩张量分解加速$m{SO}(3)$-等变网络的计算，证明了其误差界和泛化能力，并提出路径权重共享以减少参数数量，最终在多个数据集上展示了其竞争力和显著加速效果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管$m{SO}(3)$-等变网络在机器学习原子间势能（MLIPs）领域占据主导地位，但其核心操作Clebsch-Gordan (CG) 张量积计算成本高昂，因此需要一种更高效的近似等变模型来替代传统方法。

**方法:** 作者提出了张量分解网络（TDNs），用低秩张量分解（如CP分解）替代CG张量积。此外，还引入了路径权重共享机制，将所有多重态空间权重绑定到单个路径上，从而进一步减少参数数量而不影响等变性。这使得张量积的计算复杂度从$O(L^6)$降低到$O(L^4)$。

**结果:** 在PubChemQCR、OC20和OC22等多个数据集上的实验表明，TDNs不仅实现了与现有方法相当的性能，还在计算上取得了显著加速。

**结论:** TDNs作为一种近似等变网络，通过使用低秩张量分解和路径权重共享技术，在保持竞争力的同时显著提高了计算效率，为大规模分子动力学模拟提供了新的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tensor+Decomposition+Networks+for+Fast+Machine+Learning+Interatomic+Potential+Computations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01131，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01131&send_immediately=true&force_search=false)

**原文摘要:** $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [41] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren, Siyuan Liang, Aishan Liu, Dacheng Tao*

**主要类别:** cs.LG

**AI概要:** 提出Dual-learning假设，发现ICL对后门攻击的脆弱性，并提出ICLShield方法动态调整概念偏好比例以防御后门攻击，显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管ICL在LLM中表现出色，但其对后门攻击的脆弱性引起了关注，需要研究其机制并开发有效的防御方法。

**方法:** 提出Dual-learning假设，分析ICL后门效应的理论上限，并设计ICLShield通过置信度和相似度评分选择干净演示来调整任务与后门的概念偏好比例。

**结果:** 实验表明ICLShield在多个LLM和任务上达到最先进的防御效果，平均性能提升26.02%，并对闭源模型如GPT-4有良好适应性。

**结论:** ICLShield为ICL后门攻击提供了有效防御方案，展现出优越的适应性和防御性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ICLShield%3A+Exploring+and+Mitigating+In-Context+Learning+Backdoor+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01321，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01321&send_immediately=true&force_search=false)

**原文摘要:** In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [42] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira, Gabe Gomes, Meng Jiang, Nitesh V. Chawla, Nuno Moniz*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Spectral Manifold Harmonization（SMH）的新方法，通过生成保留拓扑属性的合成图样本来解决图结构数据中不平衡回归问题，特别是在科学领域中具有高价值的目标值范围。实验表明，该方法在化学和药物发现基准数据集上显著提高了目标域范围内的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 在科学领域，图结构数据非常普遍，并且模型常常面临不平衡的学习环境。不平衡回归中，领域偏好关注特定目标值范围，这些范围代表最具科学价值的情况，但目前对此的研究较少。传统方法要么忽略图拓扑结构，要么不针对特定域范围，导致模型偏向于平均目标值。

**方法:** 提出了Spectral Manifold Harmonization（SMH），一种新方法，用于解决图结构数据中的不平衡回归挑战。该方法通过生成合成图样本来保留拓扑属性，同时专注于通常被低估的目标分布区域。

**结果:** 在化学和药物发现基准数据集上的实验结果表明，SMH方法在目标域范围内持续提高了预测性能。

**结论:** Spectral Manifold Harmonization（SMH）为解决图结构数据中的不平衡回归问题提供了一种有效的方法，特别是在科学领域中关注特定目标值范围的情况下。这表明了其在提高相关领域预测性能方面的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectral+Manifold+Harmonization+for+Graph+Imbalanced+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01132&send_immediately=true&force_search=false)

**原文摘要:** Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [43] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Jiansong Chen, Ke Zeng, Xunliang Cai*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的自适应困惑感知强化学习（APARL）框架，用于检测客户服务对话中的异常事件。通过双循环动态课程学习架构，提高了模型的泛化能力和鲁棒性，在OOD测试中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 在实际客户服务对话中检测异常事件极具挑战性，因为业务数据复杂且客户互动具有动态性质。此外，模型需要具备强大的领域外（OOD）泛化能力，以快速适应不同的业务场景并最大化商业价值。

**方法:** 提出了一个名为自适应困惑感知强化学习（APARL）的新框架，利用大型语言模型的高级推理能力来检测异常事件。APARL引入了双循环动态课程学习架构，使模型能够随着其熟练程度的提高逐步关注更具挑战性的样本。

**结果:** 在食品配送对话任务上的广泛评估表明，该模型显著增强了适应性和鲁棒性，达到了最高的F1分数，平均提高了17.19%，并在OOD转移测试中平均提高了9.59%。

**结论:** APARL框架为工业部署的异常检测模型提供了一个优越的解决方案，有助于提高运营效率和商业利益。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoner+for+Real-World+Event+Detection%3A+Scaling+Reinforcement+Learning+via+Adaptive+Perplexity-Aware+Sampling+Strategy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01327，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01327&send_immediately=true&force_search=false)

**原文摘要:** Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [44] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang*

**主要类别:** cs.LG

**AI概要:** 在大语言模型（LLMs）推动技术进步的同时，其训练数据的隐私保护变得至关重要。微分隐私（DP）是一种严格的数据保护机制，但通过差分隐私随机梯度下降（DP-SGD）实现该机制面临诸多挑战，特别是每个样本梯度裁剪的复杂性。现有的显式方法（如Opacus）需要大量的存储空间来保存每个样本的梯度，大幅增加了内存需求。而隐式方法（如GhostClip）则通过多次重新计算梯度减少存储需求，但导致了冗余计算的问题。本文提出了一种创新的、对缓存友好的逐层DP-SGD方法——FlashDP，将必要的操作整合到一个任务中，并以融合的方式仅计算一次梯度。这种方法不仅减少了高达50%的内存移动，还削减了20%的冗余计算。在使用四个A100系统对Llama-13B模型进行预训练时，FlashDP的吞吐量达到了非DP方法的90%，且与标准的逐层裁剪DP-SGD在准确性上保持一致。这些改进使FlashDP成为高效且保护隐私的LLMs训练的关键进展。FlashDP的代码已开源至https://github.com/kaustpradalab/flashdp。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型（LLMs）在技术进步中的作用日益增强，其训练数据的隐私保护成为一个关键问题。当前的差分隐私随机梯度下降（DP-SGD）方法要么需要大量存储空间，要么存在冗余计算的问题，限制了效率和可扩展性。因此，亟需一种更高效的解决方案来应对这些挑战。

**方法:** 本文提出了一种名为FlashDP的新方法，这是一种对缓存友好的逐层DP-SGD。FlashDP通过将所有必要操作整合到一个任务中，并以融合的方式仅计算一次梯度，解决了现有方法的内存和计算冗余问题。具体而言，FlashDP减少了高达50%的内存移动，并削减了20%的冗余计算。此外，它不需要额外增加内存需求，并在多GPU系统上实现了接近非DP方法的吞吐量。

**结果:** 实验结果表明，在四块A100 GPU系统上对Llama-13B模型进行预训练时，FlashDP的吞吐量达到非DP方法的90%，同时与标准逐层裁剪DP-SGD在准确性上保持一致。这证明了FlashDP在提高效率和保护隐私方面的优越性能。

**结论:** FlashDP作为一种创新的、对缓存友好的逐层DP-SGD方法，显著降低了内存移动和冗余计算的需求，同时在训练大语言模型时保持了高吞吐量和准确性。这些特性使其成为高效且保护隐私的LLMs训练的重要进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlashDP%3A+Private+Training+Large+Language+Models+with+Efficient+DP-SGD，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01154&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [45] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu, Yinuo Wang, Xujie Song, Wenjun Zou, Liangfa Chen, Likun Wang, Bin Shuai, Jingliang Duan, Shengbo Eben Li*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的分布强化学习算法DSAC-D，通过引入策略熵和价值分布函数构建多模态分布策略迭代框架，并使用扩散模型生成奖励样本以准确描述多峰分布。在MuJoCo测试任务中表现出色，在9个控制任务中均达到SOTA性能，并且在实际车辆测试中能够准确刻画不同驾驶风格的多模态分布。


<details>
  <summary>更多</summary>
  
**动机:** 传统的强化学习方法通常使用单模态分布（如高斯分布）来建模值分布，容易导致值函数估计偏差，从而影响算法性能。因此需要一种能够处理多模态分布并减少估计偏差的新方法。

**方法:** 1. 提出DSAC-D算法：结合策略熵和值分布函数建立多模态分布策略迭代框架。
2. 构建扩散值网络：利用扩散模型通过反向采样生成一组奖励样本，以准确表征多峰分布。
3. 推导具有双扩散机制的分布强化学习算法：同时对值网络和策略网络进行扩散。

**结果:** 1. 在MuJoCo测试任务中，该算法不仅学习到多模态策略，还在所有9个控制任务中达到SOTA性能。
2. 相较于现有主流算法，显著抑制了估计偏差，并使总平均回报提高了超过10%。
3. 实际车辆测试表明，DSAC-D能够准确刻画不同驾驶风格的多模态分布，扩散策略网络可以表征多模态轨迹。

**结论:** DSAC-D算法成功解决了值函数估计中的偏差问题，实现了多模态策略表示，在控制任务和实际应用中均表现出优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distributional+Soft+Actor-Critic+with+Diffusion+Policy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01381&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [46] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling, Duen Horng Chau*

**主要类别:** cs.LG

**AI概要:** 作者提出了一种名为Diffusion Explorer的交互式工具，用于解释扩散模型的几何特性。用户可以在浏览器中训练2D扩散模型，并通过互动动画观察其采样过程的时间动态。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩散模型解释资源要么需要高深的理论基础，要么专注于神经网络架构而非其丰富的几何特性。

**方法:** 开发了一个名为Diffusion Explorer的交互工具，用户可以通过它在浏览器中训练2D扩散模型，并观察其采样过程的时间动态。该工具利用互动动画来制作吸引人的动态系统可视化。

**结果:** 用户可以更直观地理解扩散模型的几何特性和其作为随时间演变的随机过程的本质。

**结论:** Diffusion Explorer是一个开源工具，且有在线演示，为解释扩散模型的几何特性提供了新的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Explorer%3A+Interactive+Exploration+of+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01178，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01178&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [47] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia, Frederik Haxel, Oliver Bringmann*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于TVM编译器的工作流，将AI工作负载高效映射到RISC-V矢量单元。通过集成RVV扩展到TVM的MetaSchedule框架中，该方法在执行延迟上比GCC的自动矢量化功能改进了46%，比muRISCV-NN改进了29%。此外，生成的二进制文件具有更小的代码内存占用，并且在商用RISC-V SoC上比LLVM快35%。


<details>
  <summary>更多</summary>
  
**动机:** 目前缺少将自调优框架与RISC-V RVV扩展集成的方法，限制了复杂AI工作负载的有效部署。

**方法:** 通过将RVV扩展集成到TVM的MetaSchedule框架中，使用概率程序框架进行张量操作调整，并在FPGA上实现不同RISC-V SoC以优化多种AI工作负载。

**结果:** 相比GCC的自动矢量化功能，执行延迟平均改善46%，相比muRISCV-NN改善29%；二进制文件的代码内存占用更小；在商用RISC-V SoC上，比LLVM的映射速度快35%。

**结论:** 该方法显著提高了AI工作负载在RISC-V平台上的执行效率，并且适用于嵌入式设备。代码已开源供社区扩展到其他RISC-V扩展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tensor+Program+Optimization+for+the+RISC-V+Vector+Extension+Using+Probabilistic+Programs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01457&send_immediately=true&force_search=false)

**原文摘要:** RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [48] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen, Tom Lenaerts*

**主要类别:** cs.LG

**AI概要:** 这篇论文重新审视了强化学习中奖励频率作为任务难度可靠衡量标准的普遍假设，并揭示了当前策略学习方法在处理零激励动力学时存在的结构性挑战及根本限制。


<details>
  <summary>更多</summary>
  
**动机:** 作者观察到，在强化学习中，关键子目标的完成可能不会立即带来奖励，这挑战了传统的将奖励频率视为任务难度衡量标准的观点。因此，他们希望研究这种情况下当前学习方法的有效性及其局限性。

**方法:** 作者首先定义并形式化了零激励动力学的概念，即那些对成功至关重要的状态转换没有得到奖励的情况。然后，他们分析了最先进的深度子目标算法在这种环境下的表现，特别是考察了学习性能对子目标完成与最终奖励之间时间接近性的敏感度。

**结果:** 研究表明，现有的基于子目标的深度学习算法无法有效利用零激励动力学中的信息，并且学习性能高度依赖于子目标完成和最终奖励之间的时间间隔。

**结论:** 当前的强化学习方法存在一个根本性的局限性：它们需要依赖即时奖励来推断潜在的任务结构。因此，未来的研究需要开发出不依赖即时奖励的新机制，以更好地解决零激励动力学问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Incentive+Dynamics%3A+a+look+at+reward+sparsity+through+the+lens+of+unrewarded+subgoals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01470，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01470&send_immediately=true&force_search=false)

**原文摘要:** This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [49] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo, Yoon, Yisong Yue, Been Kim*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Joint Autoencoder Modulator (JAM) 的框架，通过多目标优化在独立训练的视觉和语言模型之间实现对齐。该框架通过重建和跨模态目标鼓励对齐，并探讨了对齐目标、层深度和基础模型规模对表示收敛的影响。实验表明，即使在冻结的独立训练表示上，该框架也能可靠地诱导对齐，为将通用单模态模型转化为专业多模态模型提供了理论和实践路径。


<details>
  <summary>更多</summary>
  
**动机:** 尽管视觉和语言模型分别在不同的表示空间中训练，但Platonic Representation Hypothesis假设这些模型可能趋向于共享现实的统计模型。因此，研究者希望超越事后统计检测对齐的方法，明确优化不同模态间的对齐问题。

**方法:** 论文将Platonic对齐问题建模为一个多目标优化任务，旨在保留每种模态的原生结构的同时实现相互一致性。提出了Joint Autoencoder Modulator (JAM) 框架，该框架联合训练模态特定的自动编码器，基于预训练单模态模型的潜在表示进行对齐。此外，还从三个关键设计轴评估了该框架：(i) 对齐目标（对比损失、硬负变体和Spread损失），(ii) 实现最有效对齐的层深度，以及(iii) 基础模型规模对表示收敛的影响。

**结果:** 实验结果表明，提出的轻量级Pareto-efficient框架能够在冻结的独立训练表示上可靠地诱导对齐。并且，该框架为将通用单模态模型转化为专业多模态模型提供了理论见解和实际路径。

**结论:** Joint Autoencoder Modulator (JAM) 框架是一种有效的工具，可以在不改变原始模型的情况下实现跨模态表示的对齐。这不仅验证了Platonic Representation Hypothesis，也为构建多模态模型提供了新的方法论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Escaping+Platos+Cave%3A+JAM+for+Aligning+Independently+Trained+Vision+and+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01201&send_immediately=true&force_search=false)

**原文摘要:** Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [50] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse, Jan Felix Kleuker, Aske Plaat, Thomas Moerland*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Chargax的基于JAX的环境，用于加速RL代理在电动车充电站模拟中的训练，并且相较于现有的环境提升了100x-1000x的计算性能。


<details>
  <summary>更多</summary>
  
**动机:** 电网系统普遍面临拥堵问题，需要提高运行效率，而传统的强化学习方法由于高样本复杂性和昂贵的仿真需求速度较慢。虽然已有工作使用GPU通过将环境转换为JAX来加速数据生成，但这些工作大多集中于经典玩具问题。

**方法:** 引入了名为Chargax的基于JAX的环境，用于真实模拟电动车充电站以加速RL代理的训练。该环境基于真实数据进行验证，并比较了强化学习代理与基线模型的表现。Chargax采用模块化架构，能够表示各种真实的充电站配置。

**结果:** Chargax在计算性能上比现有环境提高了100x-1000x，并且其模块化架构可以很好地表示现实世界的充电站配置。

**结论:** Chargax提供了一个高效的强化学习训练平台，适用于电动车充电站的真实场景模拟，极大地提高了计算性能并展示了实际应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chargax%3A+A+JAX+Accelerated+EV+Charging+Simulator，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01522，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01522&send_immediately=true&force_search=false)

**原文摘要:** Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [51] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo, Igor de Moura, Assis T. de Oliveira Filho, Djamel Sadok, Cleber Zanchettin*

**主要类别:** cs.LG

**AI概要:** 现代车辆中以太网通信面临安全威胁，深度学习入侵检测系统（IDS）可解决此问题，但需要昂贵硬件。本文研究了通过蒸馏和剪枝等快速神经网络推理技术，在低成本平台上实现实时IDS模型部署的可行性。实验表明，在Raspberry Pi 4上，检测时间可达727微秒，AUCROC值为0.9890。


<details>
  <summary>更多</summary>
  
**动机:** 随着现代车辆连接性增强，车载以太网作为车内通信基础设施之一，面临着诸如流注入攻击等安全威胁。现有的基于深度学习的入侵检测系统（IDS）虽然可以应对这些威胁，但通常需要昂贵的硬件来实现实时运行。因此，需要一种能够在低成本硬件平台上实现实时运行的解决方案。

**方法:** 本文提出评估并应用快速神经网络推理技术（如蒸馏和剪枝），以在低成本平台上部署入侵检测系统模型。具体来说，这些技术被用于优化模型，使其能够在如Raspberry Pi 4这样的低成本硬件上进行实时推理。

**结果:** 实验结果表明，使用蒸馏和剪枝技术后，可以在Raspberry Pi 4上实现高达727微秒的入侵检测时间，同时保持较高的检测精度，其AUCROC值达到了0.9890。

**结论:** 快速神经网络推理技术（如蒸馏和剪枝）能够显著提高模型的推理速度，并降低硬件需求，使得基于深度学习的入侵检测系统能够在低成本平台上实现实时应用。这对于现代车辆的安全保障具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Learning-Based+Intrusion+Detection+for+Automotive+Ethernet%3A+Evaluating+%26+Optimizing+Fast+Inference+Techniques+for+Deployment+on+Low-Cost+Platform，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01208&send_immediately=true&force_search=false)

**原文摘要:** Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [52] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua*

**主要类别:** cs.LG

**AI概要:** SPRO 是一种新的框架，通过从策略模型本身内在地推导过程奖励和引入累积过程奖励与 MSA，实现了高效的过程感知强化学习，相比 GRPO 提高了 3.4 倍的训练效率，并在测试准确率上提高了 17.5%。


<details>
  <summary>更多</summary>
  
**动机:** 现有的过程强化学习方法存在计算开销大且缺乏统一理论框架的问题，特别是在过程级优势估计方面。

**方法:** 提出了一种名为 SPRO 的新框架，包含两个关键创新：(1) 理论证明过程奖励可从策略模型本身内在推导；(2) 引入累积过程奖励和 Masked Step Advantage (MSA)，以实现严格的逐歩动作优势估计。

**结果:** 实验表明，SPRO 比普通的 GRPO 训练效率高出 3.4 倍，测试准确率提高 17.5%，同时保持稳定的策略熵并减少约 1/3 的平均响应长度，且无需额外计算开销。

**结论:** SPRO 提供了一种高效、稳定的过程强化学习方法，具有工业应用潜力，且避免了奖励欺骗问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Guided+Process+Reward+Optimization+with+Masked+Step+Advantage+for+Process+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01551，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01551&send_immediately=true&force_search=false)

**原文摘要:** Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [53] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Hao Wang, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan*

**主要类别:** cs.LG

**AI概要:** PAE MobiLLM是一种隐私保护和高效的大型语言模型微调方法，通过服务器辅助的加法侧调优技术解决移动设备资源有限的问题。它通过激活缓存、单一令牌激活快捷方式和加法适配器侧网络设计来减少通信成本、加速收敛并提高计算效率，同时确保数据、标签或微调模型不泄露给服务器。


<details>
  <summary>更多</summary>
  
**动机:** 在移动设备上使用新鲜数据对大型语言模型进行微调有许多有趣的潜在应用，但移动设备的资源有限。现有的服务器辅助方法（如分割学习或侧调优）虽然可能实现本地设备上的LLM微调，但存在传输激活带来的沉重通信负担，并且可能会向服务器披露数据、标签或微调后的模型。

**方法:** 开发了PAE MobiLLM，一种隐私保护且高效的LLM微调方法，通过服务器辅助的加法侧调优技术部署在移动设备上。该方法整合了服务器端的激活缓存，使服务器能够重用历史激活，节省移动设备反复计算前向传递的资源。为了降低通信成本，PAE MobiLLM采用了一种单令牌（即“支点”令牌）激活快捷方式，仅传输单一激活维度而非完整的激活矩阵以指导侧网络调整。此外，引入了加法适配器侧网络设计，让服务器根据设备定义的预测差异而非原始真实标签训练适配模块，从而保证服务器只能协助设备定义的侧网络计算，无法获取数据、标签或微调模型。

**结果:** 实验结果表明，PAE MobiLLM可以有效减少通信成本，加速微调收敛，并提高计算效率，同时保持数据隐私性。

**结论:** PAE MobiLLM提供了一种新颖且有效的解决方案，在移动设备上实现了高效和隐私保护的大型语言模型微调，解决了现有方法中通信负担重和隐私泄露的风险问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PAE+MobiLLM%3A+Privacy-Aware+and+Efficient+LLM+Fine-Tuning+on+the+Mobile+Device+via+Additive+Side-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01216&send_immediately=true&force_search=false)

**原文摘要:** There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [54] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg, Yam Eitan, Aviv Navon, Aviv Shamsian, Theo, Putterman, Michael Bronstein, Haggai Maron*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于三个原则设计的新型架构GradMetaNet，用于处理神经网络梯度。该方法在多种基于梯度的任务中表现出色，并且具有普适性，能够学习自然梯度相关的函数。


<details>
  <summary>更多</summary>
  
**动机:** 当前许多研究探索了直接在梯度上进行操作的学习算法，但这些算法使用的架构并非专门针对梯度处理设计，限制了其应用范围。因此需要一种更系统化、更高效的方法来设计专门处理梯度的架构。

**方法:** 论文提出了一个基于以下三个原则设计的架构：1) 保持神经元排列对称性的等变设计；2) 处理多个数据点的梯度集合以捕获曲率信息；3) 通过秩-1分解实现高效的梯度表示。基于此，构建了一个由简单等变模块组成的新型架构GradMetaNet，并证明了其普适性结果。

**结果:** 实验表明，GradMetaNet在多个基于梯度的任务（如学习优化、INR编辑和损失景观曲率估计）上表现优异，特别是在MLP和Transformer模型上。此外，它能够逼近自然梯度相关函数，而以前的方法则无法做到这一点。

**结论:** GradMetaNet是一种专门用于梯度处理的普适性架构，其设计遵循三个核心原则，能够在多种任务中有效工作，并超越以往方法的能力范围。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GradMetaNet%3A+An+Equivariant+Architecture+for+Learning+on+Gradients，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01649，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01649&send_immediately=true&force_search=false)

**原文摘要:** Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [55] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa, Bilal Farooq*

**主要类别:** cs.LG

**AI概要:** 论文研究了使用量子机器学习方法（QSVM和QNN）对复杂皮肤电反应事件进行建模，以反映虚拟现实过马路实验中行人的压力情况。尽管QSVM训练精度较高，但存在过拟合问题，测试精度仅为45%。而QNN模型的测试精度达到55%，表现优于QSVM及经典版本模型。


<details>
  <summary>更多</summary>
  
**动机:** 随着量子计算的发展，其为解决复杂的机器学习任务提供了新的可能性，例如智能交通系统中常见的高维数据表示问题。本研究旨在探索量子机器学习在建模复杂皮肤电反应事件中的应用，以反映虚拟现实道路交叉实验中行人的压力状况。

**方法:** 研究采用了两种量子机器学习方法：1) 使用八比特ZZ特征图的量子支持向量机 (QSVM)，2) 使用树张量网络参数化和八比特ZZ特征图的量子神经网络 (QNN)。所有模型均在Pennylane平台上开发。数据集包括皮肤电反应测量值以及响应幅度和经过时间等特征，并按幅度划分类别。

**结果:** QSVM模型在训练过程中表现出较高的精度，但存在严重的过拟合问题，导致测试精度仅为45%，影响了分类模型的可靠性。相比之下，QNN模型达到了更高的测试精度（55%），显示出比QSVM和经典版本更好的分类性能。

**结论:** 尽管量子机器学习方法在处理复杂数据方面具有潜力，但仍然存在挑战，如过拟合问题。QNN模型在该研究中表现优于QSVM和经典模型，表明其在分类任务中的优势，但仍需进一步优化以提高可靠性和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Machine+Learning+in+Transportation%3A+A+Case+Study+of+Pedestrian+Stress+Modelling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01235&send_immediately=true&force_search=false)

**原文摘要:** Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [56] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为AsyncFlow的异步流式强化学习框架，通过分布式数据存储与传输模块、生产者-消费者型异步工作流引擎等设计，解决了传统RL框架在扩展性、复杂数据流处理和自定义支持上的问题。实验表明其相比现有方法提升了1.59倍的吞吐量。


<details>
  <summary>更多</summary>
  
**动机:** 当前强化学习框架存在扩展性瓶颈、复杂数据流处理困难、资源闲置及负载不平衡等问题，并且大多数框架与特定的语言模型训练或推理引擎紧密耦合，难以支持自定义设计的引擎。

**方法:** 提出了一个异步流式强化学习框架AsyncFlow，包括分布式数据存储与传输模块（提供统一数据管理和细粒度调度）、基于生产者-消费者模型的异步工作流引擎（最小化计算空闲时间）以及与底层训练和推理引擎解耦的核心能力封装。

**结果:** 大量实验表明，AsyncFlow相比现有最先进基线方法平均提升了1.59倍的吞吐量。

**结论:** AsyncFlow架构为下一代强化学习训练系统的设计提供了可行的见解，解决了现有框架的诸多限制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AsyncFlow%3A+An+Asynchronous+Streaming+RL+Framework+for+Efficient+LLM+Post-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01663&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [57] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Prefix-RFT的混合方法，结合了监督微调（SFT）和强化微调（RFT）的优势，在数学推理问题上表现出色，并且易于整合到现有的开源框架中。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型后训练技术分为监督微调（SFT）和强化微调（RFT），各有优缺点：SFT擅长模仿示例数据但可能引发行为克隆问题，RFT能显著提升性能但容易学习到意外行为并对初始策略敏感。因此需要一种能够融合这两种方法优点的新方法。

**方法:** 提出了一种称为Prefix-RFT的统一方法，该方法通过结合示范学习和探索学习来协同SFT和RFT的优点。使用数学推理问题作为测试平台，进行了实证研究。

**结果:** Prefix-RFT不仅超越了单独使用SFT或RFT的性能，还优于并行混合策略的RFT方法。此外，它对演示数据的质量和数量变化具有鲁棒性。

**结论:** Prefix-RFT有效地整合了SFT和RFT两种范式，提供了一种简单且高效的方法，可以轻松集成到现有的开源框架中。这项工作为LLM后训练提供了新的视角，表明将示范和探索明智地结合起来可能是未来研究的有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Blending+Supervised+and+Reinforcement+Fine-Tuning+with+Prefix+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01679，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01679&send_immediately=true&force_search=false)

**原文摘要:** Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [58] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为SODA的算法，用于从大型语言模型（LLM）的输出中重建精确输入，解决了现有审计技术无法解决的法医问题。通过将精确输入重建形式化为具有唯一全局最小值的离散优化问题，并采用连续松弛输入搜索空间的方法，SODA在各种规模的LLM实验中显著优于现有方法，成功恢复了79.5%的较短异常分布输入，且无误报。然而，在从较长输入序列的输出中提取私密信息方面存在困难，这表明当前的标准部署实践可能足以防止该方法被恶意使用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管现有的审计技术试图识别大型语言模型（LLM）中的潜在不良行为，但本文旨在解决一个互补的法医问题，即重建导致现有LLM输出的确切输入，从而实现事后分析，并可能检测到伪造的输出报告。

**方法:** 将精确输入重建形式化为具有唯一全局最小值的离散优化问题，并引入一种高效的基于梯度的算法SODA。SODA通过对输入搜索空间进行连续松弛处理，并采用周期性重启和参数衰减的方式操作。

**结果:** 通过在参数范围从33M到3B的LLM上进行综合实验，证明SODA显著优于现有方法。成功地从下一个标记的对数中完全恢复了79.5%的较短异常分布输入，且无单一误报。但在从较长（15+标记）输入序列的输出中提取私密信息方面遇到困难。

**结论:** 标准部署实践目前可能提供了足够的保护，以防止该方法被恶意使用。代码可在https://doi.org/10.5281/zenodo.15539879获取。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GPT%2C+But+Backwards%3A+Exactly+Inverting+Language+Model+Outputs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01693，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01693&send_immediately=true&force_search=false)

**原文摘要:** While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [59] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal*

**主要类别:** cs.LG

**AI概要:** Graph federated recommendation systems enhance privacy but traditional aggregation methods have limitations. This paper introduces Dist-FedAvg, a distance-based aggregation method that improves personalization and maintains anchor user influence, showing superior performance in empirical tests.


<details>
  <summary>更多</summary>
  
**动机:** To overcome the shortcomings of existing aggregation methods in graph federated recommendation systems which ignore the complexity of user embeddings and the importance of user similarity, as well as to adapt to evolving user interactions while preserving the role of high-relevance anchor users.

**方法:** Propose Dist-FedAvg, a novel distance-based aggregation method that assigns higher weights to users with similar embeddings and ensures anchor users maintain significant influence in local updates.

**结果:** Empirical evaluations indicate that Dist-FedAvg consistently outperforms baseline techniques in improving recommendation accuracy.

**结论:** Dist-FedAvg enhances personalization and aggregation efficiency in graph federated learning, offering better recommendation accuracy while integrating smoothly into current federated learning frameworks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Far+From+Sight%2C+Far+From+Mind%3A+Inverse+Distance+Weighting+for+Graph+Federated+Recommendation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01285&send_immediately=true&force_search=false)

**原文摘要:** Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [60] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras, Matteo Negro, Ragib Ahsan, David Arbour, Elena Zheleva*

**主要类别:** cs.LG

**AI概要:** 估计现实世界关系数据中的因果效应具有挑战性，特别是在潜在混杂因素未知的情况下。现有的因果发现算法要么假设数据是独立同分布的，不适用于关系数据；要么假设因果充分性，这在许多现实世界的数据集中并不现实。为了解决这一问题，本文提出了RelFCI算法，这是一种针对带有潜在混杂因素的关系数据的可靠且完整的因果发现算法。RelFCI结合了快速因果推断（FCI）和关系因果发现（RCD）算法的优点，并定义了新的图形模型以支持关系领域的因果发现。此外，本文还建立了带有潜在混杂因素的关系d-分离的可靠性与完整性保证。实验结果表明，RelFCI在识别带潜在混杂因素的关系因果模型中的正确因果结构方面非常有效。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的关系数据包含复杂的因果关系，而潜在的混杂因素常常未知。现有方法或者假定数据为独立同分布，不适合处理关系数据，或者假定因果充分性，这在实际应用中并不总是成立。因此需要一种新的算法来解决这些问题。

**方法:** 提出了一种名为RelFCI的因果发现算法，该算法结合了Fast Causal Inference (FCI) 和 Relational Causal Discovery (RCD) 的思想，能够从带有潜在混杂因素的关系数据中学习因果模型。同时，定义了新的图形模型，并提供了关于关系d-分离的理论保障。

**结果:** 通过实验验证了RelFCI算法的有效性，证明其可以成功识别出带潜在混杂因素的关系因果模型中的正确因果结构。

**结论:** RelFCI是一种适合处理带有潜在混杂因素的关系数据的因果发现算法，它填补了现有方法在这一领域的空白，并展示了良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Relational+Causal+Discovery+with+Latent+Confounders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01700，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01700&send_immediately=true&force_search=false)

**原文摘要:** Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [61] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud*

**主要类别:** cs.LG

**AI概要:** BBoxER是一种针对大型语言模型的进化黑箱优化方法，通过隐式压缩训练数据引入信息瓶颈，在保证隐私和安全的同时提供良好的泛化性能。实验表明，BBoxER能有效提升模型性能，并在推理数据集上表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习中的梯度优化虽然高效但依赖大量标注数据，存在隐私泄露、数据投毒攻击和过拟合等问题。黑箱优化方法因其不依赖数据细节的特点，成为一种有潜力的替代方案，但其在高维参数空间中扩展性差且计算成本高。

**方法:** BBoxER是一种基于进化算法的黑箱优化方法，适用于大型语言模型的后训练阶段。它通过隐式压缩训练数据引入信息瓶颈，利用信息流的可处理性提供理论上的泛化边界、差分隐私保障以及对数据投毒和提取攻击的鲁棒性。该方法以预训练模型为基础，提供轻量级和模块化的增强功能。

**结果:** 实验结果表明，BBoxER能够在少量迭代中提升模型性能，并在推理数据集上表现出良好的泛化能力。这证明了黑箱优化方法在实际应用中的可行性。

**结论:** BBoxER作为一种黑箱优化方法，不仅能在保护隐私和安全性方面提供保障，还能与梯度优化方法结合使用，为大型语言模型的后训练阶段提供一种有效的附加组件。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tuning+without+Peeking%3A+Provable+Privacy+and+Generalization+Bounds+for+LLM+Post-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01752&send_immediately=true&force_search=false)

**原文摘要:** Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [62] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas, Christian Riess*

**主要类别:** cs.LG

**AI概要:** BranchNet是一种将决策树集成转换为稀疏、部分连接神经网络的框架。它在结构化多类分类任务中比XGBoost更准确，且模型紧凑、可解释。


<details>
  <summary>更多</summary>
  
**动机:** 为了结合决策树和神经网络的优点，创建一种既保留符号结构又允许基于梯度优化的学习框架。

**方法:** 引入BranchNet框架，将决策树的每个分支映射到隐藏神经元，从而将决策树集成转换为稀疏的神经网络。

**结果:** 在一系列结构化多类分类基准测试中，BranchNet在准确性上持续优于XGBoost，并具有统计学上的显著优势。

**结论:** BranchNet提供了一种新的方法来构建紧凑、可解释的模型，但在二元任务上可能需要进一步的自适应校准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BranchNet%3A+A+Neuro-Symbolic+Learning+Framework+for+Structured+Multi-Class+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01781&send_immediately=true&force_search=false)

**原文摘要:** We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [63] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi, Minghan Yu, Weikang Qian, Yixin Wen, Haizhao Yang*

**主要类别:** cs.LG

**AI概要:** 生成高分辨率降水数据对于有效的水文建模和极端天气分析至关重要。标准全球产品（如IMERG）提供的分辨率仅为10公里，而本文提出了小波扩散模型（WDM），一种能够实现10倍空间超分辨率（缩小至1公里）并比基于像素的扩散模型快9倍推理速度的生成框架。WDM直接从小波域中的MRMS雷达数据学习降水的复杂结构，通过关注高频小波系数，生成极其真实且详细的1公里降水场。该方法在视觉上优于像素空间模型，并显著提高了采样效率。实验结果表明，WDM为地球科学超分辨率中准确性和速度的双重挑战提供了可靠的解决方案，为更可靠的水文预报铺平了道路。


<details>
  <summary>更多</summary>
  
**动机:** 为了满足有效水文建模和极端天气分析对千米级分辨率降水数据的需求，解决当前标准全球产品（如IMERG）仅提供10公里级分辨率的问题。

**方法:** 提出了一种名为Wavelet Diffusion Model (WDM) 的条件扩散模型，该模型直接从小波域中的MRMS雷达数据学习降水的复杂结构，聚焦于高频小波系数以生成真实的1公里降水场，同时实现了10倍的空间超分辨率和9倍的推理加速。

**结果:** WDM生成的1公里降水场在视觉效果上优于像素空间模型，具有更少的伪影，并且在采样效率方面有显著提升。

**结论:** WDM为地球科学超分辨率中准确性和速度的双重挑战提供了可靠的解决方案，有助于实现更可靠的水文预报。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Kilometer-Scale+Precipitation+Downscaling+with+Conditional+Wavelet+Diffusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01354&send_immediately=true&force_search=false)

**原文摘要:** Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [64] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo, Hamza Khyari, Umberto Straccia*

**主要类别:** cs.LG

**AI概要:** 提出了一种新方法，使图神经网络（GNNs）能够通过将k-CNF公式映射到MILP问题并编码为加权二分图来解决SAT问题。该方法在有限数据集上具有通用近似能力，并在实验中表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 当前解决SAT问题的方法可能效率低下或不适用于大规模问题，而图神经网络（GNNs）在处理结构化数据方面表现出色，因此作者试图利用GNNs和MILP技术来解决SAT问题。

**方法:** 将k-CNF公式转化为MILP问题，然后将其编码为加权二分图输入到GNN中进行训练和测试。从理论上证明了方法的置换不变性、等价不变性和近似能力，并识别出对于可折叠公式标准GNNs的局限性。

**结果:** 理论上证明了方法的稳定性和近似能力，并通过实验验证了即使使用简单的神经架构，该方法也能取得有希望的结果。

**结论:** 所提出的方法能够在有限数据集上以任意精度近似SAT求解，并且对于不可折叠公式不需要RNI即可实现相同的近似保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MILP-SAT-GNN%3A+Yet+Another+Neural+SAT+Solver，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01825&send_immediately=true&force_search=false)

**原文摘要:** We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [65] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang, Dunbo Cai, Yu Zhang, Yangqing Huang, Xiangyang Feng, Zhihong Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种增强的代理模型，通过在因子分解机及其相关的Ising表示中引入额外的松弛变量，将原本设计为两步的过程统一为一个集成步骤。实验结果表明，引入松弛变量显著提高了性能，该算法为构建利用潜在量子优势的有效代理模型提供了一个有前景的方法。


<details>
  <summary>更多</summary>
  
**动机:** 受最近提出的使用量子退火优化代理函数的代理模型启发，旨在改进输入-输出映射的近似方法。

**方法:** 在因子分解机及其关联的Ising表示中引入额外的松弛变量，将两步过程整合为一步，在训练阶段迭代更新松弛变量以考虑更高阶特征交互。

**结果:** 实验结果表明，引入松弛变量后性能有了显著提升。

**结论:** 所提出的算法为构建利用潜在量子优势的有效代理模型提供了一个有前景的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Surrogate+Modeling+via+Factorization+Machine+and+Ising+Model+with+Enhanced+Higher-Order+Interaction+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01389&send_immediately=true&force_search=false)

**原文摘要:** Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [66] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet, Christian Metzner, Laura Kriener, Melika Payvand*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为mGRADE的混合内存系统，结合了带可学习间隔的一维卷积和极简门控循环单元(minGRU)，以解决边缘设备在严格内存限制下对短时和长时动态建模的需求。mGRADE通过卷积层捕捉快速时间变化，并用循环模块高效维护全局上下文。实验表明，mGRADE在合成任务中能有效分离并保留多尺度时间特征，并在像素级图像分类基准上优于纯卷积或纯循环模型，同时减少约20%的内存占用，适合边缘设备的时间处理任务。


<details>
  <summary>更多</summary>
  
**动机:** 边缘设备需要能够捕捉短时和长时动态的模型，但这些模型通常受到严格内存限制。虽然Transformer擅长序列建模，但其内存随序列长度二次增长，不适用于此类场景；RNN提供常数内存但训练顺序进行；TCN虽高效，但内存随内核大小扩展。因此需要一种新的架构来满足这一需求。

**方法:** mGRADE是一种混合内存系统，结合了带可学习间隔的一维卷积和极简门控循环单元(minGRU)。卷积层实现灵活延迟嵌入，捕捉快速时间变化，而循环模块则以最小内存开销维持全局上下文。

**结果:** 在两个合成任务中验证了mGRADE的有效性，表明其能够分离和保留多尺度时间特征。在具有挑战性的逐像素图像分类基准上，mGRADE比纯卷积和纯循环模型性能更优，同时减少约20%的内存占用。

**结论:** mGRADE作为一种高效的解决方案，非常适合在内存受限的边缘设备上进行多尺度时间处理任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是mGRADE%3A+Minimal+Recurrent+Gating+Meets+Delay+Convolutions+for+Lightweight+Sequence+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01829&send_immediately=true&force_search=false)

**原文摘要:** Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [67] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels, Dylan Davis, Dhruv Gautam, Wentinn Liao, Gireeja Ranade, Anant Sahai*

**主要类别:** cs.LG

**AI概要:** 本研究介绍了一种新的玩具问题家族，结合了线性回归风格的连续上下文学习（ICL）和离散联想回忆的特点。通过对此玩具问题的样本轨迹进行变压器模型的预训练，研究发现模型在执行任务时需要具备两种功能：(1) 回忆序列的状态并应用到最近观察的状态；(2) 继续应用正确的系统以预测后续状态。研究表明，第一种能力在模型训练过程中较晚出现，而第二种能力则较早发展。通过对模型权重的边缘修剪实验和机制分析，发现该玩具问题中的下一个标记预测涉及至少两种不同的机制。最后，在ICL翻译任务中也观察到了类似的多机制现象。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探索变压器模型是否可以在特定提示下回忆起之前在其上下文中看到的序列状态，并研究其训练动态及机制。这将有助于理解变压器模型在处理连续上下文学习与离散联想回忆结合的问题时的行为。

**方法:** 研究者设计了一个新的玩具问题家族，结合了线性回归风格的连续上下文学习与离散联想回忆。使用从随机抽取的线性确定性动力系统中获得的符号标记交错状态观测值对变压器模型进行预训练。通过分析模型在不同训练阶段的能力表现以及对模型权重的边缘修剪实验，揭示了模型中的两种不同机制。

**结果:** 研究发现，模型在执行任务时需要具备两种功能：回忆序列状态并应用到最近观察的状态，以及继续应用正确的系统以预测后续状态。第一种能力在模型训练后期才出现，而第二种能力则较早发展。此外，模型中的下一个标记预测涉及至少两种不同的机制，分别用于离散符号标签的联想回忆和基于先前标记及上下文的“贝叶斯风格”预测。

**结论:** 本研究表明，变压器模型在处理连续上下文学习与离散联想回忆结合的问题时，存在多种机制且具有不同的学习动态。这种多机制现象不仅限于玩具问题设置，在ICL翻译任务中也观察到了类似的现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decomposing+Prediction+Mechanisms+for+In-Context+Recall，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01414，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01414&send_immediately=true&force_search=false)

**原文摘要:** We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [68] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González, Pedro Casas, Emilio Martínez, Alicia Fernández*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的时间序列建模方法，称为基础自编码器（FAE），它结合了变分自编码器（VAEs）和膨胀卷积神经网络（DCNNs），用于未见过的数据集中的异常检测。实验在多维时间序列数据集上展示了初步结果。


<details>
  <summary>更多</summary>
  
**动机:** 受到大型预训练基础模型成功的启发，研究者试图将这种方法应用于时间序列建模，以解决复杂的时间模式学习、预测以及异常检测问题。

**方法:** FAE模型基于变分自编码器（VAEs）和膨胀卷积神经网络（DCNNs）构建，适用于单变量时间序列建模，并能够在无需额外训练的情况下进行零样本异常检测。

**结果:** 在不同领域内的多维时间序列数据集上进行了初步测试，包括来自实际运营的移动ISP数据和著名的KDD 2021异常检测数据集，展示了该方法的有效性。

**结论:** FAE作为一种新型的时间序列建模方法，具有良好的潜力，可以在未见过的数据集上进行准确的异常检测，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Foundation+Auto-Encoders+for+Time-Series+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01875，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01875&send_immediately=true&force_search=false)

**原文摘要:** We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [69] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker, Yao Ma, Seyed Sahand Mohammadi Ziabari*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了一种结合LSTM和Transformers的混合深度学习方法，并通过孤立森林(iForest)和自动编码器(AE)进行伪标记，以解决医疗账单中的异常检测问题。在真实世界的账单数据集上评估了该方法，结果表明iForest LSTM基线在声明级数据上取得了最高的召回率(0.963)，而混合iForest模型在操作级数据上达到了最高召回率(0.744)，但精度较低。这表明在复杂的不平衡异常检测场景中，结合伪标记与混合深度学习具有潜力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习方法在处理医疗账单中的异常检测时面临类别不平衡、标签稀缺和复杂序列模式等问题，因此需要探索新的方法来提高检测效果。

**方法:** 研究采用了一种混合深度学习方法，结合了LSTM网络和Transformers，并使用孤立森林(iForest)和自动编码器(AE)进行伪标记。此方法被应用于两个真实世界的精神健康医疗账单数据集。

**结果:** 在声明级数据上，iForest LSTM基线达到最高召回率0.963；在操作级数据上，混合iForest模型获得最高召回率0.744，但精度有所下降。

**结论:** 结合伪标记与混合深度学习方法在复杂、不平衡的异常检测环境中显示出潜力，未来可以进一步优化以提高精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+a+Hybrid+Deep+Learning+Approach+for+Anomaly+Detection+in+Mental+Healthcare+Provider+Billing%3A+Addressing+Label+Scarcity+through+Semi-Supervised+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01924，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01924&send_immediately=true&force_search=false)

**原文摘要:** The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


### [70] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato, Fabio Gasparetti, Carla Limongelli, Stefano Mastandrea, Giuseppe Sansonetti, Joaquín Torres-Sospedra*

**主要类别:** cs.LG

**AI概要:** 为了克服博物馆环境中定位算法开发和评估的障碍，本文提出了BAR，一个新颖的RSS数据集，并提供了一个基于$k$-NN算法的位置分类基准。


<details>
  <summary>更多</summary>
  
**动机:** 现有的室内定位系统（IPSs）在文化遗址机构中具有巨大的潜力，但因缺乏特定反映博物馆环境的公开RSS数据集，导致难以针对这些场所的空间特点开发和评估定位算法。

**方法:** 收集了一个名为BAR的新颖RSS数据集，该数据集包含来自13个博物馆房间前90件艺术品的数据，并使用了Android和iOS两种平台。同时，提出了一种基于邻近度的方法和$k$-NN算法的高级位置分类基线。

**结果:** 通过分析新数据集和基准方法的结果，展示了其在博物馆环境中的适用性，并为未来的研究方向提供了建议。

**结论:** BAR数据集和相关方法为研究者提供了宝贵的资源，以改进和开发适用于文化遗址机构的室内定位算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-platform+Smartphone+Positioning+at+Museums，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01469&send_immediately=true&force_search=false)

**原文摘要:** Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [71] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar, Philipp Vaeth, Magda Gregorová*

**主要类别:** cs.LG

**AI概要:** 扩散模型作为强大的生成模型，其损失函数的选择对模型性能有重要影响。本文通过理论与实证研究，系统分析了不同目标函数及其关系，并在变分下界框架下统一了这些目标函数，为未来更高效、目标导向的模型设计提供了指导。


<details>
  <summary>更多</summary>
  
**动机:** 探讨扩散模型中不同的目标函数和对应的损失函数，以理解它们之间的联系与差异，从而优化模型设计。

**方法:** 详细分析不同目标函数及损失函数的关系，并将其统一到变分下界目标框架下；结合实证研究，探索目标函数在不同条件下的性能差异及影响因素。

**结果:** 揭示了不同目标函数在特定条件下的性能差异，并展示了目标函数选择对模型生成高质量样本或准确估计似然性等目标的影响。

**结论:** 提供对扩散模型损失函数的统一理解，有助于未来研究中更高效和目标导向的模型设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Loss+Functions+in+Diffusion+Models%3A+A+Comparative+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01516，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01516&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [72] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer, Lennart Purucker, Oussama Elachqar, Chinmay Hegde*

**主要类别:** cs.LG

**AI概要:** MARVIS是一种无需训练的方法，它使小型视觉-语言模型能够以高精度预测任何数据模式。通过将潜在嵌入空间转换为视觉表示，并利用VLM的空间和精细推理能力，MARVIS在多个领域（视觉、音频、生物、表格）中使用单一3B参数模型实现了与Gemini相比平均高出16%的性能，并接近专业方法的结果，同时不泄露个人可识别信息（P.I.I.），也不需要任何特定领域的训练。


<details>
  <summary>更多</summary>
  
**动机:** 基础模型虽然具有通用性，但在非传统模态和长尾领域上通常表现不如专门化模型。因此，研究者希望找到一种既能保持灵活性又能提升小规模模型性能的方法。

**方法:** MARVIS通过将潜在嵌入空间转化为视觉表示，然后利用视觉-语言模型的空间和细粒度推理能力来解释和利用这些表示，从而实现跨模态的高精度预测。该方法无需额外训练，适用于多种数据模态。

**结果:** MARVIS使用单一3B参数模型，在视觉、音频、生物和表格等多个领域取得了优异的成绩，比Gemini高出16%的平均表现，并接近专门化方法的效果，同时避免了个人可识别信息的泄露或领域特定的训练需求。

**结论:** MARVIS展示了一种无需训练且灵活的方法，使得小型视觉-语言模型能够在多模态任务中达到与专门化模型相近的性能，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MARVIS%3A+Modality+Adaptive+Reasoning+over+VISualizations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01544&send_immediately=true&force_search=false)

**原文摘要:** Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [73] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati, Neil Traft, Jeff Clune, Nick Cheney*

**主要类别:** cs.LG

**AI概要:** 在持续学习和少量样本迁移学习中，'zapping'（重新采样神经网络最后一层的权重）能帮助模型更快地从转换到新领域的冲击中恢复。此外，在多任务设置下，'zapping'和优化器的选择都会影响学习与遗忘的动力学特性，产生复杂的任务间协同/干扰模式。


<details>
  <summary>更多</summary>
  
**动机:** 尽管经验结果表明重新采样神经网络最后一层的权重（即“zapping”）有效，但其背后的机制尚不清楚。因此，研究者希望深入探究卷积神经网络在持续学习和少量样本迁移学习情境下的学习和遗忘模式。

**方法:** 研究者在持续学习和少量样本迁移学习的情境下，使用手写字符和自然图像进行实验，观察卷积神经网络的学习和遗忘模式，并评估‘zapping’对模型在转换到新领域后的恢复速度的影响。此外，还测量了每个单独任务受影响的程度，以更好地观察持续学习在多任务设置中的效果。

**结果:** 实验表明，经过‘zapping’训练的模型能够更快地从转换到新领域的冲击中恢复。而且，不仅‘zapping’会影响学习和遗忘的动力学特性，优化器的选择也会产生影响，导致模型在顺序学习时出现复杂的任务间协同/干扰模式。

**结论:** ‘zapping’在持续学习和迁移学习情境下有助于模型更快速地适应新领域，同时优化器的选择也显著影响学习与遗忘过程中的动力学特性。这表明在设计持续学习系统时需要综合考虑这些因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Weight+Resampling+and+Optimizers+Shape+the+Dynamics+of+Continual+Learning+and+Forgetting+in+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01559&send_immediately=true&force_search=false)

**原文摘要:** Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [74] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan, Wafa Njima, Xun Zhang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于联邦学习（FL）的动态室内定位方法，使用深度神经网络（DNN）模型。实验结果表明，该方法在保持数据隐私、带宽效率和服务器可靠性的同时，性能接近集中式模型（CL）。此研究表明，所提出的FL方法为增强隐私的室内定位提供了一个可行的解决方案，推动了安全高效室内定位系统的进步。


<details>
  <summary>更多</summary>
  
**动机:** 位置信息是众多物联网（IoT）应用的基本元素。传统的室内定位技术由于集中式数据收集，通常会产生显著误差并引发隐私问题。虽然机器学习（ML）技术通过捕捉室内环境变化提供了有希望的解决方案，但它们通常需要集中数据聚合，从而导致隐私、带宽和服务器可靠性问题。

**方法:** 为了克服上述挑战，本文提出了一种基于联邦学习（FL）的方法，用于使用深度神经网络（DNN）模型进行动态室内定位。

**结果:** 实验结果表明，联邦学习方法的性能接近集中式模型（CL），同时保持了数据隐私、带宽效率和服务器可靠性。

**结论:** 本研究证明，所提出的联邦学习（FL）方法为增强隐私的室内定位提供了一个可行的解决方案，为安全高效的室内定位系统的发展铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Privacy-Preserving+Indoor+Localization+System+based+on+Hierarchical+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01581，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01581&send_immediately=true&force_search=false)

**原文摘要:** Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [75] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato, Hiroki Naganuma, Hideaki Iiduka*

**主要类别:** cs.LG

**AI概要:** This paper introduces Muon, a new optimizer for neural networks, analyzes its convergence, and derives the critical batch size.


<details>
  <summary>更多</summary>
  
**动机:** To develop and analyze a new optimizer called Muon that exploits the inherent matrix structure of neural network parameters.

**方法:** Provide convergence proofs for four variants of Muon (with/without Nesterov momentum and weight decay), examine the effects of adding weight decay, clarify the relationship between weight decay coefficient and learning rate, and derive the critical batch size minimizing SFO complexity.

**结果:** Adding weight decay leads to tighter bounds on parameter and gradient norms. The relationship between weight decay coefficient and learning rate is clarified. The critical batch size minimizing SFO complexity is derived.

**结论:** Theoretical analysis of Muon's convergence properties and computational cost is provided, with experimental validation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Analysis+of+Muon%27s+Convergence+and+Critical+Batch+Size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01598，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01598&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [76] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor, Karl Skretting*

**主要类别:** cs.LG

**AI概要:** 提出了一种高效的基于核函数稀疏表示的在线词典学习算法，该算法在不同数据集上的实验结果表明，它比现有的在线核词典学习方法表现更好，同时保持与批量训练模型相近的分类准确性，但效率显著更高。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高在线词典学习算法的效率，并解决输入信号非线性映射到高维特征空间后稀疏表示的问题，需要一种能够递归更新词典并维持低计算复杂度的方法。

**方法:** 提出了一种基于递归最小二乘法（RLS）的新型算法，用于递归更新虚拟词典，可以处理单个样本或小批量样本，从而保持较低的计算复杂度。

**结果:** 在四个不同领域的数据集上的实验表明，该方法优于现有的在线核词典学习方法，并且其分类准确性接近批量训练模型，同时效率显著更高。

**结论:** 所提出的在线词典学习算法在效率和准确性方面都具有优势，适用于需要高效稀疏表示的应用场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kernel+Recursive+Least+Squares+Dictionary+Learning+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01636，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01636&send_immediately=true&force_search=false)

**原文摘要:** We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [77] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法DDCL，用于自动生成DDR图表，相较于之前的DDC方法有显著的准确率提升。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已经存在自动DDR图表生成算法（如DDC），但其准确率仍有提升空间。

**方法:** 引入了基于ConvLSTM模型的DDCL新方法，用于自动创建DDR图表。

**结果:** 与DDC相比，DDCL在DDR图表生成任务上的准确率显著提高。

**结论:** DDCL是一种改进的自动DDR图表生成方法，能够更精确地生成图表。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dance+Dance+ConvLSTM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01644，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01644&send_immediately=true&force_search=false)

**原文摘要:** \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [78] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende, Gayathri Ananthanarayanan, Marcello Traiola*

**主要类别:** cs.LG

**AI概要:** PERTINENCE是一种新方法，通过分析输入特征复杂度并动态选择合适的模型，在不影响精度的情况下提高效率。它使用遗传算法优化模型调度器，达到准确性和计算效率的平衡。实验表明，与现有最先进模型相比，PERTINENCE可以减少36%的操作数，同时保持或提高精度。


<details>
  <summary>更多</summary>
  
**动机:** 大型深度神经网络（DNN）虽然在多种领域表现出色，但资源和能耗需求高。大多数简单任务可由轻量级模型处理，仅部分复杂输入需要高计算成本。因此，研究如何动态结合现有DNN特性、降低资源消耗而不显著影响输出精度具有重要意义。

**方法:** 提出了一种名为PERTINENCE的新方法，该方法通过分析输入特征的复杂性，从预训练模型集合中动态选择最适合的模型来处理特定输入。为了实现这一目标，采用遗传算法探索基于机器学习的输入调度器的训练空间，使其收敛到解空间中的帕累托前沿，从而在整体准确率和计算效率之间取得平衡。

**结果:** 在CIFAR-10、CIFAR-100数据集上的卷积神经网络（CNN）以及TinyImageNet数据集上的视觉变换器（ViT）上进行了实验验证。结果表明，PERTINENCE能够提供在准确率和操作数之间的权衡方面优于或媲美现有最先进模型的解决方案，并且通过机会性地选择针对同一任务训练的模型，最多可减少36%的操作数，同时保持或提高精度。

**结论:** PERTINENCE为解决DNN模型资源消耗问题提供了新思路，通过动态调度模型实现了在减少计算操作的同时保持甚至提升精度的效果，为未来设计更高效、更灵活的深度学习系统提供了参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PERTINENCE%3A+Input-based+Opportunistic+Neural+Network+Dynamic+Execution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01695&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [79] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko, Juho Kanniainen, Alexandros Iosifidis*

**主要类别:** cs.LG

**AI概要:** 本文提出变分神经网络版本的空间和时空图卷积网络，估计模型输出和层间注意力的不确定性，从而提高模型可解释性和准确性。实验在社交交易分析和基于骨架的人体动作识别任务中展示其优势。


<details>
  <summary>更多</summary>
  
**动机:** 估计模型不确定性可以同时提升图卷积网络的可解释性和模型准确性，并在关键应用中通过专家或额外模型验证结果。

**方法:** 提出空间和时空图卷积网络的变分神经网络版本，估计模型输出和层间注意力的不确定性。

**结果:** 在芬兰董事会成员、NTU-60、NTU-120和Kinetics数据集上，展示了模型准确性的提升以及估计的模型不确定性。

**结论:** 提出的变分神经网络版本的图卷积网络能够有效提升模型可解释性和准确性，适用于社交交易分析和人体动作识别任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Graph+Convolutional+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01699&send_immediately=true&force_search=false)

**原文摘要:** Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [80] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner, Franz M. Rohrhofer, Bernhard C. Geiger*

**主要类别:** cs.LG

**AI概要:** 提出用贝叶斯PINN替代传统PINN集合方法，通过评估PINN的后验方差来扩展主动训练域，实验表明该方法在基准问题上优于集合方法且与Adam和LBFGS组合训练的PINN集合具有竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 传统PINN在前向问题中存在严重的收敛性问题，信息无法从初始条件有效传播到计算域内部。Haitsiukevich和Ilin（2023）提出的集合方法虽然能扩展每个PINN的主动训练域，但本文旨在改进这一方法。

**方法:** 使用贝叶斯PINN替代传统的PINN集合，并用PINN的后验方差评估取代集合共识，以确保信息从初始条件成功传播到计算域内部。

**结果:** 在一组基准问题上的实验表明，这种方法优于传统的PINN集合方法，并且与使用Adam和LBFGS组合训练的PINN集合具有竞争力。

**结论:** 贝叶斯PINN结合后验方差评估是一种有效的改进方法，能够解决PINN中的收敛性和信息传播问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是B-PL-PINN%3A+Stabilizing+PINN+Training+with+Bayesian+Pseudo+Labeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01714，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01714&send_immediately=true&force_search=false)

**原文摘要:** Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [81] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik, Theresa Eimer, Marius Lindauer*

**主要类别:** cs.LG

**AI概要:** The paper explores different paradigms of learning rate control in deep learning, finding that while some methods work well on selected tasks, they lack reliability across settings. It emphasizes the need for algorithm selection methods and highlights trends and promising directions in hyperparameter optimization.


<details>
  <summary>更多</summary>
  
**动机:** To assess the current state of learning rate control methods in deep learning and identify their strengths, limitations, and potential future directions.

**方法:** Comparison of various learning rate control paradigms including multi-fidelity hyperparameter optimization, fixed-hyperparameter schedules, and hyperparameter-free learning methods.

**结果:** Methods perform well on selected tasks but lack reliability across different settings. Hyperparameter optimization becomes less effective with increasing model and task complexity.

**结论:** There is a need for algorithm selection methods in learning rate control, and focusing on relevant test tasks and exploring new directions like meta-learning can enhance AutoML's impact on deep learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Learning+Rate+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01724&send_immediately=true&force_search=false)

**原文摘要:** The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [82] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang, Alceu Bissoto, Vihangkumar V. Naik, Tim Flühmann, Artemii Shlychkov, José Garcia-Tirado, Lisa M. Koch*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于神经后验估计的仿真基础推断方法(SBI)，用于高效捕捉饮食摄入、胰岛素和血糖水平之间的复杂关系，解决了传统方法在高维参数空间中计算缓慢且昂贵的问题。实验表明SBI在参数估计上优于传统方法，并能更好地推广到未见条件，提供实时的后验推断和可靠的不确定性量化。


<details>
  <summary>更多</summary>
  
**动机:** 准确估计生理模型的参数对于实现可靠的数字孪生体至关重要。对于1型糖尿病，由于葡萄糖-胰岛素相互作用的复杂性，这一任务尤其具有挑战性。传统基于马尔可夫链蒙特卡洛的方法在处理高维参数空间时效率低下，并且需要从头开始拟合参数，导致速度慢且计算成本高。

**方法:** 研究采用了基于神经后验估计的仿真基础推断（Simulation-Based Inference, SBI）方法。该方法能够高效地捕捉饮食摄入、胰岛素和血糖水平间的复杂关系，提供更快的摊销推理。

**结果:** 实验结果表明，SBI在参数估计方面优于传统方法，并且对未见条件具有更好的泛化能力。此外，它还提供了实时的后验推断和可靠的不确定性量化。

**结论:** SBI是一种更高效、更快速的方法，适用于复杂的生理模型参数估计，特别是在1型糖尿病场景中，为实时应用提供了可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Real-Time+Digital+Twin+for+Type+1+Diabetes+using+Simulation-Based+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01740，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01740&send_immediately=true&force_search=false)

**原文摘要:** Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [83] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue, Meghana Madhyastha, Randal Burns, Myungjin Lee, Mahesh K. Marina*

**主要类别:** cs.LG

**AI概要:** 提出了一种利用边缘AI设备集体计算能力进行去中心化和可持续基础模型训练的愿景，以应对环境影响和集中控制风险。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基础模型尽管能够从大量数据中学习并适用于各种任务，但其巨大的计算需求带来了环境影响和开发过程中集中控制的风险。

**方法:** 通过利用未充分利用的连接边缘AI设备的集体计算能力，推动去中心化和可持续的基础模型训练。

**结果:** 提出了该愿景的合理性，特别是其可持续性优势，并概述了实现这一愿景需要解决的一系列挑战。

**结论:** 去中心化和可持续的基础模型训练是一个值得追求的方向，但需要克服多个技术和社会挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Decentralized+and+Sustainable+Foundation+Model+Training+with+the+Edge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01803&send_immediately=true&force_search=false)

**原文摘要:** Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [84] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko, Nadiya Shvai*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于模型的强化学习中的知识蒸馏新方法，成功将大参数量多任务智能体压缩至小规模模型，同时在MT30基准上显著提升性能，并通过量化进一步优化模型大小。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型世界模型虽然功能强大，但在资源受限环境下难以部署。因此需要一种高效的知识蒸馏技术，以实现模型的压缩而不损失性能。

**方法:** 通过知识蒸馏技术，将一个具有3.17亿参数的大规模多任务智能体压缩为仅有1百万参数的小型模型。随后使用FP16后训练量化进一步减小模型尺寸约50%。

**结果:** 蒸馏后的模型在MT30基准上达到28.45的归一化分数，优于原始1百万参数模型的18.93分，并且模型尺寸显著减小。

**结论:** 该方法解决了大型世界模型在资源受限环境下的部署问题，为更高效、更普及的多任务强化学习系统铺平了道路，特别是在机器人等应用领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TD-MPC-Opt%3A+Distilling+Model-Based+Multi-Task+Reinforcement+Learning+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01823&send_immediately=true&force_search=false)

**原文摘要:** We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [85] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao, Vincent Y. F. Tan*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于次模函数最大化的低秩适应（LoRA）的秩确定方法——SubLoRA，利用二阶信息（Hessian矩阵）捕捉复杂的损失景观，并通过理论基础、计算效率和实验验证展示了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LoRA方法如AdaLoRA依赖于线性化近似，在LoRA参数优化良好时变得不准确且条件差，因此需要更可靠和细致的二阶公式。

**方法:** 将秩确定问题重构成具有二次目标的组合优化问题，并引入次模函数最大化框架及贪婪算法解决计算挑战；同时推导充分必要条件并构建闭式投影以满足次模性。此外，扩展到联合优化设置中，在秩预算约束下交替更新LoRA参数和确定秩。

**结果:** 在微调物理信息神经网络（PINNs）求解偏微分方程（PDEs）的广泛实验中，SubLoRA在秩确定和联合训练性能上均优于现有方法。

**结论:** SubLoRA结合了坚实的理论基础、二阶精确性和实际计算效率，在秩确定和联合训练中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automatic+Rank+Determination+for+Low-Rank+Adaptation+via+Submodular+Function+Maximization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01841&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [86] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的生成模型MetaStone-S1，该模型通过自监督过程奖励模型（SPRM）获得了与OpenAI o3相当的性能。SPRM通过共享主干网络和使用特定任务的头部来分别进行下一个令牌预测和过程评分，成功地将策略模型和过程奖励模型整合到一个统一的接口中，减少了99%以上的PRM参数以实现高效的推理。配备SPRM的MetaStone-S1自然适用于测试时间扩展（TTS），并提供了三种推理努力模式（低、中、高），基于可控的思考长度。此外，我们经验性地建立了一个缩放定律，揭示了总思考计算量与TTS性能之间的关系。实验表明，我们的MetaStone-S1在只有32B参数规模的情况下实现了与OpenAI-o3-mini系列相当的性能。为了支持研究社区，我们在https://github.com/MetaStone-AI/MetaStone-S1开源了MetaStone-S1。


<details>
  <summary>更多</summary>
  
**动机:** 开发一种高效的生成模型，能够通过减少参数数量和优化推理过程，达到与现有顶级模型（如OpenAI o3）相媲美的性能。同时，探索推理计算量与性能之间的关系，并提供灵活的推理模式以适应不同的应用场景。

**方法:** 引入了自监督过程奖励模型（SPRM），通过共享主干网络和使用特定任务的头部来分别进行下一个令牌预测和过程评分，从而整合策略模型和过程奖励模型。此方法显著减少了PRM参数，提高了推理效率。模型还支持测试时间扩展（TTS），并提供了三种推理努力模式（低、中、高）。

**结果:** MetaStone-S1在只有32B参数规模的情况下，实现了与OpenAI-o3-mini系列相当的性能。并且，经验性地建立了一个缩放定律，揭示了总思考计算量与TTS性能之间的关系。

**结论:** MetaStone-S1是一个高效的生成模型，通过SPRM技术大幅减少了参数数量，同时保持了高水平的性能。它适用于不同的推理需求，并且其性能与计算量的关系已被明确。模型已开源，为研究社区提供了宝贵的资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-Time+Scaling+with+Reflective+Generative+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01951，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01951&send_immediately=true&force_search=false)

**原文摘要:** We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela, Pablo Romero-Sorozabal, Eduardo Rocon, Manuel Cebrian*

**主要类别:** cs.AI

**AI概要:** 今年早些时候，苹果公司通过发布《思考的幻觉》引发了争议，导致AI社区对大型推理模型（LRM）的能力产生了激烈争论。批评者认为LRM只是随机鹦鹉，缺乏真正的推理能力，而支持者则指责实验设计有缺陷且结论被夸大。本文通过复制并改进原研究中最具争议的两个基准测试——河内塔和渡河问题，引入逐步提示和代理协作对话，揭示了LRM在复杂性适度增加时仍然会遇到认知限制。同时，当仅测试可解问题时，LRM可以轻松解决涉及超过100个代理对的大规模实例。最终研究表明，LRM是离散状态空间中的随机、强化学习调优的搜索器，未来需要通过精细消融分析来探索这一领域。


<details>
  <summary>更多</summary>
  
**动机:** 由于苹果公司发布的论文引发了关于LRM推理能力的争议，本文旨在澄清这场辩论，并通过重新评估和改进实验方法，揭示LRM的实际能力和局限性。

**方法:** 作者复制并改进了原研究中的河内塔和渡河问题两个基准测试，引入了增量逐步提示和代理协作对话技术，以更细致地分析LRM在不同任务中的表现。

**结果:** 结果表明，LRM在面对复杂性适度增加的任务时确实存在认知限制，但并非完全归因于输出约束；此外，在仅测试可解问题的情况下，LRM能够成功解决大规模实例。

**结论:** 今天的LRM实际上是离散状态空间中的随机、强化学习调优的搜索器，其推理能力不能简单归类为‘没有’或‘有’。为了取得真实进展，需要通过细粒度消融分析进一步探索这一领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Illusion+of+Thinking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01231，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01231&send_immediately=true&force_search=false)

**原文摘要:** Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [88] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang, Wenli Yang, Monica R Roberts, Byeong Ho Kang, Charles B Malpas*

**主要类别:** cs.AI

**AI概要:** 近期大语言模型（LLMs）的兴起重新点燃了人工智能（AI）系统辅助医疗诊断的希望。然而，尽管在基准测试中表现出色，但LLM助手尚未在临床实践中显示出可测量的改进。本综述旨在强调AI在临床环境中实际贡献有限的领域，特别是在痴呆症诊断和护理方面。独立的机器学习模型在模式识别方面表现出色，但很少提供可操作、可解释的指导，这削弱了临床医生的信任。将LLM与专家规则知识结合的混合方法有助于提高可解释性，并更好地适应现有的临床工作流程。未来的决策支持应优先考虑解释性一致性，通过将预测与临床上有意义的原因联系起来。这可以通过结合LLM的语言能力和人类因果专业知识的神经符号或混合AI来实现。未来的研究不仅应关注准确性，还应关注改善临床医生的理解、工作流程适应性和患者结果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在基准测试中表现优异，但在临床实践中的实际应用效果不佳，特别是在痴呆症诊断和护理方面存在局限性。因此，需要探索如何利用AI技术克服这些局限性，以提高临床实践的效果。

**方法:** 本文采用综述方法，分析了独立机器学习模型和LLM在临床应用中的局限性，并探讨了结合统计学习与专家规则知识的混合方法。此外，还讨论了神经符号AI和可解释AI的发展方向，以及如何通过这些技术提高AI系统的可解释性和实用性。

**结果:** 研究发现，独立的机器学习模型和LLM在临床应用中存在局限性，如缺乏透明度、易产生幻觉和因果推理能力弱等问题。而结合统计学习与专家规则知识的混合方法可以提高可解释性，并更好地适应临床工作流程。未来的研究方向包括神经符号AI和可解释AI，以及关注临床医生理解和患者结果的评估标准。

**结论:** 为了使AI系统成为临床实践的一部分，需要更好地理解如何改善人机交互。未来的研究应不仅关注准确性，还应关注临床医生的理解、工作流程适应性和患者结果等方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Black-Box+AI%3A+Interpretable+Hybrid+Systems+for+Dementia+Care，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01282&send_immediately=true&force_search=false)

**原文摘要:** The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [89] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren, Yangyang Liu, Tang Ji, Xun Xu*

**主要类别:** cs.AI

**AI概要:** 这篇论文系统地回顾了AI和AI代理技术的演变，探讨了基于LLM、MLLM的AI代理及Agentic AI的核心概念与技术进步，并研究了它们在智能制造中的潜在应用及挑战。


<details>
  <summary>更多</summary>
  
**动机:** 随着生成式AI、大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的发展，AI代理在语义理解、复杂推理和自主决策方面的能力显著提高，同时Agentic AI强调在动态和复杂环境中的适应性和目标导向的自主性，但这些新兴AI范式在智能制造中的定义、能力边界和实际应用尚不明确。

**方法:** 通过系统地审查AI和AI代理技术的演变，分析LLM-Agents、MLLM-Agents和Agentic AI的核心概念和技术进步，探索其在智能制造中的潜在应用和整合方式，并评估可能面临的挑战。

**结果:** 明确了AI代理技术的进展及其在智能制造中的潜在应用领域，同时也揭示了实施过程中可能遇到的挑战。

**结论:** 尽管基于LLM、MLLM的AI代理和Agentic AI为智能制造提供了新的可能性，但其定义、能力范围和实际应用仍需进一步研究，以克服潜在的技术和社会挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Agents+and+Agentic+AI-Navigating+a+Plethora+of+Concepts+for+Future+Manufacturing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01376，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01376&send_immediately=true&force_search=false)

**原文摘要:** AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [90] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub, Francesca A. Lisi*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种基于伦理风险评估的形式化方法来描述道德决策模型，并展示了如何使用模糊Petri网验证和确认以模糊规则指定的这些模型。通过医学领域的案例研究说明了所提方法。


<details>
  <summary>更多</summary>
  
**动机:** 道德领域的本体论和认识论复杂性使得为道德机器的性能评估建立明确标准变得具有挑战性。

**方法:** 提出一种基于伦理风险评估的形式化方法来描述伦理决策模型，利用模糊Petri网对以模糊规则表示的模型进行验证和确认。

**结果:** 通过医学领域的案例研究展示了所提方法的应用。

**结论:** 所提出的基于伦理风险评估的方法能够有效描述伦理决策模型，并且模糊Petri网可以用于验证和确认这些模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Fuzzy+Approach+to+the+Specification%2C+Verification+and+Validation+of+Risk-Based+Ethical+Decision+Making+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01410，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01410&send_immediately=true&force_search=false)

**原文摘要:** The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [91] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang, Minjune Kim, Marlon Rondinelli, Keren Shao*

**主要类别:** cs.AI

**AI概要:** Pensieve 是一个使用大型语言模型（LLMs）来转录和评估学生手写答案的人工智能辅助评分平台，能显著减少评分时间并保持高准确率。


<details>
  <summary>更多</summary>
  
**动机:** 在大规模大学 STEM 课程中，批改手写开放性回答仍然是主要瓶颈。

**方法:** Pensieve 平台利用 LLMs 转录和评估学生作业，并通过与评分标准对齐的分数、转录和置信度评级为教师提供支持。它涵盖了从扫描学生提交的答案到最终反馈的整个评分流程，并且包含人在回路（human-in-the-loop）界面。

**结果:** Pensieve 已在超过20个机构的实际课程中部署，评阅了超过30万份学生答案。研究表明，Pensieve 平均减少了65%的评分时间，同时对于高置信度预测，与教师所给分数的一致率达到95.4%。

**结论:** Pensieve 支持完整的评分流程，在减少评分时间方面表现出色，同时保持高评分一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pensieve+Grader%3A+An+AI-Powered%2C+Ready-to-Use+Platform+for+Effortless+Handwritten+STEM+Grading，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01431，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01431&send_immediately=true&force_search=false)

**原文摘要:** Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [92] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer, Magdi Amer*

**主要类别:** cs.AI

**AI概要:** A multi-agent system combining LLMs and fuzzy logic is proposed to handle SMS customer requests while reducing hallucination risks.


<details>
  <summary>更多</summary>
  
**动机:** To improve customer service quality and response time, which are essential for maintaining customer loyalty and increasing market share, while addressing the challenge of hallucination in LLMs.

**方法:** The method involves creating a multi-agent system that processes customer requests received via SMS. This system uses LLM-based agents alongside fuzzy logic to reduce the risk of hallucinations.

**结果:** The results indicate that the multi-agent system effectively handles customer requests and significantly reduces the occurrence of hallucinations compared to using LLMs alone.

**结论:** The integration of fuzzy logic with LLMs in a multi-agent system provides a promising solution for improving customer service through SMS while mitigating the risk of hallucinations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Using+multi-agent+architecture+to+mitigate+the+risk+of+LLM+hallucinations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01446，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01446&send_immediately=true&force_search=false)

**原文摘要:** Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [93] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

**主要类别:** cs.AI

**AI概要:** 大型语言模型（LLMs）在人工智能领域取得了重要进展，其理解和生成自然语言的能力改变了我们与AI系统的交互方式。鉴于基于LLM的代理和基于强化学习的推理模型的发展，将强化学习应用于代理框架的研究成为新的研究热点。然而，以往的研究面临同时决定工具调用过程和推理过程的挑战，推理链仅依赖于工具提供的冗余信息和无关符号的原始结果，这对模型的推理能力构成了沉重负担。为了解决这一问题，我们提出了一个分层框架Agent-as-tool，该框架分离了工具调用过程和推理过程，使模型能够专注于口头推理过程，而工具调用过程由另一个代理处理。我们的工作仅通过轻微的强化微调，在180个样本上实现了可比的结果，并在Bamboogle数据集上取得了卓越的表现，精确匹配率达到63.2%，覆盖精确匹配率达到75.2%，分别比Search-R1高出4.8%和3.2%。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，大型语言模型（LLMs）作为人工智能领域最重要的技术进步之一，其理解、生成和推理自然语言的能力已经改变了我们与AI系统的互动方式。随着基于LLM的代理和基于强化学习的推理模型的发展，将强化学习应用到代理框架中的研究成为一个新的研究焦点。然而，所有先前的研究都面临着同时决定工具调用过程和推理过程的挑战，且推理链完全依赖于工具提供的包含冗余信息和与任务无关符号的原始结果，这给模型的推理能力带来了沉重负担。

**方法:** 我们提出了一种分层框架Agent-as-tool，该框架将工具调用过程和推理过程分离，从而使模型能够专注于口头推理过程，而工具调用过程则由另一个代理处理。

**结果:** 我们的工作仅通过轻微的强化微调，在180个样本上实现了可比的结果，并在Bamboogle数据集上取得了卓越的表现，精确匹配率达到63.2%，覆盖精确匹配率达到75.2%，分别比Search-R1高出4.8%和3.2%。

**结论:** 所提出的分层框架Agent-as-tool有效地解决了工具调用和推理过程中存在的问题，显著提升了模型在推理任务中的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent-as-Tool%3A+A+Study+on+the+Hierarchical+Decision+Making+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01489，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01489&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [94] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si, Zefan Zeng, Jincai Huang, Qing Cheng*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的分布特征建模方法T3DM，用于训练时序知识图推理模型，并设计了一种对抗训练的负采样策略。实验表明T3DM在大多数情况下比现有方法更好、更稳健。


<details>
  <summary>更多</summary>
  
**动机:** 现有的TKG推理研究面临两个挑战：1）训练和测试样本之间的事件分布偏移建模不足；2）生成负样本依赖随机实体替换，导致采样质量低。

**方法:** 提出Test-Time Training-guided Distribution shift Modelling (T3DM) 方法调整模型以应对分布偏移并确保全局一致性，同时设计一种基于对抗训练的负采样策略生成高质量负样本。

**结果:** 大量实验证明T3DM在大多数情况下比最先进的基线方法提供更好的结果且更加鲁棒。

**结论:** T3DM能够有效应对分布偏移问题，提高时序知识图推理模型的性能和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是T3DM%3A+Test-Time+Training-Guided+Distribution+Shift+Modelling+for+Temporal+Knowledge+Graph+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01597&send_immediately=true&force_search=false)

**原文摘要:** Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [95] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati*

**主要类别:** cs.AI

**AI概要:** 本研究设计了Agent Ideate框架，利用大型语言模型（LLMs）和自主代理从专利中自动生成基于产品的商业创意。通过跨计算机科学、自然语言处理和材料化学领域的实验表明，代理方法在创意质量、相关性和新颖性方面优于单独的LLM模型。这说明将LLMs与代理工作流结合可以显著提升创新流程，挖掘专利数据中未开发的商业创意潜力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管专利包含丰富的技术知识，可以激发创新的产品理念，但获取和解释这些信息仍然是一个挑战。因此需要探索更有效的方法来利用专利中的信息生成产品概念。

**方法:** 设计了一个名为Agent Ideate的框架，该框架利用开源的大规模语言模型（LLMs）和基于代理的架构，从三个领域（计算机科学、自然语言处理和材料化学）的专利中自动生成基于产品的商业创意。

**结果:** 评估结果显示，代理方法在创意的质量、相关性和新颖性方面始终优于单独的LLM模型。

**结论:** 结合大规模语言模型与代理工作流可以从专利数据中释放出未开发的商业创意潜力，从而显著增强创新流程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent+Ideate%3A+A+Framework+for+Product+Idea+Generation+from+Patents+Using+Agentic+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01717，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01717&send_immediately=true&force_search=false)

**原文摘要:** Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [96] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan, Mucahit Cevik, Merve Bodur, Bissan Ghaddar*

**主要类别:** cs.AI

**AI概要:** 本文探讨了在集中式众包配送系统中使用店内顾客作为配送员的方法，以满足城市地区对高效最后一公里配送日益增长的需求。通过结合神经近似动态规划（NeurADP）和深度双Q网络（DDQN），提出了一种马尔可夫决策过程（MDP）模型来管理配送流程，并实现了显著的成本节约。实验结果表明，与固定定价的NeurADP相比，集成策略节省了6.7%的成本，而相对于短视基准则节省了约18%的成本。允许灵活的配送延迟和多目的地路由可以进一步分别降低8%和17%的操作成本。


<details>
  <summary>更多</summary>
  
**动机:** 城市地区对高效最后一公里配送的需求不断增长，而传统物流方式可能无法完全满足这种需求。因此，需要探索新的配送模式，例如利用店内顾客作为配送员，以降低成本并提高效率。

**方法:** 论文采用了一个马尔可夫决策过程（MDP）模型来捕捉关键不确定性，包括订单和配送员的随机到达以及配送邀请接受的概率。解决方案结合了神经近似动态规划（NeurADP）用于自适应订单分配和深度双Q网络（DDQN）用于动态定价。此外，该方法支持多点配送路由并考虑了配送邀请接受的不确定性。

**结果:** 实验结果表明，NeurADP + DDQN联合优化策略相较于固定价格的NeurADP节省了6.7%的成本，相对于短视基线节省了大约18%的成本。此外，允许灵活的配送延迟和启用多目的地路由分别降低了8%和17%的操作成本。

**结论:** 动态、前瞻性的策略在众包配送系统中具有显著优势，能够有效降低操作成本并提高配送效率。这些发现为城市物流运营商提供了实际指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Joint+Matching+and+Pricing+for+Crowd-shipping+with+In-store+Customers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01749&send_immediately=true&force_search=false)

**原文摘要:** This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [97] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen, Thomas Eiter*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了非单调逻辑编程中的两个关键问题：（1）现有的最小模型特性、约束单调性和有基性是否应作为答案集语义的强制条件？（2）如果不是，哪些其他性质可以作为一般原则？论文提出了一种改进的Gelfond答案集（GAS）原则，包括良好支持性、默认否定的最小性和认知否定的最小性，并基于这些原则定义了新的答案集语义。此外，论文还评估了现有答案集语义并分析了计算复杂度。


<details>
  <summary>更多</summary>
  
**动机:** 当前的答案集语义可能过于严格，排除了一些预期的答案集。因此需要重新审视答案集语义的基本条件，并探索更合理的替代原则。

**方法:** 论文通过以下步骤进行研究：(1) 分析现有答案集语义的限制；(2) 提出改进的GAS原则，包括良好支持性、默认否定的最小性和认知否定的最小性；(3) 基于这些原则扩展了良好支持性的概念；(4) 定义了新的答案集语义；(5) 使用新原则评估现有答案集语义；(6) 分析计算复杂度。

**结果:** 提出了改进的GAS原则，定义了新的答案集语义，并提供了一个评估现有答案集语义的新基准。同时，对新方法的计算复杂度进行了分析。

**结论:** 改进的GAS原则为答案集语义提供了更合理的基础，能够更好地满足非单调逻辑编程的需求。新的答案集语义和评估基准为未来的研究提供了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Refining+Gelfond+Rationality+Principle+Towards+More+Comprehensive+Foundational+Principles+for+Answer+Set+Semantics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01833，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01833&send_immediately=true&force_search=false)

**原文摘要:** Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [98] [Asymptotic convexity of wide and shallow neural networks](https://arxiv.org/abs/2507.01044)
*Vivek Borkar, Parthe Pandit*

**主要类别:** stat.ML

**AI概要:** 通过研究浅层和宽神经网络模型，发现其输入-输出映射能近似凸函数，从而解释了其良好性能。


<details>
  <summary>更多</summary>
  
**动机:** 需要对浅层和宽神经网络模型的良好性能提供一个合理的解释。

**方法:** 研究了浅层和宽神经网络模型的参数函数的输入-输出映射。

**结果:** 发现该模型的输入-输出映射函数的表象在某种意义上能近似凸函数的表象。

**结论:** 浅层和宽神经网络模型的输入-输出映射函数的表现可以近似凸函数，这解释了它们的良好性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotic+convexity+of+wide+and+shallow+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01044&send_immediately=true&force_search=false)

**原文摘要:** For a simple model of shallow and wide neural networks, we show that the
epigraph of its input-output map as a function of the network parameters
approximates epigraph of a. convex function in a precise sense. This leads to a
plausible explanation of their observed good performance.

</details>


### [99] [Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles](https://arxiv.org/abs/2507.01542)
*Tom Szwagier, Pierre-Alexandre Mattei, Charles Bouveyron, Xavier Pennec*

**主要类别:** stat.ML

**AI概要:** 论文提出了一种新的高斯混合模型（GMMs）家族，通过分段常数协方差特征值分布，在低秩模型（如MPPCA）的基础上扩展了模型的灵活性。论文还提出了一个组件惩罚的EM算法来联合学习模型参数和超参数，并在多个无监督任务上展示了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的高斯混合模型在高维空间中存在过参数化问题，而球形GMM又缺乏拟合各向异性分布的灵活性。为了连接这两种极端情况并提高模型效率，作者引入了一种具有分段常数协方差特征值分布的新GMM族。

**方法:** 新模型允许任何可能的特征值重数序列，从而扩展了低秩模型的能力。当特征值重数预先指定时，可以推导出标准的EM算法；否则，作者提出了一种组件惩罚的EM算法以解决联合学习模型参数和超参数的挑战，并证明了该算法的单调性。

**结果:** 实验结果表明，该模型在多种无监督任务（包括密度拟合、聚类和单图像去噪）中实现了更好的似然-简约权衡。

**结论:** 所提出的具有分段常数协方差特征值分布的GMM族及其学习算法，在保持模型简洁的同时提高了对复杂数据分布的拟合能力，适用于各种无监督学习任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parsimonious+Gaussian+mixture+models+with+piecewise-constant+eigenvalue+profiles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01542&send_immediately=true&force_search=false)

**原文摘要:** Gaussian mixture models (GMMs) are ubiquitous in statistical learning,
particularly for unsupervised problems. While full GMMs suffer from the
overparameterization of their covariance matrices in high-dimensional spaces,
spherical GMMs (with isotropic covariance matrices) certainly lack flexibility
to fit certain anisotropic distributions. Connecting these two extremes, we
introduce a new family of parsimonious GMMs with piecewise-constant covariance
eigenvalue profiles. These extend several low-rank models like the celebrated
mixtures of probabilistic principal component analyzers (MPPCA), by enabling
any possible sequence of eigenvalue multiplicities. If the latter are
prespecified, then we can naturally derive an expectation-maximization (EM)
algorithm to learn the mixture parameters. Otherwise, to address the
notoriously-challenging issue of jointly learning the mixture parameters and
hyperparameters, we propose a componentwise penalized EM algorithm, whose
monotonicity is proven. We show the superior likelihood-parsimony tradeoffs
achieved by our models on a variety of unsupervised experiments: density
fitting, clustering and single-image denoising.

</details>


### [100] [When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery](https://arxiv.org/abs/2507.01613)
*Shirong Xu, Jingnan Zhang, Junhui Wang*

**主要类别:** stat.ML

**AI概要:** 在配对比较数据中，将序数数据二值化可以显著提高排名恢复的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管序数比较数据直觉上比二元比较数据提供更丰富的信息，本文挑战了这一传统观点。

**方法:** 提出了一种通用的参数框架来建模无平局的序数配对比较。该模型采用广义加法结构，包含一个量化两个项目偏好差异的连接函数和一个控制序数响应水平分布的模式函数。此框架将经典的二元比较模型作为特殊情况进行涵盖，并通过理论分析和实验证明了将序数数据二值化的优越性。

**结果:** 证明了在计数算法下，与序数数据相比，二元比较数据的排名误差具有更快的指数收敛速度。并且通过信噪比（SNR）确定了二元和序数数据之间的显著性能差距，识别出了最大化二值化收益的模式函数。

**结论:** 二值化序数数据可以在排名恢复任务中显著提高准确性，理论和实验结果支持这一点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Less+Is+More%3A+Binary+Feedback+Can+Outperform+Ordinal+Comparisons+in+Ranking+Recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01613&send_immediately=true&force_search=false)

**原文摘要:** Paired comparison data, where users evaluate items in pairs, play a central
role in ranking and preference learning tasks. While ordinal comparison data
intuitively offer richer information than binary comparisons, this paper
challenges that conventional wisdom. We propose a general parametric framework
for modeling ordinal paired comparisons without ties. The model adopts a
generalized additive structure, featuring a link function that quantifies the
preference difference between two items and a pattern function that governs the
distribution over ordinal response levels. This framework encompasses classical
binary comparison models as special cases, by treating binary responses as
binarized versions of ordinal data. Within this framework, we show that
binarizing ordinal data can significantly improve the accuracy of ranking
recovery. Specifically, we prove that under the counting algorithm, the ranking
error associated with binary comparisons exhibits a faster exponential
convergence rate than that of ordinal data. Furthermore, we characterize a
substantial performance gap between binary and ordinal data in terms of a
signal-to-noise ratio (SNR) determined by the pattern function. We identify the
pattern function that minimizes the SNR and maximizes the benefit of
binarization. Extensive simulations and a real application on the MovieLens
dataset further corroborate our theoretical findings.

</details>


### [101] [A generative modeling / Physics-Informed Neural Network approach to random differential equations](https://arxiv.org/abs/2507.01687)
*Georgios Arampatzis, Stylianos Katsarakis, Charalambos Makridakis*

**主要类别:** stat.ML

**AI概要:** This paper integrates SciML and UQ by advancing PINNs with probabilistic frameworks for uncertainty modeling in complex systems, demonstrated through applications to random differential equations.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of this paper is to address the challenge of effectively modeling uncertainty in complex systems by integrating Scientific Machine Learning (SciML) techniques with uncertainty quantification (UQ).

**方法:** The method involves enhancing Physics-Informed Neural Networks (PINNs) by incorporating probabilistic frameworks. This combination allows for systematic uncertainty control while maintaining model accuracy.

**结果:** The results demonstrate the utility of this method through successful applications to random differential equations and random partial differential equations (PDEs).

**结论:** This integration of generative modeling techniques with PINNs provides a powerful approach for uncertainty modeling in complex systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+generative+modeling+%2F+Physics-Informed+Neural+Network+approach+to+random+differential+equations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01687，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01687&send_immediately=true&force_search=false)

**原文摘要:** The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

</details>
