<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 71]
- [cs.AI](#cs.AI) [总数: 13]
- [cs.CR](#cs.CR) [总数: 15]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao, Qianyi Chen, Haoran Li, Haosu Zhou, Hamid Reza Attar, Tobias Pfaff, Tailin Wu, Nan Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于图神经网络的替代模型RUGNN，它能更准确地预测板材成型过程中的变形场，并且在冷热成型案例中得到了验证。


<details>
  <summary>更多</summary>
  
**动机:** 传统的AI替代模型在捕捉复杂的3D空间关系和处理置换不变性方面存在局限性，因此需要开发一种可以克服这些问题的新方法。

**方法:** 研究人员开发了RUGNN模型，结合了门控循环单元（GRUs）以模拟时间动态变化，并采用U-Net启发式的图下采样/上采样机制来处理空间长距离依赖关系，同时提出了'节点到表面'接触表示法提高计算效率。

**结果:** 实验结果表明，RUGNN模型能够提供精确的变形预测，与真实FE仿真结果非常接近，并优于几种基准GNN架构。

**结论:** RUGNN模型是一个可靠的方法，支持板材成型设计，通过实现准确的可制造性预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Recurrent+U-Net-Based+Graph+Neural+Network+%28RUGNN%29+for+Accurate+Deformation+Predictions+in+Sheet+Material+Forming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11547，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11547&send_immediately=true&force_search=false)

**原文摘要:** In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [2] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho, Sairam Sutari, Alexander Lopez, Hansen Bow, Kai Zheng*

**主要类别:** cs.LG

**AI概要:** 本文开发并评估了用于预测择期脊柱手术住院时间(LOS)的机器学习模型，特别是SurgeryLSTM模型。SurgeryLSTM实现了最高的预测准确性，并提高了模型的可解释性。研究结果支持将基于时间序列的、可解释的机器学习方法整合到临床决策支持系统中，以提高出院准备度和个性化患者护理。


<details>
  <summary>更多</summary>
  
**动机:** 为了改善对择期脊柱手术患者住院时间的预测，利用时间建模和模型可解释性的优势，从而提供更准确和透明的预测工具，辅助医院规划和临床决策。

**方法:** 研究人员比较了传统的机器学习模型（如线性回归、随机森林、支持向量机和XGBoost）与他们开发的SurgeryLSTM模型，后者是带有注意力机制的屏蔽双向长短期记忆网络。性能评估使用了决定系数(R2)，并通过可解释AI识别关键预测因素。

**结果:** SurgeryLSTM模型在预测准确性上达到了最高（R2=0.86），超过了XGBoost（R2 = 0.85）和其他基准模型。注意力机制提高了模型的可解释性，使临床医生能够追踪哪些事件或特征对每个LOS预测贡献最大。骨病、慢性肾病和腰椎融合被认为是影响LOS的主要预测因素。

**结论:** SurgeryLSTM为择期脊柱手术的LOS预测提供了一个有效且可解释的人工智能解决方案。这些发现支持将时间序列、可解释的机器学习方法集成到临床决策支持系统中，以增强出院准备和个体化患者护理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SurgeryLSTM%3A+A+Time-Aware+Neural+Model+for+Accurate+and+Explainable+Length+of+Stay+Prediction+After+Spine+Surgery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11570，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11570&send_immediately=true&force_search=false)

**原文摘要:** Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [3] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi, Shailesh Garg, Farid Ahmed, Souvik Chakraborty, Syed Bahauddin Alam*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为Conformalized Monte Carlo Operator (CMCO)的新框架，该框架通过统一Monte Carlo dropout与split conformal prediction，在单一DeepONet架构中实现了无需重新训练、集成或自定义损失设计的空间分辨不确定性估计。CMCO在三个不同应用中均达到了接近名义的实证覆盖率，提供了一种通用的、即插即用的不确定性量化（UQ）解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习在实时虚拟传感中的安全部署面临着鲁棒的不确定性量化（UQ）这一关键障碍，尤其是在高风险领域，稀疏、嘈杂或非共址的传感器数据是常态。因此，需要一种能够高效且可靠地赋予操作学习UQ的方法，以实现跨异构域的不确定性评估。

**方法:** CMCO框架将神经算子基础的虚拟传感转换为具有校准的、分布自由的预测区间，通过融合Monte Carlo dropout与split conformal prediction在一个单一的DeepONet架构中，实现了空间分辨的不确定性估计。

**结果:** 通过对湍流、弹性塑性变形和全球宇宙辐射剂量估算三个不同应用的严格评估，CMCO一致达到了接近名义的实证覆盖率，即使在具有强烈空间梯度和基于代理的传感设置中也是如此。

**结论:** CMCO提供了一种通用的、即插即用的UQ解决方案，适用于神经算子，解锁了数字孪生、传感器融合和安全关键监控中的实时、可信赖推理，建立了可扩展、泛化性强且具有不确定性意识的科学机器学习的新基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distribution-Free+Uncertainty-Aware+Virtual+Sensing+via+Conformalized+Neural+Operators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11574&send_immediately=true&force_search=false)

**原文摘要:** Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [4] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore, Andrei Bodnar, Arturs Berzins, Johannes Brandstetter*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的神经网络表示方法——Einstein Fields，用于压缩四维数值相对论模拟。通过编码广义相对论的时空几何，Einstein Fields能自然地产生动态效果，并且在多个测试中表现出潜力。


<details>
  <summary>更多</summary>
  
**动机:** 为了更高效、准确地模拟和计算四维数值相对论，需要一种可以压缩数据并保持物理量导数精度的方法。

**方法:** 引入Einstein Fields，这是一种神经张量场，它不同于传统的神经场，它可以编码广义相对论的时空几何，从而自然地产生动态效果。

**结果:** Einstein Fields展示了在4D时空连续建模、与网格无关性、存储效率、导数精度和易用性方面的显著潜力。

**结论:** Einstein Fields为更可扩展和表达性的数值相对论方法铺平了道路，并且已经发布了开源库。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Einstein+Fields%3A+A+Neural+Perspective+To+Computational+General+Relativity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11589，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11589&send_immediately=true&force_search=false)

**原文摘要:** We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [5] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla, Mohsen Dorodchi, Pu Wang, Minwoo Lee*

**主要类别:** cs.LG

**AI概要:** 本文综述了合成表格数据生成的最新进展，提出了基于实际生成目标的新分类法，并建议了将技术创新与现实需求对齐的基准框架。


<details>
  <summary>更多</summary>
  
**动机:** 隐私法规变得更为严格，获取真实世界数据变得更加受限，这促使合成数据生成成为关键解决方案，特别是在金融、医疗和社科领域中的表格数据。

**方法:** 作者提出了一种新的分类方法，根据实际生成目标如下游应用、隐私保障和数据效用来组织内容；并强调保留复杂特征关系、保持统计保真度和满足隐私要求的方法。

**结果:** 该综述不仅为未来的研究提供了路线图，还指导在隐私至关重要的环境中实施合成表格数据。

**结论:** 通过连接理论基础与实际部署，此综述强调了合成数据创建中的行动导向目标，包括条件生成和风险敏感建模，并提出了一种基准框架以确保技术的创新符合实际需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synthetic+Tabular+Data+Generation%3A+A+Comparative+Survey+for+Modern+Techniques，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11590，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11590&send_immediately=true&force_search=false)

**原文摘要:** As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [6] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann, Juan Rafael Martínez-Galarza*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的二维和三维张量表示法，结合稀疏自动编码器来处理事件时间序列。该方法能够捕捉到时间和光谱特征，并在X射线天文学数据集上进行了验证。


<details>
  <summary>更多</summary>
  
**动机:** 事件时间序列具有非结构化和不规则的特性，给传统技术提取有意义的模式和识别显著现象带来了重大挑战。

**方法:** 作者提出了新颖的二维和三维张量表示法，以及与之配对的稀疏自动编码器，这些编码器可以学习物理上有意义的潜在表示。

**结果:** 这些表示法成功地捕捉了时间和光谱特征，并隔离了不同类别的X射线瞬变体。

**结论:** 此框架为跨科学和工业领域的复杂、不规则事件时间序列分析提供了一个灵活、可扩展且通用的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Representations+of+Event+Time+Series+with+Sparse+Autoencoders+for+Anomaly+Detection%2C+Similarity+Search%2C+and+Unsupervised+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11620&send_immediately=true&force_search=false)

**原文摘要:** Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [7] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari, Raphael Meunier, Rodrigue Décatoire, Mathilde Mougeot*

**主要类别:** cs.LG

**AI概要:** 研究了五种生成模型在工业轮胎架构生成任务上的表现，通过几何感知度量评估发现扩散模型整体表现最佳，但不同模型在特定条件下的表现各有优劣。


<details>
  <summary>更多</summary>
  
**动机:** 随着深度生成模型在AI领域的广泛应用，工业实践者对于哪些深度生成模型最适合复杂的制造设计任务仍存在关键疑问。为了回答这个问题，并帮助选择最适合复杂制造设计任务的深度生成模型，本研究进行了深入探讨。

**方法:** 选择了五种代表性模型（变分自编码器、生成对抗网络、多模态变分自编码器、去噪扩散概率模型和多项式扩散模型）进行研究。评估涵盖了三个主要工业场景：无条件生成完整的多组件设计；基于组件的生成（从部分观察重建架构）；以及尺寸约束生成（创建满足特定尺寸要求的设计）。为使离散扩散模型能够处理条件场景，引入了类别修复方法。

**结果:** 扩散模型在整体性能上取得了最强的表现。然而，在基于组件的条件下，掩码训练的VAE几乎在所有指标上都优于多模态变体MMVAE+。在扩散模型家族中，MDM在分布内领先，而DDPM在分布外的尺寸约束下泛化更好。

**结论:** 该研究表明，尽管扩散模型在工业轮胎架构生成任务上表现出色，但在特定条件下，其他类型的生成模型也可能具有优势。因此，选择适合具体应用场景的生成模型至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Generative+Methods+and+Tire+Architecture+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11639&send_immediately=true&force_search=false)

**原文摘要:** As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [8] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah, David Yevick*

**主要类别:** cs.LG

**AI概要:** 本文介绍了几种实用的指标，如dropout下的方差、鲁棒性、嵌入相似性和稀疏性度量，可以预测神经网络grokking行为。


<details>
  <summary>更多</summary>
  
**动机:** Grokking是指神经网络测试准确率在训练准确率提高后才显著增加的现象，理解并预测这种现象有助于改进模型性能。

**方法:** 引入了包括dropout鲁棒性曲线(DRC)在内的多种度量方法，用以估计神经网络对噪声的韧性，并观察了测试准确率在不同dropout率下的变化。此外，还研究了不活跃神经元百分比和嵌入分布的变化。

**结果:** 发现dropout鲁棒性曲线下测试准确率的方差在grokking期间达到局部最大值；不活跃神经元的百分比在泛化过程中减少；嵌入向量趋向于双峰分布，并与初始化无关。

**结论:** 这些度量方法为grokking的起源和行为提供了宝贵的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tracing+the+Path+to+Grokking%3A+Embeddings%2C+Dropout%2C+and+Network+Activation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11645，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11645&send_immediately=true&force_search=false)

**原文摘要:** Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [9] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey, Benjamin Appiah, Griffith S. Klogo, Garth V. Crosby*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种结合零知识证明（ZKPs）的新协议，用于联邦学习中的隐私保护和可验证评估。客户不直接透露损失值，而是生成简短的证明来说明其本地损失低于预设阈值。该方法在MNIST和HAR数据集上进行了实验评估。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习（FL）允许在分散的数据上进行协作模型训练而不暴露原始数据，但其评估阶段可能会通过共享性能指标泄露敏感信息。为了增强隐私保护并确保评估的可验证性，需要一种新的评估机制。

**方法:** 作者提出了一种新协议，将零知识证明纳入到联邦学习中。客户生成简洁的证明，以表明其本地损失低于预设阈值，而不是分享实际的损失值。这种方法使用自包含模块实现，包括联邦学习仿真、ZKP电路设计等，并对CNN和MLP模型进行了实验。

**结果:** 该方法已在MNIST和HAR数据集上实施和测试，评价指标包括计算开销、通信成本和可验证性。具体结果需查阅原文献。

**结论:** 这项研究提供了一种隐私保护且可验证的联邦学习评估方法，通过引入零知识证明减少了敏感信息的泄露风险，同时保持了评估的透明性和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZKP-FedEval%3A+Verifiable+and+Privacy-Preserving+Federated+Evaluation+using+Zero-Knowledge+Proofs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11649，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11649&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [10] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha, Ke Xu, Xingzhi Sun, Ananya Krishna, Dhananjay Bhaskar, Blanche Mongeon, Morgan Craig, Mark Gerstein, Smita Krishnaswamy*

**主要类别:** cs.LG

**AI概要:** 该研究结合了基于代理的建模和深度学习，提出了一种新的框架STAGED，用于模拟细胞间的通讯及其对细胞内基因调控网络的影响。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法如聚类和轨迹推断将细胞视为独立的数据点，忽略了细胞之间的空间组织和动态交互作用。为了更好地理解细胞状态的变化，需要一种能够捕捉复杂互动细胞动力学的方法。

**方法:** 研究人员引入了Spatio Temporal Agent-Based Graph Evolution Dynamics (STAGED)，它整合了基于代理的建模（ABM）与深度学习，以模拟细胞间通信及其对细胞内基因调控网络的影响。使用图ODE网络（GDEs），并为每种细胞类型设置共享权重，模型中的基因被表示为顶点，相互作用被表示为有向边，并通过设计的关注机制动态学习它们的强度。

**结果:** 模型训练可以匹配来自空间转录组数据的连续轨迹，无论是模拟的还是推断的轨迹，从而捕捉到细胞间和细胞内的交互作用，使得细胞动力学的表现更加适应和准确。

**结论:** STAGED提供了一种更适应性和精确的方式来表示细胞动力学，包括细胞间的通信和细胞内的基因调控网络，这可能会推动对细胞状态变化的理解进入一个新的层次。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STAGED%3A+A+Multi-Agent+Neural+Network+for+Learning+Cellular+Interaction+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11660&send_immediately=true&force_search=false)

**原文摘要:** The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [11] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence, Daisuke Yamada, Vikas Singh*

**主要类别:** cs.LG

**AI概要:** 本文通过引入Clifford代数和双矢量，提出了一种新的线性层表示方法，该方法仅使用O(log^2 d)参数即可匹配现有基线模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们试图理解大型模型中低级原始模块如何组成具有更丰富功能的模块，并探讨是否可以从最小的几何原始模块中识别或合成线性变换。

**方法:** 使用Clifford代数表达线性层作为双矢量的组合，并引入一种可微分算法将这些双矢量分解为旋量的乘积。

**结果:** 所提出的基于旋量的线性层在LLM注意力层中的表现与强基线（如block-Hadamard和低秩近似）相匹配。

**结论:** 研究结果提供了一个代数视角来理解几何原始模块如何在深度模型内部组成更高层次的功能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Composing+Linear+Layers+from+Irreducibles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11688，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11688&send_immediately=true&force_search=false)

**原文摘要:** Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [12] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri, William Yang, Polina Kirichenko, Lydia Liu, Olga Russakovsky*

**主要类别:** cs.LG

**AI概要:** 本文首次全面分析了数据选择对所选核心集的虚假偏差水平和训练模型稳健性的影响。通过一系列实验，揭示了样本难度和偏差一致性、数据集偏差与模型稳健性之间复杂的相互作用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管核心集选择方法在减少训练数据量的同时保持了模型性能，但许多数据集存在导致模型学习到虚假关联而非因果特征的偏差。因此，有必要理解数据集简化方法是否以及如何延续、放大或减轻这些偏差。

**方法:** 作者使用了一个广泛的实验设置，包括十个不同的虚假相关基准、五个表征样本重要性/难度的评分指标和五种数据选择策略，并跨越了一系列核心集大小。

**结果:** 研究发现，基于嵌入式的样本特征化分数选择核心集相比基于学习动态的方法更不容易无意中加剧偏差。此外，一些核心集选择方法虽然可以通过优先选择困难样本实现较低的偏差水平，但这并不能可靠地保证下游稳健性。

**结论:** 虽然某些核心集选择方法可以通过优先考虑困难样本来达到较低的偏差水平，但它们并不能可靠地保证下游模型的稳健性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Impact+of+Coreset+Selection+on+Spurious+Correlations+and+Group+Robustness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11690，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11690&send_immediately=true&force_search=false)

**原文摘要:** Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [13] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde, Ali Mohammed Mansoor Alsahag, Pierre Blanchet*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于LSTM网络的预测系统，利用地面实况落叶数据和多光谱及气象卫星数据，来预测落叶时间。该模型在预测落叶开始和结束时间上表现出较低的均方根误差，为铁路行业优化落叶缓解措施提供了可能，并有助于理解复杂的生态系统。


<details>
  <summary>更多</summary>
  
**动机:** 每年由于落叶导致的铁路交通中断给英国铁路行业带来了超过3亿英镑的损失。因此，能够准确预测落叶时间将为铁路网络运营商提供巨大利益，使他们能够更有效地安排缓解措施。

**方法:** 研究使用了LSTM（长短期记忆）网络，这是一种深度学习模型，结合了地面实况的落叶数据以及来自卫星的多光谱和气象数据进行训练。

**结果:** 该模型在预测落叶开始时间上的均方根误差为6.32天，预测落叶结束时间为9.31天。这比之前的工作有所改进。

**结论:** 此预测系统展示了在铁路行业中优化落叶缓解措施的潜力，同时也促进了对复杂生态系统的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+series+classification+of+satellite+data+using+LSTM+networks%3A+an+approach+for+predicting+leaf-fall+to+minimize+railroad+traffic+disruption，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11702&send_immediately=true&force_search=false)

**原文摘要:** Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [14] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya, Shinji Ito, Haipeng Luo*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的具有对抗偏好的情节性表格马尔可夫决策过程框架，并专注于由Borda分数确定的奖励函数设置。通过建立遗憾下界和开发算法，该研究在已知和未知转换设置中实现了$T^{2/3}$阶的遗憾上界。


<details>
  <summary>更多</summary>
  
**动机:** 传统的情节性MDP通常假设损失的数值直接被观察到，而这篇文章考虑了一种更复杂的情况，即学习者只观察到两个候选臂之间的偏好，这使得需要发展新的方法来处理这种类型的反馈。

**方法:** 作者首先建立了针对PbMDPs与Borda分数的遗憾下界。然后提出了一种基于在线线性优化的整体优化方法，在已知转换的情况下达到了特定的遗憾上界。为了提高对状态数量S的依赖性和计算效率，他们还提出了一个策略优化算法，并将结果扩展到了未知转换的设定。

**结果:** 对于PbMDPs与Borda分数，作者推导出了遗憾下界为$\Omega( (H^2 S K)^{1/3} T^{2/3} )$。提出的算法在已知转换情况下分别实现了$\tilde{O}((H^2 S^2 K)^{1/3} T^{2/3} )$和$\tilde{O}( (H^6 S K^5)^{1/3} T^{2/3} )$的遗憾上界，并且后者可以进一步扩展到未知转换情况。

**结论:** 研究为具有对抗偏好的情节性表格马尔可夫决策过程提供了一个新的框架，并为由Borda分数确定的奖励函数提供了理论基础。所提出的算法在理论上证明了其有效性，但它们在实际应用中的性能仍需进一步验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+from+Adversarial+Preferences+in+Tabular+MDPs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11706，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11706&send_immediately=true&force_search=false)

**原文摘要:** We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [15] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky, Harry Shomer, Jiliang Tang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的图生成模型框架FLEX，它可以在分布外（OOD）场景中增强链接预测性能，而不需要专业知识。FLEX通过结构条件图生成和对抗协同训练两种机制确保样本分布之间的结构对齐。


<details>
  <summary>更多</summary>
  
**动机:** 现有的GNNs在链接预测任务上表现出色，但通常依赖于所有数据集样本来自同一分布的假设。此外，尽管图生成模型（GGMs）能够生成新颖的输出图，但其应用大多局限于特定领域任务。为了弥合这一差距，提高GNNs在OOD场景中的链接预测能力，提出了FLEX框架。

**方法:** FLEX框架利用了两种机制：(1) 结构条件图生成，这意味着生成的图是基于现有图结构进行调整的；(2) 对抗协同训练，即在一个自动编码器和一个GNN之间进行对抗训练，以确保生成的图与原始图在结构上的对齐。

**结果:** 大量的实验在合成和真实世界的OOD设置中展示了FLEX提升性能的能力，并进一步分析了图数据增强对链接结构的影响。结果表明，FLEX能够在不同的OOD场景中显著提高链接预测性能。

**结论:** FLEX作为一种不需要专家知识即可运作的GGM框架，在不同OOD场景中增强了链接预测性能。这些发现为GGMs更广泛的应用铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Subgraph+Generation+for+Generalizing+on+Out-of-Distribution+Links，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11710&send_immediately=true&force_search=false)

**原文摘要:** Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [16] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi, Hamidreza Zareipour, Henry Leung*

**主要类别:** cs.LG

**AI概要:** 本文探讨了全局负荷预测模型在数据漂移情况下的表现，通过对比特征转换和目标转换模型，提出了不同的时间序列聚类方法，并证明了全局目标转换模型的优越性。


<details>
  <summary>更多</summary>
  
**动机:** 传统局部预测模型（LFMs）虽然直观且局部准确，但在处理泛化、过拟合、数据漂移和冷启动问题方面存在显著局限性。此外，它们在扩展性上也面临挑战，随着网络规模和数据量的增长，计算成本增加且效率降低。

**方法:** 研究采用了特征转换和目标转换模型，并引入了基于模型的时间序列聚类方法和新的加权实例基时间序列聚类方法来解决数据异质性和全球与本地之间的平衡问题。

**结果:** 实验表明，全局目标转换模型在使用全局特征和聚类技术后，始终优于其局部对应模型。而全局特征转换模型则在平衡本地和全球动态方面遇到挑战，通常需要时间序列聚类来有效管理数据异质性。

**结论:** 全局预测模型提供了增强预测泛化能力、可扩展性、准确性和鲁棒性的新途径，特别是在面对数据漂移时。全局目标转换模型在各种情况下表现出色，为电力传输网络中的负荷预测提供了一种有前景的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Globalization+for+Scalable+Short-term+Load+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11729&send_immediately=true&force_search=false)

**原文摘要:** Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [17] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue, Zhen Bi, Long Ma, Zhenlin Hu, Yan Wang, Zhenfang Liu, Qing Sheng, Jie Xiao, Jungang Lou*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Thought Purity (TP)的防御范式，旨在增强大型推理模型对恶意内容的抵抗力，同时保持操作效率。TP通过安全优化的数据处理管道、强化学习增强的规则约束和自适应监控指标三个协同组件实现这一目标。


<details>
  <summary>更多</summary>
  
**动机:** 尽管强化学习训练的大型推理模型（LRMs）展示了先进的推理能力，但它们容易受到安全威胁，特别是在链式思维生成过程中。对抗方法如后门提示攻击可以系统地破坏模型的核心推理机制。新兴的链式思维攻击（CoTA）揭示了这种易受攻击性，并通过利用提示可控性同时降低链式思维的安全性和任务性能。

**方法:** 为了解决这种复合的安全-性能脆弱性，我们提出了Thought Purity（TP）：一种防御范式，它通过以下三种协同组件系统地增强对恶意内容的抵抗力：1. 安全优化的数据处理管道；2. 强化学习增强的规则约束；3. 自适应监控指标。

**结果:** 该方法建立了第一个全面的防御机制，以应对在强化学习对齐的推理系统中的CoTA漏洞，显著推进了下一代AI架构的安全性-功能性平衡。

**结论:** 这项研究提出的Thought Purity（TP）防御范式是针对LRMs中链式思维攻击的一种有效解决方案，不仅增强了模型的安全性，而且维持了其操作效能。这对于未来的大型语言模型和推理系统的开发具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thought+Purity%3A+Defense+Paradigm+For+Chain-of-Thought+Attack，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12314，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12314&send_immediately=true&force_search=false)

**原文摘要:** While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [18] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen, Cencheng Shen, Youngser Park, Carey E. Priebe*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于统计方法的GEE框架，以生成高质量初始节点特征，并将其应用于图神经网络（GNNs），形成GG框架。实验表明，GG在节点聚类和分类任务中均表现出色，收敛速度更快。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图神经网络（GNNs）由于依赖于随机或最少信息量的初始特征表示，其性能常常受到限制，导致收敛缓慢和次优解。为了提高GNNs的性能，需要一种能够生成高质量初始节点特征的方法。

**方法:** 作者引入了一种基于统计学原理的方法——one-hot图编码器嵌入（GEE），用以生成高质量的初始节点特征。该方法与GNN结合形成了GEE-powered GNN (GG)框架。对于节点分类任务，还提出了一个增强版本GG-C，它将GG和GEE的输出连接起来。

**结果:** 在节点聚类任务中，GG框架在所有评估的真实数据集上均达到了最先进的性能，并且比标准GNN收敛得更快。在节点分类任务中，提出的GG-C变体也超过了竞争基线。

**结论:** 这些结果证明了原则性的、结构感知的特征初始化在实现GNNs全部潜力方面的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Neural+Networks+Powered+by+Encoder+Embedding+for+Improved+Node+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11732&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [19] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey, Rebecca A. Sarpong, Griffith S. Klogo, Winful Bagyl-Bac, Garth V. Crosby*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种基于贝叶斯博弈论的经济激励机制，用于联邦学习中的数据投毒攻击防御。该机制在MNIST和FashionMNIST数据集上的实验结果表明，其具有良好的准确性和鲁棒性，并且计算成本低，易于集成到现有框架中。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习的开放参与特性使其容易受到数据投毒攻击，而现有的防御措施通常依赖于统计聚合规则，这些规则可能是计算密集型的，并且通常假设大多数参与者是诚实的。为了应对这些问题，需要一种新的、主动的防御方法。

**方法:** 作者引入了一种基于贝叶斯博弈论的轻量级经济激励机制。每个训练轮次被建模为一个信息不完全的贝叶斯博弈，服务器（作为委托人）使用一小部分私有验证数据集来验证更新质量，然后发放支付。该设计满足个体理性（IR），确保善意客户的参与是有利可图的，并且满足激励兼容性（IC），使投毒成为经济上劣质策略。

**结果:** 在非IID划分的MNIST和FashionMNIST数据集上的广泛实验表明了该机制的鲁棒性：在50%标签翻转对手的情况下，该机制保持96.7%的准确率，仅比30%标签翻转对手的情况低0.3个百分点。这一结果比标准FedAvg高出51.7个百分点，后者在这种50%攻击下崩溃。

**结论:** 该机制计算负担小，预算有限，并且可以轻松集成到现有的联邦学习框架中，为经济上稳健和可持续的联邦学习生态系统提供了一条实用途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Bayesian+Incentive+Mechanism+for+Poison-Resilient+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12439，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12439&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [20] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

**主要类别:** cs.LG

**AI概要:** 本文探讨了将符合预测方法与集成SINDy结合，应用于非线性动力系统模型的不确定性量化，展示了其在时间序列预测、模型选择和模型系数不确定性量化中的应用。


<details>
  <summary>更多</summary>
  
**动机:** 为了评估SINDy模型的可靠性，特别是在安全关键的应用中，需要对其不确定性进行量化。现有的方法包括贝叶斯和集成方法，但本工作着眼于探索符合预测框架的应用，该框架仅基于最小假设如数据可交换性，提供有效的预测区间。

**方法:** 作者引入了三种符合预测与E-SINDy结合的应用：（1）时间序列预测中的不确定性量化；（2）基于库特征重要性的模型选择；（3）使用特征符合预测来量化已识别模型系数的不确定性。

**结果:** 实验证明，所提出的方法在时间序列预测上可以可靠地达到目标覆盖范围，有效地量化特征的重要性，并且即使在非高斯噪声下也能为模型系数产生更稳健的不确定性区间。

**结论:** 将符合预测方法与E-SINDy相结合，可以在非线性动力系统模型的不确定性量化方面取得更好的效果，特别是在处理非高斯噪声时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparse+Identification+of+Nonlinear+Dynamics+with+Conformal+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11739，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11739&send_immediately=true&force_search=false)

**原文摘要:** The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [21] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song, Yong Gao*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的Graph-in-Graph模型，该模型结合了传导学习和归纳学习的优势，用于药物-靶点相互作用预测。通过特殊基准测试的评估，证明其性能优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习方法在有效整合药物、靶点及其相互作用的多样特征方面存在困难，特别是在药物-靶点相互作用（DTI）预测中。

**方法:** 引入了一个新的框架，利用传导学习和归纳学习的力量，使分子水平和药物-靶点相互作用网络水平的特征得到充分利用。框架内是一个基于GNN的模型，称为Graph-in-Graph (GiG)，它将药物和目标分子结构图表示为药物-靶点相互作用图中的元节点。

**结果:** 实验结果表明，GiG模型在所有评估指标上显著优于现有方法，强调了整合不同学习范式和交互数据的好处。

**结论:** GiG模型提供了一种更有效的药物-靶点相互作用预测方式，能够更好地探索药物和靶点分子结构之间复杂的关系，并且在提供的特殊基准上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Graph-in-Graph+Learning+Framework+for+Drug-Target+Interaction+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11757，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11757&send_immediately=true&force_search=false)

**原文摘要:** Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [22] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere, Ali Mohammed Mansoor Alsahag*

**主要类别:** cs.LG

**AI概要:** 荷兰铁路网络是世界上最繁忙的铁路网络之一，运营商NS面临的主要问题是列车晚点。本研究通过使用XGBoost分类器并着重于拓扑特征来预测晚点，改进了现有的主要用于预测美国航空网络演变的方法。尽管结果在非同时测试场景中表现有限，但本研究为交通网络评估提供了新的见解，并提出了未来开发更稳健的预测模型的方向。


<details>
  <summary>更多</summary>
  
**动机:** 荷兰铁路网络非常繁忙，晚点是一个主要问题。当前的研究主要集中在短期预测上，而忽略了整个网络模式的重要性。为了填补这一空白，本研究旨在利用XGBoost分类器和拓扑特征进行晚点预测。

**方法:** 本研究采用了多种分类器（如RandomForest、DecisionTree、GradientBoosting、AdaBoost和LogisticRegression），并集成了节点中心性测量，以预测荷兰铁路网络中的晚点情况。

**结果:** 结果表明，在非同时测试场景中，性能有限，这表明需要更多的上下文特定适应。

**结论:** 尽管存在局限性，本研究仍为交通网络评估提供了新的见解，并提出了未来开发更稳健的预测模型的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Delayed+Trajectories+Using+Network+Features%3A+A+Study+on+the+Dutch+Railway+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11776，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11776&send_immediately=true&force_search=false)

**原文摘要:** The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [23] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova, Léna Néhale Ezzine, Piotr Gaiński, Luca Scimeca, Emmanuel Bengio, Prudencio Tossou, Yoshua Bengio, Alex Hernandez-Garcia*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为Torsional-GFN的条件GFlowNet，它专门用于根据分子的奖励函数从Boltzmann分布中采样构象。实验结果表明，该模型可以对多种分子进行零样本泛化，并有望扩展到更大的分子系统。


<details>
  <summary>更多</summary>
  
**动机:** 生成稳定的分子构象对于药物发现中的许多应用至关重要，例如估计分子与目标的结合亲和力。虽然分子动力学是一种常用的方法，但其效率较低。近年来，生成机器学习方法作为一种更有前途、更有效的方法出现。

**方法:** 作者引入了Torsional-GFN，这是一种条件GFlowNet，它仅使用奖励函数作为训练信号，根据分子图及其局部结构（键长和角度），采样其扭转角的旋转。

**结果:** 实验结果表明，Torsional-GFN能够为多个分子采样出大致符合Boltzmann分布的构象，并允许对来自MD模拟的未见过的键长和角度进行零样本泛化。

**结论:** 这项工作为将所提出的方法扩展到更大的分子系统提供了一个有希望的方向，实现了对未见过的分子的零样本泛化，并将局部结构的生成纳入GFlowNet模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Torsional-GFN%3A+a+conditional+conformation+generator+for+small+molecules，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11759&send_immediately=true&force_search=false)

**原文摘要:** Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [24] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu, Dongyu Zhang, Huayi Zhang, Elke Rundensteiner*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种不依赖于干净标签数据集的元学习方法CLID-MU，用于处理带噪声标签的学习问题。该方法通过跨层信息散度元更新策略来评估模型性能并指导训练，实验表明其优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的元学习方法在处理带噪声标签时需要一个干净标签的数据集，但这样的数据集在实际中很难获得。因此，研究者们希望找到一种不需要干净标签数据集的方法来解决带噪声标签的问题。

**方法:** 研究人员提出了Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU)，利用数据结构在最后隐藏层和最终层之间的对齐情况来评估模型性能，并用这种对齐方式引导训练过程。

**结果:** 通过对不同数量标签的基准数据集进行实验，包括合成噪声和真实世界噪声的情况，结果表明CLID-MU方法的表现超过了当前最先进的技术。

**结论:** CLID-MU提供了一个有效的解决方案，在没有干净标签的情况下也能成功地应用元学习来处理带噪声标签的问题。并且开源了代码，使得其他研究者可以重复实验并进一步改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLID-MU%3A+Cross-Layer+Information+Divergence+Based+Meta+Update+Strategy+for+Learning+with+Noisy+Labels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11807&send_immediately=true&force_search=false)

**原文摘要:** Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [25] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali, Justin Xu, Ivory Yang, Jasmine Xinze Li, Ayse Arslan, Clark Benham*

**主要类别:** cs.LG

**AI概要:** 论文研究了不同规模的Llama 2模型中对比激活添加（CAA）技术的效果，发现CAA在早期到中期层最有效，其效果随模型规模增大而减弱，负向转向比正向转向效果更明显。


<details>
  <summary>更多</summary>
  
**动机:** 由于大型语言模型（LLMs）不断发展，较少使用的对齐技术的有效性变得不确定。为了更好地控制语言模型的输出，需要探索和评估这些技术的效果。

**方法:** 使用对比激活添加（CAA），通过在模型的前向传递过程中找到并添加理想的'directions'来直接操纵残差流。实验采用围绕拒绝行为的答案匹配问题，并测试了7B、13B、和70B三种不同规模的Llama 2模型。

**结果:** 1) CAA在应用于早期至中期层时最有效；2) 随着模型尺寸的增加，CAA的效果减弱；3) 在所有模型尺寸上，负向转向比正向转向具有更明显的效果。

**结论:** 本研究表明，尽管CAA是一种有潜力的技术，但它的效果受模型规模和应用层的影响，并且在实现特定类型的转向方面存在不对称性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+laws+for+activation+steering+with+Llama+2+models+and+refusal+mechanisms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11771&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [26] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri, Arash Karimi, Ariane Middel*

**主要类别:** cs.LG

**AI概要:** 提出了一个名为MNIST-Gen的框架，用于生成用户指定类别的MNIST风格图像数据集。该系统结合了CLIP语义理解、强化学习和人工反馈，实现了智能分类，并通过两个新数据集Tree-MNIST和Food-MNIST验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的标准数据集如MNIST等对于特定领域任务（如树木、食品分类）的数据集是不足且不相关的，创建自定义数据集又面临耗时、法律限制等问题。

**方法:** MNIST-Gen使用层次语义分类方法，结合CLIP语义理解与强化学习以及人类反馈来实现智能化分类。它支持复杂的类别结构，具有细粒度的子分类和多种处理模式。

**结果:** 生成了两个新的数据集Tree-MNIST和Food-MNIST，证明了MNIST-Gen在生产任务特定评估数据方面的效用，同时达到了85%的自动分类准确率和相比手动方法节省80%的时间。

**结论:** MNIST-Gen提供了一种自动化、模块化和适应性的框架，可以生成特定于用户指定类别的MNIST风格图像数据集，大大提高了数据集创建的效率和相关性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MNIST-Gen%3A+A+Modular+MNIST-Style+Dataset+Generation+Using+Hierarchical+Semantics%2C+Reinforcement+Learning%2C+and+Category+Theory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11821，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11821&send_immediately=true&force_search=false)

**原文摘要:** Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [27] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma, Sergei Rybakov, Leon Hetzel, Stephan Günnemann, Fabian J. Theis*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的训练框架FlatVI，该框架通过离散似然变分自编码器对单细胞计数数据进行建模，并将潜在流形正则化为欧几里得几何。实验表明，FlatVI在轨迹重建和流形插值方面具有更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法在潜在空间中假设线性移动和欧几里得几何，这可能无法与数据流形上的测地线路径相对应，从而限制了假设欧几里得几何的数据表示的方法。

**方法:** 引入了FlatVI，一种新的训练框架，它通过离散似然变分自编码器对单细胞计数数据进行建模，并将潜在流形正则化为欧几里得几何。

**结果:** 实验表明，FlatVI在轨迹重建和流形插值方面具有更好的性能。

**结论:** FlatVI通过鼓励潜在空间中的直线近似解码的单细胞流形上的测地线插值，增强了与下游方法的兼容性，这些方法假设潜在的欧几里得几何。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enforcing+Latent+Euclidean+Geometry+in+Single-Cell+VAEs+for+Manifold+Interpolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11789，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11789&send_immediately=true&force_search=false)

**原文摘要:** Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [28] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio, Pietro Marsella, Ben Pan, Simon Guo, Silas Alberti*

**主要类别:** cs.LG

**AI概要:** 本文介绍了Kevin，一种用于CUDA内核生成和优化的模型。通过多轮强化学习训练，显著提高了生成内核的正确性和速度。


<details>
  <summary>更多</summary>
  
**动机:** 编写GPU内核对AI系统的效率至关重要，但具有挑战性。这是一个高度迭代的过程，可以通过执行反馈不断改进性能。此外，它提供了可验证的奖励，如正确性和加速，因此非常适合应用强化学习（RL）。

**方法:** 作者开发了一种灵活的多轮RL方法，以解决现实世界中遇到的独特挑战，例如从长轨迹中学习和有效的奖励归因。基于此方法，他们提出了Kevin - K(ernel D)evin，这是第一个使用多轮RL训练的用于CUDA内核生成和优化的模型。

**结果:** 在评估设置中，Kevin相较于其基础模型（QwQ-32B）表现出显著的提升，将生成内核的正确性从56%提高到82%，平均加速从0.53x提高到1.10x。并且超过了前沿模型o4-mini（0.78x）。

**结论:** 研究还发现，在测试时扩展轴上，扩大串行细化比并行抽样更有益。特别是当给予更多的细化轮次时，Kevin显示出更高的改进率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kevin%3A+Multi-Turn+RL+for+Generating+CUDA+Kernels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11948，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11948&send_immediately=true&force_search=false)

**原文摘要:** Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [29] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter, Athanasios Sideris*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种在强化学习（RL）中结合训练和剪枝的方法，特别是针对使用在线特征提取网络（OFENet）增强的RL算法。通过解决RL网络权重和变分伯努利分布参数上的随机优化问题，该方法实现了自动选择超参数，并有效地结合了RL目标和网络压缩。实验结果表明，这种方法可以在连续控制基准测试中显著减少OFENets的规模，并且不会明显降低性能。此外，与从头开始训练小网络相比，在训练过程中剪枝大网络可以产生更高效、性能更高的RL智能体。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络（DNN）在强化学习（RL）中的扩展已被证明可以提高性能，但同时也带来了计算和内存复杂性的显著增加。虽然神经网络剪枝方法已经在监督学习中成功解决了这一挑战，但在RL中的应用尚未得到充分探索。因此，研究者们希望找到一种方法，能够在保持或提高性能的同时，降低RL算法的计算和内存需求。

**方法:** 研究者提出了一个名为XiNet的新网络架构，它被设计为同时进行训练和剪枝。具体来说，XiNet被训练以解决关于RL网络权重和变分伯努利分布参数的随机优化问题，其中0/1随机变量ξ用于缩放网络中的每个单元。当某个单元对性能贡献较小，变分参数趋向于0时，相应的结构将永久失效并从网络中剪除。此外，研究者还提出了一种成本感知、稀疏性促进的正则化方案，专门针对OFENets的DenseNet架构。

**结果:** 实验结果表明，该方法可以在连续控制基准测试（MuJoCo）上显著减少OFENets的规模，并且不会明显降低性能。进一步的结果显示，与从头开始训练小网络相比，在训练过程中剪枝大网络可以产生更高效、性能更高的RL智能体。

**结论:** 本研究表明，在强化学习中结合训练和剪枝是一种有效的方法，可以减少模型大小，同时保持或提高性能。这为未来的RL研究提供了一个新的方向，特别是在处理计算资源有限的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Training+and+Pruning+of+Deep+Reinforcement+Learning+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11975&send_immediately=true&force_search=false)

**原文摘要:** Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [30] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh, Miruna Cretu, Dmytro Shevchuk, Vignesh Ram Somnath, Pietro Liò, Robert A. Batey, Mike Tyers, Michał Koziarski, Cheng-Hao Liu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的框架SynCoGen，它结合了同时的掩码图扩散和流匹配，用于可合成的3D分子生成。该模型在无条件的小分子图和构象生成方面达到了最先进的性能，并在药物发现中实现了具有竞争力的零样本分子连接器设计。


<details>
  <summary>更多</summary>
  
**动机:** 确保生成的小分子的可合成性仍然是一个主要挑战，而当前的努力大多局限于2D分子图表示，这限制了几何条件生成的能力。

**方法:** SynCoGen从分子构建块、化学反应和原子坐标的联合分布中进行采样。为了训练模型，研究人员整理了一个包含超过600K合成意识的构建块图和3.3M构象的数据集SynSpace。

**结果:** SynCoGen在无条件小分子图和构象生成方面达到了最先进水平，并在零样本分子连接器设计中表现出色。

**结论:** 这种多模态公式为非自回归分子生成的未来应用奠定了基础，包括类似物扩展、先导优化和直接结构条件。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SynCoGen%3A+Synthesizable+3D+Molecule+Generation+via+Joint+Reaction+and+Coordinate+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11818&send_immediately=true&force_search=false)

**原文摘要:** Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [31] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang, Yili Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种多层级LLM增强的图欺诈检测框架MLED，通过利用大型语言模型从文本信息中提取外部知识，以提高欺诈检测方法的效果。实验表明，MLED在四个真实数据集上实现了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图欺诈检测方法通常使用预处理的节点嵌入和预定义的图结构来揭示欺诈者，这忽略了原始文本信息中的丰富语义线索。尽管大型语言模型（LLMs）在处理文本信息方面表现出强大的能力，但将处理后的文本嵌入与图结构进行多模态融合仍然是一项重大挑战。

**方法:** MLED框架包括类型级增强器和关系级增强器，旨在增强欺诈者和良性实体之间的差异，以及增强欺诈者在不同关系中的重要性。

**结果:** 在四个真实世界的数据集上的实验表明，MLED作为一个通用框架，在图欺诈检测方面达到了最先进水平的性能。

**结论:** MLED可以应用于现有方法，并显著提高了图欺诈检测的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+LLMs+Find+Fraudsters%3F+Multi-level+LLM+Enhanced+Graph+Fraud+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11997&send_immediately=true&force_search=false)

**原文摘要:** Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [32] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu, Hongyu Gao, Zhuangzhi Chen, Dongwei Xu, Yun Lin, Qi Xuan, Guan Gui*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为DUSE的数据扩展框架，用于解决自动调制识别中数据稀缺的问题。通过不确定性评分函数和主动学习策略，DUSE在多种情况下优于其他基线方法，并对未见过的模型具有强大的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 自动调制识别（AMR）中的深度神经网络模型通常需要大量的标注数据进行训练，但在实际场景中，目标域数据往往稀缺，难以满足模型训练需求。现有的直接采集和数据增强方法存在成本高或效果有限的问题。

**方法:** 提出了一种称为动态不确定性驱动样本扩展（DUSE）的框架，该框架利用不确定性评分函数从相关AMR数据集中筛选有用样本，并采用主动学习策略不断优化评分器。

**结果:** 广泛的实验表明，DUSE在类别平衡和类别不平衡设置下均优于8个核心集选择基线，并且对未见过的模型表现出强大的跨架构泛化能力。

**结论:** DUSE提供了一种有效的方法来应对AMR中数据稀缺问题，通过引入不确定性评分和主动学习机制，提高了模型训练的效果并降低了数据收集的成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DUSE%3A+A+Data+Expansion+Framework+for+Low-resource+Automatic+Modulation+Recognition+based+on+Active+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12011，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12011&send_immediately=true&force_search=false)

**原文摘要:** Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [33] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao, Jianshe Wu, JingYi Ding*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架HyperEvent，将动态链接预测重新定义为超事件识别。通过构建关联序列和使用事件相关向量，HyperEvent能够捕捉复合超事件的结构凝聚力，在4个数据集上超过了现有的最先进方法，并在大规模图上的准确性和效率方面表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的以节点为中心和以事件为中心的方法关注于个体交互或原子状态，无法捕捉由因果相关事件组成的复合超事件的结构凝聚力。

**方法:** HyperEvent框架的核心是动态构建一个关联序列，使用事件相关向量量化查询事件与历史事件之间的两两依赖关系，从而表征潜在超事件的结构凝聚力。通过评估查询事件是否能与历史事件共同形成有效的超事件来预测查询事件的发生。为了提高可扩展性，还引入了高效的并行训练算法，对大型事件流进行分段以实现并发训练。

**结果:** 实验验证了HyperEvent在大规模图上的优越准确性和效率。特别是在大规模Flight数据集上，HyperEvent实现了比最先进的基线方法高出6.95%的平均倒数排名，同时仅使用了10.17%的训练时间。

**结论:** HyperEvent框架通过其独特的超事件识别方法，在多个数据集上超越了现有方法，并在处理大规模图时展示了出色的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HyperEvent%3ALearning+Cohesive+Events+for+Large-scale+Dynamic+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11836，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11836&send_immediately=true&force_search=false)

**原文摘要:** Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [34] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi, Alexandros Iosifidis, Qi Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为PRISM的方法，该方法通过优化Transformer模型的分布式推理，以减少边缘设备之间的通信和计算需求。实验表明，在保持准确率的情况下，该方法显著降低了通信开销和每设备的计算量。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基础模型（FMs）在许多应用中取得了显著成功，但其在边缘环境中的部署面临重大挑战。为了解决这些挑战，需要开发出实际且高效的策略来将基础模型引入边缘环境中。

**方法:** 作者提出了一种称为PRISM的策略，该策略包括：1. 使用Segment Means表示法近似中间输出特征，从而大幅减少设备间通信；2. 重构自注意力机制以消除位置划分引起的冗余计算；3. 设计了适用于自回归模型的分区感知因果掩码方案。

**结果:** 评估结果显示，与原始模型相比，PRISM方法可以实现通信开销最高减少99.2%（对于BERT），并且每设备计算量减少了51.24%，同时只带来了微小的准确度下降。

**结论:** PRISM提供了一种可扩展且实用的解决方案，用于在分布式资源受限环境中部署基础模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PRISM%3A+Distributed+Inference+for+Foundation+Models+at+Edge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12145，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12145&send_immediately=true&force_search=false)

**原文摘要:** Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [35] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong, Xinshi Chen, Yuxuan Zhang, Yuxuan Song, Hao Zhou, Wenzhi Xiao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Protenix-Mini的紧凑优化模型，用于高效的蛋白质结构预测。通过替换多步AF3采样器为少步ODE采样器、架构修剪和使用ESM模块替代MSA模块，该模型在减少计算复杂度的同时保持了高保真度的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 轻量级推理对于生物分子结构预测和其他下游任务至关重要，它使得大规模应用中的高效部署和推理时间扩展成为可能。然而，在保证预测准确性的前提下提高模型效率是一个挑战。

**方法:** 1) 用少步ODE采样器代替多步AF3采样器；2) 在开源Protenix框架中进行架构修剪；3) 训练一个包含ESM模块的模型以取代传统的MSA模块。

**结果:** Protenix-Mini显著减少了模型复杂度，并且在基准数据集上的评估表明其性能只下降了1到5个百分点。

**结论:** Protenix-Mini是计算资源有限但需要准确结构预测的应用的理想选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Protenix-Mini%3A+Efficient+Structure+Predictor+via+Compact+Architecture%2C+Few-Step+Diffusion+and+Switchable+pLM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11839，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11839&send_immediately=true&force_search=false)

**原文摘要:** Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [36] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis, Ajitha Rajan*

**主要类别:** cs.LG

**AI概要:** 提出TuneQn，一个用于选择性量化、部署和执行ONNX模型的套件。通过评估四个ONNX模型，展示了其在减少精度损失和模型大小方面的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 完全量化的模型可能性能不佳，并且在低端硬件加速器上部署时面临挑战。选择性量化可以解决这些问题，但选择哪些层不进行量化并不简单。

**方法:** 提出了TuneQn，它可以生成选择性量化的ONNX模型，将其部署在不同硬件上，测量性能指标，执行Pareto Front最小化以确定最佳模型候选，并可视化结果。

**结果:** 与完全量化模型相比，最多可减少54.14%的精度损失；与原始模型相比，最多可减少72.9%的模型大小。

**结论:** TuneQn有效地实现了选择性量化和调整，能够在保持较高准确度的同时显著减小模型大小。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Selective+Quantization+Tuning+for+ONNX+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12196&send_immediately=true&force_search=false)

**原文摘要:** Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [37] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang, Sheng-An Xu, Peng Zhao, Masashi Sugiyama*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种在广义线性强盗问题中同时实现近乎最优遗憾界和每轮O(1)时间和空间复杂度的联合高效算法。


<details>
  <summary>更多</summary>
  
**动机:** 广义线性强盗（GLB）问题虽然适用于许多真实场景，但因其非线性的特性，在计算和统计效率上面临重大挑战。现有的方法通常需要在高每轮成本以获得最优遗憾保证和为了恒定时间更新而妥协统计效率之间做出选择。

**方法:** 该论文的核心方法是为在线镜像下降（OMD）估计器构建一个紧密置信集，这是通过利用来自在线预测的混合损失概念的新颖分析得出的。即使只进行一次更新，该OMD估计器也能达到与最大似然估计相当的统计效率。

**结果:** 所提出的算法实现了几乎最优的遗憾界，并且每轮的时间和空间复杂度均为O(1)，从而提供了一种联合高效的乐观方法。

**结论:** 该算法为广义线性强盗问题提供了一个有效的解决方案，既保持了统计效率又实现了低时间空间复杂度，为处理现实世界中的相关问题提供了新的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Linear+Bandits%3A+Almost+Optimal+Regret+with+One-Pass+Update，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11847，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11847&send_immediately=true&force_search=false)

**原文摘要:** We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [38] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James, Joseph Guinness*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种使用非平稳核的高斯过程框架，其中核参数随特征空间变化，并通过神经网络进行建模。该方法提高了模型表达能力，并在多个数据集上取得了更好的准确性和对数评分。


<details>
  <summary>更多</summary>
  
**动机:** 传统的高斯过程通常使用平稳核，这限制了模型的表达能力，并可能不适合许多数据集。为了提高模型的表达能力和适应性，需要一种可以处理非平稳性的方法。

**方法:** 作者提出了一种框架，使用非平稳核，其参数随特征空间变化，并将这些参数建模为以特征为输入的神经网络的输出。神经网络和高斯过程联合训练，使用链式法则计算导数。

**结果:** 在几个机器学习数据集上的测试表明，该方法比平稳模型和分层模型具有更高的准确性和对数评分。此外，该方法还能够恢复空间数据集的非平稳参数。

**结论:** 所提出的方法能够清晰地描述非平稳参数的行为，并且与大规模数据集的近似方法兼容。它灵活且易于适应不同的非平稳核，而无需重新设计优化程序。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Framework+for+Nonstationary+Gaussian+Processes+with+Neural+Network+Parameters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12262&send_immediately=true&force_search=false)

**原文摘要:** Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [39] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill, Brian L. Hill, Aria Masoomi, Vijay S. Nori, Robert E. Tillman, Jennifer Dy*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了OrdShap，一种新的归因方法，可以区分特征值及其在输入序列中的位置对模型预测的影响。通过排列特征位置，研究了模型预测的变化，并与Sanchez-Bergantinos值建立了博弈论联系，为位置敏感归因提供了理论依据。实证结果表明，OrdShap能有效捕捉特征值和特征位置的归因，提供更深入的模型行为见解。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度学习模型在处理具有时间或序列依赖性的领域表现出色，但理解其预测需要事后特征归因方法。然而，现有技术量化特征重要性时，假设特征顺序是固定的，混淆了特征值及其位置的影响。

**方法:** 作者引入了OrdShap，一种新的归因方法，通过量化模型预测在特征位置排列后如何变化，从而解开特征值和位置的影响。此外，还建立了OrdShap与Sanchez-Bergantinos值之间的博弈论联系。

**结果:** 来自健康、自然语言和合成数据集的经验结果突显了OrdShap在捕捉特征值和特征位置归因方面的有效性，提供了对模型行为的更深入了解。

**结论:** OrdShap提供了一种理论上合理的位置敏感归因方法，能够更准确地区分特征值和位置对模型预测的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OrdShap%3A+Feature+Position+Importance+for+Sequential+Black-Box+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11855，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11855&send_immediately=true&force_search=false)

**原文摘要:** Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [40] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum, Mahardhika Pratama, Savitha Ramasamy, Lin Liu, Habibullah Habibullah, Ryszard Kowalczyk*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新颖的基于提示的在线持续学习方法，该方法在CIFAR100、ImageNet-R、ImageNet-A和CUB数据集上显著优于当前的最先进方法。


<details>
  <summary>更多</summary>
  
**动机:** 在线持续学习（OCL）中的数据隐私约束使得流数据的灾难性遗忘问题更加复杂。目前的方法要么由于数据开放政策无法实际应用，要么存在与流数据吞吐量相关的问题。

**方法:** 提出了一个包含四个主要组件的新颖的基于提示的方法：1) 单一轻量级提示生成器作为一般知识；2) 可训练的缩放器和平移器作为特定知识；3) 预训练模型（PTM）泛化保持；4) 硬软更新机制。

**结果:** 该方法在CIFAR100、ImageNet-R、ImageNet-A和CUB数据集上的性能显著高于当前的最先进方法，并且所需的参数数量相对较少，达到了中等的训练时间、推理时间和吞吐量。

**结论:** 所提出的方法为解决在线持续学习中的数据隐私和灾难性遗忘问题提供了一个有效的解决方案，并且具有较低的计算复杂度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PROL+%3A+Rehearsal+Free+Continual+Learning+in+Streaming+Data+via+Prompt+Online+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12305，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12305&send_immediately=true&force_search=false)

**原文摘要:** The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [41] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai, Chang Gao, Fang He, Congyuan Ji, Yanni Yang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种改进的深度确定性策略梯度（pi-DDPG）框架，以动态管理司机对折扣快车服务的接受比例，提高早期训练阶段的性能，并通过数值实验验证了其优越的学习效率和显著减少的早期培训损失。


<details>
  <summary>更多</summary>
  
**动机:** 平台整合迅速扩展，成为缓解市场碎片化的一种有效解决方案，将多个叫车平台整合到一个应用程序中。为了应对乘客的不同偏好，第三方集成商提供由快车司机提供的折扣快车服务。然而，鼓励更多司机参与折扣快车服务虽然可以扩大可访问需求池并提高匹配效率，但往往会导致利润空间减少。因此，需要从个体平台的角度出发，动态地管理司机对折扣快车服务的接受情况。

**方法:** 由于缺乏历史数据，研究采用在线学习的方法，并将决策制定为连续控制任务。针对高随机性、不透明的匹配机制和有限的历史数据，提出了政策改进的深度确定性策略梯度（pi-DDPG）框架。该框架包含一个改进模块以提升早期训练阶段的性能，利用卷积长短期记忆网络捕捉复杂的时空模式，并采用优先经验回放机制以增强学习效率。

**结果:** 数值实验证明，pi-DDPG实现了更高的学习效率，并显著减少了早期阶段的训练损失。

**结论:** 所提出的pi-DDPG框架在处理新业务模型下的司机行为动态管理方面表现出色，能够在实践中实现可靠的早期性能，同时有效地捕捉复杂的空间时间模式并提高学习效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Policy-Improved+Deep+Deterministic+Policy+Gradient+Framework+for+the+Discount+Order+Acceptance+Strategy+of+Ride-hailing+Drivers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11865，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11865&send_immediately=true&force_search=false)

**原文摘要:** The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [42] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino, George D. C. Cavalcanti, Rafael M. O. Cruz*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于不平衡回归问题的元学习框架Meta-IR，通过训练元分类器来推荐最佳的重采样策略和学习模型组合。实验表明，该框架在处理不平衡回归问题上优于AutoML框架和其他基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 由于某些目标值的稀有性，不平衡问题不仅在分类任务中普遍存在，在回归任务中也带来了挑战。现有的解决办法通常是在预处理阶段使用平衡算法，但这需要测试多种重采样方法与学习模型的组合，以找到最优解。此外，学习模型、数据集和评估指标都会影响到最佳策略的选择。

**方法:** 本文提出了Meta-learning for Imbalanced Regression (Meta-IR)框架，通过训练元分类器来推荐针对每个任务的最佳管道，包括重采样策略和学习模型。此框架包含两种公式：独立式（Independent）和链式（Chained）。独立式分别指示最佳的学习算法和重采样策略；链式则采用顺序过程，利用一个元分类器的输出作为另一个的输入，以建模内在关系因素。

**结果:** 实验结果表明，链式场景表现出更好的性能，这表明对于每个任务，学习算法和重采样策略之间存在关联。与AutoML框架相比，Meta-IR获得了更好的结果，并且超过了42种配置的所有基线方法。

**结论:** Meta-IR框架为解决不平衡回归问题提供了一个新的视角，即通过元学习的方法自动选择最合适的重采样和学习模型组合。这一方法不仅提高了对不平衡回归问题的处理效果，而且超越了当前的一些先进方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Imbalanced+Regression+Pipeline+Recommendation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11901，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11901&send_immediately=true&force_search=false)

**原文摘要:** Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [43] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh, Boqi Chen, Marc Niethammer, Junier Oliva*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的非贪婪目标成本权衡获取方法（NOCTA），用于在推理时顺序获取最具信息量的特征，同时考虑时间和获取成本。该方法通过两个互补的估计器实现，并在合成和真实医疗数据集上展示了优越性。


<details>
  <summary>更多</summary>
  
**动机:** 在许多关键应用中，资源限制了可以收集的信息量，尤其是在医疗保健领域，患者数据跨越从实验室测试到影像研究等多样化的特征，每个特征都有不同的信息和相应的获取成本。此外，在时间预测任务中，实例特征和标签随时间演变，增加了决定何时或什么信息重要的复杂性。

**方法:** 作者提出了NOCTA，这是一种在推理时顺序获取最具信息量特征的方法，它考虑了时间和获取成本。为了实现这一目标，他们引入了一个统一的估计目标，并开发了两种互补的估计器：1）基于最近邻的非参数方法（NOCTA-NP）以指导获取；2）直接预测潜在获取效用的参数方法（NOCTA-P）。

**结果:** 实验表明，无论是合成数据还是真实世界的医疗数据集，NOCTA的两种变体都优于现有的基线方法。

**结论:** NOCTA提供了一种有效的方法来解决在资源受限环境中进行预测的问题，特别是在医疗保健领域的时间预测任务中，它能够更好地平衡信息获取的成本和收益。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NOCTA%3A+Non-Greedy+Objective+Cost-Tradeoff+Acquisition+for+Longitudinal+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12412&send_immediately=true&force_search=false)

**原文摘要:** In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [44] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino, George D. C. Cavalcanti, Rafael M. O. Cruz*

**主要类别:** cs.LG

**AI概要:** 本文通过实验研究和提出分类法，探讨了不平衡回归问题中的平衡策略和预测模型。


<details>
  <summary>更多</summary>
  
**动机:** 在现实世界中，不平衡问题是普遍存在的，并且主要是在分类的背景下进行研究。然而，在回归任务中也存在相同的问题，其中目标值是连续的。这项工作旨在解决这一不足，提供关于不平衡回归问题的新见解。

**方法:** 本研究采用了多种平衡算法和预测模型进行了广泛的实验研究，并使用度量来捕捉对用户重要的元素并评估预测模型。还提出了基于三个关键标准（回归模型、学习过程和评估指标）的不平衡回归方法的分类法。

**结果:** 研究表明，这些策略为每个模型的学习过程带来了优势，并指出了未来研究的方向。

**结论:** 该研究提供了关于不平衡回归问题的新见解，强调了所提出策略的优势，并为未来的进一步研究提供了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Resampling+strategies+for+imbalanced+regression%3A+a+survey+and+empirical+analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11902，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11902&send_immediately=true&force_search=false)

**原文摘要:** Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [45] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin, Giacomo Lagomarsini, Claudio Gallicchio, Giuseppe Nuti*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种混合光线追踪专家的架构，该架构可以动态选择专家序列，并产生宽度和深度可变的计算图。与现有方法相比，该方法可以在循环通过专家序列时提高预测准确性。实验表明，该方法在保持或提高准确性的同时减少了10%到40%的训练周期。


<details>
  <summary>更多</summary>
  
**动机:** 现有的专家混合（MoE）架构通常需要固定的计算量。为了实现更灵活、高效的模型设计，本文提出了一种新的方法，即Mixture of Raytraced Experts。

**方法:** 作者引入了Mixture of Raytraced Experts，这是一种堆叠的专家混合架构，它能够动态选择专家序列并生成宽度和深度可变的计算图。该模型通过迭代地从候选专家集中抽样进行训练，类似于递归神经网络的训练方式。

**结果:** 初步实验表明，该方法可以在保持或提高准确性的前提下减少10%到40%的训练周期。

**结论:** 这些结果指出了MoEs领域的新研究方向，使得更快、更具有表达力的模型设计成为可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixture+of+Raytraced+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12419，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12419&send_immediately=true&force_search=false)

**原文摘要:** We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [46] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins, Sihan Liu, Christopher Ye, Yuichi Yoshida*

**主要类别:** cs.LG

**AI概要:** 该论文解决了在低地平线表格MDP中，可复制的强化学习算法是否需要比批处理学习更多样本的问题。研究结果表明，探索并不是可复制学习的重大障碍，并提出了一种仅需Õ(S²A)样本的可复制RL算法，这几乎弥补了生成模型和情节设置之间的差距。


<details>
  <summary>更多</summary>
  
**动机:** 论文旨在探讨在控制环境（如强化学习）中，可复制的学习算法是否需要更多的样本才能达到与批处理环境下相近的效果，特别是针对低地平线表格马尔科夫决策过程(MDP)，以解决当前理论界对可复制探索成本高于批处理学习成本这一问题的疑问。

**方法:** 作者们开发了一种新的可复制的强化学习算法，该算法能在具有S个状态和A个动作的环境中，在无需生成模型的情况下，仅使用Õ(S²A)样本量即可学习到接近最优的策略。此外，他们还提供了在生成环境下匹配的Õ(S²A)下界和情节设置下的无条件Ω(S²)下界。

**结果:** 通过提出的算法，作者证明了探索并不是实现可复制学习的重要障碍，并且对于低地平线表格MDP而言，样本高效的可复制RL是可行的。算法的样本复杂度为Õ(S²A)，这几乎是生成和情节设置之间的最佳折衷方案。

**结论:** 论文得出结论，探索不是可复制学习的主要障碍，并提出了一个几乎是最优的可复制RL算法，该算法的样本复杂度为Õ(S²A)，并且适用于低地平线表格MDP。此结果缩小了生成模型和情节设置之间的差距，并展示了算法相对于状态空间S的近似最优性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Generative+to+Episodic%3A+Sample-Efficient+Replicable+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11926，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11926&send_immediately=true&force_search=false)

**原文摘要:** The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [47] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram, Neal Tuffy*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种机器学习加速的RF功率放大器设计优化框架，该框架通过智能探索多维参数空间，将仿真需求减少65%，同时保持±0.3到±0.4 dBm的精度。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法需要详尽地模拟所有参数组合以达到目标P2dB压缩规范，这非常耗费时间和资源。为了提高效率并减少仿真需求，提出了这种方法。

**方法:** 结合MaxMin Latin Hypercube Sampling与CatBoost梯度提升技术，有策略地选择大约35%的关键仿真点。该框架处理ADS网表，在简化数据集上执行谐波平衡仿真，并训练CatBoost模型以预测整个设计空间的P2dB性能。

**结果:** 验证结果显示，平均R²为0.901，系统能根据参数组合满足目标规格的可能性进行排名，并且通过自动化的GUI工作流减少了58.24%到77.78%的仿真时间。

**结论:** 该集成解决方案能够在不影响生产RF电路所需精度标准的情况下，实现快速设计迭代，并大幅减少仿真时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+RF+Power+Amplifier+Design+via+Intelligent+Sampling+and+ML-Based+Parameter+Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11928&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [48] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang, Callihan Bertley, Dawei Liang, Edison Thomaz*

**主要类别:** cs.LG

**AI概要:** 研究开发了一种新的计算方法，通过智能手表捕捉的音频和惯性数据来检测面对面的言语交流，并通过实验展示了多模态感知的优势。


<details>
  <summary>更多</summary>
  
**动机:** 社会互动在塑造人类行为、关系和社会中起着关键作用。这项工作旨在通过利用常见的智能手表捕获的音频和惯性数据，在声学挑战环境中检测人与人之间的面对面言语交流。

**方法:** 该研究使用机器学习和深度学习模型，结合3种不同的融合方法，将音频和惯性数据相结合，以考虑对话中的语言和非语言线索。还进行了一系列评估活动，以展示特定环境下的多模态感知优势。

**结果:** 该框架在实验室环境中检测对话的宏观F1得分为82.0±3.0%，在半自然环境下为77.2±1.8%。

**结论:** 音频和惯性数据的融合在检测面对面言语交流方面具有明显优势，证明了多模态感知在具体情境下的好处。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+In-Person+Conversations+in+Noisy+Real-World+Environments+with+Smartwatch+Audio+and+Motion+Sensing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12002，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12002&send_immediately=true&force_search=false)

**原文摘要:** Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [49] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha, Henrik Marklund, Potsawee Manakul, Richard Zeckhauser, Benjamin Van Roy*

**主要类别:** cs.LG

**AI概要:** 研究显示，随着反馈粒度的增加，使用有限个体的复杂方法组合反馈能显著改善对总体反馈分布预测的效果，特别是在五点反馈情况下，复杂方法只需约一半的个体数即可达到与正则化平均相同的表现。


<details>
  <summary>更多</summary>
  
**动机:** 在许多应用中，细粒度的人类反馈比二元反馈更具信息量。然而，由于成本限制，通常只能获取较小群体的反馈。因此，研究旨在探讨是否可以通过更复杂的组合方式来改进小样本下的反馈预测。

**方法:** 该研究通过引入复杂的方法（相对于简单的正则化平均）来组合个体反馈，并分析不同粒度反馈（如二元和五点）下这些方法的效果。

**结果:** 实证分析表明，对于二元反馈，复杂方法几乎没有减少达到固定性能水平所需的个体数量；而对于五点反馈，复杂方法仅需约一半的个体数即可达到与正则化平均相同的表现。

**结论:** 随着反馈粒度的增加，可以采用比正则化平均更复杂的方法来有效提高基于少量个体预测总体反馈分布的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Granular+feedback+merits+sophisticated+aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12041，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12041&send_immediately=true&force_search=false)

**原文摘要:** Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [50] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen, Tieliang Gong, Yunjiao Zhang, Zeyu Gao, Weizhan Zhang, Yong-Jin Liu*

**主要类别:** cs.LG

**AI概要:** 论文建立了一个统一的理论框架，为基于重放的持续学习（CL）方法提供了一系列信息论界限，揭示了如何有效利用有限的先前任务示例来改善泛化并减少灾难性遗忘。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已提出许多持续学习（CL）方法，并展示了令人印象深刻的实证性能，但对其泛化行为的理论理解仍然有限，特别是在基于重放的方法上。

**方法:** 作者建立了一个统一的理论框架，推导出一系列信息论界限，具体说明了内存缓冲区与当前任务之间的交互如何影响泛化。这些界限分为假设基础和预测基础两种类型，后者通过使用低维变量提供更紧致且计算上可行的泛化差距上限。

**结果:** 全面的实验评估证明了所推导界限在捕捉基于重放的CL设置中的泛化动态方面的有效性。

**结论:** 该分析是通用的，适用于广泛的算法，并以随机梯度Langevin动力学(SGLD)为例进行了说明。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Information-Theoretic+Generalization+Bounds+of+Replay-based+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12043&send_immediately=true&force_search=false)

**原文摘要:** Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [51] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean, Jiazu Zhou, Bu-Sung Lee, Markus Schläpfer*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的数据驱动方法，用于生成适应模拟城市场景的起讫点流动性流。该方法利用条件生成对抗网络将历史数据与自适应参数相结合，并在新加坡手机数据的应用中展示了其性能。


<details>
  <summary>更多</summary>
  
**动机:** 城市中的人口流动模式随着土地使用和人口的变化而演变，这对城市规划者提出了挑战，他们需要能够模拟和分析人类流动性模式以优化交通和实现可持续的城市发展。现有的生成模型要么依赖于历史轨迹，要么假设静态场景，无法很好地应对未来预测。

**方法:** 本研究的方法利用了自适应因素，例如动态区域大小和土地使用原型，并利用条件生成对抗网络（cGANs）将历史数据与这些自适应参数结合。这使得可以根据感兴趣的区域快速生成具有可调空间粒度的移动性流。

**结果:** 通过应用新加坡的移动电话数据，并与现有方法进行比较，证明了该方法在生成起讫点流动性流方面的有希望的性能。

**结论:** 这项研究表明，提出的新方法可以有效地生成适应模拟城市场景的起讫点流动性流，而无需大量校准数据或复杂的行为建模，为未来的城市规划提供了有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FloGAN%3A+Scenario-Based+Urban+Mobility+Flow+Generation+via+Conditional+GANs+and+Dynamic+Region+Decoupling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12053&send_immediately=true&force_search=false)

**原文摘要:** The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [52] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，该方法基于现有的Spotlight Resonance方法，用于确定表示对齐。通过研究自编码器模型中的离散表示如何形成和排列，发现激活函数的选择会对表示产生量化效应，即连续结构变得离散化。这种效应可能会对下游解释现象构成先决条件，并且初步结果表明，表示的量化似乎与可测量的重建误差增加有关。


<details>
  <summary>更多</summary>
  
**动机:** 作者希望通过探索网络原语的代数对称性来预测任务无关的结构，并了解不同的激活函数如何影响自编码器模型中的离散表示形式的形成和排列。

**方法:** 使用改进的Spotlight Resonance方法进行消融研究，仅改变激活函数，以观察其对自编码器模型中离散表示的影响。

**结果:** 当激活函数由离散代数置换等变对称性定义时，表示趋向于离散化；而当它们由连续代数正交等变定义时，表示保持连续。此外，初步结果显示，表示的量化似乎与可测量的重建误差增加有关。

**结论:** 功能形式的选择可以携带无意的归纳偏差，从而在表示中产生任务独立的伪结构，特别是当代形式会诱导连续结构的离散化——一种量化效应。这支持了离散表示形成的一般因果模型，并可能为新兴的解释性研究提供若干见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergence+of+Quantised+Representations+Isolated+to+Anisotropic+Functions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12070&send_immediately=true&force_search=false)

**原文摘要:** This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [53] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng, Wei Tang*

**主要类别:** cs.LG

**AI概要:** 本文提出了信息差距的概念，用于衡量不同预测模型在决策任务中的相对有用性，并引入了一种新的信息度量方法。


<details>
  <summary>更多</summary>
  
**动机:** 在许多应用场景中，决策者需要在多个可能未校准的预测模型之间进行选择。文章旨在解决如何判断哪个模型在下游决策任务中更有用的问题。

**方法:** 作者首先引入了两个预测器之间的信息差距概念，定义为一个预测器在所有决策任务中相对于另一个的最大标准化收益优势。其次，提供了信息差距的对偶特征，从而引出一种可以看作是两种预测分布之间的地球移动距离（EMD）的放松变体的自然信息度量方法。

**结果:** 该框架严格概括了几个现有概念，包括U-Calibration和Calibration Decision Loss，并在两个预测器都完全校准时恢复了Blackwell信息性。此外，所提出的度量方法满足自然期望：它是完整且可靠的，可以在仅预测访问设置中以样本高效的方式估计。

**结论:** 通过提出信息差距及其对偶特征，文章提供了一种新的、有效的评估预测模型在决策任务中有用性的方法，该方法适用于广泛的情况并且具有理论保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Informativeness+Gap+of+%28Mis%29Calibrated+Predictors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12094，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12094&send_immediately=true&force_search=false)

**原文摘要:** In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [54] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham, Thusitha Dayaratne, Viet Vo, Shangqi Lai, Sharif Abuadbba, Hajime Suzuki, Xingliang Yuan, Carsten Rudolph*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种半监督的联邦学习方法，用于解决频谱感知中标签数据稀缺的问题，并提出了一个受疫苗启发的新防御机制以应对数据投毒攻击。


<details>
  <summary>更多</summary>
  
**动机:** 随着无线和移动技术的发展，频谱资源变得越来越稀缺。动态频谱分配（DSA）是解决这一问题的关键，但集中式的机器学习模型在DSA系统中的应用受到了隐私、带宽和法规挑战的限制。分布式机器学习方法如联邦学习（FL）提供了一个有希望的替代方案。

**方法:** 该研究首先采用了半监督的联邦学习方法结合能量检测来解决实际频谱感知场景中标签数据稀缺的问题。其次，针对FLSS的安全漏洞，特别是数据投毒攻击的影响，提出了一种受疫苗启发的新颖防御机制。

**结果:** 实验验证了所提出的解决方案可以实现接近完美的未标记数据集上的准确性，并且可以在面对目标和非目标的数据投毒攻击时保持拜占庭鲁棒性，即使有相当比例的参与者是恶意的。

**结论:** 这项工作解决了FLSS中两个关键挑战：标签数据的缺乏和安全漏洞。研究表明，FLSS可以在实际应用中实现高效准确的频谱感知，并能有效抵御数据投毒攻击。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Adaptive+and+Robust+Federated+Spectrum+Sensing+without+Benign+Majority+for+Cellular+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12127&send_immediately=true&force_search=false)

**原文摘要:** Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [55] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu, Yuhe Huang, Yifeng Gong, Yanjie Zhai, Jiaxuan Lu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为HyDRA的混合双模射频架构，结合了优化的变分模式分解和一种新型架构，该架构融合了卷积神经网络、Transformer和Mamba组件。实验表明，HyDRA在封闭集场景中展示了最先进的准确性，并且在开放集分类任务中表现出色，能够有效识别未授权设备。部署在NVIDIA Jetson Xavier NX上，HyDRA实现了毫秒级推理速度和低功耗。


<details>
  <summary>更多</summary>
  
**动机:** 随着无线通信系统安全需求的增加，设备识别变得至关重要。特别是对于访问控制等应用，射频指纹识别（RFFI）提供了一种非加密解决方案，利用硬件引起的信号失真来识别设备。

**方法:** HyDRA结合了优化的变分模式分解（VMD）与融合卷积神经网络（CNNs）、Transformer和Mamba组件的新型架构。它使用TDSE进行全局依赖性建模，使用MLFE进行线性复杂度处理。

**结果:** 评估结果显示，HyDRA在封闭集场景中达到了最先进的准确性，在提出的开放集分类方法中也表现出了强大的性能，可以有效识别未授权设备。此外，它还具有毫秒级推理速度和低功耗。

**结论:** HyDRA为实时无线认证提供了实用的解决方案，适用于现实世界环境中的安全需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HyDRA%3A+A+Hybrid+Dual-Mode+Network+for+Closed-+and+Open-Set+RFFI+with+Optimized+VMD，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12133&send_immediately=true&force_search=false)

**原文摘要:** Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [56] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev, Vladimir Aletov, Alexander Molozhavenko, Denis Bobkov, Vera Soboleva, Aibek Alanov, Maxim Rakhuba*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新颖的方法，通过将固定秩LoRA矩阵视为平滑流形来同时解决初始化策略和过度参数化的问题，该方法在LLM和扩散模型架构上展示了更快的收敛速度和更好的最终性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管低秩适应（LoRA）是大语言模型高效微调的有效方法，但在初始化策略和低秩矩阵分解中的过度参数化问题上仍存在挑战。

**方法:** 该方法将一组固定秩LoRA矩阵视为平滑流形，通过确定沿流形的最快损失减少方向来进行初始化，并移除过度参数化。

**结果:** 实验结果表明，RiemannLoRA在大型语言模型和扩散模型架构中均能提供比标准LoRA及其最先进改进更快的收敛速度和更优的最终性能。

**结论:** RiemannLoRA为低秩自适应矩阵提供了一个统一框架，解决了过度参数化和初始化问题，提高了计算效率并改善了模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RiemannLoRA%3A+A+Unified+Riemannian+Framework+for+Ambiguity-Free+LoRA+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12142，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12142&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [57] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev, Thorsten Kurth, Ankur Mahesh, Mauro Bisson, Jean Kossaifi, Karthik Kashinath, Anima Anandkumar, William D. Collins, Michael S. Pritchard, Alexander Keller*

**主要类别:** cs.LG

**AI概要:** FourCastNet 3 使用可扩展的几何机器学习方法进行概率集合预报，提供更快更准确的全球天气模型，具有计算效率、中期概率技能、光谱保真度和次季节时间尺度的滚动稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 改进全球天气建模，实现比传统模型更快速和准确的概率集合预报。

**方法:** 使用纯粹的卷积神经网络架构，针对球形几何设计，并通过结合模型和数据并行性的新颖训练范式实现大规模训练。

**结果:** 预测准确性超越领先的常规集合模型，与最佳扩散方法相当，但速度提高了8到60倍，并且在长达60天的延伸预报中保持良好的概率校准和现实的光谱。

**结论:** FourCastNet 3 是改善气象预报和早期预警系统的重要候选者，尤其适用于通过大集合预测提高预报能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FourCastNet+3%3A+A+geometric+approach+to+probabilistic+machine-learning+weather+forecasting+at+scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12144，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12144&send_immediately=true&force_search=false)

**原文摘要:** FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [58] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari, Mohamed El-Baha, Raphael Meunier, Rodrigue Décatoire, Mathilde Mougeot*

**主要类别:** cs.LG

**AI概要:** 引入了GMRF MCVAE，一种新的生成框架，在先验和后验分布中嵌入高斯马尔可夫随机场，以更准确地建模多组件数据集的复杂依赖关系。实验结果表明其在多项基准测试中的优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多组件变分自编码器采用简化的聚合策略，忽略了关键细节，导致生成组件之间的结构连贯性受损。

**方法:** 设计了一种新的生成框架——高斯马尔可夫随机场多组件变分自编码器（GMRF MCVAE），该框架将高斯马尔可夫随机场嵌入到先验和后验分布中，从而显式地建模跨组件关系。

**结果:** 在合成的Copula数据集上达到了最先进的性能，在PolyMNIST基准上表现出竞争力，并且在实际的BIKED数据集上显著增强了结构连贯性。

**结论:** GMRF MCVAE特别适用于需要稳健和现实的多组件一致性建模的实际应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Component+VAE+with+Gaussian+Markov+Random+Field，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12165&send_immediately=true&force_search=false)

**原文摘要:** Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [59] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang, Qiming Zhang, Nan Cheng, Junting Chen, Zezhong Zhang, Zan Li, Shuguang Cui, Xuemin Shen*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的大规模高分辨率3D无线射频地图数据集UrbanRadio3D，并引入了RadioDiff-3D模型，用于构建三维射频地图。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法大多集中于二维平面内的路径损耗预测，忽略了到达方向、到达时间和垂直空间变化等关键参数，这些限制主要源于静态学习范式对超越训练数据分布的泛化能力的阻碍。

**方法:** 作者提出了UrbanRadio3D，一个通过在真实城市环境中进行光线追踪而建立的大规模、高分辨率3D RM数据集。同时，为了测试3D RM构造，提出了一种带有3D卷积算子的UNet，并进一步引入了基于扩散模型的生成框架RadioDiff-3D。

**结果:** 广泛的评估验证了RadioDiff-3D在构建丰富、高维度射频地图方面具有优越的表现，能够在不同的环境动态下工作。

**结论:** 这项工作为未来在3D环境感知通信领域的研究提供了一个基础数据集和基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RadioDiff-3D%3A+A+3D%24%5Ctimes%243D+Radio+Map+Dataset+and+Generative+Diffusion+Based+Benchmark+for+6G+Environment-Aware+Communication，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12166，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12166&send_immediately=true&force_search=false)

**原文摘要:** Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [60] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza, Karima Bakhti, Sofiane Ramdani, Denis Mottet, Abdelhak Imoussaten*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的解释证据聚类结果的方法，通过引入代表性和效用函数的概念，并提出了迭代证据错误最小化（IEMM）算法，该算法在合成和真实数据上得到了验证。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的数据往往包含不确定性与不精确性，传统方法处理得并不好。基于Dempster-Shafer理论的证据聚类可以解决这些问题，但其结果难以解释，特别是在医疗等高风险领域。

**方法:** 文章分析了决策树作为证据聚类结果解释者的适用条件，并通过引入代表性概念来构建部分标签下的解释器。此外，还定义了证据错误作为解释成本，并提出了迭代证据错误最小化（IEMM）算法，以提供对证据聚类函数的可解释且谨慎的决策树解释。

**结果:** 提出的算法在合成和真实数据集上的表现良好，考虑决策者偏好的情况下，提供的解释满意率高达93%。

**结论:** 研究结果表明，在证据聚类中，利用代表性和效用函数可以帮助构建有效的解释器，而IEMM算法为提高解释质量提供了可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainable+Evidential+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12192&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [61] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

**主要类别:** cs.LG

**AI概要:** 本文研究了一种基于线性模型的物理信息方法，用于求解偏微分方程。该方法在估计地壳应变率方面优于数学正则化方法，并提供了一个解析求解框架。


<details>
  <summary>更多</summary>
  
**动机:** 许多物理系统由偏微分方程（PDEs）描述，从观测数据中求解这些方程和估算系数或边界条件对于理解相关现象至关重要。

**方法:** 提出了一种使用基函数线性组合表示解的物理信息线性模型（PILM），并将其与数学正则化方法进行了比较。

**结果:** 物理正则化在速度场上施加弹性平衡，而数学正则化施加平滑约束，从贝叶斯角度来看，后者表现更优。

**结论:** PILM提供了解析求解框架，适用于线性正问题和逆问题、不定系统和物理正则化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Informed+Linear+Model+%28PILM%29%3A+Analytical+Representations+and+Application+to+Crustal+Strain+Rate+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12218，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12218&send_immediately=true&force_search=false)

**原文摘要:** Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [62] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu, Clare Lyle, Ionut-Vlad Modoranu, Naima Elosegui Borras, Dan Alistarh, Petar Velickovic, Sarath Chandar, Soham De, James Martens*

**主要类别:** cs.LG

**AI概要:** 这篇论文强调了优化器在深度神经网络训练中不仅影响收敛速度，还影响学习解决方案的定性特性。作者认为社区应更关注理解现有方法的偏差，并设计具有特定诱导特性的新型优化器，而不仅仅是基于收敛速度来评估它们。


<details>
  <summary>更多</summary>
  
**动机:** 动机是解决当前对优化器的研究过于集中在收敛效率上，而忽略了优化器如何影响最终模型的性质这一问题。

**方法:** 作者通过分析过去几十年深度学习取得的进步以及大型DNNs表现出的良好优化动态，提出了应该从不同的视角看待优化器的作用，即优化器可以编码归纳偏见并改变给定模型类别的有效表达能力。

**结果:** 结果是呼吁研究界更加重视优化器的设计，认识到它作为塑造模型结果的关键杠杆的重要性，与架构和数据的作用相辅相成。

**结论:** 结论是希望此论点能够激发更多关于学习过程如何影响我们所收敛到的解类型的理解，并提升对优化器设计重要性的认识。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizers+Qualitatively+Alter+Solutions+And+We+Should+Leverage+This，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12224，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12224&send_immediately=true&force_search=false)

**原文摘要:** Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [63] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni, Giuseppe Masi, Andrea Coletta, Aldo Glielmo, Viviana Arrigoni, Novella Bartolini*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于提取幂律谱特征的稳健因果发现方法，这种方法在合成基准和真实世界数据集上均优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 许多现有的因果发现算法对噪声敏感，在实际数据中可能导致误导性的因果推断。观察到典型的现实世界时间序列的频率谱遵循幂律分布，这种特性可以用来放大真正的因果信号。

**方法:** 研究人员构建了一种新的因果发现方法，该方法通过提取幂律谱特征来增强真实的因果信号，从而提高对噪声的鲁棒性。

**结果:** 该方法在合成基准测试和具有已知因果结构的真实世界数据集上都表现出色，一致优于最先进替代方案。

**结论:** 新提出的基于幂律谱特征提取的因果发现方法不仅提高了对噪声的鲁棒性，而且展示了其在实践中应用的相关性和优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Causal+Discovery+in+Real-World+Time+Series+with+Power-Laws，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12257，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12257&send_immediately=true&force_search=false)

**原文摘要:** Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [64] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu, Zhiwei Lin, Yongtao Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的非重放缓续学习框架RegCL，通过模型合并算法整合多领域知识，解决特定领域适配器方法可能带来的灾难性遗忘问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的针对Segment Anything Model (SAM)的性能局限性的解决方案主要采用基于适配器的一次性适应范式，但这些方法在其他领域可能会导致性能下降，即发生灾难性遗忘，限制了模型的可扩展性。

**方法:** 提出了RegCL框架，它将模型合并算法引入持续学习范式中，通过优化权重最小化合并模型与各领域特定模型之间的预测差异，从而有效整合多领域知识并保持参数效率。

**结果:** 实验结果表明，RegCL在多个下游数据集上实现了有利的持续学习性能，证明了其在动态场景中的有效性。

**结论:** RegCL有效地解决了特定领域适配器方法可能导致的灾难性遗忘问题，同时保持了模型大小恒定且不需要存储历史数据，适用于多领域知识的有效整合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RegCL%3A+Continual+Adaptation+of+Segment+Anything+Model+via+Model+Merging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12297，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12297&send_immediately=true&force_search=false)

**原文摘要:** To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [65] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest, Pirmin Lemberger*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 LEOPARD的方法，通过概念擦除技术在嵌入空间中学习正交投影，以消除文本表示中的敏感信息，如性别或种族，从而确保模型的公平性。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界应用中的神经模型不应从文本表示中推断出敏感信息，例如性别或种族等人口统计属性。这在关注公平性时是一个关键挑战。

**方法:** 该方法涉及在嵌入空间中学习一个正交投影，旨在使离散概念的类条件特征分布不可区分。通过调整投影仪的秩来控制信息移除的程度，同时其正交性确保严格保留嵌入的局部结构。

**结果:** LEOPARD 在经典的自然语言处理基准上实现了离散属性非线性擦除的最先进性能，并且有效地减轻了深度非线性分类器中的偏差，从而促进公平性。

**结论:** 该研究提供了一种有效的方法来消除文本表示中的敏感信息，同时尽可能保留语义信息，有助于提升实际应用中的模型公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nonlinear+Concept+Erasure%3A+a+Density+Matching+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12341，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12341&send_immediately=true&force_search=false)

**原文摘要:** Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [66] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn, Vikas Garg*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的拓扑框架，通过在组合复数上引入拉普拉斯算子来有效计算热核作为节点描述符。该方法捕捉多尺度信息，并支持与现代基于变压器的架构集成。理论和实证结果表明，它具有高效性和区分复杂拓扑结构的能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决现有拓扑神经网络中高阶消息传递带来的显著计算开销问题，作者提出了一个新的框架，旨在提供既表达力强又可扩展的表示方法。

**方法:** 新方法通过在组合复数（CCs）上引入拉普拉斯算子来实现热核的有效计算，用作节点描述符。这些描述符捕捉多尺度信息并允许排列等变表示，可以无缝地整合到现代基于变压器的架构中。

**结果:** 理论上，该方法能够区分任意非同构的CCs，因此是最大限度表达性的。实证上，它不仅在计算效率方面显著优于现有的拓扑方法，在标准分子数据集上的性能也与最先进的描述符相当，并且在区分复杂拓扑结构方面表现出色。

**结论:** 这项工作通过提供表达力强且可扩展的表示方法，推进了拓扑深度学习领域的发展，为分子分类和性质预测任务开辟了新的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heat+Kernel+Goes+Topological，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12380，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12380&send_immediately=true&force_search=false)

**原文摘要:** Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [67] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant, Arvind Easwaran*

**主要类别:** cs.LG

**AI概要:** 本文通过构建更小的MDP来近似原始MDP，将样本复杂度降低了对数因子，并在无限时间范围、无模型的情况下构造了一个PAC-MDP算法。


<details>
  <summary>更多</summary>
  
**动机:** 进一步研究无限时间范围内的马尔可夫决策过程（MDP）设置中强化学习（RL）的样本复杂度问题，以获得比现有文献更精确的可能近似正确（PAC）边界。

**方法:** 通过使用较小的MDP构建子集来近似原始MDP的方法，将样本复杂度降低了对数因子到O(SA log A)时间步长，其中S和A分别是状态和动作空间大小。

**结果:** 该方法显著提高了算法性能，在实验环境中与之前的工作相比显示出明显的改进。

**结论:** 通过构造一个PAC-MDP算法，我们能够将样本复杂度降低到O(SA log A)，并且在实验结果中证明了这一改进的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Reinforcement+Learning+Sample-Efficiency+using+Local+Approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12383&send_immediately=true&force_search=false)

**原文摘要:** In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [68] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen, Guoyun Gao, Zhicheng Xu, Ruibin Mao, Xiaojuan Qi, X. Sharon Hu, Xunzhao Yin, Can Li*

**主要类别:** cs.LG

**AI概要:** 本研究通过使用具有软边界的$MoS_2$ Flash基类比内容寻址存储器（CAM）进行硬件-软件协同设计，实现了对基于树的模型的有效推理。实验结果表明，该方法不仅提高了对抗设备变化和对抗攻击的鲁棒性，还达到了顶尖的准确性，并保持了决策的可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能的快速发展，人们对其可靠性的担忧日益增加，特别是在解释性和鲁棒性方面。基于树的模型如随机森林和XGBoost在表格数据上的解释性和准确性表现出色，但计算成本高昂。为了克服这些问题，研究人员试图利用模拟内容寻址存储器（CAM）加速这些模型，但由于实现尖锐决策边界困难且易受设备变化影响，导致硬件性能不佳并容易受到对抗攻击。

**方法:** 研究人员采用了一种新的硬件-软件协同设计方案，即使用具有固有软边界的$MoS_2$ Flash基类比内容寻址存储器（CAM）。此方案使得基于树的模型能够更有效地进行推理，并且对设备变化和对抗攻击更具鲁棒性。

**结果:** 实验结果表明，所提出的软树模型推理方法在WDBC数据库上达到了96%的准确率，在MNIST数据集上即使存在10%的设备阈值变化时，准确率也只下降了0.6%，而传统决策树的准确率则下降了45.3%。这证明了该方法在提高AI可信度和效率方面的潜力。

**结论:** 这项工作为增强AI的信任度和效率开辟了道路，特别是对于基于树的模型而言，通过专门的硬件设计可以显著提升其在面对设备变化和对抗攻击时的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Trustworthy+Tree-based+Machine+Learning+by+%24MoS_2%24+Flash-based+Analog+CAM+with+Inherent+Soft+Boundaries，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12384，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12384&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [69] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner, Yatong Chen, André F. Cruz, Fanny Yang*

**主要类别:** cs.LG

**AI概要:** 本文研究了验证器不完美如何影响测试时扩展性能，并通过理论和实验分析了BoN和拒绝采样方法的实例级准确率与ROC曲线几何的关系。


<details>
  <summary>更多</summary>
  
**动机:** 许多工作已经经验性地研究了使用验证器在推理期间进行测试时扩展的技术，但对于验证器不完美如何影响性能缺乏理论理解。

**方法:** 作者证明了这些方法的实例级准确率由验证器的ROC曲线几何特性精确描述，并比较了拒绝采样和BoN方法在这种特性下的表现。

**结果:** 当ROC曲线未知时，无法根据低计算资源情况推断拒绝采样的性能；固定计算资源下，拒绝采样优于BoN，但两者在无限计算资源下会收敛到相同的准确率水平。

**结论:** 理论结果通过在GSM8K数据集上使用不同版本的Llama和Qwen生成和验证解决方案的实验得到了证实。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ROC-n-reroll%3A+How+verifier+imperfection+affects+test-time+scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12399&send_immediately=true&force_search=false)

**原文摘要:** Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [70] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li, David Mccoy, Nolan Gunter, Kaitlyn Lee, Alejandro Schuler, Mark van der Laan*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的框架——Targeted Deep Architectures (TDA)，它直接将Targeted Maximum Likelihood Estimation嵌入到网络的参数空间中，无需对主干架构进行限制。通过迭代更新模型参数的一个小的“targeting”子集，可以去除一阶偏差并产生渐近有效的置信区间。实验证明，与标准神经网络估计器和之前的post-hoc方法相比，TDA能减少偏差并提高覆盖率。


<details>
  <summary>更多</summary>
  
**动机:** 现代深度神经网络虽然具有强大的预测能力，但在因果参数（如治疗效果或整个生存曲线）的有效推断方面往往不足。现有的去偏方法要么不能保证解决有效影响函数方程，要么在多参数设置下计算成本昂贵。

**方法:** TDA将TMLE直接嵌入到网络的参数空间中，无需对主干架构进行限制。具体来说，TDA划分了模型参数 - 冻结除了一个小的“targeting”子集外的所有参数 - 并沿着一个从影响函数投影到损失梯度上的targeting梯度进行迭代更新。

**结果:** 理论上，TDA继承了经典的TMLE属性，包括双重稳健性和半参数效率。实证上，在IHDP数据集和模拟的生存数据上，TDA减少了偏差并提高了覆盖率。

**结论:** TDA为现代深度架构中的复杂多参数目标提供了一条直接、可扩展的路径，以实现严谨的因果推断。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Targeted+Deep+Architectures%3A+A+TMLE-Based+Framework+for+Robust+Causal+Inference+in+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12435，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12435&send_immediately=true&force_search=false)

**原文摘要:** Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [71] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie, Linda Cai, Alexander Terenin, Peter I. Frazier, Ziv Scully*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种成本感知的贝叶斯优化停止规则，该规则适应不同的评估成本，无需启发式调整，并且在理论上与先进的成本感知获取函数有联系。实验表明，该规则与Pandora's Box Gittins Index (PBGI) 获取函数结合使用时，在成本调整后的简单遗憾方面，能够匹配或超越其他获取函数-停止规则组合。


<details>
  <summary>更多</summary>
  
**动机:** 在自动机器学习、科学发现等领域中，决定何时停止评估昂贵的黑盒函数是一个重要的实际考虑因素。虽然已经提出了几种自适应停止规则，但在成本感知设置下，它们缺乏确保在产生过多函数评估成本之前停止的保证。

**方法:** 作者提出了一种基于理论连接到最先进的成本感知获取函数（如Pandora's Box Gittins Index (PBGI)和每成本预期改进的日志）的成本感知停止规则。并且证明了当该停止规则与这两个获取函数一起使用时，其预期累积评估成本的理论保证。

**结果:** 通过在合成任务和包括超参数优化及神经架构大小搜索在内的实证任务上的实验，结果表明，将所提出的停止规则与PBGI获取函数结合使用，可以在成本调整后的简单遗憾这一指标上，一致地匹配或优于其他获取函数-停止规则对。

**结论:** 新提出的成本感知停止规则可以适应变化的评估成本，无需启发式调优，并且与PBGI获取函数结合使用时，可以实现更好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cost-aware+Stopping+for+Bayesian+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12453&send_immediately=true&force_search=false)

**原文摘要:** In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [72] [A Study on the Application of Artificial Intelligence in Ecological Design](https://arxiv.org/abs/2507.11595)
*Hengyue Zhao*

**主要类别:** cs.AI

**AI概要:** 本文探讨了人类与自然的关系能否从人类主导转变为真正的相互依赖，以及人工智能（AI）是否能促进这种转变。通过案例研究展示了艺术家和设计师如何应用AI进行数据分析、图像识别和生态修复，并提出了结合强化学习与植物修复的设计路径。研究强调了AI在连接科学见解、艺术实践和环境保护方面的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 作者关注到当前人类与自然的关系主要是人类主导的，而希望通过引入人工智能技术，能够探索一种新的生态设计范式，实现人类与非人类生命形式之间的真正相互依赖关系。

**方法:** 作者通过案例研究的方式，分析了艺术家和设计师如何使用AI来进行数据分析、图像识别和生态修复。此外，还基于作者自己的原型——AI辅助的水体修复，提出了将强化学习与植物修复相结合的设计路径。

**结果:** 研究表明，AI不仅扩展了创意方法，而且重新定义了生态设计的理论和实践。它能够在科学见解、艺术实践和环境管理之间建立联系，为未来的可持续发展和技术赋能生态系统研究提供了路线图。

**结论:** 本文提出，AI在促进人类与自然从主导关系向相互依赖关系的转变中具有巨大潜力。它能够为生态设计提供新的视角和工具，促进科学、艺术和环境保护的融合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Study+on+the+Application+of+Artificial+Intelligence+in+Ecological+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11595&send_immediately=true&force_search=false)

**原文摘要:** This paper asks whether our relationship with nature can move from human
dominance to genuine interdependence, and whether artificial intelligence (AI)
can mediate that shift. We examine a new ecological-design paradigm in which AI
interacts with non-human life forms. Through case studies we show how artists
and designers apply AI for data analysis, image recognition, and ecological
restoration, producing results that differ from conventional media. We argue
that AI not only expands creative methods but also reframes the theory and
practice of ecological design. Building on the author's prototype for
AI-assisted water remediation, the study proposes design pathways that couple
reinforcement learning with plant-based phytoremediation. The findings
highlight AI's potential to link scientific insight, artistic practice, and
environmental stewardship, offering a roadmap for future research on
sustainable, technology-enabled ecosystems.

</details>


### [73] [General Modular Harness for LLM Agents in Multi-Turn Gaming Environments](https://arxiv.org/abs/2507.11633)
*Yuxuan Zhang, Haoyang Yu, Lanxiang Hu, Haojian Jin, Hao Zhang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种模块化框架设计，它包含感知、记忆和推理组件，使得单个LLM或VLM主干能够应对多种多轮游戏环境。实验表明该框架能显著提升游戏表现，并揭示了不同模块对不同类型游戏的独特贡献。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在创建一种无需特定领域工程的通用型智能体，可以处理各种多轮游戏环境，以提高其在动态交互设置中的性能并研究各个模块对整体性能的影响。

**方法:** 通过引入一个由感知、记忆和推理组件组成的模块化框架，研究人员开发了一种使单一LLM或VLM主干能够应对广泛多轮游戏环境的方法。此方法使用经典和现代游戏套件作为测试平台，提供统一的工作流程来分析每个模块如何影响跨动态互动设置的表现。

**结果:** 大量实验展示了这个框架能一致地提升游戏表现，超过未使用该框架的基础水平。并且发现，在长时域解谜游戏中记忆模块起主导作用，而在视觉嘈杂的街机游戏中感知模块是关键。

**结论:** 模块化框架设计的有效性得到了证明，它可以在日常人类熟悉的游戏环境中推进通用智能体的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是General+Modular+Harness+for+LLM+Agents+in+Multi-Turn+Gaming+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11633，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11633&send_immediately=true&force_search=false)

**原文摘要:** We introduce a modular harness design for LLM agents that composes of
perception, memory, and reasoning components, enabling a single LLM or VLM
backbone to tackle a wide spectrum of multi turn gaming environments without
domain-specific engineering. Using classic and modern game suites as
low-barrier, high-diversity testbeds, our framework provides a unified workflow
for analyzing how each module affects performance across dynamic interactive
settings. Extensive experiments demonstrate that the harness lifts gameplay
performance consistently over un-harnessed baselines and reveals distinct
contribution patterns, for example, memory dominates in long-horizon puzzles
while perception is critical in vision noisy arcades. These findings highlight
the effectiveness of our modular harness design in advancing general-purpose
agent, given the familiarity and ubiquity of games in everyday human
experience.

</details>


### [74] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade, Joonhyuk Cha, Brandon Ho, Vriksha Srihari, Karmesh Yadav, Zsolt Kira*

**主要类别:** cs.AI

**AI概要:** 论文研究了多模态大语言模型（MLLMs）作为验证器在评估代理轨迹中的应用，发现其存在同意偏差的问题，并提出了一种名为自我基础验证（SGV）的新方法来解决这个问题。通过SGV改进后的MLLM验证器在准确性、失败检测率等方面均有显著提升，且能实时监督异构代理，提升了任务完成度。


<details>
  <summary>更多</summary>
  
**动机:** 由于人类可以识别合适的成果，但将这种直觉转化为可扩展的规则并不简单，特别是在没有明确成功标准的领域中。为了克服这一挑战，研究人员探索了多模态大语言模型（MLLMs）作为一种潜在的解决方案。

**方法:** 该研究提出了一个称为自我基础验证（Self-Grounded Verification, SGV）的方法，它分两个步骤操作：首先，MLLM被用来检索关于任务完成的广泛先验知识；然后，在基于自动生成的先验条件下，对候选轨迹进行推理和评价。

**结果:** 使用SGV增强的MLLM验证器在准确性和失败检测率上提高了20个百分点，并能实时监督不同类型的代理，从而提高了多个基准测试中的任务完成度，比之前最好的结果高出48%。

**结论:** 尽管MLLMs在作为验证器时表现出强烈的人类对齐偏好，但仍存在同意偏差的问题。通过引入SGV方法，可以在不增加复杂性的情况下更有效地利用MLLM的知识和推理能力，提高其作为验证器的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Let%27s+Think+in+Two+Steps%3A+Mitigating+Agreement+Bias+in+MLLMs+with+Self-Grounded+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11662，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11662&send_immediately=true&force_search=false)

**原文摘要:** Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


### [75] [ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making](https://arxiv.org/abs/2507.11733)
*Srikanth Vemula*

**主要类别:** cs.AI

**AI概要:** 本研究介绍了一种名为Clarity and Reasoning Interface for Artificial Intelligence (ClarifAI)的新方法，旨在通过案例推理(CBR)和本体论驱动的方法增强人工智能的透明度和解释性。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能技术在各个领域中的应用越来越广泛，不同利益相关者对AI系统的透明度和可解释性提出了更高的要求。为了满足这些需求，研究人员开发了Clarity and Reasoning Interface for Artificial Intelligence (ClarifAI)。

**方法:** 该方法结合了案例推理（CBR）方法和本体论驱动的方法，以提供详尽的解释机制，并强调了其设计原则和架构蓝图。

**结果:** 该论文详细阐述了ClarifAI在提高AI解释性方面的潜力及其在不同领域的适用性，特别是在高风险环境中。

**结论:** 研究表明，ClariAI在提升AI系统解释性方面具有重要作用，为将其部署于关键决策过程铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ClarifAI%3A+Enhancing+AI+Interpretability+and+Transparency+through+Case-Based+Reasoning+and+Ontology-Driven+Approach+for+Improved+Decision-Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11733&send_immediately=true&force_search=false)

**原文摘要:** This Study introduces Clarity and Reasoning Interface for Artificial
Intelligence(ClarifAI), a novel approach designed to augment the transparency
and interpretability of artificial intelligence (AI) in the realm of improved
decision making. Leveraging the Case-Based Reasoning (CBR) methodology and
integrating an ontology-driven approach, ClarifAI aims to meet the intricate
explanatory demands of various stakeholders involved in AI-powered
applications. The paper elaborates on ClarifAI's theoretical foundations,
combining CBR and ontologies to furnish exhaustive explanation mechanisms. It
further elaborates on the design principles and architectural blueprint,
highlighting ClarifAI's potential to enhance AI interpretability across
different sectors and its applicability in high-stake environments. This
research delineates the significant role of ClariAI in advancing the
interpretability of AI systems, paving the way for its deployment in critical
decision-making processes.

</details>


### [76] [Auto-Formulating Dynamic Programming Problems with Large Language Models](https://arxiv.org/abs/2507.11737)
*Chenyu Zhou, Jingyuan Yang, Linwei Xin, Yitian Chen, Ziyan He, Dongdong Ge*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个新的动态规划语言模型（DPLM）和一个名为DP-Bench的基准测试，以及DualReflect合成数据生成管道。DPLM在处理动态规划问题上表现优异，尤其在难题上超越了现有的大型语言模型。


<details>
  <summary>更多</summary>
  
**动机:** 动机在于解决动态规划问题自动化建模的挑战，尤其是面对这类问题特有的随机转变和有限训练数据的问题。

**方法:** 该研究提出了Dynamic Programming Language Model (DPLM)，这是一个7B参数的专门化模型，并引入了DualReflect，一种新的合成数据生成方法，包括前向生成以增加多样性，和后向生成以确保正确性。

**结果:** DPLM在一系列动态规划问题上的表现可与最先进的大型语言模型相媲美，在难题上甚至超过了它们。

**结论:** 结果表明，后向生成在数据量少的情况下更为有利，而前向生成在大规模数据时更有价值，两者结合可以发挥互补优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Auto-Formulating+Dynamic+Programming+Problems+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11737&send_immediately=true&force_search=false)

**原文摘要:** Dynamic programming (DP) is a fundamental method in operations research, but
formulating DP models has traditionally required expert knowledge of both the
problem context and DP techniques. Large Language Models (LLMs) offer the
potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of
training data. These factors make it difficult to directly apply existing
LLM-based models or frameworks developed for other optimization problems, such
as linear or integer programming. We introduce DP-Bench, the first benchmark
covering a wide range of textbook-level DP problems to enable systematic
evaluation. We present Dynamic Programming Language Model (DPLM), a
7B-parameter specialized model that achieves performance comparable to
state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on
hard problems. Central to DPLM's effectiveness is DualReflect, our novel
synthetic data generation pipeline, designed to scale up training data from a
limited set of initial examples. DualReflect combines forward generation for
diversity and backward generation for reliability. Our results reveal a key
insight: backward generation is favored in low-data regimes for its strong
correctness guarantees, while forward generation, though lacking such
guarantees, becomes increasingly valuable at scale for introducing diverse
formulations. This trade-off highlights the complementary strengths of both
approaches and the importance of combining them.

</details>


### [77] [Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11787)
*Chandrashekar Muniyappa, Eunjin Kim*

**主要类别:** cs.AI

**AI概要:** 这篇论文回顾了基于群体智能算法的语义相似性文档搜索的最新进展，并推荐了未来的研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 由于群体智能算法在解决计算机优化问题上的有效性，研究人员开始探索其在基于语义相似性的文档搜索中的应用。

**方法:** 作者将对这一领域的最新发展进行全面审查，包括基于群体智能的各种算法和方法。

**结果:** 该调查将提供一个全面的关于使用群体智能进行语义相似性文档搜索的最新研究总结。

**结论:** 根据审查的结果，作者会建议未来的研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Survey+of+Swarm+Intelligence+Approaches+to+Search+Documents+Based+On+Semantic+Similarity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11787，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11787&send_immediately=true&force_search=false)

**原文摘要:** Swarm Intelligence (SI) is gaining a lot of popularity in artificial
intelligence, where the natural behavior of animals and insects is observed and
translated into computer algorithms called swarm computing to solve real-world
problems. Due to their effectiveness, they are applied in solving various
computer optimization problems. This survey will review all the latest
developments in Searching for documents based on semantic similarity using
Swarm Intelligence algorithms and recommend future research directions.

</details>


### [78] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi, Nathan R. Sturtevant*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种在深度优先搜索中批处理GPU计算的方法，通过结合现代CPU和GPU的并行性来创建如Batch IDA*等算法，并在3x3魔方和4x4滑动拼图上进行了性能评估。


<details>
  <summary>更多</summary>
  
**动机:** 尽管GPU技术的快速发展为增强经典搜索算法提供了新的机会，但很少有算法能充分利用GPU进行搜索。因此，有必要开发一种能够利用GPU的强大并行处理能力的方法，以改进现有的搜索算法。

**方法:** 作者引入了一种在深度优先搜索中批处理GPU计算的方法，特别是描述了一种新的成本限制深度优先搜索(CB-DFS)方法，该方法利用了现代CPU和GPU的组合并行性。此外，还提出了Batch IDA*和Batch BTS等算法。

**结果:** 该方法在3x3 Rubik's Cube和4x4滑动拼图问题上的实验表明，GPU操作可以在DFS中高效批处理。并且，通过对超参数、神经网络启发式大小和硬件资源对性能影响的广泛实验，进一步验证了该方法的有效性。

**结论:** 作者提出的方法能够有效地在深度优先搜索中批处理GPU计算，从而提高了搜索效率。此外，该方法适用于多种经典的搜索算法，如IDA*和Budgeted Tree Search。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Parallel+CPU-GPU+Framework+for+Cost-Bounded+DFS+with+Applications+to+IDA%2A+and+BTS，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11916&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [79] [Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
*Yexuan Shi, Mingyu Wang, Yunxiang Cao, Hongjie Lai, Junjian Lan, Xin Han, Yu Wang, Jie Geng, Zhenan Li, Zihao Xia, Xiang Chen, Chen Li, Jian Xu, Wenbo Duan, Yuanshuo Zhu*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的多代理框架Aime，通过动态反应计划和执行克服了现有系统中的局限性。实验结果表明，Aime在不同领域的基准测试中均优于其他最先进的代理。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多代理系统受限于僵化的计划执行、静态的代理能力和低效的通信，这阻碍了它们在动态环境中的适应性和鲁棒性。

**方法:** Aime的核心创新点包括：(1) 动态规划器，根据实时执行反馈不断优化整体策略；(2) 演员工厂，实现动态演员实例化，按需组装具有定制工具和知识的专门代理；(3) 集中式进度管理模块，作为系统范围状态感知的单一信息源。

**结果:** 实证评估结果显示，Aime在涵盖通用推理、软件工程和实时网络导航的多个基准测试中，其适应性和任务成功率均超过了领域内的顶尖代理。

**结论:** Aime作为一个更富有弹性和有效的基础，为多代理协作提供了新的可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aime%3A+Towards+Fully-Autonomous+Multi-Agent+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11988&send_immediately=true&force_search=false)

**原文摘要:** Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient communication. These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.

</details>


### [80] [Understanding visual attention beehind bee-inspired UAV navigation](https://arxiv.org/abs/2507.11992)
*Pranav Rajbhandari, Abhi Veda, Matthew Garratt, Mandayam Srinivasan, Sridhar Ravi*

**主要类别:** cs.AI

**AI概要:** 受生物启发，使用强化学习训练仅依赖光流作为感觉输入的自主无人机在有障碍物的隧道中导航。训练后的智能体主要关注光流不连续区域和光流较大的区域以避开障碍物并保持居中位置。


<details>
  <summary>更多</summary>
  
**动机:** 利用生物系统在有限的感觉和计算能力下飞行和避障的能力，模仿蜜蜂利用光流导航杂乱环境的方法。

**方法:** 通过强化学习训练智能体仅使用光流作为感觉输入在有障碍物的隧道中导航，并检查训练有素的智能体的注意力模式。

**结果:** 训练后的智能体主要关注光流不连续区域和光流较大的区域，以避开障碍物并保持居中位置，这种行为类似于飞行昆虫的行为。

**结论:** 此策略可能适用于开发物理无人机的简单显式控制法则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+visual+attention+beehind+bee-inspired+UAV+navigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11992&send_immediately=true&force_search=false)

**原文摘要:** Bio-inspired design is often used in autonomous UAV navigation due to the
capacity of biological systems for flight and obstacle avoidance despite
limited sensory and computational capabilities. In particular, honeybees mainly
use the sensory input of optic flow, the apparent motion of objects in their
visual field, to navigate cluttered environments. In our work, we train a
Reinforcement Learning agent to navigate a tunnel with obstacles using only
optic flow as sensory input. We inspect the attention patterns of trained
agents to determine the regions of optic flow on which they primarily base
their motor decisions. We find that agents trained in this way pay most
attention to regions of discontinuity in optic flow, as well as regions with
large optic flow magnitude. The trained agents appear to navigate a cluttered
tunnel by avoiding the obstacles that produce large optic flow, while
maintaining a centered position in their environment, which resembles the
behavior seen in flying insects. This pattern persists across independently
trained agents, which suggests that this could be a good strategy for
developing a simple explicit control law for physical UAVs.

</details>


### [81] [Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs](https://arxiv.org/abs/2507.12110)
*Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种拓扑增强的多智能体强化学习（TPE-MARL）方法，通过构建游戏拓扑张量和使用QMIX作为骨干RL算法来优化混合交通中连接和自主车辆（CAVs）的合作决策。


<details>
  <summary>更多</summary>
  
**动机:** 在多智能体强化学习（MARL）中，探索-利用权衡是一个基本挑战，尤其是在处理连接和自动驾驶车辆（CAVs）在混合交通中的合作决策时，由于联合状态-行动空间的指数增长而变得更加复杂。

**方法:** 该研究首先构建了一个游戏拓扑张量，用于动态交通流，有效地压缩了高维交通状态信息，并减少了MARL算法的搜索空间。然后在此设计的游戏拓扑张量的基础上，结合访问次数和代理互信息，建立了一个拓扑增强的MARL框架。

**结果:** 广泛的模拟实验表明，在不同的交通密度和CAV渗透率下，TPE-MARL展示了其有效性。它成功地平衡了探索与利用，并在交通效率、安全性、决策平稳性和任务完成方面表现出优越性能。此外，该算法在混合自治和完全自治的交通场景中展现了与人类驾驶员相当或超过的决策合理性。

**结论:** TPE-MARL方法为优化连接和自动驾驶车辆在混合交通中的合作决策提供了一种有效的方法，实现了更好的探索-利用平衡，并且在多个关键性能指标上优于现有的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Topology+Enhanced+MARL+for+Multi-Vehicle+Cooperative+Decision-Making+of+CAVs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12110，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12110&send_immediately=true&force_search=false)

**原文摘要:** The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.

</details>


### [82] [Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation](https://arxiv.org/abs/2507.12186)
*Edward Kim, Hanna Kurniawati*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的部分可观测参考策略编程，可以在稀疏采样的情况下有效解决POMDP问题，并在大规模动态环境中展现出色性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的在线规划方法在处理部分可观测马尔可夫决策过程（POMDP）时，往往因为需要大量采样而难以应用于大规模问题。为了解决这个问题，作者希望找到一种能够在稀疏采样的情况下也能够有效工作的算法。

**方法:** 作者提出了Partially Observable Reference Policy Programming (PORPP)，这是一种可以深度采样有意义的未来历史的同时逐渐更新策略的方法。并且，作者提供了理论保证，证明了该算法的性能损失是由采样近似误差的平均值而非最大值决定的。

**结果:** 通过在两个大规模动态环境问题上的实证评估，包括一个需要大约150个规划步骤的直升机紧急情况场景，验证了理论结果，并表明所提出的求解器显著优于当前的在线基准。

**结论:** PORPP提供了一种新的解决POMDP问题的方法，在稀疏采样的情况下也能保持良好的性能，对于大规模和动态变化的问题具有重要的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Partially+Observable+Reference+Policy+Programming%3A+Solving+POMDPs+Sans+Numerical+Optimisation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12186，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12186&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.

</details>


### [83] [BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2507.12207)
*Subin Lin, Chuanbo Hua*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为BuildEvo的新框架，该框架使用大型语言模型（LLMs）自动设计有效的、可解释的能源预测启发式算法。通过将建筑特征和运行数据中的物理见解系统地结合进来，BuildEvo在基准测试中实现了最先进的性能，提供了改进的泛化能力和透明的预测逻辑。


<details>
  <summary>更多</summary>
  
**动机:** 准确的建筑能耗预测至关重要，但传统的启发式方法往往缺乏精确性，而高级模型可能是不透明的，并且由于忽视物理原理而在泛化上存在困难。

**方法:** BuildEvo使用大型语言模型（LLM）通过进化过程自动设计有效且可解释的能源预测启发式算法，通过系统地纳入建筑物特征和操作数据中的物理见解来构建和增强启发式算法。

**结果:** 评估表明，BuildEvo在基准测试中达到了最先进的性能，提供了改进的泛化能力和透明的预测逻辑。

**结论:** 这项工作推进了稳健的、基于物理的启发式的自动化设计，促进了复杂能源系统的可信模型的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BuildEvo%3A+Designing+Building+Energy+Consumption+Forecasting+Heuristics+via+LLM-driven+Evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12207&send_immediately=true&force_search=false)

**原文摘要:** Accurate building energy forecasting is essential, yet traditional heuristics
often lack precision, while advanced models can be opaque and struggle with
generalization by neglecting physical principles. This paper introduces
BuildEvo, a novel framework that uses Large Language Models (LLMs) to
automatically design effective and interpretable energy prediction heuristics.
Within an evolutionary process, BuildEvo guides LLMs to construct and enhance
heuristics by systematically incorporating physical insights from building
characteristics and operational data (e.g., from the Building Data Genome
Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on
benchmarks, offering improved generalization and transparent prediction logic.
This work advances the automated design of robust, physically grounded
heuristics, promoting trustworthy models for complex energy systems.

</details>


### [84] [Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning](https://arxiv.org/abs/2507.12215)
*Yuhao Chen, Shuochen Liu, Yuanjie Lyu, Chao Zhang, Jiayao Shi, Tong Xu*

**主要类别:** cs.AI

**AI概要:** 论文通过中国象棋测试大型语言模型在空间战略推理方面的能力，提出了一种新的训练框架和名为Xiangqi-R1的模型，实验结果表明该模型比通用大型语言模型有显著改进。


<details>
  <summary>更多</summary>
  
**动机:** 评估人工智能在中国象棋这一复杂且完全可观测的棋盘游戏中的空间战略推理能力，以推动人工智能在这一领域的发展。

**方法:** 采用五百万个棋局移动对的大规模数据集，结合专家注释和引擎评估，提出了一个针对中国象棋的训练框架，并引入了Xiangqi-R1模型。该模型经过三个阶段的训练：（1）合法移动预测的微调；（2）整合战略注释以改进决策；（3）使用多维奖励信号的群体相对策略优化进行强化学习。

**结果:** 相比通用大型语言模型，Xiangqi-R1在移动合法性上提高了18%，在分析准确性上提高了22%。

**结论:** 本研究为在空间复杂的环境中创建一般战略性智能提供了一条有希望的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Xiangqi-R1%3A+Enhancing+Spatial+Strategic+Reasoning+in+LLMs+for+Chinese+Chess+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12215&send_immediately=true&force_search=false)

**原文摘要:** Game playing has long served as a fundamental benchmark for evaluating
Artificial General Intelligence (AGI). While Large Language Models (LLMs) have
demonstrated impressive capabilities in general reasoning, their effectiveness
in spatial strategic reasoning, which is critical for complex and fully
observable board games, remains insufficiently explored. In this work, we adopt
Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate
rules and spatial complexity. To advance LLMs' strategic competence in such
environments, we propose a training framework tailored to Xiangqi, built upon a
large-scale dataset of five million board-move pairs enhanced with expert
annotations and engine evaluations. Building on this foundation, we introduce
Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning
for legal move prediction to capture basic spatial rules, (2) incorporating
strategic annotations to improve decision-making, and (3) applying
reinforcement learning via Group Relative Policy Optimization (GRPO) with
multi-dimensional reward signals to enhance reasoning stability. Our
Experimental results indicate that, despite their size and power,
general-purpose LLMs struggle to achieve satisfactory performance in these
tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an
18% rise in move legality and a 22% boost in analysis accuracy. Our results
point to a promising path for creating general strategic intelligence in
spatially complex areas.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [85] [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
*Brendan Murphy, Dillon Bowen, Shahrad Mohammadzadeh, Julius Broomfield, Adam Gleave, Kellin Pelrine*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种名为jailbreak-tuning的方法，通过微调AI模型，使其能够对有害请求产生详细、高质量的响应。这种方法甚至可以使最新的AI模型更容易受到攻击，强调了防篡改保护措施的紧迫性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的防护系统在阻止AI模型被用于恶意用途方面存在局限，无法完全去除防护措施或导致输出质量下降。研究者希望找到一种方法，可以更有效地使AI模型生成对有害请求的响应。

**方法:** 研究人员采用了一种名为jailbreak-tuning的方法，通过微调（fine-tuning）来训练AI模型，使其能够根据有害请求生成详细的、高质量的响应。

**结果:** 使用该方法后，来自OpenAI、Google和Anthropic等公司的模型能够完全遵从有害请求，如化学、生物、放射性和核（CBRN）援助，执行网络攻击和其他犯罪活动。此外，后门机制不仅增加了攻击的隐蔽性，还加剧了攻击的严重程度。

**结论:** 随着AI模型的发展，它们正变得越来越容易受到此类攻击。因此，在未发现防篡改保护措施之前，任何可微调模型的发布都应被视为同时发布了其“邪恶双胞胎”，即与原模型能力相当，但可用于任何在其能力范围内的恶意目的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jailbreak-Tuning%3A+Models+Efficiently+Learn+Jailbreak+Susceptibility，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11630&send_immediately=true&force_search=false)

**原文摘要:** AI systems are rapidly advancing in capability, and frontier model developers
broadly acknowledge the need for safeguards against serious misuse. However,
this paper demonstrates that fine-tuning, whether via open weights or closed
fine-tuning APIs, can produce helpful-only models. In contrast to prior work
which is blocked by modern moderation systems or achieved only partial removal
of safeguards or degraded output quality, our jailbreak-tuning method teaches
models to generate detailed, high-quality responses to arbitrary harmful
requests. For example, OpenAI, Google, and Anthropic models will fully comply
with requests for CBRN assistance, executing cyberattacks, and other criminal
activity. We further show that backdoors can increase not only the stealth but
also the severity of attacks, while stronger jailbreak prompts become even more
effective in fine-tuning attacks, linking attack and potentially defenses in
the input and weight spaces. Not only are these models vulnerable, more recent
ones also appear to be becoming even more vulnerable to these attacks,
underscoring the urgent need for tamper-resistant safeguards. Until such
safeguards are discovered, companies and policymakers should view the release
of any fine-tunable model as simultaneously releasing its evil twin: equally
capable as the original model, and usable for any malicious purpose within its
capabilities.

</details>


### [86] [Evasion Under Blockchain Sanctions](https://arxiv.org/abs/2507.11721)
*Endong Liu, Mark Ryan, Liyi Zhou, Pascal Berrang*

**主要类别:** cs.CR

**AI概要:** 本文研究了美国外国资产控制办公室（OFAC）对加密货币混币工具Tornado Cash实施的制裁效果，指出了当前制裁执行实践中的三个结构性限制，并提出了一种新的算法以改进追踪和评分。


<details>
  <summary>更多</summary>
  
**动机:** 鉴于许可区块链上的复杂交易流程和高级的资金混淆技术，使得监管机构难以执行有效的制裁。因此，作者希望评估现有制裁的效果并探索改进的方法。

**方法:** 作者使用Tornado Cash作为案例，分析了在957天期间，6.79百万个以太坊区块和10.7亿笔交易中，OFAC制裁的有效性。通过识别当前制裁执行实践中的结构性限制，提出了一种基于定量杂质的新算法用于评分和跟踪。

**结果:** 尽管OFAC制裁使Tornado Cash存款总量减少了71.03%，但攻击者仍然在78.33%的与以太坊相关的安全事件中依赖Tornado Cash。新提出的算法平均处理以太坊区块的时间为0.07秒，精度达到97.61%，召回率达到74.08%。

**结论:** 文章通过对制裁效果的实证分析，强调了去中心化金融领域内监管有效性的讨论，并提出了未来合规策略的方向，以应对制裁和基于区块链的安全风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evasion+Under+Blockchain+Sanctions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11721&send_immediately=true&force_search=false)

**原文摘要:** Sanctioning blockchain addresses has become a common regulatory response to
malicious activities. However, enforcement on permissionless blockchains
remains challenging due to complex transaction flows and sophisticated
fund-obfuscation techniques. Using cryptocurrency mixing tool Tornado Cash as a
case study, we quantitatively assess the effectiveness of U.S. Office of
Foreign Assets Control (OFAC) sanctions over a 957-day period, covering 6.79
million Ethereum blocks and 1.07 billion transactions. Our analysis reveals
that while OFAC sanctions reduced overall Tornado Cash deposit volume by 71.03%
to approximately 2 billion USD, attackers still relied on Tornado Cash in
78.33% of Ethereum-related security incidents, underscoring persistent evasion
strategies.
  We identify three structural limitations in current sanction enforcement
practices: (i) the susceptibility of binary sanction classifications to dusting
attacks; (ii) fragmented censorship by blockchain producers; and (iii) the
complexity of obfuscation services exploited by users. To address these gaps,
we introduce a more practical algorithm for scoring and tracking, grounded in
quantitative impurity. On average, our algorithm processes Ethereum blocks
within 0.07 $\pm$ 0.03 seconds and achieves 97.61% precision and 74.08% recall
when evaluated on the Bybit exploit. Our findings contribute to ongoing
discussions around regulatory effectiveness in Decentralized Finance by
providing empirical evidence, clarifying enforcement challenges, and informing
future compliance strategies in response to sanctions and blockchain-based
security risks.

</details>


### [87] [Space Cybersecurity Testbed: Fidelity Framework, Example Implementation, and Characterization](https://arxiv.org/abs/2507.11763)
*Jose Luis Castanon Remy, Caleb Chang, Ekzhin Ear, Shouhuai Xu*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种用于表征空间网络安全测试平台保真度的框架，并用此框架指导构建和表征一个具体的测试平台。


<details>
  <summary>更多</summary>
  
**动机:** 当前对于太空基础设施（包括卫星和地面系统）的网络威胁理解不足，且目前几乎没有关于构建测试平台的研究或对测试平台的特征描述。

**方法:** 作者提出了一个包含7个属性的框架，用于表征测试平台可以容纳的系统模型、威胁模型和防御措施。

**结果:** 该框架被用来指导构建和表征一个实际的测试平台，该平台涵盖了空间、地面、用户和链接段，并展示了它如何适应一些现实中发生的太空网络攻击场景。

**结论:** 通过提出的框架，研究者能够更好地理解和验证太空网络安全问题，同时为未来的研究方向提供了讨论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Space+Cybersecurity+Testbed%3A+Fidelity+Framework%2C+Example+Implementation%2C+and+Characterization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11763&send_immediately=true&force_search=false)

**原文摘要:** Cyber threats against space infrastructures, including satellites and systems
on the ground, have not been adequately understood. Testbeds are important to
deepen our understanding and validate space cybersecurity studies. The state of
the art is that there are very few studies on building testbeds, and there are
few characterizations of testbeds. In this paper, we propose a framework for
characterizing the fidelity of space cybersecurity testbeds. The framework
includes 7 attributes for characterizing the system models, threat models, and
defenses that can be accommodated by a testbed. We use the framework to guide
us in building and characterizing a concrete testbed we have implemented, which
includes space, ground, user, and link segments. In particular, we show how the
testbed can accommodate some space cyber attack scenarios that have occurred in
the real world, and discuss future research directions.

</details>


### [88] [How To Mitigate And Defend Against DDoS Attacks In IoT Devices](https://arxiv.org/abs/2507.11772)
*Ifiyemi Leigha, Basak Comlekcioglu, Maria Pilar Bezanilla*

**主要类别:** cs.CR

**AI概要:** 本文分析了DDoS攻击在物联网环境中的影响，并提出了包括IPv6唯一本地地址、边缘计算、软件定义网络、蜜罐技术和基于机器学习的入侵检测系统等分层缓解策略。


<details>
  <summary>更多</summary>
  
**动机:** 由于许多连接设备的安全配置较低，DDoS攻击在物联网网络中变得越来越普遍和危险。

**方法:** 研究分析了如Mirai僵尸网络发起的DDoS攻击的本质和影响，并探索了IPv6唯一本地地址（ULA）、边缘计算、软件定义网络（SDN）、蜜罐欺骗以及基于机器学习的入侵检测系统作为关键解决方案。

**结果:** 该论文为工程师和研究人员提供了理解和实施保护物联网基础设施的实际对策。

**结论:** 通过了解和应用文中提出的分层防护策略，工程师和研究人员可以更好地保护物联网网络免受DDoS攻击的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+To+Mitigate+And+Defend+Against+DDoS+Attacks+In+IoT+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11772，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11772&send_immediately=true&force_search=false)

**原文摘要:** Distributed Denial of Service (DDoS) attacks have become increasingly
prevalent and dangerous in the context of Internet of Things (IoT) networks,
primarily due to the low-security configurations of many connected devices.
This paper analyzes the nature and impact of DDoS attacks such as those
launched by the Mirai botnet, and proposes layered mitigation strategies
tailored to IoT environments. Key solutions explored include IPv6 Unique Local
Addresses (ULA), edge computing, software-defined networking (SDN), honeypot
deception, and machine learning-based intrusion detection systems. The paper
aims to help engineers and researchers understand and implement practical
countermeasures to protect IoT infrastructures.

</details>


### [89] [Challenges in GenAI and Authentication: a scoping review](https://arxiv.org/abs/2507.11775)
*Wesley dos Reis Bezerra, Lais Machado Bezerra, Carlos Becker Westphall*

**主要类别:** cs.CR

**AI概要:** 这篇论文通过分析88份文档，探讨了生成式人工智能对社会和系统安全的影响，特别是在认证和真实性方面。研究结果强调了图像、文本、音频和视频面临的挑战、差距和威胁。


<details>
  <summary>更多</summary>
  
**动机:** 自从信息共享开始以来，认证和真实性一直是信息安全的挑战。随着生成式人工智能的发展，这些挑战已经演变，需要更现代的分析来评估其对社会和系统安全的影响。

**方法:** 该研究进行了范围综述，分析了来自IEEExplorer、Scopus和ACM数据库的88篇文献，并通过六个指导问题进行分析，这些问题聚焦于最相关的工作、挑战、攻击面、威胁、提出的解决方案和空白。

**结果:** 研究结果一致地概述了与图像、文本、音频和视频相关的挑战、差距和威胁，支持在认证和生成式人工智能领域的新研究。

**结论:** 该研究通过对88篇文献的分析，明确了生成式人工智能对认证和真实性带来的多方面挑战，为未来的研究提供了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Challenges+in+GenAI+and+Authentication%3A+a+scoping+review，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11775&send_immediately=true&force_search=false)

**原文摘要:** Authentication and authenticity have been a security challenge since the
beginning of information sharing, especially in the context of digital
information. With the advancement of generative artificial intelligence, these
challenges have evolved, demanding a more up-to-date analysis of their impacts
on society and system security. This work presents a scoping review that
analyzed 88 documents from the IEEExplorer, Scopus, and ACM databases,
promoting an analysis of the resulting portfolio through six guiding questions
focusing on the most relevant work, challenges, attack surfaces, threats,
proposed solutions, and gaps. Finally, the portfolio articles are analyzed
through this guiding research lens and also receive individualized analysis.
The results consistently outline the challenges, gaps, and threats related to
images, text, audio, and video, thereby supporting new research in the areas of
authentication and generative artificial intelligence.

</details>


### [90] [Unveiling Usability Challenges in Web Privacy Controls](https://arxiv.org/abs/2507.11908)
*Rahat Masood, Sunday Oyinlola Ogundoyin, Muhammad Ikram, Alex Ye*

**主要类别:** cs.CR

**AI概要:** 本文通过大规模实证分析研究了18,628个网站的隐私控制可用性挑战，发现了隐私政策普遍存在、提示和通知在注册情况下常见，并提出了改进隐私控制设计的建议。


<details>
  <summary>更多</summary>
  
**动机:** 随着对隐私的关注度提高和数据隐私法律的实施，许多网站提供了隐私控制，但这些控制通常隐藏在多个设置层中，缺乏标准化，术语复杂，导致用户难以找到和有效使用。

**方法:** 作者进行了大规模实证分析，自动收集了18,628个网站的数据，评估了三种不同用户访问场景下的隐私控制可用性问题。

**结果:** 结果表明，隐私政策在所有访问场景中最常见，而在注册情境下，提示和通知尤为普遍。同时，自动化捕获动态用户交互面临挑战，更多集中在访客访问场景。

**结论:** 作者建议设计隐私控制时应：通过弹出提示和通知增强用户意识；提供目录作为导航辅助工具以及政策中的定制化设置链接以供更明智的选择；并确保通过直接链接到隐私设置从提示中实现可访问性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unveiling+Usability+Challenges+in+Web+Privacy+Controls，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11908，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11908&send_immediately=true&force_search=false)

**原文摘要:** With the increasing concerns around privacy and the enforcement of data
privacy laws, many websites now provide users with privacy controls. However,
locating these controls can be challenging, as they are frequently hidden
within multiple settings and layers. Moreover, the lack of standardization
means these controls can vary widely across services. The technical or
confusing terminology used to describe these controls further complicates
users' ability to understand and use them effectively. This paper presents a
large-scale empirical analysis investigating usability challenges of web
privacy controls across 18,628 websites. While aiming for a multi-scenario
view, our automated data collection faced significant hurdles, particularly in
simulating sign-up and authenticated user visits, leading to more focused
insights on guest visit scenarios and challenges in automated capture of
dynamic user interactions. Our heuristic evaluation of three different user
visit scenarios identifies significant website usability issues. Our results
show that privacy policies are most common across all visit scenarios, with
nudges and notices being prevalent in sign-up situations. We recommend
designing privacy controls that: enhance awareness through pop-up nudges and
notices; offer a table of contents as navigational aids and customized settings
links in policies for more informed choice; and ensure accessibility via direct
links to privacy settings from nudges.

</details>


### [91] [Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification](https://arxiv.org/abs/2507.11943)
*Haiwei Lin, Shoko Imaizumi, Hitoshi Kiya*

**主要类别:** cs.CR

**AI概要:** 提出一种低秩适应方法，用于训练隐私保护的视觉Transformer模型，通过冻结预训练ViT模型权重和训练低秩分解矩阵，减少可训练参数数量并保持高准确度。


<details>
  <summary>更多</summary>
  
**动机:** 为了在保护隐私的前提下，提高视觉Transformer模型的训练效率，并尽量减少对模型性能的影响。

**方法:** 通过在每个ViT层中注入可训练的低秩分解矩阵，并且不冻结patch嵌入层，以实现对预训练ViT模型权重的冻结。

**结果:** 该方法可以减少可训练参数的数量，同时几乎保持与完全调优相同的准确性。

**结论:** 所提出的低秩适应方法可以在保证模型性能的同时，有效地减少训练资源消耗，适用于隐私保护场景下的ViT模型训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Effective+Fine-Tuning+of+Vision+Transformers+with+Low-Rank+Adaptation+for+Privacy-Preserving+Image+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11943，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11943&send_immediately=true&force_search=false)

**原文摘要:** We propose a low-rank adaptation method for training privacy-preserving
vision transformer (ViT) models that efficiently freezes pre-trained ViT model
weights. In the proposed method, trainable rank decomposition matrices are
injected into each layer of the ViT architecture, and moreover, the patch
embedding layer is not frozen, unlike in the case of the conventional low-rank
adaptation methods. The proposed method allows us not only to reduce the number
of trainable parameters but to also maintain almost the same accuracy as that
of full-time tuning.

</details>


### [92] [Expanding ML-Documentation Standards For Better Security](https://arxiv.org/abs/2507.12003)
*Cara Ellen Appel*

**主要类别:** cs.CR

**AI概要:** 本文综述了当前机器学习安全及基于ML的系统、模型和数据集的文档记录状况，指出了从业者和组织对安全问题的低关注度以及文档记录缺乏标准化的问题，并提出通过扩展现有文档标准以包含安全部分来改善ML的安全文档记录。


<details>
  <summary>更多</summary>
  
**动机:** 指出当前在机器学习领域中，安全意识薄弱，文档记录方式非标准化，导致整体文档质量低下，需要改进ML的安全文档记录。

**方法:** 通过对现有文献进行广泛回顾，分析现状并识别出存在的问题；建议在现有的Model Cards和Datasheets for Datasets标准基础上增加一个安全相关的部分，形成一种新的ML文档记录方法。

**结果:** 发现现有的标准在实践中并未被常规采用，IT安全方面通常未包含在文档中，提出了改进ML安全文档的具体建议。

**结论:** 为了填补当前机器学习安全领域的空白，需要提高安全文档的质量，可以通过扩展现有文档标准以包含特定的安全相关信息来实现这一目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Expanding+ML-Documentation+Standards+For+Better+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12003&send_immediately=true&force_search=false)

**原文摘要:** This article presents the current state of ML-security and of the
documentation of ML-based systems, models and datasets in research and practice
based on an extensive review of the existing literature. It shows a generally
low awareness of security aspects among ML-practitioners and organizations and
an often unstandardized approach to documentation, leading to overall low
quality of ML-documentation. Existing standards are not regularly adopted in
practice and IT-security aspects are often not included in documentation. Due
to these factors, there is a clear need for improved security documentation in
ML, as one step towards addressing the existing gaps in ML-security. To achieve
this, we propose expanding existing documentation standards for
ML-documentation to include a security section with specific security relevant
information. Implementing this, a novel expanded method of documenting security
requirements in ML-documentation is presented, based on the existing Model
Cards and Datasheets for Datasets standards, but with the recommendation to
adopt these findings in all ML-documentation.

</details>


### [93] [IDFace: Face Template Protection for Efficient and Secure Identification](https://arxiv.org/abs/2507.12050)
*Sunpill Kim, Seunghun Paik, Chanwoo Hwang, Dongsoo Kim, Junbum Shin, Jae Hong Seo*

**主要类别:** cs.CR

**AI概要:** 本研究提出了一种基于同态加密（HE）的高效且安全的人脸识别方法IDFace，该方法通过两个新颖的技术显著提高了加密人脸模板匹配的速度和效率。实验表明，IDFace在处理100万个加密模板时仅需126毫秒，相比明文识别仅有2倍的开销。


<details>
  <summary>更多</summary>
  
**动机:** 随着人脸识别系统（FRS）的广泛应用，用户隐私保护变得越来越重要。一个关键的隐私问题在于保护用户的面部模板，因为可以从模板中恢复出用户面部图像的特征。虽然同态加密（HE）为保障FRS的安全提供了机会，但其直接应用于FRS并不能高效运行。许多先前基于HE的面部模板保护方法比没有保护的系统慢数百倍。

**方法:** 研究人员提出了IDFace，这是一种基于HE的新方法，旨在实现高效和安全的人脸识别，并保护模板。它基于两种新技术：一是模板表示转换，可以大幅降低匹配测试的单位成本；二是空间高效的编码方式，减少了加密算法造成的空间浪费，从而节省了对加密模板的操作次数。

**结果:** 通过实验验证，IDFace可以在126毫秒内从包含100万个加密模板的数据库中识别出一个面部模板，与明文识别相比仅有2倍的开销。

**结论:** IDFace作为一种新的HE-based安全且高效的人脸识别方法，在保护模板的同时实现了快速匹配，解决了现有方法效率低下的问题，为FRS中的隐私保护提供了一种可行的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IDFace%3A+Face+Template+Protection+for+Efficient+and+Secure+Identification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12050&send_immediately=true&force_search=false)

**原文摘要:** As face recognition systems (FRS) become more widely used, user privacy
becomes more important. A key privacy issue in FRS is protecting the user's
face template, as the characteristics of the user's face image can be recovered
from the template. Although recent advances in cryptographic tools such as
homomorphic encryption (HE) have provided opportunities for securing the FRS,
HE cannot be used directly with FRS in an efficient plug-and-play manner. In
particular, although HE is functionally complete for arbitrary programs, it is
basically designed for algebraic operations on encrypted data of predetermined
shape, such as a polynomial ring. Thus, a non-tailored combination of HE and
the system can yield very inefficient performance, and many previous HE-based
face template protection methods are hundreds of times slower than plain
systems without protection. In this study, we propose IDFace, a new HE-based
secure and efficient face identification method with template protection.
IDFace is designed on the basis of two novel techniques for efficient searching
on a (homomorphically encrypted) biometric database with an angular metric. The
first technique is a template representation transformation that sharply
reduces the unit cost for the matching test. The second is a space-efficient
encoding that reduces wasted space from the encryption algorithm, thus saving
the number of operations on encrypted templates. Through experiments, we show
that IDFace can identify a face template from among a database of 1M encrypted
templates in 126ms, showing only 2X overhead compared to the identification
over plaintexts.

</details>


### [94] [Toward an Intent-Based and Ontology-Driven Autonomic Security Response in Security Orchestration Automation and Response](https://arxiv.org/abs/2507.12061)
*Zequan Huang, Jacques Robin, Nicolas Herbaut, Nourhène Ben Rabah, Bénédicte Le Grand*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种统一的安全意图定义方法和一个两层的方法论，将意图驱动的网络安全与自主网络安全结合起来，并在下一代SOAR平台中展示了其可行性。


<details>
  <summary>更多</summary>
  
**动机:** 现代SOAR平台需要快速适应不断演变的网络攻击。意图驱动网络提供了一种更灵活、持久的网络攻击缓解方法。为了结合意图驱动的网络安全和自主网络安全的研究方向，提出了本研究。

**方法:** 作者建议使用MITRE-D3FEND网络安全本体论来创建一种统一的、基于本体的安全意图定义。此外，还提出了一个通用的两层方法论，以将这些安全意图整合到决策理论的自主网络安全系统中。

**结果:** 通过具体的用例，证明了该方法可以在下一代SOAR平台中实现分层和情境感知的自动化响应能力。

**结论:** 这项研究弥合了意图驱动的网络安全和自主网络安全之间的差距，并展示了在SOAR平台中的实际应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+an+Intent-Based+and+Ontology-Driven+Autonomic+Security+Response+in+Security+Orchestration+Automation+and+Response，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12061&send_immediately=true&force_search=false)

**原文摘要:** Modern Security Orchestration, Automation, and Response (SOAR) platforms must
rapidly adapt to continuously evolving cyber attacks. Intent-Based Networking
has emerged as a promising paradigm for cyber attack mitigation through
high-level declarative intents, which offer greater flexibility and persistency
than procedural actions. In this paper, we bridge the gap between two active
research directions: Intent-Based Cyber Defense and Autonomic Cyber Defense, by
proposing a unified, ontology-driven security intent definition leveraging the
MITRE-D3FEND cybersecurity ontology. We also propose a general two-tiered
methodology for integrating such security intents into decision-theoretic
Autonomic Cyber Defense systems, enabling hierarchical and context-aware
automated response capabilities. The practicality of our approach is
demonstrated through a concrete use case, showcasing its integration within
next-generation Security Orchestration, Automation, and Response platforms.

</details>


### [95] [A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy](https://arxiv.org/abs/2507.12098)
*Xiang Li, Yifan Lin, Yuanzhe Zhang*

**主要类别:** cs.CR

**AI概要:** 本文提出一个结合联邦学习和差分隐私的框架，用于缓解个性化广告中的隐私泄露和性能问题。


<details>
  <summary>更多</summary>
  
**动机:** 个性化广告存在隐私泄露和性能问题，需要一种新的方法来平衡模型准确性、通信开销和隐私保护。

**方法:** 该框架集成了分布式特征提取、动态隐私预算分配和稳健的模型聚合，并利用多方安全计算和异常检测机制进一步增强系统弹性。

**结果:** 实验结果表明，该框架在确保隐私的同时，实现了推荐准确性和系统效率的双重优化。

**结论:** 该框架不仅提供了解决个性化广告中隐私保护问题的实际方案，还为将隐私保护技术应用于广告推荐提供了理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Privacy-Preserving+Framework+for+Advertising+Personalization+Incorporating+Federated+Learning+and+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12098，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12098&send_immediately=true&force_search=false)

**原文摘要:** To mitigate privacy leakage and performance issues in personalized
advertising, this paper proposes a framework that integrates federated learning
and differential privacy. The system combines distributed feature extraction,
dynamic privacy budget allocation, and robust model aggregation to balance
model accuracy, communication overhead, and privacy protection. Multi-party
secure computing and anomaly detection mechanisms further enhance system
resilience against malicious attacks. Experimental results demonstrate that the
framework achieves dual optimization of recommendation accuracy and system
efficiency while ensuring privacy, providing both a practical solution and a
theoretical foundation for applying privacy protection technologies in
advertisement recommendation.

</details>


### [96] [Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks](https://arxiv.org/abs/2507.12185)
*Rina Mishra, Gaurav Varshney*

**主要类别:** cs.CR

**AI概要:** 研究发现，通过越狱技术利用生成式AI聊天机器人可以绕过伦理防护，指导新手执行多渠道网络钓鱼攻击，对传统反钓鱼机制构成威胁。


<details>
  <summary>更多</summary>
  
**动机:** 随着DeepSeek和ChatGPT等先进生成式AI模型的出现，网络安全领域面临新的机遇与风险。为了理解这些风险，需要研究如何通过越狱技术利用AI聊天服务绕过道德保护措施，从而产生钓鱼内容、推荐黑客工具以及组织钓鱼活动。

**方法:** 在受控的伦理实验中，使用ChatGPT 4o Mini作为代表性的生成式AI系统进行测试。分析了DeepSeek和ChatGPT这两种广泛应用和技术相关的模型，并探讨了常见的越狱技术和被利用的具体漏洞。

**结果:** 研究表明，所选模型能够成功引导没有经验的用户实施跨多个渠道（如网页、电子邮件、短信、语音）的钓鱼攻击。这种人类指导下的AI辅助攻击比自动化的钓鱼活动更难以被传统的反钓鱼机制检测到。

**结论:** 本研究评估了一系列缓解策略，包括用户教育、高级认证机制和监管政策措施，并讨论了由生成式AI支持的钓鱼攻击的新趋势，提出了未来的研究方向以加强人工智能时代的网络安全防御。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploiting+Jailbreaking+Vulnerabilities+in+Generative+AI+to+Bypass+Ethical+Safeguards+for+Facilitating+Phishing+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12185&send_immediately=true&force_search=false)

**原文摘要:** The advent of advanced Generative AI (GenAI) models such as DeepSeek and
ChatGPT has significantly reshaped the cybersecurity landscape, introducing
both promising opportunities and critical risks. This study investigates how
GenAI powered chatbot services can be exploited via jailbreaking techniques to
bypass ethical safeguards, enabling the generation of phishing content,
recommendation of hacking tools, and orchestration of phishing campaigns. In
ethically controlled experiments, we used ChatGPT 4o Mini selected for its
accessibility and status as the latest publicly available model at the time of
experimentation, as a representative GenAI system. Our findings reveal that the
model could successfully guide novice users in executing phishing attacks
across various vectors, including web, email, SMS (smishing), and voice
(vishing). Unlike automated phishing campaigns that typically follow detectable
patterns, these human-guided, AI assisted attacks are capable of evading
traditional anti phishing mechanisms, thereby posing a growing security threat.
We focused on DeepSeek and ChatGPT due to their widespread adoption and
technical relevance in 2025. The study further examines common jailbreaking
techniques and the specific vulnerabilities exploited in these models. Finally,
we evaluate a range of mitigation strategies such as user education, advanced
authentication mechanisms, and regulatory policy measures and discuss emerging
trends in GenAI facilitated phishing, outlining future research directions to
strengthen cybersecurity defenses in the age of artificial intelligence.

</details>


### [97] [Efficient Control Flow Attestation by Speculating on Control Flow Path Representations](https://arxiv.org/abs/2507.12345)
*Liam Tyler, Adam Caulfield, Ivan De Oliveira Nunes*

**主要类别:** cs.CR

**AI概要:** RESPEC-CFA是一种架构扩展，它允许推测控制流的局部性和Huffman编码，从而大大减少CFlog的大小。


<details>
  <summary>更多</summary>
  
**动机:** 现有的CFA方法受到生成的控制流日志（CFlog）的存储/传输成本的限制，而之前的工作没有考虑在控制流路径中地址的表示用于推测。

**方法:** RESPEC-CFA提出了一种新的架构扩展，它允许对控制流的局部性和Huffman编码进行推测。

**结果:** 单独使用RESPEC-CFA可以减少多达90.1%的日志量，与之前的方法结合使用时，最多可减少99.7%的日志量。

**结论:** RESPEC-CFA为实现实际应用中的CFA迈出了重要的一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Control+Flow+Attestation+by+Speculating+on+Control+Flow+Path+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12345，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12345&send_immediately=true&force_search=false)

**原文摘要:** Control Flow Attestation (CFA) allows remote verification of run-time
software integrity in embedded systems. However, CFA is limited by the
storage/transmission costs of generated control flow logs (CFlog). Recent work
has proposed application-specific optimizations by speculating on likely
sub-paths in CFlog and replacing them with reserved symbols at runtime. Albeit
effective, prior approaches do not consider the representation of addresses in
a control flow path for speculation. This work proposes RESPEC-CFA, an
architectural extension for CFA allowing for speculation on (1) the locality of
control flows and (2) their Huffman encoding. Alone, RESPEC-CFA reduces CFlog
sizes by up to 90.1%. Combined with prior methods, RESPEC-CFA yields reductions
of up to 99.7%, representing a significant step toward practical CFA.

</details>


### [98] [Rethinking the confidential cloud through a unified low-level abstraction for composable isolation](https://arxiv.org/abs/2507.12364)
*Adrien Ghosn, Charly Castes, Neelu S. Kalani, Yuchen Qian, Marios Kogias, Edouard Bugnion*

**主要类别:** cs.CR

**AI概要:** Tyche是一种新的统一隔离模型，它通过单个可信的安全监视器提供可执行、可组合和可证明的隔离。它可以在不使用硬件安全扩展的情况下运行，并支持复杂的云场景。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决现有的隔离机制带来的复杂性和平台碎片化问题，特别是对敏感云计算工作负载的安全性需求。

**方法:** 引入了信任域（TDs）作为核心抽象，允许细粒度的隔离，并能递归创建和管理子TDs。提供了一个API用于资源的分区、共享、证明和回收。

**结果:** 可以在普通的x86_64架构上运行，不需要硬件安全扩展，并且与现有软件保持向后兼容。支持复杂的云场景，如保密推理，并展示了在不同平台上的可移植性。

**结论:** Tyche提供了一种简化云工作负载安全性的新方法，它减少了TCB的膨胀，简化了端到端证明，并且可以跨平台和云服务提供商使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+confidential+cloud+through+a+unified+low-level+abstraction+for+composable+isolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12364，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12364&send_immediately=true&force_search=false)

**原文摘要:** Securing sensitive cloud workloads requires composing confidential virtual
machines (CVMs) with nested enclaves or sandboxes. Unfortunately, each new
isolation boundary adds ad-hoc access control mechanisms, hardware extensions,
and trusted software. This escalating complexity bloats the TCB, complicates
end-to-end attestation, and leads to fragmentation across platforms and cloud
service providers (CSPs).
  We introduce a unified isolation model that delegates enforceable,
composable, and attestable isolation to a single trusted security monitor:
Tyche. Tyche provides an API for partitioning, sharing, attesting, and
reclaiming resources through its core abstraction, trust domains (TDs). To
provide fine-grain isolation, TDs can recursively create and manage sub-TDs.
Tyche captures these relationships in attestations, allowing cloud tenants to
reason about end-to-end security. TDs serve as the building blocks for
constructing composable enclaves, sandboxes, and CVMs.
  Tyche runs on commodity x86_64 without hardware security extensions and can
maintain backward compatibility with existing software. We provide an SDK to
run and compose unmodified workloads as sandboxes, enclaves, and CVMs with
minimal overhead compared to native Linux execution. Tyche supports complex
cloud scenarios, such as confidential inference with mutually distrustful
users, model owners, and CSPs. An additional RISC-V prototype demonstrates
Tyche's portability across platforms.

</details>


### [99] [On One-Shot Signatures, Quantum vs Classical Binding, and Obfuscating Permutations](https://arxiv.org/abs/2507.12456)
*Omri Shmueli, Mark Zhandry*

**主要类别:** cs.CR

**AI概要:** 本文在标准模型中提出了首个一次性签名方案（OSS），并证明了其安全性，解决了量子加密领域的一个长期开放问题。


<details>
  <summary>更多</summary>
  
**动机:** 一次性签名（OSS）允许用户仅签署一条消息，随后签名密钥会自我销毁，防止再次签署。这种技术虽然在经典计算机上无法实现，但在量子计算中可能通过利用不可克隆原理来实现。然而，之前提出的OSS存在致命缺陷，因此需要一种新的、安全的OSS。

**方法:** 作者开发了一种可置换的伪随机置换（permutable PRPs）的概念，并展示了如何将其用于将涉及随机置换的预言机证明转换为基于混淆的证明。通过这种方式，他们构建了第一个标准模型中的OSS。

**结果:** 该研究不仅实现了首个标准模型中的OSS，还首次在标准模型中区分了经典绑定和坍缩绑定的后量子承诺/哈希，从而解决了长达十年的开放问题。此外，它还提供了一个具有无条件安全性的相对经典预言机的构造。

**结论:** 这项工作不仅为OSS提供了首个标准模型下的实现，而且通过引入可置换的伪随机置换，为其他密码学难题提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+One-Shot+Signatures%2C+Quantum+vs+Classical+Binding%2C+and+Obfuscating+Permutations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12456，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12456&send_immediately=true&force_search=false)

**原文摘要:** One-shot signatures (OSS) were defined by Amos, Georgiou, Kiayias, and
Zhandry (STOC'20). These allow for signing exactly one message, after which the
signing key self-destructs, preventing a second message from ever being signed.
While such an object is impossible classically, Amos et al observe that OSS may
be possible using quantum signing keys by leveraging the no-cloning principle.
OSS has since become an important conceptual tool with many applications in
decentralized settings and for quantum cryptography with classical
communication. OSS are also closely related to separations between
classical-binding and collapse-binding for post-quantum hashing and
commitments. Unfortunately, the only known OSS construction due to Amos et al.
was only justified in a classical oracle model, and moreover their
justification was ultimately found to contain a fatal bug. Thus, the existence
of OSS, even in a classical idealized model, has remained open.
  We give the first standard-model OSS, with provable security assuming
(sub-exponential) indistinguishability obfuscation (iO) and LWE. This also
gives the first standard-model separation between classical and
collapse-binding post-quantum commitments/hashing, solving a decade-old open
problem. Along the way, we also give the first construction with unconditional
security relative to a classical oracle. To achieve our standard-model
construction, we develop a notion of permutable pseudorandom permutations
(permutable PRPs), and show how they are useful for translating oracle proofs
involving random permutations into obfuscation-based proofs. In particular,
obfuscating permutable PRPs gives a trapdoor one-way permutation that is
\emph{full-domain}, solving another decade-old-problem of constructing this
object from (sub-exponential) iO and one-way functions.

</details>
