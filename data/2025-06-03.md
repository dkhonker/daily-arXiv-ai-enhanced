<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 141]
- [cs.AI](#cs.AI) [总数: 36]
- [stat.ML](#stat.ML) [总数: 15]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control](https://arxiv.org/abs/2505.23857)
*Wuhao Wang, Zhiyong Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的actor-critic算法DATD3，通过使用深度可分离卷积和多头注意力机制解决了输出反馈设置下的决策问题，并且在连续控制任务中取得了良好的效果。


<details>
  <summary>更多</summary>
  
**动机:** 解决实际应用中强化学习面临的输出反馈问题，即智能体只能获取部分状态信息。

**方法:** 引入了深度可分离卷积和多头注意力机制来编码历史观测，并提出了DATD3算法。

**结果:** 实验结果显示，DATD3在处理部分可观测环境中的任务时表现出色，同时避免了循环模型的不稳定性。

**结论:** DATD3在连续控制任务中表现优异，无论是在部分还是完全可观察环境下都优于现有的基于记忆和循环的基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DATD3%3A+Depthwise+Attention+Twin+Delayed+Deep+Deterministic+Policy+Gradient+For+Model+Free+Reinforcement+Learning+Under+Output+Feedback+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23857&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning in real-world applications often involves
output-feedback settings, where the agent receives only partial state
information. To address this challenge, we propose the Output-Feedback Markov
Decision Process (OPMDP), which extends the standard MDP formulation to
accommodate decision-making based on observation histories. Building on this
framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic
Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise
separable convolution and multi-head attention to encode historical
observations. DATD3 maintains policy expressiveness while avoiding the
instability of recurrent models. Extensive experiments on continuous control
tasks demonstrate that DATD3 outperforms existing memory-based and recurrent
baselines under both partial and full observability.

</details>


### [2] [Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration](https://arxiv.org/abs/2505.23859)
*Wenju Sun, Qingyong Li, Wen Wang, Yang Liu, Yangli-ao Geng, Boyang Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的多任务模型合并方法LOT Merging，通过最小化特征漂移来提升合并后模型的性能，在多种任务上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 观察到性能下降与特征漂移密切相关，即由模型合并引起的相同样本特征表示差异。

**方法:** 提出了一种名为Layer-wise Optimal Task Vector Merging (LOT Merging) 的技术，以凸二次优化问题的形式解决特征漂移问题，并推导出线性和归一化层参数的闭合解。

**结果:** 在多个视觉和视觉-语言基准测试中，LOT Merging比现有最先进方法提高了4.4%（ViT-B/32）。

**结论:** LOT Merging是一种有效的多任务模型合并技术，能够显著优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Minimizing+Feature+Drift+in+Model+Merging%3A+Layer-wise+Task+Vector+Fusion+for+Adaptive+Knowledge+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23859&send_immediately=true&force_search=false)

**原文摘要:** Multi-task model merging aims to consolidate knowledge from multiple
fine-tuned task-specific experts into a unified model while minimizing
performance degradation. Existing methods primarily approach this by minimizing
differences between task-specific experts and the unified model, either from a
parameter-level or a task-loss perspective. However, parameter-level methods
exhibit a significant performance gap compared to the upper bound, while
task-loss approaches entail costly secondary training procedures. In contrast,
we observe that performance degradation closely correlates with feature drift,
i.e., differences in feature representations of the same sample caused by model
merging. Motivated by this observation, we propose Layer-wise Optimal Task
Vector Merging (LOT Merging), a technique that explicitly minimizes feature
drift between task-specific experts and the unified model in a layer-by-layer
manner. LOT Merging can be formulated as a convex quadratic optimization
problem, enabling us to analytically derive closed-form solutions for the
parameters of linear and normalization layers. Consequently, LOT Merging
achieves efficient model consolidation through basic matrix operations.
Extensive experiments across vision and vision-language benchmarks demonstrate
that LOT Merging significantly outperforms baseline methods, achieving
improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.

</details>


### [3] [BiBLDR: Bidirectional Behavior Learning for Drug Repositioning](https://arxiv.org/abs/2505.23861)
*Renye Zhang, Mengyun Yang, Qichang Zhao, Jianxin Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的药物再定位方法BiBLDR，该方法通过双向行为学习策略解决了冷启动场景下的问题，并在多个基准数据集上展示了优越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于图的药物再定位方法在冷启动场景下表现不佳，因为缺乏新药与疾病之间的关联信息。因此需要提出一种新的方法来解决这一问题。

**方法:** BiBLDR利用双向行为学习策略，将药物再定位重新定义为行为序列学习任务，通过构建双向行为序列和两阶段策略来预测潜在的药物-疾病关联。

**结果:** 实验表明，BiBLDR在基准数据集上达到了最先进的性能，并且在冷启动场景下显著优于之前的方法。

**结论:** BiBLDR是一种创新的药物再定位框架，能够有效解决冷启动场景下的问题，并且在基准数据集上表现出卓越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BiBLDR%3A+Bidirectional+Behavior+Learning+for+Drug+Repositioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23861&send_immediately=true&force_search=false)

**原文摘要:** Drug repositioning aims to identify potential new indications for existing
drugs to reduce the time and financial costs associated with developing new
drugs. Most existing deep learning-based drug repositioning methods
predominantly utilize graph-based representations. However, graph-based drug
repositioning methods struggle to perform effective inference in cold-start
scenarios involving novel drugs because of the lack of association information
with the diseases. Unlike traditional graph-based approaches, we propose a
bidirectional behavior learning strategy for drug repositioning, known as
BiBLDR. This innovative framework redefines drug repositioning as a behavior
sequential learning task to capture drug-disease interaction patterns. First,
we construct bidirectional behavioral sequences based on drug and disease
sides. The consideration of bidirectional information ensures a more meticulous
and rigorous characterization of the behavioral sequences. Subsequently, we
propose a two-stage strategy for drug repositioning. In the first stage, we
construct prototype spaces to characterize the representational attributes of
drugs and diseases. In the second stage, these refined prototypes and
bidirectional behavior sequence data are leveraged to predict potential
drug-disease associations. Based on this learning approach, the model can more
robustly and precisely capture the interactive relationships between drug and
disease features from bidirectional behavioral sequences. Extensive experiments
demonstrate that our method achieves state-of-the-art performance on benchmark
datasets. Meanwhile, BiBLDR demonstrates significantly superior performance
compared to previous methods in cold-start scenarios. Our code is published in
https://github.com/Renyeeah/BiBLDR.

</details>


### [4] [Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting](https://arxiv.org/abs/2505.23863)
*Chang Liu, Bohao Zhao, Jingtao Ding, Huandong Wang, Yong Li*

**主要类别:** cs.LG

**AI概要:** PhyxMamba是一种结合物理信息和深度学习的新方法，用于解决从短期观测数据中长期预测混沌系统的难题。


<details>
  <summary>更多</summary>
  
**动机:** 长期预测混沌系统是一个重要且未充分研究的挑战，由于对初始条件的内在敏感性和复杂几何结构，现有方法难以在扩展的时间范围内保持预测稳定性和动态一致性。

**方法:** PhyxMamba结合了基于Mamba的状态空间模型和物理信息原则，利用时间延迟嵌入重建吸引子流形，并通过生成式训练方案进行训练，同时采用多标记预测和吸引子几何正则化来增强预测准确性。

**结果:** PhyxMamba在多种模拟和现实世界的混沌系统中表现出色，能够从短期数据中准确预测长期趋势并捕捉关键的动态不变量。

**结论:** PhyxMamba提供了一种新的方法，在数据有限的情况下可靠地预测混沌系统的行为，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mamba+Integrated+with+Physics+Principles+Masters+Long-term+Chaotic+System+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23863，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23863&send_immediately=true&force_search=false)

**原文摘要:** Long-term forecasting of chaotic systems from short-term observations remains
a fundamental and underexplored challenge due to the intrinsic sensitivity to
initial conditions and the complex geometry of strange attractors. Existing
approaches often rely on long-term training data or focus on short-term
sequence correlations, struggling to maintain predictive stability and
dynamical coherence over extended horizons. We propose PhyxMamba, a novel
framework that integrates a Mamba-based state-space model with physics-informed
principles to capture the underlying dynamics of chaotic systems. By
reconstructing the attractor manifold from brief observations using time-delay
embeddings, PhyxMamba extracts global dynamical features essential for accurate
forecasting. Our generative training scheme enables Mamba to replicate the
physical process, augmented by multi-token prediction and attractor geometry
regularization for physical constraints, enhancing prediction accuracy and
preserving key statistical invariants. Extensive evaluations on diverse
simulated and real-world chaotic systems demonstrate that PhyxMamba delivers
superior long-term forecasting and faithfully captures essential dynamical
invariants from short-term data. This framework opens new avenues for reliably
predicting chaotic systems under observation-scarce conditions, with broad
implications across climate science, neuroscience, epidemiology, and beyond.
Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.

</details>


### [5] [Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections](https://arxiv.org/abs/2505.23864)
*Wei Zhuo, Zhaohuan Zhan, Ziduo Yang, Han Yu*

**主要类别:** cs.LG

**AI概要:** FedAux是一种个性化的联邦学习框架，用于图结构数据，在不共享原始数据或节点嵌入的情况下实现异构本地模型的对齐、比较和聚合。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习在图结构数据上通常面临非独立同分布的挑战，尤其是在每个客户端持有从全局图中采样的独特子图时。

**方法:** FedAux通过辅助投影（APV）将节点嵌入投影到一维空间，并使用软排序操作和轻量级的一维卷积来优化这些嵌入，利用APV作为紧凑签名计算客户端间相似性并进行加权参数混合。

**结果:** FedAux在多样化的图基准测试中显著优于现有基线方法，同时具备理论收敛性和合理性分析。

**结论:** FedAux提供了一种有效的个性化联邦学习解决方案，能够在保护隐私的同时实现跨客户端知识迁移和高性能模型定制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Subgraph+Federated+Learning+with+Differentiable+Auxiliary+Projections，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23864，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23864&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) on graph-structured data typically faces non-IID
challenges, particularly in scenarios where each client holds a distinct
subgraph sampled from a global graph. In this paper, we introduce Federated
learning with Auxiliary projections (FedAux), a personalized subgraph FL
framework that learns to align, compare, and aggregate heterogeneously
distributed local models without sharing raw data or node embeddings. In
FedAux, each client jointly trains (i) a local GNN and (ii) a learnable
auxiliary projection vector (APV) that differentiably projects node embeddings
onto a 1D space. A soft-sorting operation followed by a lightweight 1D
convolution refines these embeddings in the ordered space, enabling the APV to
effectively capture client-specific information. After local training, these
APVs serve as compact signatures that the server uses to compute inter-client
similarities and perform similarity-weighted parameter mixing, yielding
personalized models while preserving cross-client knowledge transfer. Moreover,
we provide rigorous theoretical analysis to establish the convergence and
rationality of our design. Empirical evaluations across diverse graph
benchmarks demonstrate that FedAux substantially outperforms existing baselines
in both accuracy and personalization performance.

</details>


### [6] [Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration](https://arxiv.org/abs/2505.23865)
*Emanuele Masiero, Vito Trianni, Giuseppe Vizzari, Dimitri Ognibene*

**主要类别:** cs.LG

**AI概要:** 本文研究了精准农业中的自主探索问题，提出了一个两阶段深度学习框架，并比较了几种代理架构的性能，结果表明结合信息熵、信念状态和可见性跟踪的方法在探索效率上有显著优势。


<details>
  <summary>更多</summary>
  
**动机:** 精准农业需要高效的自主系统进行作物监测，其中代理必须在最小化资源消耗的同时探索大规模环境。

**方法:** 提出了一种两阶段深度学习框架，比较了三种智能体架构：未训练的信息增益代理、使用CNN的DQN代理以及具有更广泛空间上下文的Double-CNN DQN代理。

**结果:** 结果显示，在20x20地图上的模拟表明，未训练的代理尽管简单但表现出色；当包含POV掩码时，DQN代理能够达到相似性能，而Double-CNN代理在探索效率上表现更优，尤其是在更大的环境中。

**结论:** 论文得出不确定性感知策略在自主探索任务中表现良好，未来工作包括课程学习、多智能体合作、基于变压器的模型和内在动机机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Combining+Deep+Architectures+for+Information+Gain+estimation+and+Reinforcement+Learning+for+multiagent+field+exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23865，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23865&send_immediately=true&force_search=false)

**原文摘要:** Precision agriculture requires efficient autonomous systems for crop
monitoring, where agents must explore large-scale environments while minimizing
resource consumption. This work addresses the problem as an active exploration
task in a grid environment representing an agricultural field. Each cell may
contain targets (e.g., damaged crops) observable from nine predefined points of
view (POVs). Agents must infer the number of targets per cell using partial,
sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as
a belief model, updating a probabilistic map of the environment and its
associated entropy, which defines the expected information gain (IG). This
allows agents to prioritize informative regions. A key contribution is the
inclusion of a POV visibility mask in the input, preserving the Markov property
under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent
selecting actions to maximize entropy reduction; a DQN agent using CNNs over
local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent
with wider spatial context. Simulations on 20x20 maps showed that the untrained
agent performs well despite its simplicity. The DQN agent matches this
performance when the POV mask is included, while the Double-CNN agent
consistently achieves superior exploration efficiency, especially in larger
environments.
  Results show that uncertainty-aware policies leveraging entropy, belief
states, and visibility tracking lead to robust and scalable exploration. Future
work includes curriculum learning, multi-agent cooperation with shared rewards,
transformer-based models, and intrinsic motivation mechanisms to further
enhance learning efficiency and policy generalization.

</details>


### [7] [Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization](https://arxiv.org/abs/2505.23866)
*Chengli Tan, Yubo Zhou, Haishan Ye, Guang Dai, Junmin Liu, Zengjie Song, Jiangshe Zhang, Zixiang Zhao, Yunda Hao, Yong Xu*

**主要类别:** cs.LG

**AI概要:** 该论文介绍了CSAM，一种改进的SAM方法，在减少模型校准误差方面表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络在安全关键应用中容易出现校准不良和过度自信的问题。

**方法:** 提出了一种新的SAM变体CSAM，并进行了广泛的实验验证。

**结果:** 实验证明，SAM可以降低校准误差，而CSAM表现更优。

**结论:** CSAM在减少校准误差方面优于SAM和其他方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Understanding+The+Calibration+Benefits+of+Sharpness-Aware+Minimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23866&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks have been increasingly used in safety-critical
applications such as medical diagnosis and autonomous driving. However, many
studies suggest that they are prone to being poorly calibrated and have a
propensity for overconfidence, which may have disastrous consequences. In this
paper, unlike standard training such as stochastic gradient descent, we show
that the recently proposed sharpness-aware minimization (SAM) counteracts this
tendency towards overconfidence. The theoretical analysis suggests that SAM
allows us to learn models that are already well-calibrated by implicitly
maximizing the entropy of the predictive distribution. Inspired by this
finding, we further propose a variant of SAM, coined as CSAM, to ameliorate
model calibration. Extensive experiments on various datasets, including
ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.
Meanwhile, CSAM performs even better than SAM and consistently achieves lower
calibration error than other approaches

</details>


### [8] [Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert](https://arxiv.org/abs/2505.23868)
*Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou. Libo Qin, Wenhong Tian*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的噪声鲁棒适应方法LoPE，它通过对模型内部结构进行特殊设计，使其能够在存在噪声数据的情况下依然表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 当前的参数高效微调方法容易受到噪声数据的干扰，而传统的噪声处理方法要么依赖于繁琐的数据预处理，要么采用容易导致误差累积的模型架构修改。

**方法:** 提出了一种名为LoPE的新框架，该框架在不对称LoRA配置中战略性地集成了专门的中毒专家，并通过两阶段范式增强其噪声辨别和处理能力。

**结果:** 实验表明，LoPE能够有效提高模型对噪声的鲁棒性，并且在不需要数据清洗的情况下也能保持良好的性能。

**结论:** LoPE通过低成本的噪声注入实现了强大的性能和鲁棒性，完全消除了数据清洗的需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Noise-Robustness+Through+Noise%3A+Asymmetric+LoRA+Adaption+with+Poisoning+Expert，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23868，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23868&send_immediately=true&force_search=false)

**原文摘要:** Current parameter-efficient fine-tuning methods for adapting pre-trained
language models to downstream tasks are susceptible to interference from noisy
data. Conventional noise-handling approaches either rely on laborious data
pre-processing or employ model architecture modifications prone to error
accumulation. In contrast to existing noise-process paradigms, we propose a
noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a
novel framework that enhances model robustness to noise only with generated
noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE
strategically integrates a dedicated poisoning expert in an asymmetric LoRA
configuration. Through a two-stage paradigm, LoPE performs noise injection on
the poisoning expert during fine-tuning to enhance its noise discrimination and
processing ability. During inference, we selectively mask the dedicated
poisoning expert to leverage purified knowledge acquired by normal experts for
noise-robust output. Extensive experiments demonstrate that LoPE achieves
strong performance and robustness purely through the low-cost noise injection,
which completely eliminates the requirement of data cleaning.

</details>


### [9] [MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection](https://arxiv.org/abs/2505.23870)
*Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania*

**主要类别:** cs.LG

**AI概要:** MaCP is a highly efficient method for fine-tuning large AI models, achieving high performance with minimal parameters and memory usage.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind MaCP is to develop a minimal parameter and memory-intensive method for adapting large foundation models efficiently while maintaining high performance.

**方法:** MaCP uses cosine projection to exploit energy compaction and decorrelation properties. It projects weight changes into the discrete cosine space, partitions these changes across different levels of the spectrum, and selects the most critical frequency components.

**结果:** Experiments show that MaCP performs effectively across various tasks, including natural language understanding, generation, summarization, image classification, and video understanding.

**结论:** MaCP is an efficient adaptation method for large foundation models, offering superior accuracy with reduced computational complexity and memory requirements compared to existing methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MaCP%3A+Minimal+yet+Mighty+Adaptation+via+Hierarchical+Cosine+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23870，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23870&send_immediately=true&force_search=false)

**原文摘要:** We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine
Projection, that achieves exceptional performance while requiring minimal
parameters and memory for fine-tuning large foundation models. Its general idea
is to exploit the superior energy compaction and decorrelation properties of
cosine projection to improve both model efficiency and accuracy. Specifically,
it projects the weight change from the low-rank adaptation into the discrete
cosine space. Then, the weight change is partitioned over different levels of
the discrete cosine spectrum, and each partition's most critical frequency
components are selected. Extensive experiments demonstrate the effectiveness of
MaCP across a wide range of single-modality tasks, including natural language
understanding, natural language generation, text summarization, as well as
multi-modality tasks such as image classification and video understanding. MaCP
consistently delivers superior accuracy, significantly reduced computational
complexity, and lower memory requirements compared to existing alternatives.

</details>


### [10] [ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning](https://arxiv.org/abs/2505.23871)
*Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, Xiu Li*

**主要类别:** cs.LG

**AI概要:** 本论文介绍了一种名为Ambient Diffusion-Guided Dataset Recovery (ADG)的新方法，用于解决离线强化学习中数据集损坏的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的数据集容易受到噪声和错误的影响，这对应用离线强化学习提出了重大挑战，尤其是当现有方法无法充分处理高维状态空间的损坏或多元素损坏时。

**方法:** 提出了一种名为Ambient Diffusion-Guided Dataset Recovery (ADG)的新方法，利用扩散模型来处理高维状态空间和多元素数据损坏的问题。

**结果:** 实验表明，ADG可以有效减轻数据损坏的影响，提高离线RL的鲁棒性，在多个基准测试中实现了最先进的结果。

**结论:** ADG方法能够有效减轻离线强化学习中数据集损坏的影响，提高了在不同噪声设置下的鲁棒性，并实现了最先进的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADG%3A+Ambient+Diffusion-Guided+Dataset+Recovery+for+Corruption-Robust+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23871&send_immediately=true&force_search=false)

**原文摘要:** Real-world datasets collected from sensors or human inputs are prone to noise
and errors, posing significant challenges for applying offline reinforcement
learning (RL). While existing methods have made progress in addressing
corrupted actions and rewards, they remain insufficient for handling corruption
in high-dimensional state spaces and for cases where multiple elements in the
dataset are corrupted simultaneously. Diffusion models, known for their strong
denoising capabilities, offer a promising direction for this problem-but their
tendency to overfit noisy samples limits their direct applicability. To
overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a
novel approach that pioneers the use of diffusion models to tackle data
corruption in offline RL. First, we introduce Ambient Denoising Diffusion
Probabilistic Models (DDPM) from approximated distributions, which enable
learning on partially corrupted datasets with theoretical guarantees. Second,
we use the noise-prediction property of Ambient DDPM to distinguish between
clean and corrupted data, and then use the clean subset to train a standard
DDPM. Third, we employ the trained standard DDPM to refine the previously
identified corrupted data, enhancing data quality for subsequent offline RL
training. A notable strength of ADG is its versatility-it can be seamlessly
integrated with any offline RL algorithm. Experiments on a range of benchmarks,
including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively
mitigates the impact of corrupted data and improves the robustness of offline
RL under various noise settings, achieving state-of-the-art results.

</details>


### [11] [A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants](https://arxiv.org/abs/2505.23875)
*Peter Samoaa, Marcus Vukojevic, Morteza Haghir Chehreghani, Antonio Longa*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的图回归数据集RelSC，用于推动图回归方法的研究。


<details>
  <summary>更多</summary>
  
**动机:** 当前的公共基准测试主要偏向于分子图和引用网络，缺乏多样性，阻碍了能够泛化到同质和异质图结构模型的发展。

**方法:** 开发了两种不同形式的RelSC数据集：RelSC-H和RelSC-M，分别提供了单一类型边和多关系结构的图表示。

**结果:** 评估结果显示，在同质和多关系设置之间存在一致的性能差异，强调了结构表示的重要性。

**结论:** RelSC的引入为图回归方法的进步提供了一个具有挑战性和多功能的基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Benchmark+Dataset+for+Graph+Regression+with+Homogeneous+and+Multi-Relational+Variants，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23875，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23875&send_immediately=true&force_search=false)

**原文摘要:** Graph-level regression underpins many real-world applications, yet public
benchmarks remain heavily skewed toward molecular graphs and citation networks.
This limited diversity hinders progress on models that must generalize across
both homogeneous and heterogeneous graph structures. We introduce RelSC, a new
graph-regression dataset built from program graphs that combine syntactic and
semantic information extracted from source code. Each graph is labelled with
the execution-time cost of the corresponding program, providing a continuous
target variable that differs markedly from those found in existing benchmarks.
RelSC is released in two complementary variants. RelSC-H supplies rich node
features under a single (homogeneous) edge type, while RelSC-M preserves the
original multi-relational structure, connecting nodes through multiple edge
types that encode distinct semantic relationships. Together, these variants let
researchers probe how representation choice influences model behaviour. We
evaluate a diverse set of graph neural network architectures on both variants
of RelSC. The results reveal consistent performance differences between the
homogeneous and multi-relational settings, emphasising the importance of
structural representation. These findings demonstrate RelSC's value as a
challenging and versatile benchmark for advancing graph regression methods.

</details>


### [12] [A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size](https://arxiv.org/abs/2505.23876)
*Polad Geidarov*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了使用分析公式预计算权重的多层感知机在MNIST数据集上的性能，发现其训练更快且对数据减少更具鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是探索多层感知机神经网络实现度量识别方法的能力，以提高训练效率和减少对大规模训练数据的依赖。

**方法:** 论文通过使用分析公式预先计算权重值，并在不同大小的MNIST训练数据集上进行比较实验，比较了具有预计算权重和随机初始化权重的神经网络的性能。

**结果:** 实验结果表明，与随机初始化权重相比，使用预计算权重的多层感知机可以更快地训练并且对训练数据集的缩小更鲁棒。

**结论:** 论文得出结论，具有预计算权重的多层感知机在训练速度和对训练数据集缩减的鲁棒性方面优于随机初始化权重的神经网络。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+comparative+analysis+of+a+neural+network+with+calculated+weights+and+a+neural+network+with+random+generation+of+weights+based+on+the+training+dataset+size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23876，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23876&send_immediately=true&force_search=false)

**原文摘要:** The paper discusses the capabilities of multilayer perceptron neural networks
implementing metric recognition methods, for which the values of the weights
are calculated analytically by formulas. Comparative experiments in training a
neural network with pre-calculated weights and with random initialization of
weights on different sizes of the MNIST training dataset are carried out. The
results of the experiments show that a multilayer perceptron with
pre-calculated weights can be trained much faster and is much more robust to
the reduction of the training dataset.

</details>


### [13] [Actor-Critic based Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2505.23878)
*Jing Ma, Chenhao Dang, Mingjie Liao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的在线数据混合方法AC-ODM，通过引入基于actor-critic的学习机制，显著提升了大型语言模型预训练的效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据混合方法未能随着训练动态演化，而现有的在线数据混合（ODM）方法虽然解决了这一限制，但未考虑领域内的交互。本文旨在开发一种更高效且能反映训练动态的方法。

**方法:** 开发了一种基于actor-critic的在线数据混合（AC-ODM）方法，通过辅助的actor-critic网络捕捉变化的领域权重，并在奖励函数中考虑领域内的交互。利用小型代理LLM训练出的actor作为采样策略，用于构建预训练大型目标LLM的数据集。

**结果:** 实验结果表明，AC-ODM-410M使用具有410M参数的代理LLM获得的采样策略，使验证困惑度达到最优的速度比ODM快71%，并在零样本MMLU基准测试中准确率提高了27.5%，在HumanEval基准测试的pass@1指标上表现约为现有方法的2.23倍。

**结论:** AC-ODM方法能够有效提升大型语言模型预训练过程中的动态数据混合效率，同时加快目标模型的收敛速度，并在验证困惑度和基准测试性能方面显著优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Actor-Critic+based+Online+Data+Mixing+For+Language+Model+Pre-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23878&send_immediately=true&force_search=false)

**原文摘要:** The coverage and composition of pretraining data significantly impacts the
generalization ability of Large Language Models (LLMs). To reduce the carbon
footprint and financial costs of training, some data mixing methods, which
applied the optimized domain weights of a small proxy model to train a larger
one, were proposed. However, these methods did not evolute with the training
dynamics. The existing online data mixing (ODM) method addressed this
limitation by applying the multi-armed bandit algorithm as data sampling
strategy. Yet, it did not consider the intra-domain interactions. In this
paper, we develop an actor-critic based online data mixing (AC-ODM) method,
which captures the varying domain weights by auxiliary actor-critic networks
and consider the intra-domain interactions with the reward function. While
constructing the dataset to pretrain a large target LLM, we directly apply the
actor, which is trained with a small proxy LLM as the environment, as the
sampling strategy. The transfer of sampling strategy can not only ensure the
efficiency of dynamical data mixing, but also expedite the convergence of
pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,
which invokes the sampling strategy obtained by a proxy LLM with 410M
parameters, reaching the optimal validation perplexity of ODM 71% faster, and
improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,
about 2.23x better on pass@1 of HumanEval benchmark.

</details>


### [14] [CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data](https://arxiv.org/abs/2505.23879)
*Caio Cheohen, Vinnícius M. S. Gomes, Manuela L. da Silva*

**主要类别:** cs.LG

**AI概要:** 本研究开发了一种基于深度学习的混合模型，通过分析SARS-CoV-2病毒的刺突蛋白序列和临床数据来有效预测COVID-19的严重程度。


<details>
  <summary>更多</summary>
  
**动机:** 为了优化医疗资源分配和患者管理，准确预测疾病的严重程度至关重要。

**方法:** 研究人员使用了一个混合 CNN-LSTM 深度学习架构，结合卷积神经网络（CNN）层进行局部模式提取和长短期记忆网络（LSTM）层对长期依赖关系进行建模。

**结果:** 该模型达到了82.92%的F1分数、0.9084的ROC-AUC值、83.56%的精确率和82.85%的召回率，并且训练稳定在85%的准确率，过拟合最小。

**结论:** CNN-LSTM混合模型有效地利用刺突蛋白序列和临床数据预测了 COVID-19 的严重程度，突出了人工智能在基因组监测和精准公共卫生中的效用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CNN-LSTM+Hybrid+Model+for+AI-Driven+Prediction+of+COVID-19+Severity+from+Spike+Sequences+and+Clinical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23879，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23879&send_immediately=true&force_search=false)

**原文摘要:** The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need
for accurate prediction of disease severity to optimize healthcare resource
allocation and patient management. The spike protein, which facilitates viral
entry into host cells, exhibits high mutation rates, particularly in the
receptor-binding domain, influencing viral pathogenicity. Artificial
intelligence approaches, such as deep learning, offer promising solutions for
leveraging genomic and clinical data to predict disease outcomes. Objective:
This study aimed to develop a hybrid CNN-LSTM deep learning model to predict
COVID-19 severity using spike protein sequences and associated clinical
metadata from South American patients. Methods: We retrieved 9,570 spike
protein sequences from the GISAID database, of which 3,467 met inclusion
criteria after standardization. The dataset included 2,313 severe and 1,154
mild cases. A feature engineering pipeline extracted features from sequences,
while demographic and clinical variables were one-hot encoded. A hybrid
CNN-LSTM architecture was trained, combining CNN layers for local pattern
extraction and an LSTM layer for long-term dependency modeling. Results: The
model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,
and recall of 82.85%, demonstrating robust classification performance. Training
stabilized at 85% accuracy with minimal overfitting. The most prevalent
lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional
epidemiological trends, suggesting potential associations between viral
genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model
effectively predicted COVID-19 severity using spike protein sequences and
clinical data, highlighting the utility of AI in genomic surveillance and
precision public health. Despite limitations, this approach provides a
framework for early severity prediction in future outbreaks.

</details>


### [15] [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
*Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, Hao Tan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种高效的测试时训练方法LaCT，通过大规模块更新显著提升模型对长上下文的处理能力，并在多种任务中展示了其卓越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的TTT方法在处理长上下文数据时存在效率问题，主要体现在GPU利用率低和受限的状态容量上。因此，需要一种更高效、更具扩展性的解决方案。

**方法:** 采用大规模块更新（Large Chunk Update），将更新块大小从2K到1M tokens不等，并结合高效优化器如Muon进行在线更新。

**结果:** LaCT显著提升了硬件利用效率和状态容量（最高达模型参数的40%），成功应用于多种模态任务（语言模型、视频扩散模型等），并支持长达1百万tokens的上下文长度实验。

**结论:** 论文提出了一种改进的测试时训练方法LaCT，该方法通过使用大规模块更新显著提高了硬件利用率和模型状态容量，同时简化了复杂优化器的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-Time+Training+Done+Right，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23884，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23884&send_immediately=true&force_search=false)

**原文摘要:** Test-Time Training (TTT) models context dependencies by adapting part of the
model's weights (referred to as fast weights) during inference. This fast
weight, akin to recurrent states in RNNs, stores temporary memories of past
tokens in the current sequence. Existing TTT methods struggled to show
effectiveness in handling long-context data, due to their inefficiency on
modern GPUs. The TTT layers in many of these approaches operate with extremely
low FLOPs utilization (often <5%) because they deliberately apply small online
minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,
a small minibatch implies fine-grained block-wise causal dependencies in the
data, unsuitable for data beyond 1D ordered sequences, like sets or
N-dimensional grids such as images or videos. In contrast, we pursue the
opposite direction by using an extremely large chunk update, ranging from 2K to
1M tokens across tasks of varying modalities, which we refer to as Large Chunk
Test-Time Training (LaCT). It improves hardware utilization by orders of
magnitude, and more importantly, facilitates scaling of nonlinear state size
(up to 40% of model parameters), hence substantially improving state capacity,
all without requiring cumbersome and error-prone kernel implementations. It
also allows easy integration of sophisticated optimizers, e.g. Muon for online
updates. We validate our approach across diverse modalities and tasks,
including novel view synthesis with image set, language models, and
auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR
video diffusion model on sequences up to 56K tokens. In our longest sequence
experiment, we perform novel view synthesis with 1 million context length. We
hope this work will inspire and accelerate new research in the field of
long-context modeling and test-time training. Website:
https://tianyuanzhang.com/projects/ttt-done-right

</details>


### [16] [Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling](https://arxiv.org/abs/2505.23913)
*Gustavo Sutter Pessurno de Carvalho, Mohammed Abdulrahman, Hao Wang, Sriram Ganapathi Subramanian, Marc St-Aubin, Sharon O'Sullivan, Lawrence Wan, Luis Ricardez-Sandoval, Pascal Poupart, Agustinus Kristiadi*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种全新的贝叶斯优化方法，利用预训练深度生成模型进行零样本优化，大幅提高了效率，减少了传统方法所需的复杂调优过程。


<details>
  <summary>更多</summary>
  
**动机:** 传统的贝叶斯优化方法需要昂贵的再训练和采集函数优化步骤，而现有方法几乎都需要采集函数最大化来选择下一个观测点，引入了许多需要调整的参数。因此，提出一种更高效且无需调参的方法是必要的。

**方法:** 采用预训练深度生成模型直接从最优解的后验分布中采样，这一过程等价于汤普森抽样。

**结果:** 在与基于高斯过程的贝叶斯优化方法相比时，该方法实现了超过35倍的效率提升，并展示了其在实际世界基准测试中的能力和成本效益。

**结论:** 该研究提出了一种完全上下文内、零样本的贝叶斯优化解决方案，避免了代理拟合或采集函数优化的需求。通过使用预训练深度生成模型直接从最优解后验分布中采样，实现了超过35倍的效率提升，并支持高效的并行和分布式贝叶斯优化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Simplifying+Bayesian+Optimization+Via+In-Context+Direct+Optimum+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23913，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23913&send_immediately=true&force_search=false)

**原文摘要:** The optimization of expensive black-box functions is ubiquitous in science
and engineering. A common solution to this problem is Bayesian optimization
(BO), which is generally comprised of two components: (i) a surrogate model and
(ii) an acquisition function, which generally require expensive re-training and
optimization steps at each iteration, respectively. Although recent work
enabled in-context surrogate models that do not require re-training, virtually
all existing BO methods still require acquisition function maximization to
select the next observation, which introduces many knobs to tune, such as Monte
Carlo samplers and multi-start optimizers. In this work, we propose a
completely in-context, zero-shot solution for BO that does not require
surrogate fitting or acquisition function optimization. This is done by using a
pre-trained deep generative model to directly sample from the posterior over
the optimum point. We show that this process is equivalent to Thompson sampling
and demonstrate the capabilities and cost-effectiveness of our foundation model
on a suite of real-world benchmarks. We achieve an efficiency gain of more than
35x in terms of wall-clock time when compared with Gaussian process-based BO,
enabling efficient parallel and distributed BO, e.g., for high-throughput
optimization.

</details>


### [17] [Thompson Sampling in Online RLHF with General Function Approximation](https://arxiv.org/abs/2505.23927)
*Songtao Feng, Jie Fu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的在线RLHF算法，并在理论上证明了其性能界限。


<details>
  <summary>更多</summary>
  
**动机:** 研究从理论上分析强化学习与人类反馈(RLHF)算法的统计效率的重要性。

**方法:** 采用Bellman eluder (BE) 维度作为函数类复杂度的度量，并建立基于最大似然估计(MLE)泛化界的平方Bellman误差界的集中型不等式。

**结果:** 设计了一种用于在线RLHF的无模型后验抽样算法，并首次建立了基于MLE泛化界的集中型不等式，从而获得eluder-type遗憾界。

**结论:** 该研究通过设计一种受Thompson抽样启发的无模型后验抽样算法，为在线RLHF提供了理论保证，并建立了基于Bellman eluder维度的O(√T)遗憾界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thompson+Sampling+in+Online+RLHF+with+General+Function+Approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23927&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning from human feedback (RLHF) has achieved great
empirical success in aligning large language models (LLMs) with human
preference, and it is of great importance to study the statistical efficiency
of RLHF algorithms from a theoretical perspective. In this work, we consider
the online RLHF setting where the preference data is revealed during the
learning process and study action value function approximation. We design a
model-free posterior sampling algorithm for online RLHF inspired by Thompson
sampling and provide its theoretical guarantee. Specifically, we adopt Bellman
eluder (BE) dimension as the complexity measure of the function class and
establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other
multiplicative factor depending on the horizon, BE dimension and the
$log$-bracketing number of the function class. Further, in the analysis, we
first establish the concentration-type inequality of the squared Bellman error
bound based on the maximum likelihood estimator (MLE) generalization bound,
which plays the crucial rules in obtaining the eluder-type regret bound and may
be of independent interest.

</details>


### [18] [BIRD: Behavior Induction via Representation-structure Distillation](https://arxiv.org/abs/2505.23933)
*Galen Pogoncheff, Michael Beyeler*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为BIRD的新框架，它通过匹配模型的内部表示结构来有效地转移与人类价值观一致的行为，从而提高模型在不同任务上的表现力。


<details>
  <summary>更多</summary>
  
**动机:** 将与人类价值观一致的行为转移到在不同任务或数据分布上训练的模型中仍然具有挑战性，因为这些行为容易被遗忘，并且收集保留这些行为的任务特定数据可能成本高昂。

**方法:** 通过匹配学生模型和教师模型的内部表示结构，引入了BIRD（Behavior Induction via Representation-structure Distillation）框架。

**结果:** 在图像分类的分布外鲁棒性应用中，BIRD的表现优于微调、迁移学习和持续学习方法，鲁棒准确性提高了最多16%。

**结论:** BIRD提供了一种有效的框架，用于转移对齐行为，并且即使教师模型比学生模型小得多，也能够实现高效的对齐行为转移。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BIRD%3A+Behavior+Induction+via+Representation-structure+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23933，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23933&send_immediately=true&force_search=false)

**原文摘要:** Human-aligned deep learning models exhibit behaviors consistent with human
values, such as robustness, fairness, and honesty. Transferring these
behavioral properties to models trained on different tasks or data
distributions remains challenging: aligned behavior is easily forgotten during
fine-tuning, and collecting task-specific data that preserves this behavior can
be prohibitively costly. We introduce BIRD (Behavior Induction via
Representation-structure Distillation), a flexible framework for transferring
aligned behavior by matching the internal representation structure of a student
model to that of a teacher. Applied to out-of-distribution robustness in image
classification, BIRD outperforms fine-tuning, transfer learning, and continual
learning methods, improving robust accuracy by up to 16% over the next
strongest baseline. It remains effective even when the teacher is trained on a
much simpler dataset and is $25 \times$ smaller than the student. In a
large-scale study of over 400 teacher-student pairs, we show that three
interpretable and computable properties of the teacher's representations (i.e.,
task relevance, behavioral relevance, and complementary knowledge) explain up
to 85% of the variance in transfer success. These insights offer practical
guidance for teacher selection and design. BIRD turns small, well-aligned
models into scalable alignment seeds, removing a key bottleneck in deploying
safe AI systems in the wild.

</details>


### [19] [Searching Neural Architectures for Sensor Nodes on IoT Gateways](https://arxiv.org/abs/2505.23939)
*Andrea Mattia Garavagno, Edoardo Ragusa, Antonio Frisoli, Paolo Gastaldo*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种在边缘设备上自动设计神经网络的方法，能够在不泄露数据的情况下实现高效机器学习，适用于医疗和工业物联网场景。


<details>
  <summary>更多</summary>
  
**动机:** 动机是为了保护隐私敏感数据，并在资源受限的边缘设备上实现机器学习能力。

**方法:** 提出了一种在IoT网关上运行的自动神经网络设计方法，不需将数据分享到本地网络之外。

**结果:** 实验结果显示，该方法在Raspberry Pi Zero 2上不到10小时的搜索时间内，能够在Visual Wake Words数据集上达到最先进的性能。

**结论:** 论文的结论是该方法能够在保持数据隐私的同时，在边缘设备上实现高效的神经网络设计，适用于医疗和工业物联网应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Searching+Neural+Architectures+for+Sensor+Nodes+on+IoT+Gateways，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23939，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23939&send_immediately=true&force_search=false)

**原文摘要:** This paper presents an automatic method for the design of Neural Networks
(NNs) at the edge, enabling Machine Learning (ML) access even in
privacy-sensitive Internet of Things (IoT) applications. The proposed method
runs on IoT gateways and designs NNs for connected sensor nodes without sharing
the collected data outside the local network, keeping the data in the site of
collection. This approach has the potential to enable ML for Healthcare
Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing
hardware-friendly and custom NNs at the edge for personalized healthcare and
advanced industrial services such as quality control, predictive maintenance,
or fault diagnosis. By preventing data from being disclosed to cloud services,
this method safeguards sensitive information, including industrial secrets and
personal data. The outcomes of a thorough experimental session confirm that --
on the Visual Wake Words dataset -- the proposed approach can achieve
state-of-the-art results by exploiting a search procedure that runs in less
than 10 hours on the Raspberry Pi Zero 2.

</details>


### [20] [Vision Language Models are Biased](https://arxiv.org/abs/2505.23941)
*An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, Daeyoung Kim*

**主要类别:** cs.LG

**AI概要:** 研究发现视觉语言模型(VLMs)在执行客观视觉任务时存在严重偏差，例如计数和识别任务中的准确率仅为17.05%。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型可能因记忆互联网上的先验知识而产生偏差，影响下游任务的准确性。本文旨在测试这些知识如何影响视觉语言模型在客观视觉任务中的表现。

**方法:** 通过向反事实图像中插入描述主题名称的文本，并评估VLMs在七个不同领域（如动物、标志、国际象棋等）中的计数和识别任务表现来测试偏差程度。此外，还测试了指导模型复核结果或仅依赖图像细节的效果。

**结果:** 实验结果显示，最先进的VLMs在计数任务中的平均准确率为17.05%，且添加文本描述进一步降低了准确率。即使提示模型复核结果或依赖图像细节，准确率也仅提高2个百分点。

**结论:** 本研究揭示了VLMs中存在的显著偏差问题，并提出了一个自动化框架用于测试此类偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Vision+Language+Models+are+Biased，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23941&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.

</details>


### [21] [The Rich and the Simple: On the Implicit Bias of Adam and SGD](https://arxiv.org/abs/2505.24022)
*Bhavya Vasudeva, Jung Whan Lee, Vatsal Sharan, Mahdi Soltanolkotabi*

**主要类别:** cs.LG

**AI概要:** This paper shows that Adam has less simplicity bias than gradient descent (GD), resulting in richer decision boundaries and better performance under distribution shifts, especially in tasks involving spurious correlations.


<details>
  <summary>更多</summary>
  
**动机:** While Adam is widely used for deep learning, its implicit bias compared to standard methods like GD remains poorly understood. This study aims to demystify these differences, particularly regarding simplicity bias.

**方法:** The paper analyzes population gradients and conducts empirical experiments comparing Adam and GD on synthetic data with Gaussian clusters in a binary classification task.

**结果:** GD tends to produce simple, linear decision boundaries with suboptimal margins due to simplicity bias, while Adam generates richer features and nonlinear boundaries closer to the Bayes optimal predictor. Adam also achieves higher test accuracy and performs better under distribution shifts.

**结论:** Adam exhibits less simplicity bias compared to GD, leading to better generalization and more complex decision boundaries.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Rich+and+the+Simple%3A+On+the+Implicit+Bias+of+Adam+and+SGD，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24022，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24022&send_immediately=true&force_search=false)

**原文摘要:** Adam is the de facto optimization algorithm for several deep learning
applications, but an understanding of its implicit bias and how it differs from
other algorithms, particularly standard first-order methods such as
(stochastic) gradient descent (GD), remains limited. In practice, neural
networks trained with SGD are known to exhibit simplicity bias -- a tendency to
find simple solutions. In contrast, we show that Adam is more resistant to such
simplicity bias. To demystify this phenomenon, in this paper, we investigate
the differences in the implicit biases of Adam and GD when training two-layer
ReLU neural networks on a binary classification task involving synthetic data
with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting
in a linear decision boundary with a suboptimal margin, whereas Adam leads to
much richer and more diverse features, producing a nonlinear boundary that is
closer to the Bayes' optimal predictor. This richer decision boundary also
allows Adam to achieve higher test accuracy both in-distribution and under
certain distribution shifts. We theoretically prove these results by analyzing
the population gradients. To corroborate our theoretical findings, we present
empirical results showing that this property of Adam leads to superior
generalization across datasets with spurious correlations where neural networks
trained with SGD are known to show simplicity bias and don't generalize well
under certain distributional shifts.

</details>


### [22] [SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations](https://arxiv.org/abs/2505.23942)
*Gaurav Sarkar, Jay Gala, Subarna Tripathi*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的激活函数SG-Blend，其在多种任务和模型中表现优异，具有较强的通用性和低计算开销。


<details>
  <summary>更多</summary>
  
**动机:** 现有的激活函数如Swish和GELU虽然有效，但往往表现出领域特定的最优性，因此需要一种更通用的激活函数。

**方法:** 提出了一种新的激活函数SG-Blend，它通过动态插值将提出的SSwish（Swish的一阶对称变体）与GELU结合。

**结果:** 实验结果显示，SG-Blend在各种模态和架构中均优于现有方法，并实现了梯度稳定性和模型表达能力之间的更好平衡。

**结论:** SG-Blend在自然语言和计算机视觉任务中都展现了优越的性能，具有较小的计算开销，是一个通用且易于替换的激活函数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SG-Blend%3A+Learning+an+Interpolation+Between+Improved+Swish+and+GELU+for+Robust+Neural+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23942&send_immediately=true&force_search=false)

**原文摘要:** The design of activation functions remains a pivotal component in optimizing
deep neural networks. While prevailing choices like Swish and GELU demonstrate
considerable efficacy, they often exhibit domain-specific optima. This work
introduces SG-Blend, a novel activation function that blends our proposed
SSwish, a first-order symmetric variant of Swish and the established GELU
through dynamic interpolation. By adaptively blending these constituent
functions via learnable parameters, SG-Blend aims to harness their
complementary strengths: SSwish's controlled non-monotonicity and symmetry, and
GELU's smooth, probabilistic profile, to achieve a more universally robust
balance between model expressivity and gradient stability. We conduct
comprehensive empirical evaluations across diverse modalities and
architectures, showing performance improvements across all considered natural
language and computer vision tasks and models. These results, achieved with
negligible computational overhead, underscore SG-Blend's potential as a
versatile, drop-in replacement that consistently outperforms strong
contemporary baselines. The code is available at
https://anonymous.4open.science/r/SGBlend-6CBC.

</details>


### [23] [Characterising the Inductive Biases of Neural Networks on Boolean Data](https://arxiv.org/abs/2505.24060)
*Chris Mingard, Lukas Seier, Niclas Göring, Andrei-Vlad Badelita, Charles London, Ard Louis*

**主要类别:** cs.LG

**AI概要:** 本文研究了深度神经网络的归纳先验、训练动态和泛化能力之间的联系，通过在布尔函数上训练模型，揭示了特征学习和归纳偏置对泛化的影响。


<details>
  <summary>更多</summary>
  
**动机:** 现有工作对深度神经网络泛化能力的解释存在局限性，例如忽略了特征学习的部分，因此需要一个更全面的分析框架。

**方法:** 通过蒙特卡洛学习算法，利用深度-2离散全连接网络与析取范式（DNF）公式之间的一一对应关系进行模型训练和分析。

**结果:** 模型展现了可预测的训练动态以及可解释特征的出现，并能够详细追踪归纳偏置和特征形成如何驱动泛化能力。

**结论:** 深度神经网络即使在过度参数化的情况下也能很好地泛化，这与它们的归纳先验和特征形成密切相关。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Characterising+the+Inductive+Biases+of+Neural+Networks+on+Boolean+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24060&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks are renowned for their ability to generalise well across
diverse tasks, even when heavily overparameterized. Existing works offer only
partial explanations (for example, the NTK-based task-model alignment
explanation neglects feature learning). Here, we provide an end-to-end,
analytically tractable case study that links a network's inductive prior, its
training dynamics including feature learning, and its eventual generalisation.
Specifically, we exploit the one-to-one correspondence between depth-2 discrete
fully connected networks and disjunctive normal form (DNF) formulas by training
on Boolean functions. Under a Monte Carlo learning algorithm, our model
exhibits predictable training dynamics and the emergence of interpretable
features. This framework allows us to trace, in detail, how inductive bias and
feature formation drive generalisation.

</details>


### [24] [Position: The Future of Bayesian Prediction Is Prior-Fitted](https://arxiv.org/abs/2505.23947)
*Samuel Müller, Arik Reuter, Noah Hollmann, David Rügamer, Frank Hutter*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出 Prior-data Fitted Networks (PFNs) 作为贝叶斯推断的未来工具，能够在数据稀缺的情况下高效利用预训练计算资源，并有望在复杂领域发挥重要作用。


<details>
  <summary>更多</summary>
  
**动机:** 随着预训练计算资源的迅速增加，而在许多应用中真实世界新数据的生成几乎停滞，因此需要探索像 PFNs 这样能够有效应对数据稀缺问题的方法。

**方法:** 论文通过分析 PFNs 在随机生成的人工数据集上训练神经网络的能力，探讨其作为贝叶斯模型的有效性，并讨论该方法在复杂领域和更大数据集中的扩展应用。

**结果:** PFNs 已经从最初的小型贝叶斯建模任务扩展到更复杂的领域和更大的数据集，显示了其在广泛应用场景中的潜力。

**结论:** Prior-data Fitted Networks (PFNs) 和其他摊销推断方法代表了贝叶斯推断的未来，它们利用摊销学习解决数据稀缺问题，并且在低数据场景中具有高效分配预训练计算的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+The+Future+of+Bayesian+Prediction+Is+Prior-Fitted，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23947&send_immediately=true&force_search=false)

**原文摘要:** Training neural networks on randomly generated artificial datasets yields
Bayesian models that capture the prior defined by the dataset-generating
distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed
to leverage this insight. In an era of rapidly increasing computational
resources for pre-training and a near stagnation in the generation of new
real-world data in many applications, PFNs are poised to play a more important
role across a wide range of applications. They enable the efficient allocation
of pre-training compute to low-data scenarios. Originally applied to small
Bayesian modeling tasks, the field of PFNs has significantly expanded to
address more complex domains and larger datasets. This position paper argues
that PFNs and other amortized inference approaches represent the future of
Bayesian inference, leveraging amortized learning to tackle data-scarce
problems. We thus believe they are a fruitful area of research. In this
position paper, we explore their potential and directions to address their
current limitations.

</details>


### [25] [On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks](https://arxiv.org/abs/2505.24205)
*Mingze Wang, Weinan E*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了混合专家网络（MoEs）的表达能力，发现浅层MoEs可有效逼近低维流形上的函数，而深层MoEs则能处理具有组合稀疏性的复杂分段函数。


<details>
  <summary>更多</summary>
  
**动机:** 尽管MoEs在实践中取得了成功，但其理论基础仍不完善，因此需要系统研究其在特定结构先验下的表达能力。

**方法:** 作者对浅层和深层MoEs分别进行了理论分析，证明了它们在逼近低维流形上的函数以及具有组合稀疏性的分段函数方面的能力。

**结果:** 对于浅层MoEs，证明了其可以高效逼近低维流形上的函数；对于深层MoEs，展示了其能够以组合稀疏性逼近包含大量分段的函数。

**结论:** 该论文分析了混合专家网络（MoEs）在建模具有低维性和稀疏性结构先验的复杂任务中的表达能力，并揭示了其关键架构组件和超参数的作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Expressive+Power+of+Mixture-of-Experts+for+Structured+Complex+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24205&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in
modern deep learning. Despite their empirical success, the theoretical
foundations underlying their ability to model complex tasks remain poorly
understood. In this work, we conduct a systematic study of the expressive power
of MoEs in modeling complex tasks with two common structural priors:
low-dimensionality and sparsity. For shallow MoEs, we prove that they can
efficiently approximate functions supported on low-dimensional manifolds,
overcoming the curse of dimensionality. For deep MoEs, we show that
$\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise
functions comprising $E^L$ pieces with compositional sparsity, i.e., they can
exhibit an exponential number of structured tasks. Our analysis reveals the
roles of critical architectural components and hyperparameters in MoEs,
including the gating mechanism, expert networks, the number of experts, and the
number of layers, and offers natural suggestions for MoE variants.

</details>


### [26] [TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks](https://arxiv.org/abs/2505.23949)
*Xiang Meng, Mehdi Makni, Rahul Mazumder*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种高效的可转置N:M稀疏掩码生成方法，适用于大规模神经网络训练，在保证模型性能的同时实现良好压缩效果，并具备良好的硬件加速潜力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的可转置N:M稀疏方法要么无法扩展到大规模模型，要么受限于M=4，导致压缩与精度之间的权衡不佳。因此需要提出一种更高效、更具扩展性的方法。

**方法:** 将掩码生成表述为最优传输问题，通过熵正则化和Dykstra算法求解，并结合舍入过程。使用基于张量的GPU并行实现，提高了计算效率。

**结果:** 提出的求解器比现有方法快100倍，误差仅1-10%。LLaMA3.2-8B模型在16:32可转置稀疏设置下表现接近标准N:M模型，并优于标准2:4稀疏模型。

**结论:** 论文介绍了一种高效的可转置N:M稀疏掩码求解器，能够扩展到大规模模型，并通过实验验证了其在保持模型性能的同时提供更好的压缩-精度平衡的实际价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TSENOR%3A+Highly-Efficient+Algorithm+for+Finding+Transposable+N%3AM+Sparse+Masks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23949&send_immediately=true&force_search=false)

**原文摘要:** Network pruning reduces the computational requirements of large neural
networks, with N:M sparsity -- retaining only N out of every M consecutive
weights -- offering a compelling balance between compressed model quality and
hardware acceleration. However, N:M sparsity only accelerates forward-pass
computations, as N:M patterns are not preserved during matrix transposition,
limiting efficiency during training where both passes are computationally
intensive. While transposable N:M sparsity has been proposed to address this
limitation, existing methods for finding transposable N:M sparse masks either
fail to scale to large models or are restricted to M=4 which results in
suboptimal compression-accuracy trade-off. We introduce an efficient solver for
transposable N:M masks that scales to billion-parameter models. We formulate
mask generation as optimal transport problems and solve through entropy
regularization and Dykstra's algorithm, followed by a rounding procedure. Our
tensor-based implementation exploits GPU parallelism, achieving up to 100x
speedup with only 1-10% error compared to existing methods. Our approach can be
integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT
and ALPS to produce transposable N:M sparse models with arbitrary N:M values.
Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains
performance close to its standard N:M counterpart and outperforms standard 2:4
sparse model, showing the practical value of our approach.

</details>


### [27] [Model Informed Flows for Bayesian Inference of Probabilistic Programs](https://arxiv.org/abs/2505.24243)
*Joohwan Ko, Justin Domke*

**主要类别:** cs.LG

**AI概要:** 论文提出了Model-Informed Flow (MIF)架构，改进了变分推断在复杂层次贝叶斯模型中的应用效果。


<details>
  <summary>更多</summary>
  
**动机:** 变分推断在处理复杂层次贝叶斯模型的后验几何时存在困难，而基于流的变分族和Variationally Inferred Parameters (VIP)虽然各自解决了部分问题，但其形式关系尚未被探索。

**方法:** 通过将VIP与全秩高斯结合，引入了包含必要平移机制、先验信息和层次排序的Model-Informed Flow（MIF）架构。

**结果:** MIF在经验上实现了更紧密的后验逼近，并在一系列层次和非层次基准测试中达到或超过了现有最佳性能。

**结论:** MIF架构在层次和非层次的基准测试中都提供了更紧密的后验近似，并达到了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+Informed+Flows+for+Bayesian+Inference+of+Probabilistic+Programs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24243&send_immediately=true&force_search=false)

**原文摘要:** Variational inference often struggles with the posterior geometry exhibited
by complex hierarchical Bayesian models. Recent advances in flow-based
variational families and Variationally Inferred Parameters (VIP) each address
aspects of this challenge, but their formal relationship is unexplored. Here,
we prove that the combination of VIP and a full-rank Gaussian can be
represented exactly as a forward autoregressive flow augmented with a
translation term and input from the model's prior. Guided by this theoretical
insight, we introduce the Model-Informed Flow (MIF) architecture, which adds
the necessary translation mechanism, prior information, and hierarchical
ordering. Empirically, MIF delivers tighter posterior approximations and
matches or exceeds state-of-the-art performance across a suite of hierarchical
and non-hierarchical benchmarks.

</details>


### [28] [Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective](https://arxiv.org/abs/2505.23954)
*Dylan Zapzalka, Trenton Chang, Lindsay Warrenburg, Sae-Hwan Park, Daniel K. Shenfeld, Ravi B. Parikh, Jenna Wiens, Maggie Makar*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于因果关系的方法来识别和量化资源分配决策中代理的特征误报行为，适用于实际场景。


<details>
  <summary>更多</summary>
  
**动机:** 在机器学习模型用于资源分配的场景中，受影响的代理可能会有动机策略性地改变其特征，如何区分误报和真实修改是一个根本性的挑战。

**方法:** 通过区分欺骗性变化和真实修改，比较误报特征对因果后代的因果影响，利用操纵数据集与未操纵数据集之间的差异进行分析。

**结果:** 论文从理论上证明了误报率的可识别性，并刻画了估计器的方差，通过实验验证了所提方法的有效性。

**结论:** 论文提出了一种因果驱动的方法来识别和量化代理在资源分配决策中的特征误报行为，验证了该方法在半合成和真实数据集上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimating+Misreporting+in+the+Presence+of+Genuine+Modification%3A+A+Causal+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23954&send_immediately=true&force_search=false)

**原文摘要:** In settings where ML models are used to inform the allocation of resources,
agents affected by the allocation decisions might have an incentive to
strategically change their features to secure better outcomes. While prior work
has studied strategic responses broadly, disentangling misreporting from
genuine modification remains a fundamental challenge. In this paper, we propose
a causally-motivated approach to identify and quantify how much an agent
misreports on average by distinguishing deceptive changes in their features
from genuine modification. Our key insight is that, unlike genuine
modification, misreported features do not causally affect downstream variables
(i.e., causal descendants). We exploit this asymmetry by comparing the causal
effect of misreported features on their causal descendants as derived from
manipulated datasets against those from unmanipulated datasets. We formally
prove identifiability of the misreporting rate and characterize the variance of
our estimator. We empirically validate our theoretical results using a
semi-synthetic and real Medicare dataset with misreported data, demonstrating
that our approach can be employed to identify misreporting in real-world
scenarios.

</details>


### [29] [Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining](https://arxiv.org/abs/2505.24261)
*Weiyi Wang, Junwei Deng, Yuzheng Hu, Shiyuan Zhang, Xirui Jiang, Runting Zhang, Han Zhao, Jiaqi W. Ma*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了数据归因方法中超参数调优的影响，发现多数方法对超参数敏感，但评估归因性能需要昂贵的模型重训练。为此，作者提出了一种高效的正则化值选择方法，并建议未来应更关注超参数选择问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管近年来涌现出许多新的数据归因方法，但这些方法中超参数调优的影响仍未得到充分探索。因此，本研究旨在填补这一空白，探讨超参数选择对数据归因方法的影响。

**方法:** 论文采用了大规模实证研究的方法来理解常见数据归因方法的超参数敏感性，并对影响函数方法中的正则化项进行了理论分析，提出了一个无需重新训练模型的轻量级正则化值选择过程。

**结果:** 研究结果显示，大多数数据归因方法对某些关键超参数是敏感的。然而，与典型的机器学习算法不同，数据归因性能的评估往往需要在训练数据的子集上重新训练模型，导致计算成本过高。此外，作者提出了一种无需重新训练模型的轻量级正则化值选择过程，并验证了其有效性。

**结论:** 论文得出结论，大多数数据归因方法确实对某些关键超参数敏感，而评估数据归因性能通常需要在训练数据子集上重新训练模型，这使得调整超参数变得计算成本高昂。作者提出了一种轻量级的正则化值选择过程，并通过一系列标准数据归因基准验证了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Taming+Hyperparameter+Sensitivity+in+Data+Attribution%3A+Practical+Selection+Without+Costly+Retraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24261&send_immediately=true&force_search=false)

**原文摘要:** Data attribution methods, which quantify the influence of individual training
data points on a machine learning model, have gained increasing popularity in
data-centric applications in modern AI. Despite a recent surge of new methods
developed in this space, the impact of hyperparameter tuning in these methods
remains under-explored. In this work, we present the first large-scale
empirical study to understand the hyperparameter sensitivity of common data
attribution methods. Our results show that most methods are indeed sensitive to
certain key hyperparameters. However, unlike typical machine learning
algorithms -- whose hyperparameters can be tuned using computationally-cheap
validation metrics -- evaluating data attribution performance often requires
retraining models on subsets of training data, making such metrics
prohibitively costly for hyperparameter tuning. This poses a critical open
challenge for the practical application of data attribution methods. To address
this challenge, we advocate for better theoretical understandings of
hyperparameter behavior to inform efficient tuning strategies. As a case study,
we provide a theoretical analysis of the regularization term that is critical
in many variants of influence function methods. Building on this analysis, we
propose a lightweight procedure for selecting the regularization value without
model retraining, and validate its effectiveness across a range of standard
data attribution benchmarks. Overall, our study identifies a fundamental yet
overlooked challenge in the practical application of data attribution, and
highlights the importance of careful discussion on hyperparameter selection in
future method development.

</details>


### [30] [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/abs/2505.23960)
*Henry Conklin*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新方法来分析大规模神经网络的表征结构，揭示了深度学习模型如何学习信息表示，并探讨了这些结构与语言结构之间的相似性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大规模神经网络取得了显著成功，但仍然缺乏统一的方式来描述其表征空间，也缺乏可靠的方法来解释这些表征的结构如何形成以及哪些结构是理想的。

**方法:** 论文引入了一种新的、高效的熵估计方法来分析大规模神经网络的向量空间，并利用这些方法对多智能体强化学习模型、序列到序列模型和大型语言模型进行分析。

**结果:** 论文展示了如何通过定量方法识别映射中的系统性结构，并提出了一个与语言结构相似的神经网络表征结构模型，同时能够将认知分布式模型与人类类比系统进行比较。

**结论:** 论文得出结论，通过识别映射中存在的结构基元以及利用信息论量化这些结构，可以更好地理解深度学习模型如何表示信息，并揭示推动性能的表征结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Information+Structure+in+Mappings%3A+An+Approach+to+Learning%2C+Representation%2C+and+Generalisation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23960，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23960&send_immediately=true&force_search=false)

**原文摘要:** Despite the remarkable success of large large-scale neural networks, we still
lack unified notation for thinking about and describing their representational
spaces. We lack methods to reliably describe how their representations are
structured, how that structure emerges over training, and what kinds of
structures are desirable. This thesis introduces quantitative methods for
identifying systematic structure in a mapping between spaces, and leverages
them to understand how deep-learning models learn to represent information,
what representational structures drive generalisation, and how design decisions
condition the structures that emerge. To do this I identify structural
primitives present in a mapping, along with information theoretic
quantifications of each. These allow us to analyse learning, structure, and
generalisation across multi-agent reinforcement learning models,
sequence-to-sequence models trained on a single task, and Large Language
Models. I also introduce a novel, performant, approach to estimating the
entropy of vector space, that allows this analysis to be applied to models
ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models
of cognition learn, while allowing us to draw parallels between those systems
and their human analogs. They show how the structures of language and the
constraints that give rise to them in many ways parallel the kinds of
structures that drive performance of contemporary neural networks.

</details>


### [31] [GradPower: Powering Gradients for Faster Language Model Pre-Training](https://arxiv.org/abs/2505.24275)
*Mingze Wang, Jinbo Wang, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, Weinan E, Lei Wu*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GradPower%3A+Powering+Gradients+for+Faster+Language+Model+Pre-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24275，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24275&send_immediately=true&force_search=false)

**原文摘要:** We propose GradPower, a lightweight gradient-transformation technique for
accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,
GradPower first applies the elementwise sign-power transformation:
$\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds
the transformed gradient into a base optimizer. Notably, GradPower requires
only a single-line code change and no modifications to the base optimizer's
internal logic, including the hyperparameters. When applied to Adam (termed
AdamPower), GradPower consistently achieves lower terminal loss across diverse
architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,
OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The
most pronounced gains are observed when training modern mixture-of-experts
models with warmup-stable-decay schedules. GradPower also integrates seamlessly
with other state-of-the-art optimizers, such as Muon, yielding further
improvements. Finally, we provide theoretical analyses that reveal the
underlying mechanism of GradPower and highlights the influence of gradient
noise.

</details>


### [32] [Improved Approximations for Hard Graph Problems using Predictions](https://arxiv.org/abs/2505.23967)
*Anders Aamand, Justin Y. Chen, Siddharth Gollapudi, Sandeep Silwal, Hao Wu*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了如何通过结合预测来改进NP难问题的近似算法。


<details>
  <summary>更多</summary>
  
**动机:** 为了突破标准设置中的近似障碍，我们引入了从过去数据中学习的预测。

**方法:** 我们开发了一种基于边的预测模型，并分别满足与高阶顶点和低阶顶点相关的约束。

**结果:** 对于MaxCut、顶点覆盖、集合覆盖和最大独立集等问题，我们的算法具有改进的近似比。

**结论:** 通过使用预测模型，我们能够改进NP难问题的近似算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+Approximations+for+Hard+Graph+Problems+using+Predictions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23967&send_immediately=true&force_search=false)

**原文摘要:** We design improved approximation algorithms for NP-hard graph problems by
incorporating predictions (e.g., learned from past data). Our prediction model
builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad,
d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based
version of this model, where each edge provides two bits of information,
corresponding to predictions about whether each of its endpoints belong to an
optimal solution. Even with weak predictions where each bit is only
$\varepsilon$-correlated with the true solution, this information allows us to
break approximation barriers in the standard setting. We develop algorithms
with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and
Maximum Independent Set problems (among others). Across these problems, our
algorithms share a unifying theme, where we separately satisfy constraints
related to high degree vertices (using predictions) and low-degree vertices
(without using predictions) and carefully combine the answers.

</details>


### [33] [Binary Cumulative Encoding meets Time Series Forecasting](https://arxiv.org/abs/2505.24595)
*Andrei Chernov, Vitaliy Pozdnyakov, Ilya Makarov*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的时间序列预测方法，通过二进制累积编码和专用卷积神经网络架构，解决了传统分类方法忽略数值顺序信息的问题，并取得了更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于分类的时间序列预测方法通常依赖于忽略底层值固有序数结构的一热编码，导致训练期间无法提供预测值与真实值之间相对距离的信息。

**方法:** 引入了二进制累积编码（BCE），将标量目标表示为单调二进制向量，并设计了一个包含残差和扩张卷积的卷积神经网络架构用于时间建模。

**结果:** 实验表明，所提出的方法在点预测和概率预测任务中均优于现有方法，同时参数更少且训练更快。

**结论:** 本文提出了一种基于二进制累积编码（BCE）的时间序列预测方法，该方法在分类框架内实现了距离感知的表示学习，并通过设计特定的卷积神经网络架构，在基准预测数据集上优于广泛使用的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Binary+Cumulative+Encoding+meets+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24595&send_immediately=true&force_search=false)

**原文摘要:** Recent studies in time series forecasting have explored formulating
regression via classification task. By discretizing the continuous target space
into bins and predicting over a fixed set of classes, these approaches benefit
from stable training, robust uncertainty modeling, and compatibility with
modern deep learning architectures. However, most existing methods rely on
one-hot encoding that ignores the inherent ordinal structure of the underlying
values. As a result, they fail to provide information about the relative
distance between predicted and true values during training. In this paper, we
propose to address this limitation by introducing binary cumulative encoding
(BCE), that represents scalar targets into monotonic binary vectors. This
encoding implicitly preserves order and magnitude information, allowing the
model to learn distance-aware representations while still operating within a
classification framework. We propose a convolutional neural network
architecture specifically designed for BCE, incorporating residual and dilated
convolutions to enable fast and expressive temporal modeling. Through extensive
experiments on benchmark forecasting datasets, we show that our approach
outperforms widely used methods in both point and probabilistic forecasting,
while requiring fewer parameters and enabling faster training.

</details>


### [34] [Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training](https://arxiv.org/abs/2505.23971)
*William Merrill, Shane Arora, Dirk Groeneveld, Hannaneh Hajishirzi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种直接测量关键批量大小（CBS）的新方法，并揭示了其在训练过程中的演变规律。基于此，作者采用批处理大小预热策略，在减少43%梯度步骤的情况下实现了优于原始训练的效果，为大规模语言模型的高效训练提供了新思路。


<details>
  <summary>更多</summary>
  
**动机:** 论文的动机是解决大规模训练语言模型时批量大小选择的问题。批量太大可能会影响token效率，而批量太小会减慢训练速度。现有的基于梯度噪声估计CBS的方法存在较强的假设限制，因此需要一种更可靠的实证方法来指导实际训练。

**方法:** 该论文提出了一种直接测量关键批量大小（CBS）的实证方法，并研究了其在训练过程中的变化趋势。此外，作者应用批处理大小预热策略验证了其方法的有效性。

**结果:** 论文发现关键批量大小（CBS）在初始化时接近于0，随后迅速增加并最终趋于稳定。这一趋势在不同模型规模（1B和7B参数）中均成立。同时，通过使用批处理大小预热策略，以更少的梯度步骤实现了与原始训练相当甚至更好的损失表现。

**结论:** 论文得出结论，通过引入一种简单、实证的方法来直接测量关键批量大小（CBS），可以可靠地训练大规模语言模型。这种方法可以通过从小批量开始并随着CBS增长逐步增加批量大小来实现，从而提高数据并行性而不损害性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Critical+Batch+Size+Revisited%3A+A+Simple+Empirical+Approach+to+Large-Batch+Language+Model+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23971&send_immediately=true&force_search=false)

**原文摘要:** The right batch size is important when training language models at scale: a
large batch size is necessary for fast training, but a batch size that is too
large will harm token efficiency. To navigate this tradeoff, McCandlish et al.
(2018) suggest that a critical batch size (CBS), below which training will not
substantially degrade loss, can be estimated based on the gradient noise scale
during training. While their method has been adopted in practice, e.g., when
training GPT-3, strong assumptions are required to justify gradient noise as a
proxy for the CBS, which makes it unclear whether their approach should be
trusted in practice, limiting its applicability. In this paper, we introduce a
simple, empirical approach to directly measure the CBS and show how the CBS
evolves over training. Applying our approach to the OLMo models, we find that
CBS is near 0 at initialization, increases rapidly at first, and then plateaus
as training progresses. Furthermore, we find that this trend holds across
different model sizes (1B and 7B), suggesting CBS from small training runs can
inform larger-scale training runs. Our findings about how the CBS changes over
training motivate batch size warmup as a natural way to reliably train language
models at large batch size: start the batch size small and increase it as the
CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to
slightly better loss than the original training run with 43% fewer gradient
steps. This shows how our framework can be applied to reliably train language
models at larger batch sizes, increasing data parallelism without compromising
performance.

</details>


### [35] [Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms](https://arxiv.org/abs/2505.24692)
*Derek Everett, Fred Lu, Edward Raff, Fernando Camacho, James Holt*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quick-Draw+Bandits%3A+Quickly+Optimizing+in+Nonstationary+Environments+with+Extremely+Many+Arms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24692，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24692&send_immediately=true&force_search=false)

**原文摘要:** Canonical algorithms for multi-armed bandits typically assume a stationary
reward environment where the size of the action space (number of arms) is
small. More recently developed methods typically relax only one of these
assumptions: existing non-stationary bandit policies are designed for a small
number of arms, while Lipschitz, linear, and Gaussian process bandit policies
are designed to handle a large (or infinite) number of arms in stationary
reward environments under constraints on the reward function. In this
manuscript, we propose a novel policy to learn reward environments over a
continuous space using Gaussian interpolation. We show that our method
efficiently learns continuous Lipschitz reward functions with
$\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally
extends to non-stationary problems with a simple modification. We finally
demonstrate that our method is computationally favorable (100-10000x faster)
and experimentally outperforms sliding Gaussian process policies on datasets
with non-stationarity and an extremely large number of arms.

</details>


### [36] [Adaptive Deadline and Batch Layered Synchronized Federated Learning](https://arxiv.org/abs/2505.23973)
*Asaf Goren, Natalie Lang, Nir Shlezinger, Alejandro Cohen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为ADEL-FL的新框架，通过优化每轮截止时间和用户特定的批量大小，以提高异构环境下的联邦学习的收敛速度和最终准确性。


<details>
  <summary>更多</summary>
  
**动机:** 同步联邦学习由于设备异质性导致延迟瓶颈，现有解决方案将轮次时间和本地工作负载视为静态参数，限制了其在严格时间约束下的有效性。

**方法:** 提出ADEL-FL框架，联合优化每轮截止时间和用户特定的批量大小，形成一个受限优化问题，在总训练时间和全局轮次数下最小化预期L2距离到全局最优解。

**结果:** 广泛的实验表明，ADEL-FL在异构条件下，无论是在收敛速度还是最终准确性方面都优于其他方法。

**结论:** ADEL-FL提供了一种有效的解决方案来应对同步联邦学习中的设备异质性问题，并展示了其无偏更新和有界方差的理论保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Deadline+and+Batch+Layered+Synchronized+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23973，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23973&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) enables collaborative model training across
distributed edge devices while preserving data privacy, and typically operates
in a round-based synchronous manner. However, synchronous FL suffers from
latency bottlenecks due to device heterogeneity, where slower clients
(stragglers) delay or degrade global updates. Prior solutions, such as fixed
deadlines, client selection, and layer-wise partial aggregation, alleviate the
effect of stragglers, but treat round timing and local workload as static
parameters, limiting their effectiveness under strict time constraints. We
propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines
and user-specific batch sizes for layer-wise aggregation. Our approach
formulates a constrained optimization problem minimizing the expected L2
distance to the global optimum under total training time and global rounds. We
provide a convergence analysis under exponential compute models and prove that
ADEL-FL yields unbiased updates with bounded variance. Extensive experiments
demonstrate that ADEL-FL outperforms alternative methods in both convergence
rate and final accuracy under heterogeneous conditions.

</details>


### [37] [Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning](https://arxiv.org/abs/2505.24737)
*Erchi Wang, Yuqing Zhu, Yu-Xiang Wang*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+to+Linear+Separable+Subsets+with+Large-Margin+in+Differentially+Private+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24737&send_immediately=true&force_search=false)

**原文摘要:** This paper studies the problem of differentially private empirical risk
minimization (DP-ERM) for binary linear classification. We obtain an efficient
$(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of
$\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} +
\frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data
points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and
$\gamma$ is the margin of linear separation of the remaining data points (after
$S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic
terms. In the agnostic case, we improve the existing results when the number of
outliers is small. Our algorithm is highly adaptive because it does not require
knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We
also derive a utility bound for the advanced private hyperparameter tuning
algorithm.

</details>


### [38] [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/abs/2505.23987)
*Vishal Dey, Xiao Hu, Xia Ning*

**主要类别:** cs.LG

**AI概要:** 本文介绍了C-MuMOInstruct和GeLLMO-Cs，旨在通过指令调优大语言模型解决药物设计中的多属性分子优化问题，并取得了显著的成功率提升和零样本泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的计算方法和指令调整的大语言模型（LLMs）无法捕捉到药物设计中多属性优化的细微目标，限制了其实际应用。

**方法:** 开发了C-MuMOInstruct数据集以及基于此的GeLLMO-Cs系列模型，进行属性特定的分子优化。

**结果:** 实验表明GeLLMO-Cs在5个分布内任务和5个分布外任务上均优于强基线模型，成功率提高了高达126%，并展示了对新优化任务和未见过的指令的零样本泛化能力。

**结论:** 论文提出C-MuMOInstruct和GeLLMO-Cs，为支持具有属性特定目标的现实、多样化优化提供了基础模型的一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+for+Controllable+Multi-property+Multi-objective+Molecule+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23987&send_immediately=true&force_search=false)

**原文摘要:** In real-world drug design, molecule optimization requires selectively
improving multiple molecular properties up to pharmaceutically relevant levels,
while maintaining others that already meet such criteria. However, existing
computational approaches and instruction-tuned LLMs fail to capture such
nuanced property-specific objectives, limiting their practical applicability.
To address this, we introduce C-MuMOInstruct, the first instruction-tuning
dataset focused on multi-property optimization with explicit, property-specific
objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of
instruction-tuned LLMs that can perform targeted property-specific
optimization. Our experiments across 5 in-distribution and 5
out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong
baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit
impressive 0-shot generalization to novel optimization tasks and unseen
instructions. This offers a step toward a foundational LLM to support
realistic, diverse optimizations with property-specific objectives.
C-MuMOInstruct and code are accessible through
https://github.com/ninglab/GeLLMO-C.

</details>


### [39] [Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting](https://arxiv.org/abs/2505.24003)
*ChengAo Shen, Wenchao Yu, Ziming Zhao, Dongjin Song, Wei Cheng, Haifeng Chen, Jingchao Ni*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的多模态时间序列预测框架DMMV，在多个数据集上表现优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 将时间序列转换为图像和文本形式，可以使用强大的预训练大模型进行预测，但存在归纳偏差问题。

**方法:** 利用趋势-季节分解和基于回溯残差的自适应分解方法集成多模态视图。

**结果:** 在8个基准数据集中，DMMV在6个上取得了最佳均方误差（MSE）效果。

**结论:** DMMV框架在长期时间序列预测中表现出色，优于现有的单视图和其他多模态方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+View+Enhanced+Large+Vision+Models+for+Long-Term+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24003&send_immediately=true&force_search=false)

**原文摘要:** Time series, typically represented as numerical sequences, can also be
transformed into images and texts, offering multi-modal views (MMVs) of the
same underlying signal. These MMVs can reveal complementary patterns and enable
the use of powerful pre-trained large models, such as large vision models
(LVMs), for long-term time series forecasting (LTSF). However, as we identified
in this work, applying LVMs to LTSF poses an inductive bias towards
"forecasting periods". To harness this bias, we propose DMMV, a novel
decomposition-based multi-modal view framework that leverages trend-seasonal
decomposition and a novel backcast residual based adaptive decomposition to
integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art
(SOTA) models across diverse datasets show that DMMV outperforms single-view
and existing multi-modal baselines, achieving the best mean squared error (MSE)
on 6 out of 8 benchmark datasets.

</details>


### [40] [How far away are truly hyperparameter-free learning algorithms?](https://arxiv.org/abs/2505.24005)
*Priya Kasimbeg, Vincent Roulet, Naman Agarwal, Sourabh Medapati, Fabian Pedregosa, Atish Agarwala, George E. Dahl*

**主要类别:** cs.LG

**AI概要:** 本文研究了学习率自由方法作为无超参数方法组件的潜力，发现虽然其性能有所提升，但仍然落后于精心校准的NadamW基线方法，表明仍有较大改进空间。


<details>
  <summary>更多</summary>
  
**动机:** 尽管方法论取得了重大进展，但超参数调优仍然是机器学习系统开发中的关键且昂贵的部分。理想情况下，训练算法应具备无需针对特定工作负载调整超参数的默认设置。

**方法:** 通过冻结非学习率超参数至默认值，并使用AlgoPerf: Training Algorithms基准来评估学习率自由方法的表现。随后搜索在所有工作负载中表现良好的超参数配置。

**结果:** 文献提供的默认设置在基准测试中表现不佳。经过搜索优化的超参数配置后，最佳AlgoPerf校准的学习率自由方法性能显著提升，但仍略逊于同样校准的NadamW基线方法。

**结论:** 学习率自由的方法仍有很大的改进空间，测试与强大的、工作负载无关的基线对于改进超参数减少技术很重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+far+away+are+truly+hyperparameter-free+learning+algorithms%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24005&send_immediately=true&force_search=false)

**原文摘要:** Despite major advances in methodology, hyperparameter tuning remains a
crucial (and expensive) part of the development of machine learning systems.
Even ignoring architectural choices, deep neural networks have a large number
of optimization and regularization hyperparameters that need to be tuned
carefully per workload in order to obtain the best results. In a perfect world,
training algorithms would not require workload-specific hyperparameter tuning,
but would instead have default settings that performed well across many
workloads. Recently, there has been a growing literature on optimization
methods which attempt to reduce the number of hyperparameters -- particularly
the learning rate and its accompanying schedule. Given these developments, how
far away is the dream of neural network training algorithms that completely
obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as
components of hyperparameter-free methods. We freeze their (non-learning rate)
hyperparameters to default values, and score their performance using the
recently-proposed AlgoPerf: Training Algorithms benchmark. We found that
literature-supplied default settings performed poorly on the benchmark, so we
performed a search for hyperparameter configurations that performed well across
all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free
methods had much improved performance but still lagged slightly behind a
similarly calibrated NadamW baseline in overall benchmark score. Our results
suggest that there is still much room for improvement for learning-rate-free
methods, and that testing against a strong, workload-agnostic baseline is
important to improve hyperparameter reduction techniques.

</details>


### [41] [From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?](https://arxiv.org/abs/2505.24030)
*Ziming Zhao, ChengAo Shen, Hanghang Tong, Dongjin Song, Zhigang Deng, Qingsong Wen, Jingchao Ni*

**主要类别:** cs.LG

**AI概要:** 本文首次系统评估了 LVM 在时间序列分类和预测中的效果，发现 LVM 在分类中表现良好，但在预测方面仍有局限。


<details>
  <summary>更多</summary>
  
**动机:** 随着多模态领域的发展，大型视觉模型 (LVM) 成为一个有前景的方向，但其在时间序列分析中的有效性尚存疑问。

**方法:** 设计并进行了首个系统性研究，涉及 4 种 LVM 模型、8 种成像方法、18 个数据集和 26 个基线模型，并在高层（分类）和低层（预测）任务中进行了广泛的消融分析。

**结果:** 研究发现，LVM 在时间序列分类中确实有效，但在预测任务中面临挑战。当前最先进的 LVM 预测器仅限于特定类型的 LVM 和成像方法，且对预测周期具有偏向性，难以利用较长的回溯窗口。

**结论:** LVMs 是有潜力的时间序列分类工具，但在预测方面仍存在挑战。研究结果为未来基于 LVM 和多模态解决方案的时间序列任务研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Images+to+Signals%3A+Are+Large+Vision+Models+Useful+for+Time+Series+Analysis%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24030，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24030&send_immediately=true&force_search=false)

**原文摘要:** Transformer-based models have gained increasing attention in time series
research, driving interest in Large Language Models (LLMs) and foundation
models for time series analysis. As the field moves toward multi-modality,
Large Vision Models (LVMs) are emerging as a promising direction. In the past,
the effectiveness of Transformer and LLMs in time series has been debated. When
it comes to LVMs, a similar question arises: are LVMs truely useful for time
series analysis? To address it, we design and conduct the first principled
study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across
both high-level (classification) and low-level (forecasting) tasks, with
extensive ablation analysis. Our findings indicate LVMs are indeed useful for
time series classification but face challenges in forecasting. Although
effective, the contemporary best LVM forecasters are limited to specific types
of LVMs and imaging methods, exhibit a bias toward forecasting periods, and
have limited ability to utilize long look-back windows. We hope our findings
could serve as a cornerstone for future research on LVM- and multimodal-based
solutions to different time series tasks.

</details>


### [42] [LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training](https://arxiv.org/abs/2505.24034)
*Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种高效的异步强化学习框架LlamaRL，可显著提升超大规模语言模型的训练效率。


<details>
  <summary>更多</summary>
  
**动机:** 由于延迟和内存需求高，开发高效管理超大规模LLM的RL框架具有挑战性。

**方法:** 提出了一个基于原生PyTorch的完全分布式、异步的RL框架，并结合了多种最佳实践技术。

**结果:** 在405B参数的策略模型上，与类似DeepSpeed-Chat系统相比，LlamaRL实现了最高10.7倍的加速。

**结论:** LlamaRL是一个高效的、异步的强化学习框架，适用于未来大规模模型的训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LlamaRL%3A+A+Distributed+Asynchronous+Reinforcement+Learning+Framework+for+Efficient+Large-scale+LLM+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24034&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has become the most effective post-training
approach for improving the capabilities of Large Language Models (LLMs). In
practice, because of the high demands on latency and memory, it is particularly
challenging to develop an efficient RL framework that reliably manages policy
models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL
framework optimized for efficient training of large-scale LLMs with various
model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a
handful to thousands of devices. LlamaRL introduces a streamlined,
single-controller architecture built entirely on native PyTorch, enabling
modularity, ease of use, and seamless scalability to thousands of GPUs. We also
provide a theoretical analysis of LlamaRL's efficiency, including a formal
proof that its asynchronous design leads to strict RL speed-up. Empirically
during the Llama 3 post-training, by leveraging best practices such as
colocated model offloading, asynchronous off-policy training, and distributed
direct memory access for weight synchronization, LlamaRL achieves significant
efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like
systems on a 405B-parameter policy model. Furthermore, the efficiency advantage
continues to grow with increasing model scale, demonstrating the framework's
suitability for future large-scale RL training.

</details>


### [43] [NeuronTune: Towards Self-Guided Spurious Bias Mitigation](https://arxiv.org/abs/2505.24048)
*Guangtao Zheng, Wenqian Ye, Aidong Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为NeuronTune的新方法，可在不依赖外部虚假相关性注释的情况下，通过直接干预模型的内部决策过程来缓解深度神经网络中的虚假偏差问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的缓解虚假偏差的方法通常依赖于外部注释，这可能难以获得且与模型中的虚假偏差无关。因此，提出一种自导引的缓解虚假偏差的方法是很有必要的。

**方法:** 通过探测模型潜在嵌入空间中导致虚假预测行为的神经元，并对其进行调节，从而实现对模型预测行为的干预。

**结果:** 实验结果表明，NeuronTune可以在不同架构和数据模态上显著缓解虚假偏差，并使模型更加接近无偏模型。

**结论:** NeuronTune是一种无需依赖外部虚假相关性注释的模型内部决策过程干预方法，可以有效缓解深度神经网络中的虚假偏差问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeuronTune%3A+Towards+Self-Guided+Spurious+Bias+Mitigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24048&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks often develop spurious bias, reliance on correlations
between non-essential features and classes for predictions. For example, a
model may identify objects based on frequently co-occurring backgrounds rather
than intrinsic features, resulting in degraded performance on data lacking
these correlations. Existing mitigation approaches typically depend on external
annotations of spurious correlations, which may be difficult to obtain and are
not relevant to the spurious bias in a model. In this paper, we take a step
towards self-guided mitigation of spurious bias by proposing NeuronTune, a post
hoc method that directly intervenes in a model's internal decision process. Our
method probes in a model's latent embedding space to identify and regulate
neurons that lead to spurious prediction behaviors. We theoretically justify
our approach and show that it brings the model closer to an unbiased one.
Unlike previous methods, NeuronTune operates without requiring spurious
correlation annotations, making it a practical and effective tool for improving
model robustness. Experiments across different architectures and data
modalities demonstrate that our method significantly mitigates spurious bias in
a self-guided way.

</details>


### [44] [Differential Gated Self-Attention](https://arxiv.org/abs/2505.24054)
*Elpiniki Maria Lygizou, Mónika Farsang, Radu Grosu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新型的自注意力机制M-DGSA，该机制通过引入输入依赖的门控机制和生物启发的对比度增强方法，显著提升了Transformer模型在处理噪声输入时的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统Transformer中的标准自注意力机制对所有查询键交互进行统一处理，导致其对输入噪声敏感。为提升模型在有噪声输入下的鲁棒性，作者提出了M-DGSA。

**方法:** 受生物神经回路中侧向抑制的启发，并基于Differential Transformer的双softmax减法去噪方法，提出Multihead Differential Gated Self-Attention (M-DGSA)，其中每个头分为兴奋和抑制分支，通过sigmoid门控融合其双softmax映射，实现上下文感知的对比度增强。

**结果:** 在视觉和语言基准任务上的实验表明，相比传统的Transformer、Vision Transformer和Differential Transformer基线模型，M-DGSA表现出更强的鲁棒性和抗噪能力。

**结论:** 论文提出了一种新的自注意力机制M-DGSA，它通过学习每头输入依赖的门控来动态抑制注意力噪声，从而提高Transformer在噪音环境下的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Differential+Gated+Self-Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24054&send_immediately=true&force_search=false)

**原文摘要:** Transformers excel across a large variety of tasks but remain susceptible to
corrupted inputs, since standard self-attention treats all query-key
interactions uniformly. Inspired by lateral inhibition in biological neural
circuits and building on the recent use by the Differential Transformer's use
of two parallel softmax subtraction for noise cancellation, we propose
Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head
input-dependent gating to dynamically suppress attention noise. Each head
splits into excitatory and inhibitory branches whose dual softmax maps are
fused by a sigmoid gate predicted from the token embedding, yielding a
context-aware contrast enhancement. M-DGSA integrates seamlessly into existing
Transformer stacks with minimal computational overhead. We evaluate on both
vision and language benchmarks, demonstrating consistent robustness gains over
vanilla Transformer, Vision Transformer, and Differential Transformer
baselines. Our contributions are (i) a novel input-dependent gating mechanism
for self-attention grounded in lateral inhibition, (ii) a principled synthesis
of biological contrast-enhancement and self-attention theory, and (iii)
comprehensive experiments demonstrating noise resilience and cross-domain
applicability.

</details>


### [45] [Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs](https://arxiv.org/abs/2505.24055)
*Yilong Wang, Tianxiang Zhao, Zongyu Wu, Suhang Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于图神经网络的新颖无监督域适应框架，利用链接预测促进跨域消息传递，并通过身份保留学习目标优化边缘插入，从而解决了标签分布偏移的问题。


<details>
  <summary>更多</summary>
  
**动机:** 图神经网络的成功依赖于大量标记数据，而在新兴领域获取高质量标签成本高昂。因此需要一种能够在无监督情况下进行域适应的方法。

**方法:** 提出了一种新的框架，通过链接预测连接源图和目标图的节点，实现图间的消息传递，并设计了一个身份保留学习目标来指导边插入模块的学习。

**结果:** 该方法通过修改目标图的输入层面，使其在嵌入空间中更接近源域，同时对跨域的标签分布不平衡具有鲁棒性。

**结论:** 实验结果表明了所提出的框架的有效性，其能够有效解决标签分布偏移问题，并在真实世界数据集上取得了良好效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Source+and+Target+Domains+via+Link+Prediction+for+Unsupervised+Domain+Adaptation+on+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24055，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24055&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) have shown great ability for node classification
on graphs. However, the success of GNNs relies on abundant labeled data, while
obtaining high-quality labels is costly and challenging, especially for newly
emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a
classifier on the labeled source graph and adapts it to the unlabeled target
graph, is attracting increasing attention. Various approaches have been
proposed to alleviate the distribution shift between the source and target
graphs to facilitate the classifier adaptation. However, most of them simply
adopt existing UDA techniques developed for independent and identically
distributed data to gain domain-invariant node embeddings for graphs, which do
not fully consider the graph structure and message-passing mechanism of GNNs
during the adaptation and will fail when label distribution shift exists among
domains. In this paper, we proposed a novel framework that adopts link
prediction to connect nodes between source and target graphs, which can
facilitate message-passing between the source and target graphs and augment the
target nodes to have ``in-distribution'' neighborhoods with the source domain.
This strategy modified the target graph on the input level to reduce its
deviation from the source domain in the embedding space and is insensitive to
disproportional label distributions across domains. To prevent the loss of
discriminative information in the target graph, we further design a novel
identity-preserving learning objective, which guides the learning of the edge
insertion module together with reconstruction and adaptation losses.
Experimental results on real-world datasets demonstrate the effectiveness of
our framework.

</details>


### [46] [Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition](https://arxiv.org/abs/2505.24059)
*Sean Foley, Hong Nguyen, Jihwan Lee, Sudarsana Reddy Kadiri, Dani Byrd, Louis Goldstein, Shrikanth Narayanan*

**主要类别:** cs.LG

**AI概要:** 本研究利用单说话人的MRI语料库开发了用于音素识别的单模态和多模态模型，旨在解耦并解释每种模态的贡献。


<details>
  <summary>更多</summary>
  
**动机:** 以往的研究受限于依赖多人语料库，这阻碍了声学与发音之间详细关系的学习，因为存在显著的跨说话人变异性。

**方法:** 使用长格式单说话人MRI语料库，开发了单模态音频、视频模型以及多模态模型，并对模型的潜在空间和注意力权重进行了分析。

**结果:** 音频和多模态模型在不同的语音方式类别上表现相似，但在发音位置上有所差异；模型潜在空间显示了音频和多模态模型对语音空间的编码相似，而注意力权重突出了某些音素声学和发音时间上的差异。

**结论:** 通过单说话人语料库的应用，能够更精确地学习声学与发音之间的关系，同时揭示了不同模态在发音识别中的时间和贡献差异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+disentangling+the+contributions+of+articulation+and+acoustics+in+multimodal+phoneme+recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24059，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24059&send_immediately=true&force_search=false)

**原文摘要:** Although many previous studies have carried out multimodal learning with
real-time MRI data that captures the audio-visual kinematics of the vocal tract
during speech, these studies have been limited by their reliance on
multi-speaker corpora. This prevents such models from learning a detailed
relationship between acoustics and articulation due to considerable
cross-speaker variability. In this study, we develop unimodal audio and video
models as well as multimodal models for phoneme recognition using a long-form
single-speaker MRI corpus, with the goal of disentangling and interpreting the
contributions of each modality. Audio and multimodal models show similar
performance on different phonetic manner classes but diverge on places of
articulation. Interpretation of the models' latent space shows similar encoding
of the phonetic space across audio and multimodal models, while the models'
attention weights highlight differences in acoustic and articulatory timing for
certain phonemes.

</details>


### [47] [Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning](https://arxiv.org/abs/2505.24061)
*Jiashun Liu, Zihao Wu, Johan Obando-Ceron, Pablo Samuel Castro, Aaron Courville, Ling Pan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的神经元活动度量方法GraMa，并展示了其在多种深度强化学习任务中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的tau-dormant神经元比例方法在复杂架构中失去统计效力，因此需要一种更关注神经元学习能力的新度量方法。

**方法:** 将统计目标从激活转移到梯度，提出了GraMa（梯度大小神经活动度量）方法，并基于此进行了实验验证。

**结果:** GraMa能够在包括残差网络、扩散模型和使用不同激活函数的代理在内的多种架构中有效揭示持久的神经元不活动。此外，通过ReGraMa重置神经元，在多个深度强化学习算法和基准测试中一致提高了学习性能。

**结论:** GraMa是一种轻量级、架构无关的度量方法，能有效揭示神经元层面的学习能力，并且通过ReGraMa重置神经元能够持续提高深度强化学习算法的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measure+gradients%2C+not+activations%21+Enhancing+neuronal+activity+in+deep+reinforcement+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24061&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning (RL) agents frequently suffer from neuronal
activity loss, which impairs their ability to adapt to new data and learn
continually. A common method to quantify and address this issue is the
tau-dormant neuron ratio, which uses activation statistics to measure the
expressive ability of neurons. While effective for simple MLP-based agents,
this approach loses statistical power in more complex architectures. To address
this, we argue that in advanced RL agents, maintaining a neuron's learning
capacity, its ability to adapt via gradient updates, is more critical than
preserving its expressive ability. Based on this insight, we shift the
statistical objective from activations to gradients, and introduce GraMa
(Gradient Magnitude Neural Activity Metric), a lightweight,
architecture-agnostic metric for quantifying neuron-level learning capacity. We
show that GraMa effectively reveals persistent neuron inactivity across diverse
architectures, including residual networks, diffusion models, and agents with
varied activation functions. Moreover, resetting neurons guided by GraMa
(ReGraMa) consistently improves learning performance across multiple deep RL
algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.

</details>


### [48] [Primal-Dual Neural Algorithmic Reasoning](https://arxiv.org/abs/2505.24067)
*Yu He, Ellen Vitercik*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于原始对偶方法的神经算法推理框架，成功应用于复杂问题求解，并在性能和泛化能力上超越了现有算法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的神经算法推理研究主要集中在多项式时间内可解的问题上，如何将其扩展到更难的问题仍然是一个挑战。

**方法:** 通过引入原始对偶变量之间的二部图表示，并结合小规模实例的最优解来增强模型的推理能力。

**结果:** 实验结果表明，所提出的模型不仅能够模拟经典算法，还能在多个任务上超越传统近似算法的表现，并能很好地泛化到更大和分布外的图数据上。

**结论:** 论文提出了一种基于原始对偶框架的通用神经算法推理框架，该框架能够有效模拟并改进近似算法的表现，并具有实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Primal-Dual+Neural+Algorithmic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24067，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24067&send_immediately=true&force_search=false)

**原文摘要:** Neural Algorithmic Reasoning (NAR) trains neural networks to simulate
classical algorithms, enabling structured and interpretable reasoning over
complex data. While prior research has predominantly focused on learning exact
algorithms for polynomial-time-solvable problems, extending NAR to harder
problems remains an open challenge. In this work, we introduce a general NAR
framework grounded in the primal-dual paradigm, a classical method for
designing efficient approximation algorithms. By leveraging a bipartite
representation between primal and dual variables, we establish an alignment
between primal-dual algorithms and Graph Neural Networks. Furthermore, we
incorporate optimal solutions from small instances to greatly enhance the
model's reasoning capabilities. Our empirical results demonstrate that our
model not only simulates but also outperforms approximation algorithms for
multiple tasks, exhibiting robust generalization to larger and
out-of-distribution graphs. Moreover, we highlight the framework's practical
utility by integrating it with commercial solvers and applying it to real-world
datasets.

</details>


### [49] [DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures](https://arxiv.org/abs/2505.24069)
*Yu He, Yingxi Li, Colin White, Ellen Vitercik*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为DSR-Bench的新基准测试，旨在评估大型语言模型在理解与推理数据关系方面的能力，揭示了当前模型在复杂结构和自然语言任务上的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基准测试主要集中在高层次、应用驱动的评估上，而没有孤立地关注模型理解与推理数据关系的基本能力。为了填补这一空白，作者设计了DSR-Bench。

**方法:** 引入了一种新的基准测试DSR-Bench，通过提供可解释的数据关系表示的数据结构来评估LLMs的结构推理能力。

**结果:** DSR-Bench包括20种数据结构、35种操作和4140个问题实例，它是一个分层组织的、完全自动化和确定性的评估管道，并用于基准测试九个最先进的LLM。

**结论:** DSR-Bench的分析表明，即使是性能最好的模型在复杂和混合结构上的表现仍然脆弱，且在多维数据和自然语言任务描述上表现较差，这突显了现实世界部署中的一个关键缺陷。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DSR-Bench%3A+Evaluating+the+Structural+Reasoning+Abilities+of+LLMs+via+Data+Structures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24069，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24069&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly deployed for real-world tasks
that fundamentally involve data manipulation. A core requirement across these
tasks is the ability to perform structural reasoning--that is, to understand
and reason about data relationships. For example, customer requests require a
temporal ordering, which can be represented by data structures such as queues.
However, existing benchmarks primarily focus on high-level, application-driven
evaluations without isolating this fundamental capability. To address this gap,
we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning
capabilities through data structures, which provide interpretable
representations of data relationships. DSR-Bench includes 20 data structures,
35 operations, and 4,140 problem instances, organized hierarchically for
fine-grained analysis of reasoning limitations. Our evaluation pipeline is
fully automated and deterministic, eliminating subjective human or model-based
judgments. Its synthetic nature also ensures scalability and minimizes data
contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis
shows that instruction-tuned models struggle with basic multi-attribute and
multi-hop reasoning. Furthermore, while reasoning-oriented models perform
better, they remain fragile on complex and hybrid structures, with the best
model achieving an average score of only 47% on the challenge subset.
Crucially, models often perform poorly on multi-dimensional data and natural
language task descriptions, highlighting a critical gap for real-world
deployment.

</details>


### [50] [DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals](https://arxiv.org/abs/2505.24085)
*Alireza Jafari, Fereshteh Yousefirizi, Vahid Seydi*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的混合方法来改进心房颤动检测，这种方法结合了深度卷积自动编码器和梯度增强模型，无需手动特征提取即可实现端到端的AF识别。


<details>
  <summary>更多</summary>
  
**动机:** 及时检测心房颤动对于减轻中风相关的发病率至关重要，然而传统的方法存在一些局限性，需要手动特征提取。

**方法:** 本研究采用了一种创新的混合方法，包括一个19层的深度卷积自编码器（DCAE）和三种增强分类器（AdaBoost、XGBoost和LightGBM）。

**结果:** 提出的DCAE-LGBM模型达到了95.20%的F1分数，99.99%的敏感性，并且推理延迟仅为四秒，超过了现有方法并符合临床部署要求。

**结论:** 该研究提出了一种结合无监督深度学习和梯度提升模型的混合方法，显著提高了心房颤动（AF）的检测性能，具有较高的F1分数、灵敏度以及较低的推理延迟，为临床环境中的自动化AF检测提供了一个可靠的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepBoost-AF%3A+A+Novel+Unsupervised+Feature+Learning+and+Gradient+Boosting+Fusion+for+Robust+Atrial+Fibrillation+Detection+in+Raw+ECG+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24085&send_immediately=true&force_search=false)

**原文摘要:** Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with
elevated health risks, where timely detection is pivotal for mitigating
stroke-related morbidity. This study introduces an innovative hybrid
methodology integrating unsupervised deep learning and gradient boosting models
to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is
coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM
(LGBM)-to harness their complementary advantages while addressing individual
limitations. The proposed framework uniquely combines DCAE with gradient
boosting, enabling end-to-end AF identification devoid of manual feature
extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of
99.99%, and inference latency of four seconds, outperforming existing methods
and aligning with clinical deployment requirements. The DCAE integration
significantly enhances boosting models, positioning this hybrid system as a
reliable tool for automated AF detection in clinical settings.

</details>


### [51] [Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting](https://arxiv.org/abs/2505.24088)
*Chen Huang, Skyler Seto, Hadi Pouransari, Mehrdad Farajtabar, Raviteja Vemulapalli, Fartash Faghri, Oncel Tuzel, Barry-John Theobald, Josh Susskind*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的正则化方法Proxy-FDA，用于解决视觉基础模型在微调过程中出现的概念遗忘问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的鲁棒微调方法通常通过匹配原始模型和微调后模型的权重或特征对来保留知识，但这种方法可能过于强烈，忽视了特征邻域结构所编码的丰富知识。

**方法:** 提出了一种新的正则化方法Proxy-FDA，该方法通过使用最近邻图进行特征分布对齐，并利用动态生成的信息代理来增加数据多样性。

**结果:** 实验表明，Proxy-FDA能显著减少微调过程中的概念遗忘，并发现遗忘与一种分布距离度量（相较于L2距离）有很强的相关性。

**结论:** Proxy-FDA能够有效减少微调过程中的概念遗忘问题，并在多种微调设置和任务中展现出优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Proxy-FDA%3A+Proxy-based+Feature+Distribution+Alignment+for+Fine-tuning+Vision+Foundation+Models+without+Forgetting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24088，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24088&send_immediately=true&force_search=false)

**原文摘要:** Vision foundation models pre-trained on massive data encode rich
representations of real-world concepts, which can be adapted to downstream
tasks by fine-tuning. However, fine-tuning foundation models on one task often
leads to the issue of concept forgetting on other tasks. Recent methods of
robust fine-tuning aim to mitigate forgetting of prior knowledge without
affecting the fine-tuning performance. Knowledge is often preserved by matching
the original and fine-tuned model weights or feature pairs. However, such
point-wise matching can be too strong, without explicit awareness of the
feature neighborhood structures that encode rich knowledge as well. We propose
a novel regularization method Proxy-FDA that explicitly preserves the
structural knowledge in feature space. Proxy-FDA performs Feature Distribution
Alignment (using nearest neighbor graphs) between the pre-trained and
fine-tuned feature spaces, and the alignment is further improved by informative
proxies that are generated dynamically to increase data diversity. Experiments
show that Proxy-FDA significantly reduces concept forgetting during
fine-tuning, and we find a strong correlation between forgetting and a
distributional distance metric (in comparison to L2 distance). We further
demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,
few-shot and continual tuning) and across different tasks like image
classification, captioning and VQA.

</details>


### [52] [Practical Bayes-Optimal Membership Inference Attacks](https://arxiv.org/abs/2505.24089)
*Marcus Lassila, Johan Östman, Khac-Hoang Ngo, Alexandre Graell i Amat*

**主要类别:** cs.LG

**AI概要:** 研究提出了一种新的贝叶斯最优节点级成员推理攻击方法，并开发了高效近似算法BASE和G-BASE，其性能优于现有技术，同时计算开销更低。


<details>
  <summary>更多</summary>
  
**动机:** 本文旨在解决当前独立同分布数据和图结构数据中成员推理攻击（MIAs）的不足，尤其是针对图场景中的最优查询策略问题。

**方法:** 我们基于Sablayrolles等人提出的贝叶斯决策理论框架，推导了针对图神经网络的节点级成员推理攻击的贝叶斯最优规则，并提出了BASE和G-BASE两种计算高效的近似方法。

**结果:** G-BASE在图数据上的表现优于之前提出的分类器式节点级MIA攻击方法；BASE在非图数据上也达到了与LiRA和RMIA相当或更优的性能，且计算成本显著更低。

**结论:** BASE和RMIA在特定超参数设置下是等价的，这为RMIA攻击提供了基于贝叶斯最优的原则证明。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Practical+Bayes-Optimal+Membership+Inference+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24089&send_immediately=true&force_search=false)

**原文摘要:** We develop practical and theoretically grounded membership inference attacks
(MIAs) against both independent and identically distributed (i.i.d.) data and
graph-structured data. Building on the Bayesian decision-theoretic framework of
Sablayrolles et al., we derive the Bayes-optimal membership inference rule for
node-level MIAs against graph neural networks, addressing key open questions
about optimal query strategies in the graph setting. We introduce BASE and
G-BASE, computationally efficient approximations of the Bayes-optimal attack.
G-BASE achieves superior performance compared to previously proposed
classifier-based node-level MIA attacks. BASE, which is also applicable to
non-graph data, matches or exceeds the performance of prior state-of-the-art
MIAs, such as LiRA and RMIA, at a significantly lower computational cost.
Finally, we show that BASE and RMIA are equivalent under a specific
hyperparameter setting, providing a principled, Bayes-optimal justification for
the RMIA attack.

</details>


### [53] [A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke](https://arxiv.org/abs/2505.24101)
*Zhenran Xu*

**主要类别:** cs.LG

**AI概要:** 本研究开发了一种可解释的多层集成模型，有效预测了缺血性中风患者的住院时间延长情况，但在出血性中风上的效果仍需进一步验证。


<details>
  <summary>更多</summary>
  
**动机:** 现有机器学习模型在急性中风住院时间预测中的表现欠佳，泛化能力有限，且忽略了系统级因素，因此需要改进预测模型以优化护理规划。

**方法:** 研究通过改进预测因子并开发一个可解释的多层次堆叠集成模型来提升模型效率、性能和可解释性，并使用SHAP图分析模型解释性。

**结果:** 集成模型在缺血性中风预测中表现出优越的性能（AUC: 0.824），显著优于逻辑回归；但在出血性中风预测中虽有较高AUC（0.843），但未显著优于逻辑回归（P=0.136）。

**结论:** 论文得出结论，可解释的集成模型能有效预测缺血性中风患者的住院时间延长情况，但对出血性中风患者需要更大队列进一步验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+SHAP-based+explainable+multi-level+stacking+ensemble+learning+method+for+predicting+the+length+of+stay+in+acute+stroke，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24101&send_immediately=true&force_search=false)

**原文摘要:** Length of stay (LOS) prediction in acute stroke is critical for improving
care planning. Existing machine learning models have shown suboptimal
predictive performance, limited generalisability, and have overlooked
system-level factors. We aimed to enhance model efficiency, performance, and
interpretability by refining predictors and developing an interpretable
multi-level stacking ensemble model. Data were accessed from the biennial
Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models
were developed for ischaemic and haemorrhagic stroke separately. The outcome
was prolonged LOS (the LOS above the 75th percentile). Candidate predictors
(ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical,
and system domains. Feature selection with correlation-based approaches was
used to refine key predictors. The evaluation of models included discrimination
(AUC), calibration curves, and interpretability (SHAP plots). In ischaemic
stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in
haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance
[AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic
regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However,
the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform
logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for
haemorrhagic. SHAP analysis identified shared predictors for both types of
stroke: rehabilitation assessment, urinary incontinence, stroke unit care,
inability to walk independently, physiotherapy, and stroke care coordinators
involvement. An explainable ensemble model effectively predicted the prolonged
LOS in ischaemic stroke. Further validation in larger cohorts is needed for
haemorrhagic stroke.

</details>


### [54] [Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs](https://arxiv.org/abs/2505.24110)
*Sahil Rajesh Dhayalkar*

**主要类别:** cs.LG

**AI概要:** 本研究揭示了非确定性有限自动机与ReLU神经网络的数学等价性，并通过理论和实验证明了其在深度学习架构中的可行性。


<details>
  <summary>更多</summary>
  
**动机:** 将自动机理论与深度学习架构相结合，以探索符号语义在神经网络中的保持能力及理论基础。

**方法:** 通过将自动机状态编码为二进制向量并将转换编码为稀疏线性层，利用ReLU激活函数模拟非确定性分支、子集构造和ε闭包。

**结果:** 理论上证明三层宽度为O(n)的ReLU网络可以精确识别由n状态NFA接受的任何正则语言；实验验证了其与真实自动机的高度一致性。

**结论:** 该论文提出了一种形式化且具有建设性的框架，证明了非确定性有限自动机（NFAs）与标准前馈ReLU神经网络之间的等价性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Networks+as+Universal+Finite-State+Machines%3A+A+Constructive+ReLU+Simulation+Framework+for+NFAs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24110，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24110&send_immediately=true&force_search=false)

**原文摘要:** We present a formal and constructive framework establishing the equivalence
between nondeterministic finite automata (NFAs) and standard feedforward ReLU
neural networks. By encoding automaton states as binary vectors and transitions
as sparse linear layers, we show that ReLU activations simulate
nondeterministic branching, subset construction, and $\epsilon$-closures in a
mathematically precise manner. Our core theoretical results prove that a
three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any
regular language accepted by an $n$-state NFA-without recurrence, memory, or
approximation. Furthermore, we show that gradient descent over
structure-preserving networks preserves symbolic semantics and acceptance
behavior. Extensive experiments across multiple validation tasks-including
parallel path tracking, symbolic subset construction, $\epsilon$-closure
convergence, acceptance classification, structural training invariants, and
functional equivalence-achieve perfect or near-perfect empirical alignment with
ground-truth automata. This work provides the first provably complete symbolic
simulation of NFAs within standard deep learning architectures, uniting
automata theory with neural computation through ReLU dynamics.

</details>


### [55] [AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits](https://arxiv.org/abs/2505.24138)
*Yichen Shi, Ze Zhang, Hongyang Wang, Zhuofu Tao, Zhongyi Li, Bingyu Chen, Yaxin Wang, Zhiping Yu, Ting-Jung Lin, Lei He*

**主要类别:** cs.LG

**AI概要:** 本研究提出了AMSbench基准测试，发现现有大模型在高级电路设计任务上仍有明显不足，强调需要改进模型以更好地理解和应用电路领域的专业知识。


<details>
  <summary>更多</summary>
  
**动机:** 由于模拟/混合信号电路设计自动化面临挑战，而多模态大语言模型的发展为其提供了新的可能性，但目前缺乏一个全面的基准来系统评估这些模型在各种相关任务中的能力。

**方法:** 提出了一种名为AMSbench的基准测试套件，包含约8000个跨难度级别的测试问题，用于评估多模态大语言模型在电路原理图感知、电路分析和电路设计等关键任务上的表现，并测试了包括Qwen 2.5-VL和Gemini 2.5 Pro在内的八种主流模型。

**结果:** 评估结果显示，当前多模态大语言模型在复杂多模态推理和高级电路设计任务中存在明显不足，尤其是在与人类专家相比时表现出性能差距。

**结论:** 当前多模态大语言模型在复杂的电路设计和推理任务中仍存在显著局限性，需要进一步提升其对特定领域知识的理解和应用能力以实现完全自动化的模拟/混合信号电路设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AMSbench%3A+A+Comprehensive+Benchmark+for+Evaluating+MLLM+Capabilities+in+AMS+Circuits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24138，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24138&send_immediately=true&force_search=false)

**原文摘要:** Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated
circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit
design has remained a longstanding challenge due to its difficulty and
complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer
promising potential for supporting AMS circuit analysis and design. However,
current research typically evaluates MLLMs on isolated tasks within the domain,
lacking a comprehensive benchmark that systematically assesses model
capabilities across diverse AMS-related challenges. To address this gap, we
introduce AMSbench, a benchmark suite designed to evaluate MLLM performance
across critical tasks including circuit schematic perception, circuit analysis,
and circuit design. AMSbench comprises approximately 8000 test questions
spanning multiple difficulty levels and assesses eight prominent models,
encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and
Gemini 2.5 Pro. Our evaluation highlights significant limitations in current
MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit
design tasks. These results underscore the necessity of advancing MLLMs'
understanding and effective application of circuit-specific knowledge, thereby
narrowing the existing performance gap relative to human expertise and moving
toward fully automated AMS circuit design workflows. Our data is released at
https://huggingface.co/datasets/wwhhyy/AMSBench

</details>


### [56] [Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction](https://arxiv.org/abs/2505.24145)
*Wilfried Genuist, Éric Savin, Filippo Gatti, Didier Clouteau*

**主要类别:** cs.LG

**AI概要:** 本研究介绍了一种新的流体流动预测方法，结合了条件得分扩散模型和能量约束，通过简单通用的架构实现了高效、准确和物理保真的预测。


<details>
  <summary>更多</summary>
  
**动机:** 受到科学机器学习和生成模型在流体动力学中应用进展的启发，研究者试图开发一种能够在不同场景下高效且准确进行流体流动预测的方法。

**方法:** 该方法结合了能量约束和生成建模，利用随机微分方程设计了一种通用架构，支持即插即用增强功能并实现快速灵活的解决方案生成。

**结果:** 实验结果表明，该模型即使在具有挑战性的湍流条件下也能保持稳定的预测性能，并能够保留关键的物理和统计特性。

**结论:** 论文提出了一种基于条件得分的扩散模型，用于多场景流体流动预测，并展示了其在复杂流体动力学数据集上的稳定、鲁棒和物理保真的预测能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Autoregressive+regularized+score-based+diffusion+models+for+multi-scenarios+fluid+flow+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24145，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24145&send_immediately=true&force_search=false)

**原文摘要:** Building on recent advances in scientific machine learning and generative
modeling for computational fluid dynamics, we propose a conditional score-based
diffusion model designed for multi-scenarios fluid flow prediction. Our model
integrates an energy constraint rooted in the statistical properties of
turbulent flows, improving prediction quality with minimal training, while
enabling efficient sampling at low cost. The method features a simple and
general architecture that requires no problem-specific design, supports
plug-and-play enhancements, and enables fast and flexible solution generation.
It also demonstrates an efficient conditioning mechanism that simplifies
training across different scenarios without demanding a redesign of existing
models. We further explore various stochastic differential equation
formulations to demonstrate how thoughtful design choices enhance performance.
We validate the proposed methodology through extensive experiments on complex
fluid dynamics datasets encompassing a variety of flow regimes and
configurations. Results demonstrate that our model consistently achieves
stable, robust, and physically faithful predictions, even under challenging
turbulent conditions. With properly tuned parameters, it achieves accurate
results across multiple scenarios while preserving key physical and statistical
properties. We present a comprehensive analysis of stochastic differential
equation impact and discuss our approach across diverse fluid mechanics tasks.

</details>


### [57] [RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget](https://arxiv.org/abs/2505.24149)
*Adam Piaseczny, Md Kamran Chowdhury Shisher, Shiqiang Wang, Christopher G. Brinton*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的动态模型更新策略RCCDA，可以在面对概念漂移的同时严格遵守资源约束，并在多个数据集上验证了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的解决方案依赖于漂移检测方法，导致计算开销大且无法提供严格的资源使用保证，因此需要一种更高效的方法来应对概念漂移和资源限制问题。

**方法:** 通过利用过去的损失信息和可调节的漂移阈值，结合Lyapunov drift-plus-penalty框架设计了一种轻量级的动态模型更新策略。

**结果:** 实验结果表明，该方法在三个领域泛化数据集上优于基线方法，在推理准确率方面表现更好，并能严格遵守多种概念漂移调度下的资源约束。

**结论:** RCCDA方法在面对概念漂移时，能够动态优化模型更新策略，并确保符合预定义的资源约束条件，适用于实时ML部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RCCDA%3A+Adaptive+Model+Updates+in+the+Presence+of+Concept+Drift+under+a+Constrained+Resource+Budget，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24149，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24149&send_immediately=true&force_search=false)

**原文摘要:** Machine learning (ML) algorithms deployed in real-world environments are
often faced with the challenge of adapting models to concept drift, where the
task data distributions are shifting over time. The problem becomes even more
difficult when model performance must be maintained under adherence to strict
resource constraints. Existing solutions often depend on drift-detection
methods that produce high computational overhead for resource-constrained
environments, and fail to provide strict guarantees on resource usage or
theoretical performance assurances. To address these shortcomings, we propose
RCCDA: a dynamic model update policy that optimizes ML training dynamics while
ensuring strict compliance to predefined resource constraints, utilizing only
past loss information and a tunable drift threshold. In developing our policy,
we analytically characterize the evolution of model loss under concept drift
with arbitrary training update decisions. Integrating these results into a
Lyapunov drift-plus-penalty framework produces a lightweight policy based on a
measurable accumulated loss threshold that provably limits update frequency and
cost. Experimental results on three domain generalization datasets demonstrate
that our policy outperforms baseline methods in inference accuracy while
adhering to strict resource constraints under several schedules of concept
drift, making our solution uniquely suited for real-time ML deployments.

</details>


### [58] [Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning](https://arxiv.org/abs/2505.24155)
*Ehtesamul Azim, Dongjie Wang, Tae Hyun Hwang, Yanjie Fu, Wei Zhang*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种结合统计选择和生物通路知识的新型两阶段多智能体强化学习框架，用于高维基因组数据中的基因选择，旨在提高预测准确性和生物学可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的特征选择方法虽然能够识别预测基因，但常常忽略复杂的生物通路和调控网络，导致不稳定且生物学上无关的结果。因此需要一种能够在保持统计严谨性的同时整合生物通路知识的方法。

**方法:** 首先，引入了一种基于通路的预过滤策略，利用多种统计方法和KEGG通路信息进行初步降维；其次，在MARL框架下将基因建模为协作智能体，通过图神经网络状态表示、综合预测性能与基因中心性和通路覆盖率的奖励机制以及共享内存和集中式批评组件的协作学习策略来实现基因的精细筛选。

**结果:** 在多个基因表达数据集上的实验表明，该方法在预测准确性和生物学可解释性方面均显著优于传统方法。

**结论:** 论文提出了一种新颖的两阶段框架，结合了统计选择和生物通路知识，利用多智能体强化学习（MARL）在基因选择中同时优化预测能力和生物学相关性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Biological+Pathway+Guided+Gene+Selection+Through+Collaborative+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24155，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24155&send_immediately=true&force_search=false)

**原文摘要:** Gene selection in high-dimensional genomic data is essential for
understanding disease mechanisms and improving therapeutic outcomes.
Traditional feature selection methods effectively identify predictive genes but
often ignore complex biological pathways and regulatory networks, leading to
unstable and biologically irrelevant signatures. Prior approaches, such as
Lasso-based methods and statistical filtering, either focus solely on
individual gene-outcome associations or fail to capture pathway-level
interactions, presenting a key challenge: how to integrate biological pathway
knowledge while maintaining statistical rigor in gene selection? To address
this gap, we propose a novel two-stage framework that integrates statistical
selection with biological pathway knowledge using multi-agent reinforcement
learning (MARL). First, we introduce a pathway-guided pre-filtering strategy
that leverages multiple statistical methods alongside KEGG pathway information
for initial dimensionality reduction. Next, for refined selection, we model
genes as collaborative agents in a MARL framework, where each agent optimizes
both predictive power and biological relevance. Our framework incorporates
pathway knowledge through Graph Neural Network-based state representations, a
reward mechanism combining prediction performance with gene centrality and
pathway coverage, and collaborative learning strategies using shared memory and
a centralized critic component. Extensive experiments on multiple gene
expression datasets demonstrate that our approach significantly improves both
prediction accuracy and biological interpretability compared to traditional
methods.

</details>


### [59] [Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents](https://arxiv.org/abs/2505.24157)
*Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, Jungseul Ok*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的开放世界智能体规划框架REPOA，通过自适应学习和高效探索策略，解决了传统方法依赖不准确知识和低效探索的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法依赖不准确的内部知识或不切实际的环境假设，缺乏从零开始在真实环境中学习规划知识的能力。

**方法:** 引入了REPOA框架，包含三个关键组件：自适应依赖学习、细粒度失败感知操作记忆和基于难度的探索策略。

**结果:** 在两个开放世界测试平台上的评估表明，REPOA具有更强的鲁棒性和高效性，能成功获取高难度的目标物品。

**结论:** REPOA框架在开放世界环境中表现出强大的规划能力和高效的学习性能，能够成功获取先前方法无法实现的高难度目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Don%27t+Just+Follow+MLLM+Plans%3A+Robust+and+Efficient+Planning+for+Open-world+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24157&send_immediately=true&force_search=false)

**原文摘要:** Developing autonomous agents capable of mastering complex, multi-step tasks
in unpredictable, interactive environments presents a significant challenge.
While Large Language Models (LLMs) offer promise for planning, existing
approaches often rely on problematic internal knowledge or make unrealistic
environmental assumptions. Although recent work explores learning planning
knowledge, they still retain limitations due to partial reliance on external
knowledge or impractical setups. Indeed, prior research has largely overlooked
developing agents capable of acquiring planning knowledge from scratch,
directly in realistic settings. While realizing this capability is necessary,
it presents significant challenges, primarily achieving robustness given the
substantial risk of incorporating LLMs' inaccurate knowledge. Moreover,
efficiency is crucial for practicality as learning can demand prohibitive
exploration. In response, we introduce Robust and Efficient Planning for
Open-world Agents (REPOA), a novel framework designed to tackle these issues.
REPOA features three key components: adaptive dependency learning and
fine-grained failure-aware operation memory to enhance robustness to knowledge
inaccuracies, and difficulty-based exploration to improve learning efficiency.
Our evaluation in two established open-world testbeds demonstrates REPOA's
robust and efficient planning, showcasing its capability to successfully obtain
challenging late-game items that were beyond the reach of prior approaches.

</details>


### [60] [Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem](https://arxiv.org/abs/2505.24178)
*Katherine Tieu, Dongqi Fu, Jun Wu, Jingrui He*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于时间图上不变学习的新方法，在解决分布外泛化问题方面表现出色，并在多个现实任务中取得了先进成果。


<details>
  <summary>更多</summary>
  
**动机:** 由于时间图数据违反独立同分布假设，且训练与测试环境之间存在数据分布差异，导致AI模型难以泛化，因此需要研究时间图中最不变且具有代表性的组件。

**方法:** 利用信息瓶颈（IB）方法，设计了一种不变链接选择机制，并推导了一系列可泛化的优化函数，同时结合任务特定损失函数进行训练。

**结果:** 实验表明该方法在引文推荐和商品推荐等实际应用任务中表现优异，达到了最先进的水平，并提供了开源代码。

**结论:** 论文提出了一种基于信息瓶颈方法的误差有界的不变链接选择器，以实现时间图上的鲁棒不变学习，从而提高模型在不同测试场景下的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Invariant+Link+Selector+for+Spatial-Temporal+Out-of-Distribution+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24178，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24178&send_immediately=true&force_search=false)

**原文摘要:** In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,
the data discrepancy between the training environments and testing
environments, hinder AI generalization. Further, relational data like graphs
disobeying the Independent and Identically Distributed (IID) condition makes
the problem more challenging, especially much harder when it is associated with
time. Motivated by this, to realize the robust invariant learning over temporal
graphs, we want to investigate what components in temporal graphs are most
invariant and representative with respect to labels. With the Information
Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector
that can distinguish invariant components and variant components during the
training process to make the deep learning model generalizable for different
testing scenarios. Besides deriving a series of rigorous generalizable
optimization functions, we also equip the training with task-specific loss
functions, e.g., temporal link prediction, to make pretrained models solve
real-world application tasks like citation recommendation and merchandise
recommendation, as demonstrated in our experiments with state-of-the-art (SOTA)
methods. Our code is available at https://github.com/kthrn22/OOD-Linker.

</details>


### [61] [SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](https://arxiv.org/abs/2505.24179)
*Xiaodong Ji, Hailin Zhang, Fangcheng Fu, Bin Cui*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了SALE，一种用于长上下文处理的细粒度稀疏注意力方法。这种方法在模型准确性损失可以忽略不计的同时加速了LLM的预填充阶段，并且无需参数训练就可以无缝集成到现有系统中。


<details>
  <summary>更多</summary>
  
**动机:** 由于自我注意模块在其序列长度的二次时间复杂度导致的瓶颈问题，许多先进的大型语言模型（LLM）应用需要长上下文处理。现有的稀疏注意方法通常对注意力图进行粗粒度检查，从而导致模型准确性的显著损失。

**方法:** SALE通过4位量化查询键产品进行快速准确的细粒度注意力权重估计，接着使用块稀疏注意力来加速预填充计算。为了评估查询键对的重要性，采用了相对注意力分数指标，并为硬件效率实现了定制化的CUDA内核。

**结果:** 实验结果表明，该方法在长上下文基准测试中表现优异，对于超过64K的序列，至少实现了3.36倍的加速，同时保持了模型质量。此外，定制的CUDA内核将额外开销减少到全注意延迟的大约11%。

**结论:** SALE是一种细粒度的稀疏注意力方法，在模型准确性损失可以忽略不计的情况下加速了LLM的长上下文预填充阶段。 SALE无需参数训练，能够无缝集成到现有系统中，并且在长上下文基准测试中证明了该方法在准确性效率权衡方面优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SALE+%3A+Low-bit+Estimation+for+Efficient+Sparse+Attention+in+Long-context+LLM+Prefilling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24179，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24179&send_immediately=true&force_search=false)

**原文摘要:** Many advanced Large Language Model (LLM) applications require long-context
processing, but the self-attention module becomes a bottleneck during the
prefilling stage of inference due to its quadratic time complexity with respect
to sequence length. Existing sparse attention methods accelerate attention
computation by skipping less significant regions of the attention map. However,
these approaches typically perform coarse-grained inspection of the attention
map, rendering considerable loss in model accuracy. In this paper, we propose
SALE, a fine-grained sparse attention method that accelerates the long-context
prefilling stage of LLM with negligible loss in model accuracy. SALE achieves
fast and accurate fine-grained attention weight estimation through 4-bit
quantized query-key products, followed by block-sparse attention to accelerate
prefilling computations. For importance evaluation for query-key pairs, we
adopt our Relative Attention Score metric, which offers significantly higher
efficiency within our framework. We implement a custom CUDA kernel optimized
for our approach for hardware efficiency, reducing the additional overhead to
approximately 11% of the full attention latency. Notably, SALE requires no
parameter training and can be seamlessly integrated into existing systems with
trivial code modifications. Experiments on long-context benchmarks demonstrate
that our method outperforms existing approaches in accuracy-efficiency
trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences
longer than 64K while maintaining model quality.

</details>


### [62] [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.24183)
*Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为CodeV-R1的RLVR框架，用于训练Verilog生成的大语言模型，通过解决验证环境、数据质量和计算成本等问题，在Verilog生成任务上取得了显著成果。


<details>
  <summary>更多</summary>
  
**动机:** 将RLVR扩展到电子设计自动化领域面临诸多挑战，包括缺乏自动且准确的验证环境、高质量自然语言与代码配对数据稀缺以及计算成本高昂。

**方法:** 开发了一个基于规则的测试平台生成器，提出了一种往返数据合成方法，并采用了一个两阶段的“先蒸馏后强化学习”的训练流程。

**结果:** CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上的pass@1分别达到了68.6%和72.9%，超过了之前的最先进水平12~20%。

**结论:** CodeV-R1-7B在Verilog生成任务上取得了显著成果，超过了之前最先进的模型，并促进了EDA和LLM社区的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CodeV-R1%3A+Reasoning-Enhanced+Verilog+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24183&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) trained via reinforcement learning with
verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,
automatable verification, such as software programming and mathematical
problems. Extending RLVR to electronic design automation (EDA), especially
automatically generating hardware description languages (HDLs) like Verilog
from natural-language (NL) specifications, however, poses three key challenges:
the lack of automated and accurate verification environments, the scarcity of
high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To
this end, we introduce CodeV-R1, an RLVR framework for training Verilog
generation LLMs. First, we develop a rule-based testbench generator that
performs robust equivalence checking against golden references. Second, we
propose a round-trip data synthesis method that pairs open-source Verilog
snippets with LLM-generated NL descriptions, verifies code-NL-code consistency
via the generated testbench, and filters out inequivalent examples to yield a
high-quality dataset. Third, we employ a two-stage "distill-then-RL" training
pipeline: distillation for the cold start of reasoning abilities, followed by
adaptive DAPO, our novel RLVR algorithm that can reduce training cost by
adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves
68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,
surpassing prior state-of-the-art by 12~20%, while matching or even exceeding
the performance of 671B DeepSeek-R1. We will release our model, training
pipeline, and dataset to facilitate research in EDA and LLM communities.

</details>


### [63] [Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling](https://arxiv.org/abs/2505.24185)
*Yipan Wei, Yuchen Zou, Yapeng Li, Bo Du*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为FedDEA的联邦解耦聚合方法，该方法能够在不共享本地数据的情况下，有效解决多任务联邦学习中的任务异构性问题，并在多个数据集上验证了其优越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦多任务学习方法大多专注于为每个客户端构建个性化模型，难以将多个异构任务聚合到一个统一模型中，因此在实际场景中难以进行有效的联合训练。

**方法:** FedDEA基于本地更新的响应强度动态识别任务相关维度，并通过重新缩放增强其优化效果，从而实现多任务模型的集成。

**结果:** 实验结果表明，FedDEA可以轻松集成到各种主流联邦优化算法中，并在NYUD-V2和PASCAL-Context数据集上持续提供显著的整体性能提升，验证了其在高度异构任务设置下的鲁棒性和泛化能力。

**结论:** FedDEA是一个不依赖任务标签或架构修改的联邦解耦聚合方法，能够有效抑制跨任务干扰，并在统一全局模型中实现任务级解耦聚合，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Unified+Modeling+in+Federated+Multi-Task+Learning+via+Subspace+Decoupling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24185&send_immediately=true&force_search=false)

**原文摘要:** Federated Multi-Task Learning (FMTL) enables multiple clients performing
heterogeneous tasks without exchanging their local data, offering broad
potential for privacy preserving multi-task collaboration. However, most
existing methods focus on building personalized models for each client and
unable to support the aggregation of multiple heterogeneous tasks into a
unified model. As a result, in real-world scenarios where task objectives,
label spaces, and optimization paths vary significantly, conventional FMTL
methods struggle to achieve effective joint training. To address this
challenge, we propose FedDEA (Federated Decoupled Aggregation), an
update-structure-aware aggregation method specifically designed for multi-task
model integration. Our method dynamically identifies task-relevant dimensions
based on the response strength of local updates and enhances their optimization
effectiveness through rescaling. This mechanism effectively suppresses
cross-task interference and enables task-level decoupled aggregation within a
unified global model. FedDEA does not rely on task labels or architectural
modifications, making it broadly applicable and deployment-friendly.
Experimental results demonstrate that it can be easily integrated into various
mainstream federated optimization algorithms and consistently delivers
significant overall performance improvements on widely used NYUD-V2 and
PASCAL-Context. These results validate the robustness and generalization
capabilities of FedDEA under highly heterogeneous task settings.

</details>


### [64] [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
*Orlando Marquez Ayala, Patrice Bechard, Emily Chen, Maggie Baird, Jingfei Chen*

**主要类别:** cs.LG

**AI概要:** 尽管大型语言模型强大且成本下降，但在特定任务中，小型语言模型经过微调仍具有更高的质量表现。


<details>
  <summary>更多</summary>
  
**动机:** 随着逐token成本降低，小型语言模型的优势可能不再明显，但针对结构化输出的任务仍然存在质量优势。

**方法:** 将微调的小型语言模型与使用提示的大型语言模型进行比较，以生成低代码工作流（JSON格式）作为任务。

**结果:** 良好的提示可以产生合理的结果，但微调可平均提高10%的质量，并通过系统错误分析揭示了模型的局限性。

**结论:** 对于需要结构化输出的领域特定任务，小型语言模型（SLMs）相较于大型语言模型（LLMs）在质量上仍有优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Tune+an+SLM+or+Prompt+an+LLM%3F+The+Case+of+Generating+Low-Code+Workflows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24189&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) such as GPT-4o can handle a wide range of
complex tasks with the right prompt. As per token costs are reduced, the
advantages of fine-tuning Small Language Models (SLMs) for real-world
applications -- faster inference, lower costs -- may no longer be clear. In
this work, we present evidence that, for domain-specific tasks that require
structured outputs, SLMs still have a quality advantage. We compare fine-tuning
an SLM against prompting LLMs on the task of generating low-code workflows in
JSON form. We observe that while a good prompt can yield reasonable results,
fine-tuning improves quality by 10% on average. We also perform systematic
error analysis to reveal model limitations.

</details>


### [65] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/abs/2505.24190)
*Lan-Cuong Nguyen, Quan Nguyen-Tri, Bang Tran Khanh, Dung D. Le, Long Tran-Thanh, Khoat Than*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种解决少样本图像分类问题的新方法，通过理论框架和算法优化，成功弥合了真实数据与合成数据之间的差距。


<details>
  <summary>更多</summary>
  
**动机:** 由于标记训练样本稀缺，少样本图像分类仍然具有挑战性，合成数据增强成为一种有前途的方法，但面临性能下降的问题。

**方法:** 开发了一个理论框架，量化了分布差异对监督学习的影响，并在此框架基础上提出了一种新的算法。

**结果:** 大量实验结果表明，该方法在多个数据集上优于现有技术方法，表现出优越的性能。

**结论:** 论文提出了一种新的基于理论的算法，结合原型学习来优化数据划分和模型训练，有效弥合了真实少量数据与合成数据之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provably+Improving+Generalization+of+Few-Shot+Models+with+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24190&send_immediately=true&force_search=false)

**原文摘要:** Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [66] [Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback](https://arxiv.org/abs/2505.24193)
*Ofir Schlisselberg, Tal Lancewicki, Peter Auer, Yishay Mansour*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+Best-of-Both-Worlds+Regret+for+Bandits+with+Delayed+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24193&send_immediately=true&force_search=false)

**原文摘要:** We study the multi-armed bandit problem with adversarially chosen delays in
the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal
performance in both stochastic and adversarial environments. While prior work
has made progress toward this goal, existing algorithms suffer from significant
gaps to the known lower bounds, especially in the stochastic settings. Our main
contribution is a new algorithm that, up to logarithmic factors, matches the
known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of
$\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic
terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is
the cumulative delay. In the stochastic case, we provide a regret bound which
scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum
\Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$
and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to
simultaneously match the lower bounds in both stochastic and adversarial
regimes in delayed environment. Moreover, even beyond the BoBW setting, our
stochastic regret bound is the first to match the known lower bound under
adversarial delays, improving the second term over the best known result by a
factor of $K$.

</details>


### [67] [Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion](https://arxiv.org/abs/2505.24231)
*Md Shahnawaz, Bishwajit Prasad Gond, Durga Prasad Mohapatra*

**主要类别:** cs.LG

**AI概要:** 本文提出一种基于动态特征提取和卷积神经网络的恶意软件分类框架，具有高准确率和抗逃避能力。


<details>
  <summary>更多</summary>
  
**动机:** 传统静态分析难以应对多态和变形恶意软件，需要有效动态检测方法对抗高级混淆攻击。

**方法:** 提取API参数调用并将其转换为灰度图像，使用卷积神经网络进行分类。

**结果:** 实验结果显示平均准确率为98.36%，能可靠分类恶意软件和良性程序。

**结论:** 动态分析与深度学习结合的方法在恶意软件分类中表现出高准确率和对逃避策略的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Malware+Classification+of+Windows+PE+Files+using+CNNs+and+Greyscale+Images+Derived+from+Runtime+API+Call+Argument+Conversion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24231，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24231&send_immediately=true&force_search=false)

**原文摘要:** Malware detection and classification remains a topic of concern for
cybersecurity, since it is becoming common for attackers to use advanced
obfuscation on their malware to stay undetected. Conventional static analysis
is not effective against polymorphic and metamorphic malware as these change
their appearance without modifying their behavior, thus defying the analysis by
code structure alone. This makes it important to use dynamic detection that
monitors malware behavior at runtime. In this paper, we present a dynamic
malware categorization framework that extracts API argument calls at the
runtime execution of Windows Portable Executable (PE) files. Extracting and
encoding the dynamic features of API names, argument return values, and other
relative features, we convert raw behavioral data to temporal patterns. To
enhance feature portrayal, the generated patterns are subsequently converted
into grayscale pictures using a magma colormap. These improved photos are used
to teach a Convolutional Neural Network (CNN) model discriminative features,
which allows for reliable and accurate malware classification. Results from
experiments indicate that our method, with an average accuracy of 98.36% is
effective in classifying different classes of malware and benign by integrating
dynamic analysis and deep learning. It not only achieves high classification
accuracy but also demonstrates significant resilience against typical evasion
strategies.

</details>


### [68] [Rethinking Continual Learning with Progressive Neural Collapse](https://arxiv.org/abs/2505.24254)
*Zheng Wang, Wanhao Yu, Li Yang, Sen Lin*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的持续学习框架ProNC，通过逐步扩展ETF目标来解决灾难性遗忘问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习方法使用固定的全局ETF目标存在不切实际和性能有限的问题，需要一种更灵活的方法。

**方法:** 设计了ProNC框架，逐步添加新类原型作为新任务的顶点，并结合蒸馏技术平衡旧类和新类的目标。

**结果:** 实验表明，该方法在保持灵活性和效率的同时显著优于基线方法。

**结论:** ProNC为持续学习提供了一个新颖且高效的方法，有效缓解了知识干扰问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Continual+Learning+with+Progressive+Neural+Collapse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24254&send_immediately=true&force_search=false)

**原文摘要:** Continual Learning (CL) seeks to build an agent that can continuously learn a
sequence of tasks, where a key challenge, namely Catastrophic Forgetting,
persists due to the potential knowledge interference among different tasks. On
the other hand, deep neural networks (DNNs) are shown to converge to a terminal
state termed Neural Collapse during training, where all class prototypes
geometrically form a static simplex equiangular tight frame (ETF). These
maximally and equally separated class prototypes make the ETF an ideal target
for model learning in CL to mitigate knowledge interference. Thus inspired,
several studies have emerged very recently to leverage a fixed global ETF in
CL, which however suffers from key drawbacks, such as impracticability and
limited performance.To address these challenges and fully unlock the potential
of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework
that completely removes the need of a fixed global ETF in CL. Specifically,
ProNC progressively expands the ETF target in a principled way by adding new
class prototypes as vertices for new tasks, ensuring maximal separability
across all encountered classes with minimal shifts from the previous ETF. We
next develop a new CL framework by plugging ProNC into commonly used CL
algorithm designs, where distillation is further leveraged to balance between
target shifting for old classes and target aligning for new classes. Extensive
experiments show that our approach significantly outperforms related baselines
while maintaining superior flexibility, simplicity, and efficiency.

</details>


### [69] [On Fairness of Task Arithmetic: The Role of Task Vectors](https://arxiv.org/abs/2505.24262)
*Hiroki Naganuma, Kotaro Yoshida, Laura Gomezjurado Gonzalez, Takafumi Horie, Yuji Naraki, Ryotaro Shimizu*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了任务向量算术操作对模型公平性的影响，并提出了通过调整任务向量系数来实现公平性控制的可能性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管任务向量的方法在计算上有优势，但它们可能会影响模型的公平性，这在仇恨言论检测等敏感应用中存在风险。然而，任务算术的公平性影响尚未得到充分研究。

**方法:** 作者通过基准测试任务算术与全微调和低秩自适应（LoRA）方法进行比较，并探索了从易受仇恨言论影响的人口子群体中合并任务向量的效果。

**结果:** 研究提供了关于模型编辑公平性影响的新见解，并表明通过调整任务向量系数可以控制公平性结果，从而实现定制化的模型行为。

**结论:** 论文得出结论，任务向量的修改对模型公平性有显著影响，并为实现负责任的模型编辑实践奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Fairness+of+Task+Arithmetic%3A+The+Role+of+Task+Vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24262&send_immediately=true&force_search=false)

**原文摘要:** Model editing techniques, particularly task arithmetic using task vectors,
have shown promise in efficiently modifying pre-trained models through
arithmetic operations like task addition and negation. Despite computational
advantages, these methods may inadvertently affect model fairness, creating
risks in sensitive applications like hate speech detection. However, the
fairness implications of task arithmetic remain largely unexplored, presenting
a critical gap in the existing literature. We systematically examine how
manipulating task vectors affects fairness metrics, including Demographic
Parity and Equalized Odds. To rigorously assess these effects, we benchmark
task arithmetic against full fine-tuning, a costly but widely used baseline,
and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning
method. Additionally, we explore merging task vectors from models fine-tuned on
demographic subgroups vulnerable to hate speech, investigating whether fairness
outcomes can be controlled by adjusting task vector coefficients, potentially
enabling tailored model behavior. Our results offer novel insights into the
fairness implications of model editing and establish a foundation for
fairness-aware and responsible model editing practices.

</details>


### [70] [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
*James R. Golden*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种方法，通过改变梯度计算，将大型语言模型（LLMs）的推理操作映射到一个等效的线性系统中，从而揭示了其内部表示和可解释语义结构。


<details>
  <summary>更多</summary>
  
**动机:** 作者试图理解现代LLMs的内部机制，并探索其如何进行下一个词预测。他们借鉴了图像扩散模型中的局部或分段线性特性，希望找到一种方法来解析这些高度非线性的模型。

**方法:** 作者扩展了来自图像扩散模型的技术，针对给定输入序列的下一个词预测问题，战略性地改变了梯度计算，使得模型的雅可比矩阵几乎完全再现了前向预测的线性系统。

**结果:** 该方法在多个LLM上得到了验证，包括Llama 3、Gemma 3、Qwen 3、Phi 4、Mistral Ministral 和 OLMo 2，最大至Llama 3.3 70B Q4。通过对分离的雅可比矩阵进行奇异值分解，发现这些LLM在极低维子空间中运行，其中许多最大的奇异向量解码为与最可能输出词相关的概念。此外，该方法还允许将每个连续层及其注意力和MLP组件检查为几乎精确的线性系统，观察语义概念的出现。

**结论:** 尽管现代LLMs具有强大的表达能力和全局非线性，但它们可以通过几乎精确的局部线性分解来解读，从而提供对其内部表示的洞察，并揭示下一个词预测过程中的可解释语义结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+are+Locally+Linear+Mappings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24293，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24293&send_immediately=true&force_search=false)

**原文摘要:** We demonstrate that the inference operations of several open-weight large
language models (LLMs) can be mapped to an exactly equivalent linear system for
an input sequence without modifying the model weights or altering output
predictions. Extending techniques from image diffusion models that exhibit
local or piecewise linearity, we strategically alter the gradient computation
with respect to a given input sequence for a next-token prediction such that
the Jacobian of the model nearly exactly reproduces the forward prediction with
a linear system. We demonstrate this approach across models (Llama 3, Gemma 3,
Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show
through the singular value decomposition of the detached Jacobian that these
LLMs operate in extremely low-dimensional subspaces where many of the largest
singular vectors decode to concepts related to the most-likely output token.
This approach also allows us to examine the operation of each successive layer
(and its attention and MLP components) as nearly-exact linear systems and
observe the emergence of semantic concepts. Despite their expressive power and
global nonlinearity, modern LLMs can be interpreted through nearly-exact
locally linear decompositions that provide insights into their internal
representations and reveal interpretable semantic structures in the next-token
prediction process.

</details>


### [71] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/abs/2505.24298)
*Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为AReaL的全新异步强化学习系统，通过异步生成与训练及多项优化措施，显著提高了训练效率并降低了硬件资源浪费。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大规模RL系统为同步方式，导致GPU利用率低且训练效率低下，因此需要一种更高效的异步训练系统来解决这些问题。

**方法:** AReaL采用了完全异步的RL系统架构，将生成与训练完全解耦，并引入了系统级优化、工作负载平衡机制以及一种处理过时样本的PPO变体算法。

**结果:** 实验表明，AReaL相较于同步系统，在数学和代码推理基准上取得了高达2.57倍的训练速度提升，同时最终性能得到保持或提高。

**结论:** AReaL通过异步生成和训练方法提高了强化学习的效率，并在相同GPU资源下实现了高达2.57倍的训练加速，同时保持或提升了最终性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AReaL%3A+A+Large-Scale+Asynchronous+Reinforcement+Learning+System+for+Language+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24298，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24298&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has become a trending paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are synchronous
by alternating generation and training in a batch setting, where the rollouts
in each training batch are generated by the same (or latest) model. This
stabilizes RL training but suffers from severe system-level inefficiency.
Generation must wait until the longest output in the batch is completed before
model update, resulting in GPU underutilization. We present AReaL, a
\emph{fully asynchronous} RL system that completely decouples generation from
training. Rollout workers in AReaL continuously generate new outputs without
waiting, while training workers update the model whenever a batch of data is
collected. AReaL also incorporates a collection of system-level optimizations,
leading to substantially higher GPU utilization. To stabilize RL training,
AReaL balances the workload of rollout and training workers to control data
staleness, and adopts a staleness-enhanced PPO variant to better handle
outdated training samples. Extensive experiments on math and code reasoning
benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training
speedup} compared to the best synchronous systems with the same number of GPUs
and matched or even improved final performance. The code of AReaL is available
at https://github.com/inclusionAI/AReaL/.

</details>


### [72] [On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective](https://arxiv.org/abs/2505.24313)
*Gengze Xu, Wei Yao, Ziqiao Wang, Yong Liu*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了弱到强泛化现象，发现学生模型通过接近后验均值教师、避免过拟合以及使用反向交叉熵损失能够超越教师性能。


<details>
  <summary>更多</summary>
  
**动机:** 最近的研究将性能增益归因于学生与教师模型之间的预测不匹配，本文旨在从理论上探究这种现象的发生机制。

**方法:** 通过Bregman散度的广义偏差-方差分解对W2SG进行理论分析，并使用具体示例演示学生模型如何收敛到后验均值教师。

**结果:** 论文显示了学生和教师之间预期人口风险差距由两者模型间的预期不匹配量化，且在学生模型比教师具有显著更大容量时更容易实现W2SG。

**结论:** 研究表明，当学生模型近似其后验均值教师而不是模仿个别教师时，更可能出现弱到强泛化（W2SG）。此外，避免过度拟合教师监督和减少学生预测的熵可以进一步促进W2SG。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Emergence+of+Weak-to-Strong+Generalization%3A+A+Bias-Variance+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24313，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24313&send_immediately=true&force_search=false)

**原文摘要:** Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong
student model, trained on a dataset labeled by a weak teacher, ultimately
outperforms the teacher on the target task. Recent studies attribute this
performance gain to the prediction misfit between the student and teacher
models. In this work, we theoretically investigate the emergence of W2SG
through a generalized bias-variance decomposition of Bregman divergence.
Specifically, we show that the expected population risk gap between the student
and teacher is quantified by the expected misfit between the two models. While
this aligns with previous results, our analysis removes several restrictive
assumptions, most notably, the convexity of the student's hypothesis class,
required in earlier works. Moreover, we show that W2SG is more likely to emerge
when the student model approximates its posterior mean teacher, rather than
mimicking an individual teacher. Using a concrete example, we demonstrate that
if the student model has significantly larger capacity than the teacher, it can
indeed converge to this posterior mean. Our analysis also suggests that
avoiding overfitting to the teacher's supervision and reducing the entropy of
student's prediction further facilitate W2SG. In addition, we show that the
reverse cross-entropy loss, unlike the standard forward cross-entropy, is less
sensitive to the predictive uncertainty of the teacher. Finally, we empirically
verify our theoretical insights and demonstrate that incorporating the reverse
cross-entropy loss consistently improves student performance.

</details>


### [73] [ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving](https://arxiv.org/abs/2505.24317)
*Yongming Chen, Miner Chen, Liewen Liao, Mingyang Jiang, Xiang Zuo, Hengrui Zhang, Yuchen Xi, Songan Zhang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种将交通规则融入强化学习框架的责任导向奖励函数，通过自动化奖励分配提升了自动驾驶系统的安全性与合规性。


<details>
  <summary>更多</summary>
  
**动机:** 传统强化学习方法在自动驾驶中因手动设计奖励函数存在困难而在复杂场景下表现有限，因此需要一种更有效的奖励机制。

**方法:** 引入了交通规则知识图谱，并利用视觉-语言模型与检索增强生成技术自动化分配奖励值。

**结果:** 实验表明该方法显著提高了事故责任分配的准确性，并有效降低了智能体在交通事故中的责任比例。

**结论:** 本文提出的基于责任导向的奖励函数结合交通规则知识图谱和视觉-语言模型，能够有效提高自动驾驶系统在复杂环境中的决策性能并减少违规行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ROAD%3A+Responsibility-Oriented+Reward+Design+for+Reinforcement+Learning+in+Autonomous+Driving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24317，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24317&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) in autonomous driving employs a trial-and-error
mechanism, enhancing robustness in unpredictable environments. However,
crafting effective reward functions remains challenging, as conventional
approaches rely heavily on manual design and demonstrate limited efficacy in
complex scenarios. To address this issue, this study introduces a
responsibility-oriented reward function that explicitly incorporates traffic
regulations into the RL framework. Specifically, we introduced a Traffic
Regulation Knowledge Graph and leveraged Vision-Language Models alongside
Retrieval-Augmented Generation techniques to automate reward assignment. This
integration guides agents to adhere strictly to traffic laws, thus minimizing
rule violations and optimizing decision-making performance in diverse driving
conditions. Experimental validations demonstrate that the proposed methodology
significantly improves the accuracy of assigning accident responsibilities and
effectively reduces the agent's liability in traffic incidents.

</details>


### [74] [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/abs/2505.24324)
*Ivan Petrukha, Yana Kurliak, Nataliia Stulova*

**主要类别:** cs.LG

**AI概要:** 本文介绍了SwiftEval，一个专为Swift编程语言设计的代码生成基准测试，揭示了现有基准测试的局限性，并展示了模型在Swift特定问题上的表现差距。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多语言基准测试（如HumanEval-XL和MultiPL-E）主要基于自动翻译Python基准测试生成，对Swift等其他编程语言的评估不够准确或相关。因此需要一种专注于Swift的高质量基准测试。

**方法:** 该研究通过手动创建28个精心设计的问题构建了SwiftEval基准测试，并在44种流行的代码生成大型语言模型上进行了评估。

**结果:** 结果表明，在需要语言特定功能的问题上，尤其是较小规模的模型，LLM得分显著下降，这凸显了SwiftEval对于衡量Swift代码生成能力的重要性。

**结论:** 论文得出结论，现有的多语言基准测试在Swift编程语言评估方面存在不足，而SwiftEval提供了一个更准确和具体的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwiftEval%3A+Developing+a+Language-Specific+Benchmark+for+LLM-generated+Code+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24324，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24324&send_immediately=true&force_search=false)

**原文摘要:** In recent years, large language models (LLMs) have showcased significant
advancements in code generation. However, most evaluation benchmarks are
primarily oriented towards Python, making it difficult to evaluate other
programming languages, such as Swift, with high quality. By examining widely
established multilingual benchmarks like HumanEval-XL and MultiPL-E, we
identified critical issues specific to their Swift components, making them
insufficient or even irrelevant for assessing LLM coding capabilities on Swift.
Unlike these existing approaches, which prioritize rapid scaling and
generalization by automatically translating Python-centric benchmarks with
LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the
first Swift-oriented benchmark consisting of 28 carefully hand-crafted
problems, and evaluate 44 popular Code LLMs on it. Our results show significant
LLM scores drop for problems requiring language-specific features, most
noticeable in the models of smaller sizes.

</details>


### [75] [Cartan Networks: Group theoretical Hyperbolic Deep Learning](https://arxiv.org/abs/2505.24353)
*Federico Milanesio, Matteo Santoro, Pietro G. Fré, Guido Sanguinetti*

**主要类别:** cs.LG

**AI概要:** 本文通过利用双曲空间的数学特性，特别是其可解群结构，提出了一种新类型的双曲深度学习方法——Cartan网络，这种方法在实验中表现良好，并为双曲深度学习领域带来了新的可能性。


<details>
  <summary>更多</summary>
  
**动机:** 为了开发高效且信息丰富的层次数据嵌入，论文关注于双曲空间的对称空间构造所自然产生的可解群结构。

**方法:** 利用双曲空间的可解群结构，将群同态与保持度量的微分同胚交错起来设计新算法。

**结果:** 提出了一种新的双曲深度学习算法类，即Cartan网络，并在多个基准数据集上显示出了有希望的结果。

**结论:** 论文提出了名为Cartan网络的新一类双曲深度学习算法，并展示了其在各种基准数据集上的前景，开辟了新的双曲深度学习架构类别。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cartan+Networks%3A+Group+theoretical+Hyperbolic+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24353&send_immediately=true&force_search=false)

**原文摘要:** Hyperbolic deep learning leverages the metric properties of hyperbolic spaces
to develop efficient and informative embeddings of hierarchical data. Here, we
focus on the solvable group structure of hyperbolic spaces, which follows
naturally from their construction as symmetric spaces. This dual nature of Lie
group and Riemannian manifold allows us to propose a new class of hyperbolic
deep learning algorithms where group homomorphisms are interleaved with
metric-preserving diffeomorphisms. The resulting algorithms, which we call
Cartan networks, show promising results on various benchmark data sets and open
the way to a novel class of hyperbolic deep learning architectures.

</details>


### [76] [ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](https://arxiv.org/abs/2505.24357)
*Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, Xiaokang Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为ReCalKV的KV缓存压缩方法，通过分别针对Keys和Values设计优化的压缩策略，有效降低了大语言模型在长上下文推理中的内存消耗，并取得了优于现有方法的性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大语言模型在处理长上下文时受限于存储Key-Value (KV) 缓存所需的大量内存，因此需要对KV缓存进行压缩以提高效率。

**方法:** ReCalKV采用了针对Keys和Values的不同压缩策略：对于Keys，提出了基于头相似性重排序（HSR）的分组SVD方法；对于Values，则采用离线校准与矩阵融合（OCMF）方法进行压缩。

**结果:** 实验结果表明，与现有的低秩压缩方法相比，ReCalKV在压缩率和性能损失方面均表现更优。

**结论:** ReCalKV是一种有效的KV缓存压缩方法，能够在保持最小性能损失的同时实现高压缩比，为高效长上下文推理提供了可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReCalKV%3A+Low-Rank+KV+Cache+Compression+via+Head+Reordering+and+Offline+Calibration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24357，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24357&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have achieved remarkable performance, yet their
capability on long-context reasoning is often constrained by the excessive
memory required to store the Key-Value (KV) cache. This makes KV cache
compression an essential step toward enabling efficient long-context reasoning.
Recent methods have explored reducing the hidden dimensions of the KV cache,
but many introduce additional computation through projection layers or suffer
from significant performance degradation under high compression ratios. To
address these challenges, we propose ReCalKV, a post-training KV cache
compression method that reduces the hidden dimensions of the KV cache. We
develop distinct compression strategies for Keys and Values based on their
different roles and varying importance in the attention mechanism. For Keys, we
propose Head-wise Similarity-aware Reordering (HSR), which clusters similar
heads and applies grouped SVD to the key projection matrix, reducing additional
computation while preserving accuracy. For Values, we propose Offline
Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra
computational overhead. Experiments show that ReCalKV outperforms existing
low-rank compression methods, achieving high compression ratios with minimal
performance loss. Code is available at:
https://github.com/XIANGLONGYAN/ReCalKV.

</details>


### [77] [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
*Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy*

**主要类别:** cs.LG

**AI概要:** 该论文研究了稀疏自编码器（SAEs）和ITDA在大型文本到图像扩散模型Flux 1中的应用，发现它们在可解释性和重建能力上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 稀疏自编码器已被成功应用于视觉变换器图像编码器和小规模扩散模型，但尚未广泛应用于大型文本到图像扩散模型。本文旨在探索其在Flux 1模型中的应用。

**方法:** 将稀疏自编码器（SAEs）和推理时激活分解（ITDA）应用于大型文本到图像扩散模型Flux 1，并引入视觉自动化解释管道来评估其可解释性。

**结果:** SAEs能够准确重建残差流嵌入，并在可解释性方面优于MLP神经元；同时，ITDA的可解释性与SAEs相当。

**结论:** 稀疏自编码器（SAEs）和ITDA在Flux 1模型中的嵌入具有可解释性，并且SAEs能够在图像生成中通过激活加法进行引导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpreting+Large+Text-to-Image+Diffusion+Models+with+Dictionary+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24360&send_immediately=true&force_search=false)

**原文摘要:** Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.

</details>


### [78] [Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm](https://arxiv.org/abs/2505.24365)
*Vardhan Shorewala, Shivam Shorewala*

**主要类别:** cs.LG

**AI概要:** 该论文提出一种改进的聚类算法，能有效提高聚类紧密度并用于异常检测，在多个评估指标上表现优于传统方法。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决传统k-means算法在聚类效果和异常检测方面的局限性，作者提出了一种更优的迭代方法。

**方法:** 提出了一种迭代减少N个聚类内部方差的新算法，直到达到全局最小值，并通过识别引起显著方差增加的数据点来进行异常检测。

**结果:** 在合成数据和UCI Wine Quality数据集上，方差分别减少了18.7%和88.1%，并且在Wine Quality数据集上的准确率和F1分数分别提升了22.5%和20.8%。

**结论:** 论文提出了一种新的聚类优化和异常检测方法，在多个数据集上取得了显著的性能提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Anomaly+Detection+and+Improvement+of+Clusters+using+Enhanced+K-Means+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24365&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a unified approach to cluster refinement and anomaly
detection in datasets. We propose a novel algorithm that iteratively reduces
the intra-cluster variance of N clusters until a global minimum is reached,
yielding tighter clusters than the standard k-means algorithm. We evaluate the
method using intrinsic measures for unsupervised learning, including the
silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and
extend it to anomaly detection by identifying points whose assignment causes a
significant variance increase. External validation on synthetic data and the
UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity
score, V-measure, and F1 score. Results show variance reductions of 18.7% and
88.1% on the synthetic and Wine Quality datasets, respectively, along with
accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality
dataset.

</details>


### [79] [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/abs/2505.24369)
*Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Adversarial Preference Learning (APL) 的新方法，以提高语言模型的安全性和鲁棒性，同时减少了对人工反馈的依赖。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于人类反馈的强化学习方法在效率、成本和反馈偏差方面存在局限性，同时语言模型容易受到对抗攻击。

**方法:** 通过一种迭代的对抗偏好学习方法，结合三种创新：直接有害性度量、条件生成攻击者和自动化闭环反馈框架。

**结果:** 实验表明，该方法在Mistral-7B-Instruct-v0.3模型上显著提升了鲁棒性，并保持了较高的实用性。

**结论:** 论文提出了一种新的对抗训练方法APL，用于增强语言模型的安全性和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Preference+Learning+for+Robust+LLM+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24369，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24369&send_immediately=true&force_search=false)

**原文摘要:** Modern language models often rely on Reinforcement Learning from Human
Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to
adversarial attacks due to three key limitations: (1) the inefficiency and high
cost of human annotation, (2) the vast diversity of potential adversarial
attacks, and (3) the risk of feedback bias and reward hacking. To address these
challenges, we introduce Adversarial Preference Learning (APL), an iterative
adversarial training method incorporating three key innovations. First, a
direct harmfulness metric based on the model's intrinsic preference
probabilities, eliminating reliance on external assessment. Second, a
conditional generative attacker that synthesizes input-specific adversarial
variations. Third, an iterative framework with automated closed-loop feedback,
enabling continuous adaptation through vulnerability discovery and mitigation.
Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly
enhances robustness, achieving 83.33% harmlessness win rate over the base model
(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured
by LLaMA-Guard), and lowering attack success rate by up to 65% according to
HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score
of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against
the base model.

</details>


### [80] [Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer](https://arxiv.org/abs/2505.24378)
*Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao*

**主要类别:** cs.LG

**AI概要:** 本文提出M3DT，一种基于混合专家的新框架，有效解决了大规模任务下的多任务强化学习问题，实现了模型和任务数量的成功扩展。


<details>
  <summary>更多</summary>
  
**动机:** 尽管离线多任务强化学习(Transformer架构)已有进展，但大多数方法只关注有限数量的任务，而如何扩展到大量任务仍是重大挑战。

**方法:** 提出M3DT，一种新的混合专家框架，增强代理的架构和优化，使用MoE加强Decision Transformer骨干，并引入三阶段训练机制以实现高效训练。

**结果:** 实验结果显示，M3DT在增加专家数量时，不仅在固定任务数上持续提升性能，而且展现出了显著的任务扩展能力，成功应用于160个任务。

**结论:** M3DT通过增加专家数量，在固定任务数量下提升了模型性能，并展示了显著的任务扩展能力，成功扩展至160项任务并保持优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mastering+Massive+Multi-Task+Reinforcement+Learning+via+Mixture-of-Expert+Decision+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24378，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24378&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advancements in offline multi-task reinforcement learning
(MTRL) have harnessed the powerful capabilities of the Transformer
architecture, most approaches focus on a limited number of tasks, with scaling
to extremely massive tasks remaining a formidable challenge. In this paper, we
first revisit the key impact of task numbers on current MTRL method, and
further reveal that naively expanding the parameters proves insufficient to
counteract the performance degradation as the number of tasks escalates.
Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE)
framework that tackles task scalability by further unlocking the model's
parameter scalability. Specifically, we enhance both the architecture and the
optimization of the agent, where we strengthen the Decision Transformer (DT)
backbone with MoE to reduce task load on parameter subsets, and introduce a
three-stage training mechanism to facilitate efficient training with optimal
performance. Experimental results show that, by increasing the number of
experts, M3DT not only consistently enhances its performance as model expansion
on the fixed task numbers, but also exhibits remarkable task scalability,
successfully extending to 160 tasks with superior performance.

</details>


### [81] [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379)
*Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu*

**主要类别:** cs.LG

**AI概要:** 这篇论文挑战了精确遗忘作为隐私保护黄金标准的假设，并提出了一种新的数据提取攻击方法，显著提高了提取成功率，表明遗忘方法可能存在更大的隐私泄露风险。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型可能无意中包含有害或敏感的个人信息，因此提出了遗忘方法以删除特定数据对模型的影响。然而，精确遗忘是否真正安全仍是一个待解的问题。

**方法:** 通过结合预遗忘和后遗忘模型，利用预遗忘模型的信号指导后遗忘模型，并结合令牌过滤策略，提出了一种新的数据提取攻击方法。

**结果:** 所提出的攻击方法显著提高了数据提取的成功率，在MUSE、TOFU和WMDP等基准测试中性能提高了一倍。此外，在模拟医疗诊断数据集上的实验也证明了其现实世界中的隐私风险。

**结论:** 本文挑战了精确遗忘是隐私保护黄金标准的假设，并提出了一种新的数据提取攻击方法，该方法即使在精确遗忘的情况下也能危及隐私。论文建议遗忘方法的评估应考虑更广泛的威胁模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+the+Gold+Standard%3A+Extracting+Forgotten+Data+under+Exact+Unlearning+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24379，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24379&send_immediately=true&force_search=false)

**原文摘要:** Large language models are typically trained on datasets collected from the
web, which may inadvertently contain harmful or sensitive personal information.
To address growing privacy concerns, unlearning methods have been proposed to
remove the influence of specific data from trained models. Of these, exact
unlearning -- which retrains the model from scratch without the target data --
is widely regarded the gold standard, believed to be robust against
privacy-related attacks. In this paper, we challenge this assumption by
introducing a novel data extraction attack that compromises even exact
unlearning. Our method leverages both the pre- and post-unlearning models: by
guiding the post-unlearning model using signals from the pre-unlearning model,
we uncover patterns that reflect the removed data distribution. Combining model
guidance with a token filtering strategy, our attack significantly improves
extraction success rates -- doubling performance in some cases -- across common
benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our
attack's effectiveness on a simulated medical diagnosis dataset to highlight
real-world privacy risks associated with exact unlearning. In light of our
findings, which suggest that unlearning may, in a contradictory way, increase
the risk of privacy leakage, we advocate for evaluation of unlearning methods
to consider broader threat models that account not only for post-unlearning
models but also for adversarial access to prior checkpoints.

</details>


### [82] [LightSAM: Parameter-Agnostic Sharpness-Aware Minimization](https://arxiv.org/abs/2505.24399)
*Yifei Cheng, Li Shen, Hao Sun, Nan Yin, Xiaochun Cao, Enhong Chen*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightSAM%3A+Parameter-Agnostic+Sharpness-Aware+Minimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24399&send_immediately=true&force_search=false)

**原文摘要:** Sharpness-Aware Minimization (SAM) optimizer enhances the generalization
ability of the machine learning model by exploring the flat minima landscape
through weight perturbations. Despite its empirical success, SAM introduces an
additional hyper-parameter, the perturbation radius, which causes the
sensitivity of SAM to it. Moreover, it has been proved that the perturbation
radius and learning rate of SAM are constrained by problem-dependent parameters
to guarantee convergence. These limitations indicate the requirement of
parameter-tuning in practical applications. In this paper, we propose the
algorithm LightSAM which sets the perturbation radius and learning rate of SAM
adaptively, thus extending the application scope of SAM. LightSAM employs three
popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to
replace the SGD optimizer for weight perturbation and model updating, reducing
sensitivity to parameters. Theoretical results show that under weak
assumptions, LightSAM could converge ideally with any choices of perturbation
radius and learning rate, thus achieving parameter-agnostic. We conduct
preliminary experiments on several deep learning tasks, which together with the
theoretical findings validate the the effectiveness of LightSAM.

</details>


### [83] [Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation](https://arxiv.org/abs/2505.24415)
*Andreas Spilz, Heiko Oppel, Michael Munz*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种创新的数据增强技术，利用肌肉骨骼仿真生成高质量IMU数据，解决了深度学习在运动评估中的关键挑战，提升了模型性能和实用性。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度学习模型在使用IMU数据评估运动质量时受到数据有限、类别不平衡和标签模糊的限制，因此需要一种有效且实用的数据增强方法。

**方法:** 该方法结合了逆运动学参数与基于知识的评估策略，确保生物力学合理性，并实现自动、可靠的标签生成。

**结果:** 实验结果表明，所提出的增强数据与真实数据非常接近，显著提高了神经网络模型的分类准确性和泛化能力，尤其是在患者特异性微调场景中效果明显。

**结论:** 论文提出了一种新颖的数据增强方法，通过集成肌肉骨骼模拟和系统修改运动轨迹来生成真实的IMU数据，解决了深度学习模型在物理治疗运动评估中的常见问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Boosting+Automatic+Exercise+Evaluation+Through+Musculoskeletal+Simulation-Based+IMU+Data+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24415&send_immediately=true&force_search=false)

**原文摘要:** Automated evaluation of movement quality holds significant potential for
enhancing physiotherapeutic treatments and sports training by providing
objective, real-time feedback. However, the effectiveness of deep learning
models in assessing movements captured by inertial measurement units (IMUs) is
often hampered by limited data availability, class imbalance, and label
ambiguity. In this work, we present a novel data augmentation method that
generates realistic IMU data using musculoskeletal simulations integrated with
systematic modifications of movement trajectories. Crucially, our approach
ensures biomechanical plausibility and allows for automatic, reliable labeling
by combining inverse kinematic parameters with a knowledge-based evaluation
strategy. Extensive evaluations demonstrate that augmented variants closely
resembles real-world data, significantly improving the classification accuracy
and generalization capability of neural network models. Additionally, we
highlight the benefits of augmented data for patient-specific fine-tuning
scenarios, particularly when only limited subject-specific training examples
are available. Our findings underline the practicality and efficacy of this
augmentation method in overcoming common challenges faced by deep learning
applications in physiotherapeutic exercise evaluation.

</details>


### [84] [On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets](https://arxiv.org/abs/2505.24403)
*Giannis Nikolentzos, Konstantinos Skianis*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了处理多重集数据的神经网络的Lipschitz连续性问题，得出了聚合函数在特定条件下才具备Lipschitz连续性的结论，并据此推导了整个模型的Lipschitz常数上界，还通过实验验证了理论分析的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地理解和估计处理集合或多重集数据的神经网络的Lipschitz常数，从而评估其鲁棒性和泛化能力。

**方法:** 作者分析了三种无序多重集的距离函数下聚合函数的Lipschitz连续性，并基于这些结果推导出相应神经网络的Lipschitz常数上界，同时通过实验验证理论分析。

**结果:** 发现每种聚合函数通常只在一种距离函数下具有Lipschitz连续性，并得出了相关神经网络的Lipschitz常数上界及其对扰动和分布偏移的稳定性的实证验证。

**结论:** 本文得出了一些聚合函数仅在一种距离函数下是Lipschitz连续的，并推导了能够处理向量多重集的神经网络的Lipschitz常数的上界，同时研究了这些网络对扰动的稳定性以及在分布偏移下的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Lipschitz+Continuity+of+Set+Aggregation+Functions+and+Neural+Networks+for+Sets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24403，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24403&send_immediately=true&force_search=false)

**原文摘要:** The Lipschitz constant of a neural network is connected to several important
properties of the network such as its robustness and generalization. It is thus
useful in many settings to estimate the Lipschitz constant of a model. Prior
work has focused mainly on estimating the Lipschitz constant of multi-layer
perceptrons and convolutional neural networks. Here we focus on data modeled as
sets or multisets of vectors and on neural networks that can handle such data.
These models typically apply some permutation invariant aggregation function,
such as the sum, mean or max operator, to the input multisets to produce a
single vector for each input sample. In this paper, we investigate whether
these aggregation functions are Lipschitz continuous with respect to three
distance functions for unordered multisets, and we compute their Lipschitz
constants. In the general case, we find that each aggregation function is
Lipschitz continuous with respect to only one of the three distance functions.
Then, we build on these results to derive upper bounds on the Lipschitz
constant of neural networks that can process multisets of vectors, while we
also study their stability to perturbations and generalization under
distribution shifts. To empirically verify our theoretical analysis, we conduct
a series of experiments on datasets from different domains.

</details>


### [85] [Learning Safety Constraints for Large Language Models](https://arxiv.org/abs/2505.24445)
*Xin Chen, Yarden As, Andreas Krause*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了名为SaP的几何框架，通过在模型的表示空间中构建安全多面体，实现对大型语言模型的安全约束，有效提升其安全性和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型虽然功能强大，但存在有害输出和易受对抗攻击的重大安全隐患。因此需要一种有效的安全机制来增强LLM的安全性。

**方法:** 提出了一种称为SaP（Safety Polytope）的几何方法，在模型的表示空间中直接学习和实施多个安全约束，通过多面体的面来识别安全和不安全区域，从而实现对不安全输出的检测和纠正。

**结果:** 实验结果显示，该方法可以有效检测不道德输入、降低对抗攻击成功率，同时维持模型在标准任务上的性能。此外，学习到的多面体结构展现了针对不同安全语义的专业化特性。

**结论:** 论文得出结论，SaP方法能够有效地检测不安全输入并减少对抗攻击的成功率，同时保持模型性能。此外，通过分析学习到的多面体面，揭示了在检测不同语义安全性方面的专业化发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Safety+Constraints+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24445&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have emerged as powerful tools but pose
significant safety risks through harmful outputs and vulnerability to
adversarial attacks. We propose SaP, short for Safety Polytope, a geometric
approach to LLM safety that learns and enforces multiple safety constraints
directly in the model's representation space. We develop a framework that
identifies safe and unsafe regions via the polytope's facets, enabling both
detection and correction of unsafe outputs through geometric steering. Unlike
existing approaches that modify model weights, SaP operates post-hoc in the
representation space, preserving model capabilities while enforcing safety
constraints. Experiments across multiple LLMs demonstrate that our method can
effectively detect unethical inputs, reduce adversarial attack success rates
while maintaining performance on standard tasks, thus highlighting the
importance of having an explicit geometric model for safety. Analysis of the
learned polytope facets reveals emergence of specialization in detecting
different semantic notions of safety, providing interpretable insights into how
safety is captured in LLMs' representation space.

</details>


### [86] [Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data](https://arxiv.org/abs/2505.24413)
*Yang Sui, Qi Xu, Yang Bai, Annie Qu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种能够同时处理多任务学习中各种异质性的两步学习策略，并展示了其在多个任务中的高性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 为了有效地在多任务学习中利用同质和异质信息，并解决现有方法难以统一处理多种异质性的问题。

**方法:** 首先利用跨任务的同源信息提取共享表示来填补缺失块；然后将输入特征与响应之间的映射分解为共享成分和任务特定成分，从而通过共享成分实现信息借用。

**结果:** 数值实验和来自ADNI数据库的实际数据分析表明了该方法的优越性能。

**结论:** 所提出的两步学习策略在多任务学习中表现出优于其他竞争方法的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-task+Learning+for+Heterogeneous+Multi-source+Block-Wise+Missing+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24413&send_immediately=true&force_search=false)

**原文摘要:** Multi-task learning (MTL) has emerged as an imperative machine learning tool
to solve multiple learning tasks simultaneously and has been successfully
applied to healthcare, marketing, and biomedical fields. However, in order to
borrow information across different tasks effectively, it is essential to
utilize both homogeneous and heterogeneous information. Among the extensive
literature on MTL, various forms of heterogeneity are presented in MTL
problems, such as block-wise, distribution, and posterior heterogeneity.
Existing methods, however, struggle to tackle these forms of heterogeneity
simultaneously in a unified framework. In this paper, we propose a two-step
learning strategy for MTL which addresses the aforementioned heterogeneity.
First, we impute the missing blocks using shared representations extracted from
homogeneous source across different tasks. Next, we disentangle the mappings
between input features and responses into a shared component and a
task-specific component, respectively, thereby enabling information borrowing
through the shared component. Our numerical experiments and real-data analysis
from the ADNI database demonstrate the superior MTL performance of the proposed
method compared to other competing methods.

</details>


### [87] [Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy](https://arxiv.org/abs/2505.24473)
*Nikita Balagansky, Yaroslav Aksenov, Daniil Laptev, Vadim Kurochkin, Gleb Gerasimov, Nikita Koryagin, Daniil Gavrilov*

**主要类别:** cs.LG

**AI概要:** 论文介绍了一种新颖的训练目标HierarchicalTopK，用于训练能够在多个稀疏级别上同时优化重建的稀疏自动编码器。


<details>
  <summary>更多</summary>
  
**动机:** 传统的稀疏自编码器受限于固定的稀疏级别选择，在满足不同稀疏需求时需要单独模型并增加计算负担。

**方法:** 通过引入一种新的训练目标HierarchicalTopK来训练单一的SAE以在多个稀疏级别上同时优化重构。

**结果:** 实验表明，该方法在Gemma-2 2B上实现了稀疏性和方差解释之间的帕累托最优权衡，并优于传统SAE。

**结论:** 论文提出了一种新的训练目标HierarchicalTopK，可以在多个稀疏水平下优化重构性能，同时保持高解释性得分。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Train+One+Sparse+Autoencoder+Across+Multiple+Sparsity+Budgets+to+Preserve+Interpretability+and+Accuracy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24473，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24473&send_immediately=true&force_search=false)

**原文摘要:** Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting
neural networks by decomposing hidden representations into disentangled,
interpretable features via sparsity constraints. However, conventional SAEs are
constrained by the fixed sparsity level chosen during training; meeting
different sparsity requirements therefore demands separate models and increases
the computational footprint during both training and evaluation. We introduce a
novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to
optimise reconstructions across multiple sparsity levels simultaneously.
Experiments with Gemma-2 2B demonstrate that our approach achieves
Pareto-optimal trade-offs between sparsity and explained variance,
outperforming traditional SAEs trained at individual sparsity levels. Further
analysis shows that HierarchicalTopK preserves high interpretability scores
even at higher sparsity. The proposed objective thus closes an important gap
between flexibility and interpretability in SAE design.

</details>


### [88] [Object Centric Concept Bottlenecks](https://arxiv.org/abs/2505.24492)
*David Steinmann, Wolfgang Stammer, Antonia Wüst, Kristian Kersting*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了OCB，一种新的AI模型框架，它结合了概念基础模型和预训练的以对象为中心的基础模型的优点，提高了性能和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 基于概念的模型（CBMs）在物体中心的真实世界环境中表现出局限性，这限制了它们解决复杂视觉任务的能力。

**方法:** 在复杂的图像数据集上评估OCB，并进行全面的消融研究来分析框架的关键组件。

**结果:** 结果显示，OCB优于传统的CBMs，允许对复杂的视觉任务做出可解释的决策。

**结论:** OCB是一种结合CBM和预训练的以对象为中心的基础模型优势的新框架，其提升了性能和可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Object+Centric+Concept+Bottlenecks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24492&send_immediately=true&force_search=false)

**原文摘要:** Developing high-performing, yet interpretable models remains a critical
challenge in modern AI. Concept-based models (CBMs) attempt to address this by
extracting human-understandable concepts from a global encoding (e.g., image
encoding) and then applying a linear classifier on the resulting concept
activations, enabling transparent decision-making. However, their reliance on
holistic image encodings limits their expressiveness in object-centric
real-world settings and thus hinders their ability to solve complex vision
tasks beyond single-label classification. To tackle these challenges, we
introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines
the strengths of CBMs and pre-trained object-centric foundation models,
boosting performance and interpretability. We evaluate OCB on complex image
datasets and conduct a comprehensive ablation study to analyze key components
of the framework, such as strategies for aggregating object-concept encodings.
The results show that OCB outperforms traditional CBMs and allows one to make
interpretable decisions for complex visual tasks.

</details>


### [89] [Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning](https://arxiv.org/abs/2505.24424)
*Amit Peleg, Naman Deep Singh, Matthias Hein*

**主要类别:** cs.LG

**AI概要:** 本研究提出CLIC，一种基于新颖训练技巧的微调方法，能有效提升CLIP模型的组合推理能力和检索性能。


<details>
  <summary>更多</summary>
  
**动机:** 改进视觉语言模型如CLIP的组合推理能力，以提升分类、检索和语义理解的表现。

**方法:** 引入了一种名为CLIC的新颖训练技术，结合多个图像及其相关标题进行微调。

**结果:** CLIC提高了不同架构和预训练CLIP模型的组合性，并实现了检索性能的一致提升。

**结论:** CLIC是一种有效的微调方法，可以提高CLIP模型的组合理解和检索性能，成为在SugarCrepe ++上表现最好的组合CLIP模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Compositional+Awareness+in+CLIP+with+Efficient+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24424&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io

</details>


### [90] [Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting](https://arxiv.org/abs/2505.24511)
*Jiahao Wang, Mingyue Cheng, Qi Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出TimeReasoner，探索慢思考大语言模型在时间序列预测中的应用，发现其在零样本预测中具有显著能力，尤其是在趋势和上下文变化预测方面。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列预测方法多采用快速思维模式，忽略了对时间动态和上下文依赖的明确推理。而新兴的慢思考大语言模型在多步推理方面表现出色，这为重新审视时间序列预测任务提供了新机会。

**方法:** 提出了TimeReasoner，通过设计一系列提示策略将时间序列预测任务转化为条件推理任务，并对预训练的慢思考大语言模型进行了实证研究。

**结果:** 研究表明，慢思考大语言模型即使在零样本情况下也能有效进行时间序列预测，特别是在高阶趋势和上下文变化的捕捉上表现突出。同时，研究也揭示了其潜力与局限性。

**结论:** 研究发现慢思考大语言模型在时间序列预测中展现出非平凡的零样本预测能力，尤其在捕捉高层次趋势和上下文变化方面。研究为基于推理的预测范式提供了新的见解，并指出了未来研究的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Slow-thinking+LLMs+Reason+Over+Time%3F+Empirical+Studies+in+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24511&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting (TSF) is a fundamental and widely studied task,
spanning methods from classical statistical approaches to modern deep learning
and multimodal language modeling. Despite their effectiveness, these methods
often follow a fast thinking paradigm emphasizing pattern extraction and direct
value mapping, while overlooking explicit reasoning over temporal dynamics and
contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,
ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning
capabilities across diverse domains, suggesting a new opportunity for reframing
TSF as a structured reasoning task. This motivates a key question: can
slow-thinking LLMs effectively reason over temporal patterns to support time
series forecasting, even in zero-shot manner? To investigate this, in this
paper, we propose TimeReasoner, an extensive empirical study that formulates
TSF as a conditional reasoning task. We design a series of prompting strategies
to elicit inference-time reasoning from pretrained slow-thinking LLMs and
evaluate their performance across diverse TSF benchmarks. Our findings reveal
that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,
especially in capturing high-level trends and contextual shifts. While
preliminary, our study surfaces important insights into the reasoning behaviors
of LLMs in temporal domains highlighting both their potential and limitations.
We hope this work catalyzes further research into reasoning-based forecasting
paradigms and paves the way toward more interpretable and generalizable TSF
frameworks.

</details>


### [91] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/abs/2505.24434)
*Md Shahriar Rahim Siddiqui, Moshe Eliasof, Eldad Haber*

**主要类别:** cs.LG

**AI概要:** 本文提出了图流匹配（GFM），通过考虑点之间的相关性来提高流匹配方法的生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的流匹配方法通常忽略点之间的相关性，这可能会影响速度预测和下游生成质量。

**方法:** 提出了一种图流匹配（GFM）方法，将学习到的速度场分解为反应项和扩散项，并利用图神经网络模块聚合邻居信息。

**结果:** 在五个图像生成基准测试中，GFM一致提高了Fr'echet Inception Distance（FID）和回忆率。

**结论:** GFM是一种轻量级增强方法，可以提高现有流匹配架构的生成质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Flow+Matching%3A+Enhancing+Image+Generation+with+Neighbor-Aware+Flow+Fields，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24434，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24434&send_immediately=true&force_search=false)

**原文摘要:** Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [92] [Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators](https://arxiv.org/abs/2505.24533)
*Mahesh Godavarti*

**主要类别:** cs.LG

**AI概要:** 论文提出了一个高维代数结构框架，统一了信号处理和数据分析中的多个经典线性变换，并提供了一种系统方法来推导这些变换以及开发新的可学习变换。


<details>
  <summary>更多</summary>
  
**动机:** 将一维单子系统推广到高维，并找到一种能保持结构一致性的方法以统一信号处理和数据分析中的线性变换。

**方法:** 通过定义递归于矢量-矩阵对的多维组合运算符，构造一个非交换性和全局交换律下的类代数结构框架。

**结果:** 经典的变换如离散傅里叶变换、沃尔什变换和哈达玛变换都是该框架下的特例，并且可以通过选择适当的矢量和矩阵对来推导它们。

**结论:** 该框架通过统一多个经典变换为代数结构，使得可以系统地推导这些变换并开发针对特定数据模态和任务的可学习变换。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Directional+Non-Commutative+Monoidal+Structures+with+Interchange+Law+via+Commutative+Generators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24533&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel framework consisting of a class of algebraic structures
that generalize one-dimensional monoidal systems into higher dimensions by
defining per-axis composition operators subject to non-commutativity and a
global interchange law. These structures, defined recursively from a base case
of vector-matrix pairs, model directional composition in multiple dimensions
while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms
in signal processing and data analysis. In this framework, data indices are
embedded into a composite structure that decomposes into simpler components. We
show that classic transforms such as the Discrete Fourier Transform (DFT), the
Walsh transform, and the Hadamard transform are special cases of our algebraic
structure. The framework provides a systematic way to derive these transforms
by appropriately choosing vector and matrix pairs. By subsuming classical
transforms within a common structure, the framework also enables the
development of learnable transformations tailored to specific data modalities
and tasks.

</details>


### [93] [Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs](https://arxiv.org/abs/2505.24438)
*Franziska Heeg, Jonas Sauer, Petra Mutzel, Ingo Scholtes*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的时间图同构定义，即一致事件图同构，并基于此开发了时间图神经网络的消息传递方案，通过实验验证了其在时间图分类任务中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 时间图的因果拓扑结构（即哪些节点可以通过时间一致性路径相互影响）通常被时间图神经网络（TGNNs）所忽视，因此需要一种能够捕捉这种结构的时间图同构定义。

**方法:** 引入了一致事件图同构的概念，并开发了一种时间版本的Weisfeiler-Leman算法来区分非同构的时间图；在此基础上，构建了一种适用于时间图神经网络的消息传递机制。

**结果:** 实验表明，该方法在时间图分类任务中表现良好，展示了其在捕捉时间因果结构方面的优势。

**结论:** 通过引入一致事件图同构和相应算法，有效提升了时间图神经网络对时间因果结构的建模能力，并在实际任务中取得了良好的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weisfeiler+and+Leman+Follow+the+Arrow+of+Time%3A+Expressive+Power+of+Message+Passing+in+Temporal+Event+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24438&send_immediately=true&force_search=false)

**原文摘要:** An important characteristic of temporal graphs is how the directed arrow of
time influences their causal topology, i.e., which nodes can possibly influence
each other causally via time-respecting paths. The resulting patterns are often
neglected by temporal graph neural networks (TGNNs). To formally analyze the
expressive power of TGNNs, we lack a generalization of graph isomorphism to
temporal graphs that fully captures their causal topology. Addressing this gap,
we introduce the notion of consistent event graph isomorphism, which utilizes a
time-unfolded representation of time-respecting paths in temporal graphs. We
compare this definition with existing notions of temporal graph isomorphisms.
We illustrate and highlight the advantages of our approach and develop a
temporal generalization of the Weisfeiler-Leman algorithm to heuristically
distinguish non-isomorphic temporal graphs. Building on this theoretical
foundation, we derive a novel message passing scheme for temporal graph neural
networks that operates on the event graph representation of temporal graphs. An
experimental evaluation shows that our approach performs well in a temporal
graph classification experiment.

</details>


### [94] [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
*Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种称为K-Steering的新方法，用于在推理时控制大型语言模型中的多个行为属性，避免了线性假设，并且无需存储和调整单独的属性向量。


<details>
  <summary>更多</summary>
  
**动机:** 由于属性之间的干扰以及线性引导方法的局限性（假设激活空间中的可加性行为并需要每个属性进行调整），在推理时控制大型语言模型（LLMs）中的多个行为属性是一个具有挑战性的问题。

**方法:** 引入了一种名为K-Steering的方法，该方法在隐藏激活上训练一个单一的非线性多标签分类器，并在推理时通过梯度计算干预方向。

**结果:** 通过对两种新的基准测试ToneBank和DebateMix的实证结果表明，经过三个模型家族的验证，K-Steering在准确引导多种行为方面优于强基线方法。

**结论:** K-Steering是一个统一且灵活的方法，避免了线性假设，无需存储和调整单独的属性向量，并允许在不重新训练的情况下动态组合行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Linear+Steering%3A+Unified+Multi-Attribute+Control+for+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24535&send_immediately=true&force_search=false)

**原文摘要:** Controlling multiple behavioral attributes in large language models (LLMs) at
inference time is a challenging problem due to interference between attributes
and the limitations of linear steering methods, which assume additive behavior
in activation space and require per-attribute tuning. We introduce K-Steering,
a unified and flexible approach that trains a single non-linear multi-label
classifier on hidden activations and computes intervention directions via
gradients at inference time. This avoids linearity assumptions, removes the
need for storing and tuning separate attribute vectors, and allows dynamic
composition of behaviors without retraining. To evaluate our method, we propose
two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral
control. Empirical results across 3 model families, validated by both
activation-based classifiers and LLM-based judges, demonstrate that K-Steering
outperforms strong baselines in accurately steering multiple behaviors.

</details>


### [95] [AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams](https://arxiv.org/abs/2505.24584)
*Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种结合小型语言模型与仿真的闭环框架，用于自动化生成工业可行的化工流程图纸，解决了从AI设计到实际应用的转化难题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管生成式AI在新型化学品和材料的发现中取得了进展，但将其转化为工业规模生产仍然是一个瓶颈，因为现有的AI方法无法自动生成符合工程约束的PFDs和PIDs。

**方法:** 该框架结合了专门针对化学过程问答任务训练的小型语言模型（SLMs）、基于第一性原理的仿真技术，并利用包含1020多种化学品的分层知识图谱、多阶段微调流水线以及DWSIM仿真器进行验证。此外，还应用了FlashAttention、Lookahead Decoding、PagedAttention与KV-cache量化等推理优化技术，并采用结构化剪枝方法减小模型规模。

**结果:** 实验表明，该框架能够高保真地生成经过仿真验证的工艺描述，在正确性方面优于基线方法，并能推广到未见过的化学品。

**结论:** 论文提出了一种闭环、物理感知的框架，用于生成工业上可行的工艺流程图（PFDs）和管道仪表图（PIDs），从而显著缩短了从实验室发现到工厂部署的研发周期。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoChemSchematic+AI%3A+A+Closed-Loop%2C+Physics-Aware+Agentic+Framework+for+Auto-Generating+Chemical+Process+and+Instrumentation+Diagrams，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24584，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24584&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.

</details>


### [96] [Stepsize anything: A unified learning rate schedule for budgeted-iteration training](https://arxiv.org/abs/2505.24452)
*Anda Tang, Yiming Dong, Yutao Zeng, zhou Xun, Zhouchen Lin*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的预算感知学习率调度方法UBA，具有理论保证且无需复杂调参，在多种任务和架构上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 在预算迭代训练场景下，传统学习率调度方法缺乏理论支持且依赖大量试错调整，导致效率低下。

**方法:** 通过构建考虑训练预算和损失曲面曲率鲁棒性的优化框架，推导出仅需单个超参数φ控制的UBA调度算法，并证明了其收敛性。

**结果:** 实验表明，UBA调度器在不同训练迭代预算、多种视觉与语言任务以及不同网络架构（如ResNet、OLMo）上均一致优于常用调度方法。

**结论:** 论文提出了一个基于理论基础的统一预算感知学习率调度器，能够在多种任务和网络结构中优于常用调度器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stepsize+anything%3A+A+unified+learning+rate+schedule+for+budgeted-iteration+training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24452，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24452&send_immediately=true&force_search=false)

**原文摘要:** The expanding computational costs and limited resources underscore the
critical need for budgeted-iteration training, which aims to achieve optimal
learning within predetermined iteration budgets.While learning rate schedules
fundamentally govern the performance of different networks and tasks,
particularly in budgeted-iteration scenarios, their design remains largely
heuristic, lacking theoretical foundations.In addition, the optimal learning
rate schedule requires extensive trial-and-error selection, making the training
process inefficient.In this work, we propose the Unified Budget-Aware (UBA)
schedule, a theoretically grounded learning rate schedule that consistently
outperforms commonly-used schedules among diverse architectures and tasks under
different constrained training budgets.First, we bridge the gap by constructing
a novel training budget-aware optimization framework, which explicitly accounts
for the robustness to landscape curvature variations.From this framework, we
derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that
provides a trade-off between flexibility and simplicity, eliminating the need
for per-network numerical optimization. Moreover, we establish a theoretical
connection between $\varphi$ and the condition number, adding interpretation
and justification to our approach. Besides, we prove the convergence for
different values of $\varphi$.We offer practical guidelines for its selection
via theoretical analysis and empirical results.xtensive experimental results
show that UBA \textit{consistently surpasses} the commonly-used schedules
across diverse vision and language tasks, spanning network architectures (e.g.,
ResNet, OLMo) and scales, under different training-iteration budgets.

</details>


### [97] [A Flat Minima Perspective on Understanding Augmentations and Model Robustness](https://arxiv.org/abs/2505.24592)
*Weebum Yoo, Sung Whan Yoon*

**主要类别:** cs.LG

**AI概要:** 论文提出了一个统一的理论框架，解释了增强数据如何帮助提高模型在未预见分布变化下的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管增强数据在不同领域取得了巨大成功，但对于其提高模型鲁棒性的有效性缺乏一般的理论理解。

**方法:** 通过损失表面平坦性和PAC泛化界的理论分析，结合在多个基准上的模拟实验验证理论的有效性。

**结果:** 提出了一个统一的理论框架，能够广泛涵盖现有的许多增强方法，并且不局限于特定类型的分布变化。

**结论:** 本文提出了一个统一的理论框架来解释增强数据如何通过损失表面平坦性和PAC泛化界来提高模型的鲁棒性，并通过在多个基准上的模拟验证了该理论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Flat+Minima+Perspective+on+Understanding+Augmentations+and+Model+Robustness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24592，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24592&send_immediately=true&force_search=false)

**原文摘要:** Model robustness indicates a model's capability to generalize well on
unforeseen distributional shifts, including data corruption, adversarial
attacks, and domain shifts. Data augmentation is one of the prevalent and
effective ways to enhance robustness. Despite the great success of
augmentations in different fields, a general theoretical understanding of their
efficacy in improving model robustness is lacking. We offer a unified
theoretical framework to clarify how augmentations can enhance model robustness
through the lens of loss surface flatness and PAC generalization bound. Our
work diverges from prior studies in that our analysis i) broadly encompasses
much of the existing augmentation methods, and ii) is not limited to specific
types of distribution shifts like adversarial attacks. We confirm our theories
through simulations on the existing common corruption and adversarial
robustness benchmarks based on the CIFAR and ImageNet datasets, as well as
domain generalization benchmarks including PACS and OfficeHome.

</details>


### [98] [Logits-Based Finetuning](https://arxiv.org/abs/2505.24461)
*Jingyao Li, Senqiao Yang, Sitong Wu, Han Shi, Chuanyang Zheng, Hong Xu, Jiaya Jia*

**主要类别:** cs.LG

**AI概要:** 本研究发现基于重构的方法能显著提高OOD检测效果，提出了MOOD框架，在没有额外条件的情况下优于现有技术。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于识别的方法倾向于学习捷径而非全面表示，而重构方法能够显著提升OOD检测性能。

**方法:** 采用掩码图像建模（Masked Image Modeling）作为预任务来构建OOD检测框架MOOD，以学习ID数据集的内在分布特性。

**结果:** MOOD在单类OOD检测中提升了5.7%，多类OOD检测中提升了3.0%，近分布OOD检测中提升了2.1%。此外，甚至击败了每类包含10个样本的异常暴露OOD检测方法。

**结论:** 论文提出了一种基于重构的方法MOOD，在无任何额外条件下，其性能优于之前最先进的多种OOD检测方法，并且不依赖于OOD样本进行检测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Logits-Based+Finetuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24461&send_immediately=true&force_search=false)

**原文摘要:** The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection. Codes are available at
https://github.com/JulietLJY/MOOD.

</details>


### [99] [Smooth Model Compression without Fine-Tuning](https://arxiv.org/abs/2505.24469)
*Christina Runkel, Natacha Kuete Meli, Jovita Lukasik, Ander Biguri, Carola-Bibiane Schönlieb, Michael Moeller*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了在神经网络训练中引入平滑正则化的方法，并展示其如何提升模型压缩效果。


<details>
  <summary>更多</summary>
  
**动机:** 标准的剪枝和压缩技术通常没有考虑到网络权重的结构，限制了它们的有效性。因此，探索平滑正则化对神经网络训练和模型压缩的影响。

**方法:** 在训练过程中应用核范数、一阶和二阶导数惩罚，以鼓励结构化的平滑性，并使用基于奇异值分解的压缩方法利用底层平滑结构。

**结果:** 应用这些平滑模型后，标准剪枝方法的效果更好，并且在CIFAR-10上的平滑ResNet-18模型参数减少了70%，准确率仍达到91%。

**结论:** 本文提出了一种基于奇异值分解的压缩方法，能够在不进行微调的情况下实现最先进的压缩效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smooth+Model+Compression+without+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24469&send_immediately=true&force_search=false)

**原文摘要:** Compressing and pruning large machine learning models has become a critical
step towards their deployment in real-world applications. Standard pruning and
compression techniques are typically designed without taking the structure of
the network's weights into account, limiting their effectiveness. We explore
the impact of smooth regularization on neural network training and model
compression. By applying nuclear norm, first- and second-order derivative
penalties of the weights during training, we encourage structured smoothness
while preserving predictive performance on par with non-smooth models. We find
that standard pruning methods often perform better when applied to these smooth
models. Building on this observation, we apply a
Singular-Value-Decomposition-based compression method that exploits the
underlying smooth structure and approximates the model's weight tensors by
smaller low-rank tensors. Our approach enables state-of-the-art compression
without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18
on CIFAR-10 with $70\%$ fewer parameters.

</details>


### [100] [Hyperbolic Dataset Distillation](https://arxiv.org/abs/2505.24623)
*Wenyuan Li, Guang Li, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的超平面数据集蒸馏方法HDD，通过利用双曲空间的特性来更好地处理数据的层次结构和几何特征。


<details>
  <summary>更多</summary>
  
**动机:** 现有分布匹配方法受限于欧几里得空间，无法有效捕捉数据间的复杂几何和层次关系。

**方法:** HDD将浅层网络提取的特征嵌入到具有负曲率的双曲空间，并通过测量合成数据与原始数据质心之间的测地距离来优化差异。

**结果:** 实验表明，HDD在保留模型性能的同时显著提高了训练稳定性，并且仅需20%的蒸馏核心数据集即可实现良好效果。

**结论:** HDD是一种有效的数据集蒸馏方法，能够显式整合层次结构并保持数据的几何特性，兼容大多数现有的DM方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyperbolic+Dataset+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24623，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24623&send_immediately=true&force_search=false)

**原文摘要:** To address the computational and storage challenges posed by large-scale
datasets in deep learning, dataset distillation has been proposed to synthesize
a compact dataset that replaces the original while maintaining comparable model
performance. Unlike optimization-based approaches that require costly bi-level
optimization, distribution matching (DM) methods improve efficiency by aligning
the distributions of synthetic and original data, thereby eliminating nested
optimization. DM achieves high computational efficiency and has emerged as a
promising solution. However, existing DM methods, constrained to Euclidean
space, treat data as independent and identically distributed points,
overlooking complex geometric and hierarchical relationships. To overcome this
limitation, we propose a novel hyperbolic dataset distillation method, termed
HDD. Hyperbolic space, characterized by negative curvature and exponential
volume growth with distance, naturally models hierarchical and tree-like
structures. HDD embeds features extracted by a shallow network into the Lorentz
hyperbolic space, where the discrepancy between synthetic and original data is
measured by the hyperbolic (geodesic) distance between their centroids. By
optimizing this distance, the hierarchical structure is explicitly integrated
into the distillation process, guiding synthetic samples to gravitate towards
the root-centric regions of the original data distribution while preserving
their underlying geometric characteristics. Furthermore, we find that pruning
in hyperbolic space requires only 20% of the distilled core set to retain model
performance, while significantly improving training stability. Notably, HDD is
seamlessly compatible with most existing DM methods, and extensive experiments
on different datasets validate its effectiveness.

</details>


### [101] [Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs](https://arxiv.org/abs/2505.24684)
*Zihao Chen, Yu Xiang, Wenyong Wang*

**主要类别:** cs.LG

**AI概要:** 本文研究了变分自编码器中隐式归纳偏置的影响，发现了分解粒度的重要性，并提出新模型\b{eta}-STCVAE以更好地理解和实现高复杂度特征的解纠缠。


<details>
  <summary>更多</summary>
  
**动机:** 尽管变分自编码器（VAEs）及其变体在学习语义上有意义的无监督解纠缠表示方面取得了成功，但它们面临一个基本的理论挑战：没有隐式归纳偏置的情况下，无监督解纠缠是无法实现的，而这种偏置仍然难以捉摸。

**方法:** 通过分析\b{eta}-TCVAE中的总相关性，发现了一种称为解纠缠粒度的关键隐式归纳偏差，并提出了新的模型\b{eta}-STCVAE进行验证。

**结果:** 实验结果表明，传统的因子化VAE受限于固定的解纠缠粒度，本质上倾向于解纠缠低复杂度特征；而通过\b{eta}-STCVAE适当调整解纠缠粒度，可以拓宽解纠缠表示的范围，实现高复杂度特征的解纠缠。

**结论:** 论文得出结论，分解粒度作为因子化VAE中的一种隐式归纳偏置，影响了分解性能和ELBO的推断，提供了对VAE可解释性和内在偏置的新见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Disentangling+Granularity%3A+An+Implicit+Inductive+Bias+in+Factorized+VAEs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24684&send_immediately=true&force_search=false)

**原文摘要:** Despite the success in learning semantically meaningful, unsupervised
disentangled representations, variational autoencoders (VAEs) and their
variants face a fundamental theoretical challenge: substantial evidence
indicates that unsupervised disentanglement is unattainable without implicit
inductive bias, yet such bias remains elusive. In this work, we focus on
exploring the implicit inductive bias that drive disentanglement in VAEs with
factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we
uncover a crucial implicit inductive bias called disentangling granularity,
which leads to the discovery of an interesting "V"-shaped optimal Evidence
Lower Bound (ELBO) trajectory within the parameter space. This finding is
validated through over 100K experiments using factorized VAEs and our newly
proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that
conventional factorized VAEs, constrained by fixed disentangling granularity,
inherently tend to disentangle low-complexity feature. Whereas, appropriately
tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the
range of disentangled representations, allowing for the disentanglement of
high-complexity features. Our findings unveil that disentangling granularity as
an implicit inductive bias in factorized VAEs influence both disentanglement
performance and the inference of the ELBO, offering fresh insights into the
interpretability and inherent biases of VAEs.

</details>


### [102] [On Symmetric Losses for Robust Policy Optimization with Noisy Preferences](https://arxiv.org/abs/2505.24709)
*Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, Masashi Sugiyama*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种鲁棒的策略优化框架 SymPO，用于处理带有噪声的人类偏好数据。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的偏好数据通常由于人类错误或偏见而包含噪声，传统的假设准确注释的方法并不总是适用。

**方法:** 将奖励建模视为分类问题，并利用对称损失来处理偏好数据中的噪声。

**结果:** 在合成和真实世界任务上的实验展示了 SymPO 的有效性。

**结论:** SymPO 方法即使在有噪声的标签下也能成功进行策略优化，因为它保持了奖励的排名不变性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Symmetric+Losses+for+Robust+Policy+Optimization+with+Noisy+Preferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24709，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24709&send_immediately=true&force_search=false)

**原文摘要:** Optimizing policies based on human preferences is key to aligning language
models with human intent. This work focuses on reward modeling, a core
component in reinforcement learning from human feedback (RLHF), and offline
preference optimization, such as direct preference optimization. Conventional
approaches typically assume accurate annotations. However, real-world
preference data often contains noise due to human errors or biases. We propose
a principled framework for robust policy optimization under noisy preferences,
viewing reward modeling as a classification problem. This allows us to leverage
symmetric losses, known for their robustness to label noise in classification,
leading to our Symmetric Preference Optimization (SymPO) method. We prove that
symmetric losses enable successful policy optimization even under noisy labels,
as the resulting reward remains rank-preserving -- a property sufficient for
policy improvement. Experiments on synthetic and real-world tasks demonstrate
the effectiveness of SymPO.

</details>


### [103] [Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem](https://arxiv.org/abs/2505.24498)
*Andres Fernandez, Juan Azcarreta, Cagdas Bilen, Jesus Monge Alvarez*

**主要类别:** cs.LG

**AI概要:** 本文提出三种创新技术，显著降低了在线语音频谱图反演的计算成本，同时保持了高质量的重建结果。


<details>
  <summary>更多</summary>
  
**动机:** 为了降低现有在线语音频谱图反演方法的计算成本，同时保持高重建质量。

**方法:** 提出了一种具有8k参数的新神经网络架构；增加1个hop size的延迟以减少神经推理步骤的成本；观察到最小二乘问题具有三对角矩阵特性，并提出了利用该特性的线性复杂度求解器。

**结果:** 所提出的三种创新分别在不同方面降低了计算成本：新神经网络架构比之前最先进的模型小30倍；增加延迟进一步将神经推理步骤的成本减半；线性复杂度求解器实现了几个数量级的加速。

**结论:** 该论文通过引入三种创新显著降低了在线语音频谱图反演的计算成本，同时保持了高质量重建。这些方法包括一个参数更少的神经网络架构、增加延迟以减少推理步骤的成本以及利用三对角矩阵特性的线性复杂度求解器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Neural+and+Numerical+Methods+for+High-Quality+Online+Speech+Spectrogram+Inversion+via+Gradient+Theorem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24498，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24498&send_immediately=true&force_search=false)

**原文摘要:** Recent work in online speech spectrogram inversion effectively combines Deep
Learning with the Gradient Theorem to predict phase derivatives directly from
magnitudes. Then, phases are estimated from their derivatives via least
squares, resulting in a high quality reconstruction. In this work, we introduce
three innovations that drastically reduce computational cost, while maintaining
high quality: Firstly, we introduce a novel neural network architecture with
just 8k parameters, 30 times smaller than previous state of the art. Secondly,
increasing latency by 1 hop size allows us to further halve the cost of the
neural inference step. Thirdly, we we observe that the least squares problem
features a tridiagonal matrix and propose a linear-complexity solver for the
least squares step that leverages tridiagonality and positive-semidefiniteness,
achieving a speedup of several orders of magnitude. We release samples online.

</details>


### [104] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
*Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法Causal-aware LLMs，通过整合结构因果模型到决策过程中，解决了大型语言模型在复杂现实任务中应用的问题。


<details>
  <summary>更多</summary>
  
**动机:** 预训练模型缺乏推理能力且难以适应新环境，阻碍了它们在复杂现实任务中的应用。

**方法:** 利用LLM提取特定于环境的因果实体及其因果关系，初始化环境的结构因果模型；通过外部反馈更新该模型；最后利用强化学习代理利用结构因果知识进行策略制定。

**结果:** 在开放世界游戏“Crafter”中的22个不同任务上验证了所提方法的有效性。

**结论:** Causal-aware LLMs通过整合结构因果模型（SCM）到决策过程中，能够更准确地理解环境并做出更高效的决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal-aware+Large+Language+Models%3A+Enhancing+Decision-Making+Through+Learning%2C+Adapting+and+Acting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24710&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


### [105] [Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset](https://arxiv.org/abs/2505.24505)
*Ignacio Boero, Santiago Diaz, Tomás Vázquez, Enzo Coppes, Pablo Belzarena, Federico Larroca*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于电力系统操作中最优无功功率调度问题的新方法，通过引入一个包含乌拉圭电网结构和实际运行数据的新数据集，揭示了基于学习的模型在处理真实世界数据时的局限性，并指出了未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于学习的方法在合成数据集上表现出色，但在实际电网条件下的有效性仍未得到充分探索。乌拉圭电网的高可再生能源渗透率使得ORPD问题成为其主要优化挑战。

**方法:** 引入了一个新的公开可用的电力系统数据集，包含乌拉圭电网的结构特征和近两年的实际运行数据，并评估了现实数据对基于学习的ORPD解决方案的影响。

**结果:** 研究发现，从合成输入转换为实际需求和发电输入时，预测误差显著增加，表明现有模型在处理真实电网数据方面存在局限性。

**结论:** 该论文强调了现有模型在学习复杂真实电网条件下的局限性，并指出需要更具表现力的架构。通过提供新数据集，旨在促进基于学习的电力系统管理优化技术的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Optimally+Dispatch+Power%3A+Performance+on+a+Nation-Wide+Real-World+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24505&send_immediately=true&force_search=false)

**原文摘要:** The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in
power system operations, ensuring voltage stability and minimizing power
losses. Recent advances in machine learning, particularly within the ``learning
to optimize'' framework, have enabled fast and efficient approximations of ORPD
solutions, typically by training models on precomputed optimization results.
While these approaches have demonstrated promising performance on synthetic
datasets, their effectiveness under real-world grid conditions remains largely
unexplored. This paper makes two key contributions. First, we introduce a
publicly available power system dataset that includes both the structural
characteristics of Uruguay's electrical grid and nearly two years of real-world
operational data, encompassing actual demand and generation profiles. Given
Uruguay's high penetration of renewable energy, the ORPD problem has become the
primary optimization challenge in its power network. Second, we assess the
impact of real-world data on learning-based ORPD solutions, revealing a
significant increase in prediction errors when transitioning from synthetic to
actual demand and generation inputs. Our results highlight the limitations of
existing models in learning under the complex statistical properties of real
grid conditions and emphasize the need for more expressive architectures. By
providing this dataset, we aim to facilitate further research into robust
learning-based optimization techniques for power system management.

</details>


### [106] [CoRet: Improved Retriever for Code Editing](https://arxiv.org/abs/2505.24715)
*Fabio Fehr, Prabhu Teja Sivaprasad, Luca Franceschi, Giovanni Zappella*

**主要类别:** cs.LG

**AI概要:** CoRet是一种新的代码编辑任务检索模型，它通过整合多种代码信息提升了检索效果。


<details>
  <summary>更多</summary>
  
**动机:** 为了更有效地根据自然语言查询检索代码存储库的相关部分，以支持实现新功能或修复错误的任务。

**方法:** 提出了一种名为CoRet的密集检索模型，并设计了适用于存储库级检索的损失函数。

**结果:** 在SWE-bench和Long Code Arena的错误定位数据集上，CoRet的检索召回率比现有模型至少提高了15个百分点。

**结论:** CoRet通过整合代码语义、存储库结构和调用图依赖关系，在代码编辑任务中的检索召回率上取得了显著的提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoRet%3A+Improved+Retriever+for+Code+Editing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24715，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24715&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we introduce CoRet, a dense retrieval model designed for
code-editing tasks that integrates code semantics, repository structure, and
call graph dependencies. The model focuses on retrieving relevant portions of a
code repository based on natural language queries such as requests to implement
new features or fix bugs. These retrieved code chunks can then be presented to
a user or to a second code-editing model or agent. To train CoRet, we propose a
loss function explicitly designed for repository-level retrieval. On SWE-bench
and Long Code Arena's bug localisation datasets, we show that our model
substantially improves retrieval recall by at least 15 percentage points over
existing models, and ablate the design choices to show their importance in
achieving these results.

</details>


### [107] [HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts](https://arxiv.org/abs/2505.24722)
*Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于双曲空间的大型语言模型架构HELM及其变体HELM-MICE和HELM-D，通过利用非欧几何特性解决现有语言模型在表示能力和扩展性方面的不足，并在大规模基准测试中展示了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前的语言模型依赖于欧几里得操作，无法充分捕捉自然语言中固有的语义层次结构和细微的几何特性，这导致训练不稳定和生成能力下降。因此，研究者提出了转向非欧几何以更好地对齐语言模型与文本底层几何结构的想法。

**方法:** 论文引入了完全运行在双曲空间上的HypErbolic Large Language Models (HELM)，并设计了Mixture-of-Curvature Experts模型HELM-MICE以及稠密模型HELM-D。此外，开发了适用于双曲空间的rotary positional encodings、RMS归一化和Hyperbolic Multi-Head Latent Attention (HMLA)等关键技术。

**结果:** 论文首次实现了十亿参数规模的完全双曲语言模型，并在多个知名基准测试（如MMLU和ARC）上进行了评估。结果显示，HELM架构相比LLaMA和DeepSeek等流行的欧几里得架构有高达4%的持续提升。

**结论:** 论文提出了一种基于双曲空间的大型语言模型HELM及其变体HELM-MICE和HELM-D，通过采用非欧几何方法解决了现有模型在文本表示中的局限性。实验结果表明，HELM架构相比传统欧几里得架构具有更高的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HELM%3A+Hyperbolic+Large+Language+Models+via+Mixture-of-Curvature+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24722，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24722&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-KV-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.

</details>


### [108] [Airborne Neural Network](https://arxiv.org/abs/2505.24513)
*Paritosh Ranjan, Surajit Majumder, Prodip Roy*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的空中神经网络架构，旨在解决航空航天领域中深度学习系统的部署问题。


<details>
  <summary>更多</summary>
  
**动机:** 在航空航天领域，由于基础设施限制，部署具有实时数据处理和超低延迟要求的深度学习系统仍然是一个挑战。

**方法:** 提出了一种名为Airborne Neural Network的分布式架构，多个空中设备协同计算，并由空中网络控制器和特定层控制器指导。

**结果:** 该方法能够在飞行环境中进行大规模神经网络运算，有潜力彻底改变空中交通控制、实时天气与地理预测以及动态地理空间数据处理等应用。

**结论:** 本文提出了一种空中神经网络的新概念，通过分布式架构实现飞行中的实时学习和推理，为下一代人工智能驱动的航空航天系统奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Airborne+Neural+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24513，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24513&send_immediately=true&force_search=false)

**原文摘要:** Deep Learning, driven by neural networks, has led to groundbreaking
advancements in Artificial Intelligence by enabling systems to learn and adapt
like the human brain. These models have achieved remarkable results,
particularly in data-intensive domains, supported by massive computational
infrastructure. However, deploying such systems in Aerospace, where real time
data processing and ultra low latency are critical, remains a challenge due to
infrastructure limitations. This paper proposes a novel concept: the Airborne
Neural Network a distributed architecture where multiple airborne devices each
host a subset of neural network neurons. These devices compute collaboratively,
guided by an airborne network controller and layer specific controllers,
enabling real-time learning and inference during flight. This approach has the
potential to revolutionize Aerospace applications, including airborne air
traffic control, real-time weather and geographical predictions, and dynamic
geospatial data processing. By enabling large-scale neural network operations
in airborne environments, this work lays the foundation for the next generation
of AI powered Aerospace systems.

</details>


### [109] [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.24760)
*Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, Andreas Köpf*

**主要类别:** cs.LG

**AI概要:** 本文介绍了Reasoning Gym，这是一个用于强化学习的新库，能生成无限且复杂度可调的推理任务，显著提升了模型评估与训练能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服传统推理数据集固定不变的问题，提出一个可以生成无限且复杂度可调的训练数据的新工具。

**方法:** 介绍了一个名为Reasoning Gym (RG)的推理环境库，并通过实验验证了其生成数据的能力和效果。

**结果:** 该库提供了超过100个涵盖多个领域的数据生成器和验证器，并能够进行难度连续变化的评估。

**结论:** Reasoning Gym的实验结果证明了其在评估和强化学习推理模型方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REASONING+GYM%3A+Reasoning+Environments+for+Reinforcement+Learning+with+Verifiable+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24760&send_immediately=true&force_search=false)

**原文摘要:** We introduce Reasoning Gym (RG), a library of reasoning environments for
reinforcement learning with verifiable rewards. It provides over 100 data
generators and verifiers spanning multiple domains including algebra,
arithmetic, computation, cognition, geometry, graph theory, logic, and various
common games. Its key innovation is the ability to generate virtually infinite
training data with adjustable complexity, unlike most previous reasoning
datasets, which are typically fixed. This procedural generation approach allows
for continuous evaluation across varying difficulty levels. Our experimental
results demonstrate the efficacy of RG in both evaluating and reinforcement
learning of reasoning models.

</details>


### [110] [Transformers Are Universally Consistent](https://arxiv.org/abs/2505.24531)
*Sagar Ghosh, Kushal Bose, Swagatam Das*

**主要类别:** cs.LG

**AI概要:** 本文揭示了在双曲空间中，Transformer可以稳健地执行普通最小二乘回归，并提供了理论保证与实证验证。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Transformer在基础模型和大规模语言建模中起着核心作用，但其理论基础仍不完善，尤其是在处理真实世界数据（具有非欧几里得几何）的功能回归方面尚不清楚。

**方法:** 研究通过理论推导分析了Transformer在双曲空间中进行普通最小二乘回归的能力，并得出了渐近状态下的误差衰减速率。

**结果:** 论文证明了Transformer可以在双曲空间中稳健地执行OLS回归，且误差上界以可证明的速率衰减，同时实证评估验证了这一理论结果。

**结论:** 论文得出结论，配备softmax-based非线性注意力的Transformer在将输入和输出嵌入到双曲空间时，能够统一地执行普通最小二乘回归，并提供了误差上限的确定性分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformers+Are+Universally+Consistent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24531&send_immediately=true&force_search=false)

**原文摘要:** Despite their central role in the success of foundational models and
large-scale language modeling, the theoretical foundations governing the
operation of Transformers remain only partially understood. Contemporary
research has largely focused on their representational capacity for language
comprehension and their prowess in in-context learning, frequently under
idealized assumptions such as linearized attention mechanisms. Initially
conceived to model sequence-to-sequence transformations, a fundamental and
unresolved question is whether Transformers can robustly perform functional
regression over sequences of input tokens. This question assumes heightened
importance given the inherently non-Euclidean geometry underlying real-world
data distributions. In this work, we establish that Transformers equipped with
softmax-based nonlinear attention are uniformly consistent when tasked with
executing Ordinary Least Squares (OLS) regression, provided both the inputs and
outputs are embedded in hyperbolic space. We derive deterministic upper bounds
on the empirical error which, in the asymptotic regime, decay at a provable
rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens
and $d$ the embedding dimensionality. Notably, our analysis subsumes the
Euclidean setting as a special case, recovering analogous convergence
guarantees parameterized by the intrinsic dimensionality of the data manifold.
These theoretical insights are corroborated through empirical evaluations on
real-world datasets involving both continuous and categorical response
variables.

</details>


### [111] [Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding](https://arxiv.org/abs/2505.24791)
*Jiaru Zhang, Juanwu Lu, Ziran Wang, Ruqi Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出一种名为 Selective Jacobi decoding (SeJD) 的方法，在不损失生成质量的前提下显著加速了归一化流模型的推理过程。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服现有归一化流模型在推理中的顺序依赖限制，提高生成速度。

**方法:** 提出了一种选择性雅可比解码策略，并进行了理论分析和实验证明其效果。

**结果:** 实现了最高达4.7倍的推理加速，且保持生成质量和实用性。

**结论:** Selective Jacobi decoding (SeJD) 显著加速了自回归推理，同时保持了生成质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inference+Acceleration+of+Autoregressive+Normalizing+Flows+by+Selective+Jacobi+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24791&send_immediately=true&force_search=false)

**原文摘要:** Normalizing flows are promising generative models with advantages such as
theoretical rigor, analytical log-likelihood computation, and end-to-end
training. However, the architectural constraints to ensure invertibility and
tractable Jacobian computation limit their expressive power and practical
usability. Recent advancements utilize autoregressive modeling, significantly
enhancing expressive power and generation quality. However, such sequential
modeling inherently restricts parallel computation during inference, leading to
slow generation that impedes practical deployment. In this paper, we first
identify that strict sequential dependency in inference is unnecessary to
generate high-quality samples. We observe that patches in sequential modeling
can also be approximated without strictly conditioning on all preceding
patches. Moreover, the models tend to exhibit low dependency redundancy in the
initial layer and higher redundancy in subsequent layers. Leveraging these
observations, we propose a selective Jacobi decoding (SeJD) strategy that
accelerates autoregressive inference through parallel iterative optimization.
Theoretical analyses demonstrate the method's superlinear convergence rate and
guarantee that the number of iterations required is no greater than the
original sequential approach. Empirical evaluations across multiple datasets
validate the generality and effectiveness of our acceleration technique.
Experiments demonstrate substantial speed improvements up to 4.7 times faster
inference while keeping the generation quality and fidelity.

</details>


### [112] [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/abs/2505.24823)
*Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo*

**主要类别:** cs.LG

**AI概要:** 该论文介绍了PhySense，一个基于物理原理的推理基准，揭示了当前大型语言模型在模仿专家基于原理的推理方面的不足，并强调了改进AI系统科学推理的潜力方向。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs在解决复杂问题上取得了进展，但它们通常无法模仿人类专家基于原则的简洁推理过程。这种差距促使了对LLMs在物理问题解决中应用核心物理原理能力的研究。

**方法:** 作者引入了一个新的基于物理原理的推理基准PhySense，用于系统地研究LLMs在没有以原理为先的推理情况下模仿专家级推理路径的能力。

**结果:** 评估显示，多个最先进的LLMs和提示类型均未能与专家级推理路径对齐，揭示了其在基于物理原理推理上的不足。

**结论:** 论文得出当前LLMs在应用核心物理原理进行高效和可解释的问题解决方面存在局限性，这为未来开发具有高效、稳健和可解释的基于原理的科学推理AI系统提供了见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PhySense%3A+Principle-Based+Physics+Reasoning+Benchmarking+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24823&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have rapidly advanced and are increasingly
capable of tackling complex scientific problems, including those in physics.
Despite this progress, current LLMs often fail to emulate the concise,
principle-based reasoning characteristic of human experts, instead generating
lengthy and opaque solutions. This discrepancy highlights a crucial gap in
their ability to apply core physical principles for efficient and interpretable
problem solving. To systematically investigate this limitation, we introduce
PhySense, a novel principle-based physics reasoning benchmark designed to be
easily solvable by experts using guiding principles, yet deceptively difficult
for LLMs without principle-first reasoning. Our evaluation across multiple
state-of-the-art LLMs and prompt types reveals a consistent failure to align
with expert-like reasoning paths, providing insights for developing AI systems
with efficient, robust and interpretable principle-based scientific reasoning.

</details>


### [113] [HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection](https://arxiv.org/abs/2505.24534)
*Florian Frantzen, Michael T. Schaub*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于霍奇拉普拉斯谱特性的新方法HLSAD，用于检测时间演化单纯复形中的异常，相较传统图方法具有更高的准确性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统图异常检测技术难以捕捉复杂结构性异常所需的关键高阶相互作用，而这些高阶相互作用可能直接来源于底层数据本身或通过图提升技术获得。

**方法:** 利用单纯复形的霍奇拉普拉斯算子的谱特性建模数据点之间的多体相互作用，并结合高维单纯结构设计了一种新的异常检测方法（HLSAD）

**结果:** 通过对合成数据集和真实世界数据集的综合实验验证了HLSAD在事件和变化点检测方面的优越性能。

**结论:** HLSAD在检测时间演化单纯复形中的异常方面优于现有的图方法，同时具备更高的检测准确性和计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HLSAD%3A+Hodge+Laplacian-based+Simplicial+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24534，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24534&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose HLSAD, a novel method for detecting anomalies in
time-evolving simplicial complexes. While traditional graph anomaly detection
techniques have been extensively studied, they often fail to capture changes in
higher-order interactions that are crucial for identifying complex structural
anomalies. These higher-order interactions can arise either directly from the
underlying data itself or through graph lifting techniques. Our approach
leverages the spectral properties of Hodge Laplacians of simplicial complexes
to effectively model multi-way interactions among data points. By incorporating
higher-dimensional simplicial structures into our method, our method enhances
both detection accuracy and computational efficiency. Through comprehensive
experiments on both synthetic and real-world datasets, we demonstrate that our
approach outperforms existing graph methods in detecting both events and change
points.

</details>


### [114] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
*Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi*

**主要类别:** cs.LG

**AI概要:** 本研究提出了REDI框架，通过一种新的两阶段训练方法，同时利用正向和负向推理数据显著提升了小模型在数学推理任务上的性能，并在多个基准测试中达到新的SOTA。


<details>
  <summary>更多</summary>
  
**动机:** 传统的拒绝采样方法会丢弃错误的推理示例，这些数据虽然有价值但往往未被充分利用。本文旨在探索如何有效利用这些数据以提高小模型的推理能力。

**方法:** 提出了一种名为Reinforcement Distillation (REDI)的两阶段框架：第一阶段使用监督微调（SFT）学习正向推理轨迹；第二阶段通过提出的REDI目标同时利用正向和负向推理轨迹进行进一步优化。

**结果:** 实验结果显示，REDI在数学推理任务上优于基线Rejection Sampling SFT以及结合DPO/SimPO的SFT方法。Qwen-REDI-1.5B模型仅基于131k个公开数据训练，在MATH-500（pass@1）上达到了83.1%的得分，其表现与基于800k私有数据训练的DeepSeek-R1-Distill-Qwen-1.5B模型相当或更优。

**结论:** REDI框架能够有效利用正向和负向的推理轨迹，从而在离线设置下最大化LLM的推理性能，并且Qwen-REDI-1.5B模型在多个数学推理基准测试中达到新的SOTA。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Harnessing+Negative+Signals%3A+Reinforcement+Distillation+from+Teacher+Data+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24850，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24850&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [115] [Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems](https://arxiv.org/abs/2505.24578)
*Abhishek Chandra, Taniya Kapoor, Mitrofan Curti, Koen Tiels, Elena A. Lomonova*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为NSO的新框架，用于改善复杂压电系统的表征及其性能预测。


<details>
  <summary>更多</summary>
  
**动机:** 复杂压电系统在工业应用中很基础，但其性能受到电压-位移滞后关系非线性的影响，因此需要高效的表征方法以实现可靠的设计、监测和维护。

**方法:** 提出了一种神经符号算子（NSO）框架，该框架通过傅里叶神经算子映射电压场到位移剖面，然后使用基于库的稀疏模型发现方法生成白盒简约模型。

**结果:** NSO能够准确地预测电压-位移滞后，包括蝴蝶形状的关系，并且即使对于噪声和低保真度电压数据也能预测位移剖面，突显了其鲁棒性。与最先进的神经算子和模型发现方法相比，NSO在多个评估指标上表现出优势。

**结论:** NSO在设计、监测和维护及其他现实场景中，对表征复杂的压电系统做出了贡献，同时改进了神经算子的可解释性和泛化性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neuro-Symbolic+Operator+for+Interpretable+and+Generalizable+Characterization+of+Complex+Piezoelectric+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24578&send_immediately=true&force_search=false)

**原文摘要:** Complex piezoelectric systems are foundational in industrial applications.
Their performance, however, is challenged by the nonlinear voltage-displacement
hysteretic relationships. Efficient characterization methods are, therefore,
essential for reliable design, monitoring, and maintenance. Recently proposed
neural operator methods serve as surrogates for system characterization but
face two pressing issues: interpretability and generalizability.
State-of-the-art (SOTA) neural operators are black-boxes, providing little
insight into the learned operator. Additionally, generalizing them to novel
voltages and predicting displacement profiles beyond the training domain is
challenging, limiting their practical use. To address these limitations, this
paper proposes a neuro-symbolic operator (NSO) framework that derives the
analytical operators governing hysteretic relationships. NSO first learns a
Fourier neural operator mapping voltage fields to displacement profiles,
followed by a library-based sparse model discovery method, generating white-box
parsimonious models governing the underlying hysteresis. These models enable
accurate and interpretable prediction of displacement profiles across varying
and out-of-distribution voltage fields, facilitating generalizability. The
potential of NSO is demonstrated by accurately predicting voltage-displacement
hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts
displacement profiles even for noisy and low-fidelity voltage data, emphasizing
its robustness. The results highlight the advantages of NSO compared to SOTA
neural operators and model discovery methods on several evaluation metrics.
Consequently, NSO contributes to characterizing complex piezoelectric systems
while improving the interpretability and generalizability of neural operators,
essential for design, monitoring, maintenance, and other real-world scenarios.

</details>


### [116] [Conservation-preserved Fourier Neural Operator through Adaptive Correction](https://arxiv.org/abs/2505.24579)
*Chaoyu Liu, Yangming Li, Zhongying Deng, Chris Budd, Carola-Bibiane Schönlieb*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的自适应修正方法，确保傅里叶神经算子满足守恒定律，并通过实验证明其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 标准的傅里叶神经算子通常无法保持质量守恒、动量守恒等关键守恒定律，这会影响对物理系统的准确建模。

**方法:** 引入一个可学习矩阵，在训练过程中自适应调整解决方案以满足守恒定律。

**结果:** 实验结果显示，与现有方法相比，所提出的方法能够更好地满足守恒定律并获得更优的性能。

**结论:** 论文提出了一种新的自适应修正方法，以确保傅里叶神经算子满足守恒定律。实验结果表明，该方法在一系列偏微分方程上始终优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conservation-preserved+Fourier+Neural+Operator+through+Adaptive+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24579&send_immediately=true&force_search=false)

**原文摘要:** Fourier Neural Operators (FNOs) have recently emerged as a promising and
efficient approach for learning the numerical solutions to partial differential
equations (PDEs) from data. However, standard FNO often fails to preserve key
conservation laws, such as mass conservation, momentum conservation, norm
conservation, etc., which are crucial for accurately modeling physical systems.
Existing methods for incorporating these conservation laws into Fourier neural
operators are achieved by designing related loss function or incorporating
post-processing method at the training time. None of them can both exactly and
adaptively correct the outputs to satisfy conservation laws, and our
experiments show that these methods can lead to inferior performance while
preserving conservation laws. In this work, we propose a novel adaptive
correction approach to ensure the conservation of fundamental quantities. Our
method introduces a learnable matrix to adaptively adjust the solution to
satisfy the conservation law during training. It ensures that the outputs
exactly satisfy the goal conservation law and allow for more flexibility and
adaptivity for the model to correct the outputs. We theoretically show that
applying our adaptive correction to an unconstrained FNO yields a solution with
data loss no worse than that of the best conservation-satisfying FNO. We
compare our approach with existing methods on a range of representative PDEs.
Experiment results show that our method consistently outperform other methods.

</details>


### [117] [The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches](https://arxiv.org/abs/2505.24603)
*Omri Lev, Vishwak Srinivasan, Moshe Shenfeld, Katrina Ligett, Ayush Sekhari, Ashia C. Wilson*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一个改进的高斯草图技术的隐私分析方法，这种方法显著提高了性能并减少了运行时间。


<details>
  <summary>更多</summary>
  
**动机:** 高斯草图技术是一种广泛使用的操作，具有多种应用，但需要更精确的隐私分析以进一步提高其性能。

**方法:** 使用Renyi微分隐私（RDP）对高斯草图技术进行隐私分析，并展示了这种改进的分析如何在不同的线性回归设置中带来性能提升。

**结果:** 提供了比先前结果更紧密的隐私边界，并证明了改进后的分析可以提高性能。

**结论:** 通过改进的隐私分析，高斯草图技术在不同线性回归设置中实现了性能提升，并在多个数据集中实证提高了性能，有时还减少了运行时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Gaussian+Mixing+Mechanism%3A+Renyi+Differential+Privacy+via+Gaussian+Sketches，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24603，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24603&send_immediately=true&force_search=false)

**原文摘要:** Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.

</details>


### [118] [Multi-criteria Rank-based Aggregation for Explainable AI](https://arxiv.org/abs/2505.24612)
*Sujoy Chatterjee, Everton Romanzini Colombo, Marcos Medeiros Raimundo*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种用于平衡多个质量指标的多准则加权聚合方法，并改进了XAI性能指标，在多个数据集上的实验表明其有效性并推荐TOPSIS和WSUM作为首选方法。


<details>
  <summary>更多</summary>
  
**动机:** 为解决不同解释器在相同预测上可能产生相互矛盾的解释，并且现有研究很少采用多准则决策方法这一问题，引入了新的评估和聚合方法。

**方法:** 提出了一种基于排名的多准则决策方法，并对现有的XAI度量（复杂性、真实性和稳定性）进行了改进。

**结果:** 实验表明所提出的模型在多种公开数据集上均表现出良好的稳健性，并发现TOPSIS和WSUM在多准则决策和排名聚合算法中表现最佳。

**结论:** 本文介绍了基于排名的XAI指标和多准则加权聚合方法，TOPSIS和WSUM是适用于平衡多个质量指标的最佳候选方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-criteria+Rank-based+Aggregation+for+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24612，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24612&send_immediately=true&force_search=false)

**原文摘要:** Explainability is crucial for improving the transparency of black-box machine
learning models. With the advancement of explanation methods such as LIME and
SHAP, various XAI performance metrics have been developed to evaluate the
quality of explanations. However, different explainers can provide contrasting
explanations for the same prediction, introducing trade-offs across conflicting
quality metrics. Although available aggregation approaches improve robustness,
reducing explanations' variability, very limited research employed a
multi-criteria decision-making approach. To address this gap, this paper
introduces a multi-criteria rank-based weighted aggregation method that
balances multiple quality metrics simultaneously to produce an ensemble of
explanation models. Furthermore, we propose rank-based versions of existing XAI
metrics (complexity, faithfulness and stability) to better evaluate ranked
feature importance explanations. Extensive experiments on publicly available
datasets demonstrate the robustness of the proposed model across these metrics.
Comparative analyses of various multi-criteria decision-making and rank
aggregation algorithms showed that TOPSIS and WSUM are the best candidates for
this use case.

</details>


### [119] [Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees](https://arxiv.org/abs/2505.24627)
*Fu Luo, Yaoxin Wu, Zhi Zheng, Zhenkun Wang*

**主要类别:** cs.LG

**AI概要:** 这篇论文分析了神经组合优化方法在不同约束紧度下的性能问题，并提出了一种能够有效克服过拟合问题的新方法，取得了良好的实验结果。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现有的NCO方法使用具有固定约束值的训练和测试数据，缺乏对约束紧度影响的研究。因此，本文旨在填补这一研究空白，并改进现有方法的不足。

**方法:** 本文以带容量限制的车辆路径问题（CVRP）为例，实证分析了不同约束紧度下的NCO性能，并提出了一种考虑不同约束紧度的高效训练方案以及一个学习通用解决策略的多专家模块。

**结果:** 实验结果表明，所提出的方法有效地克服了过拟合问题，在各种约束紧度下均表现出优于现有方法的性能，不仅适用于CVRP，还适用于CVRP与时间窗口（CVRPTW）。

**结论:** 论文得出结论，现有的NCO方法在容量约束下存在过拟合问题，只能在较小的约束值范围内表现良好。通过开发一种新的训练方案和多专家模块，该文成功解决了这一问题，并显示出优越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Neural+Combinatorial+Optimization+for+Vehicle+Routing+Problems+with+Different+Constraint+Tightness+Degrees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24627&send_immediately=true&force_search=false)

**原文摘要:** Recent neural combinatorial optimization (NCO) methods have shown promising
problem-solving ability without requiring domain-specific expertise. Most
existing NCO methods use training and testing data with a fixed constraint
value and lack research on the effect of constraint tightness on the
performance of NCO methods. This paper takes the capacity-constrained vehicle
routing problem (CVRP) as an example to empirically analyze the NCO performance
under different tightness degrees of the capacity constraint. Our analysis
reveals that existing NCO methods overfit the capacity constraint, and they can
only perform satisfactorily on a small range of the constraint values but
poorly on other values. To tackle this drawback of existing NCO methods, we
develop an efficient training scheme that explicitly considers varying degrees
of constraint tightness and proposes a multi-expert module to learn a generally
adaptable solving strategy. Experimental results show that the proposed method
can effectively overcome the overfitting issue, demonstrating superior
performances on the CVRP and CVRP with time windows (CVRPTW) with various
constraint tightness degrees.

</details>


### [120] [Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks](https://arxiv.org/abs/2505.24629)
*Lotte Bransen, Tim Janssen, Jesse Davis*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的守门员策略分析框架，解决传统方法忽略对手互动的问题，并通过大规模数据验证其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 点球是足球比赛中关键且具有决定性作用的场景，但现有数据分析方法存在假设不现实的问题，即忽略了守门员和射门者之间的相互影响。

**方法:** 开发了一种与球员无关的模拟框架，基于包含丰富选择的数据集，并结合守门员技能信息进行分析。

**结果:** 通过使用专家注释的大规模点球数据集，框架能够考虑复杂的决策互动并提供实际策略建议。

**结论:** 论文得出了一种评估守门员策略有效性的通用模拟框架，该框架可以优化现实世界中守门员的决策策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stop+Guessing%3A+Optimizing+Goalkeeper+Policies+for+Soccer+Penalty+Kicks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24629&send_immediately=true&force_search=false)

**原文摘要:** Penalties are fraught and game-changing moments in soccer games that teams
explicitly prepare for. Consequently, there has been substantial interest in
analyzing them in order to provide advice to practitioners. From a data science
perspective, such analyses suffer from a significant limitation: they make the
unrealistic simplifying assumption that goalkeepers and takers select their
action -- where to dive and where to the place the kick -- independently of
each other. In reality, the choices that some goalkeepers make depend on the
taker's movements and vice-versa. This adds substantial complexity to the
problem because not all players have the same action capacities, that is, only
some players are capable of basing their decisions on their opponent's
movements. However, the small sample sizes on the player level mean that one
may have limited insights into a specific opponent's capacities. We address
these challenges by developing a player-agnostic simulation framework that can
evaluate the efficacy of different goalkeeper strategies. It considers a rich
set of choices and incorporates information about a goalkeeper's skills. Our
work is grounded in a large dataset of penalties that were annotated by penalty
experts and include aspects of both kicker and goalkeeper strategies. We show
how our framework can be used to optimize goalkeeper policies in real-world
situations.

</details>


### [121] [WILTing Trees: Interpreting the Distance Between MPNN Embeddings](https://arxiv.org/abs/2505.24642)
*Masahiro Negishi, Thomas Gärtner, Pascal Welke*

**主要类别:** cs.LG

**AI概要:** 研究了消息传递神经网络学习到的距离函数，提出了一种新的方法来解释这种距离，通过这种方法发现MPNNs利用少量关键子图定义嵌入的相对位置。


<details>
  <summary>更多</summary>
  
**动机:** 为了弥补之前工作在任意任务上将MPNN距离与忽略任务特定信息的图结构距离联系起来的不足。

**方法:** 使用Weisfeiler Leman标签树上的最优传输来分析消息传递神经网络学到的距离函数。

**结果:** 提出的方法可以推广两个著名的图核，并且可以在O(n)时间复杂度内计算。

**结论:** MPNNs定义嵌入的相对位置是通过关注领域中功能重要的少量子图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WILTing+Trees%3A+Interpreting+the+Distance+Between+MPNN+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24642，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24642&send_immediately=true&force_search=false)

**原文摘要:** We investigate the distance function learned by message passing neural
networks (MPNNs) in specific tasks, aiming to capture the functional distance
between prediction targets that MPNNs implicitly learn. This contrasts with
previous work, which links MPNN distances on arbitrary tasks to structural
distances on graphs that ignore task-specific information. To address this gap,
we distill the distance between MPNN embeddings into an interpretable graph
distance. Our method uses optimal transport on the Weisfeiler Leman Labeling
Tree (WILT), where the edge weights reveal subgraphs that strongly influence
the distance between embeddings. This approach generalizes two well-known graph
kernels and can be computed in linear time. Through extensive experiments, we
demonstrate that MPNNs define the relative position of embeddings by focusing
on a small set of subgraphs that are known to be functionally important in the
domain.

</details>


### [122] [Learning Distributions over Permutations and Rankings with Factorized Representations](https://arxiv.org/abs/2505.24664)
*Daniel Severo, Brian Karrer, Niklas Nolte*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新颖的排列分布学习方法，通过替代表示实现高效深度学习，并在多个任务上展现了优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的排列分布学习方法依赖于参数家族的混合或需要昂贵变分推断过程的神经网络，因此需要一种更高效且灵活的方法。

**方法:** 利用Lehmer码、Fisher-Yates抽样和插入向量等排列的替代表示，将排列与对称群一一对应，从而使用传统深度学习技术进行无约束学习。

**结果:** 实验表明，该方法在拼图基准测试中显著优于现有方法，并证明即使在最不具表达力的模式下也能学习非平凡分布，而传统模型无法生成有效排列。

**结论:** 该论文提出了一种基于排列的不同表示方法的新方法，能够在模型表达性和计算需求之间进行权衡，并通过实验证明了其在多个任务上的优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Distributions+over+Permutations+and+Rankings+with+Factorized+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24664&send_immediately=true&force_search=false)

**原文摘要:** Learning distributions over permutations is a fundamental problem in machine
learning, with applications in ranking, combinatorial optimization, structured
prediction, and data association. Existing methods rely on mixtures of
parametric families or neural networks with expensive variational inference
procedures. In this work, we propose a novel approach that leverages
alternative representations for permutations, including Lehmer codes,
Fisher-Yates draws, and Insertion-Vectors. These representations form a
bijection with the symmetric group, allowing for unconstrained learning using
conventional deep learning techniques, and can represent any probability
distribution over permutations. Our approach enables a trade-off between
expressivity of the model family and computational requirements. In the least
expressive and most computationally efficient case, our method subsumes
previous families of well established probabilistic models over permutations,
including Mallow's and the Repeated Insertion Model. Experiments indicate our
method significantly outperforms current approaches on the jigsaw puzzle
benchmark, a common task for permutation learning. However, we argue this
benchmark is limited in its ability to assess learning probability
distributions, as the target is a delta distribution (i.e., a single correct
solution exists). We therefore propose two additional benchmarks: learning
cyclic permutations and re-ranking movies based on user preference. We show
that our method learns non-trivial distributions even in the least expressive
mode, while traditional models fail to even generate valid permutations in this
setting.

</details>


### [123] [Learning geometry and topology via multi-chart flows](https://arxiv.org/abs/2505.24665)
*Hanlin Yu, Søren Hauberg, Marcelo Hartmann, Arto Klami, Georgios Arvanitidis*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种处理具有非平凡拓扑的低维流形的学习方法，并开发了相应的测地线计算算法，从而显著提高了拓扑估计的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的数据通常位于高维空间中的低维黎曼流形上，因此需要学习退化的归一化流来在环境空间和低维潜在空间之间进行映射。但若流形具有非平凡的拓扑结构，则无法通过单一流正确学习，这就激发了本研究的动机。

**方法:** 论文的方法涉及学习多个流的集合，并将它们‘粘合’在一起，以处理具有非平凡拓扑的流形。此外，还开发了用于计算这些流形上的测地线的数值算法。

**结果:** 实证结果显示，该方法在拓扑估计方面带来了显著改进。

**结论:** 论文得出结论，通过提出一种学习多个流的整体训练方案，并开发了计算流形上测地线的第一个数值算法，能够显著改进拓扑估计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+geometry+and+topology+via+multi-chart+flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24665，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24665&send_immediately=true&force_search=false)

**原文摘要:** Real world data often lie on low-dimensional Riemannian manifolds embedded in
high-dimensional spaces. This motivates learning degenerate normalizing flows
that map between the ambient space and a low-dimensional latent space. However,
if the manifold has a non-trivial topology, it can never be correctly learned
using a single flow. Instead multiple flows must be `glued together'. In this
paper, we first propose the general training scheme for learning such a
collection of flows, and secondly we develop the first numerical algorithms for
computing geodesics on such manifolds. Empirically, we demonstrate that this
leads to highly significant improvements in topology estimation.

</details>


### [124] [Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning](https://arxiv.org/abs/2505.24676)
*Mihir Bhaskar, Jun Tao Luo, Zihan Geng, Asmita Hajra, Junia Howell, Matthew R. Gormley*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种成本效益高的新工具，可以帮助学者、社区活动家和政策制定者更好地分析和理解红线政策的历史影响。


<details>
  <summary>更多</summary>
  
**动机:** 由于难以获取历史性房地产估价记录，学者们难以精确量化1930年代美国政府住房政策对种族财富差距的影响。

**方法:** 结合传统的计算机视觉技术和基于深度学习的OCR，使用OCR标记额外50,000个房产的数据，对于无法应用OCR的情况，展示如何基于建筑特征数据建立回归模型来估计历史价值。

**结果:** 手动注释了超过12,000个房产的财产卡以训练和验证方法，测试了回归模型在其他县的通用性。

**结论:** 本文提出了一个可以用来数字化历史住房评估数据的方法，并构建和发布了一个县的数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+the+Past%3A+Estimating+Historical+Appraisals+with+OCR+and+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24676，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24676&send_immediately=true&force_search=false)

**原文摘要:** Despite well-documented consequences of the U.S. government's 1930s housing
policies on racial wealth disparities, scholars have struggled to quantify its
precise financial effects due to the inaccessibility of historical property
appraisal records. Many counties still store these records in physical formats,
making large-scale quantitative analysis difficult. We present an approach
scholars can use to digitize historical housing assessment data, applying it to
build and release a dataset for one county. Starting from publicly available
scanned documents, we manually annotated property cards for over 12,000
properties to train and validate our methods. We use OCR to label data for an
additional 50,000 properties, based on our two-stage approach combining
classical computer vision techniques with deep learning-based OCR. For cases
where OCR cannot be applied, such as when scanned documents are not available,
we show how a regression model based on building feature data can estimate the
historical values, and test the generalizability of this model to other
counties. With these cost-effective tools, scholars, community activists, and
policy makers can better analyze and understand the historical impacts of
redlining.

</details>


### [125] [PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations](https://arxiv.org/abs/2505.24717)
*Benjamin Holzschuh, Qiang Liu, Georg Kohl, Nils Thuerey*

**主要类别:** cs.LG

**AI概要:** PDE-Transformer是一种改进的变压器架构，用于物理模拟，能够有效学习多种类型的偏微分方程并在大规模模拟中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 需要一种更可扩展和多用途的基于变压器的物理模拟代理模型架构，以构建物理科学中的大规模基础模型。

**方法:** 结合扩散变压器的最新架构改进与针对大规模模拟的调整，提出了一种新的嵌入不同物理通道的方法，并通过通道自注意力进行交互。

**结果:** PDE-Transformer在16种不同类型偏微分方程的大数据集上优于现有技术，在多个具有挑战性的下游任务中比从头开始训练表现更好。

**结论:** PDE-Transformer是物理模拟的通用和可扩展的变压器架构，优于最先进的计算机视觉变压器架构和其他物理模拟基础模型架构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PDE-Transformer%3A+Efficient+and+Versatile+Transformers+for+Physics+Simulations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24717，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24717&send_immediately=true&force_search=false)

**原文摘要:** We introduce PDE-Transformer, an improved transformer-based architecture for
surrogate modeling of physics simulations on regular grids. We combine recent
architectural improvements of diffusion transformers with adjustments specific
for large-scale simulations to yield a more scalable and versatile
general-purpose transformer architecture, which can be used as the backbone for
building large-scale foundation models in physical sciences. We demonstrate
that our proposed architecture outperforms state-of-the-art transformer
architectures for computer vision on a large dataset of 16 different types of
PDEs. We propose to embed different physical channels individually as
spatio-temporal tokens, which interact via channel-wise self-attention. This
helps to maintain a consistent information density of tokens when learning
multiple types of PDEs simultaneously. We demonstrate that our pre-trained
models achieve improved performance on several challenging downstream tasks
compared to training from scratch and also beat other foundation model
architectures for physics simulations.

</details>


### [126] [Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach](https://arxiv.org/abs/2505.24721)
*Nick Rossenbach, Benedikt Hilmes, Leon Brackmann, Moritz Gunz, Ralf Schlüter*

**主要类别:** cs.LG

**AI概要:** 该论文介绍了一种基于忆阻器硬件的高效机器学习模拟库，展示了其在大规模语音识别任务中的应用潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前忆阻器硬件原型无法容纳大型神经网络，相关文献仅涉及小型机器学习模型，因此需要一种方法来探索硬件属性对更大模型的影响。

**方法:** 论文提出了一种基于PyTorch的库“Synaptogen”，用以模拟神经网络在准确捕捉忆阻器硬件属性下的执行情况，并采用调整后的量化感知训练方法来优化性能。

**结果:** 论文结果显示，通过模拟忆阻器硬件上的线性操作，在使用3位权重精度时，词错误率的相对退化被限制在25%以内。

**结论:** 论文得出结论，使用基于忆阻器硬件的系统能够支持具有数百万参数的机器学习模型，且通过调整量化感知训练，可以将词错误率的相对退化限制在25%以内。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Running+Conventional+Automatic+Speech+Recognition+on+Memristor+Hardware%3A+A+Simulated+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24721&send_immediately=true&force_search=false)

**原文摘要:** Memristor-based hardware offers new possibilities for energy-efficient
machine learning (ML) by providing analog in-memory matrix multiplication.
Current hardware prototypes cannot fit large neural networks, and related
literature covers only small ML models for tasks like MNIST or single word
recognition. Simulation can be used to explore how hardware properties affect
larger models, but existing software assumes simplified hardware. We propose a
PyTorch-based library based on "Synaptogen" to simulate neural network
execution with accurately captured memristor hardware properties. For the first
time, we show how an ML system with millions of parameters would behave on
memristor hardware, using a Conformer trained on the speech recognition task
TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the
relative degradation in word error rate to 25% when using a 3-bit weight
precision to execute linear operations via simulated analog computation.

</details>


### [127] [Robust Federated Learning against Model Perturbation in Edge Networks](https://arxiv.org/abs/2505.24728)
*Dongzi Jin, Yong Xiao, Yingyu Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的联邦学习方法SMRFL，通过利用模型景观的几何特性，在不牺牲收敛速度的情况下有效提升了模型对扰动的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习(FL)在实践中由于各种扰动导致共享模型的假设被违反，进而引起性能显著下降，因此需要增强模型的鲁棒性。

**方法:** 提出了一种名为Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL)的新方法，通过解决一个最小-最大优化问题来促进模型向平坦最小值收敛，从而提高鲁棒性。

**结果:** 理论结果证明了SMRFL可以达到与无扰动FL相同的收敛速度，并且实验结果显示，与三种基线方法相比，SMRFL在两个真实数据集上的三种扰动情景下显著增强了鲁棒性。

**结论:** SMRFL方法在联邦学习中提高了模型对扰动的鲁棒性，同时保持了与无扰动联邦学习相同的收敛速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Federated+Learning+against+Model+Perturbation+in+Edge+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24728，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24728&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) is a promising paradigm for realizing edge
intelligence, allowing collaborative learning among distributed edge devices by
sharing models instead of raw data. However, the shared models are often
assumed to be ideal, which would be inevitably violated in practice due to
various perturbations, leading to significant performance degradation. To
overcome this challenge, we propose a novel method, termed Sharpness-Aware
Minimization-based Robust Federated Learning (SMRFL), which aims to improve
model robustness against perturbations by exploring the geometrical property of
the model landscape. Specifically, SMRFL solves a min-max optimization problem
that promotes model convergence towards a flat minimum by minimizing the
maximum loss within a neighborhood of the model parameters. In this way, model
sensitivity to perturbations is reduced, and robustness is enhanced since
models in the neighborhood of the flat minimum also enjoy low loss values. The
theoretical result proves that SMRFL can converge at the same rate as FL
without perturbations. Extensive experimental results show that SMRFL
significantly enhances robustness against perturbations compared to three
baseline methods on two real-world datasets under three perturbation scenarios.

</details>


### [128] [Feature Attribution from First Principles](https://arxiv.org/abs/2505.24729)
*Magamed Taimeskhanov, Damien Garreau*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的特征归因框架，它通过基于简单模型的归因构建更复杂的模型，克服了传统公理化框架过于严格的缺点，并为深度ReLU网络提供了有效的归因方法。


<details>
  <summary>更多</summary>
  
**动机:** 特征归因方法是解释机器学习模型行为的一种流行方法，但经验评估这些方法仍是一个重大挑战。作者认为现有公理体系往往过于严格，因此提出了一个新的框架以解决这一问题。

**方法:** 论文提出了一种新的特征归因方法，首先为最简单的模型（即指示函数）定义归因，并将这些作为构建更复杂模型的基础。随后，作者推导了深度ReLU网络的归因闭式表达式，并朝着优化评价指标的方向迈进了一步。

**结果:** 作者展示了根据不同的原子归因选择可以恢复几种现有的归因方法，并为深度ReLU网络的归因提供了闭式表达式，同时在优化评价指标方面取得了进展。

**结论:** 本文提出了一种新的特征归因框架，该框架通过从最简单的模型开始定义归因而不是施加公理来构建更复杂的模型，并展示了如何根据不同原子归因的选择恢复几种现有的归因方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature+Attribution+from+First+Principles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24729&send_immediately=true&force_search=false)

**原文摘要:** Feature attribution methods are a popular approach to explain the behavior of
machine learning models. They assign importance scores to each input feature,
quantifying their influence on the model's prediction. However, evaluating
these methods empirically remains a significant challenge. To bypass this
shortcoming, several prior works have proposed axiomatic frameworks that any
feature attribution method should satisfy. In this work, we argue that such
axioms are often too restrictive, and propose in response a new feature
attribution framework, built from the ground up. Rather than imposing axioms,
we start by defining attributions for the simplest possible models, i.e.,
indicator functions, and use these as building blocks for more complex models.
We then show that one recovers several existing attribution methods, depending
on the choice of atomic attribution. Subsequently, we derive closed-form
expressions for attribution of deep ReLU networks, and take a step toward the
optimization of evaluation metrics with respect to feature attributions.

</details>


### [129] [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
*Yehonathan Refael, Guy Smorodinsky, Tom Tirer, Ofir Lindenbaum*

**主要类别:** cs.LG

**AI概要:** 本文提出了SUMO优化器，通过精确的奇异值分解（SVD）在低维子空间中进行动量正交化，从而提高大型语言模型训练的收敛速度和性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的低秩梯度优化方法虽然提高了内存效率，但忽略了由于使用标准各向同性最速下降技术而导致的收敛速度潜在提升。

**方法:** 提出SUMO优化器，采用精确的SVD进行动量正交化，并动态调整低维子空间中的优化步骤。

**结果:** 理论分析表明了近似误差的上界，并证明了这些误差与动量条件数相关；实证结果表明SUMO提升了收敛速度、稳定性以及性能，同时减少了内存需求。

**结论:** SUMO优化器通过精确正交化显著改善了大型语言模型的训练效果，为现有方法提供了改进方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SUMO%3A+Subspace-Aware+Moment-Orthogonalization+for+Accelerating+Memory-Efficient+LLM+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24749&send_immediately=true&force_search=false)

**原文摘要:** Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential acceleration in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.

</details>


### [130] [AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption](https://arxiv.org/abs/2505.24773)
*Yajie Zhou, Xiaoyi Pang, Zhibo Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了AFLoRA，一种针对大型语言模型的轻量级联邦微调框架，解决了因客户端资源有限和数据非独立同分布导致的性能瓶颈问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于客户端的数据和系统资源异质且受限，现有的参数高效技术（如LoRA）在确保低系统成本的同时难以保证低秩更新的准确聚合，因此需要提出新的方法解决这些问题。

**方法:** AFLoRA通过解耦共享和客户端特定的更新、基于对角矩阵的秩剪枝以及利用公共数据优化的秩感知聚合来改进全局模型性能。

**结果:** 实验表明，AFLoRA在准确性和效率方面均优于现有最先进的方法。

**结论:** AFLoRA提供了一种高效的LLM自适应解决方案，尤其适用于异构环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AFLoRA%3A+Adaptive+Federated+Fine-Tuning+of+Large+Language+Models+with+Resource-Aware+Low-Rank+Adaption，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24773&send_immediately=true&force_search=false)

**原文摘要:** Federated fine-tuning has emerged as a promising approach to adapt foundation
models to downstream tasks using decentralized data. However, real-world
deployment remains challenging due to the high computational and communication
demands of fine-tuning Large Language Models (LLMs) on clients with data and
system resources that are heterogeneous and constrained. In such settings, the
global model's performance is often bottlenecked by the weakest clients and
further degraded by the non-IID nature of local data. Although existing methods
leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to
reduce communication and computation overhead, they often fail to
simultaneously ensure accurate aggregation of low-rank updates and maintain low
system costs, thereby hindering overall performance. To address these
challenges, we propose AFLoRA, an adaptive and lightweight federated
fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific
updates to reduce overhead and improve aggregation accuracy, incorporates
diagonal matrix-based rank pruning to better utilize local resources, and
employs rank-aware aggregation with public data refinement to strengthen
generalization under data heterogeneity. Extensive experiments demonstrate that
AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,
providing a practical solution for efficient LLM adaptation in heterogeneous
environments in the real world.

</details>


### [131] [Diffusion-Based Symbolic Regression](https://arxiv.org/abs/2505.24776)
*Zachary Bastiani, Robert M. Kirby, Jacob Hochhalter, Shandian Zhe*

**主要类别:** cs.LG

**AI概要:** 本文提出一种创新的扩散方法解决符号回归问题，效果显著。


<details>
  <summary>更多</summary>
  
**动机:** 受扩散模型在生成建模领域成功应用的启发，研究者试图将其应用于符号回归问题。

**方法:** 构建了基于随机掩码的扩散和去噪过程，并结合token-wise Group Relative Policy Optimization (GRPO) 方法进行高效的强化学习。

**结果:** 广泛实验和消融研究表明，所提出的扩散方法在符号回归任务中表现良好。

**结论:** 论文提出了一种基于扩散的新方法用于符号回归，实验表明该方法有效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion-Based+Symbolic+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24776，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24776&send_immediately=true&force_search=false)

**原文摘要:** Diffusion has emerged as a powerful framework for generative modeling,
achieving remarkable success in applications such as image and audio synthesis.
Enlightened by this progress, we propose a novel diffusion-based approach for
symbolic regression. We construct a random mask-based diffusion and denoising
process to generate diverse and high-quality equations. We integrate this
generative processes with a token-wise Group Relative Policy Optimization
(GRPO) method to conduct efficient reinforcement learning on the given
measurement dataset. In addition, we introduce a long short-term risk-seeking
policy to expand the pool of top-performing candidates, further enhancing
performance. Extensive experiments and ablation studies have demonstrated the
effectiveness of our approach.

</details>


### [132] [EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation](https://arxiv.org/abs/2505.24779)
*Yidong Luo, Chenguang Wang, Jiahao Yang, Fanzeng Xia, Tianshu Yu*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种用于评估混合整数线性规划（MILP）实例生成方法的新基准框架，通过分析求解器内部特征来评估实例质量和计算相似性。


<details>
  <summary>更多</summary>
  
**动机:** 由于MILP实例生成方法的增长速度超过了标准化评估技术的发展，因此需要一种新的方法来准确评估这些生成方法的质量和实用性。

**方法:** 通过深入分析求解器内部特征，如根节点间隙、启发式成功率和割平面使用情况等关键求解器输出分布，提出统一且可扩展的方法来评估实例质量。

**结果:** 展示了一个有效的框架，能够系统地比较不同实例集的真实性，并利用当前生成模型证明了其有效性。

**结论:** 该论文提出了一种用于系统和客观评估MILP实例生成方法的综合基准框架，旨在促进不同生成技术之间的稳健比较，并最终提高依赖合成MILP数据的研究可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EVA-MILP%3A+Towards+Standardized+Evaluation+of+MILP+Instance+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24779，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24779&send_immediately=true&force_search=false)

**原文摘要:** Mixed-Integer Linear Programming (MILP) is fundamental to solving complex
decision-making problems. The proliferation of MILP instance generation
methods, driven by machine learning's demand for diverse optimization datasets
and the limitations of static benchmarks, has significantly outpaced
standardized evaluation techniques. Consequently, assessing the fidelity and
utility of synthetic MILP instances remains a critical, multifaceted challenge.
This paper introduces a comprehensive benchmark framework designed for the
systematic and objective evaluation of MILP instance generation methods. Our
framework provides a unified and extensible methodology, assessing instance
quality across crucial dimensions: mathematical validity, structural
similarity, computational hardness, and utility in downstream machine learning
tasks. A key innovation is its in-depth analysis of solver-internal features --
particularly by comparing distributions of key solver outputs including root
node gap, heuristic success rates, and cut plane usage -- leveraging the
solver's dynamic solution behavior as an `expert assessment' to reveal nuanced
computational resemblances. By offering a structured approach with clearly
defined solver-independent and solver-dependent metrics, our benchmark aims to
facilitate robust comparisons among diverse generation techniques, spur the
development of higher-quality instance generators, and ultimately enhance the
reliability of research reliant on synthetic MILP data. The framework's
effectiveness in systematically comparing the fidelity of instance sets is
demonstrated using contemporary generative models.

</details>


### [133] [QGAN-based data augmentation for hybrid quantum-classical neural networks](https://arxiv.org/abs/2505.24780)
*Run-Ze He, Jun-Jian Su, Su-Juan Qin, Zheng-Ping Jin, Fei Gao*

**主要类别:** cs.LG

**AI概要:** This paper proposes a quantum data augmentation framework using QGANs integrated with HQCNNs, demonstrating improved performance with fewer parameters on the MNIST dataset.


<details>
  <summary>更多</summary>
  
**动机:** Data scarcity remains a challenge in quantum machine learning, and existing classical data augmentation methods may not be optimal for quantum models. The study aims to explore how QGANs can address this issue and improve the performance of hybrid quantum-classical models.

**方法:** The study integrates quantum generative adversarial networks (QGANs) with hybrid quantum-classical neural networks (HQCNNs) to develop a data augmentation framework. Two strategies are proposed: a general approach for enhancing data processing and classification across HQCNNs and a customized strategy that dynamically generates samples tailored to the HQCNN's performance on specific data categories.

**结果:** Simulation experiments on the MNIST dataset show that QGAN outperforms traditional data augmentation methods and classical GANs. Compared to baseline DCGAN, QGAN achieves comparable performance with half the parameters, balancing efficiency and effectiveness.

**结论:** QGANs can simplify models and generate high-quality data, enhancing HQCNN accuracy and performance, which suggests that quantum data augmentation techniques can be effectively applied in machine learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QGAN-based+data+augmentation+for+hybrid+quantum-classical+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24780&send_immediately=true&force_search=false)

**原文摘要:** Quantum neural networks converge faster and achieve higher accuracy than
classical models. However, data augmentation in quantum machine learning
remains underexplored. To tackle data scarcity, we integrate quantum generative
adversarial networks (QGANs) with hybrid quantum-classical neural networks
(HQCNNs) to develop an augmentation framework. We propose two strategies: a
general approach to enhance data processing and classification across HQCNNs,
and a customized strategy that dynamically generates samples tailored to the
HQCNN's performance on specific data categories, improving its ability to learn
from complex datasets. Simulation experiments on the MNIST dataset demonstrate
that QGAN outperforms traditional data augmentation methods and classical GANs.
Compared to baseline DCGAN, QGAN achieves comparable performance with half the
parameters, balancing efficiency and effectiveness. This suggests that QGANs
can simplify models and generate high-quality data, enhancing HQCNN accuracy
and performance. These findings pave the way for applying quantum data
augmentation techniques in machine learning.

</details>


### [134] [ByzFL: Research Framework for Robust Federated Learning](https://arxiv.org/abs/2505.24802)
*Marc González, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, François Taïani*

**主要类别:** cs.LG

**AI概要:** ByzFL 是一个用于开发和基准测试鲁棒联邦学习算法的开源 Python 库，提供了统一框架、聚合器、攻击套件和场景模拟工具。


<details>
  <summary>更多</summary>
  
**动机:** 开发和基准测试鲁棒联邦学习算法的需求。

**方法:** 提供一个统一且可扩展的框架，包括最先进的鲁棒聚合器、可配置攻击套件以及模拟各种 FL 场景的工具。

**结果:** 通过单一的基于 JSON 的配置文件实现系统实验，并包含结果可视化的内置工具。

**结论:** ByzFL 是一个支持 PyTorch 张量和 NumPy 数组的开源库，旨在促进鲁棒联邦学习解决方案的可重复研究和快速原型设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ByzFL%3A+Research+Framework+for+Robust+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24802&send_immediately=true&force_search=false)

**原文摘要:** We present ByzFL, an open-source Python library for developing and
benchmarking robust federated learning (FL) algorithms. ByzFL provides a
unified and extensible framework that includes implementations of
state-of-the-art robust aggregators, a suite of configurable attacks, and tools
for simulating a variety of FL scenarios, including heterogeneous data
distributions, multiple training algorithms, and adversarial threat models. The
library enables systematic experimentation via a single JSON-based
configuration file and includes built-in utilities for result visualization.
Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to
facilitate reproducible research and rapid prototyping of robust FL solutions.
ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on
GitHub: https://github.com/LPD-EPFL/byzfl.

</details>


### [135] [Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting](https://arxiv.org/abs/2505.24835)
*Fuyuan Lyu, Linfeng Du, Yunpeng Weng, Qiufang Ying, Zhiyan Xu, Wen Zou, Haolun Wu, Xiuqiang He, Xing Tang*

**主要类别:** cs.LG

**AI概要:** 本文提出了RTS-PnO框架，用于解决资金分配中的目标不一致和预测不确定性问题，实验结果显示其性能优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的资金分配方法（如仅预测或先预测后优化）存在目标不一致的问题，同时使用最先进的时序预测模型会带来额外的预测不确定性，因此需要一种更优的解决方案。

**方法:** 论文引入了Risk-aware Time-Series Predict-and-Allocate (RTS-PnO) 框架，其主要特点包括端到端训练、目标对齐测量、自适应预测不确定性校准以及与预测模型无关的特性。

**结果:** 通过离线和在线实验评估，RTS-PnO在八个金融数据集上均表现优异，并在线上实验中相比现有方法减少了8.4%的遗憾值。

**结论:** 论文提出了一种新的资金分配框架RTS-PnO，它能够有效解决目标不匹配问题，并在预测结果中进行自适应的不确定性校准。实验表明该框架优于其他基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Timing+is+important%3A+Risk-aware+Fund+Allocation+based+on+Time-Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24835，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24835&send_immediately=true&force_search=false)

**原文摘要:** Fund allocation has been an increasingly important problem in the financial
domain. In reality, we aim to allocate the funds to buy certain assets within a
certain future period. Naive solutions such as prediction-only or
Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the
introduction of the SOTA time series forecasting model inevitably introduces
additional uncertainty in the predicted result. To solve both problems
mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate
(RTS-PnO) framework, which holds no prior assumption on the forecasting models.
Such a framework contains three features: (i) end-to-end training with
objective alignment measurement, (ii) adaptive forecasting uncertainty
calibration, and (iii) agnostic towards forecasting models. The evaluation of
RTS-PnO is conducted over both online and offline experiments. For offline
experiments, eight datasets from three categories of financial applications are
used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other
competitive baselines. The online experiment is conducted on the Cross-Border
Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed
when compared with the product-line approach. The code for the offline
experiment is available at https://github.com/fuyuanlyu/RTS-PnO.

</details>


### [136] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, Alina Oprea*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了模型蒸馏中对抗性偏差注入的脆弱性，并提出需要专门的安全措施来应对这一问题。


<details>
  <summary>更多</summary>
  
**动机:** 模型蒸馏在创建较小的可部署语言模型方面变得至关重要，但广泛部署引发了对对抗性操纵的担忧。

**方法:** 该研究展示了对手如何通过最小的数据投毒将偏差注入教师模型，并提出了两种传播模式：无目标传播和有目标传播。

**结果:** 研究表明，学生模型比教师模型更容易受到偏差注入攻击，且当前的防御方法对此类攻击效果不佳。

**结论:** 论文得出结论，蒸馏模型存在严重的安全漏洞，需要专门的安全措施来防止对抗性偏差注入。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cascading+Adversarial+Bias+from+Injection+to+Distillation+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24842，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24842&send_immediately=true&force_search=false)

**原文摘要:** Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [137] [From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching](https://arxiv.org/abs/2505.24843)
*Ruqi Bai, Yao Ji, Zeyu Zhou, David I. Inouye*

**主要类别:** cs.LG

**AI概要:** 本研究提出了NCM方法，通过使用不变数据对增强机器学习模型的鲁棒性，尤其适用于数据有限的情况。


<details>
  <summary>更多</summary>
  
**动机:** 虚假相关性会导致模型在新环境中的性能下降，而现有的因果启发方法通常表现不佳。本文旨在解决这些问题并提升模型的泛化能力。

**方法:** 引入了噪声反事实匹配（NCM），这是一种基于约束的方法，利用具有不变性质的反事实样本来增强模型的稳健性。

**结果:** 实验表明，在合成数据集上验证了该方法的有效性，并在实际基准测试中展示了预训练主干上的线性探测如何提高鲁棒性。

**结论:** 论文提出了一种名为NCM的方法，通过利用不变的数据对来提高模型的鲁棒性，并且即使在仅有少量噪声数据的情况下也能有效工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Invariant+Representations+to+Invariant+Data%3A+Provable+Robustness+to+Spurious+Correlations+via+Noisy+Counterfactual+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24843，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24843&send_immediately=true&force_search=false)

**原文摘要:** Spurious correlations can cause model performance to degrade in new
environments. Prior causality-inspired works aim to learn invariant
representations (e.g., IRM) but typically underperform empirical risk
minimization (ERM). Recent alternatives improve robustness by leveraging
test-time data, but such data may be unavailable in practice. To address these
issues, we take a data-centric approach by leveraging invariant data pairs,
pairs of samples that would have the same prediction with the optimally robust
classifier. We prove that certain counterfactual pairs will naturally satisfy
this invariance property and introduce noisy counterfactual matching (NCM), a
simple constraint-based method for leveraging invariant pairs for enhanced
robustness, even with a small set of noisy pairs-in the ideal case, each pair
can eliminate one spurious feature. For linear causal models, we prove that the
test domain error can be upper bounded by the in-domain error and a term that
depends on the counterfactuals' diversity and quality. We validate on a
synthetic dataset and demonstrate on real-world benchmarks that linear probing
on a pretrained backbone improves robustness.

</details>


### [138] [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/abs/2505.24844)
*Wanyun Xie, Francesco Tonin, Volkan Cevher*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Chameleon的新框架，用于高效地进行数据混合和域加权，以优化大型语言模型的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的域重新加权方法通常依赖于昂贵的权重计算，并且在引入新数据时需要重新训练。

**方法:** 构建一个域关联矩阵，并利用诱导出的杠杆分数来决定数据集的混合比例，从而在嵌入空间中加强共享共同表示的域的重要性。

**结果:** 实验表明，与现有方法相比，Chameleon在预训练域上的性能有了显著提升，同时能够适应数据变化而无需代理重新训练，并在微调过程中实现了高效的域重新加权。

**结论:** Chameleon是一个灵活且高效的数据混合框架，它通过使用杠杆分数来量化域重要性以提高大型语言模型的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chameleon%3A+A+Flexible+Data-mixing+Framework+for+Language+Model+Pretraining+and+Finetuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24844&send_immediately=true&force_search=false)

**原文摘要:** Training data mixtures greatly impact the generalization performance of large
language models. Existing domain reweighting methods often rely on costly
weight computations and require retraining when new data is introduced. To this
end, we introduce a flexible and efficient data mixing framework, Chameleon,
that employs leverage scores to quantify domain importance within a learned
embedding space. We first construct a domain affinity matrix over domain
embeddings. The induced leverage scores determine a mixture that upweights
domains sharing common representations in embedding space. This formulation
allows direct transfer to new data by computing the new domain embeddings. In
experiments, we demonstrate improvements over three key scenarios: (i) our
computed weights improve performance on pretraining domains with a fraction of
the compute of existing methods; (ii) Chameleon can adapt to data changes
without proxy retraining, boosting few-shot reasoning accuracies when
transferred to new data; (iii) our method enables efficient domain reweighting
in finetuning, consistently improving test perplexity on all finetuning domains
over uniform mixture. Our code is available at
https://github.com/LIONS-EPFL/Chameleon.

</details>


### [139] [Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking](https://arxiv.org/abs/2505.24857)
*Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, Brian Karrer*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为EB-Sampler的新采样方法，可在不降低性能的前提下显著提高掩码扩散模型（MDMs）的采样效率。


<details>
  <summary>更多</summary>
  
**动机:** 尽管MDMs在语言建模方面表现优异，但其高效采样尚未得到充分研究。标准采样方法未能充分利用单次预测中可能包含的多标记信息。

**方法:** 基于部分掩码序列可确定多个未知标记值的观察，引入了EB-Sampler，采用熵有界去掩码程序，在一次函数评估中动态解掩多个标记，同时控制误差容忍度。

**结果:** EB-Sampler在当前最先进的MDMs上将采样速度提高了约2-3倍，且适用于多种任务，包括编码、数学推理、迷宫导航和数独。

**结论:** EB-Sampler在不损失性能的情况下加速了MDMs的采样过程，并且适用于各种任务，包括编码、数学推理、迷宫导航和数独。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerated+Sampling+from+Masked+Diffusion+Models+via+Entropy+Bounded+Unmasking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24857&send_immediately=true&force_search=false)

**原文摘要:** Recent masked diffusion models (MDMs) have shown competitive performance
compared to autoregressive models (ARMs) for language modeling. While most
literature has focused on performance enhancing sampling procedures, efficient
sampling from MDMs has been scarcely explored. We make the observation that
often a given sequence of partially masked tokens determines the values of
multiple unknown tokens deterministically, meaning that a single prediction of
a masked model holds additional information unused by standard sampling
procedures. Based on this observation, we introduce EB-Sampler, a simple
drop-in replacement for existing samplers, utilizing an Entropy Bounded
unmasking procedure that dynamically unmasks multiple tokens in one function
evaluation with predefined approximate error tolerance. We formulate the
EB-Sampler as part of a broad family of adaptive samplers for which we provide
an error analysis that motivates our algorithmic choices. EB-Sampler
accelerates sampling from current state of the art MDMs by roughly 2-3x on
standard coding and math reasoning benchmarks without loss in performance. We
also validate the same procedure works well on smaller reasoning tasks
including maze navigation and Sudoku, tasks ARMs often struggle with.

</details>


### [140] [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
*Joschka Braun, Carsten Eickhoff, Seyed Ali Bahrainian*

**主要类别:** cs.LG

**AI概要:** 本论文研究了转向向量在自由生成任务中的应用，发现其能有效控制文本属性，但高强度转向会影响文本质量；相较而言，提示方法控制力较弱但保持文本质量，两者结合可以取得最佳的效果和质量平衡。


<details>
  <summary>更多</summary>
  
**动机:** 转向向量迄今为止主要在多项选择环境中进行评估，而其在自由生成任务中的有效性研究尚不足。

**方法:** 我们全面评估了NEWTS数据集上抽象摘要中的主题焦点、情感、毒性以及可读性的适应性控制效果，并与提示方法进行了比较。

**结果:** 我们发现转向向量能够有效控制目标摘要属性，但高强度的转向会持续降低内在和外在文本质量。与转向相比，提示提供了较弱的控制力，同时保持了文本质量。结合转向和提示可以获得最强的文本属性控制能力，并在适度的转向强度下实现最有效的质量和效果平衡。

**结论:** 应用转向向量到自由生成任务中时，控制力度和文本质量保持之间存在实际的权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Multiple+Choice%3A+Evaluating+Steering+Vectors+for+Adaptive+Free-Form+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24859&send_immediately=true&force_search=false)

**原文摘要:** Steering vectors are a lightweight method for controlling text properties by
adding a learned bias to language model activations at inference time. So far,
steering vectors have predominantly been evaluated in multiple-choice settings,
while their effectiveness in free-form generation tasks remains understudied.
Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of
steering vectors in adaptively controlling topical focus, sentiment, toxicity,
and readability in abstractive summaries of the NEWTS dataset. We find that
steering effectively controls the targeted summary properties, but high
steering strengths consistently degrade both intrinsic and extrinsic text
quality. Compared to steering, prompting offers weaker control, while
preserving text quality. Combining steering and prompting yields the strongest
control over text properties and offers the most favorable efficacy-quality
trade-off at moderate steering strengths. Our results underscore the practical
trade-off between control strength and text quality preservation when applying
steering vectors to free-form generation tasks.

</details>


### [141] [The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models](https://arxiv.org/abs/2505.24874)
*Adam Stein, Aaditya Naik, Neelay Velingker, Mayur Naik, Eric Wong*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了基础模型如何改进传统神经符号学习的局限性，提出通过神经符号提示来实现更通用的解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 随着基础模型的出现，纯神经网络模型通过提示而非训练达到了最先进的性能，但它们往往不可靠且缺乏可解释性。因此，作者提出问题：神经符号学习中的专门模型训练在基础模型时代中应扮演什么角色？

**方法:** 论文通过分析传统神经符号学习在计算、数据和程序方面的三个缺陷，探讨了基础模型与符号程序结合的可能性。

**结果:** 作者指出，将基础模型与符号程序结合（即神经符号提示）为复杂推理任务提供了一种新方法，并有望解决传统神经符号学习的问题。

**结论:** 本文讨论了传统神经符号学习的局限性，并提出了基础模型如何使可推广的神经符号解决方案成为可能，从而实现神经符号学习的最初目标，而无需从头开始训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Road+to+Generalizable+Neuro-Symbolic+Learning+Should+be+Paved+with+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24874&send_immediately=true&force_search=false)

**原文摘要:** Neuro-symbolic learning was proposed to address challenges with training
neural networks for complex reasoning tasks with the added benefits of
interpretability, reliability, and efficiency. Neuro-symbolic learning methods
traditionally train neural models in conjunction with symbolic programs, but
they face significant challenges that limit them to simplistic problems. On the
other hand, purely-neural foundation models now reach state-of-the-art
performance through prompting rather than training, but they are often
unreliable and lack interpretability. Supplementing foundation models with
symbolic programs, which we call neuro-symbolic prompting, provides a way to
use these models for complex reasoning tasks. Doing so raises the question:
What role does specialized model training as part of neuro-symbolic learning
have in the age of foundation models? To explore this question, we highlight
three pitfalls of traditional neuro-symbolic learning with respect to the
compute, data, and programs leading to generalization problems. This position
paper argues that foundation models enable generalizable neuro-symbolic
solutions, offering a path towards achieving the original goals of
neuro-symbolic learning without the downsides of training from scratch.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [142] [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/abs/2505.23881)
*Christopher D. Rosin*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为CPro1的方法，它利用带有推理能力的大型语言模型来解决复杂的组合设计问题，这些问题在之前的技术条件下长时间未能得到解答。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）具有推理能力，可以迭代生成和优化答案，这对数学和代码生成的应用有帮助。

**方法:** 从特定类型设计的文本定义和有效性验证器开始，CPro1指导LLM选择并实现策略，并提供自动超参数调优和执行反馈。

**结果:** CPro1利用推理LLMs成功解决了2006年《组合设计手册》中选取的16个组合设计问题中的7个长期未解的问题，其中包括3个新解决的问题实例（Bhaskar Rao设计、对称加权矩阵、平衡三元设计），同时解决了来自最新文献的一些问题，如覆盖序列、Johnson团覆盖、删除码和均匀嵌套Steiner四重系统。

**结论:** CPro1与推理LLM一起成功解决了长期未解的组合设计问题，包括一些使用非推理LLM无法解决的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Using+Reasoning+Models+to+Generate+Search+Heuristics+that+Solve+Open+Instances+of+Combinatorial+Design+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23881&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) with reasoning are trained to iteratively
generate and refine their answers before finalizing them, which can help with
applications to mathematics and code generation. We apply code generation with
reasoning LLMs to a specific task in the mathematical field of combinatorial
design. This field studies diverse types of combinatorial designs, many of
which have lists of open instances for which existence has not yet been
determined. The Constructive Protocol CPro1 uses LLMs to generate search
heuristics that have the potential to construct solutions to small open
instances. Starting with a textual definition and a validity verifier for a
particular type of design, CPro1 guides LLMs to select and implement
strategies, while providing automated hyperparameter tuning and execution
feedback. CPro1 with reasoning LLMs successfully solves long-standing open
instances for 7 of 16 combinatorial design problems selected from the 2006
Handbook of Combinatorial Designs, including new solved instances for 3 of
these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary
Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves
open instances for several problems from recent (2025) literature, generating
new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform
Nested Steiner Quadruple System.

</details>


### [143] [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
*Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, Guohao Li*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的分层多智能体框架Workforce，通过模块化设计和强化学习实现跨领域的高效迁移，取得了优异性能。


<details>
  <summary>更多</summary>
  
**动机:** 基于大语言模型的多智能体系统在跨领域转移时面临完全重新设计架构和重新训练所有组件的问题，需要更好的方法实现跨领域迁移。

**方法:** 引入了Workforce，一种分层多智能体框架，通过(i)领域无关的Planner进行任务分解，(ii)Coordinator进行子任务管理，以及(iii)具有领域特定工具调用能力的Worker，解耦战略规划和专门执行，并使用强化学习优化领域无关的Planner。

**结果:** Workforce在GAIA基准测试中达到了开源最先进的性能（69.70%），比OpenAI的Deep Research高出2.34%；OWL训练的32B模型达到52.73%准确率（+16.37%），在挑战性任务上表现与GPT-4o相当。

**结论:** Workforce实现了可扩展的泛化和模块化领域迁移，为下一代通用AI助手奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OWL%3A+Optimized+Workforce+Learning+for+General+Multi-Agent+Assistance+in+Real-World+Task+Automation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23885，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23885&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM)-based multi-agent systems show promise for
automating real-world tasks but struggle to transfer across domains due to
their domain-specific nature. Current approaches face two critical
shortcomings: they require complete architectural redesign and full retraining
of all components when applied to new domains. We introduce Workforce, a
hierarchical multi-agent framework that decouples strategic planning from
specialized execution through a modular architecture comprising: (i) a
domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask
management, and (iii) specialized Workers with domain-specific tool-calling
capabilities. This decoupling enables cross-domain transferability during both
inference and training phases: During inference, Workforce seamlessly adapts to
new domains by adding or modifying worker agents; For training, we introduce
Optimized Workforce Learning (OWL), which improves generalization across
domains by optimizing a domain-agnostic planner with reinforcement learning
from real-world feedback. To validate our approach, we evaluate Workforce on
the GAIA benchmark, covering various realistic, multi-domain agentic tasks.
Experimental results demonstrate Workforce achieves open-source
state-of-the-art performance (69.70%), outperforming commercial systems like
OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model
achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to
GPT-4o on challenging tasks. To summarize, by enabling scalable generalization
and modular domain transfer, our work establishes a foundation for the next
generation of general-purpose AI assistants.

</details>


### [144] [Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve](https://arxiv.org/abs/2505.23946)
*Yuanzhe Liu, Ryan Deng, Tim Kaler, Xuhao Chen, Charles E. Leiserson, Yao Ma, Jie Chen*

**主要类别:** cs.AI

**AI概要:** 本文研究了如何通过一个基于课程的学习框架，让多个小型语言模型（LLM）代理在不知道彼此优势的情况下相互学习，从而提升整体性能。


<details>
  <summary>更多</summary>
  
**动机:** 观察到不同LLMs在不同任务和优化类别中的表现各异，没有一个LLM能在所有方面都占优，因此提出了如何在不了解LLMs互补优势的情况下利用多个LLM代理解决编码问题的问题。

**方法:** 提出了一种基于课程的合作框架，并设计了课程征集-银行-选择机制。

**结果:** 论文展示了一个LLM代理团队可以通过相互学习成功和失败的经验来提高自己的性能。

**结论:** 研究表明，通过团队合作和知识共享，小型LLM可以超越更大的LLM和其他多LLM协作方法的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lessons+Learned%3A+A+Multi-Agent+Framework+for+Code+LLMs+to+Learn+and+Improve，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23946&send_immediately=true&force_search=false)

**原文摘要:** Recent studies show that LLMs possess different skills and specialize in
different tasks. In fact, we observe that their varied performance occur in
several levels of granularity. For example, in the code optimization task, code
LLMs excel at different optimization categories and no one dominates others.
This observation prompts the question of how one leverages multiple LLM agents
to solve a coding problem without knowing their complementary strengths a
priori. We argue that a team of agents can learn from each other's successes
and failures so as to improve their own performance. Thus, a lesson is the
knowledge produced by an agent and passed on to other agents in the collective
solution process. We propose a lesson-based collaboration framework, design the
lesson solicitation--banking--selection mechanism, and demonstrate that a team
of small LLMs with lessons learned can outperform a much larger LLM and other
multi-LLM collaboration methods.

</details>


### [145] [InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback](https://arxiv.org/abs/2505.23950)
*Boyuan Chen, Donghai Hong, Jiaming Ji, Jiacheng Zheng, Bowen Dong, Jiayi Zhou, Kaile Wang, Juntao Dai, Xuyao Wang, Wenqi Chen, Qirui Zheng, Wenxin Li, Sirui Han, Yike Guo, Yaodong Yang*

**主要类别:** cs.AI

**AI概要:** 该论文介绍了InterMT，一个用于多轮多模态交互的首个偏好数据集，强调了人类监督的重要性，并提出了InterMT-Bench来评估多模态大模型的能力。


<details>
  <summary>更多</summary>
  
**动机:** 目前的多模态大模型缺乏复杂的交互能力，尤其是连续的多模态理解与生成能力，而这是向人类智能迈进的关键一步。

**方法:** 通过InterMT这一偏好数据集进行初步探索，并引入了一个基于工具增强的多模态大模型的工作流来构建多轮问答实例，进一步提出了InterMT-Bench以评估MLLMs在多轮、多模态任务中的能力。

**结果:** InterMT数据集包含了15.6k提示、52.6k多轮对话实例和32.4k人工标注的偏好对，捕捉了全局和局部的人类偏好，并分为九个子维度。

**结论:** 作者希望开放源码的数据能够帮助对齐当前的多模态大模型（MLLMs）到下一步的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InterMT%3A+Multi-Turn+Interleaved+Preference+Alignment+with+Human+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23950&send_immediately=true&force_search=false)

**原文摘要:** As multimodal large models (MLLMs) continue to advance across challenging
tasks, a key question emerges: What essential capabilities are still missing? A
critical aspect of human learning is continuous interaction with the
environment -- not limited to language, but also involving multimodal
understanding and generation. To move closer to human-level intelligence,
models must similarly support multi-turn, multimodal interaction. In
particular, they should comprehend interleaved multimodal contexts and respond
coherently in ongoing exchanges. In this work, we present an initial
exploration through the InterMT -- the first preference dataset for multi-turn
multimodal interaction, grounded in real human feedback. In this exploration,
we particularly emphasize the importance of human oversight, introducing expert
annotations to guide the process, motivated by the fact that current MLLMs lack
such complex interactive capabilities. InterMT captures human preferences at
both global and local levels into nine sub-dimensions, consists of 15.6k
prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled
preference pairs. To compensate for the lack of capability for multi-modal
understanding and generation, we introduce an agentic workflow that leverages
tool-augmented MLLMs to construct multi-turn QA instances. To further this
goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting
judges with multi-turn, multimodal tasks. We demonstrate the utility of
\InterMT through applications such as judge moderation and further reveal the
multi-turn scaling law of judge model. We hope the open-source of our data can
help facilitate further research on aligning current MLLMs to the next step.
Our project website can be found at https://pku-intermt.github.io .

</details>


### [146] [MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge](https://arxiv.org/abs/2505.23982)
*Jerry Junyang Cheung, Shiyao Shen, Yuchen Zhuang, Yinghao Li, Rampi Ramprasad, Chao Zhang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了MSQA，一个用于评估材料科学领域LLM的知识和推理能力的新基准。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在材料科学方面取得了进展，但缺乏评估其领域专业知识和复杂推理能力的基准。

**方法:** 介绍了一个名为MSQA的全面评估基准，包含1,757个研究生级别的材料科学问题，以详细解释性回答和二元真/假评估两种形式出现。

**结果:** 实验结果显示基于API的专有LLMs准确率高达84.5%，而开源LLMs峰值约为60.5%。

**结论:** MSQA是第一个联合评估LLMs在高级材料科学中的事实和推理能力的基准测试，它揭示了当前LLMs性能的重大差距，并且指出领域特定的LLMs由于过拟合和分布位移而常常表现不佳。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MSQA%3A+Benchmarking+LLMs+on+Graduate-Level+Materials+Science+Reasoning+and+Knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23982&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances in large language models (LLMs) for materials
science, there is a lack of benchmarks for evaluating their domain-specific
knowledge and complex reasoning abilities. To bridge this gap, we introduce
MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials
science questions in two formats: detailed explanatory responses and binary
True/False assessments. MSQA distinctively challenges LLMs by requiring both
precise factual knowledge and multi-step reasoning across seven materials
science sub-fields, such as structure-property relationships, synthesis
processes, and computational modeling. Through experiments with 10
state-of-the-art LLMs, we identify significant gaps in current LLM performance.
While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source
(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform
significantly due to overfitting and distributional shifts. MSQA represents the
first benchmark to jointly evaluate the factual and reasoning capabilities of
LLMs crucial for LLMs in advanced materials science.

</details>


### [147] [Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding](https://arxiv.org/abs/2505.23990)
*Mingyang Mao, Mariela M. Perez-Cabarcas, Utteja Kallakuri, Nicholas R. Waytowich, Xiaomin Lin, Tinoosh Mohsenin*

**主要类别:** cs.AI

**AI概要:** 本文介绍了Multi-RAG，一种多模态检索增强生成系统，旨在减轻人类认知负担并在动态、信息丰富的场景中提供自适应帮助。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器人和智能代理越来越多地融入人类生活，在动态、信息丰富的场景中将人类的认知负担转移给这些系统的需求日益增长。因此，开发能够适应不断变化情况的系统变得至关重要。

**方法:** 作者提出了Multi-RAG，这是一种多模态检索增强生成系统，旨在为人类在信息密集型环境中提供自适应帮助。他们通过在MMBench-Video数据集上进行基准测试来评估该系统的性能。

**结果:** Multi-RAG在MMBench-Video数据集上的表现优于现有的开源视频大语言模型和大型视觉-语言模型，同时使用更少的资源和较少的输入数据。

**结论:** Multi-RAG通过整合和推理多源信息流，包括视频、音频和文本，可以有效降低人类的认知负荷，并提高情境理解能力。它在MMBench-Video数据集上的表现优于现有的开源视频大语言模型和大型视觉-语言模型，同时使用更少的资源和输入数据，展示了其作为未来人机自适应协助系统基础的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-RAG%3A+A+Multimodal+Retrieval-Augmented+Generation+System+for+Adaptive+Video+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23990&send_immediately=true&force_search=false)

**原文摘要:** To effectively engage in human society, the ability to adapt, filter
information, and make informed decisions in ever-changing situations is
critical. As robots and intelligent agents become more integrated into human
life, there is a growing opportunity-and need-to offload the cognitive burden
on humans to these systems, particularly in dynamic, information-rich
scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal
retrieval-augmented generation system designed to provide adaptive assistance
to humans in information-intensive circumstances. Our system aims to improve
situational understanding and reduce cognitive load by integrating and
reasoning over multi-source information streams, including video, audio, and
text. As an enabling step toward long-term human-robot partnerships, Multi-RAG
explores how multimodal information understanding can serve as a foundation for
adaptive robotic assistance in dynamic, human-centered situations. To evaluate
its capability in a realistic human-assistance proxy task, we benchmarked
Multi-RAG on the MMBench-Video dataset, a challenging multimodal video
understanding benchmark. Our system achieves superior performance compared to
existing open-source video large language models (Video-LLMs) and large
vision-language models (LVLMs), while utilizing fewer resources and less input
data. The results demonstrate Multi- RAG's potential as a practical and
efficient foundation for future human-robot adaptive assistance systems in
dynamic, real-world contexts.

</details>


### [148] [GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs](https://arxiv.org/abs/2505.24036)
*Amel Gader, Alsayed Algergawy*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种基于大语言模型的端到端实例补全方法GenIC，用于知识图谱补全任务。


<details>
  <summary>更多</summary>
  
**动机:** 现代知识库中包含实体描述和类型信息，可以为推断缺失事实提供有价值的情境，但如何有效利用这些信息仍存在挑战。

**方法:** 提出GenIC框架，分为两步：第一步是多标签分类任务进行属性预测，第二步是生成式序列到序列任务进行链接预测。

**结果:** 在三个数据集上的实验结果表明，该方法优于现有的基线方法。

**结论:** 通过利用大语言模型从文本描述中提取事实并识别知识图谱模式的能力，GenIC在知识图谱实例补全方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GenIC%3A+An+LLM-Based+Framework+for+Instance+Completion+in+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24036&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graph completion aims to address the gaps of knowledge bases by
adding new triples that represent facts. The complexity of this task depends on
how many parts of a triple are already known. Instance completion involves
predicting the relation-tail pair when only the head is given (h, ?, ?).
Notably, modern knowledge bases often contain entity descriptions and types,
which can provide valuable context for inferring missing facts. By leveraging
these textual descriptions and the ability of large language models to extract
facts from them and recognize patterns within the knowledge graph schema, we
propose an LLM-powered, end-to-end instance completion approach. Specifically,
we introduce GenIC: a two-step Generative Instance Completion framework. The
first step focuses on property prediction, treated as a multi-label
classification task. The second step is link prediction, framed as a generative
sequence-to-sequence task. Experimental results on three datasets show that our
method outperforms existing baselines. Our code is available at
https://github.com/amal-gader/genic.

</details>


### [149] [Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution](https://arxiv.org/abs/2505.24037)
*Qiao Xiao, Alan Ansell, Boqian Wu, Lu Yin, Mykola Pechenizkiy, Shiwei Liu, Decebal Constantin Mocanu*

**主要类别:** cs.AI

**AI概要:** 本文提出了Sparsity Evolution Fine-Tuning (SEFT)，这是一种针对大型语言模型稀疏微调的新方法，它能够在保持模型稀疏性的同时提高模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于大型语言模型（LLMs）面临部署挑战，因此需要有效的后训练剪枝和微调方法。然而，现有的方法不能很好地保持稀疏性，限制了它们在下游任务中的应用。

**方法:** 提出了一种名为Sparsity Evolution Fine-Tuning (SEFT)的方法，通过权重下降和增长策略进行任务特定适应，并采用敏感性驱动的剪枝标准以保持所需的稀疏水平。

**结果:** 在各种LLM和基准测试中，SEFT表现出优于现有方法的性能，同时提供更好的内存和时间效率。

**结论:** SEFT是一种专为稀疏LLM设计的新方法，在微调过程中动态演化剪枝模型的稀疏拓扑结构，同时保持整体稀疏性。实验表明，SEFT在各种LLM和基准测试中实现了比现有基线更强的性能，同时提供了卓越的内存和时间效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leave+it+to+the+Specialist%3A+Repair+Sparse+LLMs+with+Sparse+Fine-Tuning+via+Sparsity+Evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24037&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have achieved remarkable success across various
tasks but face deployment challenges due to their massive computational
demands. While post-training pruning methods like SparseGPT and Wanda can
effectively reduce the model size, but struggle to maintain model performance
at high sparsity levels, limiting their utility for downstream tasks. Existing
fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve
sparsity as they require updating the whole dense metrics, not well-suited for
sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a
novel method designed specifically for sparse LLMs. SEFT dynamically evolves
the sparse topology of pruned models during fine-tuning, while preserving the
overall sparsity throughout the process. The strengths of SEFT lie in its
ability to perform task-specific adaptation through a weight drop-and-grow
strategy, enabling the pruned model to self-adapt its sparse connectivity
pattern based on the target dataset. Furthermore, a sensitivity-driven pruning
criterion is employed to ensure that the desired sparsity level is consistently
maintained throughout fine-tuning. Our experiments on various LLMs, including
LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks
demonstrate that SEFT achieves stronger performance while offering superior
memory and time efficiency compared to existing baselines. Our code is publicly
available at: https://github.com/QiaoXiao7282/SEFT.

</details>


### [150] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Zhengzhong Tu*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了如何利用检索增强生成（RAG）技术改善大型视觉-语言模型（LVLMs）的表现，提出了一种新的多模态RAG流水线分析方法以及一个统一的代理框架，使模型能够在不进行微调的情况下获得5%的平均性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型视觉-语言模型（LVLMs）受限于静态训练数据，容易产生幻觉，且无法验证基于最新外部证据的主张。这限制了其在动态现实应用中的表现。因此，需要一种可行的解决方案来缓解这些问题。

**方法:** 论文系统性地分析了多模态RAG流水线的三个主要阶段：检索阶段、重新排序阶段和生成阶段，并进一步研究了一种统一的代理框架，将重新排序和生成结合在一起进行自省优化。

**结果:** 论文首次对用于LVLMs的多模态RAG流水线进行了系统分析，并提出了一个统一的代理框架，能够通过自省机制动态选择相关证据并抑制无关上下文，从而显著提升模型性能。

**结论:** 论文得出结论，通过全面探索RAG方法，可以显著提高LVLMs的性能，实现5%的平均性能提升，且无需微调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是mRAG%3A+Elucidating+the+Design+Space+of+Multi-modal+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24073，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24073&send_immediately=true&force_search=false)

**原文摘要:** Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [151] [SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought](https://arxiv.org/abs/2505.24181)
*Guanghao Li, Wenhao Jiang, Mingfeng Chen, Yan Li, Hao Yu, Shuting Dong, Tao Ren, Ming Tang, Chun Yuan*

**主要类别:** cs.AI

**AI概要:** 该论文提出 Flow CoT 和 SCOUT 方法，通过递归推理建模和轻量级微调显著提升大语言模型的推理性能和解释质量。


<details>
  <summary>更多</summary>
  
**动机:** 传统基于 Chain of Thought (CoT) 的方法依赖于显式的中间推理步骤，限制了模型的可扩展性和泛化能力；而现有的递归推理方法通常需要昂贵的预训练，且缺乏对迭代中推理过程演化的理论指导。因此，本文提出了 Flow CoT 和 SCOUT 解决这些问题。

**方法:** 提出了一种名为 Flow Chain of Thought (Flow CoT) 的推理范式和 SCOUT（Stepwise Cognitive Optimization Using Teachers）的轻量级微调框架，通过递归推理建模认知状态的渐进轨迹，并利用逐步蒸馏与回顾模块实现无需预训练的推理增强。

**结果:** 实验结果显示，在八个推理基准测试中，SCOUT 在微调条件下实现了最高达 1.8% 的准确率提升，同时提升了推理的深度和解释的精细度。

**结论:** Flow CoT 以及 SCOUT 框架在提升 LLM 推理能力和解释质量方面是有效的，并且 Flow CoT 被证明是一种具有实用性的可扩展推理框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCOUT%3A+Teaching+Pre-trained+Language+Models+to+Enhance+Reasoning+via+Flow+Chain-of-Thought，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24181&send_immediately=true&force_search=false)

**原文摘要:** Chain of Thought (CoT) prompting improves the reasoning performance of large
language models (LLMs) by encouraging step by step thinking. However, CoT-based
methods depend on intermediate reasoning steps, which limits scalability and
generalization. Recent work explores recursive reasoning, where LLMs reuse
internal layers across iterations to refine latent representations without
explicit CoT supervision. While promising, these approaches often require
costly pretraining and lack a principled framework for how reasoning should
evolve across iterations. We address this gap by introducing Flow Chain of
Thought (Flow CoT), a reasoning paradigm that models recursive inference as a
progressive trajectory of latent cognitive states. Flow CoT frames each
iteration as a distinct cognitive stage deepening reasoning across iterations
without relying on manual supervision. To realize this, we propose SCOUT
(Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning
framework that enables Flow CoT style reasoning without the need for
pretraining. SCOUT uses progressive distillation to align each iteration with a
teacher of appropriate capacity, and a cross attention based retrospective
module that integrates outputs from previous iterations while preserving the
models original computation flow. Experiments across eight reasoning benchmarks
show that SCOUT consistently improves both accuracy and explanation quality,
achieving up to 1.8% gains under fine tuning. Qualitative analyses further
reveal that SCOUT enables progressively deeper reasoning across iterations
refining both belief formation and explanation granularity. These results not
only validate the effectiveness of SCOUT, but also demonstrate the practical
viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.

</details>


### [152] [Learning API Functionality from Demonstrations for Tool-based Agents](https://arxiv.org/abs/2505.24197)
*Bhrij Patel, Ashish Jagmohan, Aditya Vempaty*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了一种新的范式：直接从演示中学习API功能，以应对缺乏文档的情况。虽然提供明确的函数调用和自然语言批评能提高任务成功率，但研究显示这仍是具有挑战性的问题。


<details>
  <summary>更多</summary>
  
**动机:** 该论文旨在解决基于数字工具的代理在调用外部API时依赖文档的问题，因为这些文档经常缺失、过时、私有化或不一致，阻碍了可靠通用代理的发展。

**方法:** 论文的方法包括使用现有的API基准测试，收集来自专家API代理和自我探索的演示数据，并研究演示数量以及LLM生成的摘要和评估如何影响任务成功率。

**结果:** 实验结果表明，从演示中学习API功能仍然是一个非平凡的挑战。提供明确的函数调用和自然语言批评能够显著提高代理的任务成功率，主要原因是参数填充更加准确。

**结论:** 该论文得出结论，即使对于最先进的LLM来说，从演示中学习API功能仍然是一个非平凡的挑战。通过提供明确的函数调用和自然语言批评可以显著提高代理的任务成功率，但仍然存在关键的开放性挑战，特别是在无文档、自我改进的基于API的代理领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+API+Functionality+from+Demonstrations+for+Tool-based+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24197&send_immediately=true&force_search=false)

**原文摘要:** Digital tool-based agents that invoke external Application Programming
Interfaces (APIs) often rely on documentation to understand API functionality.
However, such documentation is frequently missing, outdated, privatized, or
inconsistent-hindering the development of reliable, general-purpose agents. In
this work, we propose learning API functionality directly from demonstrations
as a new paradigm applicable in scenarios without documentation. Using existing
API benchmarks, we collect demonstrations from both expert API-based agents and
from self-exploration. To understand what information demonstrations must
convey for successful task completion, we extensively study how the number of
demonstrations and the use of LLM-generated summaries and evaluations affect
the task success rate of the API-based agent. Our experiments across 3 datasets
and 5 models show that learning functionality from demonstrations remains a
non-trivial challenge, even for state-of-the-art LLMs. We find that providing
explicit function calls and natural language critiques significantly improves
the agent's task success rate due to more accurate parameter filling. We
analyze failure modes, identify sources of error, and highlight key open
challenges for future work in documentation-free, self-improving, API-based
agents.

</details>


### [153] [SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems](https://arxiv.org/abs/2505.24201)
*Xu He, Di Wu, Yan Zhai, Kun Sun*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种用于大型语言模型驱动的多智能体系统的系统级异常检测框架，能够有效识别多种安全风险。


<details>
  <summary>更多</summary>
  
**动机:** 现有的防护机制主要在输入输出层面提供部分保护，难以应对MAS中的系统性或多点故障问题。

**方法:** 结合结构建模和运行时行为监督的方法，包括一个基于图的框架和一个插件式的SentinelAgent。

**结果:** 通过两个案例研究验证了该框架的有效性，包括电子邮件助手和微软的Magentic-One系统，能够检测到单点故障、提示注入、多智能体勾结和潜在的利用路径。

**结论:** 这项工作为基于LLM的多智能体系统提供了一个系统级的异常检测框架，旨在提升系统的安全性、可监控性和可信度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SentinelAgent%3A+Graph-based+Anomaly+Detection+in+Multi-Agent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24201&send_immediately=true&force_search=false)

**原文摘要:** The rise of large language model (LLM)-based multi-agent systems (MAS)
introduces new security and reliability challenges. While these systems show
great promise in decomposing and coordinating complex tasks, they also face
multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent
agent miscoordination. Existing guardrail mechanisms offer only partial
protection, primarily at the input-output level, and fall short in addressing
systemic or multi-point failures in MAS. In this work, we present a
system-level anomaly detection framework tailored for MAS, integrating
structural modeling with runtime behavioral oversight. Our approach consists of
two components. First, we propose a graph-based framework that models agent
interactions as dynamic execution graphs, enabling semantic anomaly detection
at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,
an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS
execution based on security policies and contextual reasoning. By bridging
abstract detection logic with actionable enforcement, our method detects not
only single-point faults and prompt injections but also multi-agent collusion
and latent exploit paths. We validate our framework through two case studies,
including an email assistant and Microsoft's Magentic-One system, demonstrating
its ability to detect covert risks and provide explainable root-cause
attribution. Our work lays the foundation for more trustworthy, monitorable,
and secure agent-based AI ecosystems.

</details>


### [154] [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/abs/2505.24208)
*Wenhan Yang, Spencer Stice, Ali Payani, Baharan Mirzasoleiman*

**主要类别:** cs.AI

**AI概要:** 该论文探讨了视觉语言模型中模态差距对安全性的影响，并提出一种新方法在预训练阶段减少模态差距，以显著提高模型的安全对齐度。


<details>
  <summary>更多</summary>
  
**动机:** LVLMs相较于其LLM主干存在显著的安全性下降问题，甚至无关图像也可能导致生成有害响应，因此需要研究模态差距对安全性的影响及改善方法。

**方法:** 通过分析LVLMs在不同阶段的模态差距，发现其在预训练阶段引入并在微调阶段持续存在，进而提出一种正则化方法来减少预训练期间的模态差距。

**结果:** 实验表明所提方法可将LVLM的不安全率降低最多16.3%，同时不影响性能，并且可以进一步提升现有防御机制的效果最多18.2%。

**结论:** 研究表明模态差距与视觉语言模型的安全性呈高度负相关，并提出了一种在预训练期间减少模态差距的方法，从而显著提高LVLM的安全对齐度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bootstrapping+LLM+Robustness+for+VLM+Safety+via+Reducing+the+Pretraining+Modality+Gap，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24208&send_immediately=true&force_search=false)

**原文摘要:** Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for
their reliable deployment. However, LVLMs suffer from drastic safety
degradation compared to their LLM backbone. Even blank or irrelevant images can
trigger LVLMs to generate harmful responses to prompts that would otherwise be
refused in text-only contexts. The modality gap between image and text
representations has been recently hypothesized to contribute to safety
degradation of LVLMs. However, if and how the amount of modality gap affects
LVLMs' safety is not studied. In this work, we show that the amount of modality
gap is highly inversely correlated with VLMs' safety. Then, we show that this
modality gap is introduced during pretraining LVLMs and persists through
fine-tuning. Inspired by this observation, we propose a regularization to
reduce the modality gap during pretraining. Our extensive experiments on LLaVA
v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves
safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without
compromising performance, and can further boost existing defenses by up to
18.2%.

</details>


### [155] [E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness](https://arxiv.org/abs/2505.24226)
*Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, Xiang Li*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为E^2GraphRAG的新框架，通过使用摘要树、实体图以及自适应检索策略，在提高效率的同时保持了优秀的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于图的RAG方法效率低下且依赖手动预定义查询模式，限制了实际应用。

**方法:** 构建基于摘要树和实体图的双向索引，并设计了自适应检索策略。

**结果:** 实验显示E^2GraphRAG在检索速度上显著优于现有方法，同时保持了良好的问答性能。

**结论:** E^2GraphRAG在保持竞争力的问答性能的同时，实现了比GraphRAG快10倍的索引速度和比LightRAG快100倍的检索速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是E%5E2GraphRAG%3A+Streamlining+Graph-based+RAG+for+High+Efficiency+and+Effectiveness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24226，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24226&send_immediately=true&force_search=false)

**原文摘要:** Graph-based RAG methods like GraphRAG have shown promising global
understanding of the knowledge base by constructing hierarchical entity graphs.
However, they often suffer from inefficiency and rely on manually pre-defined
query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a
streamlined graph-based RAG framework that improves both Efficiency and
Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree
with large language models and an entity graph with SpaCy based on document
chunks. We then construct bidirectional indexes between entities and chunks to
capture their many-to-many relationships, enabling fast lookup during both
local and global retrieval. For the retrieval stage, we design an adaptive
retrieval strategy that leverages the graph structure to retrieve and select
between local and global modes. Experiments show that E^2GraphRAG achieves up
to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG
in retrieval while maintaining competitive QA performance.

</details>


### [156] [ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction](https://arxiv.org/abs/2505.24230)
*Murari Ambati*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了 ProofNet++，一种结合大型语言模型与形式化验证及自我修正机制的框架，有效解决了现有系统的逻辑幻觉和不可验证问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于大型语言模型的系统存在逻辑步骤幻觉和不可验证推理的问题，因此提出了 ProofNet++ 来解决这些问题。

**方法:** 论文提出了一种结合大型语言模型与形式化验证及自我修正机制的神经符号框架 ProofNet++，并采用强化学习循环和符号证明树监督等方法进行实验。

**结果:** 在 miniF2F、Lean 的 mathlib 和 HOL Light 上的实验表明，ProofNet++ 显著提高了证明的准确性、正确性和形式化可验证性。

**结论:** 论文得出结论，ProofNet++ 在自动化定理证明方面显著优于之前的模型，并提供了理论分析和资源以供未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProofNet%2B%2B%3A+A+Neuro-Symbolic+System+for+Formal+Proof+Verification+with+Self-Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24230&send_immediately=true&force_search=false)

**原文摘要:** We propose ProofNet++, a neuro-symbolic framework that enhances automated
theorem proving by combining large language models (LLMs) with formal proof
verification and self-correction mechanisms. Current LLM-based systems suffer
from hallucinated logical steps and unverifiable reasoning. ProofNet++
mitigates these limitations by integrating symbolic proof tree supervision, a
reinforcement learning loop using verifiers as reward functions, and an
iterative self-correction module. Our experiments on miniF2F, Lean's mathlib,
and HOL Light show that ProofNet++ significantly improves proof accuracy,
correctness, and formal verifiability over prior models. We provide theoretical
analysis of the convergence and stability of the verifier-guided RL framework
and release our datasets and codebase for future research.

</details>


### [157] [FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation](https://arxiv.org/abs/2505.24258)
*Vishal Pallagani, Nitin Gupta, John Aydin, Biplav Srivastava*

**主要类别:** cs.AI

**AI概要:** 本文介绍了FABLE基准测试，用于评估大型语言模型的数据流推理能力，并发现推理模型虽然更准确但速度更慢。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在自然和编程语言方面表现出色，但它们在涉及过程任务的决策中的数据流推理能力尚未得到系统评估。

**方法:** 介绍了一个名为FABLE的可扩展基准测试，改编了八种经典的数据流分析技术，并在三个现实领域中进行应用，包括烹饪食谱、旅行路线和自动化计划。

**结果:** 推理模型DeepSeek-R1 8B在准确性上表现更高，但其推理速度比其他模型慢20多倍。而通用模型LLaMA 3.1 8B和代码专用模型Granite Code 8B的表现接近随机猜测。

**结论:** FABLE提供了首个用于系统评估数据流推理能力的诊断基准，并为开发具有更强程序理解能力的模型提供了见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FABLE%3A+A+Novel+Data-Flow+Analysis+Benchmark+on+Procedural+Text+for+Large+Language+Model+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24258，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24258&send_immediately=true&force_search=false)

**原文摘要:** Understanding how data moves, transforms, and persists, known as data flow,
is fundamental to reasoning in procedural tasks. Despite their fluency in
natural and programming languages, large language models (LLMs), although
increasingly being applied to decisions with procedural tasks, have not been
systematically evaluated for their ability to perform data-flow reasoning. We
introduce FABLE, an extensible benchmark designed to assess LLMs' understanding
of data flow using structured, procedural text. FABLE adapts eight classical
data-flow analyses from software engineering: reaching definitions, very busy
expressions, available expressions, live variable analysis, interval analysis,
type-state analysis, taint analysis, and concurrency analysis. These analyses
are instantiated across three real-world domains: cooking recipes, travel
routes, and automated plans. The benchmark includes 2,400 question-answer
pairs, with 100 examples for each domain-analysis combination. We evaluate
three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a
general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code
8B). Each model is tested using majority voting over five sampled completions
per prompt. Results show that the reasoning model achieves higher accuracy, but
at the cost of over 20 times slower inference compared to the other models. In
contrast, the general-purpose and code-specific models perform close to random
chance. FABLE provides the first diagnostic benchmark to systematically
evaluate data-flow reasoning and offers insights for developing models with
stronger procedural understanding.

</details>


### [158] [Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models](https://arxiv.org/abs/2505.24260)
*Mingyi He, Yuebing Liang, Shenhao Wang, Yunhan Zheng, Qingyi Wang, Dingyi Zhuang, Li Tian, Jinhua Zhao*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种结合多模态扩散模型与人类专业知识的分步生成式城市设计框架，旨在提高设计过程的适应性和可控性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法通常采用端到端流程，缺乏对现实世界设计迭代性质的关注，需要更好地与人类设计工作流集成。

**方法:** 提出了一种分阶段的生成式城市设计框架，并通过芝加哥和纽约市的数据进行实验验证。

**结果:** 实验表明，该框架在保真度、合规性和多样性方面均优于基线模型和端到端方法。

**结论:** 该研究强调了多模态扩散模型和逐步生成在保持人类控制和促进迭代改进中的优势，为城市设计中的人机交互奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+AI+for+Urban+Design%3A+A+Stepwise+Approach+Integrating+Human+Expertise+with+Multimodal+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24260&send_immediately=true&force_search=false)

**原文摘要:** Urban design is a multifaceted process that demands careful consideration of
site-specific constraints and collaboration among diverse professionals and
stakeholders. The advent of generative artificial intelligence (GenAI) offers
transformative potential by improving the efficiency of design generation and
facilitating the communication of design ideas. However, most existing
approaches are not well integrated with human design workflows. They often
follow end-to-end pipelines with limited control, overlooking the iterative
nature of real-world design. This study proposes a stepwise generative urban
design framework that integrates multimodal diffusion models with human
expertise to enable more adaptive and controllable design processes. Instead of
generating design outcomes in a single end-to-end process, the framework
divides the process into three key stages aligned with established urban design
workflows: (1) road network and land use planning, (2) building layout
planning, and (3) detailed planning and rendering. At each stage, multimodal
diffusion models generate preliminary designs based on textual prompts and
image-based constraints, which can then be reviewed and refined by human
designers. We design an evaluation framework to assess the fidelity,
compliance, and diversity of the generated designs. Experiments using data from
Chicago and New York City demonstrate that our framework outperforms baseline
models and end-to-end approaches across all three dimensions. This study
underscores the benefits of multimodal diffusion models and stepwise generation
in preserving human control and facilitating iterative refinements, laying the
groundwork for human-AI interaction in urban design solutions.

</details>


### [159] [How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning](https://arxiv.org/abs/2505.24273)
*Hongyi James Cai, Junlin Wang, Xiaoyin Chen, Bhuwan Dhingra*

**主要类别:** cs.AI

**AI概要:** 本文研究了监督微调与强化学习在复杂推理任务中的相互作用，揭示了长推理链与回溯机制对训练效果的影响，并为优化大语言模型的推理能力提供了实践指导。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有研究表明强化学习能够有效提升大语言模型的推理能力，但回溯机制的具体贡献以及其最佳使用方式仍不清楚。

**方法:** 作者通过系统分析监督微调和强化学习在八种推理任务上的动态关系，构建了合成数据集来控制回溯步骤的数量，并通过实验评估了推理性能的变化。

**结果:** 研究发现，短推理链在监督微调阶段对强化学习训练有适度帮助，但在任务难度增加时作用减弱；更长的带回溯的推理链有助于提升训练效果，而强化学习主要受结构模式驱动而非内容正确性。

**结论:** 论文得出结论，较长的带有回溯的推理链通常会带来更好且更稳定的强化学习训练，同时表明在更具挑战性的问题上需要更高频率的回溯。此外，研究发现强化学习对长推理链的正确性并不敏感，而是更关注结构模式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Much+Backtracking+is+Enough%3F+Exploring+the+Interplay+of+SFT+and+RL+in+Enhancing+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24273，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24273&send_immediately=true&force_search=false)

**原文摘要:** Recent breakthroughs in large language models (LLMs) have effectively
improved their reasoning abilities, particularly on mathematical and logical
problems that have verifiable answers, through techniques such as supervised
finetuning (SFT) and reinforcement learning (RL). Prior research indicates that
RL effectively internalizes search strategies, enabling long chain-of-thought
(CoT) reasoning, with backtracking emerging naturally as a learned capability.
However, the precise benefits of backtracking, specifically, how significantly
it contributes to reasoning improvements and the optimal extent of its use,
remain poorly understood. In this work, we systematically investigate the
dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc
1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self
Reference. Our findings highlight that short CoT sequences used in SFT as a
warm-up do have moderate contribution to RL training, compared with cold-start
RL; however such contribution diminishes when tasks become increasingly
difficult. Motivated by this observation, we construct synthetic datasets
varying systematically in the number of backtracking steps and conduct
controlled experiments to isolate the influence of either the correctness
(content) or the structure (i.e., backtrack frequency). We find that (1) longer
CoT with backtracks generally induce better and more stable RL training, (2)
more challenging problems with larger search space tend to need higher numbers
of backtracks during the SFT stage. Additionally, we demonstrate through
experiments on distilled data that RL training is largely unaffected by the
correctness of long CoT sequences, suggesting that RL prioritizes structural
patterns over content correctness. Collectively, our results offer practical
insights into designing optimal training strategies to effectively scale
reasoning in LLMs.

</details>


### [160] [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/abs/2505.24292)
*Yueqi Zhang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li*

**主要类别:** cs.AI

**AI概要:** 论文提出QuAda，一种轻量级的训练方法，增强大型语言模型对对话中引用文本的处理能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大语言模型缺乏一种明确的机制来定位和利用引用的文本，这在人与AI的对话中是常见的。

**方法:** 提出了QuAda，这是一种基于训练的轻量级方法，通过附加两个瓶颈投影到每个注意力头上，动态放大或抑制引用跨度的注意力。

**结果:** 实验表明，QuAda适合所有场景，并能推广到未见过的话题，提供了有效的引用感知对话解决方案。

**结论:** QuAda提供了一种有效的、即插即用的对话引用感知解决方案，适用于所有场景，并能推广到未见过的话题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mind+the+Quote%3A+Enabling+Quotation-Aware+Dialogue+in+LLMs+via+Plug-and-Play+Modules，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24292，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24292&send_immediately=true&force_search=false)

**原文摘要:** Human-AI conversation frequently relies on quoting earlier text-"check it
with the formula I just highlighted"-yet today's large language models (LLMs)
lack an explicit mechanism for locating and exploiting such spans. We formalise
the challenge as span-conditioned generation, decomposing each turn into the
dialogue history, a set of token-offset quotation spans, and an intent
utterance. Building on this abstraction, we introduce a quotation-centric data
pipeline that automatically synthesises task-specific dialogues, verifies
answer correctness through multi-stage consistency checks, and yields both a
heterogeneous training corpus and the first benchmark covering five
representative scenarios. To meet the benchmark's zero-overhead and
parameter-efficiency requirements, we propose QuAda, a lightweight
training-based method that attaches two bottleneck projections to every
attention head, dynamically amplifying or suppressing attention to quoted spans
at inference time while leaving the prompt unchanged and updating < 2.8% of
backbone weights. Experiments across models show that QuAda is suitable for all
scenarios and generalises to unseen topics, offering an effective,
plug-and-play solution for quotation-aware dialogue.

</details>


### [161] [GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments](https://arxiv.org/abs/2505.24306)
*Kechen Li, Yaotian Tao, Ximing Wen, Quanwei Sun, Zifei Gong, Chang Xu, Xizhe Zhang, Tianbo Ji*

**主要类别:** cs.AI

**AI概要:** 该论文提出了GridRoute基准和Algorithm of Thought (AoT)方法，用于探索大型语言模型（LLMs）如何借助传统算法提升路径规划能力，并展示了AoT在不同规模LLMs上的优异表现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs在规划和推理任务中显示出潜力，但现有研究主要关注其独立推理能力，忽略了LLMs与传统算法之间的协同潜力，因此本研究旨在填补这一空白。

**方法:** 论文提出了一种新的混合提示技术Algorithm of Thought (AoT)，并通过一个名为GridRoute的综合评估基准来评估LLMs利用传统算法的能力。

**结果:** 结果表明，AoT技术在所有模型尺寸上都显著提升了性能，特别是在较大或较复杂的环境中，显示了其在提高正确性、最优性和效率方面的有效性。

**结论:** 论文得出结论，AoT技术能够显著提升LLMs在路径规划问题中的表现，尤其是在更大或更复杂的环境中，这表明LLMs与传统算法结合具有解决路径规划挑战的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GridRoute%3A+A+Benchmark+for+LLM-Based+Route+Planning+with+Cardinal+Movement+in+Grid+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24306&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models (LLMs) have demonstrated their
potential in planning and reasoning tasks, offering a flexible alternative to
classical pathfinding algorithms. However, most existing studies focus on LLMs'
independent reasoning capabilities and overlook the potential synergy between
LLMs and traditional algorithms. To fill this gap, we propose a comprehensive
evaluation benchmark GridRoute to assess how LLMs can take advantage of
traditional algorithms. We also propose a novel hybrid prompting technique
called Algorithm of Thought (AoT), which introduces traditional algorithms'
guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to
72B parameters across various map sizes, assessing their performance in
correctness, optimality, and efficiency in grid environments with varying
sizes. Our results show that AoT significantly boosts performance across all
model sizes, particularly in larger or more complex environments, suggesting a
promising approach to addressing path planning challenges. Our code is
open-sourced at https://github.com/LinChance/GridRoute.

</details>


### [162] [Three Kinds of Negation in Knowledge and Their Mathematical Foundations](https://arxiv.org/abs/2505.24422)
*Zhenghua Pan, Yong Wang*

**主要类别:** cs.AI

**AI概要:** 该论文探讨了否定在多个领域中的理解与特性，提出了三种否定类型，并构建了相应的数学和逻辑框架。


<details>
  <summary>更多</summary>
  
**动机:** 理解、区分、表达和计算知识中的否定是知识处理和研究中的一个基本问题。

**方法:** 基于矛盾与对立概念的区别，分析哲学、逻辑学、语言学等领域的否定理解和特性。

**结果:** 引入了SCOI（具有矛盾否定、对立否定和中介否定的集合）和LCOI（具有矛盾否定、对立否定和中介否定的逻辑），并证明了其主要运算性质和形式推理关系。

**结论:** 本文提出了三种不同的否定类型，并建立了反映其内在联系和特性的数学基础SCOI和LCOI。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Three+Kinds+of+Negation+in+Knowledge+and+Their+Mathematical+Foundations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24422，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24422&send_immediately=true&force_search=false)

**原文摘要:** In the field of artificial intelligence, understanding, distinguishing,
expressing, and computing the negation in knowledge is a fundamental issue in
knowledge processing and research. In this paper, we examine and analyze the
understanding and characteristics of negation in various fields such as
philosophy, logic, and linguistics etc. Based on the distinction between the
concepts of contradiction and opposition, we propose that there are three
different types of negation in knowledge from a conceptual perspective:
contradictory negation, opposite negation, and intermediary negation. To
establish a mathematical foundation that fully reflects the intrinsic
connections, properties, and laws of these different forms of negation, we
introduce SCOI: sets with contradictory negation, opposite negation and
intermediary negation, and LCOI: logic with contradictory negation, opposite
negation and intermediary negation, and we proved the main operational
properties of SCOI as well as the formal inference relations in LCOI.

</details>


### [163] [P: A Universal Measure of Predictive Intelligence](https://arxiv.org/abs/2505.24426)
*David Gamez*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一个基于预测能力的通用智能度量标准，解决了当前AI领域缺乏统一衡量尺度的问题，并通过实验验证其可行性。


<details>
  <summary>更多</summary>
  
**动机:** 目前缺乏公认的定义和衡量人工智能智能度的方法，且理论发展落后于系统构建能力。

**方法:** 通过与环境互动评估代理预测准确性，并使用Kolmogorov复杂度考虑预测和感知环境的复杂性。

**结果:** 两个实验验证了该算法在虚拟迷宫代理和时间序列预测代理上的可行性。

**结论:** 本文提出了一种基于预测能力的通用智能度量方法，可对人类、动物和人工智能进行比较。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是P%3A+A+Universal+Measure+of+Predictive+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24426&send_immediately=true&force_search=false)

**原文摘要:** Over the last thirty years, considerable progress has been made with the
development of systems that can drive cars, play games, predict protein folding
and generate natural language. These systems are described as intelligent and
there has been a great deal of talk about the rapid increase in artificial
intelligence and its potential dangers. However, our theoretical understanding
of intelligence and ability to measure it lag far behind our capacity for
building systems that mimic intelligent human behaviour. There is no commonly
agreed definition of the intelligence that AI systems are said to possess.
No-one has developed a practical measure that would enable us to compare the
intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on
the hypothesis that prediction is the most important component of intelligence.
As an agent interacts with its normal environment, the accuracy of its
predictions is summed up and the complexity of its predictions and perceived
environment is accounted for using Kolmogorov complexity. Two experiments were
carried out to evaluate the practical feasibility of the algorithm. These
demonstrated that it could measure the intelligence of an agent embodied in a
virtual maze and an agent that makes predictions about time-series data. This
universal measure could be the starting point for a new comparative science of
intelligence that ranks humans, animals and AIs on a single ratio scale.

</details>


### [164] [RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation](https://arxiv.org/abs/2505.24442)
*Zhentao Xie, Chengcheng Han, Jinxin Shi, Wenjun Cui, Xin Zhao, Xingjiao Wu, Jiabao Zhao*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为Residual Mixture-of-Agents (RMoA)的新方法，通过引入残差连接和一系列创新机制，优化了多智能体系统的效率和可靠性，并在多个基准测试中取得了最先进的性能，同时显著降低了计算开销。


<details>
  <summary>更多</summary>
  
**动机:** 多智能体系统基于大语言模型虽然表现出强大的能力，但仍然受限于高计算开销、信息损失和鲁棒性问题。受ResNet残差学习的启发，作者提出了RMoA来优化效率和可靠性。

**方法:** 引入了基于残差连接的Residual Mixture-of-Agents (RMoA)，包括嵌入多样性选择机制、残差提取代理和残差聚合代理，并提出了基于残差收敛的自适应终止机制。

**结果:** RMoA在对齐、数学推理、代码生成和多任务理解的基准测试中达到了最先进的性能，并显著减少了计算开销。

**结论:** RMoA在多个基准测试中实现了最先进的性能，同时显著降低了计算开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RMoA%3A+Optimizing+Mixture-of-Agents+through+Diversity+Maximization+and+Residual+Compensation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24442&send_immediately=true&force_search=false)

**原文摘要:** Although multi-agent systems based on large language models show strong
capabilities on multiple tasks, they are still limited by high computational
overhead, information loss, and robustness. Inspired by ResNet's residual
learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual
connections to optimize efficiency and reliability. To maximize information
utilization from model responses while minimizing computational costs, we
innovatively design an embedding-based diversity selection mechanism that
greedily selects responses via vector similarity. Furthermore, to mitigate
iterative information degradation, we introduce a Residual Extraction Agent to
preserve cross-layer incremental information by capturing inter-layer response
differences, coupled with a Residual Aggregation Agent for hierarchical
information integration. Additionally, we propose an adaptive termination
mechanism that dynamically halts processing based on residual convergence,
further improving inference efficiency. RMoA achieves state-of-the-art
performance on the benchmarks of across alignment, mathematical reasoning, code
generation, and multitasking understanding, while significantly reducing
computational overhead. Code is available at
https://github.com/mindhunter01/RMoA.

</details>


### [165] [SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors](https://arxiv.org/abs/2505.24458)
*Tianlong Yu, Chenghang Ye, Zheyu Yang, Ziyi Zhou, Cui Tang, Zui Tao, Jun Zhang, Kailong Wang, Liting Zhou, Yang Yang, Ting Bi*

**主要类别:** cs.AI

**AI概要:** SEAR数据集揭示了基于增强现实的社会工程攻击在诱导用户信任和顺从方面的高效性，为未来的研究提供了基础资源。


<details>
  <summary>更多</summary>
  
**动机:** 随着增强现实和多模态大语言模型的发展，社会工程攻击成为新兴威胁，需要专门的数据集来推动相关研究。

**方法:** 构建了一个包含180个标注对话的多模态数据集，涵盖60名参与者在模拟对抗场景中的互动，并结合了同步的AR视觉/音频线索、环境背景、社交媒体资料及主观指标。

**结果:** 研究发现SEAR在诱导用户顺从（如93.3%的钓鱼链接点击率）和劫持信任（76.7%的交互后信任激增）方面表现出令人担忧的高效性。

**结论:** SEAR数据集为研究增强现实驱动的社会工程攻击提供了有价值的资源，支持检测攻击、设计防御框架和理解多模态对抗性操纵。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SEAR%3A+A+Multimodal+Dataset+for+Analyzing+AR-LLM-Driven+Social+Engineering+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24458&send_immediately=true&force_search=false)

**原文摘要:** The SEAR Dataset is a novel multimodal resource designed to study the
emerging threat of social engineering (SE) attacks orchestrated through
augmented reality (AR) and multimodal large language models (LLMs). This
dataset captures 180 annotated conversations across 60 participants in
simulated adversarial scenarios, including meetings, classes and networking
events. It comprises synchronized AR-captured visual/audio cues (e.g., facial
expressions, vocal tones), environmental context, and curated social media
profiles, alongside subjective metrics such as trust ratings and susceptibility
assessments. Key findings reveal SEAR's alarming efficacy in eliciting
compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and
hijacking trust (76.7% post-interaction trust surge). The dataset supports
research in detecting AR-driven SE attacks, designing defensive frameworks, and
understanding multimodal adversarial manipulation. Rigorous ethical safeguards,
including anonymization and IRB compliance, ensure responsible use. The SEAR
dataset is available at https://github.com/INSLabCN/SEAR-Dataset.

</details>


### [166] [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)
*Vasilije Markovic, Lazar Obradovic, Laszlo Hajdu, Jovan Pavlovic*

**主要类别:** cs.AI

**AI概要:** 本文研究了在Cognee框架中，通过超参数优化提高基于知识图谱和大语言模型的问答系统性能的问题。结果表明，针对性调优可显著提升效果，但不同数据集和评估指标间存在差异，强调了优化和评估方法的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 集成大型语言模型与知识图谱的系统包含大量直接影响性能的超参数，但目前对这类系统的超参数优化研究不足。

**方法:** 在Cognee端到端KG构建和检索框架下，使用三个多跳问答基准（HotPotQA、TwoWikiMultiHop和MuSiQue）进行实验，对分块、图构建、检索和提示等相关的参数进行优化，并采用准确率、F1分数和DeepEval的LLM正确性度量进行评分。

**结果:** 通过有针对性的调参可以实现有意义的效果提升，但在不同数据集和评估指标上表现不一致，显示出调参的价值以及标准评估指标的局限性。

**结论:** 虽然超参数调优具有潜力，但未来的发展不仅需要架构上的进步，还需更清晰的优化和评估框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+the+Interface+Between+Knowledge+Graphs+and+LLMs+for+Complex+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24478，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24478&send_immediately=true&force_search=false)

**原文摘要:** Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results
in complex systems with numerous hyperparameters that directly affect
performance. While such systems are increasingly common in retrieval-augmented
generation, the role of systematic hyperparameter optimization remains
underexplored. In this paper, we study this problem in the context of Cognee, a
modular framework for end-to-end KG construction and retrieval. Using three
multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize
parameters related to chunking, graph construction, retrieval, and prompting.
Each configuration is scored using established metrics (exact match, F1, and
DeepEval's LLM-based correctness metric). Our results demonstrate that
meaningful gains can be achieved through targeted tuning. While the gains are
consistent, they are not uniform, with performance varying across datasets and
metrics. This variability highlights both the value of tuning and the
limitations of standard evaluation measures. While demonstrating the immediate
potential of hyperparameter tuning, we argue that future progress will depend
not only on architectural advances but also on clearer frameworks for
optimization and evaluation in complex, modular systems.

</details>


### [167] [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/abs/2505.24479)
*Sania Nayab, Marco Simoni, Giulio Rossolini*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一种使用知识图谱生成虚假信息的新方法，并指出当前大型语言模型在检测此类信息时存在不足。


<details>
  <summary>更多</summary>
  
**动机:** 由于虚假信息迅速传播，并受到生成式人工智能最新进展的进一步放大，对社会构成重大威胁，影响公众舆论、民主稳定和国家安全。因此，论文旨在探索能够结构化和可扩展的虚假信息生成方法，以主动评估这些威胁。

**方法:** 论文提出了一种利用知识图谱（KGs）作为结构化语义资源来系统生成虚假三元组的新方法。通过分析知识图谱的结构特性，如实体和谓词之间的距离，识别出可能错误的关系。这些三元组随后用于引导大型语言模型生成不同程度可信度的虚假信息。

**结果:** 论文结果显示，所提出的确定性方法利用结构化语义关系生成了人类难以检测的虚假信息，并且完全依赖于公开可用的知识图谱（例如WikiGraphs）。此外，还发现当前的大型语言模型在区分真实和人工生成的虚假信息方面存在显著局限性。

**结论:** 论文得出结论，当前基于大型语言模型的检测方法存在显著局限性，强调需要改进的检测策略，并深入研究生成模型中的固有偏见。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Knowledge+Graphs+and+LLMs+for+Structured+Generation+of+Misinformation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24479，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24479&send_immediately=true&force_search=false)

**原文摘要:** The rapid spread of misinformation, further amplified by recent advances in
generative AI, poses significant threats to society, impacting public opinion,
democratic stability, and national security. Understanding and proactively
assessing these threats requires exploring methodologies that enable structured
and scalable misinformation generation. In this paper, we propose a novel
approach that leverages knowledge graphs (KGs) as structured semantic resources
to systematically generate fake triplets. By analyzing the structural
properties of KGs, such as the distance between entities and their predicates,
we identify plausibly false relationships. These triplets are then used to
guide large language models (LLMs) in generating misinformation statements with
varying degrees of credibility. By utilizing structured semantic relationships,
our deterministic approach produces misinformation inherently challenging for
humans to detect, drawing exclusively upon publicly available KGs (e.g.,
WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing
between genuine and artificially generated misinformation. Our analysis
highlights significant limitations in current LLM-based detection methods,
underscoring the necessity for enhanced detection strategies and a deeper
exploration of inherent biases in generative models.

</details>


### [168] [AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models](https://arxiv.org/abs/2505.24784)
*Conor Heins, Toon Van de Maele, Alexander Tschantz, Hampus Linander, Dimitrije Markovic, Tommaso Salvatori, Corrado Pezzato, Ozan Catal, Ran Wei, Magnus Koudahl, Marco Perin, Karl Friston, Tim Verbelen, Christopher Buckley*

**主要类别:** cs.AI

**AI概要:** AXIOM是一种结合对象核心先验知识的新架构，它提高了低数据环境下的学习效率，并兼具贝叶斯方法与深度强化学习的优点。


<details>
  <summary>更多</summary>
  
**动机:** 当前的深度强化学习方法在数据效率上存在不足，而主动推理模型又缺乏领域灵活性，因此提出AXIOM以弥补这一差距。

**方法:** AXIOM将场景表示为对象的组合，并通过分段线性轨迹建模对象动态，利用在线扩展生成模型结构并使用贝叶斯模型缩减进行周期性优化。

**结果:** AXIOM在仅需10,000次交互步骤的情况下掌握多种游戏，且参数数量少、计算成本低。

**结论:** AXIOM通过结合基于对象的核心先验知识，在低数据环境下加速了学习过程，同时具备贝叶斯方法的数据效率和可解释性，以及深度强化学习的跨任务泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AXIOM%3A+Learning+to+Play+Games+in+Minutes+with+Expanding+Object-Centric+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24784&send_immediately=true&force_search=false)

**原文摘要:** Current deep reinforcement learning (DRL) approaches achieve state-of-the-art
performance in various domains, but struggle with data efficiency compared to
human learning, which leverages core priors about objects and their
interactions. Active inference offers a principled framework for integrating
sensory information with prior knowledge to learn a world model and quantify
the uncertainty of its own beliefs and predictions. However, active inference
models are usually crafted for a single task with bespoke knowledge, so they
lack the domain flexibility typical of DRL approaches. To bridge this gap, we
propose a novel architecture that integrates a minimal yet expressive set of
core priors about object-centric dynamics and interactions to accelerate
learning in low-data regimes. The resulting approach, which we call AXIOM,
combines the usual data efficiency and interpretability of Bayesian approaches
with the across-task generalization usually associated with DRL. AXIOM
represents scenes as compositions of objects, whose dynamics are modeled as
piecewise linear trajectories that capture sparse object-object interactions.
The structure of the generative model is expanded online by growing and
learning mixture models from single events and periodically refined through
Bayesian model reduction to induce generalization. AXIOM masters various games
within only 10,000 interaction steps, with both a small number of parameters
compared to DRL, and without the computational expense of gradient-based
optimization.

</details>


### [169] [MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge](https://arxiv.org/abs/2505.24493)
*Xin Jing, Jiadong Wang, Iosif Tsangko, Andreas Triantafyllopoulos, Björn W. Schuller*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种基于GPT-4o的语音情感数据自动标注方法MELT，有效提升了语音情感识别的性能。


<details>
  <summary>更多</summary>
  
**动机:** 人工情感标注成本高且存在不一致性，而大语言模型可能提供一种可扩展的自动标注方案。

**方法:** 利用GPT-4o模型对Friends情景喜剧中的文本线索进行情感标注，构建了一个多模态情感数据集MELT，并通过微调自监督学习模型评估其效果。

**结果:** 实验表明，MELT数据集在语音情感识别任务中带来了性能的显著提升。

**结论:** MELT方法在语音情感识别任务中表现出有效性，通过使用GPT-4o进行数据标注，实现了性能的提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MELT%3A+Towards+Automated+Multimodal+Emotion+Data+Annotation+by+Leveraging+LLM+Embedded+Knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24493，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24493&send_immediately=true&force_search=false)

**原文摘要:** Although speech emotion recognition (SER) has advanced significantly with
deep learning, annotation remains a major hurdle. Human annotation is not only
costly but also subject to inconsistencies annotators often have different
preferences and may lack the necessary contextual knowledge, which can lead to
varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have
emerged as a scalable alternative for annotating text data. However, the
potential of LLMs to perform emotional speech data annotation without human
supervision has yet to be thoroughly investigated. To address these problems,
we apply GPT-4o to annotate a multimodal dataset collected from the sitcom
Friends, using only textual cues as inputs. By crafting structured text
prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated
during its training, showcasing that it can generate accurate and contextually
relevant annotations without direct access to multimodal inputs. Therefore, we
propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We
demonstrate the effectiveness of MELT by fine-tuning four self-supervised
learning (SSL) backbones and assessing speech emotion recognition performance
across emotion datasets. Additionally, our subjective experiments\' results
demonstrate a consistence performance improvement on SER.

</details>


### [170] [Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction](https://arxiv.org/abs/2505.24597)
*Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong*

**主要类别:** cs.AI

**AI概要:** 论文提出NextLocMoE框架，通过双层MoE设计和历史感知路由机制，解决了现有位置预测方法在语义表达和用户行为建模方面的不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法无法捕捉真实世界地点的复杂多功能语义，且难以对不同用户群体的行为动态进行建模，因此需要一种新方法来解决这些问题。

**方法:** 引入了基于大语言模型（LLMs）和双层Mixture-of-Experts（MoE）设计的NextLocMoE框架，包含位置语义MoE模块和个人化MoE模块，并结合历史感知路由机制。

**结果:** 实验证明NextLocMoE在多个真实城市数据集上表现优异，能够更好地适应个体移动模式并提高预测稳定性。

**结论:** NextLocMoE框架在预测准确性、跨领域泛化和可解释性方面表现出色，为解决现有方法的局限性提供了有效方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixture-of-Experts+for+Personalized+and+Semantic-Aware+Next+Location+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24597&send_immediately=true&force_search=false)

**原文摘要:** Next location prediction plays a critical role in understanding human
mobility patterns. However, existing approaches face two core limitations: (1)
they fall short in capturing the complex, multi-functional semantics of
real-world locations; and (2) they lack the capacity to model heterogeneous
behavioral dynamics across diverse user groups. To tackle these challenges, we
introduce NextLocMoE, a novel framework built upon large language models (LLMs)
and structured around a dual-level Mixture-of-Experts (MoE) design. Our
architecture comprises two specialized modules: a Location Semantics MoE that
operates at the embedding level to encode rich functional semantics of
locations, and a Personalized MoE embedded within the Transformer backbone to
dynamically adapt to individual user mobility patterns. In addition, we
incorporate a history-aware routing mechanism that leverages long-term
trajectory data to enhance expert selection and ensure prediction stability.
Empirical evaluations across several real-world urban datasets show that
NextLocMoE achieves superior performance in terms of predictive accuracy,
cross-domain generalization, and interpretability

</details>


### [171] [Taxonomic Networks: A Representation for Neuro-Symbolic Pairing](https://arxiv.org/abs/2505.24601)
*Zekun Wang, Ethan L. Haarer, Nicki Barari, Christopher J. MacLellan*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种神经符号对的方法，在分类网络学习中展现了不同的优势，并探讨了其互换使用和无缝翻译的可能性。


<details>
  <summary>更多</summary>
  
**动机:** 介绍了神经符号对的概念，并展示了其在分类网络学习中的应用潜力。

**方法:** 提出了一种新型的神经符号对，利用分类网络进行评估性能。

**结果:** 符号方法在较少数据和计算资源下能够高效学习分类网络，而神经方法在提供更多资源时能够找到更高精度的分类网络。

**结论:** 神经符号对可以根据情境需求互换使用，并且在必要时可以无缝翻译，为未来更深入集成神经和符号计算的系统奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Taxonomic+Networks%3A+A+Representation+for+Neuro-Symbolic+Pairing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24601&send_immediately=true&force_search=false)

**原文摘要:** We introduce the concept of a \textbf{neuro-symbolic pair} -- neural and
symbolic approaches that are linked through a common knowledge representation.
Next, we present \textbf{taxonomic networks}, a type of discrimination network
in which nodes represent hierarchically organized taxonomic concepts. Using
this representation, we construct a novel neuro-symbolic pair and evaluate its
performance. We show that our symbolic method learns taxonomic nets more
efficiently with less data and compute, while the neural method finds
higher-accuracy taxonomic nets when provided with greater resources. As a
neuro-symbolic pair, these approaches can be used interchangeably based on
situational needs, with seamless translation between them when necessary. This
work lays the foundation for future systems that more fundamentally integrate
neural and symbolic computation.

</details>


### [172] [Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success](https://arxiv.org/abs/2505.24622)
*Ben Griffin, Joseph Ternasky, Fuat Alican, Yigit Ihlamur*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种轻量级集成框架，结合大型语言模型生成的YES/NO问题，用于预测初创企业的成功。


<details>
  <summary>更多</summary>
  
**动机:** 预测初创企业的成功需要既准确又可解释的模型，传统方法可能无法满足这一需求。

**方法:** 通过阈值投票机制筛选、排序和聚合由大型语言模型生成的弱启发式YES/NO问题，构建强集成预测器，并引入专家启发式指导以提升性能。

**结果:** 在10%初创企业被归类为成功的测试集中，该方法实现了50%的精确率，比随机选择提高了5倍；引入专家指导后，精确率进一步提高至54%。

**结论:** 结合大型语言模型推理与人类洞察力，简单的可解释性集成方法能够在风险投资等高风险领域支持关键决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Random+Rule+Forest+%28RRF%29%3A+Interpretable+Ensembles+of+LLM-Generated+Questions+for+Predicting+Startup+Success，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24622&send_immediately=true&force_search=false)

**原文摘要:** Predicting startup success requires models that are both accurate and
interpretable. We present a lightweight ensemble framework that combines YES/NO
questions generated by large language models (LLMs), forming a transparent
decision-making system. Each question acts as a weak heuristic, and by
filtering, ranking, and aggregating them through a threshold-based voting
mechanism, we construct a strong ensemble predictor. On a test set where 10% of
startups are classified as successful, our approach achieves a precision rate
of 50%, representing a 5x improvement over random selection, while remaining
fully transparent. When we incorporate expert-guided heuristics into the
generation process, performance improves further to 54% precision. These
results highlight the value of combining LLM reasoning with human insight and
demonstrate that simple, interpretable ensembles can support high-stakes
decisions in domains such as venture capital (VC).

</details>


### [173] [Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models](https://arxiv.org/abs/2505.24655)
*Frederike Lübeck, Jonas Wildberger, Frederik Träuble, Maximilian Mordig, Sergios Gatidis, Andreas Krause, Bernhard Schölkopf*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的心血管疾病风险预测框架AdaCVD，其利用大型语言模型并克服了现有模型的实际应用难题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的CVD风险预测模型在实际临床应用中面临挑战，因为它们过于简化患者特征，依赖于固定输入模式，并对分布变化敏感。

**方法:** 构建了一个基于大型语言模型的可适应CVD风险预测框架，并使用来自UK Biobank的50多万参与者数据进行微调。

**结果:** AdaCVD在基准测试比较中超越了既定的风险评分和标准机器学习方法，实现了最先进的性能，并在人口统计、社会经济和临床亚组分析中表现稳健。

**结论:** AdaCVD提供了一种灵活的、由人工智能驱动的临床决策支持工具，适用于异构和动态医疗环境的现实需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptable+Cardiovascular+Disease+Risk+Prediction+from+Heterogeneous+Data+using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24655&send_immediately=true&force_search=false)

**原文摘要:** Cardiovascular disease (CVD) risk prediction models are essential for
identifying high-risk individuals and guiding preventive actions. However,
existing models struggle with the challenges of real-world clinical practice as
they oversimplify patient profiles, rely on rigid input schemas, and are
sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk
prediction framework built on large language models extensively fine-tuned on
over half a million participants from the UK Biobank. In benchmark comparisons,
AdaCVD surpasses established risk scores and standard machine learning
approaches, achieving state-of-the-art performance. Crucially, for the first
time, it addresses key clinical challenges across three dimensions: it flexibly
incorporates comprehensive yet variable patient information; it seamlessly
integrates both structured data and unstructured text; and it rapidly adapts to
new patient populations using minimal additional data. In stratified analyses,
it demonstrates robust performance across demographic, socioeconomic, and
clinical subgroups, including underrepresented cohorts. AdaCVD offers a
promising path toward more flexible, AI-driven clinical decision support tools
suited to the realities of heterogeneous and dynamic healthcare environments.

</details>


### [174] [EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/abs/2505.24785)
*Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Matei Zaharia, Ang Chen*

**主要类别:** cs.AI

**AI概要:** EXP-Bench 是一个新的基准测试工具，用于评估AI代理进行端到端AI研究的能力，结果显示当前代理在完成完整实验方面仍面临重大挑战。


<details>
  <summary>更多</summary>
  
**动机:** 目前AI代理在处理复杂的端到端实验方面存在困难，因此需要一种系统的方法来评估和改进它们的性能。

**方法:** 引入 EXP-Bench，并利用半自动管道从顶级AI研究论文及其相关开源代码中提取和构造关键实验细节，以生成复杂且真实的研究任务。

**结果:** 通过 EXP-Bench 的评估发现，当前最先进的LLM代理在个别实验方面得分达到20-35%，但在完成完整实验的成功率仅为0.5%。

**结论:** EXP-Bench 是一个用于评估AI代理进行端到端AI研究实验能力的基准测试，其评估结果显示当前领先的LLM代理在完成完整、可执行的实验方面的成功率很低，仅为0.5%。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EXP-Bench%3A+Can+AI+Conduct+AI+Research+Experiments%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24785，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24785&send_immediately=true&force_search=false)

**原文摘要:** Automating AI research holds immense potential for accelerating scientific
progress, yet current AI agents struggle with the complexities of rigorous,
end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed
to systematically evaluate AI agents on complete research experiments sourced
from influential AI publications. Given a research question and incomplete
starter code, EXP-Bench challenges AI agents to formulate hypotheses, design
and implement experimental procedures, execute them, and analyze results. To
enable the creation of such intricate and authentic tasks with high-fidelity,
we design a semi-autonomous pipeline to extract and structure crucial
experimental details from these research papers and their associated
open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks
from 51 top-tier AI research papers. Evaluations of leading LLM-based agents,
such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial
capabilities: while scores on individual experimental aspects such as design or
implementation correctness occasionally reach 20-35%, the success rate for
complete, executable experiments was a mere 0.5%. By identifying these
bottlenecks and providing realistic step-by-step experiment procedures,
EXP-Bench serves as a vital tool for future AI agents to improve their ability
to conduct AI research experiments. EXP-Bench is open-sourced at
https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.

</details>


### [175] [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
*Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, Han Zhao*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种无需细粒度标注即可捕捉多样化人类偏好的新方法MiCRo，解决了传统Bradley-Terry模型在个性化对齐方面的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的Bradley-Terry模型假设存在一个全局奖励函数，无法准确反映人类偏好的多样性和异质性，从而限制了LLMs在个性化和多元化对齐方面的能力。

**方法:** 提出了一种两阶段的框架MiCRo，第一阶段采用上下文感知的混合建模方法捕捉多样化的人类偏好，第二阶段结合在线路由策略动态调整混合权重以解决模糊性问题。

**结果:** 实验表明，MiCRo在多个偏好数据集上能够有效捕捉多样化的偏好，并在个性化任务中取得了显著提升。

**结论:** MiCRo有效地捕捉了多样化的用户偏好，显著提升了下游个性化任务的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MiCRo%3A+Mixture+Modeling+and+Context-aware+Routing+for+Personalized+Preference+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24846，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24846&send_immediately=true&force_search=false)

**原文摘要:** Reward modeling is a key step in building safe foundation models when
applying reinforcement learning from human feedback (RLHF) to align Large
Language Models (LLMs). However, reward modeling based on the Bradley-Terry
(BT) model assumes a global reward function, failing to capture the inherently
diverse and heterogeneous human preferences. Hence, such oversimplification
limits LLMs from supporting personalization and pluralistic alignment.
Theoretically, we show that when human preferences follow a mixture
distribution of diverse subgroups, a single BT model has an irreducible error.
While existing solutions, such as multi-objective learning with fine-grained
annotations, help address this issue, they are costly and constrained by
predefined attributes, failing to fully capture the richness of human values.
In this work, we introduce MiCRo, a two-stage framework that enhances
personalized preference learning by leveraging large-scale binary preference
datasets without requiring explicit fine-grained annotations. In the first
stage, MiCRo introduces context-aware mixture modeling approach to capture
diverse human preferences. In the second stage, MiCRo integrates an online
routing strategy that dynamically adapts mixture weights based on specific
context to resolve ambiguity, allowing for efficient and scalable preference
adaptation with minimal additional supervision. Experiments on multiple
preference datasets demonstrate that MiCRo effectively captures diverse human
preferences and significantly improves downstream personalization.

</details>


### [176] [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
*Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen*

**主要类别:** cs.AI

**AI概要:** Open CaptchaWorld 是一个用于评估多模态LLM代理处理CAPTCHA能力的新基准平台，结果显示当前最先进的MLLM代理远低于人类表现。


<details>
  <summary>更多</summary>
  
**动机:** 解决现代网络代理在应对CAPTCHA时遇到的瓶颈问题，并测试多模态LLM代理在交互式、多步骤推理任务中的能力。

**方法:** 创建了包含20种现代CAPTCHA类型、总计225个CAPTCHA的基准测试平台，并提出新的度量标准CAPTCHA Reasoning Depth。

**结果:** 实验显示，人类成功率接近完美，而最先进的MLLM代理最多仅达到40.0%的成功率，远低于人类的93.3%。

**结论:** Open CaptchaWorld是一个重要的新基准，有助于诊断当前多模态代理的局限性，并指导更强大多模态推理系统的开发。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Open+CaptchaWorld%3A+A+Comprehensive+Web-based+Platform+for+Testing+and+Benchmarking+Multimodal+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24878&send_immediately=true&force_search=false)

**原文摘要:** CAPTCHAs have been a critical bottleneck for deploying web agents in
real-world applications, often blocking them from completing end-to-end
automation tasks. While modern multimodal LLM agents have demonstrated
impressive performance in static perception tasks, their ability to handle
interactive, multi-step reasoning challenges like CAPTCHAs is largely untested.
To address this gap, we introduce Open CaptchaWorld, the first web-based
benchmark and platform specifically designed to evaluate the visual reasoning
and interaction capabilities of MLLM-powered agents through diverse and dynamic
CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225
CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,
which quantifies the number of cognitive and motor steps required to solve each
puzzle. Experimental results show that humans consistently achieve near-perfect
scores, state-of-the-art MLLM agents struggle significantly, with success rates
at most 40.0% by Browser-Use Openai-o3, far below human-level performance,
93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing
the limits of current multimodal agents and guiding the development of more
robust multimodal reasoning systems. Code and Data are available at this https
URL.

</details>


### [177] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/abs/2505.05880)
*Bettina Fazzinga, Sergio Flesca, Filippo Furfaro, Luigi Pontieri, Francesco Scala*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一个结合序列标注和基于抽象论证框架（AAF）推理的新符号方法，旨在减少人工注释需求和计算成本，提高对流程跟踪事件解释的准确性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 论文动机在于解决现代公司和组织中流程跟踪监控和分析的关键任务，尤其是在事件到活动映射高度不确定或未指定的情况下，传统的基于推理的方法可能产生低信息量结果和高计算成本的问题。同时，训练序列标注模型需要大量手动注释的示例轨迹，因此需要开发一种绿色AI解决方案以减少劳动力/计算成本和碳足迹。

**方法:** 论文提出了一种数据/计算高效的新符号方法，该方法利用基于示例的序列标注器生成候选事件解释，并通过AAF-based推理器进行优化。

**结果:** 实验结果显示，该方法能够有效利用先验知识弥补示例数据的稀缺性，提供高度可能的候选事件解释，并在数据注释和模型优化成本受限的情况下表现出色。

**结论:** 论文得出的结论是，通过将基于示例的序列标注器与AAF-based推理器结合，可以有效地解决事件解释候选的稀缺数据问题，并且在数据注释和模型优化成本受限的情况下特别有用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Combining+Abstract+Argumentation+and+Machine+Learning+for+Efficiently+Analyzing+Low-Level+Process+Event+Streams，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.05880，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.05880&send_immediately=true&force_search=false)

**原文摘要:** Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [178] [Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning](https://arxiv.org/abs/2505.23783)
*Korel Gundem, Juncheng Dong, Dennis Zhang, Vahid Tarokh, Zhengling Qi*

**主要类别:** stat.ML

**AI概要:** 本文提出了 Supervised Calibration (SC) 框架，用于改进大型语言模型（LLMs）的校准效果。与现有方法不同，SC 可以调整甚至反转模型的决策边界，从而显著提升分类任务的稳定性与准确性。


<details>
  <summary>更多</summary>
  
**动机:** In-Context Learning (ICL) 在分类任务中表现出系统性偏差，现有校准技术无法有效调整LLM的决策边界方向，需要更灵活的方法来解决这一问题。

**方法:** 提出了一种基于损失最小化的监督校准（SC）框架，通过学习每个类别的仿射变换来优化LLM的预测概率，并引入了两种正则化技术：context-invariance 和 directional trust-region。

**结果:** SC 能够在不需要额外数据的情况下调整和反转LLM的决策边界，且在多个模型和数据集上展示了最先进的性能。

**结论:** Supervised Calibration (SC) 作为一种新的校准框架，不仅统一了许多现有的ICL校准方法，而且能够有效地调整LLM的决策边界，提供最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Boosting+In-Context+Learning+in+LLMs+Through+the+Lens+of+Classical+Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23783，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23783&send_immediately=true&force_search=false)

**原文摘要:** In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new
tasks with just a few examples, but their predictions often suffer from
systematic biases, leading to unstable performances in classification. While
calibration techniques are proposed to mitigate these biases, we show that, in
the logit space, many of these methods are equivalent to merely shifting the
LLM's decision boundary without having the ability to alter its orientation.
This proves inadequate when biases cause the LLM to be severely misdirected. To
address these limitations and provide a unifying framework, we propose
Supervised Calibration (SC), a loss-minimization based framework which learns
an optimal, per-class affine transformation of the LLM's predictive
probabilities in the logit space without requiring external data beyond the
context. By using a more expressive functional class, SC not only subsumes many
existing calibration methods in ICL as special cases, but also enables the
ability to alter and even completely reverse the orientation of the LLM's
decision boundary. Furthermore, SC's loss-based nature facilitates the seamless
integration of two purpose-built regularization techniques: context-invariance
and directional trust-region. The former is designed to tackle the instability
issue in ICL, while the latter controls the degree of calibration. Finally, SC
delivers state-of-the-art performance over calibration baselines in the 4-shot,
8-shot, and 16-shot settings across all nine datasets for
Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.

</details>


### [179] [Gibbs randomness-compression proposition: An efficient deep learning](https://arxiv.org/abs/2505.23869)
*M. Süzen*

**主要类别:** stat.ML

**AI概要:** 这篇论文提出了一种新的深度学习训练与压缩框架——双断层压缩（DTC），它揭示了随机性与压缩之间的联系，并通过实验验证了其在高效训练和压缩方面的优势。


<details>
  <summary>更多</summary>
  
**动机:** 作者受到新提出的DTC框架行为的启发，试图探索压缩过程中的随机性与信息保留之间的关系，并寻找一种更高效深度学习训练方法。

**方法:** 该论文提出了一种名为“双断层压缩”（DTC）的压缩训练框架，利用测量向量集上的吉布斯熵来连接随机性和压缩，并采用断层重建技术对权重矩阵进行压缩感知投影。

**结果:** 实验结果表明，这种双断层成像方法在训练过程中实现了高效的压缩，加速并支持了彩票假设，同时随机压缩训练迭代也从统计物理学角度揭示了随机性与压缩之间的联系。

**结论:** 论文的结论是随机性和压缩之间存在联系，通过吉布斯熵提出了所谓的“吉布斯随机-压缩命题”，并且DTC框架为大规模节能和资源高效的深度学习训练方法提供了有前景的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gibbs+randomness-compression+proposition%3A+An+efficient+deep+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23869，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23869&send_immediately=true&force_search=false)

**原文摘要:** A proposition that connects randomness and compression put forward via Gibbs
entropy over set of measurement vectors associated with a compression process.
The proposition states that a lossy compression process is equivalent to {\it
directed randomness} that preserves information content. The proposition
originated from the observed behaviour in newly proposed {\it Dual Tomographic
Compression} (DTC) compress-train framework. This is akin to tomographic
reconstruction of layer weight matrices via building compressed sensed
projections, so called {\it weight rays}. This tomographic approach is applied
to previous and next layers in a dual fashion, that triggers neuronal-level
pruning. This novel model compress-train scheme appear in iterative fashion and
act as smart neural architecture search, Experiments demonstrated utility of
this dual-tomography producing state-of-the-art performance with efficient
compression during training, accelerating and supporting lottery ticket
hypothesis. However, random compress-train iterations having similar
performance demonstrated the connection between randomness and compression from
statistical physics perspective, we formulated so called {\it Gibbs
randomness-compression proposition}, signifying randomness-compression
relationship via Gibbs entropy. Practically, DTC framework provides a promising
approach for massively energy and resource efficient deep learning training
approach.

</details>


### [180] [Conformal Object Detection by Sequential Risk Control](https://arxiv.org/abs/2505.24038)
*Léo Andéol, Luca Mossina, Adrien Mazoyer, Sébastien Gerchinovitz*

**主要类别:** stat.ML

**AI概要:** 论文介绍了一种新的目标检测可靠性提升方法SeqCRC，并提供了相关实验验证与工具包。


<details>
  <summary>更多</summary>
  
**动机:** 目标检测模型在关键应用中的部署受到神经网络固有不可靠性和复杂结构的阻碍，因此需要一种可靠的方法来提高其可靠性。

**方法:** 引入了顺序共形风险控制（SeqCRC），扩展了共形风险控制（CRC）的统计保证到两个顺序任务中，并提出适用于不同应用和认证要求的损失函数和预测集。

**结果:** 广泛的实验验证了所提出方法的有效性，并强调了权衡和实际影响。

**结论:** 本文提出了SeqCRC方法，扩展了在COD设置下的统计保证，并通过工具包促进了方法的复制和进一步探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conformal+Object+Detection+by+Sequential+Risk+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24038&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in object detectors have led to their adoption for industrial
uses. However, their deployment in critical applications is hindered by the
inherent lack of reliability of neural networks and the complex structure of
object detection models. To address these challenges, we turn to Conformal
Prediction, a post-hoc procedure which offers statistical guarantees that are
valid for any dataset size, without requiring prior knowledge on the model or
data distribution. Our contribution is manifold: first, we formally define the
problem of Conformal Object Detection (COD) and introduce a novel method,
Sequential Conformal Risk Control (SeqCRC), that extends the statistical
guarantees of Conformal Risk Control (CRC) to two sequential tasks with two
parameters, as required in the COD setting. Then, we propose loss functions and
prediction sets suited to applying CRC to different applications and
certification requirements. Finally, we present a conformal toolkit, enabling
replication and further exploration of our methods. Using this toolkit, we
perform extensive experiments, yielding a benchmark that validates the
investigated methods and emphasizes trade-offs and other practical
consequences.

</details>


### [181] [Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity](https://arxiv.org/abs/2505.24097)
*Victor Li, Baiting Chen, Yuzhen Mao, Qi Lei, Zhun Deng*

**主要类别:** stat.ML

**AI概要:** 这篇论文提出了一种在预测具有表现性时用于风险控制的新框架，称为“Performative Risk Control”，并通过实验证明了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 由于预测支持的决策可能影响其试图预测的结果（即预测的表现性），因此需要一种新的方法来确保在这种情况下仍能实现可靠的风险控制。这是本篇论文的研究动机。

**方法:** 论文中介绍了一种迭代优化的校准过程，并研究了不同类型的风险度量和尾部边界的选择，以在存在预测表现性的情况下实现风险控制。

**结果:** 作者展示了他们在信用违约风险预测任务上的数值实验结果，证明了所提出框架的有效性。

**结论:** 该论文得出的结论是，他们提出了一个名为“Performative Risk Control”的框架，在预测结果会影响其目标变量的情况下，通过可证明的理论保证来校准模型以实现风险控制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performative+Risk+Control%3A+Calibrating+Models+for+Reliable+Deployment+under+Performativity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24097，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24097&send_immediately=true&force_search=false)

**原文摘要:** Calibrating blackbox machine learning models to achieve risk control is
crucial to ensure reliable decision-making. A rich line of literature has been
studying how to calibrate a model so that its predictions satisfy explicit
finite-sample statistical guarantees under a fixed, static, and unknown
data-generating distribution. However, prediction-supported decisions may
influence the outcome they aim to predict, a phenomenon named performativity of
predictions, which is commonly seen in social science and economics. In this
paper, we introduce Performative Risk Control, a framework to calibrate models
to achieve risk control under performativity with provable theoretical
guarantees. Specifically, we provide an iteratively refined calibration
process, where we ensure the predictions are improved and risk-controlled
throughout the process. We also study different types of risk measures and
choices of tail bounds. Lastly, we demonstrate the effectiveness of our
framework by numerical experiments on the task of predicting credit default
risk. To the best of our knowledge, this work is the first one to study
statistically rigorous risk control under performativity, which will serve as
an important safeguard against a wide range of strategic manipulation in
decision-making processes.

</details>


### [182] [A Mathematical Perspective On Contrastive Learning](https://arxiv.org/abs/2505.24134)
*Ricardo Baptista, Andrew M. Stuart, Son Tran*

**主要类别:** stat.ML

**AI概要:** 本文介绍了一个多模态对比学习的概率框架，包括新算法及其在不同领域的应用。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决多模态数据链接问题，提供对对比学习的新理解和自然推广。

**方法:** 通过低秩矩阵逼近方法来分析新的对比学习算法，并进行了数值实验验证。

**结果:** 提出新的概率损失函数和对齐度量方法，能够近似自然统计特性并应用于实际数据集和海洋学应用。

**结论:** 作者提出了一个基于概率视角的多模态对比学习框架，并研究了其在特定模式寻求和生成任务中的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Mathematical+Perspective+On+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24134&send_immediately=true&force_search=false)

**原文摘要:** Multimodal contrastive learning is a methodology for linking different data
modalities; the canonical example is linking image and text data. The
methodology is typically framed as the identification of a set of encoders, one
for each modality, that align representations within a common latent space. In
this work, we focus on the bimodal setting and interpret contrastive learning
as the optimization of (parameterized) encoders that define conditional
probability distributions, for each modality conditioned on the other,
consistent with the available data. This provides a framework for multimodal
algorithms such as crossmodal retrieval, which identifies the mode of one of
these conditional distributions, and crossmodal classification, which is
similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This
probabilistic perspective suggests two natural generalizations of contrastive
learning: the introduction of novel probabilistic loss functions, and the use
of alternative metrics for measuring alignment in the common latent space. We
study these generalizations of the classical approach in the multivariate
Gaussian setting. In this context we view the latent space identification as a
low-rank matrix approximation problem. This allows us to characterize the
capabilities of loss functions and alignment metrics to approximate natural
statistics, such as conditional means and covariances; doing so yields novel
variants on contrastive learning algorithms for specific mode-seeking and for
generative tasks. The framework we introduce is also studied through numerical
experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data
assimilation application arising in oceanography.

</details>


### [183] [Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings](https://arxiv.org/abs/2505.24281)
*Yang Sui, Qi Xu, Yang Bai, Annie Qu*

**主要类别:** stat.ML

**AI概要:** 本文提出了一个双编码器框架来应对多任务学习中的异质性问题，并通过理论分析与实验验证了方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 多任务学习中存在分布异质性和后验异质性等挑战，现有方法难以在统一框架内解决这些问题。

**方法:** 研究引入了一个统一的算法，交替学习任务特定和任务共享编码器及其系数，并探索了学习潜在因子对应系数的内在相似性结构。

**结果:** 理论分析显示所提方法具有超额风险界，实验表明其在多种设置下优于现有数据集成方法，并在PDX数据中实现了更优的肿瘤倍增时间预测性能。

**结论:** 该论文提出了一种双编码器框架，用于构建每个任务的异构潜在因子空间，并通过任务共享和任务特定编码器分别捕获跨任务的共同信息和独特特征。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-task+Learning+for+Heterogeneous+Data+via+Integrating+Shared+and+Task-Specific+Encodings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24281&send_immediately=true&force_search=false)

**原文摘要:** Multi-task learning (MTL) has become an essential machine learning tool for
addressing multiple learning tasks simultaneously and has been effectively
applied across fields such as healthcare, marketing, and biomedical research.
However, to enable efficient information sharing across tasks, it is crucial to
leverage both shared and heterogeneous information. Despite extensive research
on MTL, various forms of heterogeneity, including distribution and posterior
heterogeneity, present significant challenges. Existing methods often fail to
address these forms of heterogeneity within a unified framework. In this paper,
we propose a dual-encoder framework to construct a heterogeneous latent factor
space for each task, incorporating a task-shared encoder to capture common
information across tasks and a task-specific encoder to preserve unique task
characteristics. Additionally, we explore the intrinsic similarity structure of
the coefficients corresponding to learned latent factors, allowing for adaptive
integration across tasks to manage posterior heterogeneity. We introduce a
unified algorithm that alternately learns the task-specific and task-shared
encoders and coefficients. In theory, we investigate the excess risk bound for
the proposed MTL method using local Rademacher complexity and apply it to a new
but related task. Through simulation studies, we demonstrate that the proposed
method outperforms existing data integration methods across various settings.
Furthermore, the proposed method achieves superior predictive performance for
time to tumor doubling across five distinct cancer types in PDX data.

</details>


### [184] [Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels](https://arxiv.org/abs/2505.24311)
*Yi Gu*

**主要类别:** stat.ML

**AI概要:** 本文研究了t-SNE算法在广义核条件下的收敛性，并证明其在数据点足够多时会达到平衡分布。


<details>
  <summary>更多</summary>
  
**动机:** 受到Auffinger和Fletcher于2023年研究成果的启发，我们希望扩展其结果并更深入地理解t-SNE算法的收敛性质。

**方法:** 通过给出广义输入和输出核的具体公式，并在特定条件下进行数学推导，证明了t-SNE算法的收敛性。

**结果:** 我们成功证明了在广义输入和输出核条件下，随着数据点数量增加，t-SNE算法会收敛到一个平衡分布。

**结论:** 我们的研究给出了广义输入和输出核条件下t-SNE算法收敛到平衡分布的证明。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Equilibrium+Distribution+for+t-Distributed+Stochastic+Neighbor+Embedding+with+Generalized+Kernels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24311，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24311&send_immediately=true&force_search=false)

**原文摘要:** T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm
for visualizing high-dimensional data by finding low-dimensional
representations. In this paper, we study the convergence of t-SNE with
generalized kernels and extend the results of Auffinger and Fletcher in 2023.
Our work starts by giving a concrete formulation of generalized input and
output kernels. Then we prove that under certain conditions, the t-SNE
algorithm converges to an equilibrium distribution for a wide range of input
and output kernels as the number of data points diverges.

</details>


### [185] [Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation](https://arxiv.org/abs/2505.24333)
*Alessio Giorlandino, Sebastian Goldt*

**主要类别:** stat.ML

**AI概要:** 本研究提出了一个用于分析变压器中信号传播的理论框架，揭示了初始化对防止训练崩溃的重要性，并指导实际模型设计。


<details>
  <summary>更多</summary>
  
**动机:** 找到适合变压器网络的初始化方法以避免训练过程中的秩崩溃和熵崩溃问题，确保训练平稳及性能良好。

**方法:** 通过借鉴统计物理学中的随机能量模型处理自注意力层，并分析不同方差初始化对信号传播的影响。

**结果:** 识别出自注意力层信号传播的两个方差主导机制：低方差机制导致秩崩溃，高方差机制导致熵崩溃；并验证了BERT风格模型在TinyStories数据集上的预测。

**结论:** 研究提出了一种分析变压器块信号传播的理论框架，统一了对自注意力两种失败模式的理解，并提供了初始化超参数选择的实用指南。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two+failure+modes+of+deep+transformers+and+how+to+avoid+them%3A+a+unified+theory+of+signal+propagation+at+initialisation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24333&send_immediately=true&force_search=false)

**原文摘要:** Finding the right initialisation for neural networks is crucial to ensure
smooth training and good performance. In transformers, the wrong initialisation
can lead to one of two failure modes of self-attention layers: rank collapse,
where all tokens collapse into similar representations, and entropy collapse,
where highly concentrated attention scores lead to training instability. While
the right initialisation has been extensively studied in feed-forward networks,
an exact description of signal propagation through a full transformer block has
so far been lacking. Here, we provide an analytical theory of signal
propagation through vanilla transformer blocks with self-attention layers,
layer normalisation, skip connections and ReLU MLP. To treat the self-attention
layer, we draw on a formal parallel with the Random Energy Model from
statistical physics. We identify and characterise two regimes governed by the
variance of the query and key initialisations: a low-variance regime, where we
recover the known rank collapse behaviour; and a previously unexplored
high-variance regime, where signal is preserved but \textit{entropy collapse}
occurs. In the low-variance regime, we calculate the critical strength for the
residual connection to ensure signal propagation. Our theory yields
trainability diagrams that identify the correct choice of initialisation
hyper-parameters for a given architecture. Experiments with BERT-style models
trained on TinyStories validate our predictions. Our theoretical framework
gives a unified perspective on the two failure modes of self-attention and
gives quantitative predictions on the scale of both weights and residual
connections that guarantees smooth training.

</details>


### [186] [Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data](https://arxiv.org/abs/2505.24556)
*Gabriel V Cardoso, Mike Pereira*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种基于扩散生成模型（DGMs）的两步方法，用于近似具有非平稳高斯过程（GP）先验的预测后验分布（PPD），并通过环境科学中的逆问题验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 非平稳先验在捕捉复杂空间模式时常常是必要的，但其导致的计算不可行性限制了贝叶斯高斯过程模型的应用。

**方法:** 用DGM替代GP先验，并利用最新的无训练指导算法从期望的后验分布中采样。

**结果:** 该方法成功应用于一个复杂的非平稳GP先验问题，并通过多个统计指标验证了其与GP对应方法的接近程度；此外，还展示了如何微调DGM以适应特定部分的GP先验。

**结论:** 所提出的DGM方法不仅能够有效模拟非平稳GP先验下的PPD，还在解决环境科学中的逆问题上表现出色，实现了最先进的预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predictive+posterior+sampling+from+non-stationnary+Gaussian+process+priors+via+Diffusion+models+with+application+to+climate+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24556&send_immediately=true&force_search=false)

**原文摘要:** Bayesian models based on Gaussian processes (GPs) offer a flexible framework
to predict spatially distributed variables with uncertainty. But the use of
nonstationary priors, often necessary for capturing complex spatial patterns,
makes sampling from the predictive posterior distribution (PPD) computationally
intractable. In this paper, we propose a two-step approach based on diffusion
generative models (DGMs) to mimic PPDs associated with non-stationary GP
priors: we replace the GP prior by a DGM surrogate, and leverage recent
advances on training-free guidance algorithms for DGMs to sample from the
desired posterior distribution. We apply our approach to a rich non-stationary
GP prior from which exact posterior sampling is untractable and validate that
the issuing distributions are close to their GP counterpart using several
statistical metrics. We also demonstrate how one can fine-tune the trained DGMs
to target specific parts of the GP prior. Finally we apply the proposed
approach to solve inverse problems arising in environmental sciences, thus
yielding state-of-the-art predictions.

</details>


### [187] [Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders](https://arxiv.org/abs/2505.24668)
*Jonghyun Ham, Maximilian Fleissner, Debarghya Ghoshdastidar*

**主要类别:** stat.ML

**AI概要:** 这篇论文主要研究了在梯度流下训练的两层线性去噪自动编码器，探讨了瓶颈层和跳跃连接对模型性能的影响，揭示了瓶颈层引入的一种类似于偏差-方差权衡的复杂度度量以及跳跃连接减轻模型方差的作用。


<details>
  <summary>更多</summary>
  
**动机:** 现代深度神经网络在高度过参数化的情况下表现出强大的泛化能力。虽然在监督学习背景下理解这一现象已取得重大进展，但对于去噪等无监督任务，仍存在许多开放问题。最近的一些工作已经成功地刻画了线性去噪问题的测试误差，但它们仅限于线性模型（单层网络）。因此，本文试图深入研究这一领域。

**方法:** 作者专注于在梯度流下训练的两层线性去噪自动编码器，结合了现代深度学习架构的两个关键组成部分：低维瓶颈层和可能绕过瓶颈的跳跃连接。他们推导了该模型在乘积正则化下的所有临界点的闭式表达式，并描述了其在最小范数原则下的全局最小解。

**结果:** 作者推导出了两层线性去噪自动编码器在过参数化状态下的测试风险公式，包括具有和不具有跳跃连接的模型。通过随机矩阵理论分析跳跃连接对去噪自动编码器的影响，并用数值证据支持他们的论断。

**结论:** 论文得出结论，瓶颈层引入了一种类似于经典偏差-方差权衡的额外复杂度度量，并且跳跃连接可以减轻去噪自编码器中的方差，特别是在模型轻度过参数化的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Impact+of+Bottleneck+Layers+and+Skip+Connections+on+the+Generalization+of+Linear+Denoising+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24668&send_immediately=true&force_search=false)

**原文摘要:** Modern deep neural networks exhibit strong generalization even in highly
overparameterized regimes. Significant progress has been made to understand
this phenomenon in the context of supervised learning, but for unsupervised
tasks such as denoising, several open questions remain. While some recent works
have successfully characterized the test error of the linear denoising problem,
they are limited to linear models (one-layer network). In this work, we focus
on two-layer linear denoising autoencoders trained under gradient flow,
incorporating two key ingredients of modern deep learning architectures: A
low-dimensional bottleneck layer that effectively enforces a rank constraint on
the learned solution, as well as the possibility of a skip connection that
bypasses the bottleneck. We derive closed-form expressions for all critical
points of this model under product regularization, and in particular describe
its global minimizer under the minimum-norm principle. From there, we derive
the test risk formula in the overparameterized regime, both for models with and
without skip connections. Our analysis reveals two interesting phenomena:
Firstly, the bottleneck layer introduces an additional complexity measure akin
to the classical bias-variance trade-off -- increasing the bottleneck width
reduces bias but introduces variance, and vice versa. Secondly, skip connection
can mitigate the variance in denoising autoencoders -- especially when the
model is mildly overparameterized. We further analyze the impact of skip
connections in denoising autoencoder using random matrix theory and support our
claims with numerical evidence.

</details>


### [188] [K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes](https://arxiv.org/abs/2505.24704)
*Hideaki Kim, Tomoharu Iwata, Akinori Fujino*

**主要类别:** stat.ML

**AI概要:** 本文提出了K²IE，它结合了核方法和经典KIE的优点，在保持良好性能的同时提高了计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 核方法和经典KIE虽然都使用“核”但理论基础不同，本文旨在结合两者的优势。

**方法:** 提出了一种基于最小二乘损失的正则化核方法（K²IE），并证明其估计器与经典KIE相同。

**结果:** 实验表明，K²IE在合成数据集上的预测性能与现有方法相当，但计算效率显著提高。

**结论:** K²IE在计算效率上优于现有的核方法估计器，同时保持了良好的预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是K%24%5E2%24IE%3A+Kernel+Method-based+Kernel+Intensity+Estimators+for+Inhomogeneous+Poisson+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24704&send_immediately=true&force_search=false)

**原文摘要:** Kernel method-based intensity estimators, formulated within reproducing
kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs)
have been among the most easy-to-implement and feasible methods for estimating
the intensity functions of inhomogeneous Poisson processes. While both
approaches share the term "kernel", they are founded on distinct theoretical
principles, each with its own strengths and limitations. In this paper, we
propose a novel regularized kernel method for Poisson processes based on the
least squares loss and show that the resulting intensity estimator involves a
specialized variant of the representer theorem: it has the dual coefficient of
unity and coincides with classical KIEs. This result provides new theoretical
insights into the connection between classical KIEs and kernel method-based
intensity estimators, while enabling us to develop an efficient KIE by
leveraging advanced techniques from RKHS theory. We refer to the proposed model
as the kernel method-based kernel intensity estimator (K$^2$IE). Through
experiments on synthetic datasets, we show that K$^2$IE achieves comparable
predictive performance while significantly surpassing the state-of-the-art
kernel method-based estimator in computational efficiency.

</details>


### [189] [Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery](https://arxiv.org/abs/2505.24727)
*Xiaochen Zhang, Haoyi Xiong*

**主要类别:** stat.ML

**AI概要:** This paper presents a new Knockoff-guided compressive sensing framework that enhances signal recovery by leveraging precise false discovery rate control, offering both theoretical guarantees and strong empirical performance.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this work is to enhance signal recovery reliability by leveraging precise false discovery rate control during the support identification phase, something traditional methods like LASSO lack.

**方法:** The method involves a novel Knockoff-guided compressive sensing framework that guarantees finite sample false discovery rate control. It separates and controls the support recovery process through statistical Knockoff filters for more accurate signal reconstruction.

**结果:** The results show that the proposed method consistently outperforms LASSO-based and other state-of-the-art compressive sensing techniques, with up to a 3.9x improvement in F1-score over baseline methods in simulation studies. It also yields lower reconstruction and relative errors and achieves top downstream predictive performance on real-world datasets.

**结论:** The paper concludes that the proposed Knockoff-guided compressive sensing framework, which leverages precise false discovery rate control during the support identification phase, provides a robust and practical alternative to existing approaches in signal recovery.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knockoff-Guided+Compressive+Sensing%3A+A+Statistical+Machine+Learning+Framework+for+Support-Assured+Signal+Recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24727，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24727&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a novel Knockoff-guided compressive sensing framework,
referred to as \TheName{}, which enhances signal recovery by leveraging precise
false discovery rate (FDR) control during the support identification phase.
Unlike LASSO, which jointly performs support selection and signal estimation
without explicit error control, our method guarantees FDR control in finite
samples, enabling more reliable identification of the true signal support. By
separating and controlling the support recovery process through statistical
Knockoff filters, our framework achieves more accurate signal reconstruction,
especially in challenging scenarios where traditional methods fail. We
establish theoretical guarantees demonstrating how FDR control directly ensures
recovery performance under weaker conditions than traditional $\ell_1$-based
compressive sensing methods, while maintaining accurate signal reconstruction.
Extensive numerical experiments demonstrate that our proposed Knockoff-based
method consistently outperforms LASSO-based and other state-of-the-art
compressive sensing techniques. In simulation studies, our method improves
F1-score by up to 3.9x over baseline methods, attributed to principled false
discovery rate (FDR) control and enhanced support recovery. The method also
consistently yields lower reconstruction and relative errors. We further
validate the framework on real-world datasets, where it achieves top downstream
predictive performance across both regression and classification tasks, often
narrowing or even surpassing the performance gap relative to uncompressed
signals. These results establish \TheName{} as a robust and practical
alternative to existing approaches, offering both theoretical guarantees and
strong empirical performance through statistically grounded support selection.

</details>


### [190] [Generalization Dynamics of Linear Diffusion Models](https://arxiv.org/abs/2505.24769)
*Claudia Merger, Sebastian Goldt*

**主要类别:** stat.ML

**AI概要:** 这项研究分析了扩散模型在有限训练数据条件下从记忆到泛化的过渡，并揭示了样本复杂度对简单模型泛化能力的影响。


<details>
  <summary>更多</summary>
  
**动机:** 该研究的动机在于理解扩散模型在有限训练数据集上从记忆到泛化的过渡，这是衡量生成模型样本效率和可靠性的重要方面，但目前对此过渡的理论理解尚不完整。

**方法:** 本文采用了一种简单的线性去噪器模型进行分析研究，通过显式计算测试误差、采样分布以及样本与目标分布之间的Kullback-Leibler散度来预测记忆到泛化转变的发生点。

**结果:** 研究结果表明，当训练样本数N大约等于输入维度d时，会发生记忆到泛化的转变。对于小样本情况(N < d)，只有部分相关变异方向存在于训练数据中，此时正则化和早期停止可以有效避免过拟合。而当N > d时，线性扩散模型的采样分布以d/N的速度趋近于其最优状态。

**结论:** 论文的结论是，在扩散模型中，当训练样本数量N大约等于输入维度d时，记忆到泛化的转变发生。对于N小于d的情况，正则化和提前停止有助于防止过拟合；而对于N大于d的情况，线性扩散模型的采样分布以d/N的速度接近最优状态，与数据分布的具体细节无关。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+Dynamics+of+Linear+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24769，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24769&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models trained on finite datasets with $N$ samples from a target
distribution exhibit a transition from memorisation, where the model reproduces
training examples, to generalisation, where it produces novel samples that
reflect the underlying data distribution. Understanding this transition is key
to characterising the sample efficiency and reliability of generative models,
but our theoretical understanding of this transition is incomplete. Here, we
analytically study the memorisation-to-generalisation transition in a simple
model using linear denoisers, which allow explicit computation of test errors,
sampling distributions, and Kullback-Leibler divergences between samples and
target distribution. Using these measures, we predict that this transition
occurs roughly when $N \asymp d$, the dimension of the inputs. When $N$ is
smaller than the dimension of the inputs $d$, so that only a fraction of
relevant directions of variation are present in the training data, we
demonstrate how both regularization and early stopping help to prevent
overfitting. For $N > d$, we find that the sampling distributions of linear
diffusion models approach their optimum (measured by the Kullback-Leibler
divergence) linearly with $d/N$, independent of the specifics of the data
distribution. Our work clarifies how sample complexity governs generalisation
in a simple model of diffusion-based generative models and provides insight
into the training dynamics of linear denoisers.

</details>


### [191] [Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV](https://arxiv.org/abs/2505.24781)
*Karim Abou-Moustafa*

**主要类别:** stat.ML

**AI概要:** 这篇论文旨在解决Regularized Tyler's M-estimator中的正则化参数估计问题，通过设计一种高效的近似方法来优化收缩系数α，从而显著提升计算效率和估计精度。


<details>
  <summary>更多</summary>
  
**动机:** 传统的LOOCV计算成本高，尤其在样本量较大时难以实现。

**方法:** 提出了一种基于留一交叉验证（LOOCV）对数似度损失的近似方法，以避免多次调用RTME过程。

**结果:** 所提出的近似方法将LOOCV的时间复杂度降低O(n)，显著提高了计算效率，并且在多个任务中表现出更高的准确性。

**结论:** 该论文提出了一种高效的正则化参数估计方法，并通过实验验证了其在合成数据和真实数据上的效率和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Estimation+of+Regularized+Tyler%27s+M-Estimator+Using+Approximate+LOOCV，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24781&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of estimating a regularization parameter, or a
shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator
(RTME). In particular, we propose to estimate an optimal shrinkage coefficient
by setting $\alpha$ as the solution to a suitably chosen objective function;
namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since
LOOCV is computationally prohibitive even for moderate sample size $n$, we
propose a computationally efficient approximation for the LOOCV log-likelihood
loss that eliminates the need for invoking the RTME procedure $n$ times for
each sample left out during the LOOCV procedure. This approximation yields an
$O(n)$ reduction in the running time complexity for the LOOCV procedure, which
results in a significant speedup for computing the LOOCV estimate. We
demonstrate the efficiency and accuracy of the proposed approach on synthetic
high-dimensional data sampled from heavy-tailed elliptical distributions, as
well as on real high-dimensional datasets for object recognition, face
recognition, and handwritten digit's recognition. Our experiments show that the
proposed approach is efficient and consistently more accurate than other
methods in the literature for shrinkage coefficient estimation.

</details>


### [192] [Statistical mechanics of extensive-width Bayesian neural networks near interpolation](https://arxiv.org/abs/2505.24849)
*Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk*

**主要类别:** stat.ML

**AI概要:** 本研究通过统计物理方法分析了两层神经网络的监督学习过程，发现了数据量影响特征学习和模型专业化的过渡现象。


<details>
  <summary>更多</summary>
  
**动机:** 为了缩小实际应用中神经网络与理论理解之间的差距，需要对更现实的模型进行分析。

**方法:** 论文采用统计物理的方法对两层全连接网络进行分析，特别关注贝叶斯最优学习情况下的师生场景。

**结果:** 研究发现，在数据稀缺的情况下，模型仅学习教师权重的非线性组合；而在数据充足时，才会发生专业化，即学生权重与教师权重对齐。

**结论:** 本文通过统计物理学分析，研究了两层全连接网络在监督学习中的表现，并揭示了随着数据量增加而出现的各种学习过渡现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Statistical+mechanics+of+extensive-width+Bayesian+neural+networks+near+interpolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24849&send_immediately=true&force_search=false)

**原文摘要:** For three decades statistical mechanics has been providing a framework to
analyse neural networks. However, the theoretically tractable models, e.g.,
perceptrons, random features models and kernel machines, or multi-index models
and committee machines with few neurons, remained simple compared to those used
in applications. In this paper we help reducing the gap between practical
networks and their theoretical understanding through a statistical physics
analysis of the supervised learning of a two-layer fully connected network with
generic weight distribution and activation function, whose hidden layer is
large but remains proportional to the inputs dimension. This makes it more
realistic than infinitely wide networks where no feature learning occurs, but
also more expressive than narrow ones or with fixed inner weights. We focus on
the Bayes-optimal learning in the teacher-student scenario, i.e., with a
dataset generated by another network with the same architecture. We operate
around interpolation, where the number of trainable parameters and of data are
comparable and feature learning emerges. Our analysis uncovers a rich
phenomenology with various learning transitions as the number of data
increases. In particular, the more strongly the features (i.e., hidden neurons
of the target) contribute to the observed responses, the less data is needed to
learn them. Moreover, when the data is scarce, the model only learns non-linear
combinations of the teacher weights, rather than "specialising" by aligning its
weights with the teacher's. Specialisation occurs only when enough data becomes
available, but it can be hard to find for practical training algorithms,
possibly due to statistical-to-computational~gaps.

</details>
