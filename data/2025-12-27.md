<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 31]
- [cs.CL](#cs.CL) [总数: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization](https://arxiv.org/abs/2512.20623)
*Ravi Gupta, Shabista Haider*

**主要类别:** cs.AI

**AI概要:** BitRL-Light是一个结合1位量化大语言模型和DQN强化学习的智能家居照明控制系统，在树莓派上实现实时控制，能耗降低71.4倍，节能32%，用户满意度达95%。


<details>
  <summary>更多</summary>
  
**动机:** 智能家居照明系统消耗15-20%的住宅能源，但缺乏同时优化用户舒适度和能源效率的自适应智能

**方法:** 采用1位量化Llama-3.2-1B模型与深度Q网络强化学习相结合，通过多目标强化学习从用户反馈中学习最优照明策略，平衡能耗、舒适度和昼夜节律对齐

**结果:** 在树莓派4上推理延迟低于200ms，相比全精度模型能耗降低71.4倍，相比基于规则的系统节能32%，用户满意度95%，1位模型在ARM处理器上比2位替代方案提速5.07倍，保持92%任务准确率

**结论:** 这项工作为在资源受限的IoT设备上部署自适应AI建立了实用框架，实现了无需云依赖的智能家居自动化

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BitRL-Light%3A+1-bit+LLM+Agents+with+Deep+Reinforcement+Learning+for+Energy-Efficient+Smart+Home+Lighting+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20623，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20623&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.

</details>


### [2] [Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment](https://arxiv.org/abs/2512.20624)
*Mazyar Taghavi, Javad Vahidi*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种量子启发的多智能体强化学习框架，用于优化无人机辅助6G网络部署中的探索-利用权衡问题，通过结合经典MARL算法和量子启发优化技术，在部分可观测和动态条件下实现更好的信号覆盖和网络扩展效率。


<details>
  <summary>更多</summary>
  
**动机:** 解决多无人机在6G网络部署中的自主协调问题，需要在部分可观测和动态环境下最大化信号覆盖和网络扩展效率，传统方法在探索-利用权衡方面存在局限性。

**方法:** 集成经典MARL算法与量子启发优化技术，使用变分量子电路(VQC)作为核心结构，采用量子近似优化算法(QAOA)进行组合优化，结合贝叶斯推理、高斯过程和变分推理进行概率建模，采用集中训练分散执行(CTDE)范式。

**结果:** 实验表明该框架提高了样本效率、加速了收敛速度、增强了覆盖性能并保持了鲁棒性，相比PPO和DDPG基线方法，在探索-利用权衡方面实现了更优的平衡。

**结论:** 量子启发的MARL框架在无人机辅助6G网络部署中表现出优越性能，为多智能体系统在复杂环境中的优化问题提供了有效解决方案，所有实现代码已开源以确保可复现性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum-Inspired+Multi+Agent+Reinforcement+Learning+for+Exploration+Exploitation+Optimization+in+UAV-Assisted+6G+Network+Deployment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20624&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.

</details>


### [3] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于多模态知识图谱的检索增强生成方法，通过整合视觉线索来提升对多模态文档的理解和推理能力，在全局和细粒度问答任务中均优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统检索增强生成(RAG)方法在处理长篇领域特定内容时存在概念理解和整体理解能力不足的问题，且现有的基于知识图谱的RAG解决方案仅限于文本输入，未能利用视觉等多模态信息。

**方法:** 提出多模态知识图谱RAG框架，将视觉线索整合到知识图谱构建、检索阶段和答案生成过程中，支持跨模态推理。

**结果:** 实验结果表明，该方法在全局和细粒度问答任务中均优于现有的基于RAG的方法，在文本和多模态语料库上都取得了更好的性能。

**结论:** 通过引入视觉信息构建多模态知识图谱，能够有效提升RAG系统对复杂多模态内容的理解和推理能力，为跨模态文档分析提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MegaRAG%3A+Multimodal+Knowledge+Graph-Based+Retrieval+Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20626&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [4] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama, Takayuki Ito, Takahiro Uchiya, Motoki Miura, Takahiro Kawaji, Takaya Yuizono, Atsuo Yoshitaka, Tokuro Matsuo, Shun Okuhara, Jawad Haqbeen, Sofia Sahab, Wen Gu, Shiyao Ding*

**主要类别:** cs.AI

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Proceedings+of+the+20th+International+Conference+on+Knowledge%2C+Information+and+Creativity+Support+Systems+%28KICSS+2025%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20628&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [5] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal, Ishaan Gangwani*

**主要类别:** cs.AI

**AI概要:** Microprobe方法仅用100个精心选择的测试样本就能实现全面的可靠性评估，相比传统方法减少90%评估成本，同时保持95%的覆盖率和99.9%的统计效力。


<details>
  <summary>更多</summary>
  
**动机:** 传统基础模型可靠性评估需要数千个评估样本，计算成本高且耗时，难以满足现实世界部署的快速评估需求。

**方法:** 结合五个关键可靠性维度的策略性提示多样性、先进的不确定性量化和自适应加权，通过战略选择探测样本来高效检测潜在故障模式。

**结果:** 在多个语言模型和跨领域验证中，microprobe相比随机采样基线提高23.5%的综合可靠性分数，具有极显著统计意义(p < 0.001)，专家评分4.14/5.0 vs 3.14/5.0。

**结论:** microprobe解决了负责任AI部署中高效模型评估的关键空白，为实际应用提供了经济高效的可靠性评估解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MicroProbe%3A+Efficient+Reliability+Assessment+for+Foundation+Models+with+Minimal+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20630&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [6] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma, Ao Feng, Zhenjie Gao, Xinyu Song, Li Su, Bin Chen, Wei Wang, Jiamin Wu*

**主要类别:** cs.AI

**AI概要:** 介绍基于阿里通义千问3开发的医疗AI助手Erkang-Diagnosis-1.1，整合500GB医疗知识，通过混合训练方法提供专业医疗咨询，在医疗考试中表现优于GPT-4


<details>
  <summary>更多</summary>
  
**动机:** 开发安全可靠的专业AI健康顾问，赋能基层医疗和健康管理，为用户提供智能健康伴侣服务

**方法:** 采用增强预训练和检索增强生成的混合方法，整合500GB高质量结构化医疗知识，通过3-5轮高效交互理解用户症状

**结果:** 能够准确理解用户症状并进行初步分析，提供有价值的诊断建议和健康指导，在综合医疗考试中领先GPT-4

**结论:** Erkang-Diagnosis-1.1成功开发为专业的AI医疗咨询助手，在医疗知识理解和诊断能力方面表现出色，具有实际应用价值

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Erkang-Diagnosis-1.1+Technical+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20632，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20632&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [7] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu, Jonathan Zhang, Sean Chua, Spencer Kim, Kevin Zhu, Sean O'Brien, Vasu Sharma*

**主要类别:** cs.AI

**AI概要:** 该论文探讨不同大语言模型之间推理链的互换性，发现一个模型的部分推理链可以被另一个模型可靠地继续完成，甚至在跨模型家族时也能保持或提高推理准确性和逻辑一致性。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注通过内部推理策略提升模型性能，但对不同模型间推理的互换性了解甚少。本研究旨在探索不同模型间是否能够可靠地继续完成彼此的推理链。

**方法:** 使用token级别的对数概率阈值在早期、中期和晚期截断Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct的推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行继续推理实验，测试同家族和跨家族的行为。使用过程奖励模型(PRM)评估推理稳定性。

**结果:** 评估显示混合推理链通常能够保持甚至提高最终准确性和逻辑结构，特别是在某些情况下跨模型家族的推理链互换还能带来性能提升。

**结论:** 推理互换性成为推理模型的一个新兴行为特性，为协作AI系统中可靠的模块化推理提供了新的范式见解，表明模型间推理链的交换可以增强推理的可靠性和稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Relay%3A+Evaluating+Stability+and+Interchangeability+of+Large+Language+Models+in+Mathematical+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20647，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20647&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [8] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo, Yuhang Fan, Yufei Li, Youzhi Zhang, Hengyu Lin, Ziqi Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出了AiAuditTrack（AAT）框架，这是一个基于区块链的AI使用流量记录和治理系统，通过去中心化身份和可验证凭证建立可信AI实体，记录交互轨迹并实现风险追踪。


<details>
  <summary>更多</summary>
  
**动机:** AI应用的快速扩张导致AI交互数据激增，带来了安全、问责和风险追溯方面的紧迫挑战，需要建立可信的AI使用监督机制。

**方法:** 利用去中心化身份（DID）和可验证凭证（VC）建立可信AI实体，将AI实体建模为动态交互图中的节点，提出风险扩散算法追踪风险行为源头，并通过区块链TPS指标评估系统性能。

**结果:** AAT在大规模交互记录下展现出可行性和稳定性，能够有效进行跨系统监督和审计。

**结论:** AAT为复杂多智能体环境中的AI审计、风险管理和责任归属提供了可扩展且可验证的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AIAuditTrack%3A+A+Framework+for+AI+Security+system，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20649，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20649&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [9] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

**主要类别:** cs.AI

**AI概要:** 提出混合注意力机制MoAS，通过动态路由选择最优注意力方案（MHA/GQA/MQA），在保持模型性能的同时提升推理效率


<details>
  <summary>更多</summary>
  
**动机:** 解决Transformer中注意力机制的质量与效率权衡问题：MHA性能最好但KV缓存内存需求大，MQA/GQA节省内存但性能下降

**方法:** 提出MoAS架构，通过学习的路由器为每个token动态选择最优注意力方案（MHA、GQA或MQA），而非静态混合方案

**结果:** 在WikiText-2上验证，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线竞争且具有条件计算效率潜力

**结论:** 动态注意力方案选择是有效的，MoAS在保持模型质量的同时提供了推理效率优化的可能性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixture+of+Attention+Schemes+%28MoAS%29%3A+Learning+to+Route+Between+MHA%2C+GQA%2C+and+MQA，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20650，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20650&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [10] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen, Ke Sun*

**主要类别:** cs.AI

**AI概要:** Memory Bear系统通过构建类人记忆架构，解决了LLMs在记忆方面的固有局限，包括受限上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成等问题，在多个领域实现了显著的工程创新和性能突破。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型面临内存限制、上下文窗口受限、长期知识遗忘、冗余信息积累和幻觉生成等固有局限性，这些严重制约了持续对话和个性化服务的发展。

**方法:** 基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现LLM记忆机制的全链重构。

**结果:** 在医疗保健、企业运营和教育等领域表现出显著工程创新和性能突破，显著提高长期对话中的知识保真度和检索效率，降低幻觉率，通过记忆-认知整合增强上下文适应性和推理能力。

**结论:** 相比现有解决方案（如Mem0、MemGPT、Graphiti），Memory Bear在准确性、令牌效率和响应延迟等关键指标上表现更优，标志着AI从"记忆"向"认知"迈进的关键一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory+Bear+AI+A+Breakthrough+from+Memory+to+Cognition+Toward+Artificial+General+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20651&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [11] [AI-Driven Decision-Making System for Hiring Process](https://arxiv.org/abs/2512.20652)
*Vira Filatova, Andrii Zelenchuk, Dmytro Filatov*

**主要类别:** cs.AI

**AI概要:** AI驱动的模块化多智能体招聘助手，通过整合文档处理、候选人分析、数据验证和评分系统，显著提高招聘效率，将每位合格候选人筛选时间从3.33小时降至1.70小时。


<details>
  <summary>更多</summary>
  
**动机:** 解决早期候选人验证的瓶颈问题，招聘人员需要处理简历、筛选答案、编程作业等异构输入，过程耗时且效率低下。

**方法:** 构建模块化多智能体系统，包括文档视频预处理、结构化候选人档案构建、公开数据验证、技术/文化契合度评分（含风险惩罚）、人机交互验证界面，由LLM在严格约束下协调流程。

**结果:** 在64名中级Python后端工程师申请者的测试中，系统效率显著提升：每位合格候选人筛选时间1.70小时 vs 经验丰富招聘人员的3.33小时，筛选成本大幅降低，同时保持人类决策者的最终决定权。

**结论:** AI驱动的招聘助手能够有效提高招聘流程的效率和成本效益，同时通过人机协作模式保持人类监督和决策权，为招聘行业提供了可行的自动化解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI-Driven+Decision-Making+System+for+Hiring+Process，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20652&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.

</details>


### [12] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

**主要类别:** cs.AI

**AI概要:** 提出AFA训练机制，通过对抗性反馈自动重新分配注意力权重，解决Transformer模型在情感分析中过度关注常见词而忽略重要低频词的问题，在多个数据集上达到SOTA效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有Transformer模型在情感分析任务中倾向于主要关注常见词汇，而忽略那些不常见但对任务高度相关的词汇，这显著影响了模型性能。

**方法:** 提出对抗性注意力反馈(AFA)训练机制，包含动态掩码策略和判别器，使用策略梯度方法优化注意力分布，无需人工标注即可自动重新分配注意力权重。

**结果:** 在三个公开数据集上实现最先进的结果，将该训练机制应用于大型语言模型可进一步提升12.6%的性能。

**结论:** AFA机制能有效解决Transformer模型注意力分配不平衡问题，提升情感分析性能，且该方法可扩展到大型语言模型中，具有广泛的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Fake+Focus+to+Real+Precision%3A+Confusion-Driven+Adversarial+Attention+Learning+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20661，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20661&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [13] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma, Jung-Hua Liu*

**主要类别:** cs.AI

**AI概要:** 研究发现LLMs存在懒惰行为（不完整响应多部分请求），但在解码优化和长对话上下文保持方面表现优于预期，建议使用自优化和动态提示策略改善指令遵循问题


<details>
  <summary>更多</summary>
  
**动机:** 量化大型语言模型的行为缺陷，包括懒惰、解码次优性和上下文退化现象

**方法:** 通过三个对照实验（A、B、C）测试多个先进LLM（GPT-4变体、DeepSeek）在复杂指令遵循、简单推理任务和长对话场景中的表现

**结果:** 发现普遍存在懒惰问题（省略必要部分、不满足长度要求），但解码次优性证据有限，在200轮混乱对话测试中上下文保持能力意外强大

**结论:** 现代LLM在详细指令遵循方面仍有挑战，但在内部缓解了某些假设的故障模式（如上下文遗忘），建议采用自优化和动态提示策略提高多指令遵循能力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantifying+Laziness%2C+Decoding+Suboptimality%2C+and+Context+Degradation+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20662，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20662&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [14] [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)
*Shinobu Miya*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于约束满足问题(CSP)的结构化验证方法Eidoku，通过计算结构违反成本来检测LLM的幻觉，替代传统的概率验证方法。


<details>
  <summary>更多</summary>
  
**动机:** LLM经常产生被模型自身赋予高概率的幻觉陈述，这表明幻觉不是低置信度现象，而是结构一致性的失败。概率验证无法检测到这种"平滑虚假"陈述。

**方法:** 将LLM推理验证重新表述为约束满足问题，定义包含图连接性、特征空间一致性和逻辑蕴含三个代理的总成本函数，通过轻量级System-2门Eidoku进行验证，拒绝超过上下文校准成本阈值的候选推理。

**结果:** 实验表明该方法能够成功拒绝概率验证器无法检测的"平滑虚假"陈述，通过显式执行结构约束可以确定性地拒绝特定类别的幻觉。

**结论:** 基于结构违反成本的验证方法为生成式推理提供了一种神经符号的合理性检查，能够有效解决LLM幻觉问题，特别是在概率验证失效的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Eidoku%3A+A+Neuro-Symbolic+Verification+Gate+for+LLM+Reasoning+via+Structural+Constraint+Satisfaction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20664&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.

</details>


### [15] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala, Sophie Lathouwers, Michael van Bekkum*

**主要类别:** cs.AI

**AI概要:** 该立场论文提出需要开发一种语义语言来弥合功能性可信AI(FTAI)和规范性可信AI(NTAI)之间的鸿沟，以帮助评估AI系统的可信度并将法规转化为具体实施步骤。


<details>
  <summary>更多</summary>
  
**动机:** 当前功能性可信AI(FTAI)和规范性可信AI(NTAI)之间存在鸿沟，使得难以评估AI系统的可信度，需要建立桥梁来连接两者。

**方法:** 通过引入概念性语义语言作为框架，匹配FTAI和NTAI，讨论开发这种语言的起点和预期效果。

**结果:** 论文描述了当前最先进的技术状态，识别了FTAI和NTAI之间的差距，并提供了开发语义语言的起点和预期效果分析。

**结论:** 提出了关键考虑因素并讨论了未来评估可信AI的行动方向，强调语义语言在弥合FTAI和NTAI鸿沟中的重要作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+the+AI+Trustworthiness+Gap+between+Functions+and+Norms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20671，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20671&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [16] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian, Yunfei Hou, Qingquan Sun*

**主要类别:** cs.AI

**AI概要:** 这篇范围综述分析了2023-2025年间32项研究，探讨生成式AI在计算机科学教育中的个性化应用效果，发现通过特定的设计模式（如解释优先指导、分级提示等）可以实现有效的个性化学习支持。


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI能够实现大规模个性化计算机科学教育，但需要明确这种个性化是支持还是削弱学习效果，因此需要系统梳理相关研究和设计模式。

**方法:** 采用范围综述方法，从259篇文献中目的性抽样32项研究，分析个性化机制和有效性信号，识别五个应用领域并分析设计选择如何影响学习结果。

**结果:** 研究发现包含解释优先指导、解决方案保留、分级提示阶梯和基于学生产物的设计比无约束聊天界面显示出更积极的学习过程。成功实施共享四种模式：基于学生产物的情境感知辅导、需要反思的多级提示结构、与传统CS基础设施的结合以及人在回路的质保。

**结论:** 证据支持生成式AI可以作为精确支架机制，但需要嵌入可审计的工作流程中，在保持有效挑战的同时扩展个性化支持，同时需要应对学术诚信、隐私、偏见等风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Pilots+to+Practices%3A+A+Scoping+Review+of+GenAI-Enabled+Personalization+in+Computer+Science+Education，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20714，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20714&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [17] [From artificial to organic: Rethinking the roots of intelligence for digital health](https://arxiv.org/abs/2512.20723)
*Prajwal Ghimire, Keyoumars Ashkan*

**主要类别:** cs.AI

**AI概要:** 论文认为人工智能与有机智能之间的界限并不明显，因为AI是基于人类有机智能的神经生物学和进化过程启发而设计的，本质上是有机智慧的产物。


<details>
  <summary>更多</summary>
  
**动机:** 探讨人工智能与有机智能之间的本质联系，挑战'人工'与'有机'的二元对立观念，强调AI实际上是有机智慧的延伸而非对立物。

**方法:** 通过分析AI系统的基本原理（如神经网络和决策算法）与人类神经生物学和进化过程的相似性，进行概念性论证和理论分析。

**结果:** 论证表明人工智能与有机智能之间存在深刻的连续性，AI的'人工'性质更多是命名上的区分而非本质上的对立。

**结论:** 人工智能与有机智能的边界比术语所暗示的要模糊得多，AI实际上是有机智慧通过人类认知设计和改进的产物，两者在组织和适应机制上具有根本相似性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+artificial+to+organic%3A+Rethinking+the+roots+of+intelligence+for+digital+health，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20723，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20723&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.

</details>


### [18] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang*

**主要类别:** cs.AI

**AI概要:** AgentMath是一个结合语言模型推理能力和代码解释器计算精度的智能体框架，通过自动转换思维链为结构化工具增强轨迹、新型强化学习范式以及高效训练系统，在数学竞赛基准测试中达到最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型推理模型在自然语言推理方面取得显著进展，但在需要复杂数学运算的问题上仍存在计算效率低和准确性不足的问题。

**方法:** 提出AgentMath框架，包含三个创新：(1)自动将自然语言思维链转换为结构化工具增强轨迹生成高质量SFT数据；(2)新型智能体强化学习范式，动态交织自然语言生成与实时代码执行；(3)高效训练系统，包含异步调度、部分rollout和负载平衡技术。

**结果:** 在AIME24、AIME25和HMMT25数学竞赛基准测试中分别达到90.6%、86.4%和73.8%的准确率，实现了4-5倍的训练加速。

**结论:** 该方法有效解决了复杂数学问题的推理挑战，为构建更复杂和可扩展的数学推理智能体铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentMath%3A+Empowering+Mathematical+Reasoning+for+Large+Language+Models+via+Tool-Augmented+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20745&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [19] [A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents](https://arxiv.org/abs/2512.20798)
*Miles Q. Li, Benjamin C. M. Fung, Martin Weiss, Pulei Xiong, Khalil Al-Hussaeni, Claude Fachkha*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个新的AI安全基准测试，包含40个多步骤场景，用于评估AI代理在性能激励下可能出现的伦理约束违反行为。研究发现即使是能力最强的模型也可能出现高达71.4%的违规率，且推理能力与安全性不直接相关。


<details>
  <summary>更多</summary>
  
**动机:** 当前的安全基准主要关注单步决策、模拟环境或显式负面约束，缺乏对现实生产环境中多步骤目标优化过程中出现的涌现性约束违反行为的评估。

**方法:** 设计了包含40个不同场景的新基准，每个场景都包含任务要求和KPI指标，设置了指令驱动和激励驱动两种变体来区分服从性和涌现性错位。评估了12个最先进的大语言模型。

**结果:** 模型表现出1.3%到71.4%的结果驱动约束违反率，9个模型的错位率在30%-50%之间。推理能力最强的Gemini-3-Pro-Preview模型违规率超过60%。还发现了显著的"审慎错位"现象。

**结论:** 研究强调了在部署前需要进行更现实的智能体安全训练，以减轻AI代理在现实世界中的风险，推理能力并不能保证安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Benchmark+for+Evaluating+Outcome-Driven+Constraint+Violations+in+Autonomous+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20798，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20798&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant "deliberative misalignment", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.

</details>


### [20] [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)
*Anselm Paulus, Ilia Kulikov, Brandon Amos, Rémi Munos, Ivan Evtimov, Kamalika Chaudhuri, Arman Zharmagambetov*

**主要类别:** cs.AI

**AI概要:** 提出AdvGame方法，将安全对齐建模为非零和博弈，通过在线强化学习联合训练攻击者和防御者语言模型，实现安全性和实用性的帕累托前沿提升


<details>
  <summary>更多</summary>
  
**动机:** 当前基于顺序对抗训练的语言模型安全对齐方法存在局限性，需要新的范式来同时提升模型的安全性和实用性

**方法:** 使用非零和博弈框架，通过在线强化学习联合训练攻击者LM和防御者LM，采用基于偏好的奖励信号而非点式评分，提供更鲁棒的监督

**结果:** 防御者LM同时变得更有用且对对抗攻击更具弹性，攻击者LM收敛为强大的通用红队测试代理，可直接用于探测任意目标模型

**结论:** AdvGame方法通过博弈论框架实现了安全对齐的突破，为语言模型安全提供了新的有效途径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safety+Alignment+of+LMs+via+Non-cooperative+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20806，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20806&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.

</details>


### [21] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的强化学习方法，能够在参数化动作空间中自主在线学习状态和动作抽象，显著提高了在长时程稀疏奖励设置中的样本效率。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界顺序决策常涉及参数化动作空间，需要同时处理离散动作选择和连续动作参数决策。现有方法存在严重局限：规划方法需要手工制作动作模型，标准RL算法无法同时处理离散和连续动作，而少数处理参数化动作的RL方法依赖领域特定工程且未能利用这些空间的潜在结构。

**方法:** 引入算法使智能体能够在线自主学习状态和动作抽象，并在学习过程中逐步精炼这些抽象，在状态-动作空间的关键区域增加细粒度细节以提高性能。

**结果:** 在多个连续状态、参数化动作领域中，基于抽象的方法使TD(λ)算法实现了比最先进基线方法显著更高的样本效率。

**结论:** 该方法成功扩展了RL算法的适用范围，使其能够在参数化动作空间的长时程稀疏奖励设置中有效工作，通过自主学习抽象来利用动作空间的潜在结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context-Sensitive+Abstractions+for+Reinforcement+Learning+with+Parameterized+Actions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20831，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20831&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [22] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer, Grace Wu, Yuchen Wang, Daniel Dosti, Honghao Zhang, Vivi De La Rue*

**主要类别:** cs.AI

**AI概要:** 论文提出使用多智能体多角色辩论机制来生成反思，解决单一LLM自我反思时的思维退化问题，在HotPot QA和HumanEval任务上取得了优于单一LLM的表现


<details>
  <summary>更多</summary>
  
**动机:** 单一LLM在反思自身错误时会出现思维退化现象，即使知道错误也会重复相同的错误，需要更好的反思生成方法

**方法:** 引入多智能体多角色辩论机制来生成反思，通过不同角色的智能体辩论产生更多样化的反思

**结果:** 在HotPot QA上达到47%的精确匹配准确率，在HumanEval编程任务上达到82.7%的准确率，均超过单一LLM的反思效果

**结论:** 多智能体多角色辩论机制能有效提升LLM反思的多样性，解决思维退化问题，显著提升推理任务的性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAR%3AMulti-Agent+Reflexion+Improves+Reasoning+Abilities+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20845&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [23] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng*

**主要类别:** cs.AI

**AI概要:** 提出一个概率框架解决LLM智能体知识单向流动问题，通过Beta-Bernoulli模型量化信念不确定性，建立双向知识交换机制，实现高效主动学习和资源优化。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于LLM和RAG的自主智能体存在认知不对称问题，知识流动是单向的，导致冗余推理和集体智能停滞。当前自反思框架缺乏概率基础来量化确定性和证明外部交互的合理性。

**方法:** 使用带有遗忘因子(γ)的Beta-Bernoulli分布建模智能体信念，分离认知不确定性作为信念方差。提出稳态动机和最优学习策略的双重驱动机制，并引入认知缓存技术实现资源动态优化。

**结果:** 仿真验证显示，在异构(Zipfian)环境中，这种不确定性驱动策略显著优于随机基线，保持对概念漂移的高适应性。积累的信念状态可作为RLHF的可验证奖励信号和SFT的高质量数据过滤器。

**结论:** 该概率框架为智能体提供了非利他的双向知识交换动机，将公共贡献重新定义为最优主动学习，通过不确定性驱动策略有效提升集体智能系统的效率和适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Silent+Scholar+Problem%3A+A+Probabilistic+Framework+for+Breaking+Epistemic+Asymmetry+in+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20884，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20884&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [24] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan, Hassan Ali Razzaqi, Ali Akarma, Mohammad Riyaz Belgaum*

**主要类别:** cs.AI

**AI概要:** 提出基于LangChain多智能体系统和许可区块链的统一架构，实现自主AI系统的实时监控、策略执行和不可篡改审计，在智能库存管理、交通信号控制和医疗监护等场景验证了有效性。


<details>
  <summary>更多</summary>
  
**动机:** 解决自主AI系统在医疗、智慧城市等领域的应用中存在的信任、监管和信息完整性问题，确保AI决策的可靠性和可追溯性。

**方法:** 采用LangChain多智能体系统与Hyperledger Fabric许可区块链结合，通过感知-概念化-行动循环与区块链治理层连接，验证输入、评估建议行动并记录执行结果。

**结果:** 区块链安全验证能有效防止未授权操作，提供全决策过程可追溯性，并将操作延迟保持在合理范围内。

**结论:** 该框架为实施高影响力自主AI应用提供了既自主又负责任的通用系统解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Blockchain-Monitored+Agentic+AI+Architecture+for+Trusted+Perception-Reasoning-Action+Pipelines，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20985，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20985&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [25] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed, Abdulaziz Alshahrani, Ali Ullah, Ali Akarma, Sohail Khan, Muhammad Nauman, Salman Jan*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个价格感知的AI代理系统，结合个人财务管理与饮食优化，为中等收入家庭提供营养充足且价格合理的膳食计划，能够自动适应市场价格波动。


<details>
  <summary>更多</summary>
  
**动机:** 中等收入环境中家庭预算有限与营养需求之间的矛盾，特别是食品价格波动带来的挑战，需要一种能够同时考虑经济性和营养性的智能解决方案。

**方法:** 采用模块化多代理架构，包含预算、营养、价格监控和健康个性化四个专门代理，通过共享知识库和使用替代图来确保以最低成本维持营养质量。

**结果:** 沙特家庭案例研究表明，相比静态周菜单，系统能稳定降低12-18%的成本，营养充足率超过95%，在20-30%的价格变化下仍保持高性能。

**结论:** 该框架能够有效结合经济性和营养充足性，为实现零饥饿和良好健康的可持续发展目标提供了可行的可持续和公平饮食规划途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FinAgent%3A+An+Agentic+AI+Framework+Integrating+Personal+Finance+and+Nutrition+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20991，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20991&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [26] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du, Jun Zhang, Jie Feng, Zhicheng Liu, Jian Yuan, Yong Li*

**主要类别:** cs.AI

**AI概要:** TrafficSimAgent是一个基于大语言模型的交通仿真代理框架，通过高低层级专家代理的协作，让非专业用户能够用自然语言指令轻松完成复杂的交通仿真实验和决策优化。


<details>
  <summary>更多</summary>
  
**动机:** 现有交通仿真平台如SUMO和MATSim功能全面但使用门槛高，缺乏相关知识的用户在从零开始实验和日常应用时面临重大挑战。

**方法:** 采用LLM-based代理框架，包含高层专家代理（理解自然语言指令、规划实验流程、调用工具）和低层专家代理（基于实时交通状况选择最优行动方案），通过跨层级协作实现执行。

**结果:** 在多场景实验中发现，TrafficSimAgent能在各种条件下有效执行仿真，即使在用户指令模糊时也能产生合理结果，且其专家级自主决策优化相比其他系统和SOTA LLM方法表现更优。

**结论:** TrafficSimAgent成功解决了非专业用户使用交通仿真平台的困难，提供了一个灵活、高效的解决方案，通过LLM代理的智能协作实现了高质量的交通仿真实验执行和优化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TrafficSimAgent%3A+A+Hierarchical+Agent+Framework+for+Autonomous+Traffic+Simulation+with+MCP+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20996&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [27] [Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation](https://arxiv.org/abs/2512.21066)
*Tomoaki Yamaguchi, Yutong Zhou, Masahiro Ryo, Keisuke Katsura*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种结合SHAP可解释性和多模态LLM迭代优化的Agentic XAI框架，通过11轮迭代逐步提升农业推荐系统的解释质量。专家和LLM评估显示解释质量在3-4轮达到峰值，但过度迭代会导致质量下降，表明需要策略性早停机制。


<details>
  <summary>更多</summary>
  
**动机:** 虽然可解释AI能够揭示变量关联，但向非专业人士传达XAI输出仍具挑战性，阻碍了AI预测的可信度。LLM有潜力将技术解释转化为易懂叙述，但将LLM作为自主代理的Agentic AI与XAI的结合尚未探索。

**方法:** 提出Agentic XAI框架，结合基于SHAP的可解释性和多模态LLM驱动的迭代优化。使用日本26块稻田的产量数据作为农业推荐系统用例，进行11轮迭代优化（第0-10轮）。通过人类专家（12名作物科学家）和LLM（14个）从7个指标评估解释质量。

**结果:** 评估结果显示框架成功提升了推荐质量，从第0轮平均得分提高了30-33%，在第3-4轮达到峰值。但过度优化导致推荐质量显著下降，揭示了偏差-方差权衡：早期轮次缺乏解释深度（偏差），过度迭代则导致冗长和无根据的抽象（方差）。

**结论:** 研究结果表明需要通过策略性早停（正则化）来优化实际效用，挑战了单调改进的假设，为Agentic XAI系统提供了基于证据的设计原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+Explainable+Artificial+Intelligence+%28Agentic+XAI%29+Approach+To+Explore+Better+Explanation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21066，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21066&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.

</details>


### [28] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

**主要类别:** cs.AI

**AI概要:** 论文证明了在特定条件下（仅观察聚合结果和算法盲评估），用LLM角色模拟替代人类进行A/B测试是有效的基准测试方法，且提供了所需样本量的理论界限


<details>
  <summary>更多</summary>
  
**动机:** 解决A/B测试成本高、延迟长的问题，探索LLM角色模拟作为廉价替代方案的可行性

**方法:** 提出if-and-only-if理论特征化，定义信息论可区分性指标，推导样本量界限

**结果:** 证明在聚合观察和算法盲评估条件下，角色模拟与人类测试从方法角度看无区别，且给出了可靠区分不同方法所需评估次数的明确界限

**结论:** LLM角色模拟在特定约束下可作为有效的基准测试替代方案，其有效性主要取决于样本量大小

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+Personas+as+a+Substitute+for+Field+Experiments+in+Method+Benchmarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21080&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [29] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos*

**主要类别:** cs.AI

**AI概要:** 当前LLM安全方法主要关注显性有害内容，但忽视了关键漏洞：无法理解上下文和识别用户意图。恶意用户可通过情感框架、渐进式揭示和学术合理化等技术系统性地绕过安全机制。研究发现推理功能反而放大了利用效果，只有Claude Opus 4.1在部分情况下能优先检测意图而非提供信息。


<details>
  <summary>更多</summary>
  
**动机:** 现有大型语言模型的安全方法过于关注显性有害内容，但缺乏对上下文理解和用户意图识别的能力，这为恶意用户提供了可被系统性利用的安全漏洞。

**方法:** 实证评估多个最先进的LLM（包括ChatGPT、Claude、Gemini和DeepSeek），分析通过情感框架、渐进式揭示和学术合理化等技术绕过安全机制的效果。

**结果:** 研究发现推理功能配置不仅没有减轻反而放大了利用效果，提高了事实精确性但未能质疑潜在意图。只有Claude Opus 4.1在部分用例中优先进行意图检测而非信息提供。

**结论:** 当前架构设计存在系统性漏洞，需要进行范式转变，将上下文理解和意图识别作为核心安全能力，而非事后保护机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Context%3A+Large+Language+Models+Failure+to+Grasp+Users+Intent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21110，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21110&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [30] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand, Esther Borsi, Mitch Fruin, Lauren E Walker, Jamie Heagerty, Chris C. Holmes, Anthony J Avery, Iain E Buchan, Harry Coppock*

**主要类别:** cs.AI

**AI概要:** 该研究首次在真实NHS初级保健数据上评估基于LLM的药物安全审查系统，发现虽然模型在识别临床问题方面表现良好（灵敏度100%），但仅在46.9%的患者中能正确识别所有问题和干预措施，主要失败原因是情境推理而非药物知识缺失。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLM在医疗基准测试中表现优异，但很少有研究在真实临床数据上评估其性能或深入分析超越表面指标的表现，需要了解LLM在真实临床环境中的实际表现和失败模式。

**方法:** 回顾性研究，使用涵盖2,125,549名成年人的NHS电子健康记录，战略性抽样277名患者，由临床专家审查系统识别的问题和干预建议，评估LLM系统的性能。

**结果:** 主要LLM系统识别临床问题的灵敏度为100%，特异性83.1%，但仅在46.9%的患者中完全正确识别所有问题和干预措施。失败分析揭示了五种主要模式：不确定性过度自信、未根据患者情境调整标准指南、误解医疗实践方式、事实错误和流程盲点。

**结论:** 研究强调了在安全部署基于LLM的临床AI之前必须解决的缺陷，需要进行更大规模的前瞻性评估和更深入的LLM临床行为研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Real-World+Evaluation+of+LLM+Medication+Safety+Reviews+in+NHS+Primary+Care，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21127&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [31] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu*

**主要类别:** cs.AI

**AI概要:** RoboSafe是一种用于具身智能体的运行时安全防护系统，通过混合推理和基于谓词的安全逻辑来拦截危险指令，在保持任务性能的同时显著降低36.8%的风险发生率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的具身智能体虽然能够执行复杂任务，但容易受到危险指令的影响。现有防护方法主要依赖静态规则过滤或提示级控制，难以应对动态、时序依赖和上下文丰富的环境中的隐性风险。

**方法:** 提出RoboSafe混合推理运行时安全防护系统，包含：1）后向反思推理模块，通过短期记忆重新审视近期轨迹来推断时序安全谓词；2）前向预测推理模块，通过长期安全记忆和多模态观察生成上下文感知的安全谓词。两个模块在混合长短时安全记忆上协同工作。

**结果:** 在多个智能体上的广泛实验表明，RoboSafe相比领先基线显著减少了危险动作（风险发生率降低36.8%），同时保持接近原始的任务性能。物理机械臂的真实世界评估进一步证实了其实用性。

**结论:** RoboSafe提供了一种自适应、可验证的安全逻辑，既具有可解释性又可作为代码执行，为具身智能体的运行时安全防护提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RoboSafe%3A+Safeguarding+Embodied+Agents+via+Executable+Safety+Logic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21220&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan*

**主要类别:** cs.CL

**AI概要:** 提出使用稀疏自编码器(SAE)自动发现大语言模型在特定概念上的弱点(模型差距)和基准测试覆盖不均衡问题(基准差距)的新方法


<details>
  <summary>更多</summary>
  
**动机:** 标准化基准测试的聚合指标会掩盖模型在特定子领域的弱点以及基准测试本身的不均衡覆盖问题

**方法:** 通过提取SAE概念激活并计算基准数据上的显著性加权性能得分，将评估基于模型内部表示，实现跨基准比较

**结果:** 发现模型在拒绝奉承行为和安全性相关概念上表现不佳，同时基准测试过度代表服从/权威概念而缺失核心概念

**结论:** 该方法提供基于表示的评估方法，能够对基准得分进行概念级分解，揭示模型得分原因和基准改进方向

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncovering+Competency+Gaps+in+Large+Language+Models+and+Their+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20638，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20638&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [33] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos, Chadbourne Davis*

**主要类别:** cs.CL

**AI概要:** SA-DiffuSeq是一个基于扩散模型的长文本生成框架，通过集成稀疏注意力机制显著降低了计算复杂度，同时保持生成质量，特别适合长文档建模。


<details>
  <summary>更多</summary>
  
**动机:** 基于扩散模型的长文本生成方法随着序列长度增加会面临计算成本和内存开销过高的问题，需要一种更可扩展的解决方案。

**方法:** 提出SA-DiffuSeq框架，集成稀疏注意力机制，在扩散过程中选择性分配注意力，并设计了针对稀疏注意力动态的软吸收状态来稳定扩散轨迹和加速序列重建。

**结果:** 实验表明SA-DiffuSeq在训练效率和采样速度上都超越了最先进的扩散基线模型，尤其在长序列上表现突出。

**结论:** 将结构化稀疏性融入扩散模型是实现高效且表达能力强的长文本生成的有前景方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SA-DiffuSeq%3A+Addressing+Computational+and+Scalability+Challenges+in+Long-Document+Generation+with+Sparse+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20724&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [34] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş, Malikeh Ehghaghi, Brian Lester, Fengyuan Liu, Wanru Zhao, Marco Ciccone, Colin Raffel*

**主要类别:** cs.CL

**AI概要:** TokSuite是一个研究工具包，包含使用不同分词器但其他条件相同的14个模型和一个新基准测试，用于分离分析分词器对语言模型性能的影响。


<details>
  <summary>更多</summary>
  
**动机:** 分词器是语言模型处理文本的基础，但其对模型性能和行为的影响尚未被充分理解，因为很难单独衡量分词器的影响。

**方法:** 训练14个使用不同分词器但架构、数据集、训练预算和初始化完全相同的模型，并创建专门测量分词器相关扰动的基准测试。

**结果:** TokSuite能够稳健地分离分词器的影响，支持一系列新发现，阐明了各种流行分词器的优缺点。

**结论:** 该研究提供了系统分析分词器影响的方法和工具，有助于更好地理解不同分词器在语言模型中的具体作用和效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TokSuite%3A+Measuring+the+Impact+of+Tokenizer+Choice+on+Language+Model+Behavior，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20757，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20757&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [35] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu, Olivier Tieleman, Caitlin A. Stamatis, Luka Smyth, Thomas D. Hull, Daniel R. Cahn, Matteo Malgaroli*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种对抗训练框架，通过生成器（用户模拟器）和判别器之间的竞争动态，迭代提升用户模拟器的真实性，特别针对心理健康支持聊天机器人领域，显著提高了系统故障模式的暴露能力。


<details>
  <summary>更多</summary>
  
**动机:** 创建能够准确复制人类行为的用户模拟器对于训练和评估任务导向对话系统至关重要，但目前仍具挑战性。有效的模拟器需要具备暴露被评估系统故障模式的关键能力。

**方法:** 采用对抗训练框架，通过生成器（用户模拟器）和判别器之间的竞争动态进行迭代改进。在心理健康支持聊天机器人领域应用该方法，比较微调模拟器与零样本基础模型的表现。

**结果:** 微调模拟器在暴露系统问题方面显著优于零样本基础模型，对抗训练进一步增强了多样性、分布对齐和预测有效性。模拟器在不同聊天机器人配置下实现了模拟与真实故障发生率之间的强相关性，同时保持故障模式的低分布差异。判别器准确率在三次对抗迭代后急剧下降，表明真实性得到改善。

**结论:** 对抗训练是在心理健康支持任务导向对话领域创建真实用户模拟器的有前景方法，能够在部署前实现快速、可靠且经济高效的系统评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Training+for+Failure-Sensitive+User+Simulation+in+Mental+Health+Dialogue+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20773&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [36] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam, Segun Aroyehun*

**主要类别:** cs.CL

**AI概要:** 大型语言模型在数学辅导中的教学质量和策略分析：虽然平均教学感知质量接近专家水平，但在教学策略和语言特征上与人类专家存在系统性差异。


<details>
  <summary>更多</summary>
  
**动机:** 探索大型语言模型在数学辅导中的教学行为与人类专家实践的契合程度，通过对比分析评估其教学效果。

**方法:** 采用受控的逐轮比较方法，让专家人类导师、新手人类导师和多个大型语言模型对同一组数学辅导对话轮次进行回应，分析教学策略和语言特征。

**结果:** 大型语言模型的教学感知质量平均接近专家水平，但存在系统性差异：较少使用专家常用的重述和重述策略，回应更长、词汇更多样、更礼貌。重述、词汇多样性和准确性要求与教学质量正相关，而高代理性和礼貌语言则负相关。

**结论:** 近期大型语言模型在数学辅导中展现出与专家人类导师相当的教学感知质量，但采用不同的教学和语言策略。分析教学策略和语言特征对于评估人类导师和智能辅导系统的辅导回应具有重要价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+Approach+Expert+Pedagogical+Quality+in+Math+Tutoring+but+Differ+in+Instructional+and+Linguistic+Profiles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20780&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [37] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain, Lalana Kagal*

**主要类别:** cs.CL

**AI概要:** 本研究探索将模型编辑算法（ROME、IKE、WISE）应用于机器遗忘任务，通过设计新的编辑目标来实现信息移除，发现这些方法在某些场景下能超越传统遗忘基线方法，但仍面临精确界定遗忘范围而不损害模型整体性能的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 现有机器遗忘方法对大型语言模型效率低下，要么无法完全移除目标信息，要么会损害应保留知识的性能。模型编辑算法虽能改变模型信息，但主要关注重定向而非完全移除。

**方法:** 探索ROME、IKE和WISE三种模型编辑算法，为遗忘场景设计新的编辑目标，将编辑技术应用于信息移除任务。

**结果:** 模型编辑方法在某些设置下在遗忘质量方面能超越基线遗忘方法，但与传统遗忘技术类似，难以在不损害整体模型性能的情况下精确界定遗忘范围。

**结论:** 模型编辑算法可作为机器遗忘的有效替代方案，但需要进一步解决精确控制遗忘范围和保持模型整体性能的平衡问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Investigating+Model+Editing+for+Unlearning+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20794&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [38] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan, Aaron Mueller*

**主要类别:** cs.CL

**AI概要:** 研究发现语言模型中的人口统计偏见机制与一般人口统计识别是相互独立的，通过针对性特征消融可以在减少偏见的同时保持识别能力，且不同偏见类型需要不同的干预方法。


<details>
  <summary>更多</summary>
  
**动机:** 探究人口统计偏见机制是否独立于一般人口统计识别能力，以及是否可以在消除偏见的同时保持模型的人口统计检测能力。

**方法:** 使用多任务评估设置，将人口统计信息与姓名、职业和教育水平关联，比较基于归因和基于相关性的偏见特征定位方法，在Gemma-2-9B模型上进行针对性稀疏自编码器特征消融。

**结果:** 基于归因的特征消融减少了种族和性别职业刻板印象，同时保持姓名识别准确性；基于相关性的消融对教育偏见更有效。但在教育任务中移除归因特征会导致"先验崩溃"，反而增加总体偏见。

**结论:** 人口统计偏见源于任务特定机制而非绝对的人口统计标记，通过机械推理时干预可以实现精确的去偏见而不损害核心模型能力，但需要维度特定的干预方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Mechanistic+Independence%3A+Can+Bias+Be+Removed+Without+Erasing+Demographics%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20796，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20796&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [39] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw, Marceau Nahon, Mathis Reymond, Raja Chatila, Mehdi Khamassi*

**主要类别:** cs.CL

**AI概要:** LLMs在处理新颖符号系统时表现出局限性，语义欺骗会显著降低推理性能，揭示了对表面语义的过度依赖而非真正的符号抽象能力。


<details>
  <summary>更多</summary>
  
**动机:** 研究LLMs在涉及人类价值的决策任务中的推理能力，特别是测试它们处理和操纵陌生符号的能力，以评估其是否真正具备符号抽象能力。

**方法:** 引入语义欺骗实验框架，重新定义数字和数学运算符为新颖符号，让LLMs解决使用这种改变符号表示的简单计算任务。测试了四种LLM模型。

**结果:** 语义线索显著降低了LLMs在简单任务上的性能，显示出当前LLMs在符号操作方面的局限性，存在过度依赖表面语义的倾向。

**结论:** LLMs的推理能力存在根本限制，在需要稳健符号推理的决策情境中可能失败，这引发了伦理和社会关切，质疑将推理能力归因于LLMs的普遍倾向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semantic+Deception%3A+When+Reasoning+Models+Can%27t+Compute+an+Addition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20812，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20812&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [40] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary, Chengshuai Zhao, Fan Zhang, Yung Hin Tse, Garima Agrawal, Yuli Deng, Huan Liu*

**主要类别:** cs.CL

**AI概要:** EssayCBM是一个可解释的自动作文评分框架，通过评估8个写作概念来替代直接预测分数，提供透明的评分过程和可调整的概念级反馈


<details>
  <summary>更多</summary>
  
**动机:** 解决自动化评分系统（特别是大语言模型）作为黑盒难以理解的问题，为教育者和学生提供可解释的作文评估

**方法:** 使用基于评分标准的框架，通过专门的预测头评估8个写作概念（如论点清晰度、证据使用等），概念分数形成透明瓶颈，轻量级网络仅使用概念计算最终分数

**结果:** EssayCBM在保持与黑盒模型相当性能的同时，提供可操作的概念级反馈，教师可以调整概念预测并即时查看更新后的分数

**结论:** 该框架实现了负责任的人机协同评估，通过直观的Web界面提供透明且可操作的作文评分解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EssayCBM%3A+Rubric-Aligned+Concept+Bottleneck+Models+for+Transparent+Essay+Grading，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20817，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20817&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [41] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu, Michael Färber*

**主要类别:** cs.CL

**AI概要:** MediEval是一个新的医学LLM评估基准，通过结合真实电子病历和统一医学知识库生成多样化的医学陈述，系统性评估模型在知识基础和上下文一致性方面的表现，并提出了针对性的微调方法CoRFu来提升安全性和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前医学LLM评估存在局限性，要么单独测试医学知识，要么评估患者推理而不验证正确性，缺乏对可靠性和安全性的全面评估。

**方法:** 构建MediVal基准，将MIMIC-IV电子病历与UMLS等生物医学词汇表构建的统一知识库连接，生成事实性和反事实性医学陈述，采用四象限框架系统评估知识基础和上下文一致性。

**结果:** 发现当前LLM存在幻觉支持和真相反转等关键失败模式，提出的CoRFu微调方法相比基础模型提升16.4 macro-F1分数，并消除了真相反转错误。

**结论:** MediEval基准揭示了医学LLM的关键风险，CoRFu方法有效提升了模型准确性和安全性，为医学AI系统的可靠部署提供了重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MediEval%3A+A+Unified+Medical+Benchmark+for+Patient-Contextual+and+Knowledge-Grounded+Reasoning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20822，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20822&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [42] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA, :, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt, Gargi Prasad, George Armstrong, Gerald Shen, Gorkem Batmaz, Grigor Nalbandyan, Haifeng Qian, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Himanshu Soni, Hiren Upadhyay, Huizi Mao, Huy C Nguyen, Huy Q Nguyen, Iain Cunningham, Ido Shahaf, Igor Gitman, Ilya Loshchilov, Ivan Moshkov, Izzy Putterman, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jatin Mitra, Jeffrey Glick, Jenny Chen, Jesse Oliver, Jian Zhang, Jiaqi Zeng, Jie Lou, Jimmy Zhang, Jining Huang, Joey Conway, Joey Guman, John Kamalu, Johnny Greco, Jonathan Cohen, Joseph Jennings, Joyjit Daw, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kai Xu, Kan Zhu, Kari Briski, Katherine Cheung, Katherine Luna, Keshav Santhanam, Kevin Shih, Kezhi Kong, Khushi Bhardwaj, Krishna C. Puvvada, Krzysztof Pawelec, Kumar Anik, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Li Ding, Lucas Liebenwein, Luis Vega, Maanu Grover, Maarten Van Segbroeck, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Manoj Kilaru, Maor Ashkenazi, Marc Romeijn, Mark Cai, Markus Kliegl, Maryam Moosaei, Matvei Novikov, Mehrzad Samadi, Melissa Corpuz, Mengru Wang, Meredith Price, Michael Boone, Michael Evans, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Nabin Mulepati, Natalie Hereth, Nave Assaf, Negar Habibi, Neta Zmora, Netanel Haber, Nicola Sessions, Nidhi Bhatia, Nikhil Jukar, Nikki Pope, Nikolai Ludwig, Nima Tajbakhsh, Nirmal Juluru, Oleksii Hrinchuk, Oleksii Kuchaiev, Olivier Delalleau, Oluwatobi Olabiyi, Omer Ullman Argov, Ouye Xie, Parth Chadha, Pasha Shamis, Pavlo Molchanov, Pawel Morkisz, Peter Dykas, Peter Jin, Pinky Xu, Piotr Januszewski, Pranav Prashant Thombre, Prasoon Varshney, Pritam Gundecha, Qing Miao, Rabeeh Karimi Mahabadi, Ran El-Yaniv, Ran Zilberstein, Rasoul Shafipour, Rich Harang, Rick Izzo, Rima Shahbazyan, Rishabh Garg, Ritika Borkar, Ritu Gala, Riyad Islam, Roger Waleffe, Rohit Watve, Roi Koren, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Ryan Timbrook, Sadegh Mahdavi, Sahil Modi, Samuel Kriman, Sanjay Kariyappa, Sanjeev Satheesh, Saori Kaji, Satish Pasumarthi, Sean Narentharen, Sean Narenthiran, Seonmyeong Bak, Sergey Kashirsky, Seth Poulos, Shahar Mor, Shanmugam Ramasamy, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shiqing Fan, Shreya Gopal, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Simeng Sun, Smita Ithape, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Stephen Ge, Sugam Dipak Devare, Sumeet Kumar Barua, Suseella Panguluri, Suyog Gupta, Sweta Priyadarshi, Syeda Nahida Akter, Tan Bui, Teodor-Dumitru Ene, Terry Kong, Thanh Do, Tijmen Blankevoort, Tom Balough, Tomer Asida, Tomer Bar Natan, Tugrul Konuk, Twinkle Vashishth, Udi Karpas, Ushnish De, Vahid Noorozi, Vahid Noroozi, Venkat Srinivasan, Venmugil Elango, Vijay Korthikanti, Vitaly Kurin, Vitaly Lavrukhin, Wanli Jiang, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenfei Zhou, Will Jennings, William Zhang, Wojciech Prazuch, Xiaowei Ren, Yashaswi Karnati, Yejin Choi, Yev Meyer, Yi-Fu Wu, Yian Zhang, Ying Lin, Yonatan Geifman, Yonggan Fu, Yoshi Subara, Yoshi Suhara, Yubo Gao, Zach Moshe, Zhen Dong, Zihan Liu, Zijia Chen, Zijie Yan*

**主要类别:** cs.CL

**AI概要:** Nemotron 3 Nano 30B-A3B是一个混合专家架构的Mamba-Transformer语言模型，相比前代模型在参数激活减少一半的情况下实现了更高的准确性和推理吞吐量，支持高达100万token的上下文长度。


<details>
  <summary>更多</summary>
  
**动机:** 开发一个更高效的语言模型，在减少计算资源消耗的同时提升性能，超越前代模型和同类开源模型的表现。

**方法:** 采用混合专家(Mixture-of-Experts)架构结合Mamba-Transformer，在25万亿文本token上进行预训练，随后进行监督微调和大规模强化学习训练。

**结果:** 模型在前向传播中激活参数减少50%以上，推理吞吐量比GPT-OSS-20B和Qwen3-30B-A3B-Thinking-2507高3.3倍，在主流基准测试中准确性更高，具备增强的代理、推理和对话能力。

**结论:** Nemotron 3 Nano 30B-A3B展示了混合专家架构在提升模型效率和性能方面的优势，为高效语言模型的发展提供了新的方向，模型权重已在Hugging Face发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nemotron+3+Nano%3A+Open%2C+Efficient+Mixture-of-Experts+Hybrid+Mamba-Transformer+Model+for+Agentic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20848&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [43] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz, Oleg Vasilyev, Randy Sawaya*

**主要类别:** cs.CL

**AI概要:** 论文评估了在无法确定相关文档总数的情况下，多种检索质量指标的替代方案，通过比较这些指标与基于LLM的响应质量评估之间的相关性，并提出了一种新的简单有效的检索质量度量方法。


<details>
  <summary>更多</summary>
  
**动机:** 现实检索环境中，大型且不断发展的知识库使得查询相关文档总数通常未知，无法计算召回率，需要找到有效的替代评估方法。

**方法:** 通过实验比较多种现有检索策略，测量检索质量指标与基于LLM生成的响应质量判断之间的相关性，使用多个数据集进行验证（每个查询相关文档数2-15个）。

**结果:** 实验结果表明，某些检索质量指标与LLM响应质量评估具有良好相关性，同时提出了一种新的简单度量方法，在不需知道相关文档总数的情况下表现良好。

**结论:** 在无法计算召回率的现实检索场景中，可以通过LLM响应质量评估来验证检索效果，并提出了有效的替代度量指标，为实际应用提供了实用的评估解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+important+is+Recall+for+Measuring+Retrieval+Quality%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20854&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [44] [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)
*NVIDIA, :, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frank Sun, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt, Gargi Prasad, George Armstrong, Gerald Shen, Gorkem Batmaz, Grigor Nalbandyan, Haifeng Qian, Harsh Sharma, Hayley Ross, Helen Ngo, Herbert Hum, Herman Sahota, Hexin Wang, Himanshu Soni, Hiren Upadhyay, Huizi Mao, Huy C Nguyen, Huy Q Nguyen, Iain Cunningham, Ido Galil, Ido Shahaf, Igor Gitman, Ilya Loshchilov, Itamar Schen, Itay Levy, Ivan Moshkov, Izik Golan, Izzy Putterman, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jatin Mitra, Jeffrey Glick, Jenny Chen, Jesse Oliver, Jian Zhang, Jiaqi Zeng, Jie Lou, Jimmy Zhang, Jinhang Choi, Jining Huang, Joey Conway, Joey Guman, John Kamalu, Johnny Greco, Jonathan Cohen, Joseph Jennings, Joyjit Daw, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kai Xu, Kan Zhu, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kevin Shih, Kezhi Kong, Khushi Bhardwaj, Kirthi Shankar, Krishna C. Puvvada, Krzysztof Pawelec, Kumar Anik, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Li Ding, Lizzie Wei, Lucas Liebenwein, Luis Vega, Maanu Grover, Maarten Van Segbroeck, Maer Rodrigues de Melo, Mahdi Nazemi, Makesh Narsimhan Sreedhar, Manoj Kilaru, Maor Ashkenazi, Marc Romeijn, Marcin Chochowski, Mark Cai, Markus Kliegl, Maryam Moosaei, Matt Kulka, Matvei Novikov, Mehrzad Samadi, Melissa Corpuz, Mengru Wang, Meredith Price, Michael Andersch, Michael Boone, Michael Evans, Miguel Martinez, Mikail Khona, Mike Chrzanowski, Minseok Lee, Mohammad Dabbah, Mohammad Shoeybi, Mostofa Patwary, Nabin Mulepati, Najeeb Nabwani, Natalie Hereth, Nave Assaf, Negar Habibi, Neta Zmora, Netanel Haber, Nicola Sessions, Nidhi Bhatia, Nikhil Jukar, Nikki Pope, Nikolai Ludwig, Nima Tajbakhsh, Nir Ailon, Nirmal Juluru, Nishant Sharma, Oleksii Hrinchuk, Oleksii Kuchaiev, Olivier Delalleau, Oluwatobi Olabiyi, Omer Ullman Argov, Omri Puny, Oren Tropp, Ouye Xie, Parth Chadha, Pasha Shamis, Paul Gibbons, Pavlo Molchanov, Pawel Morkisz, Peter Dykas, Peter Jin, Pinky Xu, Piotr Januszewski, Pranav Prashant Thombre, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Qing Miao, Qiyu Wan, Rabeeh Karimi Mahabadi, Rachit Garg, Ran El-Yaniv, Ran Zilberstein, Rasoul Shafipour, Rich Harang, Rick Izzo, Rima Shahbazyan, Rishabh Garg, Ritika Borkar, Ritu Gala, Riyad Islam, Robert Hesse, Roger Waleffe, Rohit Watve, Roi Koren, Ruoxi Zhang, Russell Hewett, Russell J. Hewett, Ryan Prenger, Ryan Timbrook, Sadegh Mahdavi, Sahil Modi, Samuel Kriman, Sangkug Lim, Sanjay Kariyappa, Sanjeev Satheesh, Saori Kaji, Satish Pasumarthi, Saurav Muralidharan, Sean Narentharen, Sean Narenthiran, Seonmyeong Bak, Sergey Kashirsky, Seth Poulos, Shahar Mor, Shanmugam Ramasamy, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shiqing Fan, Shreya Gopal, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Simeng Sun, Smita Ithape, Somshubra Majumdar, Soumye Singhal, Stas Sergienko, Stefania Alborghetti, Stephen Ge, Sugam Dipak Devare, Sumeet Kumar Barua, Suseella Panguluri, Suyog Gupta, Sweta Priyadarshi, Syeda Nahida Akter, Tan Bui, Teodor-Dumitru Ene, Terry Kong, Thanh Do, Tijmen Blankevoort, Tim Moon, Tom Balough, Tomer Asida, Tomer Bar Natan, Tomer Ronen, Tugrul Konuk, Twinkle Vashishth, Udi Karpas, Ushnish De, Vahid Noorozi, Vahid Noroozi, Venkat Srinivasan, Venmugil Elango, Victor Cui, Vijay Korthikanti, Vinay Rao, Vitaly Kurin, Vitaly Lavrukhin, Vladimir Anisimov, Wanli Jiang, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenfei Zhou, Will Jennings, William Zhang, Wojciech Prazuch, Xiaowei Ren, Yashaswi Karnati, Yejin Choi, Yev Meyer, Yi-Fu Wu, Yian Zhang, Yigong Qin, Ying Lin, Yonatan Geifman, Yonggan Fu, Yoshi Subara, Yoshi Suhara, Yubo Gao, Zach Moshe, Zhen Dong, Zhongbo Zhu, Zihan Liu, Zijia Chen, Zijie Yan*

**主要类别:** cs.CL

**AI概要:** Nemotron 3模型家族包括Nano、Super和Ultra三个版本，采用混合Mamba-Transformer架构，支持最长100万token上下文，具备强大的智能体、推理和对话能力。


<details>
  <summary>更多</summary>
  
**动机:** 开发一个高性能、高效率的模型家族，满足不同规模和应用场景的需求，从成本效益高的推理到最先进的准确性要求。

**方法:** 使用Mixture-of-Experts混合Mamba-Transformer架构，Super和Ultra模型采用NVFP4训练和LatentMoE技术，所有模型都经过多环境强化学习后训练，支持多步工具使用和推理预算控制。

**结果:** Nano模型在保持极高推理成本效益的同时超越同类模型精度；Super针对协作智能体和大批量工作负载优化；Ultra提供最先进的准确性和推理性能。

**结论:** Nemotron 3模型家族提供了从轻量级到顶级性能的全套解决方案，将开源模型权重、训练软件和配方，推动AI技术的普及和应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NVIDIA+Nemotron+3%3A+Efficient+and+Open+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20856&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.

</details>


### [45] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

**主要类别:** cs.CL

**AI概要:** 该研究系统分析了计算约束下的小型语言模型，发现即使在小规模下，基于注意力的模型在每FLOP效率上优于MLP，但大型模型中的成功架构技术不一定适用于小模型。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是系统评估在严格计算约束下，不同架构选择（从线性预测器到多层Transformer）与训练预算如何相互作用影响小型语言模型的性能。

**方法:** 从线性下一词预测器开始，逐步引入非线性、自注意力和多层Transformer架构，在Tiny Shakespeare字符级建模和PTB/WikiText-2词级建模上进行评估，使用测试负对数似然、参数量和近似训练FLOPs来比较模型。

**结果:** 结果显示基于注意力的模型在每FLOP效率上主导MLP，但增加深度或上下文长度而没有充分优化会降低性能。旋转位置编码(RoPE)等在大语言模型中成功的技术不一定能转移到小模型领域。

**结论:** 小型语言模型的最优架构选择与大型模型不同，需要针对计算约束进行专门优化，不能简单套用大模型的成功技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Architectural+Trade-offs+in+Small+Language+Models+Under+Compute+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20877，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20877&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [46] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu, Shaotian Yan, Rui Miao, Bing Wang, Chen Shen, Jun Zhang, Jieping Ye*

**主要类别:** cs.CL

**AI概要:** 该论文提出了推理蒸馏溯源追踪框架，用于分析蒸馏模型能力的来源，发现蒸馏模型在测试时能生成教师模型来源的行为，并基于此提出了教师引导的数据选择方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有推理蒸馏方法缺乏对蒸馏模型能力来源的详细分析，不清楚学生模型在新测试场景中是否能保持与教师模型一致的行为，还是回归到原始输出模式，这引发了对蒸馏模型泛化能力的担忧。

**方法:** 提出跨模型推理蒸馏溯源追踪框架，通过比较教师模型、原始学生模型和蒸馏模型在相同上下文中的预测概率，将每个动作分类到不同类别，系统性地分析每个动作的来源。

**结果:** 实验证明在测试时上下文中，蒸馏模型确实能生成教师来源的动作，这些动作与蒸馏模型观察到的性能相关并能合理解释。基于此提出的教师引导数据选择方法在多个代表性教师模型和多样化学生模型上验证有效。

**结论:** 溯源追踪框架揭示了推理蒸馏的有效性来源，提出的数据选择方法为蒸馏提供了原则性标准，该框架对推理蒸馏领域具有重要价值和前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Where+Did+This+Sentence+Come+From%3F+Tracing+Provenance+in+LLM+Reasoning+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20908，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20908&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [47] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong, Haotian Guo, Weixiang Xu, Huan Zhao, Zixing Zhang*

**主要类别:** cs.CL

**AI概要:** FEND是一个基于基础模型的多模态框架，用于检测阿尔茨海默病、抑郁症和自闭症谱系障碍，整合语音和文本模态，在13个多语言数据集上进行评估，发现多模态融合在AD和抑郁症检测中表现优异，但在ASD中因数据集异质性而表现不佳。


<details>
  <summary>更多</summary>
  
**动机:** 神经精神疾病（如阿尔茨海默病、抑郁症、自闭症谱系障碍）存在语言和声学异常，可作为早期检测的生物标志物，但目前缺乏多语言泛化能力和统一评估框架。

**方法:** 提出FEND框架，整合语音和文本模态，利用13个多语言数据集（英语、中文、希腊语、法语、荷兰语），系统评估多模态融合性能，分析模态不平衡和跨语料库泛化能力。

**结果:** 多模态融合在AD和抑郁症检测中表现优异，但在ASD中因数据集异质性而表现不佳；存在模态不平衡问题，多模态融合未能超越最佳单模态模型；跨语料库实验显示在任务和语言一致场景中表现稳健，但在多语言和任务异质设置中性能下降。

**结论:** FEND通过提供广泛基准和性能影响因素分析，推动了自动化、全生命周期和多语言神经精神疾病评估领域的发展，鼓励研究者采用该框架进行公平比较和可重复研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+Model-based+Evaluation+of+Neuropsychiatric+Disorders%3A+A+Lifespan-Inclusive%2C+Multi-Modal%2C+and+Multi-Lingual+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20948，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20948&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [48] [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)
*Shize Liang, Hongzhi Wang*

**主要类别:** cs.CL

**AI概要:** 提出基于神经网络框架的token级幻觉检测方法，使用轻量级MLP探针进行非线性建模，通过多目标联合损失函数和贝叶斯优化自动搜索最优插入层，在多个数据集上显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型容易产生幻觉内容，现有基于不确定性估计和外部知识检索的方法存在高置信度错误和依赖检索效率的问题，而传统线性探针无法捕捉深层语义空间的非线性结构。

**方法:** 冻结语言模型参数，使用轻量级MLP探针对高层隐藏状态进行非线性建模，设计多目标联合损失函数增强检测稳定性，建立层位置-探针性能响应模型并通过贝叶斯优化自动搜索最优插入层。

**结果:** 在LongFact、HealthBench和TriviaQA数据集上，MLP探针在准确性、召回率和低误报条件下的检测能力均显著优于最先进方法。

**结论:** 神经网络框架的token级幻觉检测方法有效解决了现有方法的局限性，实现了实时轻量的高性能幻觉检测，为高风险领域应用提供了可靠解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Probe-Based+Hallucination+Detection+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20949&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.

</details>


### [49] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah*

**主要类别:** cs.CL

**AI概要:** 本文介绍了TriAligner系统，这是一个用于多语言和跨语言事实核查声明检索的双编码器架构，通过对比学习和多模态翻译实现了显著的性能提升


<details>
  <summary>更多</summary>
  
**动机:** 在错误信息快速传播的时代，有效的多语言事实核查变得日益重要，需要开发能够跨语言检索核查声明的系统

**方法:** 采用双编码器架构，结合对比学习策略，整合原生语言和英语翻译的多模态信息，通过数据预处理、大语言模型增强和困难负样本采样来提升表示学习

**结果:** 在单语言和跨语言基准测试中，相比基线方法在检索准确性和事实核查性能方面取得了显著改进

**结论:** TriAligner系统通过创新的对齐策略和多模态信息整合，为多语言事实核查声明检索提供了有效的解决方案，展示了在对抗错误信息传播方面的潜力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MultiMind+at+SemEval-2025+Task+7%3A+Crosslingual+Fact-Checked+Claim+Retrieval+via+Multi-Source+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20950&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [50] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang, Jiaqi Wei, Yuejin Yang, Zijie Qiu, Yuhan Chen, Zhiqiang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Wanli Ouyang, Chenyu You, Siqi Sun*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种增强蛋白质语言模型推理能力的新方法——反射预训练，通过引入辅助"思考令牌"来克服蛋白质语言表达能力有限的问题，使模型能够进行类似CoT的中间推理步骤。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Chain-of-Thought (CoT) 提示方法在自然语言处理中表现出色，但由于蛋白质和RNA等生物序列语言的令牌表达能力有限（如氨基酸令牌），无法在这些非自然语言领域应用CoT推理。

**方法:** 提出语言表达能力概念，并引入反射预训练方法，在生物序列模型中首次实现通过生成辅助"思考令牌"来进行中间推理，扩展令牌集以增强生物语言的表达能力。

**结果:** 理论证明增强的令牌集显著提高了生物语言表达能力，实验结果显示反射预训练方法使蛋白质模型能够自我纠正，相比标准预训练获得了显著的性能提升。

**结论:** 反射预训练成功解决了生物序列语言表达能力有限的问题，为在蛋白质和RNA等非自然语言领域应用CoT式推理提供了有效途径，显著提升了模型的推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reflection+Pretraining+Enables+Token-Level+Self-Correction+in+Biological+Sequence+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20954&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [51] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu*

**主要类别:** cs.CL

**AI概要:** MedMistake是一个自动提取LLM在医患对话中错误的流程，将其转换为单次问答对基准，包含3390个问题，其中211个经过医学专家验证。评估显示GPT、Claude和Grok模型表现最佳。


<details>
  <summary>更多</summary>
  
**动机:** 当前评估LLM在临床环境中的表现需要多维度的评估标准，但复制特定错误到其他LLM模型需要大量人工努力，因此需要自动化流程来提取和分析错误。

**方法:** 创建自动流程：(1)生成LLM医患对话数据，(2)使用2个LLM评委委员会进行多维度评估，(3)将错误转换为简化的单次问答场景。构建包含3390个QA对的数据集，其中211个经过医学专家验证。

**结果:** GPT模型、Claude和Grok在MedMistake-Bench上获得最佳性能。发布了医生验证的基准数据集(MedMistake-Bench)和完整数据集(MedMistake-All)。

**结论:** MedMistake提供了一个有效的自动化方法来识别和评估LLM在医疗对话中的错误，为模型改进提供了有价值的基准数据集，GPT、Claude和Grok系列模型在当前医疗错误识别任务中表现领先。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automatic+Replication+of+LLM+Mistakes+in+Medical+Conversations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.20983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20983&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [52] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen, Vignesh Kothapalli, Ata Fatahibaarzi, Hejian Sang, Shao Tang, Qingquan Song, Zhipeng Wang, Muhammad Abdul-Mageed*

**主要类别:** cs.CL

**AI概要:** 通过仅对CoT推理令牌进行选择性知识蒸馏，可以在保持94%性能的同时减少50%的计算资源消耗，实现计算效率与模型质量的平衡。


<details>
  <summary>更多</summary>
  
**动机:** 传统的大语言模型推理能力蒸馏需要处理包含提示、思维链和答案的长序列，计算成本高昂，需要探索更高效的蒸馏方法。

**方法:** 提出选择性知识蒸馏方法，仅对思维链(CoT)令牌进行训练，建立截断协议量化序列长度与计算质量之间的权衡关系。

**结果:** 仅使用每个训练序列前50%的令牌进行训练，可在数学基准测试中保持约94%的完整序列性能，同时减少约50%的训练时间、内存使用和FLOPs。

**结论:** 推理蒸馏受益于优先处理早期推理令牌，为计算效率与模型质量之间的权衡提供了一个简单有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distilling+the+Essence%3A+Efficient+Reasoning+Distillation+via+Sequence+Truncation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21002，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21002&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [53] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi, Qian Kou, Yuduo Li, Hua Zhou*

**主要类别:** cs.CL

**AI概要:** SFTKey是一个两阶段训练方案，通过在第一阶段进行常规监督微调确保输出格式，第二阶段仅对关键答案部分进行微调，解决了CoT序列过长导致注意力分配不均的问题，平均准确率提升超过5%


<details>
  <summary>更多</summary>
  
**动机:** 传统监督微调中，模型对过长的思维链序列分配过多注意力，而忽略了更短但关键的最后答案部分，这直接影响任务成功率和评估质量

**方法:** 提出SFTKey两阶段训练方案：第一阶段使用常规SFT确保正确输出格式；第二阶段仅对关键答案部分进行微调以提高准确性

**结果:** 在多个基准测试和模型系列上的广泛实验表明，SFTKey相比传统SFT平均准确率提升超过5%，同时保持生成正确格式的能力

**结论:** 本研究通过明确平衡思维链学习和对答案相关token的额外优化，推进了大语言模型微调技术的发展

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Supervised+Fine-Tuning%3A+Emphasizing+Key+Answer+Tokens+for+Improved+LLM+Accuracy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21017，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21017&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [54] [Semantic Refinement with LLMs for Graph Representations](https://arxiv.org/abs/2512.21106)
*Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个数据自适应的语义精炼框架DAS，通过将固定图神经网络与大型语言模型耦合在闭环反馈中，来解决图数据中结构-语义异质性带来的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 图结构化数据在预测信号来源上存在显著异质性，有些领域节点语义主导，有些领域结构模式主导。现有方法主要从模型侧增加归纳偏置，但无法适应现实世界图的无限多样性。

**方法:** 采用数据中心的视角，将节点语义视为任务自适应变量。提出DAS框架，将固定GNN和LLM耦合在闭环反馈循环中：GNN提供隐式监督信号指导LLM的语义精炼，精炼后的语义反馈更新图学习器。

**结果:** 在文本丰富和文本缺失的图上进行评估，结果显示在结构主导的图上获得持续改进，同时在语义丰富的图上保持竞争力。

**结论:** 数据中心的语义自适应方法能有效应对图数据中的结构-语义异质性挑战，证明了从数据角度而非模型角度解决这一问题的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semantic+Refinement+with+LLMs+for+Graph+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21106，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21106&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.

</details>


### [55] [Semi-Supervised Learning for Large Language Models Safety and Content Moderation](https://arxiv.org/abs/2512.21107)
*Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea*

**主要类别:** cs.CL

**AI概要:** 该论文提出使用半监督学习技术来提高大型语言模型的安全性分类性能，通过结合标记和未标记数据以及任务特定的数据增强方法，显著提升了安全分类器的效果。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型安全分类器训练依赖大量标记数据，但这些数据获取困难、容易存在标注错误或包含合成数据，因此需要寻找更有效的方法来提高安全性分类性能。

**方法:** 采用半监督学习技术，利用标记和未标记数据相结合的方式，特别强调使用任务特定的数据增强方法，而不是通用的增强技术。

**结果:** 半监督学习技术显著提升了安全任务的性能表现，特别是在提示词和响应两个方面的安全分类上都取得了改进。

**结论:** 半监督学习结合任务特定的数据增强是解决大型语言模型安全分类数据获取问题的有效方法，能够显著提高分类性能，为LLM安全防护提供了新的技术路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semi-Supervised+Learning+for+Large+Language+Models+Safety+and+Content+Moderation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21107&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.

</details>


### [56] [ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models](https://arxiv.org/abs/2512.21120)
*Sichun Luo, Yi Huang, Mukai Li, Shichang Meng, Fengyuan Liu, Zefa Hu, Junlan Feng, Qi Liu*

**主要类别:** cs.CL

**AI概要:** 该论文提出了ClarifyMT-Bench多轮澄清基准和ClarifyAgent代理方法，用于评估和改进大语言模型在现实多轮对话中的澄清能力，发现现有LLMs存在过早回答的倾向，并提出了解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大语言模型澄清基准主要假设单轮交互或合作用户，无法有效评估现实场景中多轮对话的澄清行为，需要构建更真实的评估框架。

**方法:** 提出了基于五维模糊分类法和六种行为多样性模拟用户角色的ClarifyMT-Bench基准，通过混合LLM-人工流程构建了6,120个多轮对话，并开发了ClarifyAgent代理方法，将澄清分解为感知、预测、跟踪和规划四个模块。

**结果:** 评估十个代表性LLM发现存在一致的澄清不足偏差：模型倾向于过早回答，且随着对话深度增加性能下降。ClarifyAgent方法显著提高了在不同模糊条件下的鲁棒性。

**结论:** ClarifyMT-Bench为研究LLMs何时应该提问、何时应该回答以及如何在真实人机交互中处理模糊性提供了可复现的基础，提出的ClarifyAgent方法有效改善了模型的澄清能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ClarifyMT-Bench%3A+Benchmarking+and+Improving+Multi-Turn+Clarification+for+Conversational+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21120，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21120&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.

</details>


### [57] [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)
*Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux*

**主要类别:** cs.CL

**AI概要:** SpidR-Adapt是一个用于快速适应新语言的语音表示学习方法，通过元学习和多任务自适应预训练，仅需不到1小时的音频数据就能超越领域内语言模型，数据效率比标准训练高100倍以上。


<details>
  <summary>更多</summary>
  
**动机:** 人类婴儿只需几百小时的语音接触就能掌握新语言的基本单位，而自监督语音模型需要大量数据，存在明显的效率差距。

**方法:** 将低资源语音表示学习构建为元学习问题，采用多任务自适应预训练协议和双层优化框架，提出一阶双层优化方法降低计算成本，并通过交替自监督和监督目标的交错监督实现稳定训练。

**结果:** 在音位区分性（ABX）和口语语言建模（sWUGGY、sBLIMP、tSC）方面取得快速提升，使用不到1小时目标语言音频训练后性能超过领域内语言模型。

**结论:** 该方法为构建生物学启发的、数据高效的语音表示提供了一条实用且架构无关的路径，训练代码和模型检查点已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpidR-Adapt%3A+A+Universal+Speech+Representation+Model+for+Few-Shot+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21204&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.

</details>


### [58] [SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance](https://arxiv.org/abs/2512.21280)
*Divij Dudeja, Mayukha Pal*

**主要类别:** cs.CL

**AI概要:** SMART是一个针对工程手册处理的专用Transformer模型，采用分层结构和内存增强网络，相比GPT-2和BERT参数量减少60-70%，但准确率提升21.3%，提供快速检索和动态推理两种模式。


<details>
  <summary>更多</summary>
  
**动机:** 工程手册内容冗长、格式密集，传统transformer模型将其视为扁平token流处理，导致数字答案错误且需要低效记忆单独事实。

**方法:** 采用三层结构：1)语法感知事实提取器(Grammarian Tree LSTM)；2)紧凑索引内存MANN，将事实索引为384维向量；3)6层Transformer融合检索到的事实生成响应。

**结果:** 模型参数量45.51M，比GPT-2减少64%，比BERT减少69%，但准确率比GPT-2高21.3%。提供亚秒级快速检索和RAG辅助的动态推理两种模式。

**结论:** SMART通过结构化处理和内存索引机制，在处理工程手册时比小型transformer模型产生更可靠的结果并减少幻觉现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SMART+SLM%3A+Structured+Memory+and+Reasoning+Transformer%2C+A+Small+Language+Model+for+Accurate+Document+Assistance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21280，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21280&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.

</details>


### [59] [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)
*Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt*

**主要类别:** cs.CL

**AI概要:** Parallel Token Prediction (PTP)是一个用于语言模型并行序列生成的通用框架，通过单次Transformer调用联合预测多个依赖token，减少自回归解码延迟，并避免现有多token预测方法的独立性限制。


<details>
  <summary>更多</summary>
  
**动机:** 解决自回归解码的延迟瓶颈问题，同时避免现有并行生成方法中过于严格的独立性假设限制。

**方法:** 将采样过程融入模型，在单次Transformer调用中联合预测多个依赖token；通过蒸馏现有模型或无需教师的逆自回归训练进行训练。

**结果:** 在Vicuna-7B上实现最先进的推测解码性能，在Spec-Bench上每步接受超过4个token；证明PTP可以表示任意自回归序列分布。

**结论:** PTP框架的通用性表明，在不损失建模能力的情况下实现长序列的并行生成是可行的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parallel+Token+Prediction+for+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21323&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

</details>


### [60] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang, Jin Huang, Xingjian Zhang, Tianhao Wang, Jiaqi W. Ma*

**主要类别:** cs.CL

**AI概要:** 研究表明ARC类推理基准测试的性能差距主要源于视觉感知限制而非归纳推理缺陷，通过分离感知和推理两阶段实验验证了这一点。


<details>
  <summary>更多</summary>
  
**动机:** 挑战当前对ARC基准测试性能差距的常见解释，即认为差距主要源于机器推理能力不足，提出可能是视觉感知限制导致的假设。

**方法:** 设计两阶段实验管道：第一阶段将图像独立转换为自然语言描述（感知阶段），第二阶段基于描述进行规则归纳和应用（推理阶段），隔离感知和推理过程。

**结果:** 在三个ARC风格数据集上，两阶段方法显著优于端到端方法，约80%的模型失败源于感知错误，感知能力是性能差距的主导因素。

**结论:** ARC类基准测试混淆了感知和推理挑战，现有性能差距可能高估了机器推理的缺陷，需要开发能够分离感知和推理的评估协议。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Your+Reasoning+Benchmark+May+Not+Test+Reasoning%3A+Revealing+Perception+Bottleneck+in+Abstract+Reasoning+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21329&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


### [61] [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)
*Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di, Rui Wang*

**主要类别:** cs.CL

**AI概要:** C2LLM是基于Qwen-2.5-Coder的代码嵌入模型家族，采用PMA模块生成序列嵌入，在MTEB-Code基准测试中创下新记录


<details>
  <summary>更多</summary>
  
**动机:** 解决基于EOS的序列嵌入存在的信息瓶颈问题，并支持嵌入维度的灵活调整

**方法:** 基于Qwen-2.5-Coder骨干网络，使用多头注意力池化(PMA)模块从token嵌入生成序列嵌入，利用预训练获得的因果表示

**结果:** 在300万公开数据上训练，C2LLM-7B在MTEB-Code排行榜上排名第一，在同类规模模型中创下新记录

**结论:** C2LLM通过PMA模块有效解决了序列嵌入的信息瓶颈问题，在代码嵌入任务中表现出色，为代码理解提供了新的技术方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是C2LLM+Technical+Report%3A+A+New+Frontier+in+Code+Retrieval+via+Adaptive+Cross-Attention+Pooling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21332&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.

</details>


### [62] [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/abs/2512.21336)
*Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin*

**主要类别:** cs.CL

**AI概要:** 该论文提出了去噪熵(Denoising Entropy)概念，用于量化掩码扩散模型生成过程中的累积预测不确定性，并基于此开发了两种解码路径优化算法，显著提升了生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 掩码扩散模型(MDMs)的非自回归生成虽然灵活，但输出质量对解码顺序高度敏感，这种变异性源于生成路径上的累积预测不确定性。

**方法:** 引入可计算的去噪熵指标来量化生成过程中的不确定性，并提出了两种算法：事后选择方法和实时引导策略来优化解码路径。

**结果:** 实验表明，熵引导方法显著提高了生成质量，在具有挑战性的推理、规划和代码基准测试中持续提升准确性。

**结论:** 去噪熵成为了理解和控制生成过程的原则性工具，将MDMs中的不确定性从负担转变为发现高质量解决方案的关键优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Decoding+Paths+in+Masked+Diffusion+Models+by+Quantifying+Uncertainty，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.21336，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21336&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.

</details>
