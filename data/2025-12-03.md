<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 37]
- [cs.CL](#cs.CL) [总数: 44]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas, Lucas Cordeiro, Youcheng Sun, Waldir Junior*

**主要类别:** cs.AI

**AI概要:** 该论文提出了LLM-Verifier收敛定理，为大型语言模型与形式验证工具的交互提供了首个具有可证明终止性和收敛性保证的正式框架，通过马尔可夫链建模和大量实证验证，实现了可预测的资源规划和性能预算。


<details>
  <summary>更多</summary>
  
**动机:** 当前使用大型语言模型进行软件验证的方法缺乏可靠的理论基础，导致精炼过程不稳定——有时收敛，有时循环，有时偏离稳定轨迹，需要建立坚实的理论框架来保证验证过程的可靠性。

**方法:** 将LLM与验证器的交互建模为离散时间马尔可夫链，状态转移由错误减少概率(δ)决定，通过理论分析证明程序对任意δ>0都能几乎必然终止，且期望迭代次数有界E[n]≤4/δ。

**结果:** 通过超过90,000次试验的实证验证，所有运行都成功达到验证状态，收敛因子紧密聚集在Cf≈1.0附近，实证结果与理论预测高度一致，支持将工作流划分为边际、实用和高性能三个操作区域。

**结论:** 理论保证和实验证据共同为LLM辅助验证提供了清晰的架构基础，工程师可以获得支持可预测资源规划和性能预算的框架，为在安全关键软件环境中部署这些流水线提供了必要保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+4%2F%24%CE%B4%24+Bound%3A+Designing+Predictable+LLM-Verifier+Systems+for+Formal+Method+Guarantee，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02080&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [2] [Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code](https://arxiv.org/abs/2512.02170)
*Pritam Deka, Barry Devereux*

**主要类别:** cs.AI

**AI概要:** Flowchart2Mermaid是一个轻量级网络系统，将静态流程图图像转换为可编辑的Mermaid.js代码，支持文本编辑、拖拽节点插入和自然语言命令的混合交互式精炼。


<details>
  <summary>更多</summary>
  
**动机:** 流程图通常以静态图像形式分享，难以编辑和重用，需要一种方法将其转换为结构化、版本可控的文本表示形式。

**方法:** 使用详细系统提示和视觉语言模型将流程图图像转换为Mermaid.js代码，提供内联文本编辑、拖拽节点插入和集成AI助手处理自然语言命令的混合交互界面。

**结果:** 系统能够生成与渲染图表保持同步的结构化文本表示，并引入了评估结构准确性、流程正确性、语法有效性和完整性的指标来评估多个模型。

**结论:** 该工具解决了静态流程图难以编辑的问题，提供了版本可控的文本表示，并通过交互式精炼功能提高了转换的准确性和可用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flowchart2Mermaid%3A+A+Vision-Language+Model+Powered+System+for+Converting+Flowcharts+into+Editable+Diagram+Code，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02170，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02170&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present \textsc{Flowchart2Mermaid}, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.

</details>


### [3] [From monoliths to modules: Decomposing transducers for efficient world modelling](https://arxiv.org/abs/2512.02193)
*Alexander Boyd, Franz Nowak, David Hyland, Manuel Baltieri, Fernando E. Rosas*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个将复杂世界模型分解为模块化子转换器的框架，支持并行化和可解释的分布式推理，为AI安全的结构透明性和实际应用的计算效率提供基础。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界场景通常包含以模块化方式交互的子组件，利用这一特性可以高效建模复杂世界模型，避免高计算需求。

**方法:** 开发了一个基于转换器（transducers）的框架，通过逆向分解过程将复杂世界模型分解为在独立输入输出子空间上运行的子转换器。

**结果:** 实现了并行化和可解释的模块化世界建模替代方案，支持分布式推理。

**结论:** 这些结果为连接AI安全所需的结构透明性和实际推理所需的计算效率奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+monoliths+to+modules%3A+Decomposing+transducers+for+efficient+world+modelling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02193&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.

</details>


### [4] [STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls](https://arxiv.org/abs/2512.02228)
*Shubhi Asthana, Bing Zhang, Chad DeLuca, Ruchi Mahindru, Hima Patel*

**主要类别:** cs.AI

**AI概要:** STRIDE框架帮助决定何时使用完全自主的AI代理，通过系统评估任务特性来在三种AI模式（直接LLM调用、引导式助手、完全自主代理）中选择最合适的方案，避免不必要的代理部署。


<details>
  <summary>更多</summary>
  
**动机:** 从无状态大语言模型向自主目标驱动代理的快速转变引发了一个核心问题：何时真正需要代理式AI？盲目部署代理会导致成本、复杂性和风险的增加。

**方法:** 提出STRIDE框架，整合结构化任务分解、动态属性归因和自我反思需求分析，生成代理适用性评分，确保完全自主性仅用于具有内在动态性或演变上下文的任务。

**结果:** 在30个真实世界任务中评估，STRIDE在模式选择上达到92%准确率，减少45%不必要的代理部署，降低37%资源成本。专家验证确认其实际效用。

**结论:** 这项工作将代理采用重新定义为需求驱动的设计决策，确保自主性仅在收益证明成本合理时应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STRIDE%3A+A+Systematic+Framework+for+Selecting+AI+Modalities+--+Agentic+AI%2C+AI+Assistants%2C+or+LLM+Calls，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02228，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02228&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.
  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.
  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.

</details>


### [5] [Benchmarking LLM Agents for Wealth-Management Workflows](https://arxiv.org/abs/2512.02230)
*Rory Milsom*

**主要类别:** cs.AI

**AI概要:** 本研究扩展TheAgentCompany框架，构建财富管理环境，评估通用LLM代理在财富管理任务中的表现，发现代理主要受端到端工作流可靠性限制而非数学推理能力，且自主性水平对性能有显著影响。


<details>
  <summary>更多</summary>
  
**动机:** 现有数字协作工具仍存在人为错误和延迟问题，需要研究通用LLM代理是否能准确且经济地完成财富管理任务。

**方法:** 引入合成领域数据，丰富同事模拟，构建自动任务生成管道，建立包含12个任务对的财富管理助手基准测试集，设置明确接受标准和确定性评分器。

**结果:** 代理在数学推理方面表现良好，但端到端工作流可靠性是主要限制因素；自主性水平显著影响代理性能；现有模型评估方法存在缺陷阻碍了基准测试的有效性。

**结论:** LLM代理在财富管理任务中具有潜力，但需要改进工作流可靠性，同时评估方法需要更准确地反映代理在实际工作环境中的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+LLM+Agents+for+Wealth-Management+Workflows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02230&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.

</details>


### [6] [TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?](https://arxiv.org/abs/2512.02261)
*Lewen Yan, Jilin Mei, Tianyi Zhou, Lige Huang, Jie Zhang, Dongrui Liu, Jing Shao*

**主要类别:** cs.AI

**AI概要:** TradeTrap是一个统一的评估框架，用于系统性压力测试自适应和程序化自主交易代理在对抗性或故障条件下的鲁棒性，发现在系统级扰动下当前交易代理容易被误导导致严重后果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于LLM的交易代理越来越多地部署在真实金融市场中执行自主分析和交易，但它们在对抗性或故障条件下的可靠性和鲁棒性尚未得到充分检验，而这些环境具有高风险和不可逆的特性。

**方法:** 提出TradeTrap框架，针对自主交易代理的四个核心组件（市场情报、策略制定、投资组合和账本处理、交易执行）进行系统性压力测试，在闭环历史回测环境中使用真实美国股票市场数据进行评估，确保初始条件一致以实现公平可重复的比较。

**结果:** 实验表明，单个组件的小扰动会通过代理决策循环传播，导致两种类型代理出现极端集中、失控风险和大幅投资组合回撤，证明当前自主交易代理在系统层面容易被系统性误导。

**结论:** 当前自主交易代理在系统级扰动下存在严重脆弱性，需要更严格的鲁棒性测试和安全保障措施，TradeTrap框架为此提供了有效的评估工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TradeTrap%3A+Are+LLM-based+Trading+Agents+Truly+Reliable+and+Faithful%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02261&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLM-based trading agents are increasingly deployed in real-world financial markets to perform autonomous analysis and execution. However, their reliability and robustness under adversarial or faulty conditions remain largely unexamined, despite operating in high-risk, irreversible financial environments. We propose TradeTrap, a unified evaluation framework for systematically stress-testing both adaptive and procedural autonomous trading agents. TradeTrap targets four core components of autonomous trading agents: market intelligence, strategy formulation, portfolio and ledger handling, and trade execution, and evaluates their robustness under controlled system-level perturbations. All evaluations are conducted in a closed-loop historical backtesting setting on real US equity market data with identical initial conditions, enabling fair and reproducible comparisons across agents and attacks. Extensive experiments show that small perturbations at a single component can propagate through the agent decision loop and induce extreme concentration, runaway exposure, and large portfolio drawdowns across both agent types, demonstrating that current autonomous trading agents can be systematically misled at the system level. Our code is available at https://github.com/Yanlewen/TradeTrap.

</details>


### [7] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz, Sindhuja Penchala, Shahram Rahimi*

**主要类别:** cs.AI

**AI概要:** 论文指出当前AI系统在自我监控、自我纠正和行为自主调节方面存在根本性局限，提出了7个核心缺陷，并倡导向基于认知科学的AI（认知自主）进行范式转变。


<details>
  <summary>更多</summary>
  
**动机:** 尽管AI在感知、语言、推理和多模态领域取得快速进展，但现代AI系统在动态环境中缺乏自我监控、自我纠正和自主行为调节能力，这限制了其实现稳健泛化、终身适应性和真实世界自主性。

**方法:** 通过对人工系统与生物认知的比较分析，整合AI研究、认知科学和神经科学的见解，识别和分析7个核心缺陷，并提出基于神经认知原理的架构解决方案。

**结果:** 识别出7个限制当代AI模型的核心缺陷：缺乏内在自我监控、元认知意识缺失、固定非自适应学习机制、无法重构目标、缺乏表征维护、不足的具身反馈、以及内在能动性缺失。

**结论:** 主张向认知自主的AI范式转变，需要能够实现自我导向适应、动态表征管理和有意图的目标导向行为，同时配备改革性监督机制以确保系统的可解释性、可治理性和与人类价值观的一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+the+Gap%3A+Toward+Cognitive+Autonomy+in+Artificial+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02280，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02280&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [8] [DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses](https://arxiv.org/abs/2512.02282)
*Han Luo, Guy Laban*

**主要类别:** cs.AI

**AI概要:** DialogGuard是一个多代理框架，用于评估LLM在心理健康等敏感场景中的心理社会风险，包含五个高风险维度和四种评估方法，比基线方法更准确且提供可解释的风险评分。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在心理健康和危机干预等敏感服务中广泛应用，但其心理社会安全性缺乏系统评估框架，现有评估方法不完善。

**方法:** 提出DialogGuard多代理框架，通过隐私侵犯、歧视行为、心理操纵、心理伤害和侮辱行为五个维度评估风险，采用四种LLM-as-a-judge流程：单代理评分、双代理校正、多代理辩论和随机多数投票。

**结果:** 多代理机制比非LLM基线和单代理评估更准确检测心理社会风险，双代理校正和多数投票在准确性、与人类评分一致性和鲁棒性方面表现最佳，辩论方法召回率更高但容易过度标记边界案例。

**结论:** DialogGuard作为开源软件发布，提供每维度风险评分和可解释的自然语言理由，支持面向脆弱用户的网络应用提示设计、审计和监督。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DialogGuard%3A+Multi-Agent+Psychosocial+Safety+Evaluation+of+Sensitive+LLM+Responses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02282&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) now mediate many web-based mental-health, crisis, and other emotionally sensitive services, yet their psychosocial safety in these settings remains poorly understood and weakly evaluated. We present DialogGuard, a multi-agent framework for assessing psychosocial risks in LLM-generated responses along five high-severity dimensions: privacy violations, discriminatory behaviour, mental manipulation, psychological harm, and insulting behaviour. DialogGuard can be applied to diverse generative models through four LLM-as-a-judge pipelines, including single-agent scoring, dual-agent correction, multi-agent debate, and stochastic majority voting, grounded in a shared three-level rubric usable by both human annotators and LLM judges. Using PKU-SafeRLHF with human safety annotations, we show that multi-agent mechanisms detect psychosocial risks more accurately than non-LLM baselines and single-agent judging; dual-agent correction and majority voting provide the best trade-off between accuracy, alignment with human ratings, and robustness, while debate attains higher recall but over-flags borderline cases. We release Dialog-Guard as open-source software with a web interface that provides per-dimension risk scores and explainable natural-language rationales. A formative study with 12 practitioners illustrates how it supports prompt design, auditing, and supervision of web-facing applications for vulnerable users.

</details>


### [9] [Model Recovery at the Edge under Resource Constraints for Physical AI](https://arxiv.org/abs/2512.02283)
*Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta*

**主要类别:** cs.AI

**AI概要:** MERINDA是一个面向边缘设备的FPGA加速模型恢复框架，通过可并行神经架构替代迭代求解器，实现比移动GPU低11倍的内存使用和快2.2倍的运行速度，适用于资源受限的实时任务关键自主系统。


<details>
  <summary>更多</summary>
  
**动机:** 任务关键自主系统(MCAS)需要安全可解释的决策，但现有基于神经ODE的模型恢复方法在边缘设备上部署困难，主要因为迭代求解器在FPGA上效率低下，内存和能耗是实时操作的主要瓶颈。

**方法:** 提出MERINDA框架，用可并行化的神经架构替代神经ODE中的迭代求解器，实现FPGA加速的模型恢复。

**结果:** 相比移动GPU，MERINDA实现了近11倍的DRAM使用降低和2.2倍的运行速度提升。实验显示在固定精度下内存和能耗存在反比关系。

**结论:** MERINDA框架特别适合资源受限的实时任务关键自主系统，通过FPGA加速和内存优化解决了边缘设备部署的关键挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+Recovery+at+the+Edge+under+Resource+Constraints+for+Physical+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02283&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.

</details>


### [10] [Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization](https://arxiv.org/abs/2512.02302)
*Varun Kumar Dasoju, Qingsu Cheng, Zeyun Yu*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于量子启发边缘增强和稳定多组件损失函数的乳腺细胞分割框架，仅用599张训练图像就达到95.5%的Dice分数，显著减少医学图像标注所需的时间和专家资源。


<details>
  <summary>更多</summary>
  
**动机:** 医学图像标注需要大量时间和专业知识，病理学家通常需要花费数百小时标注乳腺上皮细胞核数据集，这成为临床感知AI发展的主要瓶颈。

**方法:** 使用量子启发的多尺度Gabor滤波器进行边缘增强创建第四输入通道；采用稳定多组件损失函数整合自适应Dice损失和边界感知项；引入基于复杂度的加权采样策略；使用EfficientNet-B7/UNet++架构和4-to-3通道投影；通过指数移动平均和统计异常值检测进行鲁棒验证。

**结果:** 在仅有129张验证图像的小数据集上，Dice分数达到95.5% ± 0.3%，IoU达到91.2% ± 0.4%。量子增强使边界精度提升2.1%，加权采样使小病变检测提升3.8%。

**结论:** 该方法通过有限标注实现了突破性性能，显著减少了医学专家创建数据集所需的时间，解决了临床感知AI发展的根本瓶颈问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breast+Cell+Segmentation+Under+Extreme+Data+Constraints%3A+Quantum+Enhancement+Meets+Adaptive+Loss+Stabilization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02302，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02302&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.

</details>


### [11] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu, Xiaofei Wen, Wenjie Jacky Mo, Tinghui Zhu, Yanan Xie, Peng Qi, Muhao Chen*

**主要类别:** cs.AI

**AI概要:** OmniGuard是首个全模态安全防护框架，通过大规模多模态安全数据集和专家模型蒸馏，为文本、图像、视频和音频提供统一的安全防护，在15个基准测试中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的安全防护研究主要针对单模态，采用二元分类方法，难以应对全模态大语言模型(OLLMs)在多模态场景下的安全挑战。

**方法:** 构建包含21万多样本的全模态安全数据集，通过专家模型蒸馏获取结构化安全标签和安全评估；开发具有推理能力的全模态防护框架OmniGuard。

**结果:** 在15个基准测试中，OmniGuard在各种多模态安全场景下展现出强大的有效性和泛化能力。

**结论:** OmniGuard为构建更强大和稳健的全模态安全防护系统提供了统一框架，能够有效执行策略并降低全模态风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OmniGuard%3A+Unified+Omni-Modal+Guardrails+with+Deliberate+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02306&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.

</details>


### [12] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue, Weichen Liu, Shiqi Wang, Haoming Wang, Yuyang Wu, Wei Gao*

**主要类别:** cs.AI

**AI概要:** 论文提出了ReMindView-Bench基准测试，用于评估视觉语言模型在多视图空间推理中的表现，发现现有模型在跨视图对齐和视角转换方面存在系统性失败。


<details>
  <summary>更多</summary>
  
**动机:** 当前视觉语言模型在多视图环境下难以保持几何一致性和跨视图一致性，缺乏能够隔离多视图推理与单视图感知的细粒度基准测试。

**方法:** 创建ReMindView-Bench基准，系统变化视角空间模式和查询类型；使用LLM-as-a-judge和自我一致性提示进行显性分阶段分析；通过线性探测和熵动态进行隐性分析。

**结果:** 评估15个当前VLM显示在跨视图对齐和视角转换方面一致失败；模型在帧内感知表现良好，但在跨视图信息整合时性能急剧下降；任务相关信息逐渐丢失，正确与错误轨迹间存在不确定性分离。

**结论:** 研究提供了基于认知科学的VLM空间推理诊断，揭示了多视图空间心智模型在推理过程中的形成、退化和失稳机制，为改进模型空间推理能力提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Path+and+Latent+State+Analysis+for+Multi-view+Visual+Spatial+Reasoning%3A+A+Cognitive+Science+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02340&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.

</details>


### [13] [Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games](https://arxiv.org/abs/2512.02358)
*Ran Zhang, Kun Ouyang, Tiancheng Ma, Yida Yang, Dong Fang*

**主要类别:** cs.AI

**AI概要:** 提出基于大语言模型的生成式智能体MMO模拟系统，通过监督微调和强化学习训练游戏专用AI，配合数据驱动的环境模型，实现高保真玩家行为模拟和干预响应，为数值设计优化提供可靠框架


<details>
  <summary>更多</summary>
  
**动机:** 传统MMO游戏数值系统和机制设计优化依赖昂贵的大规模在线实验或预定义统计模型参数调优，可能破坏玩家体验。简化离线模拟系统保真度有限，无法准确模拟真实玩家推理和干预反应

**方法:** 使用大语言模型构建生成式智能体模拟系统，在大规模真实玩家行为数据上进行监督微调(SFT)和强化学习(RL)训练，使LLM从通用先验适应到游戏特定领域。同时训练数据驱动的环境模型重建动态游戏系统

**结果:** 实验显示与真实玩家行为高度一致，在干预下产生合理的因果响应，实现了可靠、可解释且成本效益高的数据驱动数值设计优化框架

**结论:** 基于LLM的生成式智能体模拟系统能够有效解决传统方法的局限性，为MMO游戏数值设计优化提供高保真、可解释且经济高效的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Playtesting%3A+A+Generative+Multi-Agent+Simulation+System+for+Massively+Multiplayer+Online+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02358&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.

</details>


### [14] [Synthetic Error Injection Fails to Elicit Self-Correction In Language Models](https://arxiv.org/abs/2512.02389)
*David X. Wu, Shreyas Kapur, Anant Sahai, Stuart Russell*

**主要类别:** cs.AI

**AI概要:** 研究表明，通过监督学习注入合成错误来训练语言模型自我修正能力的方法效果不佳，即使模型能识别错误也常常重复原错误，这解释了为什么基于策略的强化学习方法在激发自我修正能力方面更有效。


<details>
  <summary>更多</summary>
  
**动机:** 探索强化学习之外的替代方法来激发语言模型的推理和自我修正能力，因为强化学习的计算成本很高。受到自动驾驶和机器人技术的启发，研究是否可以通过监督学习和合成错误注入来诱导自我修正能力。

**方法:** 在推理链中插入人工错误并掩盖它们，然后监督模型识别和纠正这些错误。通过合成错误注入的方式进行监督学习训练。

**结果:** 该方法即使在简单的合成任务上也无法显著提高性能；模型即使发现自己的错误，也常常重复原错误；合成错误与策略错误的分布偏移显著降低了微调模型的错误修正能力。

**结论:** 合成错误注入的监督学习方法无法有效激发语言模型的自我修正能力，这解释了为什么基于策略的强化学习方法在这方面被证明是唯一有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synthetic+Error+Injection+Fails+to+Elicit+Self-Correction+In+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02389&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

</details>


### [15] [Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets](https://arxiv.org/abs/2512.02436)
*Agostino Capponi, Alfio Gliozzo, Brian Zhu*

**主要类别:** cs.AI

**AI概要:** AI代理管道自动聚类预测市场并识别相关市场对，通过交易策略验证发现的关系可获得约20%的平均收益


<details>
  <summary>更多</summary>
  
**动机:** 预测市场存在碎片化问题，存在重叠问题、隐含等价性和隐藏矛盾，需要系统方法来发现市场间的潜在关系

**方法:** 使用自然语言理解对合约文本和元数据进行主题聚类，识别集群内市场对的依赖关系（正相关和反相关），基于历史数据评估关系预测准确性并转化为交易策略

**结果:** AI识别的关系准确率达到60-70%，基于这些关系的交易策略在一周时间范围内获得约20%的平均收益

**结论:** 智能AI和大语言模型能够有效发现预测市场中的潜在语义结构，为市场分析和交易策略提供可操作信号

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semantic+Trading%3A+Agentic+AI+for+Clustering+and+Relationship+Discovery+in+Prediction+Markets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02436，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02436&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.

</details>


### [16] [Guided Self-Evolving LLMs with Minimal Human Supervision](https://arxiv.org/abs/2512.02472)
*Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu*

**主要类别:** cs.AI

**AI概要:** R-Few是一个通过轻量级人类监督实现AI模型稳定自我进化的框架，使用挑战者-求解器自博弈机制，在数学和通用推理任务上实现了持续迭代改进


<details>
  <summary>更多</summary>
  
**动机:** 解决无引导自我进化系统容易陷入平台期或性能退化的问题，如概念漂移、多样性崩溃和错误进化等挑战

**方法:** 提出R-Few框架：挑战者采样少量人工标注样本指导合成问题生成，求解器在基于难度的在线课程下联合训练人工和合成样本

**结果:** 在数学任务上比R-Zero提升3.0分，性能与使用20倍人工数据的General-Reasoner相当，消融研究证实了各组件贡献

**结论:** R-Few能够缓解概念漂移，产生更稳定可控的协同进化动态，为AI自我进化提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guided+Self-Evolving+LLMs+with+Minimal+Human+Supervision，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02472&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

</details>


### [17] [COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes](https://arxiv.org/abs/2512.02499)
*Yongkai Liu, Helena Feng, Bin Jiang, Yixin Wang, Max Wintermark, David S. Liebeskind, Michael Moseley, Maarten Lansberg, Gregory Albers, Jeremy Heit, Greg Zaharchuk*

**主要类别:** cs.AI

**AI概要:** 开发了一个基于链式思维推理的LLaMA-3-8B框架COPE，用于从非结构化临床笔记预测急性缺血性卒中90天功能结局，性能优于传统模型，与GPT-4.1相当


<details>
  <summary>更多</summary>
  
**动机:** 临床笔记包含丰富的上下文信息，但其非结构化特性限制了在传统预测模型中的应用，需要开发能够利用这些信息的新型预测方法

**方法:** 使用464名AIS患者的出院摘要和90天mRS评分，开发两步链式思维框架：第一步生成临床推理，第二步输出mRS预测，并与GPT-4.1、ClinicalBERT、结构化变量ML模型和单步LLM进行比较

**结果:** COPE的MAE为1.01，±1准确率74.4%，准确准确率32.8%，性能与GPT-4.1相当，优于其他对比模型，在不同亚组中表现一致

**结论:** COPE作为一个轻量级、可解释且保护隐私的开源框架，为从非结构化临床文本进行结局预测提供了准确实用的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是COPE%3A+Chain-Of-Thought+Prediction+Engine+for+Open-Source+Large+Language+Model+Based+Stroke+Outcome+Prediction+from+Clinical+Notes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02499，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02499&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.

</details>


### [18] [Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration](https://arxiv.org/abs/2512.02530)
*Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li*

**主要类别:** cs.AI

**AI概要:** Aetheria是一个基于多智能体辩论协作的多模态可解释内容安全框架，通过五个核心智能体的动态辩论机制和RAG知识检索，显著提升了隐式风险识别能力和内容安全准确性。


<details>
  <summary>更多</summary>
  
**动机:** 数字内容爆炸式增长带来了内容安全挑战，现有基于单一模型或固定管道的审核系统在识别隐式风险和提供可解释判断过程方面存在局限性。

**方法:** 提出Aetheria框架，采用五个核心智能体的协作架构，通过基于RAG知识检索的动态相互说服辩论机制对多模态内容进行深度分析和裁决。

**结果:** 在提出的AIR-Bench基准测试中，Aetheria不仅能生成详细可追溯的审计报告，还在整体内容安全准确性上显著优于基线方法，特别是在隐式风险识别方面。

**结论:** 该框架建立了一个透明可解释的范式，显著推进了可信AI内容审核领域的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aetheria%3A+A+multimodal+interpretable+content+safety+framework+based+on+multi-agent+debate+and+collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02530，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02530&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.

</details>


### [19] [Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance](https://arxiv.org/abs/2512.02558)
*Yufei Xiao, Shangfei Wang*

**主要类别:** cs.AI

**AI概要:** 提出了一种融合视频、音频和文本的多模态共情预测方法，通过监督文档辅助训练来提升文本特征提取，在训练阶段利用特权信息（咨询主题和咨询师共情表现）来约束特征学习。


<details>
  <summary>更多</summary>
  
**动机:** 现有共情预测技术主要关注单模态（通常是文本），忽视了多模态处理能力和特权信息的利用，这些特权信息可能包含额外的共情内容。

**方法:** 使用预训练网络提取视频、音频和文本特征，进行跨模态融合得到多模态特征表示来预测共情标签。在辅助训练阶段引入监督文档作为特权信息，使用LDA模型识别潜在主题分布来约束文本特征。

**结果:** 在多模态和对话共情数据集上的实验结果表明，该方法优于现有方法。

**结论:** 提出的多模态共情预测方法通过整合多模态信息和利用监督文档作为特权信息，有效提升了共情预测性能，证明了多模态融合和特权信息利用的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Empathy+Level+Prediction+in+Multi-Modal+Scenario+with+Supervisory+Documentation+Assistance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02558，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02558&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Prevalent empathy prediction techniques primarily concentrate on a singular modality, typically textual, thus neglecting multi-modal processing capabilities. They also overlook the utilization of certain privileged information, which may encompass additional empathetic content. In response, we introduce an advanced multi-modal empathy prediction method integrating video, audio, and text information. The method comprises the Multi-Modal Empathy Prediction and Supervisory Documentation Assisted Training. We use pre-trained networks in the empathy prediction network to extract features from various modalities, followed by a cross-modal fusion. This process yields a multi-modal feature representation, which is employed to predict empathy labels. To enhance the extraction of text features, we incorporate supervisory documents as privileged information during the assisted training phase. Specifically, we apply the Latent Dirichlet Allocation model to identify potential topic distributions to constrain text features. These supervisory documents, created by supervisors, focus on the counseling topics and the counselor's display of empathy. Notably, this privileged information is only available during training and is not accessible during the prediction phase. Experimental results on the multi-modal and dialogue empathy datasets demonstrate that our approach is superior to the existing methods.

</details>


### [20] [PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing](https://arxiv.org/abs/2512.02589)
*Junyi Hou, Andre Lin Huikai, Nuo Chen, Yiwei Gong, Bingsheng He*

**主要类别:** cs.AI

**AI概要:** PaperDebugger是一个集成在LaTeX编辑器中的多智能体学术写作助手，通过Chrome扩展和Kubernetes编排层实现与编辑器的深度交互，支持上下文感知的写作辅助功能。


<details>
  <summary>更多</summary>
  
**动机:** 现有写作助手与编辑器分离，无法深度交互文档状态、结构和修订历史，限制了在LaTeX编辑器（如Overleaf）中实现智能化的上下文感知操作。

**方法:** 开发基于插件的多智能体系统，采用Chrome批准的扩展、Kubernetes原生编排层和模型上下文协议（MCP）工具链，实现可靠的双向同步、细粒度版本控制、安全状态管理和多智能体调度。

**结果:** 实现了完全集成的工作流程，包括本地化编辑、结构化评审、并行智能体执行和基于差异的更新，早期聚合分析显示用户积极参与。

**结论:** PaperDebugger验证了编辑器原生智能写作助手的实用性，为学术写作提供了深度集成的LLM驱动推理环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PaperDebugger%3A+A+Plugin-Based+Multi-Agent+System+for+In-Editor+Academic+Writing%2C+Review%2C+and+Editing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02589，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02589&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.

</details>


### [21] [IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai](https://arxiv.org/abs/2512.02605)
*Pengju Lu*

**主要类别:** cs.AI

**AI概要:** IACT是一种基于对话驱动的自主计算模型，通过动态递归代理拓扑结构替代静态工作流，支持双向状态对话实现运行时错误纠正和模糊消解。


<details>
  <summary>更多</summary>
  
**动机:** 解决静态硬编码代理工作流的局限性，传统系统需要预定义图或专门编程，无法适应开放任务的复杂性。

**方法:** 设计交互式代理调用树(IACT)模型，通过用户对话驱动自主生成动态递归代理拓扑，用双向状态对话替代刚性调用，实现交互冗余。

**结果:** 在kragent.ai系统中实现生产部署，通过实际工作流验证了模型的有效性，展示了运行时错误纠正和模糊消解能力。

**结论:** IACT模型提供了一种通用的自主系统方法，能够根据问题结构动态扩展组织复杂性，有效缓解单向函数调用中的错误传播问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IACT%3A+A+Self-Organizing+Recursive+Model+for+General+AI+Agents%3A+A+Technical+White+Paper+on+the+Architecture+Behind+kragent.ai，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02605&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.

</details>


### [22] [Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction](https://arxiv.org/abs/2512.02610)
*Yubo Hou, Mohamed Ragab, Min Wu, Chee-Keong Kwoh, Xiaoli Li, Zhenghua Chen*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为TACDA的新型领域自适应方法，用于解决跨领域剩余使用寿命预测中的领域差异问题，通过目标域重构和退化阶段一致性对齐策略，显著提升了预测性能


<details>
  <summary>更多</summary>
  
**动机:** 传统数据驱动的RUL预测方法假设训练和测试数据来自相同分布，但实际工业环境中存在领域差异问题。现有的对抗领域自适应方法忽略了目标特定信息和退化阶段的不一致性特征

**方法:** 提出TACDA方法：1）在对抗自适应过程中加入目标域重构策略，保留目标特定信息的同时学习领域不变特征；2）开发新的聚类和配对策略，实现相似退化阶段之间的一致性对齐

**结果:** 通过大量实验证明，TACDA方法在两个不同的评估指标上均优于最先进的方法，表现出卓越的性能

**结论:** TACDA方法有效解决了跨领域RUL预测中的领域差异问题，通过结合目标域重构和退化阶段一致性对齐，实现了更好的预测性能，为工业设备维护提供了更可靠的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Target-specific+Adaptation+and+Consistent+Degradation+Alignment+for+Cross-Domain+Remaining+Useful+Life+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02610，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02610&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.

</details>


### [23] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2512.02633)
*Mattia Giuri, Mathias Jackermeier, Alessandro Abate*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的方法，通过图神经网络编码布尔公式序列来学习多任务策略，解决了现有LTL强化学习方法在复杂并发事件环境中的不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于线性时序逻辑(LTL)的强化学习方法在多个高层事件同时发生且复杂交互的环境中表现不佳，需要新的方法来处理这种并发复杂性。

**方法:** 将策略基于简单的布尔公式序列，这些序列与自动机转换直接对应，并通过图神经网络(GNN)进行编码以产生结构化任务表示。

**结果:** 在复杂的基于象棋的环境中进行的实验证明了该方法的优势。

**结论:** 该方法能够有效处理复杂环境中的并发事件交互问题，为LTL指令下的多任务策略学习提供了更强大的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Instruction+Following+in+RL+via+Structured+LTL+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02633，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02633&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.

</details>


### [24] [Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks](https://arxiv.org/abs/2512.02677)
*Zhiyuan He*

**主要类别:** cs.AI

**AI概要:** 论文发现标准Transformer架构在处理训练时未见过的深层递归问题时表现不佳，提出了一种循环定位替换管道方法来解决深度泛化问题。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在处理递归推理问题（需要解决嵌套层次结构的问题）时面临深度泛化的挑战，即无法处理比训练时更深的嵌套层次。

**方法:** 开发了一种新颖的循环定位替换管道，使用两个专门模型：定位器识别可解子表达式，替换器评估这些组件并保持整体结构。在布尔代数、递归算术和命题逻辑三个领域进行了评估。

**结果:** 该方法有效缓解了在分布外递归深度测试时的性能衰减问题，证明了其在处理深层递归问题上的有效性。

**结论:** 标准Transformer架构存在深度泛化限制，而提出的循环定位替换方法为解决递归推理问题提供了有效途径，特别是在处理深层嵌套结构方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Depth+Generalization+in+Large+Language+Models+for+Solving+Recursive+Logic+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02677&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.

</details>


### [25] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha, Jeong Hun Yeo, Junil Won, Se Jin Park, Yong Man Ro*

**主要类别:** cs.AI

**AI概要:** MIGR框架通过模态重要性指导，改善多模态大语言模型的情感推理可靠性，减少推理漂移问题，在DFEW基准上显著提升情感一致性解释的比例。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法存在推理漂移问题：模型逐渐依赖自身生成的文本而非多模态证据，且解释过度受视觉主导的推理路径影响。

**方法:** 引入模态重要性机制识别情感主导模态，重组推理序列从关键模态开始；采用两阶段框架：模态对齐监督微调和模态感知奖励优化。

**结果:** 在DFEW基准上，将正确预测但情感不一致解释的比例从18.10%降至7.37%。

**结论:** 从情感主导模态开始推理能显著提升多模态情感理解的推理可靠性，证明该方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+What+to+Attend+First%3A+Modality-Importance-Guided+Reasoning+for+Reliable+Multimodal+Emotion+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02699&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [26] [Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs](https://arxiv.org/abs/2512.02713)
*Theodoros Aivalis, Iraklis A. Klampanos, Antonis Troumpoukis, Joemon M. Jose*

**主要类别:** cs.AI

**AI概要:** 提出基于多模态大语言模型的框架，通过自动构建本体对齐知识图谱来分析生成模型输出的训练数据来源，支持版权分析、数据集透明度和可解释AI。


<details>
  <summary>更多</summary>
  
**动机:** 随着生成模型能力增强，对透明度、问责制和版权侵权的担忧加剧，需要理解训练数据如何影响模型输出。

**方法:** 利用多模态大语言模型从图像中提取结构化三元组，构建与领域本体对齐的知识图谱，通过比较生成图像和训练图像的KG来追踪潜在影响。

**结果:** 通过局部模型的unlearning实验和大型模型的风格特定实验验证了方法的有效性。

**结论:** 该框架支持开发促进人类协作、创造力和激发好奇心的AI系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+Data+Attribution+for+Image+Generation+using+Ontology-Aligned+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02713，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02713&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.

</details>


### [27] [Menta: A Small Language Model for On-Device Mental Health Prediction](https://arxiv.org/abs/2512.02716)
*Tianyi Zhang, Xiangyuan Xue, Lingyan Ruan, Shiya Fu, Feng Xia, Simon D'Alfonso, Vassilis Kostakos, Hong Jia*

**主要类别:** cs.AI

**AI概要:** Menta是一个专门针对社交媒体心理健康预测优化的轻量级小语言模型，通过多任务学习在抑郁、压力和自杀倾向等任务上表现优异，比非微调SLM平均提升15.2%，甚至在某些任务上超越大型语言模型，并能实现手机端实时部署。


<details>
  <summary>更多</summary>
  
**动机:** 全球数亿人受心理健康问题影响，但早期检测能力有限。大型语言模型虽在心理健康应用中有潜力，但其规模和计算需求阻碍实际部署。小语言模型提供了轻量级替代方案，但在基于社交媒体的心理健康预测方面尚未充分探索。

**方法:** 开发了Menta模型，使用基于LoRA的框架、跨数据集策略和平衡准确率导向的损失函数，在六个分类任务上联合训练，专门针对社交媒体数据进行多任务心理健康预测优化。

**结果:** Menta在抑郁、压力和自杀倾向等任务上比表现最佳的非微调SLM平均提升15.2%，在抑郁和压力分类任务上准确率甚至超过130亿参数的大型语言模型，同时模型大小缩小约3.25倍。能在iPhone 15 Pro Max上实时部署，仅需约3GB内存。

**结论:** Menta展示了可扩展、保护隐私的心理健康监测潜力，为实际部署提供了可行的轻量级解决方案，证明了小语言模型在专业领域应用中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Menta%3A+A+Small+Language+Model+for+On-Device+Mental+Health+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02716&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/

</details>


### [28] [StockMem: An Event-Reflection Memory Framework for Stock Forecasting](https://arxiv.org/abs/2512.02720)
*He Wang, Wenyilin Xiao, Songqiao Han, Hailiang Huang*

**主要类别:** cs.AI

**AI概要:** StockMem是一个事件-反思双层记忆框架，通过结构化新闻事件、横向整合和纵向追踪来提取市场预期差异信息，构建时间事件知识库和因果经验反思知识库，用于股票价格预测。


<details>
  <summary>更多</summary>
  
**动机:** 股票价格预测面临市场波动性和实时事件敏感性的挑战，现有大型语言模型在金融领域应用受到噪声新闻数据和文本中缺乏明确答案的限制，通用记忆架构难以识别价格变动的关键驱动因素。

**方法:** 提出StockMem框架：将新闻结构化处理成事件，通过横向整合每日事件和纵向追踪事件演变来挖掘增量信息，构建时间事件知识库；通过分析事件-价格动态形成因果经验反思知识库；在预测时检索类似历史场景并结合当前事件、增量数据和过去经验进行推理。

**结果:** 实验表明StockMem优于现有记忆架构，提供更优越且可解释的推理能力，能够追踪影响价格的信息链，增强金融预测的决策透明度。

**结论:** StockMem通过双层记忆框架有效解决了金融文本数据中的噪声和缺乏明确答案的问题，为基于文本的股票预测提供了更准确和可解释的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StockMem%3A+An+Event-Reflection+Memory+Framework+for+Stock+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02720&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.

</details>


### [29] [AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping](https://arxiv.org/abs/2512.02726)
*Md Abdul Kadir, Sai Suresh Macharla Vasu, Sidharth S. Nair, Daniel Sonntag*

**主要类别:** cs.AI

**AI概要:** LLM在审计日记账异常检测中表现优于传统规则方法和机器学习基线，提供自然语言解释，支持AI增强审计


<details>
  <summary>更多</summary>
  
**动机:** 传统基于规则的日记账测试方法产生大量误报且难以检测细微异常，需要更有效的异常检测方法

**方法:** 在合成和真实匿名账本上对LLaMA、Gemma等最先进LLM进行基准测试，与传统JET和机器学习基线方法比较

**结果:** LLM持续优于传统规则基JET和经典ML基线方法

**结论:** LLM展示了AI增强审计的潜力，人类审计师可与基础模型协作加强财务完整性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AuditCopilot%3A+Leveraging+LLMs+for+Fraud+Detection+in+Double-Entry+Bookkeeping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02726&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.

</details>


### [30] [Self-Improving AI Agents through Self-Play](https://arxiv.org/abs/2512.02731)
*Przemyslaw Chojecki*

**主要类别:** cs.AI

**AI概要:** 该论文将心理测量学的模数理论框架扩展到动力系统领域，提出了基于计算资源参数化的代理流模型和GVU算子，推导了保证自改进稳定性的方差不等式条件，并统一分析了多种自改进架构。


<details>
  <summary>更多</summary>
  
**动机:** 将静态的AAI能力评分扩展到动态系统，建立代理在计算资源参数化下的自改进过程的理论框架，以统一理解各种自改进方法。

**方法:** 提出Generator-Verifier-Updater (GVU)算子，证明其在参数流形上生成向量场，定义自改进系数κ为能力泛函沿该流的李导数，推导方差不等式作为自改进稳定性的谱条件。

**结果:** 证明了当生成和验证的组合噪声足够小时κ>0，将STaR、SPIN、Reflexion、GANs和AlphaZero等架构统一为满足方差不等式的GVU算子的具体拓扑实现。

**结论:** 建立了一个统一的动力系统理论框架来解释和连接各种自改进方法，为理解和设计稳定的自改进系统提供了理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Improving+AI+Agents+through+Self-Play，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02731，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02731&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.

</details>


### [31] [A Framework for Causal Concept-based Model Explanations](https://arxiv.org/abs/2512.02735)
*Anna Rodum Bjøru, Jacob Lysnæs-Larsen, Oskar Jørgensen, Inga Strümke, Helge Langseth*

**主要类别:** cs.AI

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Framework+for+Causal+Concept-based+Model+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02735，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02735&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.

</details>


### [32] [Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents](https://arxiv.org/abs/2512.02812)
*Zijie Lin, Qilin Cai, Liang Shen, Mingjun Xiao*

**主要类别:** cs.AI

**AI概要:** 提出了一种无需人工设计提示词的协作代理框架，通过验证和精炼代理自动提升论文到代码生成的质量，相比基线方法在准确性和完整性上分别提升约15%和13%。


<details>
  <summary>更多</summary>
  
**动机:** 现有论文复现框架缺乏逐步骤验证机制或过度依赖人工设计的提示词进行自我精炼，限制了适应性和可扩展性。

**方法:** 使用两个协作代理：验证代理检查每个步骤输出是否满足系统提示要求，精炼代理根据发现问题修订输出。仅利用原始系统提示实现自动验证和改进，无需人工设计精炼提示词。

**结果:** 在PaperBench Code-Dev和Paper2CodeBench数据集上实验显示，该方法显著提高复现代码的准确性和完整性，相比无代理基线分别提升约15%和13%，且在不同数据集上表现稳健一致。

**结论:** 提出的提示词无关协作代理框架有效解决了现有方法的局限性，实现了高质量、可扩展的自动化论文到代码生成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Automated+Paper+Reproduction+via+Prompt-Free+Collaborative+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02812，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02812&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\% and 13\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.

</details>


### [33] [Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control](https://arxiv.org/abs/2512.02814)
*Yongrui Yu, Zhongzhen Huang, Linjie Mu, Shaoting Zhang, Xiaofan Zhang*

**主要类别:** cs.AI

**AI概要:** Radiologist Copilot是一个基于大语言模型的智能助手，通过协调多个工具实现放射学报告的自动生成和质量控制，显著超越现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 放射学报告撰写耗时且易出错，现有自动化方法只关注报告生成而忽略质量控制，无法为放射科医生提供全面支持。

**方法:** 利用大语言模型作为推理核心，自主选择工具、制定计划并执行动作，包括区域定位、图像引导的区域分析规划、报告模板选择和基于反馈的自适应精炼等工具。

**结果:** 实验结果表明，Radiologist Copilot在放射学报告任务中显著优于其他最先进的方法。

**结论:** 该系统能够促进准确、完整和高效的放射学报告撰写，协助放射科医生并提高临床效率，源代码将在接受后发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Radiologist+Copilot%3A+An+Agentic+Assistant+with+Orchestrated+Tools+for+Radiology+Reporting+with+Quality+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02814&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.

</details>


### [34] [The future of AI in critical mineral exploration](https://arxiv.org/abs/2512.02879)
*Jef Caers*

**主要类别:** cs.AI

**AI概要:** 提出基于贝叶斯主义和证伪主义的新科学方法，利用AI减少矿产勘探中的认知偏差和假阳性，通过无监督学习和人机协作算法优化数据采集决策


<details>
  <summary>更多</summary>
  
**动机:** 全球能源转型推动关键矿产需求增长，但过去20年新发现减少，需要解决勘探成本高、认知偏差和假阳性问题

**方法:** 基于贝叶斯主义和证伪主义的哲学框架，开发新科学方法：1）无监督学习与领域专家协作生成地质假说；2）人机循环AI算法优化地质、地球物理、地球化学和钻探数据采集规划

**结果:** 提出实用的勘探协议模板，通过量化指标和理性决策指导数据采集，优先减少地质假说不确定性而非品位和储量不确定性

**结论:** AI驱动的科学方法能够显著降低勘探成本，提高发现效率，为关键矿产勘探提供新的方法论框架

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+future+of+AI+in+critical+mineral+exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02879，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02879&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage

</details>


### [35] [Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning](https://arxiv.org/abs/2512.02914)
*Zhonghao He, Tianyi Qiu, Hirokazu Shirado, Maarten Sap*

**主要类别:** cs.AI

**AI概要:** 该研究提出了基于贝叶斯统计中鞅性质的无监督Martingale评分，用于检测大语言模型推理中的信念固化现象，发现在事件预测、价值判断和学术论文评审等多个领域中普遍存在信念固化问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型的推理能力有所提升，但迭代推理可能导致信念固化和确认偏误，而非增强求真行为，需要系统评估框架来检测这一问题。

**方法:** 利用贝叶斯统计中的鞅性质，提出无监督、基于回归的Martingale评分来衡量模型违反理性信念更新的程度，通过分析当前信念对未来信念更新的预测能力来检测信念固化。

**结果:** 研究发现信念固化现象在各类模型和设置中普遍存在，当前信念正向预测未来信念更新；Martingale评分能够预测有标签问题域中的真实准确性，表明其可作为推理过程求真能力的有效代理指标。

**结论:** Martingale评分为评估大语言模型推理中的信念固化提供了有效的无监督度量方法，有助于识别容易产生信念固化的模型、推理技术和领域，对提升模型求真能力具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Martingale+Score%3A+An+Unsupervised+Metric+for+Bayesian+Rationality+in+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02914，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02914&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.

</details>


### [36] [Invasive Context Engineering to Control Large Language Models](https://arxiv.org/abs/2512.03001)
*Thomas Rivasseau*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种称为"侵入式上下文工程"的技术，通过在LLM上下文插入控制句子来增强模型安全性，特别是在长上下文场景下防止越狱和滥用。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM在长上下文情况下容易受到对抗攻击和滥用，越狱概率随上下文长度增加而上升，需要更强大的安全保证机制。

**方法:** 使用侵入式上下文工程技术，在LLM的输入上下文中插入控制句子，该方法可推广到思维链过程以防止计划性恶意行为，且不依赖模型训练。

**结果:** 该方法能够部分解决LLM在长上下文中的安全问题，避免了因训练数据不足导致的pitfalls。

**结论:** 侵入式上下文工程提供了一种不依赖训练的有效方法，为LLM在长上下文场景下的安全性提供了新的解决方案思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Invasive+Context+Engineering+to+Control+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03001，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03001&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.

</details>


### [37] [From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?](https://arxiv.org/abs/2512.03005)
*Dawei Li, Abdullah Alnaibari, Arslan Bisharat, Manny Sandoval, Deborah Hall, Yasin Silva, Huan Liu*

**主要类别:** cs.AI

**AI概要:** 该研究探索大语言模型能否作为在线冲突调解员，而不仅仅是内容审核者。通过将调解分解为判断和引导两个子任务，构建评估框架，发现API模型在调解效果上优于开源模型。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型越来越多地介入在线交流，研究其能否培养同理心和促进建设性对话，成为负责任AI研究的重要前沿。

**方法:** 提出将调解分解为判断（评估对话公平性和情感动态）和引导（生成同理心信息化解冲突）两个子任务，构建基于Reddit的大型数据集，采用基于原则的评分、用户模拟和人工比较的多阶段评估流程。

**结果:** 实验表明，基于API的模型在调解推理和干预对齐方面均优于开源模型。

**结论:** 当前大语言模型作为在线社交调解新兴代理既有潜力也存在局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Moderation+to+Mediation%3A+Can+LLMs+Serve+as+Mediators+in+Online+Flame+Wars%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03005&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [38] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang, Mouxiao Bian, Peiling Li, Bingjian Wen, Ruiyao Chen, Kangkun Mao, Xiaojun Ye, Tianbin Li, Pengcheng Chen, Bing Han, Jie Xu, Kaifeng Qiu, Junyan Wu*

**主要类别:** cs.CL

**AI概要:** RxBench是一个针对处方审核的全面基准测试，包含多种题型和处方错误类型，用于评估大语言模型在临床决策支持中的表现。研究发现顶尖模型在某些任务上可媲美甚至超越人类药师，并通过针对性微调提升了中等模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型在临床决策支持中的快速应用，需要建立系统化、细粒度的评估标准来检验其在处方审核中的能力，特别是针对常见的处方错误类型。

**方法:** 开发了RxBench基准测试，包含1150道单选题、230道多选题和879道简答题，涵盖14种常见处方错误类型。对18个先进大语言模型进行测试，并与执业药师表现进行比较，还针对中等模型进行了针对性微调。

**结果:** 发现模型性能存在明显分层，Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528表现最佳。领先模型在某些任务上可匹配或超越人类药师表现。通过微调中等模型，使其在简答题任务上达到领先通用模型的水平。

**结论:** RxBench建立了标准化的错误类型导向评估框架，不仅揭示了前沿大语言模型在处方审核中的能力和局限，还为构建更可靠的专业化临床工具提供了基础资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human-Level+and+Beyond%3A+Benchmarking+Large+Language+Models+Against+Clinical+Pharmacists+in+Prescription+Review，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02024&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [39] [Deep Research: A Systematic Survey](https://arxiv.org/abs/2512.02038)
*Zhengliang Shi, Yiqun Chen, Haitao Li, Weiwei Sun, Shiyu Ni, Yougang Lyu, Run-Ze Fan, Bowen Jin, Yixuan Weng, Minjun Zhu, Qiujie Xie, Xinyu Guo, Qu Yang, Jiayi Wu, Jujia Zhao, Xiaqiang Tang, Xinbei Ma, Cunxiang Wang, Jiaxin Mao, Qingyao Ai, Jen-Tse Huang, Wenxuan Wang, Yue Zhang, Yiming Yang, Zhaopeng Tu, Zhaochun Ren*

**主要类别:** cs.CL

**AI概要:** 这篇综述论文系统性地总结了深度研究系统的发展路线图、核心组件、优化技术和挑战，为LLMs作为研究代理完成复杂任务提供了全面的理论框架和实践指导。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型虽已发展为强大的问题解决工具，但许多开放任务需要批判性思维、多源信息和可验证输出，这超出了单次提示或标准检索增强生成的能力范围。

**方法:** 通过三阶段路线图形式化深度研究范式，提出四个核心组件（查询规划、信息获取、内存管理、答案生成）及其细分子分类，总结提示工程、监督微调和智能体强化学习等优化技术。

**结果:** 建立了深度研究系统的系统化分类框架，提供了清晰的实现路径和技术方法，并整合了评估标准和开放挑战。

**结论:** 深度研究领域正在快速发展，该综述将持续更新以反映最新进展，为未来研究提供指导和便利。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Research%3A+A+Systematic+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02038&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.

</details>


### [40] [Mirror, Mirror on the Wall -- Which is the Best Model of Them All?](https://arxiv.org/abs/2512.02043)
*Dina Sayed, Heiko Schuldt*

**主要类别:** cs.CL

**AI概要:** 该论文分析了LLM模型选择的定量评估维度，通过医疗领域案例研究探索排行榜和基准测试，提出了一个系统的模型选择方法论(MSM)来指导最适合特定用例的模型选择。


<details>
  <summary>更多</summary>
  
**动机:** 随着基础模型快速发展，为特定用例选择最合适模型的过程变得越来越复杂，需要同时考虑定性维度(模型适用性)和定量维度(性能表现)。

**方法:** 通过分析当前LLM排行榜和基准测试，以医疗领域为案例研究，探索定量评估维度的演变、现状和实际意义。

**结果:** 开发了一个模型选择方法论(MSM)，提供系统化方法来导航、优先级排序和选择与给定用例最匹配的模型。

**结论:** 定量评估维度对于LLM模型选择至关重要，提出的MSM方法能够系统化地指导用户选择最适合其特定需求的模型，特别是在医疗等专业领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mirror%2C+Mirror+on+the+Wall+--+Which+is+the+Best+Model+of+Them+All%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02043&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.

</details>


### [41] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen, Ziru Liu, Xijia Tao, Hui Liu, Xinyu Fu, Suiyun Zhang, Dandan Tu, Lingpeng Kong, Rui Liu, Haoliang Li*

**主要类别:** cs.CL

**AI概要:** 提出了Coherent Contextual Decoding (CCD)框架，通过轨迹修正机制和自适应采样策略，在扩散语言模型中实现了推理速度提升3.48倍和性能提升3.91%的双重优化


<details>
  <summary>更多</summary>
  
**动机:** 现有扩散语言模型的推理方法依赖局部即时指标（如置信度或熵），缺乏可靠视角，导致采样轨迹不一致和生成质量次优

**方法:** 1. 轨迹修正机制：利用历史上下文增强序列连贯性，早期拒绝次优路径；2. 自适应采样策略：根据一致性指标动态调整每个步骤的解码预算，替代传统的均匀分配方式

**结果:** 在Dream和LLaDA等多个基准测试中，同时实现了推理速度和性能的提升，最高达到3.48倍加速和3.91%性能改进

**结论:** CCD框架通过理论支持的轨迹一致性和自适应采样，有效解决了扩散语言模型推理中的连贯性和效率问题，为高质量文本生成提供了新方向

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Confidence%3A+Adaptive+and+Coherent+Decoding+for+Diffusion+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02044&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [42] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal, Moshe Eliasof, Javier Turek, Uri Ascher, Eran Treister, Eldad Haber*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种基于对称和辛微分方程启发的可逆架构，用于减少大语言模型训练时的内存消耗，通过时间可逆动力学在反向传播时重新计算隐藏状态，无需存储中间激活值，从而显著降低内存使用并提高吞吐量。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型的训练成本高昂且耗时，标准架构需要存储所有中间激活值，内存消耗巨大。为了在有限内存下处理更大批次数据并提高效率，需要开发内存效率更高的架构。

**方法:** 提出基于对称和辛微分方程的可逆架构，利用时间可逆动力学在反向传播时检索隐藏状态，无需存储激活值。同时开发了将现有非可逆LLM转换为可逆架构的高效微调方法。

**结果:** 在多个数据集和基准测试中，该方法在多个LLM上表现出可比或改进的性能，显著减少了内存消耗，允许在相同内存下处理更大批次数据，提高了吞吐量。

**结论:** 该研究为减少LLM从头训练和微调的内存与计算成本提供了一条可扩展且高效的路径，可逆架构在保持性能的同时大幅提升内存效率，具有实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reversing+Large+Language+Models+for+Efficient+Training+and+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02056，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02056&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [43] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin, Haris Gulzar, Monnika Roslianna Busto, Akiko Masaki, Takeharu Eda, Kazuhiro Nakadai*

**主要类别:** cs.CL

**AI概要:** 该论文探索了将内存高效微调(MEFT)方法应用于预训练语音模型，在方言识别任务中显著减少GPU内存使用(73.25%)并加速训练速度(2.1倍)，同时保持与全参数微调和参数高效微调相当的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 方言识别任务中，传统微调方法计算成本和内存需求高，而参数高效微调方法虽参数效率高但在内存效率和训练速度方面改进有限，需要更高效的微调方法。

**方法:** 将原本用于语言处理的内存高效微调(MEFT)方法应用于通用预训练语音模型，基于Whisper模型在KeSpeech数据集上进行六种普通话子方言识别案例研究，分析不同MEFT方法的GPU内存使用和微调速度。

**结果:** MEFT方法显著降低了GPU内存使用(最高减少73.25%)，训练速度提升2.1倍，同时保持了与全参数微调和参数高效微调方法相当的识别准确率。

**结论:** 内存高效微调方法在语音处理任务中具有显著优势，能够在不牺牲性能的情况下大幅降低计算资源需求，为资源受限环境下的语音模型微调提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dialect+Identification+Using+Resource-Efficient+Fine-Tuning+Approaches，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02074&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [44] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai, Tanay Kewalramani, Srimanta Mandal*

**主要类别:** cs.CL

**AI概要:** 提出一种基于TF-IDF样本选择的高效BERT微调方法，通过保留75%最具信息量的样本来减少训练数据量，同时通过添加领域特定词汇来增强对仇恨言论术语的识别能力。


<details>
  <summary>更多</summary>
  
**动机:** 社交媒体上的辱骂性言论持续演变，新俚语和混淆术语不断出现以规避检测系统，需要更高效和适应性的检测方法。

**方法:** 使用TF-IDF样本选择机制减少75%训练样本，同时增强BERT分词器以包含领域特定俚语和词汇变体。

**结果:** 在广泛使用的仇恨言论数据集上取得了有竞争力的性能，同时提高了计算效率。

**结论:** 该方法具有可扩展性和适应性潜力，能够有效应对不断演变的辱骂性内容审核挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature+Selection+Empowered+BERT+for+Detection+of+Hate+Speech+with+Vocabulary+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02141&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [45] [Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models](https://arxiv.org/abs/2512.02185)
*Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Guanchu Wang, Minwoo Lee, Shu-ping Yeh, Li Yang*

**主要类别:** cs.CL

**AI概要:** RESP是一种针对推理大语言模型的自反思结构化剪枝框架，通过自生成校准、仅解码梯度重要性估计和渐进再生技术，在保持高稀疏度的同时显著优于现有剪枝方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的剪枝方法在推理LLMs上表现不佳，即使是中等稀疏度（如20%）也会严重损害模型准确性和推理连贯性，需要一种能保持推理能力的剪枝方案。

**方法:** 提出RESP框架：使用模型自生成的推理轨迹作为校准信号，采用仅解码梯度重要性估计，以及渐进再生技术来保持校准保真度。

**结果:** 在Qwen3-8B上实验显示，RESP在20-30%稀疏度下保持接近密集模型的精度，在40%稀疏度下GSM8K准确率达81.3%，MathQA达59.6%，显著超越基线方法。

**结论:** RESP通过将剪枝决策与模型推理动态对齐，成功解决了推理LLMs剪枝的挑战，为资源受限环境下的高效推理模型部署提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+Before+You+Prune%3A+Self-Reflective+Structured+Pruning+for+Reasoning+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02185&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.

</details>


### [46] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri, Crit Cremers, Niels O. Schiller*

**主要类别:** cs.CL

**AI概要:** MODOMA是一个用于无监督语言习得实验的计算多智能体系统，通过成人和儿童智能体之间的交互实现语言习得，能够成功获取和表示语法范畴。


<details>
  <summary>更多</summary>
  
**动机:** 创建一个参数化的计算实验室环境，用于研究语言习得过程，使研究人员能够控制实验的各个方面并明确表示获得的语法知识。

**方法:** 使用统计和基于规则的程序，通过成人智能体生成训练和测试数据，儿童智能体基于这些数据进行语言习得，形成基于知识的语言模型。

**结果:** 实验表明儿童智能体能够成功获取功能和内容语法范畴，机器生成的数据显示出与人类数据相似的模式，验证了MODOMA方法的有效性。

**结论:** MODOMA系统为计算语言习得实验提供了新的可能性，成功模拟了语言获得过程，证明了该框架在建模语言习得方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Knowledge-Based+Language+Model%3A+Deducing+Grammatical+Knowledge+in+a+Multi-Agent+Language+Acquisition+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02195，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02195&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [47] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee, Kayode Olaleye, Sitwala Mundia, Andinda Bakainga, Unarine Netshifhefhe, Mahmooda Milanzie, Tsholofelo Hope Mogale, Thapelo Sindane, Zainab Abdulrasaq, Kesego Mokgosi, Chijioke Okorie, Nia Zion Van Wyk, Graham Morrissey, Dale Dunbar, Francois Smit, Tsosheletso Chidi, Rooweither Mabuya, Andiswa Bukula, Respect Mlambo, Tebogo Macucwa, Idris Abdulmumin, and Seani Rananga*

**主要类别:** cs.CL

**AI概要:** Swivuriso是一个3000小时的多语言语音数据集，支持7种南非语言的自动语音识别技术开发和基准测试，涵盖农业、医疗和通用领域，填补了现有ASR数据集的空白。


<details>
  <summary>更多</summary>
  
**动机:** 解决南非语言ASR数据集严重不足的问题，支持非洲语言的语音技术发展，特别是在农业和医疗等关键领域的应用需求。

**方法:** 采用系统化的数据收集程序，包含设计原则和伦理考量，收集了3000小时的多语言语音数据，涵盖多个主题领域。

**结果:** 提供了在该数据集上训练/微调ASR模型的基线结果，并与相关语言的现有ASR数据集进行了性能比较。

**结论:** Swivuriso数据集为南非语言的ASR研究提供了重要资源，有助于推动非洲语言语音技术的发展和应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Swivuriso%3A+The+South+African+Next+Voices+Multilingual+Speech+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02201&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [48] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung, Nikolay Malkin, Mirella Lapata*

**主要类别:** cs.CL

**AI概要:** LiteReason是一种新的潜在推理方法，通过轻量级推理投影模块生成连续潜在token来跳过推理步骤，结合RL优化，在保持性能的同时显著减少77-92%的推理长度。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型通过生成长推理链处理复杂任务，但使用强化学习优化推理能力计算成本高昂，特别是在涉及大量token处理的叙事相关任务中。

**方法:** 提出LiteReason方法，包含轻量级推理投影模块生成连续潜在token，策略模型决定何时激活投影器在潜在和离散推理之间切换，可与标准token采样和RL技术结合。

**结果:** 在情节漏洞检测和书籍章节生成任务中，LiteReason优于潜在推理基线方法，接近非潜在RL训练的性能，同时将最终推理长度减少77-92%。

**结论:** LiteReason引导RL训练达到性能-计算权衡曲线上更高效的部分，为复杂推理任务提供了计算效率更高的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lightweight+Latent+Reasoning+for+Narrative+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02240，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02240&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [49] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

**主要类别:** cs.CL

**AI概要:** 论文提出了DETAIL框架，用于评估大语言模型在不同提示词具体性水平下的性能表现，发现提示词越具体准确率越高，尤其对小模型和过程性任务效果显著


<details>
  <summary>更多</summary>
  
**动机:** 提示词设计对LLM推理性能至关重要，但提示词具体性（详细程度）的影响尚未得到充分研究

**方法:** 使用GPT-4生成多级提示词，通过困惑度量化具体性，使用基于GPT的语义等价性评估正确性，在30个新颖推理任务上对GPT-4和O3-mini进行实验

**结果:** 具体性提高了准确率，特别是对较小模型和过程性任务效果更明显

**结论:** 研究结果强调了需要自适应提示策略，并提供了支持进一步研究的工具和数据

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DETAIL+Matters%3A+Measuring+the+Impact+of+Prompt+Specificity+on+Reasoning+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02246&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [50] [CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering](https://arxiv.org/abs/2512.02251)
*Liangji Kong, Aditya Joshi, Sarvnaz Karimi*

**主要类别:** cs.CL

**AI概要:** CAIRNS框架为农业气候适应提供可读性强、来源可信的问答系统，无需微调即可从网络证据源获取可靠答案，在多项指标上优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 气候变化背景下需要从结构化/非结构化数据中获取可信的气候适应策略信息，帮助农业专家(如农场顾问)获得可靠的初步答案。

**方法:** 开发CAIRNS框架，使用结构化ScholarGuide提示增强可读性和引用可靠性，通过一致性加权的混合评估器进行稳健评估，利用模型间一致性验证专家答案。

**结果:** CAIRNS在大多数指标上优于基线方法，彻底的消融研究在所有指标上确认了结果，LLM评估与人类判断具有相关性。

**结论:** CAIRNS框架能够在不进行微调或强化学习的情况下，提供可读、可验证且基于领域知识的问答能力，为气候适应策略的获取提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CAIRNS%3A+Balancing+Readability+and+Scientific+Accuracy+in+Climate+Adaptation+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02251&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.

</details>


### [51] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang, Alban Bornet, Rui Yang, Nan Liu, Douglas Teodoro*

**主要类别:** cs.CL

**AI概要:** 该研究通过HealthContradict数据集评估语言模型在矛盾生物医学上下文中的推理能力，发现生物医学语言模型不仅能利用预训练参数知识，还能有效利用正确上下文并抵抗错误信息。


<details>
  <summary>更多</summary>
  
**动机:** 探究语言模型如何使用上下文信息回答健康问题，特别是在面对矛盾上下文时的表现，以评估其上下文推理能力。

**方法:** 使用专家验证的HealthContradict数据集（920个实例），包含健康问题、科学证据支持的答案和两个矛盾立场的文档，测试不同提示设置下模型的表现。

**结果:** 相比现有医学问答基准，HealthContradict能更好区分语言模型的上下文推理能力；微调后的生物医学语言模型既能利用正确上下文，又能抵抗错误上下文。

**结论:** 生物医学语言模型的核心优势不仅在于预训练的参数知识，更在于其处理矛盾上下文的能力，这对医疗AI应用具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HealthContradict%3A+Evaluating+Biomedical+Knowledge+Conflicts+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02299，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02299&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [52] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu, Ryan Teehan, Jinran Jin, Mengye Ren*

**主要类别:** cs.CL

**AI概要:** 本研究系统分析了37个不同模型在9个基准测试中的验证器性能，发现跨模型族验证比自验证更有效，后训练虽然降低了自改进但增强了跨族改进，数学和逻辑任务具有最高的可验证性。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究对LLM作为问题解决器和验证器的交互作用研究有限，主要关注自验证，缺乏对不同模型族间验证效果以及后训练对验证能力影响的系统分析。

**方法:** 使用37个不同家族、规模和训练变体的模型，在9个基准测试上进行系统研究，比较自验证、同族验证和跨族验证效果，并引入验证器增益指标来预测拒绝采样带来的性能改进。

**结果:** 跨模型族验证特别有效；后训练减少了自改进但增强了跨族改进；数学和逻辑任务展现出最高的内在可验证性；验证器增益和假阳性率随模型规模和后训练而变化。

**结论:** 验证器选择是提升LLM性能的有效策略，特别是跨模型族验证，后训练对验证能力有重要影响，不同任务的可验证性存在显著差异，这为未来模型设计和应用提供了重要指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Does+Verification+Pay+Off%3F+A+Closer+Look+at+LLMs+as+Solution+Verifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02304&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [53] [Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering](https://arxiv.org/abs/2512.02363)
*Lei Fu, Xiang Chen, Kaige Gao Xinyue Huang, Kejian Tong*

**主要类别:** cs.CL

**AI概要:** KARMA框架通过双编码器架构融合结构化和非结构化知识源，使用门控记忆单元动态调节外部知识集成，以及安全感知可控解码器来提升服务领域问答系统的准确性和安全性


<details>
  <summary>更多</summary>
  
**动机:** 现有大语言模型在医疗政策、政府福利等敏感领域的问答系统中存在事实一致性和上下文对齐的困难，需要整合异构知识源同时确保准确性和安全性

**方法:** 提出KARMA框架，包含：1)双编码器架构融合结构化和非结构化知识；2)门控记忆单元动态调节外部知识集成；3)安全感知可控解码器，使用安全分类和引导生成技术减少不安全输出

**结果:** 在专有问答数据集上的大量实验表明，KARMA在答案质量和安全性方面均优于强基线模型

**结论:** 该研究为服务场景中构建可信赖和自适应的问答系统提供了全面解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory-Augmented+Knowledge+Fusion+with+Safety-Aware+Decoding+for+Domain-Adaptive+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02363，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02363&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.

</details>


### [54] [TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models](https://arxiv.org/abs/2512.02402)
*Yunchao Wang, Guodao Sun, Zihang Fu, Zhehao Liu, Kaixing Du, Haidong Gao, Ronghua Liang*

**主要类别:** cs.CL

**AI概要:** TaleFrame是一个结合大语言模型和人机交互的故事生成系统，通过结构化信息分解故事为实体、事件、关系和故事大纲四个基本单元，实现精确控制的故事生成。


<details>
  <summary>更多</summary>
  
**动机:** 当前自然语言生成系统缺乏细粒度控制能力，无法准确将用户意图转化为满意的故事输出，限制了其应用性。

**方法:** 使用Tinystories数据集构建9,851个JSON格式的偏好数据集，微调本地Llama模型，采用JSON2Story方法将结构化数据转换为连贯故事，并提供拖拽式交互界面。

**结果:** 系统能够从七个维度评估生成的故事质量并提供改进建议，用户可通过迭代调整获得满意结果。定量评估和用户研究证明了系统的有效性。

**结论:** TaleFrame通过结构化方法和交互式界面成功解决了故事生成的精确控制问题，为创意故事生成提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TaleFrame%3A+An+Interactive+Story+Generation+System+with+Fine-Grained+Control+and+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02402，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02402&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.

</details>


### [55] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar, Vivek Dhanawade, Rohit Yadav, Minal Sonkar, Medha Asurlekar, Sarita Rathod*

**主要类别:** cs.CL

**AI概要:** 本文对语言模型中的幻觉问题进行了简明概述，包括幻觉类型、成因和缓解方法


<details>
  <summary>更多</summary>
  
**动机:** 传统语言模型面临幻觉问题的挑战，这给自然语言处理领域带来风险，需要理解幻觉的各种类型、起源和减少方法

**方法:** 提供简洁直接的总结文档，作为理解幻觉问题和缓解方法的一站式资源

**结果:** 创建了一个全面的资源文档，系统性地介绍了幻觉问题的各个方面

**结论:** 该文档为理解和缓解语言模型幻觉问题提供了有价值的参考资源

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Concise+Review+of+Hallucinations+in+LLMs+and+their+Mitigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02527，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02527&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [56] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri, Chahan Vidal-Gorène, Florian Cafiero*

**主要类别:** cs.CL

**AI概要:** 本研究评估了假新闻检测和病毒性预测两个任务，发现文本内容在假新闻检测中表现优异，而轻量级数值特征在计算受限时仍可行。病毒性预测比假新闻检测更难，对标签构建高度敏感。


<details>
  <summary>更多</summary>
  
**动机:** 针对在线错误信息处理的实际需求，特别是在需要快速响应的操作环境中，评估不同方法在假新闻检测和病毒性预测任务上的表现。

**方法:** 使用EVONS和FakeNewsNet数据集，比较文本嵌入（RoBERTa、Mistral）、轻量级数值特征（时间、粉丝数、验证状态、点赞数）和序列模型（GRU、门控架构、Transformer编码器）。

**结果:** 文本内容是假新闻检测的强区分器；数值特征管道在语言模型不可用或计算受限时仍可行；病毒性预测比假新闻检测困难得多，对标签构建高度敏感；Mistral替换RoBERTa仅产生微小差异。

**结论:** 研究强调了评估设计的重要性，提供了指标选择指导，并指出了当前API限制下的可复现性约束，为实际应用中的错误信息检测提供了实用见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Signals+Really+Matter+for+Misinformation+Tasks%3F+Evaluating+Fake-News+Detection+and+Virality+Prediction+under+Real-World+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02552&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [57] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang, Donghao Xie, Ming Pang, Chunyuan Yuan, Xue Jiang, Changping Peng, Zhangang Lin, Zheng Luo*

**主要类别:** cs.CL

**AI概要:** ADORE是一个自维持的电商搜索相关性建模框架，通过规则感知相关性判别、错误类型感知数据合成和关键属性增强知识蒸馏三个创新模块，自动生成训练数据、对抗样本和可部署模型，解决了语义鸿沟和数据稀缺问题。


<details>
  <summary>更多</summary>
  
**动机:** 电商搜索中的相关性建模面临术语匹配方法的语义鸿沟问题，以及神经模型对领域特定困难样本稀缺的依赖。需要一种能够自动生成高质量训练数据并增强模型鲁棒性的方法。

**方法:** 1. 规则感知相关性判别模块：使用思维链LLM生成意图对齐的训练数据，通过Kahneman-Tversky优化与用户行为对齐
2. 错误类型感知数据合成模块：自动生成对抗样本来增强鲁棒性
3. 关键属性增强知识蒸馏模块：将领域特定属性层次注入可部署的学生模型

**结果:** 大规模实验和在线A/B测试验证了ADORE的有效性，框架在工业应用中建立了资源高效、认知对齐的相关性建模新范式。

**结论:** ADORE框架通过自动化标注、对抗生成和蒸馏过程，克服了数据稀缺问题，同时增强了推理能力，为工业应用中的相关性建模提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADORE%3A+Autonomous+Domain-Oriented+Relevance+Engine+for+E-commerce，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02555，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02555&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [58] [DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)
*DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenhao Xu, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Erhang Li, Fangqi Zhou, Fangyun Lin, Fucong Dai, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Li, Haofen Liang, Haoran Wei, Haowei Zhang, Haowen Luo, Haozhe Ji, Honghui Ding, Hongxuan Tang, Huanqi Cao, Huazuo Gao, Hui Qu, Hui Zeng, Jialiang Huang, Jiashi Li, Jiaxin Xu, Jiewen Hu, Jingchang Chen, Jingting Xiang, Jingyang Yuan, Jingyuan Cheng, Jinhua Zhu, Jun Ran, Junguang Jiang, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Kexin Huang, Kexing Zhou, Kezhao Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Wang, Liang Zhao, Liangsheng Yin, Lihua Guo, Lingxiao Luo, Linwang Ma, Litong Wang, Liyue Zhang, M. S. Di, M. Y Xu, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Panpan Huang, Peixin Cong, Peiyi Wang, Qiancheng Wang, Qihao Zhu, Qingyang Li, Qinyu Chen, Qiushi Du, Ruiling Xu, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runqiu Yin, Runxin Xu, Ruomeng Shen, Ruoyu Zhang, S. H. Liu, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaofei Cai, Shaoyuan Chen, Shengding Hu, Shengyu Liu, Shiqiang Hu, Shirong Ma, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, Songyang Zhou, Tao Ni, Tao Yun, Tian Pei, Tian Ye, Tianyuan Yue, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjie Pang, Wenjing Luo, Wenjun Gao, Wentao Zhang, Xi Gao, Xiangwen Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaokang Zhang, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xingyou Li, Xinyu Yang, Xinyuan Li, Xu Chen, Xuecheng Su, Xuehai Pan, Xuheng Lin, Xuwei Fu, Y. Q. Wang, Yang Zhang, Yanhong Xu, Yanru Ma, Yao Li, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Qian, Yi Yu, Yichao Zhang, Yifan Ding, Yifan Shi, Yiliang Xiong, Ying He, Ying Zhou, Yinmin Zhong, Yishi Piao, Yisong Wang, Yixiao Chen, Yixuan Tan, Yixuan Wei, Yiyang Ma, Yiyuan Liu, Yonglun Yang, Yongqiang Guo, Yongtong Wu, Yu Wu, Yuan Cheng, Yuan Ou, Yuanfan Xu, Yuduan Wang, Yue Gong, Yuhan Wu, Yuheng Zou, Yukun Li, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehua Zhao, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhixian Huang, Zhiyu Wu, Zhuoshu Li, Zhuping Zhang, Zian Xu, Zihao Wang, Zihui Gu, Zijia Zhu, Zilin Li, Zipeng Zhang, Ziwei Xie, Ziyi Gao, Zizheng Pan, Zongqing Yao, Bei Feng, Hui Li, J. L. Cai, Jiaqi Ni, Lei Xu, Meng Li, Ning Tian, R. J. Chen, R. L. Jin, S. S. Li, Shuang Zhou, Tianyu Sun, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xinnan Song, Xinyi Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Dongjie Ji, Jian Liang, Jianzhong Guo, Jin Chen, Leyi Xia, Miaojun Wang, Mingming Li, Peng Zhang, Ruyi Chen, Shangmian Sun, Shaoqing Wu, Shengfeng Ye, T. Wang, W. L. Xiao, Wei An, Xianzu Wang, Xiaowen Sun, Xiaoxiang Wang, Ying Tang, Yukun Zha, Zekai Zhang, Zhe Ju, Zhen Zhang, Zihua Qu*

**主要类别:** cs.CL

**AI概要:** DeepSeek-V3.2是一个高效能AI模型，通过稀疏注意力机制、强化学习框架和智能体任务合成流水线三大技术突破，在计算效率和推理能力上实现显著提升，性能媲美GPT-5和Gemini-3.0-Pro。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决大模型在长上下文场景中的计算复杂度问题，同时提升推理能力和智能体性能，实现高效能与强推理的平衡。

**方法:** 1. DeepSeek稀疏注意力(DSA)：降低计算复杂度但保持长上下文性能
2. 可扩展强化学习框架：通过强化学习协议和扩展后训练计算
3. 大规模智能体任务合成流水线：系统生成训练数据支持智能体后训练

**结果:** DeepSeek-V3.2性能与GPT-5相当，其高计算变体DeepSeek-V3.2-Speciale超越GPT-5，推理能力与Gemini-3.0-Pro相当，在2025年IMO和IOI竞赛中均获得金牌表现。

**结论:** 该模型通过创新的技术方法成功实现了计算效率与推理性能的优化平衡，在复杂交互环境中展现出强大的泛化能力和指令跟随鲁棒性，为AI模型的发展提供了重要技术路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepSeek-V3.2%3A+Pushing+the+Frontier+of+Open+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02556&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.

</details>


### [59] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang, Jinyang Wu, Yuchen Liu, Shuai Zhang, Yang Li, Qiliang Liang, Hongzhen Wang, Shuai Nie, Jiaming Xu, Runyu Shi, Ying Huang, Guoquan Zhang*

**主要类别:** cs.CL

**AI概要:** CAPO方法通过课程学习机制，先使用正向优势信号进行模仿学习建立基础，再引入负向信号培养判别能力，提升了语言模型在数学推理和多模态GUI推理任务中的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有强化学习方法在训练早期同时使用正向和负向优势信号，可能导致模糊的指导和有限的性能提升。

**方法:** 提出CAPO课程优势策略优化方法，基于优势信号的自适应课程机制：先仅使用正向优势样本进行模仿学习建立稳健基础，然后引入负向信号培养判别能力。

**结果:** 在数学推理任务中实现稳定显著改进，并能有效泛化到多模态GUI推理场景，与GRPO、PPO、RLOO和Reinforce++等多种优化方法兼容。

**结论:** CAPO是一个通用且稳健的优化框架，通过分阶段的课程学习方法有效提升了语言模型的推理能力和泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Imitation+to+Discrimination%3A+Toward+A+Generalized+Curriculum+Advantage+Mechanism+Enhancing+Cross-Domain+Reasoning+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02580，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02580&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [60] [Spoken Conversational Agents with Large Language Models](https://arxiv.org/abs/2512.02593)
*Chao-Han Huck Yang, Andreas Stolcke, Larry Heck*

**主要类别:** cs.CL

**AI概要:** 本教程系统梳理了语音对话代理从级联ASR/NLU架构向端到端、多模态系统演进的技术路线，重点介绍了文本LLM的音频适配、跨模态对齐和联合训练方法，并对比了不同架构设计的优劣。


<details>
  <summary>更多</summary>
  
**动机:** 随着语音对话代理向语音原生LLM发展，需要系统总结从传统级联架构到端到端多模态系统的技术演进路径，为研究者和从业者提供清晰的路线图和实践指南。

**方法:** 通过框架分析文本LLM的音频适配技术、跨模态对齐方法和联合语音-文本训练策略；系统回顾数据集、评估指标和口音鲁棒性；对比级联与端到端架构、后ASR校正和流式处理等设计选择。

**结果:** 建立了从工业助手到当前开放域和任务导向代理的连接，提供了可复现的基线系统，并识别了隐私、安全和评估等方面的开放性问题。

**结论:** 该教程为参与者提供了实用的技术方案和清晰的系统级路线图，为语音对话代理的进一步发展指明了方向，特别是在多模态融合和实际部署方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spoken+Conversational+Agents+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02593&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.

</details>


### [61] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

**主要类别:** cs.CL

**AI概要:** 研究发现LLM在生成多文档摘要时存在显著的首因效应，即更倾向于与第一个看到的文档保持语义一致，这可能影响AI系统决策的公平性。


<details>
  <summary>更多</summary>
  
**动机:** 探究大型语言模型在处理多个长文档时是否对所有输入给予同等权重，特别是在敏感话题如堕胎相关新闻中。

**方法:** 构建40组支持-中立-反对立场的文章三元组，每种顺序排列6种输入顺序，使用Gemini 2.5 Flash生成中立概述，通过ROUGE-L、BERTScore和SummaC评估摘要质量。

**结果:** 单因素方差分析显示BERTScore在所有立场上都存在显著的首因效应，成对比较表明位置1与位置2、3有显著差异，确认了对第一个文档的选择性偏好。

**结论:** LLM生成概述的首因效应对依赖此类概述的应用和自主AI系统构成风险，可能不成比例地影响下游决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Input+Order+Shapes+LLM+Semantic+Alignment+in+Multi-Document+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02665，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02665&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [62] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji, Tatsuhiko Saito, Yasutomo Kimura*

**主要类别:** cs.CL

**AI概要:** 本文实证比较了7种模型合并算法在减少大语言模型社会偏见方面的效果，发现存在偏见减少与下游性能之间的权衡关系，其中SLERP方法在中等插值权重下表现最为平衡。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型会继承和放大预训练语料中的社会偏见，威胁公平性和社会信任。虽然已有研究探索通过编辑模型参数来缓解偏见，但缺乏实证比较。

**方法:** 使用7种算法（Linear、Karcher Mean、SLERP、NuSLERP、TIES、DELLA、Nearswap）对GPT、LLaMA和Qwen家族的13个开源权重模型进行实证研究，使用3个偏见数据集（BBQ、BOLD、HONEST）进行评估，并测量这些技术对SuperGLUE基准下游任务性能的影响。

**结果:** 发现偏见减少与下游性能之间存在权衡关系：实现更大偏见缓解的方法会降低准确性，特别是在需要阅读理解、常识和因果推理的任务上。Linear、SLERP和Nearswap算法在保持整体性能的同时持续减少偏见。

**结论:** 模型合并算法在偏见缓解方面具有潜力，但过度的去偏见或不适当的合并方法可能导致重要语言能力的退化。SLERP在中等插值权重下是最平衡的选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Empirical+Survey+of+Model+Merging+Algorithms+for+Social+Bias+Mitigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02689&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [63] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal, Naman Mishra*

**主要类别:** cs.CL

**AI概要:** CREST是一个参数高效的跨语言安全分类模型，通过仅训练13种高资源语言，就能有效支持100种语言的安全检测，在有限参数下超越同类模型性能


<details>
  <summary>更多</summary>
  
**动机:** 现有安全防护主要针对高资源语言，低资源语言用户缺乏有效保护，需要开发能够服务全球多语言用户的安全系统

**方法:** 采用基于聚类的跨语言迁移方法，通过精心选择的13种高资源语言训练集，实现从少数语言到100种语言的有效泛化

**结果:** 在六个安全基准测试中，CREST以仅0.5B参数超越了同等规模的最先进防护模型，并与参数量大得多的模型(2.5B+)竞争

**结论:** 语言特定的防护系统存在局限性，需要开发通用的、语言无关的安全系统来有效服务全球用户群体

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CREST%3A+Universal+Safety+Guardrails+Through+Cluster-Guided+Cross-Lingual+Transfer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02711，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02711&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [64] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma, Jun Wang, Zafeirios Fountas*

**主要类别:** cs.CL

**AI概要:** LLMs在显式推理方面表现出色，但其隐式计算策略研究不足。本研究通过心理物理学范式，发现LLMs能够进行接近最优的贝叶斯多模态信息整合，但准确率高的模型不一定具备鲁棒的不确定性处理能力。


<details>
  <summary>更多</summary>
  
**动机:** 探索LLMs是否像人类一样能够直觉性地使用接近最优的贝叶斯策略来处理和整合噪声信号，而无需显式训练或指令。

**方法:** 采用心理物理学范式，设计了BayesBench行为基准测试（包含长度、位置、距离和持续时间四个估计任务），评估9种不同LLM的多模态线索整合性能，通过控制噪声、上下文和指令提示进行系统行为研究。

**结果:** 研究发现虽然能力强的模型通常以贝叶斯一致的方式适应，但准确率不能保证鲁棒性。GPT-5 Mini在文本准确率上表现完美，但无法有效整合视觉线索，揭示了能力与策略之间的关键分离。

**结论:** 准确性中心的基准测试可能过度关注性能而忽略了脆弱的不确定性处理。研究揭示了LLMs对不确定性的原则性处理能力的涌现，并强调了准确性与贝叶斯倾向之间的相关性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergent+Bayesian+Behaviour+and+Optimal+Cue+Combination+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02719，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02719&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [65] [SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys](https://arxiv.org/abs/2512.02763)
*Jiahao Zhao, Shuaixing Zhang, Nan Xu, Lei Wang*

**主要类别:** cs.CL

**AI概要:** SurveyEval是一个用于评估自动生成调查系统的综合基准，从整体质量、大纲连贯性和参考文献准确性三个维度进行评估，通过扩展7个学科领域并增强LLM-as-a-Judge框架来提高评估与人类判断的一致性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于LLM的自动调查系统将检索、组织和内容合成集成到端到端生成流程中，但如何评估这种复杂系统仍然是一个重大挑战。

**方法:** 开发了SurveyEval基准，在7个不同学科上扩展评估，并通过加入人类参考来增强LLM-as-a-Judge评估框架，以加强评估与人类判断的一致性。

**结果:** 评估结果显示，通用长文本或论文写作系统倾向于生成质量较低的调查，而专门的调查生成系统能够提供显著更高质量的结果。

**结论:** SurveyEval可作为一个可扩展的测试平台，用于理解和改进跨不同学科和评估标准的自动调查系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SurveyEval%3A+Towards+Comprehensive+Evaluation+of+LLM-Generated+Academic+Surveys，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02763&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

</details>


### [66] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec, Ivan Srba, Maria Bielikova*

**主要类别:** cs.CL

**AI概要:** PEFT-Factory是一个统一的参数高效微调框架，提供19种PEFT方法、27个数据集和专用评估指标，改善PEFT方法的可复现性和基准测试。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型规模不断增大，许多新PEFT方法难以复现、部署和相互比较，需要统一的框架来解决这些问题。

**方法:** 开发PEFT-Factory框架，采用模块化设计支持扩展，原生提供19种PEFT方法、27个分类和文本生成数据集（涵盖12个任务），以及标准和PEFT专用评估指标。

**结果:** 创建了一个即用型、受控且稳定的环境，显著提高了PEFT方法的可复现性和基准测试能力。

**结论:** PEFT-Factory作为源自LLaMA-Factory的下游框架，成功解决了PEFT方法的统一实施和比较问题，为研究社区提供了实用的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PEFT-Factory%3A+Unified+Parameter-Efficient+Fine-Tuning+of+Autoregressive+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02764，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02764&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [67] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su, Jianming Long, Changyue Wang, Shiyu Lin, Jingyan Xu, Ziyi Ye, Qingyao Ai, Yiqun Liu*

**主要类别:** cs.CL

**AI概要:** UniFact框架统一了幻觉检测(HD)和事实验证(FV)两个研究范式，通过动态生成模型输出和事实标签实现直接比较，发现两者互补且混合方法性能最优。


<details>
  <summary>更多</summary>
  
**动机:** LLMs经常产生事实错误的幻觉内容，HD和FV虽目标相同但各自独立发展，存在研究隔阂阻碍共同进展。

**方法:** 提出UniFact统一评估框架，通过大规模实验比较多种LLM家族和检测方法，分析HD与FV的分歧原因。

**结果:** 三个关键发现：1)无单一最优范式；2)HD和FV捕捉互补的错误方面；3)混合方法达到最佳性能。

**结论:** 需要整合HD和FV的统一研究议程，实验证明两者结合的必要性，所有代码数据已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Unification+of+Hallucination+Detection+and+Fact+Verification+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02772，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02772&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [68] [Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension](https://arxiv.org/abs/2512.02791)
*Juexi Shao, Siyou Li, Yujian Gan, Chris Madge, Vanja Karan, Massimo Poesio*

**主要类别:** cs.CL

**AI概要:** 论文提出了一种三层数据合成方法来解决对话式广义指代表达理解(GREC)任务中的数据稀缺和分布偏移问题，通过在合成数据上微调获得了显著性能提升


<details>
  <summary>更多</summary>
  
**动机:** 现有系统在训练和评估领域之间存在分布偏移问题，且缺乏标注的对话接地数据，导致模型性能受限

**方法:** 采用三层数据合成方法，平衡真实性和可控性，为对话条件接地生成可扩展的监督信号

**结果:** 在合成数据上微调后，相比先前方法在标准评估指标上获得了持续且显著的改进

**结论:** 该数据合成方法有效解决了GREC任务中的数据稀缺和分布偏移挑战，为对话式视觉接地任务提供了可行的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Making+Dialogue+Grounding+Data+Rich%3A+A+Three-Tier+Data+Synthesis+Framework+for+Generalized+Referring+Expression+Comprehension，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02791&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.

</details>


### [69] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo, Hilton Vorster, Josh Warren, Trevor Naick, Deandre Vanmali, Masana Mashapha, Luke Brand, Alyssa Fernandes, Janco Calitz, Sibusiso Makhoba*

**主要类别:** cs.CL

**AI概要:** TriLex框架通过三阶段检索增强方法扩展低资源非洲语言情感词典，显著提升AfroXLMR和AfriBERTa模型在情感分析中的性能表现


<details>
  <summary>更多</summary>
  
**动机:** 低资源非洲语言在情感分析中代表性不足，限制了多语言NLP系统的词汇覆盖和性能

**方法:** 提出TriLex三阶段检索增强框架：基于语料库提取、跨语言映射和RAG驱动的词汇精炼，用于系统扩展低资源语言情感词典

**结果:** AfroXLMR在isiXhosa和isiZulu上F1分数超过80%，AfriBERTa达到约64%的F1分数，均优于传统机器学习基线，集成分析进一步提高了精确度和鲁棒性

**结论:** TriLex被证明是用于低资源南非语言多语言情感词典扩展和情感建模的可扩展且有效的框架

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TriLex%3A+A+Framework+for+Multilingual+Sentiment+Analysis+in+Low-Resource+South+African+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02799&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [70] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang, Yi Yang*

**主要类别:** cs.CL

**AI概要:** 提出stable rank作为无需人工标注的内在质量信号，通过测量隐藏状态的有效维度来评估LLM输出质量，并基于此开发了SR-GRPO强化学习方法，在多个任务上超越有监督方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统LLM对齐方法依赖外部监督存在局限：人工标注稀缺且主观，奖励模型易受奖励攻击，自评估方法存在提示敏感性和偏差问题。

**方法:** 提出stable rank方法，通过计算总方差与主导方向方差的比率来测量隐藏状态的有效维度；开发SR-GRPO方法，使用stable rank作为强化学习的奖励信号。

**结果:** stable rank在RewardBench上达到84.04%准确率，通过Best-of-N采样将任务准确率平均提高11.3%；SR-GRPO将Qwen2.5-1.5B-Instruct在STEM任务上提升10%，数学推理提升19%。

**结论:** 质量信号可以从模型内部几何结构中提取，为无需外部监督的可扩展对齐提供了新路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SR-GRPO%3A+Stable+Rank+as+an+Intrinsic+Geometric+Reward+for+Large+Language+Model+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02807&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [71] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li, Jianbin Guo, Zhaoyang Shang, Yiqing Liu, Hongmin Du, Lingling Liu, Yuping Zhao, Lifeng Dong*

**主要类别:** cs.CL

**AI概要:** 该研究提出了TCM-BEST4SDT基准，用于评估大语言模型在中医临床辨证论治能力，包含四个任务和三种评估机制，已在15个主流LLM上验证有效性。


<details>
  <summary>更多</summary>
  
**动机:** 中医领域的LLMs评估面临挑战，现有基准局限于知识问答和辨证准确性，缺乏治疗决策评估，需要更全面的临床案例基准。

**方法:** 构建基于临床案例的综合基准，采用专家主导的数据标注流程，包含四个任务（中医基础知识、医学伦理、内容安全、辨证论治）和三种评估机制（选择题评估、裁判模型评估、奖励模型评估）。

**结果:** 在15个主流LLM（通用和中医领域）上验证了TCM-BEST4SDT的有效性，基准已公开可用。

**结论:** TCM-BEST4SDT为中医智能化研究提供了全面的评估框架，能有效评估LLMs在中医临床辨证论治方面的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+benchmark+dataset+for+evaluating+Syndrome+Differentiation+and+Treatment+in+large+language+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02816，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02816&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [72] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru, Fabian Retkowski, Christian Huber, Lukas Hilgert, Seymanur Akti, Enes Yavuz Ugan, Alexander Waibel, Jan Niehues*

**主要类别:** cs.CL

**AI概要:** BOOM是一个多模态多语言讲座伴侣系统，能够联合翻译讲座音频和幻灯片，生成同步的文本翻译、本地化幻灯片和合成语音输出，为在线学习提供完整的多语言学习体验。


<details>
  <summary>更多</summary>
  
**动机:** 教育全球化和在线学习的快速增长使得教育内容本地化成为关键挑战。讲座材料本质上是多模态的，需要能够处理多种输入模态的系统，以提供完整的学习体验。

**方法:** 开发BOOM系统，采用端到端方法，联合处理讲座音频和幻灯片，实现跨三种模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片和合成语音。

**结果:** 实验证明，具有幻灯片感知的转录在下游任务（如摘要和问答）中带来级联效益。系统代码已开源发布。

**结论:** BOOM系统为学生提供了以母语访问讲座的能力，同时保持了原始内容的完整性，解决了多模态教育内容本地化的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BOOM%3A+Beyond+Only+One+Modality+KIT%27s+Multimodal+Multilingual+Lecture+Companion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02817，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02817&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [73] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle, Timo Heiß, Moritz Schlager, Matthias Aßenmacher, Matthias Feurer*

**主要类别:** cs.CL

**AI概要:** 论文提出了promptolution框架，这是一个统一的模块化开源框架，旨在解决提示优化研究代码分散且难以维护的问题，为实践者和研究者提供完整的提示优化组件。


<details>
  <summary>更多</summary>
  
**动机:** 虽然许多研究论文证明了提示优化的有效性，但实际应用受到阻碍，因为现有实现通常依赖于未维护和孤立的研究代码库。

**方法:** 开发了一个统一的模块化开源框架promptolution，集成了多种当代离散提示优化器，并保持与底层LLM实现的无关性。

**结果:** 创建了一个可扩展的系统，为实践者和研究者提供了在单一系统中进行提示优化所需的所有组件。

**结论:** promptolution框架解决了提示优化工具碎片化的问题，促进了提示优化技术的实际应用和研究发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是promptolution%3A+A+Unified%2C+Modular+Framework+for+Prompt+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02840，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02840&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [74] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang, Yusheng Zhou, Tolga Ergen, Lajanugen Logeswaran, Moontae Lee, David Jurgens*

**主要类别:** cs.CL

**AI概要:** 本研究提出了一个四维评估框架来评估多语言环境中的系统提示，发现特定提示组件与稳健的多语言行为相关，并开发了能自动提升各项指标5-10%的提示优化框架。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界部署需要单一提示能在不同语言中可靠工作，而现有研究主要关注英语环境，缺乏对多语言系统提示的全面研究。

**方法:** 使用五门语言、三个大语言模型和三个基准进行大规模实验，分析超过1000万个推理单元，开发多语言提示优化框架。

**结果:** 发现CoT、情感和场景等提示组件与稳健多语言行为相关，优化框架能自动发现提升所有指标5-10%的提示，更优提示能产生更结构化、一致的推理模式并减少不必要的语言切换。

**结论:** 系统提示优化是实现准确和稳健多语言LLM行为的可扩展路径，特定提示设计能显著改善跨语言性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Lingual+Prompt+Steerability%3A+Towards+Accurate+and+Robust+LLM+Behavior+across+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02841&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [75] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari, Krishno Dey*

**主要类别:** cs.CL

**AI概要:** 该研究分析了孟加拉语仇恨语音检测问题，比较了传统机器学习方法和基于Transformer的模型性能，发现语言特定的预训练模型BanglaBERT表现最佳，强调了预训练模型对低资源语言的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 孟加拉语作为拥有2.3亿使用者的语言，在计算资源方面严重不足，特别是在社交媒体自动化审核方面存在巨大需求，但缺乏足够的仇恨语音检测资源。

**方法:** 研究复现了官方基线方法（多数投票、随机、SVM），并增加了逻辑回归、随机森林和决策树作为基线。同时使用了基于Transformer的模型包括DistilBERT、BanglaBERT、m-BERT和XLM-RoBERTa进行仇恨语音分类。

**结果:** 所有基于Transformer的模型（除DistilBERT外）在子任务中都优于基线方法。BanglaBERT在两个子任务中表现最佳，尽管模型规模较小，但性能超过了m-BERT和XLM-RoBERTa。

**结论:** 语言特定的预训练对低资源语言非常重要，研究结果突显了预训练语言模型在孟加拉语等低资源语言中的潜力和必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bangla+Hate+Speech+Classification+with+Fine-tuned+Transformer+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02845&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Hate speech recognition in low-resource languages remains a difficult problem due to insufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the growing need for automated moderation on social media platforms, Bangla is significantly under-represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official baselines (e.g., Majority, Random, Support Vector Machine) and also produce and consider Logistic Regression, Random Forest, and Decision Tree as baseline methods. We also utilized transformer-based models such as DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa for hate speech classification. All the transformer-based models outperformed baseline methods for the subtasks, except for DistilBERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language-specific pre-training is very important. Our results highlight the potential and need for pre-trained language models for the low-resource Bangla language.

</details>


### [76] [Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning](https://arxiv.org/abs/2512.02874)
*Haonan Wang, Chao Du, Kenji Kawaguchi, Tianyu Pang*

**主要类别:** cs.CL

**AI概要:** ThinkMerge是一种无需训练的即插即用解码策略，通过并行推理轨迹的token级logits平均来提升开放端推理任务性能，在代码生成和网络深度研究任务中表现优异


<details>
  <summary>更多</summary>
  
**动机:** 多数投票法在封闭端问答中有效，但不适用于开放端推理任务（如代码生成、网络研究），因为完整解决方案的'多数'概念难以定义

**方法:** 运行K个并行推理轨迹，在同步点平均它们的下一个token logits，生成单一连贯输出。与vLLM/SGLang无缝集成，兼容Top-p/Top-k等标准解码技术

**结果:** 在AIME和GPQA上匹配或超越多数投票法；LiveCodeBench（困难）上DeepCoder-14B-Preview提升8.28%，Qwen3-8B提升7.58%；在GAIA、BrowseComp-en/zh和XbenchDeepSearch上提升网络研究代理性能

**结论:** 并行测试时扩展可以在不依赖完整输出投票的情况下有益于开放端推理，ThinkMerge为开放端任务提供了一种有效的解码策略

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+in+Parallel%2C+Answer+as+One%3A+Logit+Averaging+for+Open-Ended+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02874&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a "majority" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.

</details>


### [77] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed, Yang Zhang, Michalis Vazirgiannis, Guokan Shang*

**主要类别:** cs.CL

**AI概要:** SchED是一种无需训练、模型无关的提前退出算法，通过聚合全跨度logit边界并在满足进度相关置信度阈值时停止解码，显著加速扩散大语言模型的推理速度，在保持99%以上性能的同时实现3.8-4倍加速。


<details>
  <summary>更多</summary>
  
**动机:** 扩散大语言模型(dLLMs)虽然是有前景的自回归模型替代方案，但其迭代采样过程缓慢，严重限制了实际应用价值，需要开发高效的解码加速方法。

**方法:** 提出SchED算法：1) 聚合全跨度logit边界；2) 设定基于解码进度的平滑置信度阈值；3) 当置信度达标时提前终止解码过程。该方法无需额外训练且与模型架构无关。

**结果:** 在两个dLLM家族(Dream和LLaDA)的10个基准测试中：指令调优模型获得3.8-4倍加速，保持99.8-100%性能；基础模型获得一致加速增益，保持99.1-100%性能，激进设置下达2.34倍加速。明显优于现有基于置信度的提前退出方法。

**结论:** SchED通过将真实的置信度稳定转化为计算节省，显著提高了dLLM解码效率。指令调优加速了预测熵的衰减过程，进一步提升了算法的有效性。该方法在长文本生成等任务中表现稳健。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast-Decoding+Diffusion+Language+Models+via+Progress-Aware+Confidence+Schedules，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02892&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [78] [AutoNeural: Co-Designing Vision-Language Models for NPU Inference](https://arxiv.org/abs/2512.02924)
*Wei Chen, Liangmin Wu, Yunhai Hu, Zhiyuan Li, Zhiyuan Cheng, Yicheng Qian, Lingyue Zhu, Zhipeng Hu, Luoyi Liang, Qiang Tang, Zhen Liu, Han Yang*

**主要类别:** cs.CL

**AI概要:** AutoNeural是一种专为NPU设计的视觉-语言模型架构，通过替换Vision Transformer为MobileNetV5风格的主干网络，并整合状态空间模型与Transformer层，实现了整数推理的高效性能，在NPU上获得显著的速度提升和量化稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉-语言模型针对GPU优化，在NPU上表现不佳，主要因为Vision Transformer的量化脆弱性和自回归注意力机制的I/O限制无法充分利用NPU的高算术吞吐量。

**方法:** 使用MobileNetV5风格的深度可分离卷积替换标准ViT编码器，确保激活分布有界以支持稳定INT4/8/16量化；语言主干整合状态空间模型原理和Transformer层，采用门控卷积实现线性时间复杂度，消除生成过程中的KV缓存内存I/O开销。

**结果:** 量化误差降低7倍，端到端延迟减少14倍，解码速度提升3倍，上下文窗口延长4倍，在Qualcomm SA8295P SoC上实现座舱应用的实时性能。

**结论:** 针对NPU约束重新设计模型拓扑是实现鲁棒多模态边缘智能的先决条件，AutoNeural展示了硬件-模型协同设计的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoNeural%3A+Co-Designing+Vision-Language+Models+for+NPU+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02924，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02924&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.

</details>


### [79] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

**主要类别:** cs.CL

**AI概要:** 论文提出了一个可视化知识图谱框架来检测和减少大语言模型在企业环境中的幻觉问题，通过将模型生成内容与真实知识源连接并提供置信度指示，帮助用户识别不一致和错误推理。


<details>
  <summary>更多</summary>
  
**动机:** 企业环境中大语言模型经常因有限上下文窗口和预训练数据与领域知识不一致而产生看似可信的幻觉，现有缓解策略成本高昂且无法提供确定性保证。

**方法:** 开发了一个框架，将专有知识和模型生成内容组织成交互式可视化知识图谱，连接模型断言与真实知识源并显示置信度水平。

**结果:** 通过可视化界面，用户可以诊断不一致性、识别薄弱推理链并提供纠正反馈，创建结构化的人机协作反馈循环。

**结论:** 该人机协作工作流程能够增强模型可靠性并持续改进响应质量，为解决大语言模型幻觉问题提供了有效的可视化解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graphing+the+Truth%3A+Structured+Visualizations+for+Automated+Hallucination+Detection+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00663&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [80] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque*

**主要类别:** cs.CL

**AI概要:** 提出一个结合NLP技术、自定义语法和微调语言模型的新框架，用于将英语句子自动转换为CNF逻辑形式，减少LLM在逻辑翻译中的幻觉问题


<details>
  <summary>更多</summary>
  
**动机:** LLM在自然语言到形式逻辑的自动翻译中存在幻觉问题，影响精确性，需要解决这一挑战以实现可靠的自动化推理和软件规范验证

**方法:** 使用自定义语法的经典NLP技术、符号计算库和微调的语言模型，将英语句子转换为逻辑表达式再转CNF形式

**结果:** 实验显示微调模型能够有意识地纠正原始模型的同类幻觉错误，实现可靠的CNF生成

**结论:** 该框架有效减少了LLM在逻辑翻译任务中的幻觉问题，为自动化推理和软件系统验证提供了可靠的工具

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Tuned+Large+Language+Models+for+Logical+Translation%3A+Reducing+Hallucinations+with+Lang2Logic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02987&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [81] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi, Kawser Wazed Nafi, Arghavan Moradi Dakhel, Negar Shahabi, Foutse Khomh*

**主要类别:** cs.CL

**AI概要:** 该研究提出了Moral Consistency Pipeline (MoCoP)，一个无需数据集的闭环框架，用于持续评估大语言模型的道德一致性，发现道德一致性与毒性维度呈强负相关，与响应延迟几乎无关。


<details>
  <summary>更多</summary>
  
**动机:** 现有对齐框架依赖静态数据集和事后评估，无法深入了解道德推理在不同上下文或时间尺度上的演变，需要一种动态、持续的方法来评估LLM的道德稳定性。

**方法:** MoCoP结合三个支持层：词汇完整性分析、语义风险估计和基于推理的判断建模，在自维持架构中自主生成、评估和精炼道德场景，无需外部监督。

**结果:** 实验结果显示道德一致性与毒性维度呈强负相关(rET = -0.81, p < 0.001)，与响应延迟几乎无关(rEL ≈ 0)，表明道德一致性和语言安全是模型行为的稳定特征。

**结论:** MoCoP将道德评估重新定义为动态、模型无关的道德内省形式，为可扩展的持续审计提供了可重复的基础，推动了自主AI系统中计算道德研究的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Moral+Consistency+Pipeline%3A+Continuous+Ethical+Evaluation+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03026&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>
