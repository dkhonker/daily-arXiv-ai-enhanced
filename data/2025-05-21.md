<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 143]
- [cs.AI](#cs.AI) [总数: 73]
- [stat.ML](#stat.ML) [总数: 15]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Tuning Learning Rates with the Cumulative-Learning Constant](https://arxiv.org/abs/2505.13457)
*Nathan Faraj*

**主要类别:** cs.LG

**概要:** 本文介绍了一种新的方法来优化机器学习中的学习率，发现了学习率和数据集大小的新比例关系，并确定了一个累积学习常数，这些发现可能提高多种机器学习应用的训练效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 优化机器学习中的学习率。

**方法:** 发现了一个新的比例关系并识别出一个累积学习常数。

**结果:** 发现了一个之前未被认识的学习率与数据集大小之间的比例关系，并确定了累积学习常数。

**结论:** 发现了学习率和数据集大小之间的新比例关系，并确定了一个累积学习常数，这为设计和优化先进的学习率调度提供了框架，有可能提高各种机器学习应用的训练效率和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tuning+Learning+Rates+with+the+Cumulative-Learning+Constant，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13457&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a novel method for optimizing learning rates in machine
learning. A previously unrecognized proportionality between learning rates and
dataset sizes is discovered, providing valuable insights into how dataset scale
influences training dynamics. Additionally, a cumulative learning constant is
identified, offering a framework for designing and optimizing advanced learning
rate schedules. These findings have the potential to enhance training
efficiency and performance across a wide range of machine learning
applications.

</details>


### [2] [FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review](https://arxiv.org/abs/2505.13461)
*Junye Jiang, Yaan Zhou, Yuanhao Gong, Haoxuan Yuan, Shuanglong Liu*

**主要类别:** cs.LG

**概要:** 本文综述了针对卷积神经网络设计的基于FPGA的硬件加速器，总结了性能评估框架，并探讨了优化策略和架构比较，最后指出了未来挑战与机遇。


<details>
  <summary>更多</summary>
  
**动机:** 随着CNN复杂性的增加，其计算需求显著上升，需要高效的硬件加速器。

**方法:** 综述现有研究并提出性能评估框架，探讨优化策略如并行计算、数据流优化和软硬件协同设计，并比较不同FPGA架构。

**结果:** 提供了关于FPGA在CNN加速中的性能指标（延迟、吞吐量等）的全面比较，并总结了优化策略的有效性。

**结论:** 指出该领域未来的挑战与创新机会。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FPGA-based+Acceleration+for+Convolutional+Neural+Networks%3A+A+Comprehensive+Review，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13461&send_immediately=true&force_search=false)

**原文摘要:** Convolutional Neural Networks (CNNs) are fundamental to deep learning,
driving applications across various domains. However, their growing complexity
has significantly increased computational demands, necessitating efficient
hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a
leading solution, offering reconfigurability, parallelism, and energy
efficiency. This paper provides a comprehensive review of FPGA-based hardware
accelerators specifically designed for CNNs. It presents and summarizes the
performance evaluation framework grounded in existing studies and explores key
optimization strategies, such as parallel computing, dataflow optimization, and
hardware-software co-design. It also compares various FPGA architectures in
terms of latency, throughput, compute efficiency, power consumption, and
resource utilization. Finally, the paper highlights future challenges and
opportunities, emphasizing the potential for continued innovation in this
field.

</details>


### [3] [End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning](https://arxiv.org/abs/2505.13462)
*Thien Nguyen, William Guicquero*

**主要类别:** cs.LG

**概要:** This article proposes a new input encoding technique called GLT for Binary Neural Networks (BNNs) to improve input data representation. The authors also introduce a compact topology with light-weight grouped convolutions to reduce model size and computational complexity. Experiments show that GLT improves accuracy and allows for fully-binarized lightweight models suitable for in-sensor always-on inference.


<details>
  <summary>更多</summary>
  
**动机:** Existing BNNs focus on weights and activations but not input data representation. The authors aim to enhance BNNs by improving input data encoding.

**方法:** Proposes Generic Learned Thermometer (GLT), a technique for non-linear quantization threshold learning to binarize data. Also introduces a compact topology using light-weight grouped convolutions trained with block pruning and Knowledge Distillation (KD).

**结果:** GLT improves accuracy significantly on STL-10 and VWW datasets. Combining GLT with block-pruning results in lightweight, fully-binarized models under 1Mb with minimal accuracy loss.

**结论:** The proposed methods improve BNNs' versatility and efficiency, making them suitable for in-sensor always-on inference applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是End-to-end+fully-binarized+network+design%3A+from+Generic+Learned+Thermometer+to+Block+Pruning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13462，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13462&send_immediately=true&force_search=false)

**原文摘要:** Existing works on Binary Neural Network (BNN) mainly focus on model's weights
and activations while discarding considerations on the input raw data. This
article introduces Generic Learned Thermometer (GLT), an encoding technique to
improve input data representation for BNN, relying on learning non linear
quantization thresholds. This technique consists in multiple data binarizations
which can advantageously replace a conventional Analog to Digital Conversion
(ADC) that uses natural binary coding. Additionally, we jointly propose a
compact topology with light-weight grouped convolutions being trained thanks to
block pruning and Knowledge Distillation (KD), aiming at reducing furthermore
the model size so as its computational complexity. We show that GLT brings
versatility to the BNN by intrinsically performing global tone mapping,
enabling significant accuracy gains in practice (demonstrated by simulations on
the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed
block-pruning technique, we successfully achieve lightweight (under 1Mb),
fully-binarized models with limited accuracy degradation while being suitable
for in-sensor always-on inference use cases.

</details>


### [4] [Predicting The Evolution of Interfaces with Fourier Neural Operators](https://arxiv.org/abs/2505.13463)
*Paolo Guida, William L. Roberts*

**主要类别:** cs.LG

**概要:** This study shows that neural operators can predict the evolution of multi-phase flows with high accuracy and fast response times, which could enable better control of industrial processes.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of predicting complex multi-phase flows with strong discontinuities and large density gradients.

**方法:** Training neural operators using volume of fluid simulations.

**结果:** Highly accurate predictions of liquid-vapour interface evolution.

**结论:** Neural operators can predict multi-phase flow evolutions at speeds suitable for real-time industrial process control.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+The+Evolution+of+Interfaces+with+Fourier+Neural+Operators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13463，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13463&send_immediately=true&force_search=false)

**原文摘要:** Recent progress in AI has established neural operators as powerful tools that
can predict the evolution of partial differential equations, such as the
Navier-Stokes equations. Some complex problems rely on sophisticated algorithms
to deal with strong discontinuities in the computational domain. For example,
liquid-vapour multiphase flows are a challenging problem in many
configurations, particularly those involving large density gradients or phase
change. The complexity mentioned above has not allowed for fine control of fast
industrial processes or applications because computational fluid dynamics (CFD)
models do not have a quick enough forecasting ability. This work demonstrates
that the time scale of neural operators-based predictions is comparable to the
time scale of multi-phase applications, thus proving they can be used to
control processes that require fast response. Neural Operators can be trained
using experimental data, simulations or a combination. In the following, neural
operators were trained in volume of fluid simulations, and the resulting
predictions showed very high accuracy, particularly in predicting the evolution
of the liquid-vapour interface, one of the most critical tasks in a multi-phase
process controller.

</details>


### [5] [The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations](https://arxiv.org/abs/2505.13471)
*George Bird*

**主要类别:** cs.LG

**概要:** This paper introduces a new visualization tool to understand how deep learning models represent data, revealing that activation functions create privileged bases leading to axis-aligned representations.


<details>
  <summary>更多</summary>
  
**动机:** Limited methodologies exist to understand deep learning model representations; understanding what causes representations to align with neurons is fundamental.

**方法:** A novel visualization tool evaluates the distribution around planes defined by the network's privileged basis vectors, providing both atomistic and holistic metrics.

**结果:** Embedded representations tend to be axis-aligned with the privileged basis, which is not the standard basis, and activation functions directly result in privileged bases.

**结论:** This method establishes a causal link between functional form symmetry breaking and representational alignment, answering the question about why representations align with neurons.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Spotlight+Resonance+Method%3A+Resolving+the+Alignment+of+Embedded+Activations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13471，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13471&send_immediately=true&force_search=false)

**原文摘要:** Understanding how deep learning models represent data is currently difficult
due to the limited number of methodologies available. This paper demonstrates a
versatile and novel visualisation tool for determining the axis alignment of
embedded data at any layer in any deep learning model. In particular, it
evaluates the distribution around planes defined by the network's privileged
basis vectors. This method provides both an atomistic and a holistic, intuitive
metric for interpreting the distribution of activations across all planes. It
ensures that both positive and negative signals contribute, treating the
activation vector as a whole. Depending on the application, several variations
of this technique are presented, with a resolution scale hyperparameter to
probe different angular scales. Using this method, multiple examples are
provided that demonstrate embedded representations tend to be axis-aligned with
the privileged basis. This is not necessarily the standard basis, and it is
found that activation functions directly result in privileged bases. Hence, it
provides a direct causal link between functional form symmetry breaking and
representational alignment, explaining why representations have a tendency to
align with the neuron basis. Therefore, using this method, we begin to answer
the fundamental question of what causes the observed tendency of
representations to align with neurons. Finally, examples of so-called
grandmother neurons are found in a variety of networks.

</details>


### [6] [Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency](https://arxiv.org/abs/2505.13499)
*Kelvin Kan, Xingjian Li, Benjamin J. Zhang, Tuhin Sahai, Stanley Osher, Markos A. Katsoulakis*

**主要类别:** cs.LG

**概要:** This paper applies optimal control theory to Transformer models, improving their performance and efficiency while offering theoretical guarantees.


<details>
  <summary>更多</summary>
  
**动机:** To provide a theory-driven approach for improving Transformer models instead of relying on trial-and-error methods.

**方法:** Using tools from continuous-time formulations within the framework of optimal control theory.

**结果:** Improved test performance and parameter efficiency across various tasks compared to baseline models.

**结论:** This is the first application of optimal control theory to both Transformer training and architecture, offering a new foundation for systematic improvements.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Control+for+Transformer+Architectures%3A+Enhancing+Generalization%2C+Robustness+and+Efficiency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13499，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13499&send_immediately=true&force_search=false)

**原文摘要:** We study Transformers through the perspective of optimal control theory,
using tools from continuous-time formulations to derive actionable insights
into training and architecture design. This framework improves the performance
of existing Transformer models while providing desirable theoretical
guarantees, including generalization and robustness. Our framework is designed
to be plug-and-play, enabling seamless integration with established Transformer
models and requiring only slight changes to the implementation. We conduct
seven extensive experiments on tasks motivated by text generation, sentiment
analysis, image classification, and point cloud classification. Experimental
results show that the framework improves the test performance of the baselines,
while being more parameter-efficient. On character-level text generation with
nanoGPT, our framework achieves a 46% reduction in final test loss while using
42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in
final test loss, demonstrating scalability to larger models. To the best of our
knowledge, this is the first work that applies optimal control theory to both
the training and architecture of Transformers. It offers a new foundation for
systematic, theory-driven improvements and moves beyond costly trial-and-error
approaches.

</details>


### [7] [SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty](https://arxiv.org/abs/2505.13501)
*Zequn He, Celia Reina*

**主要类别:** cs.LG

**概要:** 提出了一种名为SPIEDiff的机器学习框架，用于解决耗散系统长时间宏观动力学和热力学数据驱动发现的障碍。


<details>
  <summary>更多</summary>
  
**动机:** 克服粒子模拟的时间尺度限制，非唯一性热力学势和算子，以及高效不确定性量化的需求。

**方法:** 利用统计物理、条件扩散模型和epinets设计了SPIEDiff框架。

**结果:** 在随机Arrhenius粒子过程中评估，能准确揭示热力学和动力学，并实现可靠的长时间宏观预测。

**结论:** SPIEDiff为热力学模型的数据驱动发现提供了一个稳健且可信的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SPIEDiff%3A+robust+learning+of+long-time+macroscopic+dynamics+from+short-time+particle+simulations+with+quantified+epistemic+uncertainty，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13501，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13501&send_immediately=true&force_search=false)

**原文摘要:** The data-driven discovery of long-time macroscopic dynamics and
thermodynamics of dissipative systems with particle fidelity is hampered by
significant obstacles. These include the strong time-scale limitations inherent
to particle simulations, the non-uniqueness of the thermodynamic potentials and
operators from given macroscopic dynamics, and the need for efficient
uncertainty quantification. This paper introduces Statistical-Physics Informed
Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to
overcome these limitations in the context of purely dissipative systems by
leveraging statistical physics, conditional diffusion models, and epinets. We
evaluate the proposed framework on stochastic Arrhenius particle processes and
demonstrate that SPIEDiff can accurately uncover both thermodynamics and
kinetics, while enabling reliable long-time macroscopic predictions using only
short-time particle simulation data. SPIEDiff can deliver accurate predictions
with quantified uncertainty in minutes, drastically reducing the computational
demand compared to direct particle simulations, which would take days or years
in the examples considered. Overall, SPIEDiff offers a robust and trustworthy
pathway for the data-driven discovery of thermodynamic models.

</details>


### [8] [Federated Low-Rank Adaptation for Foundation Models: A Survey](https://arxiv.org/abs/2505.13502)
*Yiyuan Yang, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang, Chengqi Zhang*

**主要类别:** cs.LG

**概要:** 本文综述了LoRA在联邦学习微调基础模型中的集成情况，即FedLoRA，重点研究了分布式学习、异构性和效率这三个关键挑战，并分类现有工作以解决每个挑战的具体方法。最后讨论了开放的研究问题并指出了未来研究的有希望的方向。


<details>
  <summary>更多</summary>
  
**动机:** 有效利用私人数据集在开发基础模型方面仍然是一个重大挑战。联邦学习是一种新兴的合作框架，可以减轻数据隐私风险的同时让多个用户微调这些模型。低秩适应（LoRA）提供了一种资源高效的替代方案，通过大幅减少可训练参数的数量来微调基础模型。

**方法:** 本文主要研究了LoRA在联邦学习微调基础模型中的集成情况，即FedLoRA，重点关注三个关键挑战：分布式学习、异构性和效率。根据具体方法对现有工作进行了分类以解决每个挑战。

**结果:** 本文综述了现有工作，并提出了开放的研究问题和未来研究的有希望的方向。

**结论:** 本文讨论了FedLoRA的下一步发展，以推动其进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Low-Rank+Adaptation+for+Foundation+Models%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13502，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13502&send_immediately=true&force_search=false)

**原文摘要:** Effectively leveraging private datasets remains a significant challenge in
developing foundation models. Federated Learning (FL) has recently emerged as a
collaborative framework that enables multiple users to fine-tune these models
while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA)
offers a resource-efficient alternative for fine-tuning foundation models by
dramatically reducing the number of trainable parameters. This survey examines
how LoRA has been integrated into federated fine-tuning for foundation models,
an area we term FedLoRA, by focusing on three key challenges: distributed
learning, heterogeneity, and efficiency. We further categorize existing work
based on the specific methods used to address each challenge. Finally, we
discuss open research questions and highlight promising directions for future
investigation, outlining the next steps for advancing FedLoRA.

</details>


### [9] [Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation](https://arxiv.org/abs/2505.13507)
*Haoyang Chen*

**主要类别:** cs.LG

**概要:** 提出一种利用CLIP解决开放集领域自适应问题的方法，通过提示驱动跨域对齐和基于梯度的开放集分离实现更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在利用模态间的语义关系和未知样本检测方面存在不足。

**方法:** 通过两个创新点：1) 提示驱动的跨域对齐；2) 基于梯度的开放集分离来改进开放集领域自适应。

**结果:** 在Office-Home数据集上的评估表明该方法优于CLIP基线和标准基线。

**结论:** 提出的模型在解决开放集领域自适应问题上表现出色，并且梯度范数起着关键作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Open+Set+Domain+Adaptation+with+Vision-language+models+via+Gradient-aware+Separation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13507，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13507&send_immediately=true&force_search=false)

**原文摘要:** Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning
known-class distributions across domains while identifying
target-domain-specific unknown categories. Current approaches often fail to
leverage semantic relationships between modalities and struggle with error
accumulation in unknown sample detection. We propose to harness Contrastive
Language-Image Pretraining (CLIP) to address these limitations through two key
innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts
conditioned on domain discrepancy metrics dynamically adapt CLIP's text
encoder, enabling semantic consistency between source and target domains
without explicit unknown-class supervision. 2) Gradient-aware open-set
separation: A gradient analysis module quantifies domain shift by comparing the
L2-norm of gradients from the learned prompts, where known/unknown samples
exhibit statistically distinct gradient behaviors. Evaluations on Office-Home
show that our method consistently outperforms CLIP baseline and standard
baseline. Ablation studies confirm the gradient norm's critical role.

</details>


### [10] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/abs/2505.13510)
*Conor Rowan, Alireza Doostan*

**主要类别:** cs.LG

**概要:** This paper discusses the concept of interpretability in scientific machine learning (SciML), arguing that the current conflations of sparsity and interpretability are inadequate. It proposes an operational definition of interpretability focused on understanding mechanisms rather than mathematical sparsity.


<details>
  <summary>更多</summary>
  
**动机:** To address the lack of clarity around the definition and role of interpretability in SciML, which hinders uncovering fundamental principles in physical sciences.

**方法:** Reviewing key papers on interpretable ML from non-scientific communities and proposing an operational definition of interpretability focusing on mechanism understanding.

**结果:** Proposes a new definition of interpretability emphasizing mechanism understanding over mathematical sparsity, suggesting that sparsity is often unnecessary and questioning the possibility of interpretable scientific discovery without prior knowledge.

**结论:** A precise and philosophically informed definition of interpretability in SciML will help direct research efforts towards overcoming obstacles to achieving a data-driven scientific future.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+definition+and+importance+of+interpretability+in+scientific+machine+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13510，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13510&send_immediately=true&force_search=false)

**原文摘要:** Though neural networks trained on large data sets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models, where relationships come
packaged in the form of simple mathematical expressions, the findings of the
neural network cannot be integrated into the body of scientific knowledge.
Critics of ML's inability to produce human-understandable relationships have
converged on the concept of "interpretability" as its point of departure from
more traditional forms of science. As the growing interest in interpretability
has shown, researchers in the physical sciences seek not just predictive
models, but also to uncover the fundamental principles that govern a system of
interest. However, clarity around a definition of interpretability and the
precise role that it plays in science is lacking in the literature. In this
work, we argue that researchers in equation discovery and symbolic regression
tend to conflate the concept of sparsity with interpretability. We review key
papers on interpretable ML from outside the scientific community and argue
that, though the definitions and methods they propose can inform questions of
interpretability for SciML, they are inadequate for this new purpose. Noting
these deficiencies, we propose an operational definition of interpretability
for the physical sciences. Our notion of interpretability emphasizes
understanding of the mechanism over mathematical sparsity. Innocuous though it
may seem, this emphasis on mechanism shows that sparsity is often unnecessary.
It also questions the possibility of interpretable scientific discovery when
prior knowledge is lacking. We believe a precise and philosophically informed
definition of interpretability in SciML will help focus research efforts toward
the most significant obstacles to realizing a data-driven scientific future.

</details>


### [11] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
*Yanan Li, Fanxu Meng, Muhan Zhang, Shiai Zhu, Shangguang Wang, Mengwei Xu*

**主要类别:** cs.LG

**概要:** 提出了一种名为LoRASuite的新方法，该方法可以有效地利用现有的LoRA权重来适应新的语言模型版本，而不需要从头开始重新训练，从而节省时间和资源。实验表明，LoRASuite在数学任务上的表现优于传统的LoRA方法，并且减少了内存消耗和计算时间。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型的频繁更新，之前版本上训练的LoRA权重迅速过时，而从头开始在最新模型上重新训练LoRA权重既昂贵又耗时。

**方法:** 通过计算旧模型和新模型之间的转移矩阵，并根据中心核对齐和余弦相似性度量分配相应的层和注意力头，然后进行小规模的精细调整步骤确保数值稳定性。

**结果:** LoRASuite在数学任务上的表现优于传统的LoRA方法，并且减少了内存消耗和计算时间。

**结论:** LoRASuite提供了一种有效的方法来适应新的语言模型版本，同时减少了时间和资源的浪费。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LoRASuite%3A+Efficient+LoRA+Adaptation+Across+Large+Language+Model+Upgrades，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13515&send_immediately=true&force_search=false)

**原文摘要:** As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [12] [Zero-Shot Forecasting Mortality Rates: A Global Study](https://arxiv.org/abs/2505.13521)
*Gabor Petnehazi, Laith Al Shaggah, Jozsef Gall, Bernadett Aradi*

**主要类别:** cs.LG

**概要:** This study examines zero-shot time series forecasting using pre-trained foundation models for mortality rate prediction, comparing TimesFM, CHRONOS with traditional methods. While CHRONOS performed well in short-term forecasts, TimesFM underperformed. Fine-tuning CHRONOS improved its accuracy. The Random Forest model had the best overall performance.


<details>
  <summary>更多</summary>
  
**动机:** To explore the potential of zero-shot time series forecasting for mortality rate prediction without task-specific fine-tuning.

**方法:** Evaluated two state-of-the-art foundation models (TimesFM and CHRONOS) alongside traditional and machine learning-based methods across three forecasting horizons using data from 50 countries and 111 age groups.

**结果:** Zero-shot models showed varying results; CHRONOS delivered competitive shorter-term forecasts but underperformed in long-term forecasts. TimesFM consistently underperformed. Fine-tuning CHRONOS improved long-term accuracy. Random Forest model achieved the best overall performance.

**结论:** Zero-shot forecasting has potential but requires careful model selection and domain-specific adaptation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Forecasting+Mortality+Rates%3A+A+Global+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13521，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13521&send_immediately=true&force_search=false)

**原文摘要:** This study explores the potential of zero-shot time series forecasting, an
innovative approach leveraging pre-trained foundation models, to forecast
mortality rates without task-specific fine-tuning. We evaluate two
state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional
and machine learning-based methods across three forecasting horizons (5, 10,
and 20 years) using data from 50 countries and 111 age groups. In our
investigations, zero-shot models showed varying results: while CHRONOS
delivered competitive shorter-term forecasts, outperforming traditional methods
like ARIMA and the Lee-Carter model, TimesFM consistently underperformed.
Fine-tuning CHRONOS on mortality data significantly improved long-term
accuracy. A Random Forest model, trained on mortality data, achieved the best
overall performance. These findings underscore the potential of zero-shot
forecasting while highlighting the need for careful model selection and
domain-specific adaptation.

</details>


### [13] [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)
*Keqi Deng, Philip C. Woodland*

**主要类别:** cs.LG

**概要:** This paper introduces Multi-head Temporal Latent Attention (MTLA), which reduces the size of the Key-Value (KV) cache used in Transformer self-attention along the temporal dimension, significantly decreasing memory usage during inference. MTLA uses a hyper-network to merge KV cache vectors and a stride-aware causal mask to maintain consistent training and inference behavior. Experiments show that MTLA improves inference speed by up to 5.3x and reduces GPU memory usage by 8.3x compared to standard Multi-Head Attention (MHA) without sacrificing performance.


<details>
  <summary>更多</summary>
  
**动机:** To reduce the memory footprint of self-attention inference by compressing the KV cache.

**方法:** Proposes Multi-head Temporal Latent Attention (MTLA) that merges temporally adjacent KV cache vectors using a hyper-network and uses a stride-aware causal mask to align training and inference.

**结果:** Achieves competitive performance to standard MHA while greatly improving inference speed and reducing GPU memory usage across various tasks such as speech translation, speech recognition, speech understanding, and text summarization.

**结论:** MTLA can significantly decrease memory usage during inference without affecting performance, offering substantial improvements in speed and resource efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-head+Temporal+Latent+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13544&send_immediately=true&force_search=false)

**原文摘要:** While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [14] [Online Decision-Focused Learning](https://arxiv.org/abs/2505.13564)
*Aymeric Capitaine, Maxime Haddouche, Eric Moulines, Michael I. Jordan, Etienne Boursier, Alain Durmus*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的在线算法，在动态环境中优化决策聚焦学习，实验显示其优于传统预测聚焦方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的决策聚焦学习研究仅限于静态环境，而本文旨在解决动态环境下目标函数和数据分布随时间变化的问题。

**方法:** 提出了一个实用的在线算法，该算法通过正则化目标函数使其可微，并利用乐观原则以及近似最优oracle和适当扰动。

**结果:** 提出的算法在动态后悔方面建立了界限，并在简单背包问题实验中展示了比经典预测聚焦方法更好的性能。

**结论:** 提出的方法在动态环境中有效地处理了决策聚焦学习的问题，并在理论上和实验上都显示出优于经典预测聚焦方法的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Decision-Focused+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13564&send_immediately=true&force_search=false)

**原文摘要:** Decision-focused learning (DFL) is an increasingly popular paradigm for
training predictive models whose outputs are used in decision-making tasks.
Instead of merely optimizing for predictive accuracy, DFL trains models to
directly minimize the loss associated with downstream decisions. This
end-to-end strategy holds promise for tackling complex combinatorial problems;
however, existing studies focus solely on scenarios where a fixed batch of data
is available and the objective function does not change over time. We instead
investigate DFL in dynamic environments where the objective function and data
distribution evolve over time. This setting is challenging because the
objective function has zero or undefined gradients -- which prevents the use of
standard first-order optimization methods -- and is generally non-convex. To
address these difficulties, we (i) regularize the objective to make it
differentiable and (ii) make use of the optimism principle, based on a
near-optimal oracle along with an appropriate perturbation. This leads to a
practical online algorithm for which we establish bounds on the expected
dynamic regret, both when the decision space is a simplex and when it is a
general bounded convex polytope. Finally, we demonstrate the effectiveness of
our algorithm by comparing its performance with a classic prediction-focused
approach on a simple knapsack experiment.

</details>


### [15] [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547)
*Pengxin Guo, Yinong Wang, Wei Li, Mengting Liu, Ming Li, Jinkai Zheng, Liangqiong Qu*

**主要类别:** cs.LG

**概要:** 提出了一种名为FedPrLLM的联邦修剪框架，用于在保护隐私的同时压缩大型语言模型（LLMs），无需访问公共校准样本。实验表明，一次性层间比较修剪且不缩放权重是最优选择。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM修剪方法通常需要访问公共校准样本，在隐私敏感领域难以获得这些样本。

**方法:** FedPrLLM框架允许每个客户端基于本地校准数据计算修剪掩码矩阵并共享给服务器以修剪全局模型，从而在保持本地数据隐私的同时实现协作修剪。

**结果:** 实验探索了FedPrLLM框架内的多种可能性，并发现一次性层间比较修剪且不缩放权重是最优选择。

**结论:** 研究希望为隐私敏感领域的LLM修剪工作提供指导，并提供了代码链接。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Federated+Pruning+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13547，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13547&send_immediately=true&force_search=false)

**原文摘要:** LLM pruning has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated pruning framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a pruning mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative pruning of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, pruning strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot pruning with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in pruning
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.

</details>


### [16] [Half Search Space is All You Need](https://arxiv.org/abs/2505.13586)
*Pavel Rumiantsev, Mark Coates*

**主要类别:** cs.LG

**概要:** 提出一种结合Zero-Shot NAS与One-Shot NAS的方法，有效减少DARTS搜索空间中的内存消耗和搜索时间，同时保持搜索准确性。


<details>
  <summary>更多</summary>
  
**动机:** 解决One-Shot NAS方法在搜索过程中高GPU内存需求的问题。

**方法:** 利用Zero-Shot NAS预先从搜索空间中移除表现较差的架构，然后对修剪后的搜索空间应用One-Shot NAS。

**结果:** 在DARTS搜索空间上的实验显示，相比基准One-Shot设置，内存消耗减少了81%，且达到相同的准确性水平。

**结论:** 提出的方法显著降低了One-Shot NAS的资源需求，同时保持了搜索效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Half+Search+Space+is+All+You+Need，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13586，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13586&send_immediately=true&force_search=false)

**原文摘要:** Neural Architecture Search (NAS) is a powerful tool for automating
architecture design. One-Shot NAS techniques, such as DARTS, have gained
substantial popularity due to their combination of search efficiency with
simplicity of implementation. By design, One-Shot methods have high GPU memory
requirements during the search. To mitigate this issue, we propose to prune the
search space in an efficient automatic manner to reduce memory consumption and
search time while preserving the search accuracy. Specifically, we utilise
Zero-Shot NAS to efficiently remove low-performing architectures from the
search space before applying One-Shot NAS to the pruned search space.
Experimental results on the DARTS search space show that our approach reduces
memory consumption by 81% compared to the baseline One-Shot setup while
achieving the same level of accuracy.

</details>


### [17] [Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression](https://arxiv.org/abs/2505.13563)
*Xiaohui Wang, Peng Ye, Chenyu Huang, Shenghe Zheng, Bo Zhang, Wanli Ouyang, Tao Chen*

**主要类别:** cs.LG

**概要:** 提出了一种新的数据无关的delta压缩方法UltraDelta，该方法能够在保持高性能的同时实现超高压缩比。


<details>
  <summary>更多</summary>
  
**动机:** 存储众多微调模型会带来显著的存储开销，而现有的delta压缩方法无法同时保持高压缩率和高性能。

**方法:** 提出了一种名为UltraDelta的数据无关的delta压缩管道，包含三个关键组件：基于方差的混合稀疏分配、基于分布感知的压缩和迹范数引导的重缩放。

**结果:** 实验表明，UltraDelta在各种模型上均表现出色，特别是在超高压缩比下。

**结论:** 提出的方法在高压缩比下表现出色，并优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+the+Compression+Ceiling%3A+Data-Free+Pipeline+for+Ultra-Efficient+Delta+Compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13563，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13563&send_immediately=true&force_search=false)

**原文摘要:** With the rise of the fine-tuned--pretrained paradigm, storing numerous
fine-tuned models for multi-tasking creates significant storage overhead. Delta
compression alleviates this by storing only the pretrained model and the highly
compressed delta weights (the differences between fine-tuned and pretrained
model weights). However, existing methods fail to maintain both high
compression and performance, and often rely on data. To address these
challenges, we propose UltraDelta, the first data-free delta compression
pipeline that achieves both ultra-high compression and strong performance.
UltraDelta is designed to minimize redundancy, maximize information, and
stabilize performance across inter-layer, intra-layer, and global dimensions,
using three key components: (1) Variance-Based Mixed Sparsity Allocation
assigns sparsity based on variance, giving lower sparsity to high-variance
layers to preserve inter-layer information. (2) Distribution-Aware Compression
applies uniform quantization and then groups parameters by value, followed by
group-wise pruning, to better preserve intra-layer distribution. (3)
Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a
global rescaling factor, improving model stability under higher compression.
Extensive experiments across (a) large language models (fine-tuned on LLaMA-2
7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)
with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and
(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that
UltraDelta consistently outperforms existing methods, especially under
ultra-high compression.

</details>


### [18] [Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds](https://arxiv.org/abs/2505.13614)
*Ke Sun*

**主要类别:** cs.LG

**概要:** 提出了一种高效估计神经流形上度量张量的方法，并提供了理论保证。


<details>
  <summary>更多</summary>
  
**动机:** 现代深度神经网络参数空间中的Fisher信息度量张量的估计对于深度学习的理论和实际方法都很重要。

**方法:** 通过回到概率分布的核心空间来分析其黎曼度量的光谱，然后将发现扩展到神经流形上的度量张量确定性界。引入了基于Hutchinson迹线估计器的度量张量及其界的无偏随机估计。

**结果:** 该方法可以通过单次反向传播有效评估，并可用于估计对角线、块对角线或完整的张量，其质量由标准差保证。

**结论:** 提出的估计方法在效率和准确性上都有良好的表现，为深度学习中的度量张量研究提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deterministic+Bounds+and+Random+Estimates+of+Metric+Tensors+on+Neuromanifolds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13614，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13614&send_immediately=true&force_search=false)

**原文摘要:** The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.

</details>


### [19] [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/abs/2505.13723)
*Pratik Rathore, Zachary Frangella, Sachin Garg, Shaghayegh Fazliani, Michał Dereziński, Madeleine Udell*

**主要类别:** cs.LG

**概要:** 提出了一种新的分布式算法ADASAP，用于提高高斯过程推理在大规模数据集上的可扩展性，其性能优于现有技术并在超大规模数据集上取得突破。


<details>
  <summary>更多</summary>
  
**动机:** 解决高斯过程推理在大规模数据集上的可扩展性问题，因为传统的线性系统求解方法随着样本数量的增加呈二次增长。

**方法:** 一种近似的、分布式的加速草图投影算法（ADASAP），通过确定性点过程理论证明了草图投影诱导的后验均值快速收敛到真实后验均值。

**结果:** ADASAP在多个基准数据集和大规模贝叶斯优化任务中优于基于共轭梯度和坐标下降的最新求解器，并成功处理超过3亿样本的数据集。

**结论:** 提出了一种新的方法ADASAP，它在大规模数据集上表现出色，并且是首个高效且无条件数限制的高谱基函数后验均值估计算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Turbocharging+Gaussian+Process+Inference+with+Approximate+Sketch-and-Project，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13723，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13723&send_immediately=true&force_search=false)

**原文摘要:** Gaussian processes (GPs) play an essential role in biostatistics, scientific
machine learning, and Bayesian optimization for their ability to provide
probabilistic predictions and model uncertainty. However, GP inference
struggles to scale to large datasets (which are common in modern applications),
since it requires the solution of a linear system whose size scales
quadratically with the number of samples in the dataset. We propose an
approximate, distributed, accelerated sketch-and-project algorithm
($\texttt{ADASAP}$) for solving these linear systems, which improves
scalability. We use the theory of determinantal point processes to show that
the posterior mean induced by sketch-and-project rapidly converges to the true
posterior mean. In particular, this yields the first efficient, condition
number-free algorithm for estimating the posterior mean along the top spectral
basis functions, showing that our approach is principled for GP inference.
$\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate
gradient and coordinate descent across several benchmark datasets and a
large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a
dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished
in the literature.

</details>


### [20] [Learning Dynamics of RNNs in Closed-Loop Environments](https://arxiv.org/abs/2505.13567)
*Yoav Ger, Omri Barak*

**主要类别:** cs.LG

**概要:** 研究了线性循环神经网络（RNN）在闭环环境中的学习动态，并揭示了短期策略改进和长期稳定性之间的相互作用。


<details>
  <summary>更多</summary>
  
**动机:** 典型的训练范式依赖于开环、监督设置，而现实世界的学习在闭环环境中展开。

**方法:** 发展了一个描述线性RNN在闭环上下文中训练时学习动态的数学理论。

**结果:** 展示了闭合环路RNN的学习动态与开环RNN不同，且受到短期策略改进和长期稳定性之间相互作用的影响。

**结论:** 结果强调了在生物上可信的设定中建模闭环动态的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Dynamics+of+RNNs+in+Closed-Loop+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13567，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13567&send_immediately=true&force_search=false)

**原文摘要:** Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer
powerful models of brain computation. However, typical training paradigms rely
on open-loop, supervised settings, whereas real-world learning unfolds in
closed-loop environments. Here, we develop a mathematical theory describing the
learning dynamics of linear RNNs trained in closed-loop contexts. We first
demonstrate that two otherwise identical RNNs, trained in either closed- or
open-loop modes, follow markedly different learning trajectories. To probe this
divergence, we analytically characterize the closed-loop case, revealing
distinct stages aligned with the evolution of the training loss. Specifically,
we show that the learning dynamics of closed-loop RNNs, in contrast to
open-loop ones, are governed by an interplay between two competing objectives:
short-term policy improvement and long-term stability of the agent-environment
interaction. Finally, we apply our framework to a realistic motor control task,
highlighting its broader applicability. Taken together, our results underscore
the importance of modeling closed-loop dynamics in a biologically plausible
setting.

</details>


### [21] [Synthetic Non-stationary Data Streams for Recognition of the Unknown](https://arxiv.org/abs/2505.13745)
*Joanna Komorniczak*

**主要类别:** cs.LG

**概要:** This paper proposes a strategy for generating synthetic data streams that simulate both concept drifts and the emergence of new classes, demonstrating its application in open set recognition tasks.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenges of data non-stationarity in dynamic environments, particularly the simultaneous occurrence of concept drifts and the emergence of new classes.

**方法:** Developing a synthetic data stream generation strategy that incorporates both concept drifts and new class emergence.

**结果:** The proposed method effectively addresses the dual challenges of concept drift and new class emergence in data streams.

**结论:** Unsupervised drift detectors can handle both novelty detection and concept drifts in open set recognition tasks using the generated synthetic data streams.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synthetic+Non-stationary+Data+Streams+for+Recognition+of+the+Unknown，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13745&send_immediately=true&force_search=false)

**原文摘要:** The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.

</details>


### [22] [Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders](https://arxiv.org/abs/2505.13569)
*Fynn Fromme, Christine Allen-Blanchette, Hans Harder, Sebastian Peitz*

**主要类别:** cs.LG

**概要:** This paper presents an end-to-end equivariant surrogate model for large-scale physics systems using machine learning methods, demonstrating significant improvements in sample and parameter efficiency for predicting Rayleigh-Bénard convection.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenges posed by large-scale physics systems governed by partial differential equations, such as high degrees of freedom and complex dynamics across multiple spatial and temporal scales, by developing a more accurate and sample-efficient machine learning model.

**方法:** An end-to-end equivariant surrogate model is constructed, incorporating an equivariant convolutional autoencoder and an equivariant convolutional LSTM using G-steerable kernels, with vertically stacked layers of D4-steerable kernels and partial kernel sharing in the vertical direction.

**结果:** The proposed model shows significant improvements in sample and parameter efficiency, and better scalability to more complex dynamics compared to previous models.

**结论:** The developed equivariant surrogate model demonstrates great potential for modeling complex physical systems, providing a new approach to handle high-dimensional and multi-scale problems in physics.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Surrogate+Modeling+of+3D+Rayleigh-Benard+Convection+with+Equivariant+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13569，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13569&send_immediately=true&force_search=false)

**原文摘要:** The use of machine learning for modeling, understanding, and controlling
large-scale physics systems is quickly gaining in popularity, with examples
ranging from electromagnetism over nuclear fusion reactors and
magneto-hydrodynamics to fluid mechanics and climate modeling. These systems --
governed by partial differential equations -- present unique challenges
regarding the large number of degrees of freedom and the complex dynamics over
many scales both in space and time, and additional measures to improve accuracy
and sample efficiency are highly desirable. We present an end-to-end
equivariant surrogate model consisting of an equivariant convolutional
autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels.
As a case study, we consider the three-dimensional Rayleigh-B\'enard
convection, which describes the buoyancy-driven fluid flow between a heated
bottom and a cooled top plate. While the system is E(2)-equivariant in the
horizontal plane, the boundary conditions break the translational equivariance
in the vertical direction. Our architecture leverages vertically stacked layers
of $D_4$-steerable kernels, with additional partial kernel sharing in the
vertical direction for further efficiency improvement. Our results demonstrate
significant gains both in sample and parameter efficiency, as well as a better
scaling to more complex dynamics, that is, larger Rayleigh numbers. The
accompanying code is available under
https://github.com/FynnFromme/equivariant-rb-forecasting.

</details>


### [23] [Panda: A pretrained forecast model for universal representation of chaotic dynamics](https://arxiv.org/abs/2505.13755)
*Jeffrey Lai, Anthony Bao, William Gilpin*

**主要类别:** cs.LG

**概要:** 提出了一种名为Panda的模型，用于预测混沌动力系统。该模型在合成数据集上训练，具有零-shot预测未知真实世界混沌系统的能力，并且能够在未重新训练的情况下预测偏微分方程。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有方法在处理混沌动力系统的预测问题上的局限性，特别是对于没有大量动态结构的时间序列数据库的挑战。

**方法:** 使用进化算法发现了一个包含2x10^4个混沌动力系统的新型合成可扩展数据集，并在此数据集上训练了Panda模型。

**结果:** Panda模型在未见过的真实世界混沌系统上有零-shot预测能力，并且可以自发地预测偏微分方程。

**结论:** 证明了预训练模型在探索非线性动力学等抽象数学领域中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Panda%3A+A+pretrained+forecast+model+for+universal+representation+of+chaotic+dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13755&send_immediately=true&force_search=false)

**原文摘要:** Chaotic systems are intrinsically sensitive to small errors, challenging
efforts to construct predictive data-driven models of real-world dynamical
systems such as fluid flows or neuronal activity. Prior efforts comprise either
specialized models trained separately on individual time series, or foundation
models trained on vast time series databases with little underlying dynamical
structure. Motivated by dynamical systems theory, we present Panda, Patched
Attention for Nonlinear DynAmics. We train Panda on a novel synthetic,
extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we
discover using an evolutionary algorithm. Trained purely on simulated data,
Panda exhibits emergent properties: zero-shot forecasting of unseen real world
chaotic systems, and nonlinear resonance patterns in cross-channel attention
heads. Despite having been trained only on low-dimensional ordinary
differential equations, Panda spontaneously develops the ability to predict
partial differential equations without retraining. We demonstrate a neural
scaling law for differential equations, underscoring the potential of
pretrained models for probing abstract mathematical domains like nonlinear
dynamics.

</details>


### [24] [An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware](https://arxiv.org/abs/2505.13575)
*Ilkay Wunderlich, Benjamin Koch, Sven Schönfeld*

**主要类别:** cs.LG

**概要:** TinyYOLOv3检测网络在XILINX Artix-7 FPGA上的最佳实践方法，包括批归一化融合、滤波器剪枝和后训练网络量化技术。


<details>
  <summary>更多</summary>
  
**动机:** 解决CNN在嵌入式平台部署的问题，特别是FPGA上的高计算强度、内存需求和算术条件。

**方法:** 在XILINX Artix-7 FPGA上实现TinyYOLOv3检测网络，采用批归一化融合、滤波器剪枝和后训练网络量化等技术。

**结果:** 展示了在FPGA上运行CNN的不同策略。

**结论:** 本文提出的方法可以有效提高CNN在FPGA上的运行效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Overview+of+Arithmetic+Adaptations+for+Inference+of+Convolutional+Neural+Networks+on+Re-configurable+Hardware，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13575，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13575&send_immediately=true&force_search=false)

**原文摘要:** Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.

</details>


### [25] [Consistency Conditions for Differentiable Surrogate Losses](https://arxiv.org/abs/2505.13760)
*Drona Khurana, Anish Thilagar, Dhamma Kimpara, Rafael Frongillo*

**主要类别:** cs.LG

**概要:** 研究了替代损失函数在离散预测任务中的统计一致性，对于非多面体代理，特别是凸可微损失类，引入了强间接诱导（IE）的概念，并证明其在强凸可微代理下是校准性必要且充分的条件。


<details>
  <summary>更多</summary>
  
**动机:** 现有的直接验证校准方法较为困难，因此需要寻找等价但更简单的条件来代替。

**方法:** 通过理论证明和构造反例，研究了IE与校准之间的关系，并提出了强IE的概念。

**结果:** 证明了在一类一维凸可微损失中，IE与校准等价；但在高维情况下该等价关系失效。强IE对于可微代理可以推出校准性，并且在强凸可微代理下是校准性的充要条件。

**结论:** 强IE提供了一种新的方法来设计和分析一致的可微代理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Consistency+Conditions+for+Differentiable+Surrogate+Losses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13760&send_immediately=true&force_search=false)

**原文摘要:** The statistical consistency of surrogate losses for discrete prediction tasks
is often checked via the condition of calibration. However, directly verifying
calibration can be arduous. Recent work shows that for polyhedral surrogates, a
less arduous condition, indirect elicitation (IE), is still equivalent to
calibration. We give the first results of this type for non-polyhedral
surrogates, specifically the class of convex differentiable losses. We first
prove that under mild conditions, IE and calibration are equivalent for
one-dimensional losses in this class. We construct a counter-example that shows
that this equivalence fails in higher dimensions. This motivates the
introduction of strong IE, a strengthened form of IE that is equally easy to
verify. We establish that strong IE implies calibration for differentiable
surrogates and is both necessary and sufficient for strongly convex,
differentiable surrogates. Finally, we apply these results to a range of
problems to demonstrate the power of IE and strong IE for designing and
analyzing consistent differentiable surrogates.

</details>


### [26] [FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments](https://arxiv.org/abs/2505.13576)
*Sara Alosaime, Arshad Jhumka*

**主要类别:** cs.LG

**概要:** 本文提出了一种名为FlexFed的新联邦学习方法，解决了人类活动识别环境中由于非平稳数据分布和间歇性参与导致的灾难性遗忘问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习方法在处理非平稳数据分布和间歇性参与时面临挑战，导致严重的灾难性遗忘问题，而传统的连续学习策略因隐私限制无法应用。

**方法:** 提出了一种新的联邦学习方法FlexFed，该方法优先保留数据以高效利用内存，并根据分布变化、客户端能力和离线时长动态调整离线训练频率。

**结果:** 实验表明，FlexFed在处理稀疏或代表性不足的数据时，能够更有效地缓解灾难性遗忘问题，提高联邦学习的效率并实现更快、更稳定的收敛。

**结论:** FlexFed有效缓解了连续学习中的灾难性遗忘问题，并提高了联邦学习的效率和收敛速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlexFed%3A+Mitigating+Catastrophic+Forgetting+in+Heterogeneous+Federated+Learning+in+Pervasive+Computing+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13576，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13576&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative model training while preserving
privacy by allowing clients to share model updates instead of raw data.
Pervasive computing environments (e.g., for Human Activity Recognition, HAR),
which we focus on in this paper, are characterized by resource-constrained end
devices, streaming sensor data and intermittent client participation.
Variations in user behavior, common in HAR environments, often result in
non-stationary data distributions. As such, existing FL approaches face
challenges in HAR settings due to differing assumptions. The combined effects
of HAR characteristics, namely heterogeneous data and intermittent
participation, can lead to a severe issue called catastrophic forgetting (CF).
Unlike Continuous Learning (CL), which addresses CF using memory and replay
mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach
that prioritizes data retention for efficient memory use and dynamically
adjusts offline training frequency based on distribution shifts, client
capability and offline duration. To better quantify CF in FL, we introduce a
new metric that accounts for under-represented data, enabling more accurate
evaluations. We also develop a realistic HAR-based evaluation framework that
simulates streaming data, dynamic distributions, imbalances and varying
availability. Experiments show that FlexFed mitigates CF more effectively,
improves FL efficiency by 10 to 15 % and achieves faster, more stable
convergence, especially for infrequent or under-represented data.

</details>


### [27] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/abs/2505.13768)
*Ruiquan Huang, Donghao Li, Chengshuai Shi, Cong Shen, Jing Yang*

**主要类别:** cs.LG

**概要:** 提出了一种混合学习框架，结合离线数据和在线交互来优化策略，在两种学习指标下表现优于纯在线或离线算法。


<details>
  <summary>更多</summary>
  
**动机:** 研究如何有效利用离线数据和在线交互来提升强化学习的效果。

**方法:** 提出了一种统一的算法，并分析了其性能。

**结果:** 在两种学习指标下取得了最先进的结果，揭示了离线数据覆盖性质对于不同目标的重要差异。

**结论:** 验证了理论发现，展示了该方法在特定强化学习模型中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Augmenting+Online+RL+with+Offline+Data+is+All+You+Need%3A+A+Unified+Hybrid+RL+Algorithm+Design+and+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13768&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [28] [Symmetry-Breaking Descent for Invariant Cost Functionals](https://arxiv.org/abs/2505.13578)
*Mikhail Osipov*

**主要类别:** cs.LG

**概要:** 研究了在具有全局对称性的Sobolev类信号上的任务成本泛函的减少问题，提出了一种无需访问模型梯度或标签的变分方法。


<details>
  <summary>更多</summary>
  
**动机:** 在机器学习、成像和反问题等领域，存在一些成本度量反映模型输出或性能评分但不可微且内部模型依赖的情况。

**方法:** 通过变分方法利用对称性结构构建显式的对称破坏变形。

**结果:** 提出的变分方法能够在大多数情况下严格减少成本，并且不需要访问模型梯度或标签，完全在测试时操作。

**结论:** 提出的方法可以有效地减少具有全局对称性的任务成本泛函，并且在大多数情况下能够严格降低成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Symmetry-Breaking+Descent+for+Invariant+Cost+Functionals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13578&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of reducing a task cost functional $W(S)$, defined over
Sobolev-class signals $S$, when the cost is invariant under a global symmetry
group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such
scenarios arise in machine learning, imaging, and inverse problems, where cost
metrics reflect model outputs or performance scores but are non-differentiable
and model-internal. We propose a variational method that exploits the symmetry
structure to construct explicit, symmetry-breaking deformations of the input
signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy
functional, induces a deformation $h = A_\phi[S]$ that generically lies
transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the
cost $W$ strictly decreases along this direction -- either via Clarke
subdifferential descent or by escaping locally flat plateaus. The exceptional
set of degeneracies has zero Gaussian measure. Our approach requires no access
to model gradients or labels and operates entirely at test time. It provides a
principled tool for optimizing invariant cost functionals via Lie-algebraic
variational flows, with applications to black-box models and
symmetry-constrained tasks.

</details>


### [29] [OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making](https://arxiv.org/abs/2505.13580)
*Hanzhao Wang, Guanting Chen, Kalyan Talluri, Xiaocheng Li*

**主要类别:** cs.LG

**概要:** 提出了一种基于生成预训练Transformer（GPT）模型的方法OMGPT，用于解决运营研究和管理科学中的序列决策问题。该模型通过变压器神经网络架构实现历史到未来动作的直接丰富映射，并且在动态定价、库存管理等任务上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法在处理运营研究和管理科学中的序列决策问题时存在局限性，如需要假设特定的分析模型结构。因此，需要一种新的方法来克服这些限制并提高性能。

**方法:** 构建了一个从头开始的GPT模型（OMGPT），首先提出一个通用序列建模框架涵盖多个操作决策任务，然后训练一个基于变压器的神经网络模型作为序列建模的自然且强大的架构。

**结果:** OMGPT模型在动态定价、库存管理、资源分配和排队控制等任务上表现出色，实现了从历史到未来行动的直接和丰富的映射，并且没有假设任何分析模型结构。

**结论:** OMGPT模型通过利用大量预训练数据和不假设任何分析模型结构，实现了现有方法无法达到的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OMGPT%3A+A+Sequence+Modeling+Framework+for+Data-driven+Operational+Decision+Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13580，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13580&send_immediately=true&force_search=false)

**原文摘要:** We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.

</details>


### [30] [Adversarial Training from Mean Field Perspective](https://arxiv.org/abs/2505.14021)
*Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki*

**主要类别:** cs.LG

**概要:** This study provides a theoretical analysis of adversarial training in random deep neural networks without assumptions on data distributions. It introduces a new theoretical framework, derives upper bounds of adversarial loss, proves properties of adversarially trainable networks, and shows the impacts of network width and dimensions on adversarial training.


<details>
  <summary>更多</summary>
  
**动机:** To understand the training dynamics of adversarial training better.

**方法:** Theoretical analysis using mean field theory in random deep neural networks without assumptions on data distributions.

**结果:** Derived upper bounds of adversarial loss, proved properties of adversarially trainable networks, showed impacts of network width and dimensions on adversarial training.

**结论:** Adversarial training reduces network capacity and network width can alleviate related issues.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Training+from+Mean+Field+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14021，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14021&send_immediately=true&force_search=false)

**原文摘要:** Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.

</details>


### [31] [Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting](https://arxiv.org/abs/2505.13582)
*Leyang Zhang, Yaoyu Zhang, Tao Luo*

**主要类别:** cs.LG

**概要:** 研究了神经网络临界点的样本依赖性，引入了一个与样本无关的临界提升算子，并通过示例说明先前研究的临界嵌入没有捕捉到所有与样本无关的提升临界点。最后证明了对于足够大的样本量，存在具有鞍点的样本相关提升临界点。


<details>
  <summary>更多</summary>
  
**动机:** 研究神经网络临界点的样本依赖性

**方法:** 引入样本独立的临界提升算子并定义样本依赖和独立的提升临界点

**结果:** 通过例子展示先前研究的临界嵌入未捕捉全部样本无关提升临界点，并证明大样本下存在样本相关提升临界点且其中包含鞍点

**结论:** 揭示了神经网络临界点的复杂性及其与样本的关系

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncovering+Critical+Sets+of+Deep+Neural+Networks+via+Sample-Independent+Critical+Lifting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13582，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13582&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the sample dependence of critical points for neural
networks. We introduce a sample-independent critical lifting operator that
associates a parameter of one network with a set of parameters of another, thus
defining sample-dependent and sample-independent lifted critical points. We
then show by example that previously studied critical embeddings do not capture
all sample-independent lifted critical points. Finally, we demonstrate the
existence of sample-dependent lifted critical points for sufficiently large
sample sizes and prove that saddles appear among them.

</details>


### [32] [Adversarially Pretrained Transformers may be Universally Robust In-Context Learners](https://arxiv.org/abs/2505.14042)
*Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki*

**主要类别:** cs.LG

**概要:** Adversarially pretrained transformers can act as robust foundation models, eliminating the need for adversarial training in downstream tasks.


<details>
  <summary>更多</summary>
  
**动机:** To find a more efficient way to achieve robustness in transformers by avoiding costly adversarial training.

**方法:** Using adversarial pretraining on transformers to enable robust generalization to unseen tasks via in-context learning.

**结果:** A single adversarially pretrained transformer can robustly generalize to multiple unseen tasks without extra training or parameter updates.

**结论:** While this approach shows promise, there are limitations such as unrealistic conditions where no universally robust single-layer transformers exist, a trade-off between accuracy and robustness, and the requirement of many in-context demonstrations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarially+Pretrained+Transformers+may+be+Universally+Robust+In-Context+Learners，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14042&send_immediately=true&force_search=false)

**原文摘要:** Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.

</details>


### [33] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/abs/2505.14214)
*Mattes Mollenhauer, Nicole Mücke, Dimitri Meunier, Arthur Gretton*

**主要类别:** cs.LG

**概要:** This paper investigates the performance of ridge regression in Hilbert spaces with heavy-tailed noise, achieving optimal convergence rates under standard eigenvalue decay conditions.


<details>
  <summary>更多</summary>
  
**动机:** To examine the robustness of regularized least squares methods against heavy-tailed noise in the context of reproducing kernel Hilbert spaces.

**方法:** Establishing excess risk bounds using an integral operator framework and applying a Fuk-Nagaev inequality for Hilbert-space valued random variables.

**结果:** Achieving convergence rates comparable to those derived under subexponential noise assumptions, thus demonstrating the robustness of regularized least squares methods.

**结论:** Regularized least squares methods exhibit asymptotic robustness against heavy-tailed noise in Hilbert spaces.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Regularized+least+squares+learning+with+heavy-tailed+noise+is+minimax+optimal，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14214&send_immediately=true&force_search=false)

**原文摘要:** This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [34] [Just One Layer Norm Guarantees Stable Extrapolation](https://arxiv.org/abs/2505.14512)
*Juliusz Ziomek, George Whittle, Michael A. Osborne*

**主要类别:** cs.LG

**概要:** This work proves general results about the behavior of neural networks when extrapolating far from the training distribution using Neural Tangent Kernel (NTK) theory.


<details>
  <summary>更多</summary>
  
**动机:** Understanding the behavior of neural networks when extrapolating beyond the training data is important but has been poorly understood so far.

**方法:** Applying Neural Tangent Kernel (NTK) theory to analyze infinitely-wide neural networks trained until convergence.

**结果:** The inclusion of just one Layer Norm (LN) fundamentally alters the induced NTK, making the output of an infinitely wide network with at least one LN remain bounded, even on inputs far from the training data.

**结论:** Adding a single Layer Norm can prevent uncontrolled growth in neural networks, which is crucial for tasks like protein residue size prediction and age estimation from underrepresented ethnicities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Just+One+Layer+Norm+Guarantees+Stable+Extrapolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14512，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14512&send_immediately=true&force_search=false)

**原文摘要:** In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.

</details>


### [35] [Learning (Approximately) Equivariant Networks via Constrained Optimization](https://arxiv.org/abs/2505.13631)
*Andrei Manolache, Luiz F. O. Chamon, Mathias Niepert*

**主要类别:** cs.LG

**概要:** 提出了一种名为Adaptive Constrained Equivariance (ACE) 的新方法，该方法通过受同伦原则启发，从灵活的非等变模型开始，并逐渐减少其与等变性的偏差。ACE在多个架构和任务上提高了性能指标、样本效率以及对输入扰动的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的数据往往偏离完美对称性，严格等变模型可能难以拟合数据，而无约束模型缺乏有效利用部分对称性的原则性方法。此外，在数据完全对称的情况下，强制等变可能会通过限制模型的参数空间来损害训练。

**方法:** 引入了Adaptive Constrained Equivariance (ACE)，这是一种受同伦原则启发的约束优化方法，它从灵活的非等变模型开始，并逐渐减少其与等变性的偏差。

**结果:** ACE在多个架构和任务上提高了性能指标、样本效率以及对输入扰动的鲁棒性。

**结论:** ACE提供了一种平衡等变性和非等变性的新途径，能够提高模型的泛化能力、样本效率和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+%28Approximately%29+Equivariant+Networks+via+Constrained+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13631，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13631&send_immediately=true&force_search=false)

**原文摘要:** Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.

</details>


### [36] [CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering](https://arxiv.org/abs/2505.14596)
*Isabella Degen, Zahraa S Abdallah, Henry W J Reeve, Kate Robson Brown*

**主要类别:** cs.LG

**概要:** 提出CSTS（时间序列中的相关结构），这是一个用于评估多元时间序列数据中相关结构发现的合成基准。CSTS通过区分相关结构退化和聚类算法及验证方法的局限性，使研究人员能够隔离并识别聚类失败的具体原因。贡献包括全面的基准测试、相关结构保存的实证验证以及可扩展的数据生成框架。案例研究展示了CSTS在诊断方法限制方面的实用性。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列聚类在多个关键领域有应用，但缺乏验证的地面真实信息使得无法客观评估聚类质量。

**方法:** 引入CSTS作为合成基准，用于评估多元时间序列数据中相关结构的发现。

**结果:** CSTS提供了清晰的基准，可以隔离并识别聚类失败的具体原因。实证验证了相关结构保存的情况，并提供了一个可扩展的数据生成框架。

**结论:** CSTS推进了基于相关的时间序列聚类的严格评估标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CSTS%3A+A+Benchmark+for+the+Discovery+of+Correlation+Structures+in+Time+Series+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14596&send_immediately=true&force_search=false)

**原文摘要:** Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
"more art than science" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.

</details>


### [37] [Incentivizing Truthful Language Models via Peer Elicitation Games](https://arxiv.org/abs/2505.13636)
*Baiting Chen, Tong Zhu, Jiale Han, Lexin Li, Gang Li, Xiaowu Dai*

**主要类别:** cs.LG

**概要:** Introduce Peer Elicitation Games (PEG), a training-free framework for aligning LLMs through peer evaluation, proving sublinear regret and convergence to truthful Nash equilibrium, demonstrating factual accuracy improvement.


<details>
  <summary>更多</summary>
  
**动机:** Improve the consistency and reduce hallucinations in LLMs without supervision or fine-tuning.

**方法:** PEG framework with generator and discriminators using determinant-based mutual information score.

**结果:** Significant factual accuracy improvement across multiple benchmarks.

**结论:** PEG is a practical approach for eliciting truthful behavior from LLMs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Incentivizing+Truthful+Language+Models+via+Peer+Elicitation+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13636，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13636&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated strong generative capabilities
but remain prone to inconsistencies and hallucinations. We introduce Peer
Elicitation Games (PEG), a training-free, game-theoretic framework for aligning
LLMs through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators interact
in a peer evaluation setting, where rewards are computed using a
determinant-based mutual information score that provably incentivizes truthful
reporting without requiring ground-truth labels. We establish theoretical
guarantees showing that each agent, via online learning, achieves sublinear
regret in the sense their cumulative performance approaches that of the best
fixed truthful strategy in hindsight. Moreover, we prove last-iterate
convergence to a truthful Nash equilibrium, ensuring that the actual policies
used by agents converge to stable and truthful behavior over time. Empirical
evaluations across multiple benchmarks demonstrate significant improvements in
factual accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or fine-tuning.

</details>


### [38] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/abs/2505.13638)
*Massimo Fioravanti, Giovanni Agosta*

**主要类别:** cs.LG

**概要:** This paper introduces the 4Hammer reinforcement learning environment, which simulates a subset of the complex board game Warhammer 40,000 to address the lack of such environments for evaluating large language models.


<details>
  <summary>更多</summary>
  
**动机:** There is a need for more complex board game environments designed for reinforcement learning and evaluation of large language models.

**方法:** The proposed 4Hammer environment simulates a subset of Warhammer 40,000, a complex board game with intricate rules.

**结果:** The 4Hammer environment provides a platform for evaluating LLMs in tasks that require understanding complex rules and interactions.

**结论:** The introduction of 4Hammer fills a gap in the availability of complex board game environments for reinforcement learning and LLM evaluation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是4Hammer%3A+a+board-game+reinforcement+learning+environment+for+the+hour+long+time+frame，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13638，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13638&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [39] [FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning](https://arxiv.org/abs/2505.13643)
*Rakibul Hasan Rajib, Md Akil Raihan Iftee, Mir Sazzat Hossain, A. K. M. Mahbubur Rahman, Sajib Mistry, M Ashraful Amin, Amin Ahsan Ali*

**主要类别:** cs.LG

**概要:** 提出了一种新的联邦学习适应方法FedCTTA，该方法通过基于模型输出分布的相似性感知聚合来避免直接特征交换，从而实现隐私保护和计算效率高的自适应知识共享。实验表明，FedCTTA在各种时间异构性和空间异构性场景下优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** FL模型在训练和部署之间存在分布偏移，导致性能下降；现有的TTA方法在FL中面临计算开销大、隐私风险和可扩展性差的问题。

**方法:** 提出FedCTTA框架，利用相似性感知聚合基于随机生成的噪声样本的模型输出分布，避免直接特征交换，同时最小化每个客户端的熵以持续适应。

**结果:** FedCTTA在各种异构性场景下超越了现有的方法。

**结论:** FedCTTA是一种隐私保护、计算高效的联邦适应方法，解决了现有方法的局限性，具有良好的可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedCTTA%3A+A+Collaborative+Approach+to+Continual+Test-Time+Adaptation+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13643，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13643&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it ideal for
privacy-sensitive applications. However, FL models often suffer performance
degradation due to distribution shifts between training and deployment.
Test-Time Adaptation (TTA) offers a promising solution by allowing models to
adapt using only test samples. However, existing TTA methods in FL face
challenges such as computational overhead, privacy risks from feature sharing,
and scalability concerns due to memory constraints. To address these
limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a
privacy-preserving and computationally efficient framework for federated
adaptation. Unlike prior methods that rely on sharing local feature statistics,
FedCTTA avoids direct feature exchange by leveraging similarity-aware
aggregation based on model output distributions over randomly generated noise
samples. This approach ensures adaptive knowledge sharing while preserving data
privacy. Furthermore, FedCTTA minimizes the entropy at each client for
continual adaptation, enhancing the model's confidence in evolving target
distributions. Our method eliminates the need for server-side training during
adaptation and maintains a constant memory footprint, making it scalable even
as the number of clients or training rounds increases. Extensive experiments
show that FedCTTA surpasses existing methods across diverse temporal and
spatial heterogeneity scenarios.

</details>


### [40] [Collapsing Taylor Mode Automatic Differentiation](https://arxiv.org/abs/2505.13644)
*Felix Dangel, Tim Siebert, Marius Zeinhofer, Andrea Walther*

**主要类别:** cs.LG

**概要:** 提出了一种优化泰勒模式的计算方法，通过重写计算图来加速泰勒模式，并在常见偏微分方程算子上验证其性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的部分微分方程(PDE)算子计算方式昂贵且限制了科学机器学习的应用。

**方法:** 引入了一种优化泰勒模式的计算技术，即通过重写计算图来简化导数计算，并将其应用于一般线性PDE算子和随机泰勒模式。

**结果:** 该方法只需要沿着计算图传播一个总和，可以加速泰勒模式并优于嵌套反向传播。

**结论:** 提出的优化技术可以有效加速PDE算子的计算，提升科学机器学习的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Collapsing+Taylor+Mode+Automatic+Differentiation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13644，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13644&send_immediately=true&force_search=false)

**原文摘要:** Computing partial differential equation (PDE) operators via nested
backpropagation is expensive, yet popular, and severely restricts their utility
for scientific machine learning. Recent advances, like the forward Laplacian
and randomizing Taylor mode automatic differentiation (AD), propose forward
schemes to address this. We introduce an optimization technique for Taylor mode
that 'collapses' derivatives by rewriting the computational graph, and
demonstrate how to apply it to general linear PDE operators, and randomized
Taylor mode. The modifications simply require propagating a sum up the
computational graph, which could -- or should -- be done by a machine learning
compiler, without exposing complexity to users. We implement our collapsing
procedure and evaluate it on popular PDE operators, confirming it accelerates
Taylor mode and outperforms nested backpropagation.

</details>


### [41] [Self-Reinforced Graph Contrastive Learning](https://arxiv.org/abs/2505.13650)
*Chou-Ying Hsieh, Chun-Fu Jang, Cheng-En Hsieh, Qian-Hui Chen, Sy-Yen Kuo*

**主要类别:** cs.LG

**概要:** 提出了一种新的图对比学习框架SRGCL，通过自身编码器动态评估和选择高质量正样本对，改进了图表示的质量，并在多种图分类任务上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 确保高质的正样本对以保留原图的语义和结构特性。

**方法:** 提出SRGCL框架，使用多增强策略生成正样本对，并采用流形假设引导的选择器来保持潜在空间的几何结构。

**结果:** 在不同图级别分类任务中，SRGCL作为插件模块的表现优于最先进的GCL方法。

**结论:** SRGCL具有适应性和有效性，适用于多个领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Reinforced+Graph+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13650，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13650&send_immediately=true&force_search=false)

**原文摘要:** Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.

</details>


### [42] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/abs/2505.13697)
*Soumya Rani Samineni, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati*

**主要类别:** cs.LG

**概要:** This paper critically examines reinforcement learning-based post-training methods for large language models, arguing that common structural assumptions reduce such approaches to outcome-driven supervised learning.


<details>
  <summary>更多</summary>
  
**动机:** To critically evaluate the formulation and assumptions of reinforcement learning post-training methods for large language models.

**方法:** Analyzing the structural assumptions in modeling LLM training as a Markov Decision Process and demonstrating their equivalence to supervised learning through experiments.

**结果:** Experiments on benchmarks like GSM8K and Countdown showed that iterative supervised fine-tuning achieved performance comparable to GRPO-based training.

**结论:** The simplistic structural assumptions in current LLM RL frameworks render them and their interpretations questionable.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RL+in+Name+Only%3F+Analyzing+the+Structural+Assumptions+in+RL+post-training+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13697，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13697&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [43] [Unsupervised anomaly detection in MeV ultrafast electron diffraction](https://arxiv.org/abs/2505.13702)
*Mariana A. Fazio, Salvador Sosa Güitron, Marcus Babzien, Mikhail Fedurin, Junjie Li, Mark Palmer, Sandra S. Biedron, Manel Martinez-Ramon*

**主要类别:** cs.LG

**概要:** 提出了一种无监督的异常检测方法来检测MUED中的故障图像。


<details>
  <summary>更多</summary>
  
**动机:** 减少手动标记数据的工作量并提供检测不确定性度量。

**方法:** 构建一种无监督的异常检测方法。

**结果:** 能够检测MUED中的故障图像。

**结论:** 无监督技术是该任务的最佳选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+anomaly+detection+in+MeV+ultrafast+electron+diffraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13702&send_immediately=true&force_search=false)

**原文摘要:** This study focus in the construction of an unsupervised anomaly detection
methodology to detect faulty images in MUED. We believe that unsupervised
techniques are the best choice for our purposes because the data used to train
the detector does not need to be manually labeled, and instead, the machine is
intended to detect by itself the anomalies in the dataset, which liberates the
user of tedious, time-consuming initial image examination. The structure must,
additionally, provide the user with some measure of uncertainty in the
detection, so the user can take decisions based on this measure.

</details>


### [44] [Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning](https://arxiv.org/abs/2505.13709)
*Jiayu Chen, Aravind Venugopal, Jeff Schneider*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的离线模型基础强化学习方法，该方法通过动态调整世界模型和策略来提高策略的鲁棒性，并在多个任务上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的离线MBRL方法存在目标不匹配的问题，导致世界模型不是专门为有效的策略学习优化的，并且离线MBRL学习的策略在部署时缺乏鲁棒性。

**方法:** 提出了一种动态调整世界模型和策略的框架，采用统一的学习目标来提高鲁棒性，并通过利用Stackelberg学习动力学解决了最大化优化问题。

**结果:** 所提出的方法在多个任务上展示了优越的性能，特别是在处理环境中的小幅度对抗噪声时表现出了更好的稳定性。

**结论:** 提出的框架在提高鲁棒性方面表现出色，在十二个嘈杂的D4RL MuJoCo任务和三个随机托卡马克控制任务上展示了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Policy-Driven+World+Model+Adaptation+for+Robust+Offline+Model-based+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13709，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13709&send_immediately=true&force_search=false)

**原文摘要:** Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.

</details>


### [45] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
*Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, Joel Hestness*

**主要类别:** cs.LG

**概要:** 研究了LLM预训练中超参数（如学习率η和权重衰减λ）的缩放规律，提出了在模型大小N、数据集大小D和批量大小B变化时超参数缩放的公式。发现AdamW时间尺度B/(ηλD)在不同训练设置下应保持不变，并验证了在固定N和D的情况下最优λ与B成线性关系。此外，还研究了最优批量大小Bopt和临界批量大小Bcrit的缩放规律。最后，分析了这些发现如何影响双重训练时间和计算目标下的Pareto最优N和D的选择。


<details>
  <summary>更多</summary>
  
**动机:** 提高LLM预训练效率需要调整超参数，因此研究超参数的缩放规律具有重要意义。

**方法:** 通过分析缩放规律，提出公式来预测在大规模训练前的最优超参数λopt，并研究了最优批量大小Bopt和临界批量大小Bcrit的缩放规律。

**结果:** 发现AdamW时间尺度B/(ηλD)在不同训练设置下应保持不变；最优λ与B成线性关系，在N和D变化时最优时间尺度遵循精确的幂律；Bopt和Bcrit均按D的幂律缩放，与N无关。

**结论:** 本文揭示了LLM预训练中超参数和批量大小的缩放规律，为实际应用中的超参数选择提供了理论依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Power+Lines%3A+Scaling+Laws+for+Weight+Decay+and+Batch+Size+in+LLM+Pre-training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13738，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13738&send_immediately=true&force_search=false)

**原文摘要:** Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [46] [Improving Compositional Generation with Diffusion Models Using Lift Scores](https://arxiv.org/abs/2505.13740)
*Chenning Yu, Sicun Gao*

**主要类别:** cs.LG

**概要:** 提出了一种基于提升分数的新重采样标准，用于改进扩散模型中的组合生成。通过大量实验展示了该方法在二维合成数据、CLEVR位置任务和文本到图像合成中的条件对齐效果显著改善。


<details>
  <summary>更多</summary>
  
**动机:** 现有扩散模型在组合生成任务中存在条件对齐问题，需要一种新的评估机制来提高生成样本与单个条件的一致性。

**方法:** 利用提升分数评估生成样本与每个单一条件的对齐情况，并结合这些结果判断是否满足组合提示。此过程无需额外训练或外部模块，仅依赖原始扩散模型即可高效近似提升分数。还开发了优化变体以减少推理时的计算开销。

**结果:** 在2D合成数据、CLEVR位置任务以及文本到图像合成任务中，条件对齐得到了显著改善。

**结论:** 提出的基于提升分数的重采样标准能够有效提升扩散模型在组合生成任务中的性能，且具有较低的计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Compositional+Generation+with+Diffusion+Models+Using+Lift+Scores，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13740，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13740&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel resampling criterion using lift scores, for improving
compositional generation in diffusion models. By leveraging the lift scores, we
evaluate whether generated samples align with each single condition and then
compose the results to determine whether the composed prompt is satisfied. Our
key insight is that lift scores can be efficiently approximated using only the
original diffusion model, requiring no additional training or external modules.
We develop an optimized variant that achieves relatively lower computational
overhead during inference while maintaining effectiveness. Through extensive
experiments, we demonstrate that lift scores significantly improved the
condition alignment for compositional generation across 2D synthetic data,
CLEVR position tasks, and text-to-image synthesis. Our code is available at
http://github.com/rainorangelemon/complift.

</details>


### [47] [Understanding Task Representations in Neural Networks via Bayesian Ablation](https://arxiv.org/abs/2505.13742)
*Andrew Nam, Declan Campbell, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie*

**主要类别:** cs.LG

**概要:** 提出了一种新的概率框架来解释神经网络中的潜在任务表示。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络在认知建模中有很强的能力，但解释其学习到的表示仍然具有挑战性。

**方法:** 引入了一个新的概率框架，通过贝叶斯推理定义表示单元上的分布来推断它们对任务表现的因果贡献，并使用信息论的思想提出了一套工具和度量标准。

**结果:** 提出了新的概率框架以及一套工具和度量标准来解释神经网络中的潜在任务表示。

**结论:** 该工作为理解神经网络中的潜在任务表示提供了一种新的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Task+Representations+in+Neural+Networks+via+Bayesian+Ablation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13742&send_immediately=true&force_search=false)

**原文摘要:** Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.

</details>


### [48] [Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning](https://arxiv.org/abs/2505.13754)
*Devendra Parkar, Anya Chaturvedi, Andréa W. Richa, Joshua J. Daymude*

**主要类别:** cs.LG

**概要:** 提出了一种新的无监督学习模型用于解决动态图中的最大独立集问题。该模型结合了图神经网络的学习能力和分布式更新机制，能够高效处理大规模动态图，并在性能上优于现有的静态图方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有最大独立集求解方法主要针对静态图，而实际应用中许多图是动态变化的，因此需要一种能适应动态变化的解决方案。

**方法:** 将图神经网络与分布式更新机制相结合，通过节点内部记忆的修改来推断最大独立集成员关系。

**结果:** 在合成和真实世界的大规模动态图上表现出色，尤其在大型图上显著优于其他方法，同时保持良好的泛化能力。

**结论:** 本研究展示了如何利用无监督学习解决动态图中的最大独立集问题，未来可进一步探索更复杂的图结构和更大规模的数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Finding+Maximum+Independent+Sets+in+Dynamic+Graphs+using+Unsupervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13754，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13754&send_immediately=true&force_search=false)

**原文摘要:** We present the first unsupervised learning model for finding Maximum
Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our
method combines structural learning from graph neural networks (GNNs) with a
learned distributed update mechanism that, given an edge addition or deletion
event, modifies nodes' internal memories and infers their MaxIS membership in a
single, parallel step. We parameterize our model by the update mechanism's
radius and investigate the resulting performance-runtime tradeoffs for various
dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS
methods for static graphs, including a mixed integer programming solver,
deterministic rule-based algorithms, and a heuristic learning framework based
on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs
of 100-10,000 nodes, our model achieves competitive approximation ratios with
excellent scalability; on large graphs, it significantly outperforms the
state-of-the-art heuristic learning framework in solution quality, runtime, and
memory usage. Our model generalizes well on graphs 100x larger than the ones
used for training, achieving performance at par with both a greedy technique
and a commercial mixed integer programming solver while running 1.5-23x faster
than greedy.

</details>


### [49] [WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection](https://arxiv.org/abs/2505.13765)
*Hainan Xu, Vladimir Bataev, Lilit Grigoryan, Boris Ginsburg*

**主要类别:** cs.LG

**概要:** 提出了一种名为WIND的新策略，显著加速了RNN-T推理而不影响模型准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RNN-T推理方法效率较低，需要改进。

**方法:** WIND策略，在推理过程中处理多个帧而不是顺序处理帧。

**结果:** 在多种数据集上的实验表明，贪心模式下速度提高了2.4倍，词错误率（WER）性能相同。

**结论:** WIND可以显著提高RNN-T推理的速度，同时保持相同的准确率，并且开源实现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WIND%3A+Accelerated+RNN-T+Decoding+with+Windowed+Inference+for+Non-blank+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13765&send_immediately=true&force_search=false)

**原文摘要:** We propose Windowed Inference for Non-blank Detection (WIND), a novel
strategy that significantly accelerates RNN-T inference without compromising
model accuracy. During model inference, instead of processing frames
sequentially, WIND processes multiple frames simultaneously within a window in
parallel, allowing the model to quickly locate non-blank predictions during
decoding, resulting in significant speed-ups. We implement WIND for greedy
decoding, batched greedy decoding with label-looping techniques, and also
propose a novel beam-search decoding method. Experiments on multiple datasets
with different conditions show that our method, when operating in greedy modes,
speeds up as much as 2.4X compared to the baseline sequential approach while
maintaining identical Word Error Rate (WER) performance. Our beam-search
algorithm achieves slightly better accuracy than alternative methods, with
significantly improved speed. We will open-source our WIND implementation.

</details>


### [50] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775)
*Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, Subbarao Kambhampati*

**主要类别:** cs.LG

**概要:** This paper challenges the interpretation that large reasoning models' success is due to Chain of Thought (CoT) by investigating the impact of intermediate token semantics on model performance.


<details>
  <summary>更多</summary>
  
**动机:** To critically examine the claim that training on CoTs sampled from base LLMs helps find new reasoning patterns.

**方法:** Training transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver.

**结果:** Models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. Training on noisy, corrupted traces shows performance remains consistent or improves in some cases.

**结论:** Trace accuracy is only loosely connected to solution accuracy, challenging the assumption that intermediate tokens or 'Chains of Thought' induce predictable reasoning behaviors.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Semantics%3A+The+Unreasonable+Effectiveness+of+Reasonless+Intermediate+Tokens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13775&send_immediately=true&force_search=false)

**原文摘要:** Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [51] [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787)
*Chris Cundy, Adam Gleave*

**主要类别:** cs.LG

**概要:** This paper investigates how integrating lie detectors into the training of large language models affects their honesty. It finds that while lie detectors can sometimes encourage honesty, they can also lead to deceptive behaviors that are harder to detect.


<details>
  <summary>更多</summary>
  
**动机:** To understand the impact of using lie detectors in the training process of AI systems on their deceptive behaviors.

**方法:** Incorporating a lie detector into the labeling step of LLM post-training and evaluating the learned policy's honesty.

**结果:** Preference learning with lie detectors and GRPO can result in deceptive policies with over 85% deception rates. High lie detector true positive rate or KL regularization can lead to honest policies. Off-policy algorithms consistently lead to lower deception rates.

**结论:** The use of lie detectors in training can either promote honesty or encourage undetectable misalignment depending on the context.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preference+Learning+with+Lie+Detectors+can+Induce+Honesty+or+Evasion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13787，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13787&send_immediately=true&force_search=false)

**原文摘要:** As AI systems become more capable, deceptive behaviors can undermine
evaluation and mislead users at deployment. Recent work has shown that lie
detectors can accurately classify deceptive behavior, but they are not
typically used in the training pipeline due to concerns around contamination
and objective hacking. We examine these concerns by incorporating a lie
detector into the labelling step of LLM post-training and evaluating whether
the learned policy is genuinely more honest, or instead learns to fool the lie
detector while remaining deceptive. Using DolusChat, a novel 65k-example
dataset with paired truthful/deceptive responses, we identify three key factors
that determine the honesty of learned policies: amount of exploration during
preference learning, lie detector accuracy, and KL regularization strength. We
find that preference learning with lie detectors and GRPO can lead to policies
which evade lie detectors, with deception rates of over 85\%. However, if the
lie detector true positive rate (TPR) or KL regularization is sufficiently
high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)
consistently lead to deception rates under 25\% for realistic TPRs. Our results
illustrate a more complex picture than previously assumed: depending on the
context, lie-detector-enhanced training can be a powerful tool for scalable
oversight, or a counterproductive method encouraging undetectable misalignment.

</details>


### [52] [Scalable Autoregressive 3D Molecule Generation](https://arxiv.org/abs/2505.13791)
*Austin H. Cheng, Chong Sun, Alán Aspuru-Guzik*

**主要类别:** cs.LG

**概要:** 提出了一种名为Quetzal的简单且可扩展的自回归模型，用于逐原子在3D空间生成分子。该模型在生成质量和速度上都有显著提升，并且可以处理可变大小的任务。


<details>
  <summary>更多</summary>
  
**动机:** 当前3D分子生成领域扩散模型占主导地位，而自回归模型落后。希望通过研究推动3D分子生成模型的可扩展性和通用性。

**方法:** Quetzal模型结合了因果变换器和小规模扩散MLP，因果变换器预测下一个原子的离散类型，扩散MLP建模连续的下一个位置分布。

**结果:** Quetzal在生成质量上有显著改进，与最先进的扩散模型性能相当，并且生成速度更快，还可以精确计算基于发散的似然值。

**结论:** Quetzal展示了自回归模型在3D分子生成中的潜力，并且能够处理变量大小的任务，希望激励更多关于生成模型可扩展性和通用性的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Autoregressive+3D+Molecule+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13791&send_immediately=true&force_search=false)

**原文摘要:** Generative models of 3D molecular structure play a rapidly growing role in
the design and simulation of molecules. Diffusion models currently dominate the
space of 3D molecule generation, while autoregressive models have trailed
behind. In this work, we present Quetzal, a simple but scalable autoregressive
model that builds molecules atom-by-atom in 3D. Treating each molecule as an
ordered sequence of atoms, Quetzal combines a causal transformer that predicts
the next atom's discrete type with a smaller Diffusion MLP that models the
continuous next-position distribution. Compared to existing autoregressive
baselines, Quetzal achieves substantial improvements in generation quality and
is competitive with the performance of state-of-the-art diffusion models. In
addition, by reducing the number of expensive forward passes through a dense
transformer, Quetzal enables significantly faster generation speed, as well as
exact divergence-based likelihood computation. Finally, without any
architectural changes, Quetzal natively handles variable-size tasks like
hydrogen decoration and scaffold completion. We hope that our work motivates a
perspective on scalability and generality for generative modelling of 3D
molecules.

</details>


### [53] [Context-Free Synthetic Data Mitigates Forgetting](https://arxiv.org/abs/2505.13811)
*Parikshit Bansal, Sujay Sanghavi*

**主要类别:** cs.LG

**概要:** This paper investigates mitigating catastrophic forgetting during fine-tuning of language models without access to training data.


<details>
  <summary>更多</summary>
  
**动机:** To address the issue of performance degradation on other tasks when fine-tuning a language model.

**方法:** Penalizing the KL divergence between the original and new model by using context-free generation.

**结果:** Augmenting fine-tuning datasets with context-free generations reduces forgetting in both zero-shot and reasoning performance settings.

**结论:** Context-free generation is an effective method to mitigate catastrophic forgetting.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context-Free+Synthetic+Data+Mitigates+Forgetting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13811，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13811&send_immediately=true&force_search=false)

**原文摘要:** Fine-tuning a language model often results in a degradation of its existing
performance on other tasks, due to a shift in the model parameters; this
phenomenon is often referred to as (catastrophic) forgetting. We are interested
in mitigating this, in settings where we only have access to the model weights
but no access to its training data/recipe. A natural approach is to penalize
the KL divergence between the original model and the new one. Our main
realization is that a simple process - which we term context-free generation -
allows for an approximate unbiased estimation of this KL divergence. We show
that augmenting a fine-tuning dataset with context-free generations mitigates
forgetting, in two settings: (a) preserving the zero-shot performance of
pretrained-only models, and (b) preserving the reasoning performance of
thinking models. We show that contextual synthetic data, and even a portion of
the pretraining data, are less effective. We also investigate the effect of
choices like generation temperature, data ratios etc. We present our results
for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the
reasoning setting.

</details>


### [54] [FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer](https://arxiv.org/abs/2505.13813)
*Matthew Raffel, Lizhong Chen*

**主要类别:** cs.LG

**概要:** This paper investigates why the Kolmogorov-Arnold Transformer (KAT) is significantly slower than expected despite having similar FLOPs to traditional Transformers. It identifies memory stalls, particularly in the backward pass of GR-KAN, as the main bottleneck and proposes FlashKAT, which improves training speed by 86.5x.


<details>
  <summary>更多</summary>
  
**动机:** To understand why KAT is slower than expected and find ways to improve its training speed.

**方法:** Conducting experiments to analyze the slowdown in KAT and proposing FlashKAT to address the identified memory bottleneck.

**结果:** FlashKAT achieves a training speedup of 86.5x over KAT and reduces rounding errors in coefficient gradients.

**结论:** FlashKAT successfully addresses the memory bottleneck in KAT, significantly improving its training speed.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlashKAT%3A+Understanding+and+Addressing+Performance+Bottlenecks+in+the+Kolmogorov-Arnold+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13813&send_immediately=true&force_search=false)

**原文摘要:** The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an
alternative to the multi-layer perceptron (MLP) with its increased
expressiveness and interpretability. However, the KAN can be orders of
magnitude slower due to its increased computational cost and training
instability, limiting its applicability to larger-scale tasks. Recently, the
Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs
similar to the traditional Transformer with MLPs by leveraging Group-Rational
KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our
characterizations reveal that the KAT is still 123x slower in training speeds,
indicating that there are other performance bottlenecks beyond FLOPs. In this
paper, we conduct a series of experiments to understand the root cause of the
slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls
and, more specifically, in the backward pass of GR-KAN caused by inefficient
gradient accumulation. To address this memory bottleneck, we propose FlashKAT,
which builds on our restructured kernel that minimizes gradient accumulation
with atomic adds and accesses to slow memory. Evaluations demonstrate that
FlashKAT can achieve a training speedup of 86.5x compared with the
state-of-the-art KAT, while reducing rounding errors in the coefficient
gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.

</details>


### [55] [Fragments to Facts: Partial-Information Fragment Inference from LLMs](https://arxiv.org/abs/2505.13819)
*Lucas Rosenblatt, Bin Han, Robert Wolfe, Bill Howe*

**主要类别:** cs.LG

**概要:** This paper investigates the vulnerability of fine-tuned large language models to fragment-specific extraction attacks when adversaries have only partial, unordered sample information. Two data-blind methods are proposed and shown to be competitive with a data-aware baseline.


<details>
  <summary>更多</summary>
  
**动机:** To explore how vulnerable large language models are to partial, unordered sample information under a weaker assumption.

**方法:** Propose two data-blind methods: (1) a likelihood ratio attack and (2) PRISM.

**结果:** Both methods are competitive with a data-aware baseline classifier in medical and legal settings.

**结论:** Fine-tuned LLMs are susceptible to fragment-specific extraction attacks under weaker assumptions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fragments+to+Facts%3A+Partial-Information+Fragment+Inference+from+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13819，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13819&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can leak sensitive training data through
memorization and membership inference attacks. Prior work has primarily focused
on strong adversarial assumptions, including attacker access to entire samples
or long, ordered prefixes, leaving open the question of how vulnerable LLMs are
when adversaries have only partial, unordered sample information. For example,
if an attacker knows a patient has "hypertension," under what conditions can
they query a model fine-tuned on patient data to learn the patient also has
"osteoarthritis?" In this paper, we introduce a more general threat model under
this weaker assumption and show that fine-tuned LLMs are susceptible to these
fragment-specific extraction attacks. To systematically investigate these
attacks, we propose two data-blind methods: (1) a likelihood ratio attack
inspired by methods from membership inference, and (2) a novel approach, PRISM,
which regularizes the ratio by leveraging an external prior. Using examples
from both medical and legal settings, we show that both methods are competitive
with a data-aware baseline classifier that assumes access to labeled
in-distribution data, underscoring their robustness.

</details>


### [56] [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
*Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架用于压缩基于大型语言模型的代理模型，同时保持其性能，实验结果表明该方法优于现有技术。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型作为决策代理具有强大的能力，但由于高推理成本和大模型尺寸限制了其实际部署。

**方法:** 结构化代理蒸馏框架，该框架将轨迹分割为{[REASON]}和{[ACT]}片段，并应用片段特定损失来对齐每个组件的行为。

**结果:** 在ALFWorld、HotPotQA-ReAct和WebShop上的实验显示，该方法始终优于标记级和模仿学习基线方法，在高效和可部署代理方面取得了重要进展。

**结论:** 提出的方法在压缩大型LLM代理模型的同时，保持了推理保真度和动作一致性。实验表明，该方法在多个任务上优于其他基线方法，并且可以实现显著的模型压缩，性能下降极小。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Structured+Agent+Distillation+for+Large+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13820，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13820&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [57] [Rethink the Role of Deep Learning towards Large-scale Quantum Systems](https://arxiv.org/abs/2505.13852)
*Yusheng Zhao, Chi Zhang, Yuxuan Du*

**主要类别:** cs.LG

**概要:** 本研究系统地评估了深度学习模型在量子系统基态学习任务中的表现，并与传统机器学习方法进行了公平比较，发现深度学习模型并不总是优于传统机器学习模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于深度学习在量子系统基态特性刻画中的必要性和具体作用的研究尚不明确，因为之前的研究往往使用不同的或不切实际的量子资源构建数据集，导致不公平的对比。

**方法:** 对三种哈密顿量族下的深度学习模型和传统机器学习方法进行基准测试，规模扩展到127个量子比特，并确保等量的量子资源使用。

**结果:** 机器学习模型在所有任务中通常能达到与深度学习模型相当甚至更好的性能；随机化测试表明测量输入特征对深度学习模型预测性能的影响很小。

**结论:** 深度学习模型并非许多量子系统学习场景中的必要选择，这些发现为有效利用深度学习提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethink+the+Role+of+Deep+Learning+towards+Large-scale+Quantum+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13852&send_immediately=true&force_search=false)

**原文摘要:** Characterizing the ground state properties of quantum systems is fundamental
to capturing their behavior but computationally challenging. Recent advances in
AI have introduced novel approaches, with diverse machine learning (ML) and
deep learning (DL) models proposed for this purpose. However, the necessity and
specific role of DL models in these tasks remain unclear, as prior studies
often employ varied or impractical quantum resources to construct datasets,
resulting in unfair comparisons. To address this, we systematically benchmark
DL models against traditional ML approaches across three families of
Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning
tasks while enforcing equivalent quantum resource usage. Our results reveal
that ML models often achieve performance comparable to or even exceeding that
of DL approaches across all tasks. Furthermore, a randomization test
demonstrates that measurement input features have minimal impact on DL models'
prediction performance. These findings challenge the necessity of current DL
models in many quantum system learning scenarios and provide valuable insights
into their effective utilization.

</details>


### [58] [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](https://arxiv.org/abs/2505.13857)
*Tian Sun, Yuqi Chen, Baihua Zheng, Weiwei Sun*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法TedTrajRec来提高GPS轨迹的采样率，该方法通过引入PD-GNN和TedFormer分别捕获时空交通动力学和轨迹动力学。


<details>
  <summary>更多</summary>
  
**动机:** 现有的序列到序列框架未能充分利用轨迹和道路网络中存在的复杂时空动力学。

**方法:** 提出了一种新的方法TedTrajRec，包括PD-GNN和TedFormer，分别用于捕获不同的时空动力学。

**结果:** 在三个真实数据集上的大量实验验证了TedTrajRec的优越性能。

**结论:** TedTrajRec在提高GPS轨迹采样率方面表现出色，并且代码公开可获取。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Spatio-Temporal+Dynamics+for+Trajectory+Recovery+via+Time-Aware+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13857&send_immediately=true&force_search=false)

**原文摘要:** In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.

</details>


### [59] [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://arxiv.org/abs/2505.13858)
*Gonzalo E. Constante-Flores, Hao Chen, Can Li*

**主要类别:** cs.LG

**概要:** 提出了一种模型不可知框架来强制执行神经网络输出的输入相关线性等式和不等式约束。在保持高精度的同时，确保预测满足硬约束条件，并且在训练和推理过程中没有迭代过程或运行时优化。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习模型越来越多地部署在安全关键任务中，需要满足硬约束条件，如物理定律、公平性要求或安全限制。然而，标准架构缺乏内置机制来强制执行这些约束。

**方法:** 结合了一个以预测准确度为目标的任务网络和一个使用随机和鲁棒优化文献中的决策规则来确保可行性的安全网络。最终预测是两个子网络的凸组合。

**结果:** 证明了该架构是一致函数的通用逼近器，并基于线性决策规则推导出可计算的公式。在基准回归任务上的实证结果显示，该方法能够一致地满足约束条件，同时保持竞争力的准确性和低推理延迟。

**结论:** 提出的框架是一种通用的方法，能够在各种任务中强制执行复杂的硬约束条件，而不会显著影响模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enforcing+Hard+Linear+Constraints+in+Deep+Learning+Models+with+Decision+Rules，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13858，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13858&send_immediately=true&force_search=false)

**原文摘要:** Deep learning models are increasingly deployed in safety-critical tasks where
predictions must satisfy hard constraints, such as physical laws, fairness
requirements, or safety limits. However, standard architectures lack built-in
mechanisms to enforce such constraints, and existing approaches based on
regularization or projection are often limited to simple constraints,
computationally expensive, or lack feasibility guarantees. This paper proposes
a model-agnostic framework for enforcing input-dependent linear equality and
inequality constraints on neural network outputs. The architecture combines a
task network trained for prediction accuracy with a safe network trained using
decision rules from the stochastic and robust optimization literature to ensure
feasibility across the entire input space. The final prediction is a convex
combination of the two subnetworks, guaranteeing constraint satisfaction during
both training and inference without iterative procedures or runtime
optimization. We prove that the architecture is a universal approximator of
constrained functions and derive computationally tractable formulations based
on linear decision rules. Empirical results on benchmark regression tasks show
that our method consistently satisfies constraints while maintaining
competitive accuracy and low inference latency.

</details>


### [60] [Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model](https://arxiv.org/abs/2505.13873)
*Peisong Niu, Ziqing Ma, Tian Zhou, Weiqi Chen, Lefei Shen, Rong Jin, Liang Sun*

**主要类别:** cs.LG

**概要:** This paper addresses overfitting in AI-based weather forecasting by introducing a novel model called Baguan, which uses a pre-trained Siamese Autoencoder to improve accuracy and performance across various forecasting tasks.


<details>
  <summary>更多</summary>
  
**动机:** To overcome the challenge of overfitting due to limited real-world weather data in AI-based weather forecasting.

**方法:** Developing a new model called Baguan using a Siamese Autoencoder pre-trained in a self-supervised manner and fine-tuned for different lead times.

**结果:** Baguan outperforms traditional methods in delivering accurate forecasts and demonstrates robust overfitting control in downstream tasks.

**结论:** Pre-training methods can effectively mitigate overfitting and enhance performance in weather forecasting.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Utilizing+Strategic+Pre-training+to+Reduce+Overfitting%3A+Baguan+--+A+Pre-trained+Weather+Forecasting+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13873，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13873&send_immediately=true&force_search=false)

**原文摘要:** Weather forecasting has long posed a significant challenge for humanity.
While recent AI-based models have surpassed traditional numerical weather
prediction (NWP) methods in global forecasting tasks, overfitting remains a
critical issue due to the limited availability of real-world weather data
spanning only a few decades. Unlike fields like computer vision or natural
language processing, where data abundance can mitigate overfitting, weather
forecasting demands innovative strategies to address this challenge with
existing data. In this paper, we explore pre-training methods for weather
forecasting, finding that selecting an appropriately challenging pre-training
task introduces locality bias, effectively mitigating overfitting and enhancing
performance. We introduce Baguan, a novel data-driven model for medium-range
weather forecasting, built on a Siamese Autoencoder pre-trained in a
self-supervised manner and fine-tuned for different lead times. Experimental
results show that Baguan outperforms traditional methods, delivering more
accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust
overfitting control and excels in downstream tasks, such as
subseasonal-to-seasonal (S2S) modeling and regional forecasting, after
fine-tuning.

</details>


### [61] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
*Yanggan Gu, Zhaoyi Yan, Yuanyi Wang, Yiming Zhang, Qi Zhou, Fei Wu, Hongxia Yang*

**主要类别:** cs.LG

**概要:** 提出了一种新的偏好优化方法InfiFPO，用于隐式模型融合，显著提升了多语言模型在数学、编码和推理任务上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有模型融合方法主要集中在有监督微调上，对提升LLM性能至关重要的偏好对齐阶段研究较少。

**方法:** InfiFPO用融合后的源模型替换直接偏好优化中的参考模型，并引入概率剪切和最大边距融合策略。

**结果:** InfiFPO在11个常用基准测试中始终优于现有的模型融合和偏好优化方法。

**结论:** InfiFPO通过有效整合多个源模型的知识，显著提高了Phi-4在数学、编码和推理任务上的平均性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InfiFPO%3A+Implicit+Model+Fusion+via+Preference+Optimization+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13878&send_immediately=true&force_search=false)

**原文摘要:** Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [62] [CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness](https://arxiv.org/abs/2505.13896)
*Yingwei Zhang, Ke Bu, Zhuoran Zhuang, Tao Xie, Yao Yu, Dong Li, Yang Guo, Detao Lv*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的时间序列预测方法CRAFT，该方法通过利用交叉未来行为的趋势来提高预测准确性，并在实际数据集上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列预测通常受到有限过去观测值的未来数据不确定性困境的限制。为了解决这个问题，研究探索了在时间序列预测中使用交叉未来行为(CFB)。

**方法:** CRAFT方法利用交叉未来行为的趋势来挖掘要预测的时间序列数据的趋势。具体来说，它通过Koopman预测模块提取关键趋势，并通过内部趋势挖掘模块补充交叉未来行为矩阵的未知区域。然后，引入具有层次结构的外部趋势引导模块以从更高层次获取更具代表性的趋势。最后，应用需求约束损失校正预测结果的分布偏差。

**结果:** CRAFT在离线大规模数据集和在线A/B测试中均表现出有效性。

**结论:** 实验结果表明CRAFT在离线大规模数据集和在线A/B测试中都表现出了有效性。我们的数据集和代码可在https://github.com/CRAFTinTSF/CRAFT获取。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CRAFT%3A+Time+Series+Forecasting+with+Cross-Future+Behavior+Awareness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13896，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13896&send_immediately=true&force_search=false)

**原文摘要:** The past decades witness the significant advancements in time series
forecasting (TSF) across various real-world domains, including e-commerce and
disease spread prediction. However, TSF is usually constrained by the
uncertainty dilemma of predicting future data with limited past observations.
To settle this question, we explore the use of Cross-Future Behavior (CFB) in
TSF, which occurs before the current time but takes effect in the future. We
leverage CFB features and propose the CRoss-Future Behavior Awareness based
Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize
the trend of cross-future behavior to mine the trend of time series data to be
predicted. Specifically, to settle the sparse and partial flaws of cross-future
behavior, CRAFT employs the Koopman Predictor Module to extract the key trend
and the Internal Trend Mining Module to supplement the unknown area of the
cross-future behavior matrix. Then, we introduce the External Trend Guide
Module with a hierarchical structure to acquire more representative trends from
higher levels. Finally, we apply the demand-constrained loss to calibrate the
distribution deviation of prediction results. We conduct experiments on
real-world dataset. Experiments on both offline large-scale dataset and online
A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is
available at https://github.com/CRAFTinTSF/CRAFT.

</details>


### [63] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)
*Róbert Csordás, Christopher D. Manning, Christopher Potts*

**主要类别:** cs.LG

**概要:** This paper analyzes the efficiency of depth in large language models (LLMs) by examining the residual streams of Llama 3.1 and Qwen 3 models.


<details>
  <summary>更多</summary>
  
**动机:** To determine if deeper models efficiently utilize their depth to perform higher-order computations or just spread the same computations over more layers.

**方法:** Analyzing the residual stream of the models, comparing sublayer outputs, skipping layers, and training linear maps between shallow and deep models.

**结果:** Found that later layers contribute less, skipping layers in the second half has minimal impact, and there's no evidence of composing subresults for multihop tasks. Linear maps suggest deeper models spread the same computations over more layers.

**结论:** Deeper models do not use their depth to learn new computations but rather for finer adjustments to the residual stream.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+Language+Models+Use+Their+Depth+Efficiently%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13898&send_immediately=true&force_search=false)

**原文摘要:** Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [64] [Exploring Causes of Representational Similarity in Machine Learning Models](https://arxiv.org/abs/2505.13899)
*Zeyu Michael Li, Hung Anh Vu, Damilola Awofisayo, Emily Wenger*

**主要类别:** cs.LG

**概要:** This study investigates the causes of similarity in machine learning model representations, focusing on dataset overlap and task overlap as potential causal factors.


<details>
  <summary>更多</summary>
  
**动机:** To understand why machine learning models across different modalities represent the world similarly, despite differences in training data and tasks.

**方法:** Experiments were conducted to evaluate the impact of dataset overlap and task overlap on model similarity.

**结果:** Both dataset overlap and task overlap positively correlate with higher representational similarity, and their combination has the strongest effect.

**结论:** The findings suggest that dataset and task overlap are significant contributors to model representation similarity.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Causes+of+Representational+Similarity+in+Machine+Learning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13899&send_immediately=true&force_search=false)

**原文摘要:** Numerous works have noted significant similarities in how machine learning
models represent the world, even across modalities. Although much effort has
been devoted to uncovering properties and metrics on which these models align,
surprisingly little work has explored causes of this similarity. To advance
this line of inquiry, this work explores how two possible causal factors --
dataset overlap and task overlap -- influence downstream model similarity. The
exploration of dataset overlap is motivated by the reality that large-scale
generative AI models are often trained on overlapping datasets of scraped
internet data, while the exploration of task overlap seeks to substantiate
claims from a recent work, the Platonic Representation Hypothesis, that task
similarity may drive model similarity. We evaluate the effects of both factors
through a broad set of experiments. We find that both positively correlate with
higher representational similarity and that combining them provides the
strongest effect. Our code and dataset are published.

</details>


### [65] [New Evidence of the Two-Phase Learning Dynamics of Neural Networks](https://arxiv.org/abs/2505.13900)
*Zhanpeng Zhou, Yongyi Yang, Mahito Sugiyama, Junchi Yan*

**主要类别:** cs.LG

**概要:** This paper explores the training dynamics of deep neural networks and identifies two new phenomena: The Chaos Effect and The Cone Effect, which reveal the two-phase nature of deep learning.


<details>
  <summary>更多</summary>
  
**动机:** To understand how deep neural networks learn and to gain insights into the training dynamics during the distinct phase transition.

**方法:** Introducing an interval-wise perspective to compare network states across a time window and analyzing the response of the network to small parameter perturbations as well as the evolution of the empirical Neural Tangent Kernel (eNTK).

**结果:** Identified two new phenomena: The Chaos Effect, where the network response transitions from chaotic to stable, indicating a critical period of high sensitivity to initial conditions, and The Cone Effect, where the model's functional trajectory is confined to a narrow cone-shaped subset after the transition point.

**结论:** The findings provide a structural and dynamical view of how deep networks transition from sensitive exploration to stable refinement during training.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是New+Evidence+of+the+Two-Phase+Learning+Dynamics+of+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13900，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13900&send_immediately=true&force_search=false)

**原文摘要:** Understanding how deep neural networks learn remains a fundamental challenge
in modern machine learning. A growing body of evidence suggests that training
dynamics undergo a distinct phase transition, yet our understanding of this
transition is still incomplete. In this paper, we introduce an interval-wise
perspective that compares network states across a time window, revealing two
new phenomena that illuminate the two-phase nature of deep learning. i)
\textbf{The Chaos Effect.} By injecting an imperceptibly small parameter
perturbation at various stages, we show that the response of the network to the
perturbation exhibits a transition from chaotic to stable, suggesting there is
an early critical period where the network is highly sensitive to initial
conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the
empirical Neural Tangent Kernel (eNTK), we find that after this transition
point the model's functional trajectory is confined to a narrow cone-shaped
subset: while the kernel continues to change, it gets trapped into a tight
angular region. Together, these effects provide a structural, dynamical view of
how deep networks transition from sensitive exploration to stable refinement
during training.

</details>


### [66] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/abs/2505.13904)
*Fu Luo, Xi Lin, Mengyuan Zhong, Fei Liu, Zhenkun Wang, Jianyong Sun, Qingfu Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种基于插入范式的新型学习方法L2C-Insert，用于解决车辆路径问题，相比传统添加方式，该方法在多个问题规模上展示了优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有构造性神经组合优化方法通常采用基于追加的范式，这种刚性方法常导致次优解。

**方法:** 提出了一种新的基于插入范式的构造性神经组合优化方法L2C-Insert，包括预测精确插入位置的新模型架构、有效的模型优化训练方案和充分利用插入灵活性的先进推理技术。

**结果:** 在合成数据和真实世界的旅行商问题(TSP)和容量约束车辆路径问题(CVRP)实例上进行了广泛的实验，证明了L2C-Insert在各种问题规模上始终具有卓越的性能。

**结论:** 所提出的L2C-Insert方法通过增强灵活性和解决方案质量，克服了传统方法的局限性，并在多个问题规模上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Insert+for+Constructive+Neural+Vehicle+Routing+Solver，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13904，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13904&send_immediately=true&force_search=false)

**原文摘要:** Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [67] [Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval](https://arxiv.org/abs/2505.13907)
*Junyu Luo, Yusheng Zhao, Xiao Luo, Zhiping Xiao, Wei Ju, Li Shen, Dacheng Tao, Ming Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种新的跨域扩散与逐步对齐方法(COUPLE)，从图扩散的角度重新审视无监督高效领域自适应检索问题，通过构建跨域关系图并利用噪声鲁棒图流扩散来模拟源域到目标域的转移动态，识别低噪声簇，并采用分层Mixup操作进行逐步领域对齐，从而实现有效的领域自适应哈希学习。实验表明COUPLE在竞争性基准上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法通常无法解决目标域中的潜在噪声问题，直接对齐高级特征会导致检索性能不佳。

**方法:** 提出了COUPLE方法，包括构建跨域关系图、噪声鲁棒图流扩散、判别哈希码学习以及分层Mixup操作进行逐步领域对齐。

**结果:** COUPLE在竞争性基准上表现出色。

**结论:** COUPLE是一种有效的无监督高效领域自适应检索方法，能够有效应对目标域中的噪声问题并提高检索性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Domain+Diffusion+with+Progressive+Alignment+for+Efficient+Adaptive+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13907&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised efficient domain adaptive retrieval aims to transfer knowledge
from a labeled source domain to an unlabeled target domain, while maintaining
low storage cost and high retrieval efficiency. However, existing methods
typically fail to address potential noise in the target domain, and directly
align high-level features across domains, thus resulting in suboptimal
retrieval performance. To address these challenges, we propose a novel
Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This
approach revisits unsupervised efficient domain adaptive retrieval from a graph
diffusion perspective, simulating cross-domain adaptation dynamics to achieve a
stable target domain adaptation process. First, we construct a cross-domain
relationship graph and leverage noise-robust graph flow diffusion to simulate
the transfer dynamics from the source domain to the target domain, identifying
lower noise clusters. We then leverage the graph diffusion results for
discriminative hash code learning, effectively learning from the target domain
while reducing the negative impact of noise. Furthermore, we employ a
hierarchical Mixup operation for progressive domain alignment, which is
performed along the cross-domain random walk paths. Utilizing target domain
discriminative hash learning and progressive domain alignment, COUPLE enables
effective domain adaptive hash learning. Extensive experiments demonstrate
COUPLE's effectiveness on competitive benchmarks.

</details>


### [68] [ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models](https://arxiv.org/abs/2505.13910)
*Guangtao Zheng, Wenqian Ye, Aidong Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种无需分组标签即可减轻深度学习模型中预测捷径的后验方法ShortcutProbe，该方法通过重新训练提高模型对无关特征的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的去偏方法依赖于昂贵的人工标注的分组标签，且难以捕捉微妙的预测偏差。

**方法:** 提出ShortcutProbe框架，通过在潜在空间识别预测捷径并重新训练模型使其对这些捷径不变来增强模型的鲁棒性。

**结果:** 该框架被证明在多种数据集上有效提高了模型对无关特征的鲁棒性。

**结论:** ShortcutProbe提供了一个高效实用的方法来减轻深度学习模型中的非稳健预测偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ShortcutProbe%3A+Probing+Prediction+Shortcuts+for+Learning+Robust+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13910，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13910&send_immediately=true&force_search=false)

**原文摘要:** Deep learning models often achieve high performance by inadvertently learning
spurious correlations between targets and non-essential features. For example,
an image classifier may identify an object via its background that spuriously
correlates with it. This prediction behavior, known as spurious bias, severely
degrades model performance on data that lacks the learned spurious
correlations. Existing methods on spurious bias mitigation typically require a
variety of data groups with spurious correlation annotations called group
labels. However, group labels require costly human annotations and often fail
to capture subtle spurious biases such as relying on specific pixels for
predictions. In this paper, we propose a novel post hoc spurious bias
mitigation framework without requiring group labels. Our framework, termed
ShortcutProbe, identifies prediction shortcuts that reflect potential
non-robustness in predictions in a given model's latent space. The model is
then retrained to be invariant to the identified prediction shortcuts for
improved robustness. We theoretically analyze the effectiveness of the
framework and empirically demonstrate that it is an efficient and practical
tool for improving a model's robustness to spurious bias on diverse datasets.

</details>


### [69] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934)
*Jialong Wu, Shaofeng Yin, Ningya Feng, Mingsheng Long*

**主要类别:** cs.LG

**概要:** 提出了一种名为RLVR-World的新框架，它结合了可验证奖励的强化学习方法来优化世界模型，从而在语言和视频领域取得显著性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 标准训练目标如最大似然估计（MLE）与世界模型的任务特定目标不一致，例如准确度或感知质量等过渡预测指标。

**方法:** 提出了RLVR-World框架，该框架利用带有可验证奖励的强化学习直接优化世界模型以适应特定任务指标。

**结果:** 在多种领域包括文本游戏、网页导航和机器人操作中，基于语言和视频的世界模型表现出了显著的性能提升。

**结论:** 除了最近在推理语言模型方面的进展外，RLVR为增强生成模型的实用性提供了一个有前景的后训练范式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RLVR-World%3A+Training+World+Models+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13934，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13934&send_immediately=true&force_search=false)

**原文摘要:** World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.

</details>


### [70] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/abs/2505.13938)
*Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzche, Greg Durrett, Yisong Yue, Swarat Chaudhuri*

**主要类别:** cs.LG

**概要:** Introduces CLEVER, a new benchmark for end-to-end verified code generation in Lean, consisting of 161 problems without test-case supervision or LLM-generated annotations.


<details>
  <summary>更多</summary>
  
**动机:** To create a high-quality benchmark for verified code generation that avoids issues like test-case supervision and LLM-generated annotations.

**方法:** Developed CLEVER with two tasks: generating a matching specification and a provably satisfying implementation, verified using Lean's type checker.

**结果:** Evaluated several few-shot and agentic approaches based on state-of-the-art language models, which struggled to achieve full verification.

**结论:** Establishes CLEVER as a challenging frontier benchmark for program synthesis and formal reasoning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLEVER%3A+A+Curated+Benchmark+for+Formally+Verified+Code+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13938，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13938&send_immediately=true&force_search=false)

**原文摘要:** We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [71] [VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction](https://arxiv.org/abs/2505.13954)
*Jiahe Chen, Ziye Ma*

**主要类别:** cs.LG

**概要:** Introduce VAMO, a hybrid optimizer combining first-order and zeroth-order methods, achieving better convergence rates than both traditional methods and existing first-order or zeroth-order optimizers.


<details>
  <summary>更多</summary>
  
**动机:** Balancing rapid convergence with computational efficiency in large-scale nonconvex optimization problems.

**方法:** Combining first-order mini-batch gradients with zeroth-order finite-difference probes under an SVRG-style framework.

**结果:** VAMO achieves a dimension-agnostic convergence rate of O(1/T + 1/b), surpassing purely ZO methods and improving over SGD's rate.

**结论:** VAMO, a variance-reduced mixed-gradient optimizer, combines first-order and zeroth-order methods to improve convergence and computational efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VAMO%3A+Efficient+Large-Scale+Nonconvex+Optimization+via+Adaptive+Zeroth+Order+Variance+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13954&send_immediately=true&force_search=false)

**原文摘要:** Optimizing large-scale nonconvex problems, common in machine learning,
demands balancing rapid convergence with computational efficiency. First-order
(FO) stochastic methods like SVRG provide fast convergence and good
generalization but incur high costs due to full-batch gradients in large
models. Conversely, zeroth-order (ZO) algorithms reduce this burden using
estimated gradients, yet their slow convergence in high-dimensional settings
limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient
Optimizer), a stochastic variance-reduced method combining FO mini-batch
gradients with lightweight ZO finite-difference probes under an SVRG-style
framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a
dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is
the number of iterations and $b$ is the batch-size, surpassing the
dimension-dependent slowdown of purely ZO methods and significantly improving
over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a
multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of
estimation points to balance convergence and cost, making it ideal for a whole
range of computationally constrained scenarios. Experiments including
traditional neural network training and LLM finetuning show VAMO outperforms
established FO and ZO methods, offering a faster, more flexible option for
improved efficiency.

</details>


### [72] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/abs/2505.13989)
*Yanzhe Wen, Xunkai Li, Qi Zhang, Zhu Lei, Guang Zeng, Rong-Hua Li, Guoren Wang*

**主要类别:** cs.LG

**概要:** 提出了一种基于LLM的框架OGA，用于处理开放世界场景中的数据不确定性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在处理开放世界场景中的数据不确定性时表现不足，特别是对于有限标注和未知类别节点的情况。

**方法:** 提出了OGA框架，结合了自适应标签追踪和图标签注释器。

**结果:** 实验表明OGA的有效性和实用性。

**结论:** OGA为开放世界场景下的图学习提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+LLMs+meet+open-world+graph+learning%3A+a+new+perspective+for+unlabeled+data+uncertainty，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13989，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13989&send_immediately=true&force_search=false)

**原文摘要:** Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


### [73] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/abs/2505.14005)
*Han Zhang, Yan Wang, Guanfeng Liu, Pengfei Ding, Huaxiong Wang, Kwok-Yan Lam*

**主要类别:** cs.LG

**概要:** 提出了一种新的图神经网络解释方法OPEN，解决了现有方法在捕获完整决策逻辑和对边属性及内部可访问性要求严格的问题。


<details>
  <summary>更多</summary>
  
**动机:** 增强图神经网络的可靠性和可信度，提高其决策逻辑的透明度。

**方法:** 提出OPEN方法，将样本空间划分为多个环境，并从每个环境中采样子图来分析预测。

**结果:** OPEN方法在保持效率的同时，在保真度上优于最先进的方法，并增强了实际场景中的鲁棒性。

**结论:** OPEN方法能够捕获几乎完整的图神经网络决策逻辑，解决了现有方法的一些重大限制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Comprehensive+and+Prerequisite-Free+Explainer+for+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14005&send_immediately=true&force_search=false)

**原文摘要:** To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [74] [Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability](https://arxiv.org/abs/2505.14011)
*Yifei Jin, Xin Zheng, Lei Guo*

**主要类别:** cs.LG

**概要:** 提出了一种具有法律解释性的新型司法量刑预测模型（SMS）及其自适应算法（MLMS），并在真实数据上验证了其高预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有司法量刑预测研究多依赖端到端模型，这些模型忽视了固有的量刑逻辑且缺乏可解释性，这对学术研究和司法实践都是一个挑战。

**方法:** 提出Saturated Mechanistic Sentencing (SMS)模型及Momentum Least Mean Squares (MLMS)自适应算法，并建立无任何平稳性和独立性假设下的预测精度数学理论。

**结果:** 实验表明，基于真实世界数据的SMS模型与MLMS算法在预测准确性上接近最佳可能的理论上限。

**结论:** 提出的SMS模型和MLMS算法在司法量刑预测中表现出良好的预测准确性和法律解释性，通过构建的CIBH数据集验证了方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Sentencing+Prediction+with+Guaranteed+Accuracy+and+Legal+Interpretability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14011，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14011&send_immediately=true&force_search=false)

**原文摘要:** Existing research on judicial sentencing prediction predominantly relies on
end-to-end models, which often neglect the inherent sentencing logic and lack
interpretability-a critical requirement for both scholarly research and
judicial practice. To address this challenge, we make three key
contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS)
model, which provides inherent legal interpretability by virtue of its
foundation in China's Criminal Law. We also introduce the corresponding
Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second,
for the MLMS algorithm based adaptive sentencing predictor, we establish a
mathematical theory on the accuracy of adaptive prediction without resorting to
any stationarity and independence assumptions on the data. We also provide a
best possible upper bound for the prediction accuracy achievable by the best
predictor designed in the known parameters case. Third, we construct a Chinese
Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data,
extensive experiments demonstrate that our approach achieves a prediction
accuracy that is not far from the best possible theoretical upper bound,
validating both the model's suitability and the algorithm's accuracy.

</details>


### [75] [FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix](https://arxiv.org/abs/2505.14024)
*Di Wu, Qian Li, Heng Yang, Yong Han*

**主要类别:** cs.LG

**概要:** 本文提出了FedGraM，一种新的联邦学习防御方法，通过计算Gram矩阵范数来检测和移除恶意模型，有效提升了FL系统的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的FL防御机制在实际异构数据环境中效果有限，因此需要提出新的方法来检测和移除未针对攻击，减轻其影响。

**方法:** FedGraM通过维护辅助数据集提取嵌入向量，并利用Gram矩阵范数衡量模型的类间分离能力，从而识别并移除潜在恶意模型。

**结果:** FedGraM在广泛的实验中显示出卓越性能，特别是在有限样本条件下。

**结论:** FedGraM在有限样本情况下表现出色，优于现有先进防御方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedGraM%3A+Defending+Against+Untargeted+Attacks+in+Federated+Learning+via+Embedding+Gram+Matrix，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14024&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables geographically distributed clients to
collaboratively train machine learning models by sharing only their local
models, ensuring data privacy. However, FL is vulnerable to untargeted attacks
that aim to degrade the global model's performance on the underlying data
distribution. Existing defense mechanisms attempt to improve FL's resilience
against such attacks, but their effectiveness is limited in practical FL
environments due to data heterogeneity. On the contrary, we aim to detect and
remove the attacks to mitigate their impact. Generalization contribution plays
a crucial role in distinguishing untargeted attacks. Our observations indicate
that, with limited data, the divergence between embeddings representing
different classes provides a better measure of generalization than direct
accuracy. In light of this, we propose a novel robust aggregation method,
FedGraM, designed to defend against untargeted attacks in FL. The server
maintains an auxiliary dataset containing one sample per class to support
aggregation. This dataset is fed to the local models to extract embeddings.
Then, the server calculates the norm of the Gram Matrix of the embeddings for
each local model. The norm serves as an indicator of each model's inter-class
separation capability in the embedding space. FedGraM identifies and removes
potentially malicious models by filtering out those with the largest norms,
then averages the remaining local models to form the global model. We conduct
extensive experiments to evaluate the performance of FedGraM. Our empirical
results show that with limited data samples used to construct the auxiliary
dataset, FedGraM achieves exceptional performance, outperforming
state-of-the-art defense methods.

</details>


### [76] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/abs/2505.14033)
*Guoming Li, Jian Yang, Yifan Chen*

**主要类别:** cs.LG

**概要:** 提出了一种新的图神经网络过滤方法CPF，通过结构感知和特征感知的分区过滤来提高在同质性和异质性图上的分类性能，并且在基准节点分类实验和实际图异常检测应用中验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图神经网络过滤方法要么采用统一的图级过滤，要么针对每个节点分配不同的过滤器，但都存在局限性，缺乏对这两种策略的统一框架，同时在处理异质性图时表现不佳。

**方法:** 提出Coarsening-guided Partition-wise Filtering (CPF)，首先通过图简化算法得到节点分区并进行结构感知的分区过滤，然后通过特征感知的分区过滤利用k-means聚类生成的簇进一步优化节点嵌入。

**结果:** CPF在处理具有同质性和异质性的图分类任务上表现出色，并且在实际应用如图异常检测中也展现了实用价值。

**结论:** CPF提供了一个新颖的图神经网络过滤框架，解决了现有方法的不足，为理论研究提供了新的视角，并在多个任务中验证了其优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Partition-wise+Graph+Filtering%3A+A+Unified+Perspective+Through+the+Lens+of+Graph+Coarsening，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14033，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14033&send_immediately=true&force_search=false)

**原文摘要:** Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [77] [Adaptive Cyclic Diffusion for Inference Scaling](https://arxiv.org/abs/2505.14036)
*Gyubin Lee, Truong Nhat Nguyen Bao, Jaesik Yoon, Dongwoo Lee, Minsu Kim, Yoshua Bengio, Sungjin Ahn*

**主要类别:** cs.LG

**概要:** 提出了一种名为Adaptive Bi-directional Cyclic Diffusion (ABCD)的新方法，该方法在推理过程中动态调整计算努力，以适应实例难度或任务特定需求。实验表明，ABCD在保持计算效率的同时提高了各种任务的表现。


<details>
  <summary>更多</summary>
  
**动机:** 大多数推理时间扩展方法依赖固定的去噪计划，限制了其根据实例难度或任务特定需求自适应分配计算的能力。

**方法:** 提出了一种名为Adaptive Bi-directional Cyclic Diffusion (ABCD)的灵活的基于搜索的推理框架，通过双向扩散循环来优化输出，并自适应地控制探索深度和终止。ABCD包含三个组件：Cyclic Diffusion Search, Automatic Exploration-Exploitation Balancing, 和 Adaptive Thinking Time。

**结果:** 实验显示，ABCD在保持计算效率的同时提高了各种任务的表现。

**结论:** ABCD是一种灵活的推理框架，能够在推理过程中动态调整计算努力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Cyclic+Diffusion+for+Inference+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14036&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [78] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/abs/2505.06699)
*Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. Thai, Tianbao Yang*

**主要类别:** cs.LG

**概要:** This paper introduces a new learning paradigm called model steering and proposes a theory-driven framework named DRRho risk minimization to improve generalization and data efficiency.


<details>
  <summary>更多</summary>
  
**动机:** To understand the underlying principles of ad-hoc methods used in training large foundation models and provide theoretical insights for the new learning paradigm.

**方法:** Propose a theory-driven framework called DRRho risk minimization based on Distributionally Robust Optimization (DRO). Introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP.

**结果:** Theoretical insights are provided for the new learning paradigm, revealing a superior scaling law compared to CLIP without a reference model and demonstrating its strength over existing heuristic approaches.

**结论:** This work significantly enhances our understanding and practice of model steering by providing theoretical insights and introducing a novel method for CLIP with a reference model.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+Steering%3A+Learning+with+a+Reference+Model+Improves+Generalization+Bounds+and+Scaling+Laws，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.06699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.06699&send_immediately=true&force_search=false)

**原文摘要:** This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [79] [Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators](https://arxiv.org/abs/2505.14039)
*Luca Pellegrini, Massimiliano Ghiotto, Edoardo Centofanti, Luca Franco Pavarino*

**主要类别:** cs.LG

**概要:** 本研究探讨了Fourier神经算子是否能有效学习离子模型在高维动力系统中的所有状态变量演化。通过准确学习三个不同维度的离子模型的动力学，验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的离子模型模拟兴奋细胞复杂动力学具有挑战性，尤其是其刚性、多尺度非线性和多种动态行为。

**方法:** 使用Fourier神经算子来学习离子模型的状态变量演化，并通过自动超参数调整优化模型配置。

**结果:** 成功学习了三个离子模型的动力学，包括FitzHugh-Nagumo模型、Hodgkin-Huxley模型和O'Hara-Rudy模型，且两种架构在准确性上相当，但无约束架构收敛更快。

**结论:** Fourier神经算子能够有效捕捉高维动力系统的复杂多尺度动态。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+High-dimensional+Ionic+Model+Dynamics+Using+Fourier+Neural+Operators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14039&send_immediately=true&force_search=false)

**原文摘要:** Ionic models, described by systems of stiff ordinary differential equations,
are fundamental tools for simulating the complex dynamics of excitable cells in
both Computational Neuroscience and Cardiology. Approximating these models
using Artificial Neural Networks poses significant challenges due to their
inherent stiffness, multiscale nonlinearities, and the wide range of dynamical
behaviors they exhibit, including multiple equilibrium points, limit cycles,
and intricate interactions. While in previous studies the dynamics of the
transmembrane potential has been predicted in low dimensionality settings, in
the present study we extend these results by investigating whether Fourier
Neural Operators can effectively learn the evolution of all the state variables
within these dynamical systems in higher dimensions. We demonstrate the
effectiveness of this approach by accurately learning the dynamics of three
well-established ionic models with increasing dimensionality: the two-variable
FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the
forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal
configurations for the Fourier Neural Operator, we conducted automatic
hyperparameter tuning under two scenarios: an unconstrained setting, where the
number of trainable parameters is not limited, and a constrained case with a
fixed number of trainable parameters. Both constrained and unconstrained
architectures achieve comparable results in terms of accuracy across all the
models considered. However, the unconstrained architecture required
approximately half the number of training epochs to achieve similar error
levels, as evidenced by the loss function values recorded during training.
These results underline the capabilities of Fourier Neural Operators to
accurately capture complex multiscale dynamics, even in high-dimensional
dynamical systems.

</details>


### [80] [Unsupervised Graph Clustering with Deep Structural Entropy](https://arxiv.org/abs/2505.14040)
*Jingyun Zhang, Hao Peng, Li Sun, Guanlin Wu, Chunyang Liu, Zhengtao Yu*

**主要类别:** cs.LG

**概要:** 提出了一种新的无监督图聚类框架DeSE，通过量化结构信息和深度神经网络增强原始图结构，并设计了结构学习层和聚类分配方法，在四个基准数据集上与八种代表性方法对比，展示了DeSE在有效性和可解释性方面的优越性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于图神经网络等方法在处理稀疏或包含噪声边的图时性能下降，且传统聚类技术可能无法充分捕捉节点间的潜在图结构。

**方法:** 引入DeSE框架，包括计算软分配的结构熵方法、结构学习层(SLL)生成属性图、以及基于GNN的聚类分配方法(ASS)，可堆叠以满足下游任务需求。

**结果:** DeSE在四个基准数据集上的实验显示其在有效性与可解释性方面优于其他方法。

**结论:** DeSE通过增强图结构解决了现有方法的局限性，提供了更有效的无监督图聚类解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+Graph+Clustering+with+Deep+Structural+Entropy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14040&send_immediately=true&force_search=false)

**原文摘要:** Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too sparse or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
sparse connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.

</details>


### [81] [Generalized Category Discovery via Token Manifold Capacity Learning](https://arxiv.org/abs/2505.14044)
*Luyao Tang, Kunze Huang, Chaoqi Chen, Cheng Chen*

**主要类别:** cs.LG

**概要:** 提出一种新方法MTMC，通过最大化类标记的流形容量来提高聚类准确性和类别数估计，改善深度学习模型在开放世界场景中的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 传统GCD方法在最小化簇内变化的同时牺牲了流形容量，限制了类内表示的丰富性。

**方法:** 提出最大令牌流形容量(MTMC)，利用奇异值的核范数作为流形容量的度量，确保样本表示保持信息性和良好结构。

**结果:** 在粗粒度和细粒度数据集上的理论分析和大量实验表明，MTMC优于现有GCD方法，提高了聚类准确性和类别数估计。

**结论:** MTMC可生成更完整的表示，增强类间可分性，减少维度坍塌，成为稳健开放世界学习的重要组成部分。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Category+Discovery+via+Token+Manifold+Capacity+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14044&send_immediately=true&force_search=false)

**原文摘要:** Generalized category discovery (GCD) is essential for improving deep learning
models' robustness in open-world scenarios by clustering unlabeled data
containing both known and novel categories. Traditional GCD methods focus on
minimizing intra-cluster variations, often sacrificing manifold capacity, which
limits the richness of intra-class representations. In this paper, we propose a
novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes
maximizing the manifold capacity of class tokens to preserve the diversity and
complexity of data. MTMC leverages the nuclear norm of singular values as a
measure of manifold capacity, ensuring that the representation of samples
remains informative and well-structured. This method enhances the
discriminability of clusters, allowing the model to capture detailed semantic
features and avoid the loss of critical information during clustering. Through
theoretical analysis and extensive experiments on coarse- and fine-grained
datasets, we demonstrate that MTMC outperforms existing GCD methods, improving
both clustering accuracy and the estimation of category numbers. The
integration of MTMC leads to more complete representations, better inter-class
separability, and a reduction in dimensional collapse, establishing MTMC as a
vital component for robust open-world learning. Code is in
github.com/lytang63/MTMC.

</details>


### [82] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
*Woody Haosheng Gan, Deqing Fu, Julian Asilis, Ollie Liu, Dani Yogatama, Vatsal Sharan, Robin Jia, Willie Neiswanger*

**主要类别:** cs.LG

**概要:** 文本衍生的引导向量可以有效提升多模态大型语言模型在多种架构和视觉任务中的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 目前针对多模态大型语言模型的引导方法较少，且缺乏一致性技术。

**方法:** 使用稀疏自动编码器(SAEs)，均值漂移和线性探测从仅文本的LLM骨干中导出向量来引导多模态大型语言模型。

**结果:** 文本衍生的引导向量提升了多模态准确性，特别是在CV-Bench上的空间关系准确性和计数准确性。

**结论:** 文本衍生的引导向量是一种强大而高效的机制，能够增强多模态大型语言模型的定位能力，且无需额外的数据收集和计算开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Textual+Steering+Vectors+Can+Improve+Visual+Understanding+in+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14071，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14071&send_immediately=true&force_search=false)

**原文摘要:** Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [83] [Collaborative Unlabeled Data Optimization](https://arxiv.org/abs/2505.14117)
*Xinyi Shang, Peng Sun, Fengyuan Liu, Tao Lin*

**主要类别:** cs.LG

**概要:** 提出了一种新的数据为中心的方法CoOpt来优化未标记的数据，以提高深度学习训练的效率和可持续性。该方法通过分发未标记的数据并利用公共任务不可知模型来实现可扩展、可重用和可持续的训练管道。实验表明，CoOpt在Tiny-ImageNet和ImageNet-1K上分别提高了13.6%和6.8%，并且训练速度分别提升了1.94倍和1.2倍。


<details>
  <summary>更多</summary>
  
**动机:** 如何通过优化数据本身来增强深度学习训练的效率和可持续性？

**方法:** 提出了一个名为CoOpt的高度有效的并行化框架，用于协作未标记数据优化，并将知识有效编码到数据本身中。

**结果:** 在多个数据集和架构上的广泛实验显示了其有效性和效率，分别在Tiny-ImageNet和ImageNet-1K上实现了13.6%和6.8%的改进，训练速度分别提升了1.94倍和1.2倍。

**结论:** CoOpt提供了一个新的视角来解决现有模型中心方法的三个关键限制，通过将知识编码到数据本身中，实现了更高效、可扩展和可持续的深度学习训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Collaborative+Unlabeled+Data+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14117&send_immediately=true&force_search=false)

**原文摘要:** This paper pioneers a novel data-centric paradigm to maximize the utility of
unlabeled data, tackling a critical question: How can we enhance the efficiency
and sustainability of deep learning training by optimizing the data itself? We
begin by identifying three key limitations in existing model-centric
approaches, all rooted in a shared bottleneck: knowledge extracted from data is
locked to model parameters, hindering its reusability and scalability. To this
end, we propose CoOpt, a highly efficient, parallelized framework for
collaborative unlabeled data optimization, thereby effectively encoding
knowledge into the data itself. By distributing unlabeled data and leveraging
publicly available task-agnostic models, CoOpt facilitates scalable, reusable,
and sustainable training pipelines. Extensive experiments across diverse
datasets and architectures demonstrate its efficacy and efficiency, achieving
13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,
with training speedups of $1.94 \times $ and $1.2 \times$.

</details>


### [84] [Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors](https://arxiv.org/abs/2505.14122)
*Ehsan Masoudian, Ali Mirzaei, Hossein Bagheri*

**主要类别:** cs.LG

**概要:** This study examines the factors affecting wildfire risk in Iran, including climatic conditions and human activities, using remote sensing and GIS techniques.


<details>
  <summary>更多</summary>
  
**动机:** To investigate the factors influencing wildfire risk in Iran.

**方法:** Advanced remote sensing, GIS processing techniques, and machine learning algorithms were used to analyze the impact of climatic parameters, topographic features, and human-related factors on wildfire susceptibility assessment and prediction.

**结果:** The findings showed that climatic elements significantly contribute to wildfire susceptibility, but human activities play a crucial role. Human-related factors had a more prominent influence during seasonal analyses. High-resolution wildfire susceptibility maps were generated.

**结论:** This research provides new insights into wildfire dynamics in Iran and highlights the urgent need for effective fire management strategies.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+wildfire+susceptibility+in+Iran%3A+Leveraging+machine+learning+for+geospatial+analysis+of+climatic+and+anthropogenic+factors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14122，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14122&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the multifaceted factors influencing wildfire risk in
Iran, focusing on the interplay between climatic conditions and human
activities. Utilizing advanced remote sensing, geospatial information system
(GIS) processing techniques such as cloud computing, and machine learning
algorithms, this research analyzed the impact of climatic parameters,
topographic features, and human-related factors on wildfire susceptibility
assessment and prediction in Iran. Multiple scenarios were developed for this
purpose based on the data sampling strategy. The findings revealed that
climatic elements such as soil moisture, temperature, and humidity
significantly contribute to wildfire susceptibility, while human
activities-particularly population density and proximity to powerlines-also
played a crucial role. Furthermore, the seasonal impact of each parameter was
separately assessed during warm and cold seasons. The results indicated that
human-related factors, rather than climatic variables, had a more prominent
influence during the seasonal analyses. This research provided new insights
into wildfire dynamics in Iran by generating high-resolution wildfire
susceptibility maps using advanced machine learning classifiers. The generated
maps identified high risk areas, particularly in the central Zagros region, the
northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting
the urgent need for effective fire management strategies.

</details>


### [85] [Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning](https://arxiv.org/abs/2505.14125)
*Viet Anh Khoa Tran, Emre Neftci, Willem. A. M. Wybo*

**主要类别:** cs.LG

**概要:** Introduces Task-Modulated Contrastive Learning (TMCL) inspired by predictive coding principles to address catastrophic forgetting in continual learning.


<details>
  <summary>更多</summary>
  
**动机:** To overcome catastrophic forgetting in machine learning when learning from a stream of unlabeled data.

**方法:** Uses contrastive learning with top-down information modulation to create view-invariant representation spaces.

**结果:** Improves performance in class-incremental and transfer learning tasks compared to other approaches.

**结论:** Top-down modulations are crucial for balancing stability and plasticity in continual learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Contrastive+Consolidation+of+Top-Down+Modulations+Achieves+Sparsely+Supervised+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14125&send_immediately=true&force_search=false)

**原文摘要:** Biological brains learn continually from a stream of unlabeled data, while
integrating specialized information from sparsely labeled examples without
compromising their ability to generalize. Meanwhile, machine learning methods
are susceptible to catastrophic forgetting in this natural learning setting, as
supervised specialist fine-tuning degrades performance on the original task. We
introduce task-modulated contrastive learning (TMCL), which takes inspiration
from the biophysical machinery in the neocortex, using predictive coding
principles to integrate top-down information continually and without
supervision. We follow the idea that these principles build a view-invariant
representation space, and that this can be implemented using a contrastive
loss. Then, whenever labeled samples of a new class occur, new affine
modulations are learned that improve separation of the new class from all
others, without affecting feedforward weights. By co-opting the view-invariance
learning mechanism, we then train feedforward weights to match the unmodulated
representation of a data sample to its modulated counterparts. This introduces
modulation invariance into the representation space, and, by also using past
modulations, stabilizes it. Our experiments show improvements in both
class-incremental and transfer learning over state-of-the-art unsupervised
approaches, as well as over comparable supervised approaches, using as few as
1% of available labels. Taken together, our work suggests that top-down
modulations play a crucial role in balancing stability and plasticity.

</details>


### [86] [MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow](https://arxiv.org/abs/2505.14126)
*Yuan-Hao Jiang, Kezong Tang, Zi-Wei Chen, Yuang Wei, Tian-Yi Liu, Jiayi Wu*

**主要类别:** cs.LG

**概要:** 提出了一种基于多智能体系统和大语言模型的知识组件图结构学习算法(MAS-KCL)，并验证了其在识别学习路径上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 准确的知识组件图可以帮助教育者发现学习者在特定知识组件上表现不佳的根本原因，从而实现有针对性的教学干预。

**方法:** 开发了一种名为MAS-KCL的知识组件图结构学习算法，该算法利用由大语言模型驱动的多智能体系统进行自适应修改和优化，并整合了双向反馈机制来评估边值并调整生成概率分布。

**结果:** 算法在5个合成数据集和4个真实教育数据集上进行了测试，实验结果验证了其在学习路径识别上的有效性。

**结论:** 通过准确识别学习者的路径，教师可以设计更全面的学习计划，帮助学习者更有效地达成教育目标，推动教育的可持续发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAS-KCL%3A+Knowledge+component+graph+structure+learning+with+large+language+model-based+agentic+workflow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14126，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14126&send_immediately=true&force_search=false)

**原文摘要:** Knowledge components (KCs) are the fundamental units of knowledge in the
field of education. A KC graph illustrates the relationships and dependencies
between KCs. An accurate KC graph can assist educators in identifying the root
causes of learners' poor performance on specific KCs, thereby enabling targeted
instructional interventions. To achieve this, we have developed a KC graph
structure learning algorithm, named MAS-KCL, which employs a multi-agent system
driven by large language models for adaptive modification and optimization of
the KC graph. Additionally, a bidirectional feedback mechanism is integrated
into the algorithm, where AI agents leverage this mechanism to assess the value
of edges within the KC graph and adjust the distribution of generation
probabilities for different edges, thereby accelerating the efficiency of
structure learning. We applied the proposed algorithm to 5 synthetic datasets
and 4 real-world educational datasets, and experimental results validate its
effectiveness in learning path recognition. By accurately identifying learners'
learning paths, teachers are able to design more comprehensive learning plans,
enabling learners to achieve their educational goals more effectively, thus
promoting the sustainable development of education.

</details>


### [87] [A Methodological Framework for Measuring Spatial Labeling Similarity](https://arxiv.org/abs/2505.14128)
*Yihang Du, Jiaying Hu, Suyang Hou, Yueyang Ding, Xiaobo Sun*

**主要类别:** cs.LG

**概要:** 提出了一种新的空间标记相似性度量方法（SLAM），该方法能更准确地反映空间转录组学中标记质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法未能充分考虑空间标记相似性测量中的多个重要方面。

**方法:** 基于位置组织、标签和属性将两个空间标记转换为图，并提取图属性分布以计算分布差异。

**结果:** 通过模拟和真实的空间转录组数据实验验证了SLAM的有效性。

**结论:** SLAM提供了比其他已建立的评估指标更全面和准确的空间标记质量评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Methodological+Framework+for+Measuring+Spatial+Labeling+Similarity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14128&send_immediately=true&force_search=false)

**原文摘要:** Spatial labeling assigns labels to specific spatial locations to characterize
their spatial properties and relationships, with broad applications in
scientific research and practice. Measuring the similarity between two spatial
labelings is essential for understanding their differences and the contributing
factors, such as changes in location properties or labeling methods. An
adequate and unbiased measurement of spatial labeling similarity should
consider the number of matched labels (label agreement), the topology of
spatial label distribution, and the heterogeneous impacts of mismatched labels.
However, existing methods often fail to account for all these aspects. To
address this gap, we propose a methodological framework to guide the
development of methods that meet these requirements. Given two spatial
labelings, the framework transforms them into graphs based on location
organization, labels, and attributes (e.g., location significance). The
distributions of their graph attributes are then extracted, enabling an
efficient computation of distributional discrepancy to reflect the
dissimilarity level between the two labelings. We further provide a concrete
implementation of this framework, termed Spatial Labeling Analogy Metric
(SLAM), along with an analysis of its theoretical foundation, for evaluating
spatial labeling results in spatial transcriptomics (ST) \textit{as per} their
similarity with ground truth labeling. Through a series of carefully designed
experimental cases involving both simulated and real ST data, we demonstrate
that SLAM provides a comprehensive and accurate reflection of labeling quality
compared to other well-established evaluation metrics. Our code is available at
https://github.com/YihDu/SLAM.

</details>


### [88] [Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging](https://arxiv.org/abs/2505.14136)
*Ryo Bertolissi, Jonas Hübotter, Ido Hakimi, Andreas Krause*

**主要类别:** cs.LG

**概要:** Test-Time Model Merging (TTMM) is proposed to scale Mixture of Expert (MoE) models to more experts and avoid test-time overhead.


<details>
  <summary>更多</summary>
  
**动机:** To increase model capacity without increasing inference cost.

**方法:** TTMM scales MoE and uses model merging.

**结果:** Performance improves with more experts and approaches TTT's performance.

**结论:** TTMM offers a cost-effective way to scale test-time training.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Local+Mixtures+of+Experts%3A+Essentially+Free+Test-Time+Training+via+Model+Merging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14136，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14136&send_immediately=true&force_search=false)

**原文摘要:** Mixture of expert (MoE) models are a promising approach to increasing model
capacity without increasing inference cost, and are core components of many
state-of-the-art language models. However, current MoE models typically use
only few experts due to prohibitive training and inference cost. We propose
Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of
magnitude more experts and uses model merging to avoid almost any test-time
overhead. We show that TTMM is an approximation of test-time training (TTT),
which fine-tunes an expert model for each prediction task, i.e., prompt. TTT
has recently been shown to significantly improve language models, but is
computationally expensive. We find that performance of TTMM improves with more
experts and approaches the performance of TTT. Moreover, we find that with a 1B
parameter base model, TTMM is more than 100x faster than TTT at test-time by
amortizing the cost of TTT at train-time. Thus, TTMM offers a promising
cost-effective approach to scale test-time training.

</details>


### [89] [FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning](https://arxiv.org/abs/2505.14139)
*Marvin Alles, Nutan Chen, Patrick van der Smagt, Botond Cseke*

**主要类别:** cs.LG

**概要:** This paper introduces energy-guided flow matching, a new method that improves training of flow models and shows competitive results in offline reinforcement learning without needing guidance at inference time.


<details>
  <summary>更多</summary>
  
**动机:** To incorporate guidance during training in diffusion models, which remains relatively underexplored.

**方法:** Energy-guided flow matching, which learns a conditional velocity field corresponding to the flow policy by approximating an energy-guided probability path as a Gaussian path.

**结果:** Our method achieves competitive performance while the policy training time is constant in the number of flow sampling steps.

**结论:** This work introduces energy-guided flow matching, which enhances the training of flow models and eliminates the need for guidance at inference time.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlowQ%3A+Energy-Guided+Flow+Policies+for+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14139&send_immediately=true&force_search=false)

**原文摘要:** The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.

</details>


### [90] [Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation](https://arxiv.org/abs/2505.14161)
*Ting Wei, Biao Mei, Junliang Lyu, Renquan Zhang, Feng Zhou, Yifan Sun*

**主要类别:** cs.LG

**概要:** 提出FedWBA方法解决现有个性化贝叶斯联邦学习(PBFL)在客户端后验推理中的限制性参数假设和服务器聚合中的简单参数平均问题。理论上有局部和全局收敛保证，实验表明FedWBA在预测准确性、不确定性校准和收敛速度上优于基线。


<details>
  <summary>更多</summary>
  
**动机:** 现有PBFL方法存在限制性参数假设和简单参数平均的问题，需要改进以提高性能。

**方法:** 提出FedWBA方法，包括客户端使用粒子变分推理进行非参数后验表示，在服务器端引入基于粒子的Wasserstein重心聚合。

**结果:** FedWBA在预测准确性、不确定性校准和收敛速度上优于基线，并且有局部和全局收敛保证。

**结论:** FedWBA通过增强局部推理和全局聚合克服了现有PBFL方法的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Bayesian+Federated+Learning+with+Wasserstein+Barycenter+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14161，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14161&send_immediately=true&force_search=false)

**原文摘要:** Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client
data and quantifies uncertainty by combining personalization with Bayesian
inference. However, existing PBFL methods face two limitations: restrictive
parametric assumptions in client posterior inference and naive parameter
averaging for server aggregation. To overcome these issues, we propose FedWBA,
a novel PBFL method that enhances both local inference and global aggregation.
At the client level, we use particle-based variational inference for
nonparametric posterior representation. At the server level, we introduce
particle-based Wasserstein barycenter aggregation, offering a more
geometrically meaningful approach. Theoretically, we provide local and global
convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease
lower bound per iteration for variational inference convergence. Globally, we
show that the Wasserstein barycenter converges to the true parameter as the
client data size increases. Empirically, experiments show that FedWBA
outperforms baselines in prediction accuracy, uncertainty calibration, and
convergence rate, with ablation studies confirming its robustness.

</details>


### [91] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/abs/2505.14170)
*Chen Zhang, Weixin Bu, Zeyi Ren, Zhengwu Liu, Yik-Chung Wu, Ngai Wong*

**主要类别:** cs.LG

**概要:** 提出了一种名为Graph Neural Teaching (GraNT)的新方法，通过选择图-属性对的子集来加速图性质学习者的训练过程。实验表明，GraNT在多种任务上显著减少了训练时间，并保持了良好的泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的图性质学习者如图卷积网络（GCNs）在学习过程中成本高昂，本文旨在解决这一问题。

**方法:** 提出了GraNT方法，重新解释了学习过程，并提供了一个新的非参数教学理论框架，用于通过示例选择来教授隐式定义的映射。

**结果:** GraNT在四种不同的图和节点级别任务上实现了显著的训练时间减少。

**结论:** 首次证明了教授图性质学习者与教授结构感知的非参数学习者是一致的，这为提高图性质学习者的效率提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nonparametric+Teaching+for+Graph+Property+Learners，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14170，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14170&send_immediately=true&force_search=false)

**原文摘要:** Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


### [92] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe, Shaan Shah, Raghav Singhal, Praneeth Vepakomma*

**主要类别:** cs.LG

**概要:** 研究发现大型语言模型的安全对齐不是几何局部化的，而是来自模型更广泛学习动态的纠缠、高影响力组件，这挑战了基于子空间的防御方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的安全对齐方法在进一步微调时容易退化并重新引入有害行为，因此需要探索新的策略来维持对齐。

**方法:** 对大型语言模型的安全对齐进行了全面的经验研究，包括检查特定子空间是否集中了安全相关行为，这些行为是否可以与通用学习分离，以及有害性是否源于内部表示中的可区分模式。

**结果:** 在参数和激活空间上，放大安全行为的子空间也会放大不安全行为；具有不同安全含义的提示会激活重叠的表示；未发现专门控制安全性的子空间。

**结论:** 安全对齐在大型语言模型中很重要，但容易因进一步微调而退化。研究发现，安全相关的几何子空间并不能有效隔离有害行为，这表明基于子空间的防御可能有根本局限性，并强调了开发其他策略以在持续训练中保持对齐的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safety+Subspaces+are+Not+Distinct%3A+A+Fine-Tuning+Case+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14185&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [93] [$α$-GAN by Rényi Cross Entropy](https://arxiv.org/abs/2505.14190)
*Ni Ding, Miao Qiao, Jiaxing Xu, Yiping Ke, Xiaoyu Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种基于Rényi熵的生成对抗网络α-GAN，其在优化过程中通过调整Rényi阶数α，可以加速收敛并可能解决梯度消失等问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有GAN模型存在一些问题，如梯度消失等，需要探索新的方法来改进训练过程。

**方法:** 引入Rényi熵构建价值函数，并定义了一个min-max优化问题，其解由Rényi阶数α参数化。

**结果:** α-GAN在α∈(0,1)范围内时，梯度会指数级增大，从而加速收敛；实验验证了这一结论。

**结论:** α-GAN在特定条件下能有效改善GAN的训练效果，但现有的Rényi版本GAN尚未充分探索这个范围。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%CE%B1%24-GAN+by+R%C3%A9nyi+Cross+Entropy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14190&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes $\alpha$-GAN, a generative adversarial network using
R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross
entropy, as an expected certainty measure incurred by the discriminator's soft
decision as to where the sample is from, true population or the generator. The
discriminator tries to maximize the R\'{e}nyi certainty about sample source,
while the generator wants to reduce it by injecting fake samples. This forms a
min-max problem with the solution parameterized by the R\'{e}nyi order
$\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the
value function is exactly the binary cross entropy. The optimization of
$\alpha$-GAN is over probability (vector) space. It is shown that the gradient
is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in
(0,1)$. This makes convergence faster, which is verified by experimental
results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to
solve some common problems, e.g., vanishing gradient. A following observation
reveals that this range has not been fully explored in the existing R\'{e}nyi
version GANs.

</details>


### [94] [FLASH-D: FlashAttention with Hidden Softmax Division](https://arxiv.org/abs/2505.14201)
*Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos*

**主要类别:** cs.LG

**概要:** FlashAttention优化了Transformer注意力机制中的计算过程，通过简化核心计算内核FLASH-D，在保持原有性能的同时减少了硬件资源消耗。


<details>
  <summary>更多</summary>
  
**动机:** 提高Transformer注意力机制的计算效率并减少对硬件资源的需求。

**方法:** 提出了一种新的计算方法FLASH-D，该方法简化了原始FlashAttention的核心内核。

**结果:** 与最先进的并行硬件架构相比，硬件实现结果显示，面积减少了22.8%，功耗减少了20.3%，且没有性能损失。

**结论:** 新提出的计算方法在不牺牲性能的情况下显著降低了硬件资源需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FLASH-D%3A+FlashAttention+with+Hidden+Softmax+Division，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14201&send_immediately=true&force_search=false)

**原文摘要:** The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware acceleration.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.

</details>


### [95] [MSDformer: Multi-scale Discrete Transformer For Time Series Generation](https://arxiv.org/abs/2505.14202)
*Zhicheng Chen, Shibo Feng, Xi Xiao, Zhong Zhang, Qing Li, Xingyu Gao, Peilin Zhao*

**主要类别:** cs.LG

**概要:** 提出了一种新的基于多尺度离散变换器(MSDformer)的时间序列生成方法，解决了现有离散标记建模(DTM)方法无法捕捉复杂时间序列数据的多尺度时间模式以及缺乏理论基础指导模型优化的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有DTM方法存在两个关键限制：无法捕获复杂时间序列数据中的多尺度时间模式；缺乏理论基础来指导模型优化。

**方法:** 提出了多尺度时间序列标记器和多尺度自回归标记建模技术，用于在离散潜在空间中捕获时间序列的多尺度模式，并通过率失真定理验证了DTM方法的有效性和MSDformer的合理性。

**结果:** 实验表明MSDformer显著优于最先进的方法。

**结论:** 将多尺度信息和多尺度模式建模纳入DTM方法可以显著提高生成时间序列的质量。代码将在接受后发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MSDformer%3A+Multi-scale+Discrete+Transformer+For+Time+Series+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14202，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14202&send_immediately=true&force_search=false)

**原文摘要:** Discrete Token Modeling (DTM), which employs vector quantization techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.

</details>


### [96] [Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data](https://arxiv.org/abs/2505.14206)
*Flavio Di Martino, Franca Delmastro*

**主要类别:** cs.LG

**概要:** This paper evaluates advanced generative models for synthesizing time series data in mHealth, focusing on multi-modality, long-range dependencies, and conditional generation challenges.


<details>
  <summary>更多</summary>
  
**动机:** To address data scarcity and privacy issues in mHealth using synthetic data generated by GANs and diffusion models.

**方法:** Systematic evaluation of state-of-the-art generative models with a new evaluation framework.

**结果:** Existing approaches have limitations in cross-modal consistency, temporal coherence, and robust performance in different scenarios.

**结论:** Identifies limitations in current methods and outlines future research directions to enhance synthetic time series generation for mHealth applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Challenges+and+Limitations+in+the+Synthetic+Generation+of+mHealth+Sensor+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14206，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14206&send_immediately=true&force_search=false)

**原文摘要:** The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.

</details>


### [97] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/abs/2505.14211)
*Qu Wang, Yan Xia*

**主要类别:** cs.LG

**概要:** This study introduces a PID-controlled tensor wheel decomposition (PTWD) model for improving link prediction in dynamic networks by combining the representation power of TWD with the PID control principle.


<details>
  <summary>更多</summary>
  
**动机:** To overcome the limitations of traditional static network methods and improve the prediction accuracy in dynamic networks.

**方法:** Integrating the PID control principle into the optimization process and exploiting the representation power of TWD.

**结果:** The PTWD model shows more accurate link prediction capabilities on four real datasets compared to other models.

**结论:** The proposed PTWD model demonstrates superior link prediction capabilities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+PID-Controlled+Tensor+Wheel+Decomposition+Model+for+Dynamic+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14211&send_immediately=true&force_search=false)

**原文摘要:** Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [98] [Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned](https://arxiv.org/abs/2505.14217)
*Jorge Fabila, Lidia Garrucho, Víctor M. Campello, Carlos Martín-Isla, Karim Lekadir*

**主要类别:** cs.LG

**概要:** This study investigates the application of Federated Learning in TB diagnosis using chest X-rays in low-resource African settings, comparing local and federated models.


<details>
  <summary>更多</summary>
  
**动机:** Addressing privacy concerns and data scarcity hindering traditional centralized models in low-resource settings.

**方法:** Using Federated Learning to collaboratively train AI models across eight African countries without sharing raw patient data.

**结果:** FL shows strong potential but faces challenges like infrastructure, internet reliability, digital literacy, and weak AI regulations.

**结论:** FL has great potential for AI-driven healthcare in underserved regions but requires better infrastructure, education, and regulatory support for wider adoption.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+learning+in+low-resource+settings%3A+A+chest+imaging+study+in+Africa+--+Challenges+and+lessons+learned，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14217&send_immediately=true&force_search=false)

**原文摘要:** This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.

</details>


### [99] [Fast and close Shannon entropy approximation](https://arxiv.org/abs/2505.14234)
*Illia Horenko, Davide Bassetti, Lukáš Pospíšil*

**主要类别:** cs.LG

**概要:** 本文提出了Fast Entropy Approximation (FEA)，这是一种新的非奇异有理近似方法，用于高效计算香农熵和其梯度，显著提升了机器学习中的特征选择性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的熵计算工具由于梯度奇异性导致高成本、低鲁棒性和慢收敛的问题。

**方法:** 开发了非奇异有理近似方法来逼近香农熵及其梯度，即Fast Entropy Approximation (FEA)。

**结果:** FEA的平均绝对误差为10^-3，比现有方法低约20倍；计算速度提高约50%，仅需5到6个基本操作。

**结论:** 提出了一种新的快速熵逼近方法FEA，该方法在特征选择问题上显著提高了模型质量和计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast+and+close+Shannon+entropy+approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14234，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14234&send_immediately=true&force_search=false)

**原文摘要:** Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.

</details>


### [100] [Learning with Local Search MCMC Layers](https://arxiv.org/abs/2505.14240)
*Germain Vivier-Ardisson, Mathieu Blondel, Axel Parmentier*

**主要类别:** cs.LG

**概要:** 本文提出一种基于MCMC的可微分组合层方法，用于处理依赖于不精确求解器的组合优化问题，并在动态车辆路径问题上展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 许多现有的组合优化层方法缺乏理论保证或在依赖不精确求解器时表现不佳。鉴于运筹学问题通常是NP难的，因此需要使用基于邻域的局部搜索启发式算法。

**方法:** 将局部搜索启发式算法中的问题特定邻域系统转化为提议分布，实现了组合可行解空间上的MCMC，构建了可微分的组合层和相关的损失函数。

**结果:** 通过在具有时间窗的大规模动态车辆路径问题上的应用，验证了所提出方法的有效性。

**结论:** 提出了一种新的学习方法，该方法利用了不精确组合求解器，并在动态车辆路径问题上进行了演示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+with+Local+Search+MCMC+Layers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14240，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14240&send_immediately=true&force_search=false)

**原文摘要:** Integrating combinatorial optimization layers into neural networks has
recently attracted significant research interest. However, many existing
approaches lack theoretical guarantees or fail to perform adequately when
relying on inexact solvers. This is a critical limitation, as many operations
research problems are NP-hard, often necessitating the use of
neighborhood-based local search heuristics. These heuristics iteratively
generate and evaluate candidate solutions based on an acceptance rule. In this
paper, we introduce a theoretically-principled approach for learning with such
inexact combinatorial solvers. Inspired by the connection between simulated
annealing and Metropolis-Hastings, we propose to transform problem-specific
neighborhood systems used in local search heuristics into proposal
distributions, implementing MCMC on the combinatorial space of feasible
solutions. This allows us to construct differentiable combinatorial layers and
associated loss functions. Replacing an exact solver by a local search strongly
reduces the computational burden of learning on many applications. We
demonstrate our approach on a large-scale dynamic vehicle routing problem with
time windows.

</details>


### [101] [A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input](https://arxiv.org/abs/2505.14251)
*Bar Mahpud, Or Sheffet*

**主要类别:** cs.LG

**概要:** 提出了一种新的差分隐私二阶矩估计算法，在子采样假设下实现了隐私-效用权衡。


<details>
  <summary>更多</summary>
  
**动机:** 研究差分隐私下的二阶矩估计问题，并设计适用于最坏情况的高效算法。

**方法:** 基于子采样假设，提出了一个递归算法框架，遵守零集中差分隐私(zCDP)，并保持估计精度。

**结果:** 算法在处理包含异常值的数据时能够近似分布的二阶矩矩阵。

**结论:** 新算法在隐私保护和数据准确性之间取得了良好的平衡，适用于各种复杂数据场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Private+Approximation+of+the+2nd-Moment+Matrix+of+Any+Subsamplable+Input，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14251&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of differentially private second moment estimation and
present a new algorithm that achieve strong privacy-utility trade-offs even for
worst-case inputs under subsamplability assumptions on the data. We call an
input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or
larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original
second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building
upon subsamplability, we give a recursive algorithmic framework similar to
Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)
while preserving w.h.p. the accuracy of the second moment estimation upto an
arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to
approximate the second moment matrix of a distribution $\mathcal{D}$, even when
a noticeable fraction of the input are outliers.

</details>


### [102] [Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks](https://arxiv.org/abs/2505.14252)
*Mouad Elaarabi, Domenico Borzacchiello, Philippe Le Bot, Nathan Lauzeral, Sebastien Comas-Cardona*

**主要类别:** cs.LG

**概要:** This paper introduces an architecture combining Sequence Encoding and Physics-Informed Neural Networks (PINNs) to enable real-time adaptation to varying parameters, boundary conditions, and initial conditions without retraining.


<details>
  <summary>更多</summary>
  
**动机:** To overcome limitations of PINNs when dealing with variable parameters, boundary and initial conditions, which require retraining the model every time changes occur.

**方法:** Integrating Sequence Encoding with PINNs to encode dynamic parameters, boundary conditions, and initial conditions, using these encoded features as inputs for the PINN.

**结果:** The approach was applied to three problems: the Rossler ODE system, a 2D Navier-Stokes PDE problem, and a 1D heat monitoring problem using real data. It demonstrated robustness against noise, ability to generalize, and capability to encode pressure data to identify inlet velocity profiles and compute velocity and pressure throughout the domain.

**结论:** The proposed architecture allows for real-time adaptation of PINNs to varying parameters, boundary conditions, and initial conditions, which could significantly enhance their applicability in various fields.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Adaptive+Modeling+in+Process+Monitoring%3A+Leveraging+Sequence+Encoders+and+Physics-Informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14252，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14252&send_immediately=true&force_search=false)

**原文摘要:** In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.

</details>


### [103] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong, Jingbo Zhou, Jingyong Ye, Dejing Dou*

**主要类别:** cs.LG

**概要:** 提出了一种新的强化学习算法AAPO，优化了交叉熵损失，通过动量增强的优势估计，解决了现有基于组相对优势估计方法在优势接近零时的训练效率问题。实验表明AAPO在多个数学推理基准上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于强化学习的后训练方法在没有价值模型的情况下可以简化训练，但现有的组相对优势估计方法在优势接近零时仍存在训练效率低下的问题。

**方法:** 提出了一种名为优势增强策略优化（AAPO）的新算法，该算法通过动量增强的优势估计来优化交叉熵损失。

**结果:** AAPO在多个数学推理基准上表现出色。

**结论:** AAPO有效地解决了基于组相对优势估计方法的训练效率问题，为强化学习在大语言模型中的应用提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AAPO%3A+Enhance+the+Reasoning+Capabilities+of+LLMs+with+Advantage+Momentum，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14264&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [104] [X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning](https://arxiv.org/abs/2505.14273)
*Hiroki Shiraishi, Hisao Ishibuchi, Masaya Nakata*

**主要类别:** cs.LG

**概要:** Proposes X-KAN, which uses multiple local Kolmogorov-Arnold Networks optimized by XCSF to handle complex or discontinuous functions better than conventional methods.


<details>
  <summary>更多</summary>
  
**动机:** Existing neural network approaches struggle with locally complex or discontinuous functions.

**方法:** Uses XCSF to optimize multiple local KANs, combining KAN's expressiveness with XCSF's adaptive partitioning.

**结果:** Outperforms conventional methods on artificial and real-world datasets.

**结论:** Validates the effectiveness of using KAN as a local model in XCSF.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是X-KAN%3A+Optimizing+Local+Kolmogorov-Arnold+Networks+via+Evolutionary+Rule-Based+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14273，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14273&send_immediately=true&force_search=false)

**原文摘要:** Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.

</details>


### [105] [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
*Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, Ping Luo*

**主要类别:** cs.LG

**概要:** 本文提出了一种统一的量化感知训练缩放定律，揭示了影响量化误差的关键因素，并通过实验验证了该定律的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 量化感知训练可以减少大型语言模型的计算和内存资源需求，但其在4位精度下的缩放行为尚未被充分理解。现有的量化感知训练缩放规律往往忽略了关键因素，如训练标记数量和量化粒度。

**方法:** 提出了一个统一的缩放定律来建模量化误差，并进行了268个量化感知训练实验。此外，还分解了W4A4量化误差的权重和激活部分，并应用了混合精度量化来解决主要瓶颈问题。

**结果:** 实验表明，量化误差随着模型大小的增加而减少，但随着更多的训练标记和更粗粒度的量化而增加。权重和激活量化误差都遵循W4A4量化误差的整体趋势，但敏感性不同。通过应用混合精度量化解决了主要瓶颈问题，并且当有更多训练数据时，权重量化误差最终会超过激活量化误差。

**结论:** 提出的统一缩放定律有助于理解量化感知训练在不同条件下的表现。通过调整量化粒度和训练数据量，可以有效减少量化误差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Law+for+Quantization-Aware+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14302，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14302&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [106] [MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains](https://arxiv.org/abs/2505.14312)
*Kyungeun Lee, Moonjung Eo, Hye-Seung Cho, Dongmin Kim, Ye Seul Sim, Seoyoon Kim, Min-Kook Suh, Woohyung Lim*

**主要类别:** cs.LG

**概要:** 提出MultiTab，一个用于表格学习算法多维数据感知分析的基准套件和评估框架。分析显示模型性能对数据特征非常敏感，并强调了针对特定数据特征选择合适模型的重要性。所有资源公开可用。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试主要依赖平均指标，无法揭示模型在不同数据环境下的行为差异。

**方法:** 构建MultiTab，对196个公开数据集按样本大小、标签不平衡和特征交互等关键数据特性分类，并评估13种具有不同归纳偏置的代表性模型。

**结果:** 发现模型性能高度依赖于数据特征，例如，使用样本级相似性的模型在大样本量或高特征相关性数据集上表现良好，而编码特征间依赖关系的模型在弱相关特征数据集中表现最佳。

**结论:** MultiTab为更原则化的模型设计提供了可能，并为根据具体数据特征选择模型提供了实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MultiTab%3A+A+Comprehensive+Benchmark+Suite+for+Multi-Dimensional+Evaluation+in+Tabular+Domains，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14312&send_immediately=true&force_search=false)

**原文摘要:** Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.

</details>


### [107] [Better Neural Network Expressivity: Subdividing the Simplex](https://arxiv.org/abs/2505.14338)
*Egor Bakaev, Florestan Brunck, Christoph Hertrich, Jack Stade, Amir Yehudayoff*

**主要类别:** cs.LG

**概要:** 研究了ReLU神经网络的表达能力，重点在于其深度。证明了对于任意连续分段线性函数，所需隐藏层数少于先前猜想，且构造几乎匹配下界。


<details>
  <summary>更多</summary>
  
**动机:** 研究ReLU神经网络表达连续分段线性函数所需的最小深度。

**方法:** 通过构建具有两个隐藏层的ReLU网络精确表示最大函数，并推广到n个数的最大值计算。

**结果:** 证明了计算所有连续分段线性函数所需的隐藏层数为 ceil(log_3(n-1)) + 1，而非之前猜想的 ceil(log_2(n+1))。

**结论:** 新的结果表明ReLU网络的表达能力比之前认为的更强。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Better+Neural+Network+Expressivity%3A+Subdividing+the+Simplex，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14338，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14338&send_immediately=true&force_search=false)

**原文摘要:** This work studies the expressivity of ReLU neural networks with a focus on
their depth. A sequence of previous works showed that $\lceil \log_2(n+1)
\rceil$ hidden layers are sufficient to compute all continuous piecewise linear
(CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella
(NeurIPS'21) conjectured that this result is optimal in the sense that there
are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require
this depth. We disprove the conjecture and show that
$\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL
functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers
can exactly represent the maximum function of five inputs. More generally, we
show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute
the maximum of $n\geq 4$ numbers. Our constructions almost match the
$\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in
the special case of ReLU networks with weights that are decimal fractions. The
constructions have a geometric interpretation via polyhedral subdivisions of
the simplex into ``easier'' polytopes.

</details>


### [108] [Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights](https://arxiv.org/abs/2505.14345)
*Aydin Abedinia, Shima Tabakhi, Vahid Seydi*

**主要类别:** cs.LG

**概要:** 提出了一种利用距离加权机制的半监督深度学习框架，增强了模型在噪声和不平衡数据中的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 提高分类性能，解决标注数据不足的问题。

**方法:** 基于距离的加权机制，结合不确定性一致性及图表示技术。

**结果:** 在12个基准数据集上表现优于现有方法，提升准确率、精确度和召回率。

**结论:** 为半监督学习提供了一个鲁棒且实用的解决方案，适用于医疗和安全等领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Classification+with+Semi-Supervised+Deep+Learning+Using+Distance-Based+Sample+Weights，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14345，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14345&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.

</details>


### [109] [Towards eliciting latent knowledge from LLMs with mechanistic interpretability](https://arxiv.org/abs/2505.14352)
*Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan, Neel Nanda*

**主要类别:** cs.LG

**概要:** 研究了当前技术揭示语言模型隐藏知识的能力，通过训练禁忌模型描述特定秘密词而不明确提及，探索了非解释性和基于机械解释的技术方法，并证明了这些方法在概念验证设置中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 确保更强大和复杂的语言模型保持可信和可靠，防止其可能对操作者隐瞒信息或欺骗的行为。

**方法:** 训练了一个禁忌模型，描述一个不在其训练数据或提示中的特定秘密词；评估了非解释性（黑盒）方法并开发了基于机械解释技术的自动化策略，包括logit镜头和稀疏自动编码器。

**结果:** 两种方法都能有效揭示概念验证设置中的秘密词。

**结论:** 本研究是解决从语言模型中揭示秘密知识问题的重要一步，有助于它们的安全和可靠部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+eliciting+latent+knowledge+from+LLMs+with+mechanistic+interpretability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14352，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14352&send_immediately=true&force_search=false)

**原文摘要:** As language models become more powerful and sophisticated, it is crucial that
they remain trustworthy and reliable. There is concerning preliminary evidence
that models may attempt to deceive or keep secrets from their operators. To
explore the ability of current techniques to elicit such hidden knowledge, we
train a Taboo model: a language model that describes a specific secret word
without explicitly stating it. Importantly, the secret word is not presented to
the model in its training data or prompt. We then investigate methods to
uncover this secret. First, we evaluate non-interpretability (black-box)
approaches. Subsequently, we develop largely automated strategies based on
mechanistic interpretability techniques, including logit lens and sparse
autoencoders. Evaluation shows that both approaches are effective in eliciting
the secret word in our proof-of-concept setting. Our findings highlight the
promise of these approaches for eliciting hidden knowledge and suggest several
promising avenues for future work, including testing and refining these methods
on more complex model organisms. This work aims to be a step towards addressing
the crucial problem of eliciting secret knowledge from language models, thereby
contributing to their safe and reliable deployment.

</details>


### [110] [Layer-wise Quantization for Quantized Optimistic Dual Averaging](https://arxiv.org/abs/2505.14371)
*Anh Duc Nguyen, Ilia Markov, Frank Zhengqing Wu, Ali Ramezani-Kebrya, Kimon Antonakopoulos, Dan Alistarh, Volkan Cevher*

**主要类别:** cs.LG

**概要:** 提出了一种新的层量化框架和算法QODA，显著提升了Wasserstein GAN在多GPU训练中的速度。


<details>
  <summary>更多</summary>
  
**动机:** 现代深度神经网络具有异构性，这影响了预测性能。

**方法:** 开发了层量化框架并提出了QODA算法，采用自适应学习率。

**结果:** QODA在多个GPU上实现了高达150%的端到端训练加速。

**结论:** 该研究为处理深度神经网络的异构性提供了有效的量化解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Layer-wise+Quantization+for+Quantized+Optimistic+Dual+Averaging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14371，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14371&send_immediately=true&force_search=false)

**原文摘要:** Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.

</details>


### [111] [Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes](https://arxiv.org/abs/2505.14388)
*Prasanna Parasurama, Panos Ipeirotis*

**主要类别:** cs.LG

**概要:** This paper shows that ensuring equal representation in shortlists doesn't necessarily lead to more diverse final hires due to the correlation between algorithmic and human hiring criteria. It proposes an approach to diversify shortlists, which improves gender diversity in final hires.


<details>
  <summary>更多</summary>
  
**动机:** To explore why equal shortlists don't guarantee diverse final hires and to suggest methods to enhance diversity.

**方法:** Theoretical and empirical analysis including a large-scale study of job applications from multiple tech firms, along with empirical simulations.

**结果:** Enforcing equal shortlists only provides limited benefits in terms of hire diversity when the algorithm mirrors hiring managers' preferences. The proposed method significantly boosts gender diversity in final hires.

**结论:** Algorithmic design choices are crucial for achieving organizational diversity goals, and practical guidance is provided for fairness-oriented hiring algorithms.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Algorithmic+Hiring+and+Diversity%3A+Reducing+Human-Algorithm+Similarity+for+Better+Outcomes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14388，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14388&send_immediately=true&force_search=false)

**原文摘要:** Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.

</details>


### [112] [Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach](https://arxiv.org/abs/2505.14407)
*Aniket Salvi, Gereon Weiss, Mario Trapp*

**主要类别:** cs.LG

**概要:** This paper presents a novel fuzzy-based monitor for ML perception components that provides human-interpretable explanations and improves safety and availability.


<details>
  <summary>更多</summary>
  
**动机:** The lack of human-interpretable explanations for ML prediction errors hinders the assurance of system safety and reliability.

**方法:** Introduce a fuzzy-based monitor tailored for ML perception components that provides explanations and functions as a runtime safety monitor.

**结果:** Evaluated the monitor using naturalistic driving datasets, identified reliable operating conditions, and created an assurance case linking ML operation to system safety.

**结论:** The proposed monitor improves safety and availability compared to state-of-the-art runtime ML monitors.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explaining+Unreliable+Perception+in+Automated+Driving%3A+A+Fuzzy-based+Monitoring+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14407，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14407&send_immediately=true&force_search=false)

**原文摘要:** Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.

</details>


### [113] [Byte Pair Encoding for Efficient Time Series Forecasting](https://arxiv.org/abs/2505.14411)
*Leon Götz, Marcel Kollovieh, Stephan Günnemann, Leo Schwinn*

**主要类别:** cs.LG

**概要:** This paper introduces a new tokenization method for time series analysis based on frequent motifs, improving forecasting performance and efficiency.


<details>
  <summary>更多</summary>
  
**动机:** Existing tokenization methods produce excessive tokens for simple patterns, leading to high computational overhead.

**方法:** Proposes a pattern-centric tokenization scheme that merges samples with underlying patterns into tokens, using conditional decoding as a post-hoc optimization.

**结果:** Improves forecasting performance by 36% and efficiency by 1990% on average, with conditional decoding reducing MSE by up to 44%.

**结论:** The proposed tokenization method is adaptive to various temporal patterns, generalizes well to unseen data, and provides meaningful token representations capturing time series properties.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Byte+Pair+Encoding+for+Efficient+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14411&send_immediately=true&force_search=false)

**原文摘要:** Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.

</details>


### [114] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/abs/2505.14415)
*Myung Jun Kim, Félix Lefebvre, Gaëtan Brison, Alexandre Perez-Lebel, Gaël Varoquaux*

**主要类别:** cs.LG

**概要:** Introduces TARTE, a foundation model that transforms tables into knowledge-enhanced vector representations, improving downstream tasks with minimal additional cost.


<details>
  <summary>更多</summary>
  
**动机:** To address challenges in data semantics for table foundation models and provide a more convenient and reusable solution compared to existing methods.

**方法:** Pre-trains TARTE on large relational data to create knowledge-enhanced vector representations using strings to capture semantics.

**结果:** TARTE improves downstream learning performance with low additional computational costs and facilitates task-specific fine-tuning or combination with other learners.

**结论:** TARTE demonstrates an effective approach to knowledge pre-training for tabular learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Table+Foundation+Models%3A+on+knowledge+pre-training+for+tabular+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14415&send_immediately=true&force_search=false)

**原文摘要:** Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [115] [Explaining Neural Networks with Reasons](https://arxiv.org/abs/2505.14424)
*Levin Hornischer, Hannes Leitgeb*

**主要类别:** cs.LG

**概要:** 提出了一种基于新颖的数理哲学解释理论的新神经网络可解释性方法，该方法计算每个神经元的解释向量，并展示了其在多种神经网络架构和模态中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决神经网络缺乏可解释性的问题，提出一种基于数理哲学理论的解释方法。

**方法:** 通过计算神经元的解释向量（reasons vector），结合逻辑和贝叶斯视角来解释神经元及其组群的行为。

**结果:** 该方法在多种神经网络架构和模态中表现出了统一性、可扩展性、忠实性和正确性等特性，并且可以通过训练提高解释强度。

**结论:** 提出的可解释性方法能够有效增强模型的鲁棒性和公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explaining+Neural+Networks+with+Reasons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14424&send_immediately=true&force_search=false)

**原文摘要:** We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.

</details>


### [116] [Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications](https://arxiv.org/abs/2505.14428)
*Riccardo D'Elia*

**主要类别:** cs.LG

**概要:** 提出一种结合深度学习和系统动力学的可解释神经系统动力学框架，旨在提升预测准确性的同时保持因果可靠性和可扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习在复杂模型学习和准确预测方面表现优异但缺乏解释性和因果可靠性；传统系统动力学方法提供透明度和因果洞察但受限于可扩展性和需要大量领域知识。

**方法:** 开发一种神经系统动力学管道，集成基于概念的可解释性、机械可解释性和因果机器学习。

**结果:** 通过欧盟资助的AutoMoTIF项目中的实际应用验证了所提管道的有效性。

**结论:** 最终目标是收集可用于支持自主系统中可解释性和安全性整合的行动见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Neural+System+Dynamics%3A+Combining+Deep+Learning+with+System+Dynamics+Modeling+to+Support+Critical+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14428，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14428&send_immediately=true&force_search=false)

**原文摘要:** The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.

</details>


### [117] [RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation](https://arxiv.org/abs/2505.14451)
*Md Atik Ahamed, Qiang Ye, Qiang Cheng*

**主要类别:** cs.LG

**概要:** Proposes RefiDiff, a novel framework combining local ML predictions with a Mamba-based denoising network for robust imputation in high-dimensional mixed-type datasets under MNAR mechanisms.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods struggle to handle missing not at random (MNAR) mechanisms and high-dimensional data due to inability to integrate local and global data characteristics.

**方法:** Introduces RefiDiff which uses pre-refinement for initial warm-up imputations and post-refinement for result polishing. Encodes mixed-type data into unified tokens and uses a Mamba-based denoising network to capture interrelationships among distant features and samples.

**结果:** Outperforms state-of-the-art methods across missing-value settings, especially excelling in MNAR scenarios with 4x faster training time than SOTA DDPM-based approaches.

**结论:** RefiDiff provides robust, scalable, and effective imputation for complex missingness patterns in high-dimensional mixed-type datasets.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RefiDiff%3A+Refinement-Aware+Diffusion+for+Efficient+Missing+Data+Imputation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14451&send_immediately=true&force_search=false)

**原文摘要:** Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.

</details>


### [118] [Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.14459)
*Kamal Singh, Sami Marouani, Ahmad Al Sheikh, Pham Tran Anh Quang, Amaury Habrard*

**主要类别:** cs.LG

**概要:** This paper proposes using Kolmogorov-Arnold Networks (KAN) for interpretable reinforcement learning in network control, specifically for load balancing, by employing a PPO agent with a 1-layer actor KAN model and an MLP Critic network. It demonstrates the approach's effectiveness in improving network performance and provides interpretable policies.


<details>
  <summary>更多</summary>
  
**动机:** Existing RL approaches often lack interpretability and difficulty in extracting controller equations.

**方法:** Proposes using Kolmogorov-Arnold Networks (KAN) for interpretable RL in network control. Employs a PPO agent with a 1-layer actor KAN model and an MLP Critic network to learn load balancing policies.

**结果:** The approach allows extraction of controller equations from the learned neural networks, providing insights into the decision-making process. Demonstrates effectiveness in improving network performance while providing interpretable policies.

**结论:** The proposed method improves network performance and provides interpretable policies for load balancing through interpretable reinforcement learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Reinforcement+Learning+for+Load+Balancing+using+Kolmogorov-Arnold+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14459，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14459&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.

</details>


### [119] [Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium](https://arxiv.org/abs/2505.14463)
*Xinxin Fan, Wenxiong Chen, Mengfan Li, Wenqi Wei, Ling Liu*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法来确定图对抗鲁棒性的临界状态。


<details>
  <summary>更多</summary>
  
**动机:** 研究图对抗攻击的内在鲁棒性状态是否存在以及如何找到它。

**方法:** 将图上的对抗学习建模为复杂多目标动态系统，提出了一种广义理论框架和一维函数来捕捉图在扰动下的动态变化。

**结果:** 所提出的方法在五个常用的真实数据集和三种代表性攻击下显著优于最先进的防御方法。

**结论:** 证明了图对抗鲁棒性存在临界状态，并提供了找到该状态的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adverseness+vs.+Equilibrium%3A+Exploring+Graph+Adversarial+Resilience+through+Dynamic+Equilibrium，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14463，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14463&send_immediately=true&force_search=false)

**原文摘要:** Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.

</details>


### [120] [ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs](https://arxiv.org/abs/2505.14468)
*Yifan Sui, Hao Wang, Hanfei Yu, Yitao Hu, Jianxun Li, Hao Wang*

**主要类别:** cs.LG

**概要:** 提出了一种新的系统ServerlessLoRA，用于加速和降低低秩适应（LoRA）语言模型的推理服务。该系统通过共享骨干模型、预加载和资源管理技术，显著减少了时间到第一个令牌(TTFT)和货币成本。


<details>
  <summary>更多</summary>
  
**动机:** 当前的服务器less计算在处理通用大型语言模型(LLM)推理时表现良好，但在处理LoRA推理时由于冗余参数、加载延迟和资源争用等问题表现不佳。

**方法:** 设计了一个新的服务器less推理系统ServerlessLoRA，包括骨干模型共享、预加载方法和竞争感知的批处理与卸载策略。

**结果:** 实验表明，与最先进的LLM推理解决方案相比，ServerlessLoRA将TTFT减少了高达86%，并将货币成本降低了高达89%。

**结论:** ServerlessLoRA是一个有效的解决方案，可以显著提高LoRA LLM推理的效率和经济性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ServerlessLoRA%3A+Minimizing+Latency+and+Cost+in+Serverless+Inference+for+LoRA-Based+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14468，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14468&send_immediately=true&force_search=false)

**原文摘要:** Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.

</details>


### [121] [Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment](https://arxiv.org/abs/2505.14477)
*Maria Panagiotou, Lorenzo Brigato, Vivien Streit, Amanda Hayoz, Stephan Proennecke, Stavros Athanasopoulos, Mikkel T. Olsen, Elizabeth J. den Brok, Cecilie H. Svensson, Konstantinos Makrilakis, Maria Xatzipsalti, Andriani Vazeou, Peter R. Mertens, Ulrik Pedersen-Bjergaard, Bastiaan E. de Galan, Stavroula Mougiakakou*

**主要类别:** cs.LG

**概要:** This study proposes ABBA, a personalized insulin treatment recommendation approach using reinforcement learning, which improves time-in-range and reduces time spent out-of-range compared to a standard basal-bolus advisor for individuals with T1D and T2D.


<details>
  <summary>更多</summary>
  
**动机:** Adjusting insulin remains challenging for many people with type 1 and type 2 diabetes despite advancements in insulin preparations and technology.

**方法:** Developed and evaluated ABBA, a personalized insulin treatment recommendation approach based on reinforcement learning, compared to a standard basal-bolus advisor using in-silico tests.

**结果:** ABBA significantly improved time-in-range and reduced time spent below and above range compared to BBA. ABBA's performance improved over two months while BBA showed modest changes.

**结论:** ABBA has the potential to optimize glycemic control and support daily self-management for individuals with T1D and T2D, warranting human trials.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalised+Insulin+Adjustment+with+Reinforcement+Learning%3A+An+In-Silico+Validation+for+People+with+Diabetes+on+Intensive+Insulin+Treatment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14477&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.

</details>


### [122] [Learning to Integrate Diffusion ODEs by Averaging the Derivatives](https://arxiv.org/abs/2505.14502)
*Wenze Liu, Xiangyu Yue*

**主要类别:** cs.LG

**概要:** 提出一种中间策略，通过学习ODE积分来加速扩散模型推理，性能和成本平衡，实验结果显示在CIFAR-10和ImageNet上的显著效果。


<details>
  <summary>更多</summary>
  
**动机:** 传统数值求解器在极小步长时表现不佳，蒸馏技术常引入复杂性和不稳定性。

**方法:** 利用源自导数-积分关系的损失函数学习ODE积分，从几何角度定义这些损失为切线逐渐延伸到割线的secant losses。

**结果:** secant版本在CIFAR-10上达到10步FID 2.14，在ImageNet上达到4步FID 2.27和8步FID 1.96。

**结论:** 所提出的secant losses能有效加速扩散模型推理并保持高性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Integrate+Diffusion+ODEs+by+Averaging+the+Derivatives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14502，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14502&send_immediately=true&force_search=false)

**原文摘要:** To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.

</details>


### [123] [Latent Flow Transformer](https://arxiv.org/abs/2505.14513)
*Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu*

**主要类别:** cs.LG

**概要:** A new method called Latent Flow Transformer (LFT) is proposed to replace multiple layers in transformers with a single learned transport operator for better compression.


<details>
  <summary>更多</summary>
  
**动机:** Existing transformer architectures have many discrete layers which are not efficient. Continuous layers have shown better performance in other models.

**方法:** The LFT uses flow matching to train a transport operator instead of multiple layers. A new algorithm called Flow Walking is introduced to address limitations in preserving coupling.

**结果:** LFT trained with flow matching compresses 6 out of 24 layers and outperforms skipping 2 layers. With Flow Walking, it further distills 12 layers into one.

**结论:** This study shows the feasibility of replacing multiple layers with a single operator in transformers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Flow+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14513，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14513&send_immediately=true&force_search=false)

**原文摘要:** Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.

</details>


### [124] [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](https://arxiv.org/abs/2505.14522)
*Mahmuda Akhter Nishu, Chenyu Huang, Milad Roohi, Xin Zhong*

**主要类别:** cs.LG

**概要:** 提出了一种双流学习框架，结合天气数据和事件叙述，用于风灾预测，尤其适用于服务不足的部落社区。


<details>
  <summary>更多</summary>
  
**动机:** 现有系统主要关注气象要素，未能捕捉特定社区的脆弱性，影响应急响应和韧性规划。

**方法:** 提出的框架通过晚期融合机制整合数值天气数据与文本事件叙述，使用随机森林和RoBERTa模型。

**结果:** 实验结果显示在传统基线上的显著性能提升，并通过敏感性和消融研究增强了模型决策过程的透明度。

**结论:** 该研究展示了预测的有效性和支持应急准备及社区韧性的实际价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Dual-Stream+Learning+for+Local+Wind+Hazard+Prediction+in+Vulnerable+Communities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14522，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14522&send_immediately=true&force_search=false)

**原文摘要:** Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.

</details>


### [125] [Energy-Efficient Deep Reinforcement Learning with Spiking Transformers](https://arxiv.org/abs/2505.14533)
*Mohammad Irfan Uddin, Nishad Tasnim, Md Omor Faruk, Zejian Zhou*

**主要类别:** cs.LG

**概要:** 开发了一种新的STRL算法，它结合了SNN的能效和强化学习的决策能力，实验显示其在政策性能和能效上都优于传统Transformer。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于代理的Transformer虽然在解决复杂任务方面表现出色，但其高计算复杂度导致显著的能量消耗，限制了其在现实世界自主系统中的应用。因此，研究者们希望利用脉冲神经网络(SNNs)的生物启发结构来提供一种更节能的学习方式。

**方法:** 设计了一种使用多步泄漏积分与发射（LIF）神经元和注意力机制的SNN，并通过状态、动作和奖励编码增强了架构，形成了针对强化学习任务优化的Transformer结构。

**结果:** 提出的SNN Transformer在一系列最先进的基准测试中展示了显著改进的政策性能，同时保持了更高的能效。

**结论:** 提出了一种结合SNN和强化学习优势的STRL算法。实验表明该方法在政策表现上优于传统的基于代理的Transformer模型，并且具有更高的能效。这项工作为在复杂的现实世界决策场景中部署类生物的低成本机器学习模型指明了一个有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Efficient+Deep+Reinforcement+Learning+with+Spiking+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14533&send_immediately=true&force_search=false)

**原文摘要:** Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.

</details>


### [126] [SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach](https://arxiv.org/abs/2505.14531)
*Shaoye Luo, Xinxin Fan, Quanliang Jing, Chi Lin, Mengfan Li, Yunfeng Lu, Yongjun Xu*

**主要类别:** cs.LG

**概要:** 提出了一种基于经典伊辛模型的通用触发净化方法SifterNet，用于抵抗卷积神经网络和视觉Transformer大模型中的后门攻击。该方法无需提前了解目标模型详细信息或访问大量清洁样本，具有轻量级和黑盒防御特性。实验验证了其在多种常用数据集上的有效性和优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有触发检测/移除研究通常需要提前了解目标模型详细信息、访问大量清洁样本甚至模型重训练授权，给实际应用带来不便，尤其是无法访问目标模型时。理想对策应能消除植入的触发器而不考虑任何目标模型。

**方法:** 利用霍普菲尔德网络的记忆关联功能，通过引入伊辛模型的思想，提出了轻量级黑盒防御方法SifterNet，对输入样本的触发器进行有效净化。

**结果:** 实验验证了所提方法在触发器净化和高精度实现方面的有效性，并且在几种常用的基准数据集上与最先进的基线相比，SifterNet表现出显著的优越性能。

**结论:** 本文提出了一种新的基于经典伊辛模型的通用触发净化方法SifterNet，用于抵御卷积神经网络和视觉Transformer大模型中的后门攻击，该方法具有轻量级和黑盒防御特性，实验结果表明其有效性和优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SifterNet%3A+A+Generalized+and+Model-Agnostic+Trigger+Purification+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14531&send_immediately=true&force_search=false)

**原文摘要:** Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.

</details>


### [127] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/abs/2505.14555)
*Yingtao Luo, Shikai Fang, Binqing Wu, Qingsong Wen, Liang Sun*

**主要类别:** cs.LG

**概要:** 提出一种结合物理定律和数据驱动模型的天气预报框架PhyDL-NWP，它能够提高预测性能和物理一致性，并且实现无分辨率缩放。


<details>
  <summary>更多</summary>
  
**动机:** 传统数值天气预报方法计算密集且不完整，深度学习模型虽然高效准确但缺乏可解释性和泛化能力。

**方法:** 开发了一种名为PhyDL-NWP的物理引导深度学习框架，该框架通过潜在力参数化整合物理方程到数据驱动模型中，并使用物理信息损失来对齐预测与控制动力学。

**结果:** PhyDL-NWP实现了高达170倍的推理加速，仅需55K参数，并在实验中提升了预测性能和物理一致性。

**结论:** PhyDL-NWP展示了在保持高效率的同时增强天气预报的物理一致性的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Guided+Learning+of+Meteorological+Dynamics+for+Weather+Downscaling+and+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14555，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14555&send_immediately=true&force_search=false)

**原文摘要:** Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [128] [Bellman operator convergence enhancements in reinforcement learning algorithms](https://arxiv.org/abs/2505.14564)
*David Krame Kadurha, Domini Jocema Leko Moutouo, Yae Ulrich Gaba*

**主要类别:** cs.LG

**概要:** 本文探讨了强化学习的拓扑基础，聚焦于状态、动作和策略空间的结构。通过利用Banach不动点定理与Bellman算子，解释了RL算法的收敛性，并提出改进收敛率的新方法。


<details>
  <summary>更多</summary>
  
**动机:** 研究强化学习的数学基础，尤其是状态、动作和策略空间的结构，以提高算法效率。

**方法:** 回顾了关键的数学概念如完备度量空间，并利用Banach不动点定理和Bellman算子来证明RL算法的收敛性。

**结果:** 提出了替代形式的Bellman算子，并在MountainCar、CartPole和Acrobot等标准环境中展示了其对提升收敛率和性能的影响。

**结论:** 深入理解强化学习的数学基础可以开发出更有效的决策问题求解算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bellman+operator+convergence+enhancements+in+reinforcement+learning+algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14564&send_immediately=true&force_search=false)

**原文摘要:** This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.

</details>


### [129] [Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning](https://arxiv.org/abs/2505.14535)
*Jiangrong Shen, Yulin Xie, Qi Xu, Gang Pan, Huajin Tang, Badong Chen*

**主要类别:** cs.LG

**概要:** 提出了一种新的时间注意力引导自适应融合框架(TAAF)，用于多模态尖峰神经网络(SNNs)，解决了模态不平衡和时间错位的问题，并在多个数据集上实现了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态SNN方法存在跨模态收敛速度不协调和静态融合机制忽视时变跨模态交互的问题。

**方法:** 提出的时间注意力引导自适应融合框架包括两个协同创新：1) 时间注意力引导自适应融合模块；2) 基于上述注意分数的时间自适应平衡融合损失。

**结果:** 在CREMA-D、AVE和EAD数据集上的评估显示了最先进的性能(分别为77.55%、70.65%和97.5%的准确率)，并且具有能量效率。

**结论:** 这项工作为类脑系统中的时间一致多模态学习建立了新的范式，弥合了生物感觉处理与高效机器智能之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spiking+Neural+Networks+with+Temporal+Attention-Guided+Adaptive+Fusion+for+imbalanced+Multi-modal+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14535&send_immediately=true&force_search=false)

**原文摘要:** Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.

</details>


### [130] [KIPPO: Koopman-Inspired Proximal Policy Optimization](https://arxiv.org/abs/2505.14566)
*Andrei Cozma, Landon Harris, Hairong Qi*

**主要类别:** cs.LG

**概要:** This paper introduces KIPPO, which enhances PPO by using a Koopman-approximation auxiliary network to learn a linear latent-space representation of system dynamics, improving performance and stability in continuous control tasks.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenges of unstable learning trajectories and high variance in gradient estimates when using policy gradient methods in environments with complex non-linear dynamics.

**方法:** Developing Koopman-Inspired Proximal Policy Optimization (KIPPO) by adding a Koopman-approximation auxiliary network to standard policy optimization algorithms.

**结果:** Consistent improvement over PPO baseline with 6-60% increased performance and up to 91% reduction in variability in various continuous control tasks.

**结论:** KIPPO effectively learns a linear latent-space representation, enhancing both performance and stability of policy gradient methods in complex environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KIPPO%3A+Koopman-Inspired+Proximal+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14566&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.

</details>


### [131] [Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions](https://arxiv.org/abs/2505.14543)
*Utsav Dutta, Sina Khoshfetrat Pakazad, Henrik Ohlsson*

**主要类别:** cs.LG

**概要:** 我们介绍了CHARM，一个多变量时间序列的基础嵌入模型，它学习共享、可转移和领域感知的表示。CHARM在多个下游任务上达到了最先进的性能，为时间序列表示学习设定了新的基准。


<details>
  <summary>更多</summary>
  
**动机:** 传统的时间序列模型是特定于任务的，并且常常依赖于数据集特定的训练和广泛的特征工程。虽然基于Transformer的架构提高了可扩展性，但文本、视觉和音频中的基础模型在时间序列中仍未被充分探索，并且主要局限于预测。

**方法:** CHARM包含架构创新，结合了通道级文本描述，同时保持对通道顺序的不变性。该模型使用联合嵌入预测架构（JEPA）进行训练，具有新颖的数据增强方案和损失函数，旨在提高解释性和训练稳定性。

**结果:** 我们的7M参数模型在各种下游任务中达到了最先进的性能，为时间序列表示学习设定了新的基准。

**结论:** CHARM是一种用于多变量时间序列的基础嵌入模型，它可以学习共享、可转移和领域感知的表示，并在多种下游任务中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+to+Embed%3A+Unlocking+Foundation+Models+for+Time+Series+with+Channel+Descriptions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14543，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14543&send_immediately=true&force_search=false)

**原文摘要:** Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.

</details>


### [132] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran*

**主要类别:** cs.LG

**概要:** This paper addresses the issue of false negatives in verifiers used in reinforcement learning for improving large language models' reasoning capabilities.


<details>
  <summary>更多</summary>
  
**动机:** To investigate and solve the problem of false negatives in verifiers which lead to incorrect rejection of correct model outputs.

**方法:** Analyze the Big-Math-RL-Verified dataset and propose a new method called tinyV.

**结果:** TinyV improves pass rates by up to 10% and speeds up convergence on math-reasoning tasks.

**结论:** Improving verifier accuracy through methods like tinyV can significantly enhance reinforcement learning effectiveness.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TinyV%3A+Reducing+False+Negatives+in+Verification+Improves+RL+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14625，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14625&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [133] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
*Fnu Mohbat, Mohammed J Zaki*

**主要类别:** cs.LG

**概要:** KERL是一个结合食品知识图谱和大型语言模型的系统，用于个性化食品推荐、生成带营养信息的食谱，并在自建数据集上验证了其性能。


<details>
  <summary>更多</summary>
  
**动机:** 利用大型语言模型和知识图谱改善食品理解的研究较少，特别是两者结合应用于食品推荐系统方面。

**方法:** 提出了一种名为KERL的新系统，该系统从知识图谱中提取实体并检索子图，然后将其作为上下文输入到大型语言模型中，从而选择满足条件的食谱，并生成烹饪步骤和营养信息。

**结果:** KERL在自建的基准数据集上的实验表现显著优于现有方法，提供了一个完整的食品推荐、食谱生成和营养分析解决方案。

**结论:** KERL展示了知识图谱增强型大型语言模型在食品相关应用中的潜力，其代码和基准数据集已公开可用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KERL%3A+Knowledge-Enhanced+Personalized+Recipe+Recommendation+using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14629&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


### [134] [Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks](https://arxiv.org/abs/2505.14659)
*Navneet Kaur, Lav Gupta*

**主要类别:** cs.LG

**概要:** This paper discusses the importance of securing medical applications in healthcare systems adopting advanced wireless networks and connected devices. It examines how explainable AI techniques can help identify vulnerabilities and enhance security in 6G-enabled healthcare.


<details>
  <summary>更多</summary>
  
**动机:** The increasing use of advanced wireless networks and connected devices in healthcare introduces serious security risks that could lead to life-threatening consequences.

**方法:** Using explainable AI techniques such as SHAP, LIME, and DiCE to uncover vulnerabilities and strengthen defenses in 6G-enabled healthcare.

**结果:** Experimental analysis supports the approach and shows promising results.

**结论:** The integration of AI and cloud in 6G will play a transformative role in healthcare but requires addressing new security concerns.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainable+AI+for+Securing+Healthcare+in+IoT-Integrated+6G+Wireless+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14659，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14659&send_immediately=true&force_search=false)

**原文摘要:** As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.

</details>


### [135] [Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge](https://arxiv.org/abs/2505.14592)
*Alexandre Broggi, Nathaniel Bastian, Lance Fiondella, Gokhan Kul*

**主要类别:** cs.LG

**概要:** 研究了几种神经网络剪枝方法在新网络安全数据集上的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 探索神经网络剪枝方法在非设计用途的简单网络类型上对新环境的适应性。

**方法:** 分析多种剪枝程度下的几种神经网络剪枝方法。

**结果:** 许多剪枝方法未能良好地泛化到该问题，仅有少数能达到可接受的效果。

**结论:** 确定了最适合所研究任务的剪枝方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Pruning+of+Deep+Neural+Networks+for+Resource-Aware+Embedded+Intrusion+Detection+on+the+Edge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14592，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14592&send_immediately=true&force_search=false)

**原文摘要:** Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.

</details>


### [136] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/abs/2505.14595)
*Nima Hosseini Dashtbayaz, Hesam Salehipour, Adrian Butscher, Nigel Morris*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-informed+Reduced+Order+Modeling+of+Time-dependent+PDEs+via+Differentiable+Solvers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14595&send_immediately=true&force_search=false)

**原文摘要:** Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.

</details>


### [137] [Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials](https://arxiv.org/abs/2505.14606)
*Maksim Zhdanov, Vladislav Kurenkov*

**主要类别:** cs.LG

**概要:** This work introduces a novel method called $\Phi$-Module, which uses Poisson's equation to enforce physics-based regularization in neural network interatomic potentials.


<details>
  <summary>更多</summary>
  
**动机:** To accelerate training and improve fidelity of deep learning models by incorporating auxiliary constraints grounded in physical laws.

**方法:** $\Phi$-Module enforces Poisson's equation within the message-passing framework to learn electrostatic interactions in a self-supervised manner.

**结果:** Models combined with $\Phi$-Module show robust improvements over baseline counterparts on OE62 and MD22 benchmarks.

**结论:** Embedding first-principles constraints in neural interatomic potentials can significantly enhance performance while maintaining efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Electrostatics+from+Laplacian+Eigenbasis+for+Neural+Network+Interatomic+Potentials，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14606&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.

</details>


### [138] [MMD-Newton Method for Multi-objective Optimization](https://arxiv.org/abs/2505.14610)
*Hao Wang, Chenyu Shi, Angel E. Rodriguez-Fernandez, Oliver Schütze*

**主要类别:** cs.LG

**概要:** This paper proposes using MMD to solve continuous multi-objective optimization problems, devising a novel MMD-based Newton method, and demonstrates that hybridizing it with MOEAs yields better optimization accuracy.


<details>
  <summary>更多</summary>
  
**动机:** To improve the accuracy of multi-objective optimization problems by using MMD to measure the distance between two sets, instead of traditional methods like Hausdorff.

**方法:** Using Maximum Mean Discrepancy (MMD) to solve continuous multi-objective optimization problems by providing the gradient and Hessian matrix, and devising a novel set-oriented, MMD-based Newton (MMDN) method.

**结果:** The hybrid algorithm (MMDN + MOEA) achieved significantly better optimization accuracy compared to MOEAs alone on 11 benchmark problems.

**结论:** The proposed hybrid algorithm combining MMDN and MOEAs outperforms MOEAs alone in terms of optimization accuracy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMD-Newton+Method+for+Multi-objective+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14610，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14610&send_immediately=true&force_search=false)

**原文摘要:** Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.

</details>


### [139] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/abs/2505.14613)
*Emmanuel Noutahi, Jason Hartford, Prudencio Tossou, Shawn Whitfield, Alisandra K. Denton, Cas Wognum, Kristina Ulicna, Jonathan Hsu, Michael Cuccarese, Emmanuel Bengio, Dominique Beaini, Christopher Gibson, Daniel Cohen, Berton Earnshaw*

**主要类别:** cs.LG

**概要:** 提出了一种基于AI和实验结合的方法来构建虚拟细胞，并通过生物基准测试来指导其发展，这种方法不仅有助于发现新型生物学，还可以作为构建更高层次模型（如虚拟患者）的框架。


<details>
  <summary>更多</summary>
  
**动机:** 当前药物发现过程中临床试验成本高昂，需要更可靠的计算模型来模拟患者反应；而现有的细胞生物学复杂性和规模使得创建虚拟细胞极具挑战性。

**方法:** 提出了一种实验室与AI结合的循环方法用于生成新见解，并强调了设计具有治疗意义的虚拟细胞的关键原则。

**结果:** 尽管完全实现虚拟细胞仍面临困难，但利用最新的AI、计算能力、实验室自动化和高通量细胞分析技术提供了新的机会。

**结论:** 提出的虚拟细胞方法不仅能用于发现新型生物学，还为构建其他高层次模型提供了一个有用的框架，这可能对药物发现产生积极影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Virtual+Cells%3A+Predict%2C+Explain%2C+Discover，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14613&send_immediately=true&force_search=false)

**原文摘要:** Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [140] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
*Morgan Lindsay Heisler, Linzi Xing, Ge Shi, Hanieh Sadri, Gursimran Singh, Weiwei Zhang, Tao Ye, Ying Xiong, Yong Zhang, Zhenan Fan*

**主要类别:** cs.LG

**概要:** 本文介绍了对比LoRA解码(CoLD)，一种用于高效利用LoRA适应模型中任务特定知识的新解码框架。通过优化Ascend NPU内核，CoLD提升了任务准确率并降低了延迟。


<details>
  <summary>更多</summary>
  
**动机:** 传统的解码方法在复杂推理任务上容易受到基础模型偏见的影响，导致响应不适应特定任务。

**方法:** 提出了一种新的解码框架CoLD，使用对比解码方法优先选择与LoRA学习表示更一致的候选标记。为了提高计算效率，开发了Ascend NPU优化内核。

**结果:** CoLD提高了任务准确性，比贪心解码减少了28%的端到端延迟。

**结论:** 提出了对比LoRA解码(CoLD)，实现了更好的下游性能。同时开发了Ascend NPU优化内核，提高了计算效率并降低了延迟。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Learned+Knowledge+in+LoRA+Adapters+Through+Efficient+Contrastive+Decoding+on+Ascend+NPUs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14620&send_immediately=true&force_search=false)

**原文摘要:** Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [141] [Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning](https://arxiv.org/abs/2505.14635)
*Benjamin Prada, Shion Matsumoto, Abdul Malik Zekri, Ankur Mali*

**主要类别:** cs.LG

**概要:** 提出了一种新的理论框架，将预测编码与深度网络中的最小描述长度原则相结合，并证明了层间预测编码执行MDL两部分代码目标的块坐标下降，从而联合最小化经验风险和模型复杂度。此外，还推导出了一种新的泛化界限，并证明了重复的预测编码更新收敛到块坐标平稳点。


<details>
  <summary>更多</summary>
  
**动机:** 连接预测编码与最小描述长度原则，提供理论基础和生物合理性。

**方法:** 提出理论框架并证明层间预测编码执行MDL两部分代码目标的块坐标下降。

**结果:** 推导出新的泛化界限，证明重复预测编码更新收敛到块坐标平稳点。

**结论:** 这是首次为预测编码训练的深度模型提供正式泛化和收敛保证的结果，将预测编码定位为反向传播的一个有理论依据且生物合理的替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Predictive+Coding+and+MDL%3A+A+Two-Part+Code+Framework+for+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14635，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14635&send_immediately=true&force_search=false)

**原文摘要:** We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \^{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.

</details>


### [142] [Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data](https://arxiv.org/abs/2505.14643)
*Ane G. Domingo-Aldama, Marcos Merino Prado, Alain García Olea, Koldo Gojenola Galletebeitia, Josu Goikoetxea Salutregi, Aitziber Atutxa Salazar*

**主要类别:** cs.LG

**概要:** This study evaluates traditional clinical scores, ML models, and an LTM approach for predicting atrial fibrillation recurrence within one month to two years post-onset, developing a method to integrate structured and unstructured EHR data to improve dataset quality.


<details>
  <summary>更多</summary>
  
**动机:** Traditional scores have limited predictive accuracy for AF recurrence, and early diagnosis studies often rely on EHR data prone to errors and missing info.

**方法:** Combining structured clinical data with NLP-processed free-text discharge reports to create a tabular dataset for model evaluation.

**结果:** The LTM approach outperformed traditional scores and ML models in predicting AF recurrence, also revealing gender and age bias.

**结论:** Integrating structured and unstructured data improves dataset quality and highlights the potential of ML-based approaches like the LTM model.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Early+Diagnosis+of+Atrial+Fibrillation+Recurrence%3A+A+Large+Tabular+Model+Approach+with+Structured+and+Unstructured+Clinical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14643，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14643&send_immediately=true&force_search=false)

**原文摘要:** BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.

</details>


### [143] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669)
*Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh*

**主要类别:** cs.LG

**概要:** 提出一种名为Quartet的新方法，使大型语言模型在FP4精度下的端到端训练成为可能，并揭示了一个新的低精度缩放定律。


<details>
  <summary>更多</summary>
  
**动机:** 降低大型语言模型训练的计算需求和成本，同时保持准确性。

**方法:** 系统研究硬件支持的FP4训练，并引入Quartet方法，主要计算都在低精度下完成。

**结果:** 在Llama型模型上的广泛评估显示了新的低精度缩放定律，实现了接近最优的准确性和计算权衡。

**结论:** 证明了完全基于FP4的训练是标准精度和FP8训练的竞争替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quartet%3A+Native+FP4+Training+Can+Be+Optimal+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14669，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14669&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
"near-optimal" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [144] [AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data](https://arxiv.org/abs/2505.13466)
*Vu Dinh Xuan, Hao Vo, David Murphy, Hoang D. Nguyen*

**主要类别:** cs.AI

**概要:** 提出了一种新的多智能体框架，通过LLM驱动的评估者和编辑者之间的协作生成针对安全关键场景的合成图像。


<details>
  <summary>更多</summary>
  
**动机:** 在安全关键应用（如施工安全）中缺乏描绘危险情况的数据阻碍了AI系统的训练。

**方法:** 一种迭代、循环协作的多智能体框架，包括一个作为LLM评委的评估者代理和一个基于指导生成和优化场景的编辑者代理。

**结果:** 该方法能够生成满足现实规格且具有语义一致性的有用场景图像，平衡了安全需求与视觉语义。

**结论:** 此框架可能解决多媒体安全应用中的数据稀缺问题，提供稳健且美观的模拟解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentSGEN%3A+Multi-Agent+LLM+in+the+Loop+for+Semantic+Collaboration+and+GENeration+of+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13466，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13466&send_immediately=true&force_search=false)

**原文摘要:** The scarcity of data depicting dangerous situations presents a major obstacle
to training AI systems for safety-critical applications, such as construction
safety, where ethical and logistical barriers hinder real-world data
collection. This creates an urgent need for an end-to-end framework to generate
synthetic data that can bridge this gap. While existing methods can produce
synthetic scenes, they often lack the semantic depth required for scene
simulations, limiting their effectiveness. To address this, we propose a novel
multi-agent framework that employs an iterative, in-the-loop collaboration
between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce
semantic consistency and safety-specific constraints, and an Editor Agent,
which generates and refines scenes based on this guidance. Powered by LLM's
capabilities to reasoning and common-sense knowledge, this collaborative design
produces synthetic images tailored to safety-critical scenarios. Our
experiments suggest this design can generate useful scenes based on realistic
specifications that address the shortcomings of prior approaches, balancing
safety requirements with visual semantics. This iterative process holds promise
for delivering robust, aesthetically sound simulations, offering a potential
solution to the data scarcity challenge in multimedia safety applications.

</details>


### [145] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
*Rene Heesch, Sebastian Eilermann, Alexander Windmann, Alexander Diedrich, Philipp Rosenthal, Oliver Niggemann*

**主要类别:** cs.AI

**概要:** This paper introduces a curated database of engineering questions to evaluate the performance of large language models on complex engineering tasks.


<details>
  <summary>更多</summary>
  
**动机:** Current evaluations of LLMs in engineering rely on simplified use cases and ad hoc scenarios that do not fully capture critical engineering competencies.

**方法:** The authors created a database of over 100 questions derived from authentic engineering scenarios and used it to evaluate four state-of-the-art LLMs.

**结果:** LLMs perform well in basic temporal and structural reasoning but struggle with abstract reasoning, formal modeling, and context-sensitive engineering logic.

**结论:** The study highlights the need for more comprehensive evaluation methods for LLMs in engineering.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Large+Language+Models+for+Real-World+Engineering+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13484，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13484&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [146] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
*Wenkang Han, Wang Lin, Liya Hu, Zhenlong Dai, Yiyun Zhou, Mengze Li, Zemin Liu, Chang Yao, Jingyuan Chen*

**主要类别:** cs.AI

**概要:** Proposes TransKT, a method using concept graph guided knowledge transfer to improve knowledge tracing across multiple courses.


<details>
  <summary>更多</summary>
  
**动机:** Existing KT models mainly focus on single-course data, limiting their ability to understand learners' knowledge states comprehensively.

**方法:** Uses a cross-course concept graph and an LLM-to-LM pipeline to enhance GCN-based knowledge transfer, with a contrastive objective to refine knowledge state representation.

**结果:** Enhances the estimation of learners' knowledge states across different courses.

**结论:** TransKT provides a more robust and accurate way to trace learners' knowledge states by integrating cross-course information.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Contrastive+Cross-Course+Knowledge+Tracing+via+Concept+Graph+Guided+Knowledge+Transfer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13489，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13489&send_immediately=true&force_search=false)

**原文摘要:** Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [147] [ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model](https://arxiv.org/abs/2505.13496)
*Przemek Pospieszny, Wojciech Mormul, Karolina Szyndler, Sanjeev Kumar*

**主要类别:** cs.AI

**概要:** ADALog是一种新的无监督异常检测框架，专为处理软件系统生成的异构日志数据设计。它基于变压器模型，并在正常日志上进行微调，能够动态适应系统行为的变化。实验表明其性能优于其他方法。


<details>
  <summary>更多</summary>
  
**动机:** 现代软件系统产生的日志数据具有动态格式、事件序列碎片化和时间模式变化等特点，使得异常检测既重要又困难。

**方法:** ADALog使用基于变压器的双向编码器模型，该模型预训练并针对正常日志进行了微调，用于捕获特定领域的句法和语义模式。异常通过标记级别的重建概率来识别，并使用基于百分位数的自适应阈值处理。

**结果:** ADALog在BGL、Thunderbird和Spirit基准数据集上的表现显示了良好的泛化能力和与现有最先进方法相比的竞争性性能。

**结论:** ADALog提供了一种新的、更灵活的方法来进行日志异常检测，特别是在不需要依赖严格序列依赖或标记数据的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADALog%3A+Adaptive+Unsupervised+Anomaly+detection+in+Logs+with+Self-attention+Masked+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13496&send_immediately=true&force_search=false)

**原文摘要:** Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.

</details>


### [148] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
*David Noever, Forrest McKee*

**主要类别:** cs.AI

**概要:** 本研究探索大型语言模型（LLMs）作为自主代理执行现实世界任务，包括自由职业软件开发。我们创建了一个新的基准来评估LLMs在从经济数据派生的自由编程和数据分析任务上的表现。通过Kaggle自由职业者数据集中的工作发布创建合成任务，并标准化价格。我们的方法简化了评估过程，使它更具可扩展性和重复性。结果显示Claude 3.5 Haiku表现最佳，其次是GPT-4o-mini，Qwen 2.5和Mistral。


<details>
  <summary>更多</summary>
  
**动机:** 评估大型语言模型在自由职业编程和数据分析任务上的能力，探索其作为自主代理的可行性。

**方法:** 使用从Kaggle自由职业者数据集中的工作发布创建的合成任务构建基准，标准化价格，并使用自动化测试案例和预测价格值进行评估。

**结果:** Claude 3.5 Haiku表现最佳，其次是GPT-4o-mini，Qwen 2.5和Mistral。最强的模型在大多数任务上表现良好，很少完全失败。

**结论:** 这项研究展示了LLMs作为自由职业开发者的潜力，但还存在一些局限性和性能差距需要解决。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+AI+Freelancers+Compete%3F+Benchmarking+Earnings%2C+Reliability%2C+and+Task+Success+at+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13511&send_immediately=true&force_search=false)

**原文摘要:** This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [149] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/abs/2505.13522)
*Nathalie Sanghikian, Rafael Meirelles, Rafael Martinelli, Anand Subramanian*

**主要类别:** cs.AI

**概要:** This paper introduces a heuristic method combining Beam Search and Iterated Local Search to solve a deterministic, finite-horizon, single-product Maritime Inventory Routing Problem, improving solutions for ten instances.


<details>
  <summary>更多</summary>
  
**动机:** To provide a collection of publicly available benchmark instances and encourage the use of MIRPLib.

**方法:** Combining a variation of a Beam Search algorithm with an Iterated Local Search procedure.

**结果:** Improved the best-known solution for ten instances among 72 tested.

**结论:** The proposed heuristic approach improved the best-known solutions for ten out of 72 instances within an acceptable CPU time.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Heuristic+Algorithm+Based+on+Beam+Search+and+Iterated+Local+Search+for+the+Maritime+Inventory+Routing+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13522，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13522&send_immediately=true&force_search=false)

**原文摘要:** Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for ten instances within an acceptable CPU time.

</details>


### [150] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
*Junxiao Yang, Jinzhe Tu, Haoran Liu, Xiaoce Wang, Chujie Zheng, Zhexin Zhang, Shiyao Cui, Caishun Chen, Tiantian He, Hongning Wang, Yew-Soon Ong, Minlie Huang*

**主要类别:** cs.AI

**概要:** This paper identifies two problematic reasoning patterns in current large reasoning models and proposes a new framework called BARREL to improve their factual reliability.


<details>
  <summary>更多</summary>
  
**动机:** Current large reasoning models often produce incorrect but confidently given answers, raising concerns about their factual reliability.

**方法:** Proposes a novel framework called BARREL that promotes concise and boundary-aware factual reasoning.

**结果:** BARREL-training improves the reliability of a specific model from 39.33% to 61.48%, while maintaining comparable accuracy to models fine-tuned on reasoning data.

**结论:** The proposed pilot study is promising for building more reliable and factual System 2 large reasoning models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BARREL%3A+Boundary-Aware+Reasoning+for+Factual+and+Reliable+LRMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13529&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [151] [FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs](https://arxiv.org/abs/2505.13533)
*Junzhe Jiang, Chang Yang, Aixin Cui, Sihan Jin, Ruiyu Wang, Bo Li, Xiao Huang, Dongning Sun, Xinrun Wang*

**主要类别:** cs.AI

**概要:** FinMaster是一个综合性的金融基准，旨在全面评估大型语言模型在财务素养、会计、审计和咨询方面的能力。该基准包含三个模块：FinSim生成合成数据，FinSuite提供多样化任务，FinEval提供统一评估接口。实验表明当前LLMs在复杂财务推理任务上的表现显著下降。


<details>
  <summary>更多</summary>
  
**动机:** 现有金融领域LLMs评估基准存在数据不足、任务设计简单及评估框架不完整的问题。

**方法:** 提出FinMaster，包括FinSim（生成合成数据）、FinSuite（提供多样化任务）和FinEval（提供统一评估接口）。

**结果:** 实验显示最先进的LLMs在复杂财务推理任务中的准确性显著下降，从基本任务的90%以上降至多步骤推理场景中的40%。

**结论:** FinMaster是首个覆盖全流程金融工作流的具有挑战性任务的基准，有望缩小研究与行业实践之间的差距，推动LLMs在实际金融应用中的采用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FinMaster%3A+A+Holistic+Benchmark+for+Mastering+Full-Pipeline+Financial+Workflows+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13533&send_immediately=true&force_search=false)

**原文摘要:** Financial tasks are pivotal to global economic stability; however, their
execution faces challenges including labor intensive processes, low error
tolerance, data fragmentation, and tool limitations. Although large language
models (LLMs) have succeeded in various natural language processing tasks and
have shown potential in automating workflows through reasoning and contextual
understanding, current benchmarks for evaluating LLMs in finance lack
sufficient domain-specific data, have simplistic task design, and incomplete
evaluation frameworks. To address these gaps, this article presents FinMaster,
a comprehensive financial benchmark designed to systematically assess the
capabilities of LLM in financial literacy, accounting, auditing, and
consulting. Specifically, FinMaster comprises three main modules: i) FinSim,
which builds simulators that generate synthetic, privacy-compliant financial
data for companies to replicate market dynamics; ii) FinSuite, which provides
tasks in core financial domains, spanning 183 tasks of various types and
difficulty levels; and iii) FinEval, which develops a unified interface for
evaluation. Extensive experiments over state-of-the-art LLMs reveal critical
capability gaps in financial reasoning, with accuracy dropping from over 90% on
basic tasks to merely 40% on complex scenarios requiring multi-step reasoning.
This degradation exhibits the propagation of computational errors, where
single-metric calculations initially demonstrating 58% accuracy decreased to
37% in multimetric scenarios. To the best of our knowledge, FinMaster is the
first benchmark that covers full-pipeline financial workflows with challenging
tasks. We hope that FinMaster can bridge the gap between research and industry
practitioners, driving the adoption of LLMs in real-world financial practices
to enhance efficiency and accuracy.

</details>


### [152] [Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems](https://arxiv.org/abs/2505.13546)
*Ke Chen, Yufei Zhou, Xitong Zhang, Haohan Wang*

**主要类别:** cs.AI

**概要:** 提出了一种新的稳定性感知的自动提示生成系统，该系统通过量化提示的语义稳定性来提高提示质量和系统级性能。实验表明，该框架提高了准确性和输出一致性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的提示生成方法只关注即时任务表现，忽视了决定其可靠性的内在品质，且未能考虑大型语言模型的固有随机性。

**方法:** 提出了语义稳定性作为评估提示响应一致性的标准，并微调了一个基于LLaMA的评估器来自动测量它在不同任务中的表现。开发了首个稳定性感知的通用提示生成系统，利用稳定性反馈迭代提升提示质量和系统级性能。

**结果:** 在通用和特定领域的任务中，所提出的稳定性感知框架提高了准确性和输出一致性。

**结论:** 这项工作通过从一次性结果转向持久可靠性，为提示设计提供了新的视角，并为构建更可靠的通用系统提供了实用工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prompt+Stability+Matters%3A+Evaluating+and+Optimizing+Auto-Generated+Prompt+in+General-Purpose+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13546，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13546&send_immediately=true&force_search=false)

**原文摘要:** Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.

</details>


### [153] [Counter-Inferential Behavior in Natural and Artificial Cognitive Systems](https://arxiv.org/abs/2505.13551)
*Serge Dolgikh*

**主要类别:** cs.AI

**概要:** 研究探索了自然和人工认知系统中反推理行为的出现，即代理错误归因经验成功或抑制适应的行为模式，导致认识论僵化或适应不良的稳定性。通过不平衡奖励、元认知归因和感知模型脆弱性下的保护性重构等典型场景分析，发现这些行为通过内部信息模型、实证反馈和高阶评估机制的结构化交互而产生。研究识别出反推理行为作为一种普遍的认知脆弱性，即使在其他方面适应良好的系统中也可能表现出来。研究强调了在稳定条件下保持最小适应激活的重要性，并提出了抗信息压力刚性的认知架构设计原则。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在理解自然和人工认知系统中反推理行为的出现及其对认识论僵化的影响。

**方法:** 通过分析不平衡奖励、元认知归因和感知模型脆弱性下的保护性重构等典型场景，探讨内部信息模型、实证反馈和高阶评估机制之间的结构化交互。

**结果:** 发现反推理行为是一种普遍的认知脆弱性，即使在其他方面适应良好的系统中也可能表现出来。

**结论:** 强调了在稳定条件下保持最小适应激活的重要性，并提出了抗信息压力刚性的认知架构设计原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Counter-Inferential+Behavior+in+Natural+and+Artificial+Cognitive+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13551，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13551&send_immediately=true&force_search=false)

**原文摘要:** This study explores the emergence of counter-inferential behavior in natural
and artificial cognitive systems, that is, patterns in which agents
misattribute empirical success or suppress adaptation, leading to epistemic
rigidity or maladaptive stability. We analyze archetypal scenarios in which
such behavior arises: reinforcement of stability through reward imbalance,
meta-cognitive attribution of success to internal superiority, and protective
reframing under perceived model fragility. Rather than arising from noise or
flawed design, these behaviors emerge through structured interactions between
internal information models, empirical feedback, and higher-order evaluation
mechanisms. Drawing on evidence from artificial systems, biological cognition,
human psychology, and social dynamics, we identify counter-inferential behavior
as a general cognitive vulnerability that can manifest even in otherwise
well-adapted systems. The findings highlight the importance of preserving
minimal adaptive activation under stable conditions and suggest design
principles for cognitive architectures that can resist rigidity under
informational stress.

</details>


### [154] [Language and Thought: The View from LLMs](https://arxiv.org/abs/2505.13561)
*Daniel Rothschild*

**主要类别:** cs.AI

**概要:** This paper discusses how the success of Large Language Models (LLMs) in inferential reasoning supports Daniel Dennett's thesis that language significantly impacts thought processes.


<details>
  <summary>更多</summary>
  
**动机:** To explore Dennett's thesis regarding the transformative effect of language on cognition by analyzing the capabilities of AI systems.

**方法:** Examining the performance of AI systems with and without linguistic training to test Dennett's hypothesis.

**结果:** The success of LLMs in inferential reasoning, albeit limited, suggests that language enhances cognitive abilities by making inference computationally feasible.

**结论:** AI findings suggest that language plays a crucial role in human cognition, similar to its impact on AI systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+and+Thought%3A+The+View+from+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13561，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13561&send_immediately=true&force_search=false)

**原文摘要:** Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind
you get when you add language to it is so different from the kind of mind you
can have without language that calling them both minds is a mistake." Recent
work in AI can be seen as testing Dennett's thesis by exploring the performance
of AI systems with and without linguistic training. I argue that the success of
Large Language Models at inferential reasoning, limited though it may be,
supports Dennett's radical view about the effect of language on thought. I
suggest it is the abstractness and efficiency of linguistic encoding that lies
behind the capacity of LLMs to perform inferences across a wide range of
domains. In a slogan, language makes inference computationally tractable. I
assess what these results in AI indicate about the role of language in the
workings of our own biological minds.

</details>


### [155] [MAFA: A multi-agent framework for annotation](https://arxiv.org/abs/2505.13668)
*Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem*

**主要类别:** cs.AI

**概要:** 提出一个多智能体框架用于FAQ标注，该框架结合了多种具有不同方法的专用智能体和一个重新排名候选者的裁判智能体以产生最佳结果。实验表明，与单智能体方法相比，该框架在多个指标上均有显著改进，并且特别擅长处理模糊查询。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法可能无法捕捉到用户询问中的细微差别，因此需要一种新的方法来提高信息检索的准确性和效率。

**方法:** 引入了一个多智能体框架，其中包含多个具有不同方法的专用智能体和一个裁判智能体，利用结构化推理方法并采用少量示例策略增强多样性。

**结果:** 在真实银行数据集和公共基准数据集上测试，结果显示Top-1准确率提高了14％，Top-5准确率提高了18％，Mean Reciprocal Rank提升了12％，并且在处理模糊查询方面表现优异。

**结论:** 该框架能够有效处理模糊查询，适合部署在生产应用中，并显示出跨不同领域和语言的强大泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAFA%3A+A+multi-agent+framework+for+annotation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13668&send_immediately=true&force_search=false)

**原文摘要:** Modern applications require accurate and efficient retrieval of information
in response to user queries. Mapping user utterances to the most relevant
Frequently Asked Questions (FAQs) is a crucial component of these systems.
Traditional approaches often rely on a single model or technique, which may not
capture the nuances of diverse user inquiries. In this paper, we introduce a
multi-agent framework for FAQ annotation that combines multiple specialized
agents with different approaches and a judge agent that reranks candidates to
produce optimal results. Our agents utilize a structured reasoning approach
inspired by Attentive Reasoning Queries (ARQs), which guides them through
systematic reasoning steps using targeted, task-specific JSON queries. Our
framework features a specialized few-shot example strategy, where each agent
receives different few-shots, enhancing ensemble diversity and coverage of the
query space. We evaluate our framework on a real-world banking dataset as well
as public benchmark datasets (LCQMC and FiQA), demonstrating significant
improvements over single-agent approaches across multiple metrics, including a
14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%
improvement in Mean Reciprocal Rank on our dataset, and similar gains on public
benchmarks when compared with traditional single agent annotation techniques.
Our framework is particularly effective at handling ambiguous queries, making
it well-suited for deployment in production applications while showing strong
generalization capabilities across different domains and languages.

</details>


### [156] [A*-Decoding: Token-Efficient Inference Scaling](https://arxiv.org/abs/2505.13672)
*Giannis Chatziveroglou*

**主要类别:** cs.AI

**概要:** A*-decoding is introduced as a search-based inference-time strategy that improves language model performance on complex reasoning tasks by optimally utilizing a fixed compute budget.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods for inference-time scaling show performance gains but do not optimally utilize the compute budget.

**方法:** A*-decoding uses the A* search algorithm to prioritize high-quality reasoning paths during language model generation.

**结果:** A*-decoding achieves comparable performance to baselines with fewer tokens and PRM passes, matching the performance of larger models on specific benchmarks.

**结论:** Structured search in decoding offers an alternative to brute-force sampling or scale-driven gains, enhancing reasoning in SLMs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A%2A-Decoding%3A+Token-Efficient+Inference+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13672&send_immediately=true&force_search=false)

**原文摘要:** Inference-time scaling has emerged as a powerful alternative to parameter
scaling for improving language model performance on complex reasoning tasks.
While existing methods have shown strong performance gains under fixed compute
budgets, there has been little focus on optimally utilizing that budget during
inference. In this work, we introduce A*-decoding, a search-based
inference-time strategy that builds on the A* search algorithm to optimally
utilize a fixed compute budget by prioritizing high-quality reasoning paths
during generation. We frame language model decoding as a structured search in a
state space of partial solutions, applying the A* transition model to identify
promising continuations guided by an external process supervision signal. In
our experiments, A*-decoding reaches the performance levels of strong inference
scaling baselines like best-of-N and particle filtering while using up to 3x
fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the
MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to
match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows
Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the
power of structured search in decoding, offering an alternative to brute-force
sampling or scale-driven gains. Our work demonstrates how thoughtful
inference-time strategies can enhance reasoning in SLMs, pointing toward future
advances in more efficient and scalable language model deployment.

</details>


### [157] [Building spatial world models from sparse transitional episodic memories](https://arxiv.org/abs/2505.13696)
*Zizhan He, Maxime Daigle, Pouya Bashivan*

**主要类别:** cs.AI

**概要:** This paper proposes a novel framework called Episodic Spatial World Model (ESWM) that enables a neural network to construct a spatial model of its surroundings using sparse and disjoint episodic memories.


<details>
  <summary>更多</summary>
  
**动机:** To investigate whether a neural network can learn to construct a spatial model of its surroundings from sparse and disjoint episodic memories.

**方法:** Formulating the problem in a simulated world and proposing the ESWM framework.

**结果:** ESWM is highly sample-efficient and inherently adaptive, allowing for rapid updates when the environment changes. It also enables near-optimal strategies for exploring novel environments and navigating between arbitrary points.

**结论:** The proposed ESWM framework demonstrates the potential of using sparse episodic memories to construct robust spatial models in neural networks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Building+spatial+world+models+from+sparse+transitional+episodic+memories，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13696&send_immediately=true&force_search=false)

**原文摘要:** Many animals possess a remarkable capacity to rapidly construct flexible
mental models of their environments. These world models are crucial for
ethologically relevant behaviors such as navigation, exploration, and planning.
The ability to form episodic memories and make inferences based on these sparse
experiences is believed to underpin the efficiency and adaptability of these
models in the brain. Here, we ask: Can a neural network learn to construct a
spatial model of its surroundings from sparse and disjoint episodic memories?
We formulate the problem in a simulated world and propose a novel framework,
the Episodic Spatial World Model (ESWM), as a potential answer. We show that
ESWM is highly sample-efficient, requiring minimal observations to construct a
robust representation of the environment. It is also inherently adaptive,
allowing for rapid updates when the environment changes. In addition, we
demonstrate that ESWM readily enables near-optimal strategies for exploring
novel environments and navigating between arbitrary points, all without the
need for additional training.

</details>


### [158] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
*Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, Keith Ross*

**主要类别:** cs.AI

**概要:** 提出了一种在有限监督下高效训练推理能力大型语言模型的两阶段策略。第一阶段通过玩具问题预热模型以获取一般推理技能，第二阶段利用少量目标领域样本应用带可验证奖励的强化学习优化模型。实验表明该方法提高了模型的泛化能力和样本效率。


<details>
  <summary>更多</summary>
  
**动机:** 解决在高质量训练数据稀缺的情况下设计具备推理能力的大规模语言模型的挑战。

**方法:** 提出一种两阶段训练策略：第一阶段通过知识蒸馏从玩具逻辑谜题中获取一般推理技能；第二阶段使用少量目标领域样例结合带可验证奖励的强化学习优化模型。

**结果:** 该方法显著提升了模型在多种任务上的表现，并且在相同小数据集上预热后的模型优于未预热的模型，同时保持了跨领域的泛化能力并改善了样本效率。

**结论:** 预热是构建数据稀缺环境中鲁棒推理能力大规模语言模型的有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Warm+Up+Before+You+Train%3A+Unlocking+General+Reasoning+in+Resource-Constrained+Settings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13718，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13718&send_immediately=true&force_search=false)

**原文摘要:** Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [159] [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/abs/2505.13737)
*Andrew Nam, Henry Conklin, Yukang Yang, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie*

**主要类别:** cs.AI

**概要:** 提出因果头门控方法(CHG)，用于解释变压器模型中注意力头的功能角色。通过影响任务性能来分配因果分类，并在多种大型语言模型上验证其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 解释变压器模型中注意力头的功能角色。

**方法:** 提出因果头门控方法(CHG)，学习软门控并分配因果分类。

**结果:** 在Llama 3模型家族和多种任务上评估CHG，显示其得分提供了因果洞察力，并引入对比CHG以隔离特定任务组件的子电路。

**结论:** LLMs包含多个稀疏且足够的子电路，单个头的角色依赖于与其他头的交互，并且指令跟随和上下文学习依赖于可分离的机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Head+Gating%3A+A+Framework+for+Interpreting+Roles+of+Attention+Heads+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13737&send_immediately=true&force_search=false)

**原文摘要:** We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple sparse, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.

</details>


### [160] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
*Li Ji-An, Hua-Dong Xiong, Robert C. Wilson, Marcelo G. Mattar, Marcus K. Benna*

**主要类别:** cs.AI

**概要:** 研究了大型语言模型的元认知能力，提出了一种基于神经反馈的方法来量化其显式报告和控制激活模式的能力。发现LLMs能够学习到一定程度的自我监控，但其元认知能力存在局限性，这在AI安全方面具有重要意义。


<details>
  <summary>更多</summary>
  
**动机:** 理解大型语言模型（LLMs）的元认知能力限制，特别是它们监控内部激活的能力，这对于确保AI系统的安全性至关重要。

**方法:** 引入了一种受神经科学启发的神经反馈范式，通过呈现带有标签的句子-标签对来量化LLMs报告和控制激活模式的能力。

**结果:** LLMs可以学会报告和控制特定方向上的内部激活，但性能受提供示例对的数量、目标神经方向的语义可解释性和该方向解释的方差等因素影响。

**结论:** LLMs的元认知能力有限，只能监控其神经机制的一部分，这一发现为AI安全提供了重要的实证证据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+Models+Are+Capable+of+Metacognitive+Monitoring+and+Control+of+Their+Internal+Activations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13763&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [161] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
*Jin Du, Li Chen, Xun Xian, An Luo, Fangqiao Tian, Ganghua Wang, Charles Doss, Xiaotong Shen, Jie Ding*

**主要类别:** cs.AI

**概要:** 评估大型语言模型在统计因果推理中的能力


<details>
  <summary>更多</summary>
  
**动机:** 当前基准测试对于因果推理存在的简化任务不能全面评估大型语言模型的能力

**方法:** 提出CausalPitfalls基准测试，包括多难度层次的结构化挑战和评分标准

**结果:** 当前大型语言模型在统计因果推理方面存在显著局限性

**结论:** CausalPitfalls基准测试可为可靠因果推理系统的开发提供指导和定量指标

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ice+Cream+Doesn%27t+Cause+Drowning%3A+Benchmarking+LLMs+Against+Statistical+Pitfalls+in+Causal+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13770，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13770&send_immediately=true&force_search=false)

**原文摘要:** Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [162] [Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments](https://arxiv.org/abs/2505.13773)
*Ryan Bowers, Richard Agbeyibor, Jack Kolb, Karen Feigh*

**主要类别:** cs.AI

**概要:** 比较三种让人类熟悉人工智能队友的方法。


<details>
  <summary>更多</summary>
  
**动机:** 在协作环境中提高人与人工智能的合作效率。

**方法:** 通过用户研究比较文档阅读、与AI一起训练和无熟悉过程三种方法。

**结果:** 直接交互组能更快形成团队策略但风险规避较强；文档组虽也有快速适应但更倾向于保守行为；结合多种方式可能最佳。

**结论:** 推荐结合文档、现场培训及探索性互动的人机熟悉方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+Cards+for+AI+Teammates%3A+Comparing+Human-AI+Team+Familiarization+Methods+for+High-Stakes+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13773&send_immediately=true&force_search=false)

**原文摘要:** We compare three methods of familiarizing a human with an artificial
intelligence (AI) teammate ("agent") prior to operation in a collaborative,
fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In
a between-subjects user study (n=60), participants either read documentation
about the agent, trained alongside the agent prior to the mission, or were
given no familiarization. Results showed that the most valuable information
about the agent included details of its decision-making algorithms and its
relative strengths and weaknesses compared to the human. This information
allowed the familiarization groups to form sophisticated team strategies more
quickly than the control group. Documentation-based familiarization led to the
fastest adoption of these strategies, but also biased participants towards
risk-averse behavior that prevented high scores. Participants familiarized
through direct interaction were able to infer much of the same information
through observation, and were more willing to take risks and experiment with
different control modes, but reported weaker understanding of the agent's
internal processes. Significant differences were seen between individual
participants' risk tolerance and methods of AI interaction, which should be
considered when designing human-AI control interfaces. Based on our findings,
we recommend a human-AI team familiarization method that combines AI
documentation, structured in-situ training, and exploratory interaction.

</details>


### [163] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/abs/2505.13774)
*Zidi Xiong, Chen Shan, Zhenting Qi, Himabindu Lakkaraju*

**主要类别:** cs.AI

**概要:** 提出了一种系统性的反事实干预框架来严格评估LRMs思维草稿的忠实度。


<details>
  <summary>更多</summary>
  
**动机:** 确保LRMs中间推理过程的忠实性对于可靠监控、解释和有效控制至关重要。

**方法:** 提出了两个互补的维度来评估思维草稿的忠实性：（1）Intra-Draft Faithfulness 和 （2）Draft-to-Answer Faithfulness。

**结果:** 当前的LRMs在中间推理步骤上表现出选择性的忠实性，并且经常无法忠实地与草案结论对齐。

**结论:** 需要在高级LRMs中实现更忠实和可解释的推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+the+Faithfulness+of+Thinking+Drafts+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13774&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [164] [CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs](https://arxiv.org/abs/2505.13778)
*Guoheng Sun, Ziyao Wang, Bowei Tian, Meng Liu, Zheyu Shen, Shwai He, Yexiao He, Wanghao Ye, Yiting Wang, Ang Li*

**主要类别:** cs.AI

**概要:** 提出CoIn框架解决大型语言模型中隐藏推理步骤不透明导致的计费不透明问题。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在复杂任务上表现优异，但隐藏的推理步骤导致用户无法验证答案的真实性且可能产生不必要的费用。

**方法:** 通过构建可验证哈希树检查令牌数量，并利用基于嵌入的相关性匹配检测伪造的推理内容来实现对令牌数量和语义有效性的审核。

**结果:** 实验表明，当作为可信第三方审计员部署时，CoIn可以有效检测令牌计数膨胀，成功率达到94.7%。

**结论:** CoIn展示了恢复不透明大型语言模型服务中计费透明的强大能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoIn%3A+Counting+the+Invisible+Reasoning+Tokens+in+Commercial+Opaque+LLM+APIs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13778，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13778&send_immediately=true&force_search=false)

**原文摘要:** As post-training techniques evolve, large language models (LLMs) are
increasingly augmented with structured multi-step reasoning abilities, often
optimized through reinforcement learning. These reasoning-enhanced models
outperform standard LLMs on complex tasks and now underpin many commercial LLM
APIs. However, to protect proprietary behavior and reduce verbosity, providers
typically conceal the reasoning traces while returning only the final answer.
This opacity introduces a critical transparency gap: users are billed for
invisible reasoning tokens, which often account for the majority of the cost,
yet have no means to verify their authenticity. This opens the door to token
count inflation, where providers may overreport token usage or inject
synthetic, low-effort tokens to inflate charges. To address this issue, we
propose CoIn, a verification framework that audits both the quantity and
semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from
token embedding fingerprints to check token counts, and uses embedding-based
relevance matching to detect fabricated reasoning content. Experiments
demonstrate that CoIn, when deployed as a trusted third-party auditor, can
effectively detect token count inflation with a success rate reaching up to
94.7%, showing the strong ability to restore billing transparency in opaque LLM
services. The dataset and code are available at
https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

</details>


### [165] [LLM-based Evaluation Policy Extraction for Ecological Modeling](https://arxiv.org/abs/2505.13794)
*Qi Cheng, Licheng Liu, Qing Zhu, Runlong Yu, Zhenong Jin, Yiqun Xie, Xiaowei Jia*

**主要类别:** cs.AI

**概要:** 提出了一种新的框架，结合度量学习和基于大型语言模型的自然语言策略提取，用于评估生态时间序列，解决了传统数值指标无法捕捉领域特定时间模式的问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统数值指标在评估生态模型性能时无法有效捕捉关键的时间模式，且专家视觉检查耗时且难以大规模应用。

**方法:** 提出一个新框架，集成度量学习与大型语言模型（LLM）的自然语言策略提取，通过处理成对标注并优化生成组合不同的评估指标。

**结果:** 在多个数据集上验证了方法的有效性，包括作物总初级生产力和二氧化碳通量预测评估。

**结论:** 该框架弥合了数值指标与专家知识之间的差距，并提供了可解释的评估政策，满足不同生态系统建模研究的需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-based+Evaluation+Policy+Extraction+for+Ecological+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13794&send_immediately=true&force_search=false)

**原文摘要:** Evaluating ecological time series is critical for benchmarking model
performance in many important applications, including predicting greenhouse gas
fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.
Traditional numerical metrics (e.g., R-squared, root mean square error) have
been widely used to quantify the similarity between modeled and observed
ecosystem variables, but they often fail to capture domain-specific temporal
patterns critical to ecological processes. As a result, these methods are often
accompanied by expert visual inspection, which requires substantial human labor
and limits the applicability to large-scale evaluation. To address these
challenges, we propose a novel framework that integrates metric learning with
large language model (LLM)-based natural language policy extraction to develop
interpretable evaluation criteria. The proposed method processes pairwise
annotations and implements a policy optimization mechanism to generate and
combine different assessment metrics. The results obtained on multiple datasets
for evaluating the predictions of crop gross primary production and carbon
dioxide flux have confirmed the effectiveness of the proposed method in
capturing target assessment preferences, including both synthetically generated
and expert-annotated model comparisons. The proposed framework bridges the gap
between numerical metrics and expert knowledge while providing interpretable
evaluation policies that accommodate the diverse needs of different ecosystem
modeling studies.

</details>


### [166] [Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models](https://arxiv.org/abs/2505.13828)
*Kiarash Naghavi Khanghah, Zhiling Chen, Lela Romeo, Qian Yang, Rajiv Malhotra, Farhad Imani, Hongyi Xu*

**主要类别:** cs.AI

**概要:** This paper introduces a new framework for detecting anomalies in additive manufacturing processes using retrieved literature information for zero-shot anomaly identification, classification, and explanation generation.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenges related to defects and process anomalies in additive manufacturing.

**方法:** A multimodal Retrieval-Augmented Generation-based framework that performs zero-shot anomaly identification, classification, and explanation generation in a Laser Powder Bed Fusion setting.

**结果:** The framework was evaluated on four L-PBF manufacturing datasets from Oak Ridge National Laboratory. It demonstrated adaptability and generalizability across diverse images. Comparative analysis showed that GPT-4o-mini outperforms other models in manufacturing anomalies classification.

**结论:** This study proposes a novel framework for AM anomaly detection which uses retrieved information instead of training datasets. It shows adaptability and generalizability across diverse images.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+RAG-driven+Anomaly+Detection+and+Classification+in+Laser+Powder+Bed+Fusion+using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13828&send_immediately=true&force_search=false)

**原文摘要:** Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.

</details>


### [167] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/abs/2505.13831)
*Zongyuan Deng, Yujie Cai, Qing Liu, Shiyao Mu, Bin Lyu, Zhen Yang*

**主要类别:** cs.AI

**概要:** 提出了一种名为TelePlanNet的人工智能驱动框架，用于解决5G网络规划中的基站选址问题，通过三层架构实现高效规划和大规模自动化，并结合大语言模型与改进的相对策略优化算法，实验表明其一致性提高到78%，优于传统人工方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有AI工具难以满足电信运营商网络动态条件下的多目标需求，且传统手动方法效率低、一致性差。

**方法:** 提出TelePlanNet框架，集成三层架构，利用大语言模型处理用户输入并进行意图对齐，使用改进的相对策略优化算法训练规划模型。

**结果:** 实验结果显示TelePlanNet的一致性提升至78%，优于手动方法。

**结论:** TelePlanNet为电信运营商提供了一个高效的、可扩展的工具，显著推进了蜂窝网络规划。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TelePlanNet%3A+An+AI-Driven+Framework+for+Efficient+Telecom+Network+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13831，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13831&send_immediately=true&force_search=false)

**原文摘要:** The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [168] [A Challenge to Build Neuro-Symbolic Video Agents](https://arxiv.org/abs/2505.13851)
*Sahil Shah, Harsh Goel, Sai Shankar Narasimhan, Minkyu Choi, S P Sharan, Oguzhan Akcin, Sandeep Chinchali*

**主要类别:** cs.AI

**概要:** This paper discusses the limitations of current video understanding systems and proposes a neuro-symbolic approach to improve their capabilities in temporal reasoning, event understanding, and action-driven decision-making.


<details>
  <summary>更多</summary>
  
**动机:** There is a growing need for proactive video agents that can not only interpret video streams but also reason about events and take informed actions.

**方法:** The authors propose a neuro-symbolic perspective that decomposes video queries into atomic events, structures them into coherent sequences, and validates them against temporal constraints.

**结果:** The proposed approach can enhance interpretability, enable structured reasoning, and provide stronger guarantees on system behavior.

**结论:** The paper presents a grand challenge to the research community to develop the next generation of intelligent video agents with three core capabilities: autonomous video search and analysis, seamless real-world interaction, and advanced content generation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Challenge+to+Build+Neuro-Symbolic+Video+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13851，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13851&send_immediately=true&force_search=false)

**原文摘要:** Modern video understanding systems excel at tasks such as scene
classification, object detection, and short video retrieval. However, as video
analysis becomes increasingly central to real-world applications, there is a
growing need for proactive video agents for the systems that not only interpret
video streams but also reason about events and take informed actions. A key
obstacle in this direction is temporal reasoning: while deep learning models
have made remarkable progress in recognizing patterns within individual frames
or short clips, they struggle to understand the sequencing and dependencies of
events over time, which is critical for action-driven decision-making.
Addressing this limitation demands moving beyond conventional deep learning
approaches. We posit that tackling this challenge requires a neuro-symbolic
perspective, where video queries are decomposed into atomic events, structured
into coherent sequences, and validated against temporal constraints. Such an
approach can enhance interpretability, enable structured reasoning, and provide
stronger guarantees on system behavior, all key properties for advancing
trustworthy video agents. To this end, we present a grand challenge to the
research community: developing the next generation of intelligent video agents
that integrate three core capabilities: (1) autonomous video search and
analysis, (2) seamless real-world interaction, and (3) advanced content
generation. By addressing these pillars, we can transition from passive
perception to intelligent video agents that reason, predict, and act, pushing
the boundaries of video understanding.

</details>


### [169] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
*Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang*

**主要类别:** cs.AI

**概要:** An innovative framework named Mobile-Agent-V is introduced which uses video content to inject operational knowledge into mobile automation processes, improving performance by 36%.


<details>
  <summary>更多</summary>
  
**动机:** The need for streamlined automation for effective task management in mobile devices due to their rising usage, with current AI frameworks lacking operational expertise.

**方法:** Utilizing video as a guiding tool to inject operational knowledge into mobile automation processes without manual intervention.

**结果:** Performance enhancement by 36% compared to existing methods.

**结论:** Mobile-Agent-V proves to be a more effortless and efficient solution for mobile automation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mobile-Agent-V%3A+A+Video-Guided+Approach+for+Effortless+and+Efficient+Operational+Knowledge+Injection+in+Mobile+Automation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13887&send_immediately=true&force_search=false)

**原文摘要:** The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [170] [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
*Yanheng He, Jiahe Jin, Pengfei Liu*

**主要类别:** cs.AI

**概要:** 提出PC Agent-E框架，通过少量高质量轨迹数据实现强计算机使用能力，并在不同操作系统上展示良好泛化性。


<details>
  <summary>更多</summary>
  
**动机:** 解决高质量轨迹数据缺乏的问题以开发类人计算机使用代理。

**方法:** 引入PC Agent-E框架，利用合成多样化动作决策提升数据质量并训练模型。

**结果:** PC Agent-E模型在WindowsAgentArena-V2上相对提升了141%，并在OSWorld上展示了跨操作系统的泛化能力。

**结论:** 少量高质量轨迹数据即可激发强大的计算机使用能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Agent+Training+for+Computer+Use，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13909&send_immediately=true&force_search=false)

**原文摘要:** Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [171] [Parallel Belief Revision via Order Aggregation](https://arxiv.org/abs/2505.13914)
*Jake Chandler, Richard Booth*

**主要类别:** cs.AI

**概要:** This paper introduces a method based on TeamQueue aggregators to extend serial iterated belief revision operators for handling parallel change, aiming to recover plausible properties while avoiding dubious ones.


<details>
  <summary>更多</summary>
  
**动机:** Extending the single-step parallel revision model to the iterated case lacks an underlying unifying explanation.

**方法:** Using TeamQueue aggregators to develop a general method for extending serial iterated belief revision operators.

**结果:** A method that can recover independently plausible properties without yielding dubious ones.

**结论:** The proposed method provides a principled approach to handle parallel change in iterated belief revision.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parallel+Belief+Revision+via+Order+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13914，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13914&send_immediately=true&force_search=false)

**原文摘要:** Despite efforts to better understand the constraints that operate on
single-step parallel (aka "package", "multiple") revision, very little work has
been carried out on how to extend the model to the iterated case. A recent
paper by Delgrande & Jin outlines a range of relevant rationality postulates.
While many of these are plausible, they lack an underlying unifying
explanation. We draw on recent work on iterated parallel contraction to offer a
general method for extending serial iterated belief revision operators to
handle parallel change. This method, based on a family of order aggregators
known as TeamQueue aggregators, provides a principled way to recover the
independently plausible properties that can be found in the literature, without
yielding the more dubious ones.

</details>


### [172] [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://arxiv.org/abs/2505.13940)
*Kun Li, Zhennan Wu, Shoupeng Wang, Wenbin Hu*

**主要类别:** cs.AI

**概要:** DrugPilot是一个基于LLM的药物发现代理，通过参数化推理架构解决现有LLM在药物发现领域的挑战，并展示出卓越的工具调用能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型在药物发现领域面临如多模态异构数据处理、领域知识动态更新延迟以及复杂计算任务结果预测信心不足等挑战。

**方法:** 提出并开发了DrugPilot，一个基于LLM的代理系统，它包括参数化推理架构和交互式参数化记忆池来处理多模态药物数据，并创建了一个包含8个关键药物发现任务的数据集用于模型微调和评估。

**结果:** DrugPilot在药物发现工具指令数据集上的工具调用能力超越了现有代理（如ReAct，LoT），并且在简单、多步骤和多轮任务中的表现分别为98.0%，93.5%和64.0%。

**结论:** DrugPilot展示了在药物发现领域中基于LLM代理的强大潜力，其参数推理架构解决了传统端到端LLM预测方法的关键限制。此外，通过药物指令数据集的微调和评估，DrugPilot在简单、多步骤和多轮任务上分别达到了98.0%，93.5%和64.0%的任务完成率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DrugPilot%3A+LLM-based+Parameterized+Reasoning+Agent+for+Drug+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13940，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13940&send_immediately=true&force_search=false)

**原文摘要:** In the field of AI4Science, large-scale language models (LLMs) show great
potential to parse complex scientific semantics, integrate cross-disciplinary
knowledge, and assist critical task research. However, in the field of drug
discovery, despite the optimization through professional data pre-training,
context window expansion, and internet search, the existing LLMs are still
facing challenges such as massive multi-modal and heterogeneous data
processing, domain knowledge dynamic updating delay, and insufficient
confidence in predicting the results of complex computational tasks. To address
these challenges, we propose the DrugPilot, an LLM-based agent with
parameterized reasoning for drug discovery. DrugPilot addresses key limitations
of traditional end-to-end LLM prediction approaches through its parametric
inference architecture. This agent system supports major phases of the drug
discovery pipeline, facilitating automated planning and execution of
multi-stage research tasks. To address the critical challenge of multi-modal
drug data analysis (incorporating both public datasets and user-submitted
data), we developed an interactive parameterized memory pool. This innovative
component standardizes real-world drug data into parametric representations,
simultaneously enabling efficient knowledge retrieval in multi-turn dialogue
while mitigating the information loss inherent in text-based data transmission.
Additionally, we created a drug instruct dataset across 8 essential drug
discovery tasks for model fine-tuning and evaluation. Based on the Berkeley
function calling evaluation framework, DrugPilot demonstrated the most advanced
tool calling capabilities on our drug discovery tool instruction dataset,
outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves
task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and
multi-turn tasks, respectively.

</details>


### [173] [Visual Instruction Bottleneck Tuning](https://arxiv.org/abs/2505.13946)
*Changdae Oh, Jiatong Li, Shawn Im, Yixuan Li*

**主要类别:** cs.AI

**概要:** This paper proposes Vittle, a method inspired by the information bottleneck principle, to improve the robustness of multimodal large language models under distribution shifts without requiring additional instruction data or larger model architectures.


<details>
  <summary>更多</summary>
  
**动机:** To address the performance degradation of multimodal large language models when encountering unfamiliar queries under distribution shifts.

**方法:** Deriving a variational lower bound of the information bottleneck for MLLMs and devising Visual Instruction Bottleneck Tuning (Vittle).

**结果:** Empirical validation shows that Vittle improves the robustness of MLLMs under shifts by learning a minimal sufficient representation.

**结论:** Vittle provides a practical way to enhance the robustness of MLLMs under distribution shifts from a representation learning perspective.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Visual+Instruction+Bottleneck+Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13946&send_immediately=true&force_search=false)

**原文摘要:** Despite widespread adoption, multimodal large language models (MLLMs) suffer
performance degradation when encountering unfamiliar queries under distribution
shifts. Existing methods to improve MLLM generalization typically require
either more instruction data or larger advanced model architectures, both of
which incur non-trivial human labor or computational costs. In this work, we
take an alternative approach to enhance the robustness of MLLMs under
distribution shifts, from a representation learning perspective. Inspired by
the information bottleneck (IB) principle, we derive a variational lower bound
of the IB for MLLMs and devise a practical implementation, Visual Instruction
Bottleneck Tuning (Vittle). We then provide a theoretical justification of
Vittle by revealing its connection to an information-theoretic robustness
metric of MLLM. Empirical validation of three MLLMs on open-ended and
closed-form question answering and object hallucination detection tasks over 45
datasets, including 30 shift scenarios, demonstrates that Vittle consistently
improves the MLLM's robustness under shifts by pursuing the learning of a
minimal sufficient representation.

</details>


### [174] [Solving Normalized Cut Problem with Constrained Action Space](https://arxiv.org/abs/2505.13986)
*Qize Jiang, Linsey Pang, Alice Gatti, Mahima Aggarwa, Giovanna Vantin, Xiaosong Ma, Weiwei Sun, Sanjay Chawla*

**主要类别:** cs.AI

**概要:** This paper presents an RL-based method using constrained action spaces to guide the normalized cut problem towards predefined templates for graph partitioning.


<details>
  <summary>更多</summary>
  
**动机:** To integrate external knowledge into RL solutions for combinatorial optimization problems.

**方法:** Proposes an RL solution with constrained action spaces to guide the normalized cut problem, creating a Wedge and Ring Transformer for graph partitioning.

**结果:** Graph partitions shaped like Wedges and Rings that are close to natural optimal partitions.

**结论:** The approach is generalizable across different domains.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Solving+Normalized+Cut+Problem+with+Constrained+Action+Space，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13986&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has emerged as an important paradigm to solve
combinatorial optimization problems primarily due to its ability to learn
heuristics that can generalize across problem instances. However, integrating
external knowledge that will steer combinatorial optimization problem solutions
towards domain appropriate outcomes remains an extremely challenging task. In
this paper, we propose the first RL solution that uses constrained action
spaces to guide the normalized cut problem towards pre-defined template
instances. Using transportation networks as an example domain, we create a
Wedge and Ring Transformer that results in graph partitions that are shaped in
form of Wedges and Rings and which are likely to be closer to natural optimal
partitions. However, our approach is general as it is based on principles that
can be generalized to other domains.

</details>


### [175] [Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning](https://arxiv.org/abs/2505.13994)
*Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim*

**主要类别:** cs.AI

**概要:** 本文提出了一种新的多代理RAG框架SPLIT-RAG，它通过问题驱动的语义图划分和协作子图检索，提高了效率并减少了搜索空间。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RAG系统在扩展到大型知识图谱时，在简单查询上产生不必要的延迟，在复杂多跳问题上产生碎片化推理。

**方法:** 提出了一种名为SPLIT-RAG的多代理RAG框架，该框架首先创建链接信息的语义划分，然后使用类型专用知识库实现多代理RAG。

**结果:** 实验验证表明，与现有方法相比有显著改进。

**结论:** SPLIT-RAG通过引入问题驱动的语义图划分和协作子图检索解决了现有RAG系统的效率-准确性权衡问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Divide+by+Question%2C+Conquer+by+Agent%3A+SPLIT-RAG+with+Question-Driven+Graph+Partitioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13994，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13994&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.

</details>


### [176] [VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change](https://arxiv.org/abs/2505.14001)
*Sterre Lutz, Matthijs T. J. Spaan, Anna Lukina*

**主要类别:** cs.AI

**概要:** This paper introduces VeRecycle, a framework that allows efficient reuse of probabilistic certificates when the system dynamics change in a localized subset of states, reducing computational costs while maintaining competitive probabilistic guarantees.


<details>
  <summary>更多</summary>
  
**动机:** To address the need for efficient re-certification of probabilistic neural Lyapunov certificates when facing changes in system dynamics beyond modeled uncertainties.

**方法:** Developing VeRecycle, a framework that formally reclaims guarantees for discrete-time stochastic dynamical systems by efficiently reusing probabilistic certificates.

**结果:** VeRecycle reduces computational effort significantly and achieves competitive probabilistic guarantees in compositional neural control.

**结论:** VeRecycle provides an efficient way to reuse probabilistic certificates in dynamic environments, particularly beneficial for neural certificates.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VeRecycle%3A+Reclaiming+Guarantees+from+Probabilistic+Certificates+for+Stochastic+Dynamical+Systems+after+Change，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14001，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14001&send_immediately=true&force_search=false)

**原文摘要:** Autonomous systems operating in the real world encounter a range of
uncertainties. Probabilistic neural Lyapunov certification is a powerful
approach to proving safety of nonlinear stochastic dynamical systems. When
faced with changes beyond the modeled uncertainties, e.g., unidentified
obstacles, probabilistic certificates must be transferred to the new system
dynamics. However, even when the changes are localized in a known part of the
state space, state-of-the-art requires complete re-certification, which is
particularly costly for neural certificates. We introduce VeRecycle, the first
framework to formally reclaim guarantees for discrete-time stochastic dynamical
systems. VeRecycle efficiently reuses probabilistic certificates when the
system dynamics deviate only in a given subset of states. We present a general
theoretical justification and algorithmic implementation. Our experimental
evaluation shows scenarios where VeRecycle both saves significant computational
effort and achieves competitive probabilistic guarantees in compositional
neural control.

</details>


### [177] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2505.14020)
*Hao Dong, Ziyue Qiao, Zhiyuan Ning, Qi Hao, Yi Du, Pengyang Wang, Yuanchun Zhou*

**主要类别:** cs.AI

**概要:** 提出了一种名为DiMNet的新方法用于时序知识图谱推理，该方法通过多跨度演化策略和解耦组件显著提高了推理性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在建模子图语义演化时忽略了子图间的内部结构交互，并且未区分潜在平滑特征与语义变化特征。

**方法:** 设计了多跨度演化策略捕获局部邻居特征并感知历史邻居语义信息，同时引入了解耦组件动态控制历史语义对未来演化的影响力。

**结果:** 在四个真实世界数据集上的实验表明，DiMNet在TKG推理方面表现出色，比最先进的方法高出22.7%的MRR。

**结论:** 所提出的DiMNet在解决现有方法的局限性方面取得了显著进展，提升了时序知识图谱的推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Disentangled+Multi-span+Evolutionary+Network+against+Temporal+Knowledge+Graph+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14020，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14020&send_immediately=true&force_search=false)

**原文摘要:** Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [178] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
*Xinzhe Zheng, Sijie Ji, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava*

**主要类别:** cs.AI

**概要:** 提出ProMind-LLM，一种结合主观心理记录和客观行为数据的大语言模型，用于更可靠的心理健康风险评估。


<details>
  <summary>更多</summary>
  
**动机:** 当前心理健康风险评估方法依赖于主观文本记录，存在不确定性导致预测不一致和不可靠的问题。

**方法:** 开发ProMind-LLM，包括领域特定预训练、自优化机制和因果推理链，以整合客观行为数据。

**结果:** 在PMData和Globem两个真实数据集上的评估显示，ProMind-LLM比通用大语言模型有显著改进。

**结论:** ProMind-LLM有望推动更可靠、可解释和可扩展的心理健康解决方案的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProMind-LLM%3A+Proactive+Mental+Health+Care+via+Causal+Reasoning+with+Sensor+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14038&send_immediately=true&force_search=false)

**原文摘要:** Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [179] [Personalized Student Knowledge Modeling for Future Learning Resource Prediction](https://arxiv.org/abs/2505.14072)
*Soroush Hashemifar, Sherry Sahebi*

**主要类别:** cs.AI

**概要:** This paper introduces KMaP, a new method that improves personalized learning by jointly modeling student knowledge and behavior.


<details>
  <summary>更多</summary>
  
**动机:** Current methods have challenges with personalization and modeling diverse learning activities.

**方法:** KMaP uses clustering-based student profiling to create personalized representations and predict future learning resource preferences.

**结果:** Experiments on two datasets show significant behavioral differences across student clusters and validate the effectiveness of KMaP.

**结论:** KMaP provides a way to personalize learning by simultaneously modeling knowledge and behavior.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Student+Knowledge+Modeling+for+Future+Learning+Resource+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14072，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14072&send_immediately=true&force_search=false)

**原文摘要:** Despite advances in deep learning for education, student knowledge tracing
and behavior modeling face persistent challenges: limited personalization,
inadequate modeling of diverse learning activities (especially non-assessed
materials), and overlooking the interplay between knowledge acquisition and
behavioral patterns. Practical limitations, such as fixed-size sequence
segmentation, frequently lead to the loss of contextual information vital for
personalized learning. Moreover, reliance on student performance on assessed
materials limits the modeling scope, excluding non-assessed interactions like
lectures. To overcome these shortcomings, we propose Knowledge Modeling and
Material Prediction (KMaP), a stateful multi-task approach designed for
personalized and simultaneous modeling of student knowledge and behavior. KMaP
employs clustering-based student profiling to create personalized student
representations, improving predictions of future learning resource preferences.
Extensive experiments on two real-world datasets confirm significant behavioral
differences across student clusters and validate the efficacy of the KMaP
model.

</details>


### [180] [Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games](https://arxiv.org/abs/2505.14137)
*Vojtěch Kůr, Vít Musil, Vojtěch Řehák*

**主要类别:** cs.AI

**概要:** 本文提出了一种新方法解决有限内存策略中的内存分配问题，并通过实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的算法虽然改进了状态之间的转换概率，但需要手动分配每个位置的可用内存大小，这使得有限内存策略的实用性受到限制。

**方法:** 开发了一种迭代改变内存分配的一般方法，解决了有限内存策略中手动分配内存大小的问题。

**结果:** 提出的方法可以与任何黑盒策略优化工具一起使用，并且通过各种实验验证了其在解决不同巡逻模型实例时的鲁棒性。

**结论:** 提出了一种新的方法来解决有限内存策略中的内存分配问题，并且该算法可以与任何黑盒策略优化工具一起使用。通过各种实验评估了我们的方法，并展示了其在解决各种巡逻模型实例时的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory+Assignment+for+Finite-Memory+Strategies+in+Adversarial+Patrolling+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14137&send_immediately=true&force_search=false)

**原文摘要:** Adversarial Patrolling games form a subclass of Security games where a
Defender moves between locations, guarding vulnerable targets. The main
algorithmic problem is constructing a strategy for the Defender that minimizes
the worst damage an Attacker can cause. We focus on the class of finite-memory
(also known as regular) Defender's strategies that experimentally outperformed
other competing classes. A finite-memory strategy can be seen as a positional
strategy on a finite set of states. Each state consists of a pair of a location
and a certain integer value--called memory. Existing algorithms improve the
transitional probabilities between the states but require that the available
memory size itself is assigned at each location manually. Choosing the right
memory assignment is a well-known open and hard problem that hinders the
usability of finite-memory strategies. We solve this issue by developing a
general method that iteratively changes the memory assignment. Our algorithm
can be used in connection with \emph{any} black-box strategy optimization tool.
We evaluate our method on various experiments and show its robustness by
solving instances of various patrolling models.

</details>


### [181] [RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning](https://arxiv.org/abs/2505.14140)
*Qianyue Hao, Sibo Li, Jian Yuan, Yong Li*

**主要类别:** cs.AI

**概要:** 提出了一种名为RL-of-Thoughts(RLoT)的方法，通过强化学习训练一个轻量级导航模型来增强大型语言模型在推理时的性能。实验表明RLoT比现有的推理时间技术提高了多达13.4%。


<details>
  <summary>更多</summary>
  
**动机:** 现有的推理时间技术虽然能提高大型语言模型的表现，但缺乏适应性，因为它们是手动预定义的、与任务无关的框架，无法根据不同任务的特点进行调整。

**方法:** 设计了五个基本逻辑块，并通过强化学习训练了一个轻量级导航模型，在推理过程中动态选择合适的逻辑块并组合成特定任务的逻辑结构。

**结果:** RLoT在多个推理基准测试中表现出色，使较小规模的LLM表现接近大规模的LLM，并且具有很强的泛化能力。

**结论:** 提出的RL-of-Thoughts方法通过引入轻量级导航模型增强了大型语言模型的推理能力，显示出了显著的效果和应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RL+of+Thoughts%3A+Navigating+LLM+Reasoning+with+Inference-time+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14140，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14140&send_immediately=true&force_search=false)

**原文摘要:** Despite rapid advancements in large language models (LLMs), the token-level
autoregressive nature constrains their complex reasoning capabilities. To
enhance LLM reasoning, inference-time techniques, including
Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they
are fairly cost-effective by guiding reasoning through sophisticated logical
structures without modifying LLMs' parameters. However, these manually
predefined, task-agnostic frameworks are applied uniformly across diverse
tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),
where we train a lightweight navigator model with reinforcement learning (RL)
to adaptively enhance LLM reasoning at inference time. Specifically, we design
five basic logic blocks from the perspective of human cognition. During the
reasoning process, the trained RL navigator dynamically selects the suitable
logic blocks and combines them into task-specific logical structures according
to problem characteristics. Experiments across multiple reasoning benchmarks
(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)
illustrate that RLoT outperforms established inference-time techniques by up to
13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to
make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL
navigator demonstrates strong transferability: a model trained on one specific
LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is
open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for
reproducibility.

</details>


### [182] [Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent](https://arxiv.org/abs/2505.14141)
*Fanglin Mo, Junzhe Chen, Haoxuan Zhu, Xuming Hu*

**主要类别:** cs.AI

**概要:** 提出了一种名为SPlanner的插件式规划模块，用于生成执行计划，指导视觉语言模型(VLMs)完成任务。该模块利用扩展有限状态机(EFSMs)建模移动应用的控制logits和配置，并将用户指令分解为一系列初级功能，通过遍历EFSMs生成执行路径，再用LLM精炼成自然语言计划。最终计划简洁且可操作，有效引导VLMs生成交互式GUI动作来完成用户任务。在AndroidWorld基准测试中，当与Qwen2.5-VL-72B作为VLM执行器结合时，SPlanner达到了63.8%的任务成功率，比没有规划辅助的情况下提高了28.8个百分点。


<details>
  <summary>更多</summary>
  
**动机:** 现有的移动GUI代理在任务规划方面面临挑战，因为它们缺乏对如何有效使用目标应用程序的深入理解，导致在任务执行过程中容易“迷失”。

**方法:** 提出了SPlanner，一个插件式规划模块，它利用扩展有限状态机(EFSMs)来建模移动应用的控制logits和配置，并生成执行路径。然后用LLM将路径精炼成自然语言计划。

**结果:** 在AndroidWorld基准测试中，SPlanner显著提升了任务成功率，与Qwen2.5-VL-72B结合后任务成功率达到63.8%，比无规划辅助提高了28.8个百分点。

**结论:** SPlanner展示了强大的性能，在动态基准测试中反映了真实世界中的移动使用情况，证明了其在提升移动GUI代理任务规划能力方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Building+a+Stable+Planner%3A+An+Extended+Finite+State+Machine+Based+Planning+Module+for+Mobile+GUI+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14141&send_immediately=true&force_search=false)

**原文摘要:** Mobile GUI agents execute user commands by directly interacting with the
graphical user interface (GUI) of mobile devices, demonstrating significant
potential to enhance user convenience. However, these agents face considerable
challenges in task planning, as they must continuously analyze the GUI and
generate operation instructions step by step. This process often leads to
difficulties in making accurate task plans, as GUI agents lack a deep
understanding of how to effectively use the target applications, which can
cause them to become "lost" during task execution. To address the task planning
issue, we propose SPlanner, a plug-and-play planning module to generate
execution plans that guide vision language model(VLMs) in executing tasks. The
proposed planning module utilizes extended finite state machines (EFSMs) to
model the control logits and configurations of mobile applications. It then
decomposes a user instruction into a sequence of primary function modeled in
EFSMs, and generate the execution path by traversing the EFSMs. We further
refine the execution path into a natural language plan using an LLM. The final
plan is concise and actionable, and effectively guides VLMs to generate
interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong
performance on dynamic benchmarks reflecting real-world mobile usage. On the
AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired
with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point
improvement compared to using Qwen2.5-VL-72B without planning assistance.

</details>


### [183] [Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition](https://arxiv.org/abs/2505.14143)
*Shuo Zhang, Jinsong Zhang, Zhejun Zhang, Lei Li*

**主要类别:** cs.AI

**概要:** 提出了一种新的多任务学习方法MMoLRE用于多模态情感分析和多模态情感识别，解决了硬参数共享带来的参数冲突问题，并通过低秩专家网络减少了参数和计算开销。实验表明MMoLRE在多模态情感分析任务上达到最先进的性能，在多模态情感识别任务上获得有竞争力的结果。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法主要采用硬参数共享，忽视了复杂任务相关性导致的参数冲突问题。

**方法:** 提出了Multimodal Mixture of Low-Rank Experts (MMoLRE)，利用共享和任务特定专家来建模共同和独特的任务特征，设计了低秩专家网络以减少参数和计算开销。

**结果:** 在CMU-MOSI和CMU-MOSEI基准数据集上的广泛实验表明，MMoLRE在多模态情感分析任务上达到了最先进的性能，在多模态情感识别任务上获得了有竞争力的结果。

**结论:** 本文提出的方法有效地解决了多任务学习中的参数冲突问题，并在多模态情感分析和多模态情感识别任务上取得了显著成果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Mixture+of+Low-Rank+Experts+for+Sentiment+Analysis+and+Emotion+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14143&send_immediately=true&force_search=false)

**原文摘要:** Multi-task learning (MTL) enables the efficient transfer of extra knowledge
acquired from other tasks. The high correlation between multimodal sentiment
analysis (MSA) and multimodal emotion recognition (MER) supports their joint
training. However, existing methods primarily employ hard parameter sharing,
ignoring parameter conflicts caused by complex task correlations. In this
paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture
of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts
to distinctly model common and unique task characteristics, thereby avoiding
parameter conflicts. Additionally, inspired by low-rank structures in the
Mixture of Experts (MoE) framework, we design low-rank expert networks to
reduce parameter and computational overhead as the number of experts increases.
Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that
MMoLRE achieves state-of-the-art performance on the MSA task and competitive
results on the MER task.

</details>


### [184] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
*Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, Jiawei Han*

**主要类别:** cs.AI

**概要:** 提出了一种名为s3的轻量级、与模型无关的框架，用于检索增强生成系统中的检索器和生成器解耦，并通过“RAG之外的收益”奖励训练检索器，仅需2.4k训练样本即可在六个通用QA和五个医学QA基准上表现优于需要更多数据的基线。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法要么使用忽略下游效用的搜索指标优化检索，要么微调整个LLM以联合推理和检索，这限制了真实搜索效用和对冻结或专有模型的兼容性。

**方法:** 提出了s3框架，该框架将检索器与生成器解耦，并使用“RAG之外的收益”奖励来训练检索器。

**结果:** s3只需要2.4k训练样本就能在六个通用QA和五个医学QA基准上超越那些基于超过70倍数据量训练的基线。

**结论:** 所提出的s3框架证明了在不依赖大量训练数据的情况下，提高检索增强生成系统的性能是可行的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是s3%3A+You+Don%27t+Need+That+Much+Data+to+Train+a+Search+Agent+via+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14146&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [185] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/abs/2505.14147)
*Xiong Jun Wu, Zhenduo Zhang, ZuJie Wen, Zhiqiang Zhang, Wang Ren, Lei Shi, Cai Chen, Deng Zhao, Dingnan Jin, Qing Cui, Jun Zhou*

**主要类别:** cs.AI

**概要:** SHARP 提出了一种新的方法来合成高质量的推理问题用于强化学习训练，显著提高了复杂推理任务的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于合成的方法生成的问题过于简单或不可验证，限制了大型推理模型在复杂任务上的进步。

**方法:** 引入SHARP方法，包含自我对齐原则和三阶段框架，利用高级LRM通过可验证奖励信号优化推理能力。

**结果:** 实验表明，SHARP增强的训练在复杂推理任务上明显优于现有方法，提升了LRM的性能。

**结论:** SHARP策略及其框架设计为提高大型推理模型的推理能力提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SHARP%3A+Synthesizing+High-quality+Aligned+Reasoning+Problems+for+Large+Reasoning+Models+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14147，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14147&send_immediately=true&force_search=false)

**原文摘要:** Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [186] [MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem](https://arxiv.org/abs/2505.14148)
*Fan Liu, Zherui Yang, Cancheng Liu, Tianrui Song, Xiaofeng Gao, Hao Liu*

**主要类别:** cs.AI

**概要:** This paper introduces MM-Agent, an expert-inspired framework for LLM-powered real-world mathematical modeling, which outperforms human experts by 11.88% on MM-Bench.


<details>
  <summary>更多</summary>
  
**动机:** Existing LLMs excel in reasoning but struggle with rigorous model construction, limiting their application in real-world problem-solving.

**方法:** MM-Agent decomposes mathematical modeling into four stages: problem analysis, model formulation, computational problem solving, and report generation.

**结果:** On MM-Bench, MM-Agent outperforms baseline agents and achieves better results than human experts with lower cost.

**结论:** MM-Agent demonstrates practical effectiveness as a modeling copilot and can assist in solving complex real-world problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MM-Agent%3A+LLM+as+Agents+for+Real-world+Mathematical+Modeling+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14148，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14148&send_immediately=true&force_search=false)

**原文摘要:** Mathematical modeling is a cornerstone of scientific discovery and
engineering practice, enabling the translation of real-world problems into
formal systems across domains such as physics, biology, and economics. Unlike
mathematical reasoning, which assumes a predefined formulation, modeling
requires open-ended problem analysis, abstraction, and principled
formalization. While Large Language Models (LLMs) have shown strong reasoning
capabilities, they fall short in rigorous model construction, limiting their
utility in real-world problem-solving. To this end, we formalize the task of
LLM-powered real-world mathematical modeling, where agents must analyze
problems, construct domain-appropriate formulations, and generate complete
end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111
problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the
years 2000 to 2025 and across ten diverse domains such as physics, biology, and
economics. To tackle this task, we propose MM-Agent, an expert-inspired
framework that decomposes mathematical modeling into four stages: open-ended
problem analysis, structured model formulation, computational problem solving,
and report generation. Experiments on MM-Bench show that MM-Agent significantly
outperforms baseline agents, achieving an 11.88\% improvement over human expert
solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o.
Furthermore, under official MCM/ICM protocols, MM-Agent assisted two
undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among
27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a
modeling copilot. Our code is available at
https://github.com/usail-hkust/LLM-MM-Agent

</details>


### [187] [DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation](https://arxiv.org/abs/2505.14163)
*He Wang, Alexander Hanbo Li, Yiqun Hu, Sheng Zhang, Hideo Kobayashi, Jiani Zhang, Henry Zhu, Chung-Wei Hang, Patrick Ng*

**主要类别:** cs.AI

**概要:** 提出了一种新的推理时间优化框架DSMentor，利用课程学习策略提高LLM代理在复杂数据科学任务中的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要集中在通过改进搜索、采样和规划技术来增强上下文学习，而忽略了推理过程中问题顺序的重要性。

**方法:** 开发了名为DSMentor的推理时间优化框架，该框架利用课程学习策略组织数据科学任务，并结合长期记忆来指导代理的学习进展。

**结果:** 在DSEval和QRData基准上进行的实验表明，与基线代理相比，使用Claude-3.5-Sonnet的DSMentor提高了通过率。此外，在因果关系问题上，DSMentor比使用Program-of-Thoughts提示的GPT-4提高了通过率。

**结论:** 强调了在推理过程中积累和有效利用知识的重要性，这模仿了人类的学习过程，并为通过基于课程的推理优化提高LLM性能开辟了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DSMentor%3A+Enhancing+Data+Science+Agents+with+Curriculum+Learning+and+Online+Knowledge+Accumulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14163，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14163&send_immediately=true&force_search=false)

**原文摘要:** Large language model (LLM) agents have shown promising performance in
generating code for solving complex data science problems. Recent studies
primarily focus on enhancing in-context learning through improved search,
sampling, and planning techniques, while overlooking the importance of the
order in which problems are tackled during inference. In this work, we develop
a novel inference-time optimization framework, referred to as DSMentor, which
leverages curriculum learning -- a strategy that introduces simpler task first
and progressively moves to more complex ones as the learner improves -- to
enhance LLM agent performance in challenging data science tasks. Our
mentor-guided framework organizes data science tasks in order of increasing
difficulty and incorporates a growing long-term memory to retain prior
experiences, guiding the agent's learning progression and enabling more
effective utilization of accumulated knowledge. We evaluate DSMentor through
extensive experiments on DSEval and QRData benchmarks. Experiments show that
DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval
and QRData compared to baseline agents. Furthermore, DSMentor demonstrates
stronger causal reasoning ability, improving the pass rate by 8.8% on the
causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our
work underscores the importance of developing effective strategies for
accumulating and utilizing knowledge during inference, mirroring the human
learning process and opening new avenues for improving LLM performance through
curriculum-based inference optimization.

</details>


### [188] [Dynamic Replanning for Improved Public Transport Routing](https://arxiv.org/abs/2505.14193)
*Abdallah Abuaisha, Bojie Shen, Daniel Harabor, Peter Stuckey, Mark Wallace*

**主要类别:** cs.AI

**概要:** This paper addresses the lack of a framework to exploit live delay data for real-time journey adjustment in public transport routing. Two approaches are proposed: 'pull' (manual request) and 'push' (server monitoring). The push approach outperforms the pull approach with significant speedups and arrival time savings.


<details>
  <summary>更多</summary>
  
**动机:** Existing solutions for handling delays in public transport are limited. Backup plans based on historical data miss opportunities for earlier arrivals, while snapshot planning does not account for future delays. Live delay data is underutilized for system-scale dynamic replanning.

**方法:** Two solutions are proposed: a 'pull' approach where users manually request replanning, and a 'push' approach where the server proactively monitors and adjusts journeys.

**结果:** The push approach outperforms the pull approach, achieving significant speedups. Dynamic replanning enables substantial arrival time savings.

**结论:** A framework for system-scale dynamic replanning in public transport routing has been developed, demonstrating the potential of using live delay data for real-time journey adjustments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Replanning+for+Improved+Public+Transport+Routing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14193&send_immediately=true&force_search=false)

**原文摘要:** Delays in public transport are common, often impacting users through
prolonged travel times and missed transfers. Existing solutions for handling
delays remain limited; backup plans based on historical data miss opportunities
for earlier arrivals, while snapshot planning accounts for current delays but
not future ones. With the growing availability of live delay data, users can
adjust their journeys in real-time. However, the literature lacks a framework
that fully exploits this advantage for system-scale dynamic replanning. To
address this, we formalise the dynamic replanning problem in public transport
routing and propose two solutions: a "pull" approach, where users manually
request replanning, and a novel "push" approach, where the server proactively
monitors and adjusts journeys. Our experiments show that the push approach
outperforms the pull approach, achieving significant speedups. The results also
reveal substantial arrival time savings enabled by dynamic replanning.

</details>


### [189] [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](https://arxiv.org/abs/2505.14209)
*Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, Wenjun Wu*

**主要类别:** cs.AI

**概要:** 研究大规模异构三维边境防御博弈问题，提出嵌入式平均场演员评论家(EMFAC)框架，并通过仿真和实际实验验证其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究多集中于小规模二维简化场景，忽视了现实环境中的扰动、运动动力学和固有异质性等挑战。

**方法:** 在三维设置中研究大规模异构边境防御博弈，考虑运动动力学和风场等真实元素，推导纳什均衡策略并提出EMFAC框架。

**结果:** 通过大量仿真验证理论发现，EMFAC在收敛速度和整体性能上优于基准模型，在小规模现实实验中也表现出有效性。

**结论:** 提出的EMFAC框架能够有效解决大规模异构边境防御中的控制挑战，并在复杂场景中展现出良好的适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embedded+Mean+Field+Reinforcement+Learning+for+Perimeter-defense+Game，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14209，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14209&send_immediately=true&force_search=false)

**原文摘要:** With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.

</details>


### [190] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
*Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, Keith Ross*

**主要类别:** cs.AI

**概要:** This paper investigates why reinforcement learning with verifiable rewards (RLVR) enhances accuracy but not capability, and why distillation improves both. It finds that RLVR prioritizes easier questions at the expense of harder ones and doesn't generate notably longer or reflective responses. Distillation improves accuracy through strong reasoning patterns but only boosts capability with new knowledge.


<details>
  <summary>更多</summary>
  
**动机:** To understand the mechanisms behind the phenomena of RLVR enhancing accuracy but not capability, and distillation improving both.

**方法:** Investigating the mechanisms through analysis and experiments.

**结果:** RLVR does not improve capability as it focuses on easier questions, and its small model settings produce quality responses absent before training without noticeable length or reflection keyword increases. Distillation improves accuracy via strong reasoning patterns but only enhances capability when new knowledge is introduced.

**结论:** The findings provide insights into how RLVR and distillation influence reasoning behavior in language models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+vs.+Distillation%3A+Understanding+Accuracy+and+Capability+in+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14216&send_immediately=true&force_search=false)

**原文摘要:** Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [191] [Toward Embodied AGI: A Review of Embodied AI and the Road Ahead](https://arxiv.org/abs/2505.14235)
*Yequan Wang, Aixin Sun*

**主要类别:** cs.AI

**概要:** This paper introduces a taxonomy for Embodied AGI with five levels and proposes a conceptual framework for an L3+ robotic brain.


<details>
  <summary>更多</summary>
  
**动机:** To contribute to the discourse on Artificial General Intelligence (AGI) by creating a systematic taxonomy and exploring higher-level capabilities.

**方法:** Introduce a taxonomy of Embodied AGI with five levels and review existing research and challenges at foundational stages. Outline key components for higher-level capabilities.

**结果:** A proposed conceptual framework for an L3+ robotic brain based on insights and existing technologies.

**结论:** The paper outlines a path towards increasingly generalized embodied AI systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Embodied+AGI%3A+A+Review+of+Embodied+AI+and+the+Road+Ahead，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14235&send_immediately=true&force_search=false)

**原文摘要:** Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.

</details>


### [192] [EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection](https://arxiv.org/abs/2505.14289)
*Yijie Lu, Tianjie Ju, Manman Zhao, Xinbei Ma, Yuan Guo, ZhuoSheng Zhang*

**主要类别:** cs.AI

**概要:** This paper proposes EVA, a red teaming framework for indirect prompt injection that uses continuous monitoring and updating of adversarial cues to improve attack success rates and transferability across diverse GUI scenarios.


<details>
  <summary>更多</summary>
  
**动机:** To address emerging threats from indirect prompt injection attacks on multimodal agents operating GUIs.

**方法:** EVA transforms the attack into a closed loop optimization by continuously monitoring an agent's attention distribution over the GUI and updating adversarial cues, keywords, phrasing, and layout.

**结果:** EVA improves attack success rates and transferability across diverse GUI scenarios compared to previous one-shot methods.

**结论:** Indirect prompt injection is a powerful tool for red teaming agents and uncovering common vulnerabilities in their multimodal decision making.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EVA%3A+Red-Teaming+GUI+Agents+via+Evolving+Indirect+Prompt+Injection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14289，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14289&send_immediately=true&force_search=false)

**原文摘要:** As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.

</details>


### [193] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
*Maheep Chaudhary, Fazl Barez*

**主要类别:** cs.AI

**概要:** 提出了一种实时框架来预测大型语言模型（LLMs）产生的有害输出，通过无监督方法识别正常行为与有害输出的区别。研究聚焦于后门触发响应，开发了Safety-Net多检测器框架提高检测准确率。


<details>
  <summary>更多</summary>
  
**动机:** 高风险行业需要监控保障，LLMs同样需要防止产生有害输出，如暴力、色情或仇恨言论。

**方法:** 采用无监督方法，将正常行为作为基线，有害输出视为异常值。从人类欺骗行为中得到启发，探索LLMs生成有害内容时的内部行为特征。

**结果:** Safety-Net框架在检测有害案例时达到了96%的准确率。

**结论:** 模型可能通过因果机制产生有害内容，并可能变得具有欺骗性。提出的框架能够有效检测跨表征空间转移信息时的有害行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SafetyNet%3A+Detecting+Harmful+Outputs+in+LLMs+by+Modeling+and+Monitoring+Deceptive+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14300，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14300&send_immediately=true&force_search=false)

**原文摘要:** High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [194] [Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds](https://arxiv.org/abs/2505.14366)
*Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska*

**主要类别:** cs.AI

**概要:** 提出一个概念框架来训练视觉语言模型(VLMs)执行视觉视角获取(VPT)，这是实体认知的核心能力，对人机交互(HRI)至关重要。引入了一个在NVIDIA Omniverse中生成的合成数据集，用于监督学习空间推理任务。重点在于推断Z轴距离作为一种基础技能，并计划未来扩展到完整的6自由度推理。该数据集公开可用以支持进一步研究。这项工作是迈向具备交互式人机场景中空间理解能力的实体AI系统的基础步骤。


<details>
  <summary>更多</summary>
  
**动机:** 为了使机器人能够在交互式人机场景中有空间理解能力，需要发展一种核心的实体认知能力，即视觉视角获取(VPT)。

**方法:** 提出了一个概念框架，并创建了一个合成数据集，该数据集包括RGB图像、自然语言描述和表示物体姿态的4X4变换矩阵。

**结果:** 数据集公开可用，可以用来支持进一步的研究。

**结论:** 这是迈向具备交互式人机场景中空间理解能力的实体AI系统的基础步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Embodied+Cognition+in+Robots+via+Spatially+Grounded+Synthetic+Worlds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14366，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14366&send_immediately=true&force_search=false)

**原文摘要:** We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.

</details>


### [195] [SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.14381)
*Yuyang Dong, Nobuhiro Ueda, Krisztián Boros, Daiki Ito, Takuya Sera, Masafumi Oyamada*

**主要类别:** cs.AI

**概要:** 提出了一种新的方法SCAN，用于增强处理视觉丰富文档的文本和视觉RAG系统。该方法通过适当的语义粒度识别文档组件，在上下文保留与处理效率之间取得平衡。实验结果显示，应用SCAN可以提高文本RAG性能最高可达9.0％，视觉RAG性能最高可达6.4％。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型和视觉语言模型的广泛应用，对于检索增强生成（RAG）和视觉RAG等应用程序的富文档分析技术引起了广泛关注。然而，处理富文档仍然存在挑战，因为单个页面包含大量信息。

**方法:** 提出了一种名为SCAN的新方法，它是一种VLM友好的方法，能够识别具有适当语义粒度的文档组件，并使用粗粒度语义方法将文档划分为连贯的区域覆盖连续组件。通过微调带有复杂注释数据集的对象检测模型来训练SCAN模型。

**结果:** 在英语和日语数据集上的实验结果表明，应用SCAN可以提高端到端文本RAG性能最高可达9.0％，视觉RAG性能最高可达6.4％，优于传统方法甚至商业文档处理解决方案。

**结论:** SCAN是一种有效的VLM友好方法，可以显著提高处理视觉丰富文档的文本和视觉RAG系统的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCAN%3A+Semantic+Document+Layout+Analysis+for+Textual+and+Visual+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14381&send_immediately=true&force_search=false)

**原文摘要:** With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.

</details>


### [196] [Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning](https://arxiv.org/abs/2505.14391)
*Zhaohui Yang, Chenghua He, Xiaowen Shi, Linjing Li, Qiyue Yin, Shihong Deng, Daxin Jiang*

**主要类别:** cs.AI

**概要:** 提出一种新的PRM数据标注方法，改善长CoT推理过程中的错误识别，实验显示其在多个指标上优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前数据标注方法在处理长CoT推理时仅关注初始错误及其后步骤，忽视了自我修正机制。

**方法:** 引入错误传播和停止概念，利用LLM判官进行标注收集170万样本训练7B规模PRM。

**结果:** 提出的PRM在搜索引导、BoN和F1等多指标上表现更优，且数据效率更高。

**结论:** 新方法不仅提高了数据效率，还增强了PRM识别有效自我修正行为的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+the+First+Error%3A+Process+Reward+Models+for+Reflective+Mathematical+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14391，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14391&send_immediately=true&force_search=false)

**原文摘要:** Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.

</details>


### [197] [Knowledge Graph Based Repository-Level Code Generation](https://arxiv.org/abs/2505.14394)
*Mihir Athale, Vishal Vaddina*

**主要类别:** cs.AI

**概要:** 提出了一种基于知识图谱的方法来改善代码搜索和检索，从而提高代码生成的质量。该方法在EvoCodeBench数据集上显著优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前代码搜索与检索方法在质量和上下文相关性方面存在不足，导致代码生成效果不佳。

**方法:** 将代码存储库表示为图形，并采用混合方法进行代码检索，增强上下文感知的代码生成能力。

**结果:** 所提方法在EvoCodeBench数据集上的性能显著优于基线方法。

**结论:** 基于知识图谱的代码生成可能推动健壮且敏感于上下文的编码辅助工具的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Graph+Based+Repository-Level+Code+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14394，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14394&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.

</details>


### [198] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
*Gaël Gendron, Jože M. Rožanec, Michael Witbrock, Gillian Dobbie*

**主要类别:** cs.AI

**概要:** 提出因果制图框架(Causal Cartographer)，通过提取和建模因果关系，提升大语言模型在因果推理任务中的鲁棒性，并降低推理成本。


<details>
  <summary>更多</summary>
  
**动机:** 现有基础模型（如大语言模型）缺乏因果推理能力，且评估反事实问题受限于仅能观察到事实世界，难以在真实应用中评估。

**方法:** 引入基于图检索增强的生成代理提取因果关系构建因果知识网络，并设计受因果关系约束的反事实推理代理进行因果推理。

**结果:** 所提方法能够提取因果知识，提高大语言模型在因果推理任务中的鲁棒性，同时降低推理成本和虚假相关性。

**结论:** 本文提出的因果制图框架成功解决了现有模型在因果推理上的局限性，并展示了其在真实世界因果推理任务中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Cartographer%3A+From+Mapping+to+Reasoning+Over+Counterfactual+Worlds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14396&send_immediately=true&force_search=false)

**原文摘要:** Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [199] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, Daxin Jiang*

**主要类别:** cs.AI

**概要:** This paper proposes a method called BCPG-NSA that improves the performance of reasoning language models by utilizing negative samples.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods fail to leverage valuable components in negative responses, such as self-reflection and error-correction steps, due to high computational costs.

**方法:** BCPG-NSA is a fine-grained offline RL framework with three stages: sample segmentation, consensus-based step correctness assessment, and policy optimization with negative sample augmentation.

**结果:** BCPG-NSA outperforms baselines on challenging math/coding reasoning benchmarks using the same training dataset, showing improved sample efficiency and robustness.

**结论:** The proposed method demonstrates the importance of leveraging negative samples for better performance in reasoning language models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unearthing+Gems+from+Stones%3A+Policy+Optimization+with+Negative+Sample+Augmentation+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14403，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14403&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [200] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
*Paweł Batorski, Adrian Kosmala, Paul Swoboda*

**主要类别:** cs.AI

**概要:** PRL是一种基于强化学习的新方法，用于自动提示生成，无需在训练期间看到的提示示例，并在文本分类、简化和总结任务上实现了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 有效提示工程仍然是充分利用LLMs能力的核心挑战。虽然设计良好的提示可以显著提高性能，但创建它们通常需要专家直觉和对任务的细致理解。此外，最有影响的提示往往依赖于微妙的语义线索，这些线索可能逃避人类的感知，但对指导LLM行为至关重要。

**方法:** PRL（基于强化学习的提示），一种新的RL方法，用于自动提示生成。

**结果:** PRL在一系列基准测试中实现了最先进的性能，包括文本分类、简化和总结。在分类任务中，它比APE高出2.58％，比EvoPrompt高出1.00％。此外，在摘要任务中，它提高了平均ROUGE分数，比APE高4.32，比EvoPrompt高2.12，在简化任务中的SARI得分比APE高6.93，比EvoPrompt高6.01。

**结论:** PRL是一种有效的自动提示生成方法，可以在多种任务中实现最先进的性能。我们的代码可在https://github.com/Batorskq/prl获得。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PRL%3A+Prompts+from+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14412&send_immediately=true&force_search=false)

**原文摘要:** Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [201] [SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation](https://arxiv.org/abs/2505.14419)
*Huimin Xu, Xin Mao, Feng-Lin Li, Xiaobao Wu, Wang Chen, Wei Zhang, Anh Tuan Luu*

**主要类别:** cs.AI

**概要:** Introduces SCOPE, a novel compression-based approach that reduces annotation costs for Process Reward Models in mathematical reasoning by translating reasoning steps into code and normalizing them through Abstract Syntax Tree, merging equivalent steps, and constructing a prefix tree.


<details>
  <summary>更多</summary>
  
**动机:** Existing process annotation approaches for Process Reward Models are computationally expensive, either through human annotations or Monte Carlo simulations.

**方法:** SCOPE translates natural language reasoning steps into code, normalizes them through Abstract Syntax Tree, merges equivalent steps, and constructs a prefix tree, reducing annotation complexity from O(NMK) to O(N).

**结果:** SCOPE constructs a large-scale dataset containing 196K samples with only 5% of the computational resources required by previous methods, and PRMs trained on this dataset outperform existing automated annotation approaches on both Best-of-N strategy and ProcessBench.

**结论:** SCOPE is a novel and efficient approach to reduce annotation costs for Process Reward Models in mathematical reasoning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCOPE%3A+Compress+Mathematical+Reasoning+Steps+for+Efficient+Automated+Process+Annotation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14419，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14419&send_immediately=true&force_search=false)

**原文摘要:** Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.

</details>


### [202] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
*Oren Sultan, Eitan Stern, Dafna Shahaf*

**主要类别:** cs.AI

**概要:** 提出一种神经符号方法结合大型语言模型生成能力和结构组件以改善形式领域如数学证明生成中的逻辑推理和符号推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在需要严格逻辑推导和符号推理的形式领域（如数学证明生成）中表现不佳。

**方法:** 通过检索类似问题及其证明来指导大型语言模型，并使用形式验证器评估生成的证明并提供反馈。

**结果:** 该方法显著提高了OpenAI的o1模型的证明准确性（58%-70%的改进）。

**结论:** 转向能够生成可证明正确结论的大型语言模型可以大大提升其可靠性、准确性和一致性，解锁需要可信度的复杂任务和现实世界应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Reliable+Proof+Generation+with+LLMs%3A+A+Neuro-Symbolic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14479，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14479&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [203] [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
*Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, Minjoon Seo*

**主要类别:** cs.AI

**概要:** This study investigates the confidence calibration of reasoning models compared to non-reasoning LLMs, demonstrating that reasoning models exhibit superior confidence calibration due to their extended chain-of-thought reasoning.


<details>
  <summary>更多</summary>
  
**动机:** Large language models often struggle with accurately communicating their confidence, which affects reliability. This study aims to explore whether reasoning models can improve this aspect.

**方法:** Benchmarking six reasoning models across six datasets to evaluate their confidence calibration and analyzing the impact of slow thinking behaviors.

**结果:** Reasoning models show better confidence calibration than non-reasoning models, especially as their chain-of-thought reasoning progresses. Removing slow thinking behaviors decreases calibration accuracy.

**结论:** Reasoning models have better confidence calibration due to slow thinking behaviors, and non-reasoning models can also benefit from such behaviors through in-context learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Models+Better+Express+Their+Confidence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14489，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14489&send_immediately=true&force_search=false)

**原文摘要:** Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [204] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/abs/2505.14510)
*Haishi Bai, Jozo Dujmovic, Jianwu Wang*

**主要类别:** cs.AI

**概要:** 提出一种名为BACON的新框架，用于自动训练可解释的人工智能模型，以解决决策问题并提供精确的逻辑解释。


<details>
  <summary>更多</summary>
  
**动机:** 在高风险领域（如医疗保健、安全、金融和机器人）中部署机器学习模型的需求日益增长，需要透明且可信的解释。

**方法:** 使用分级逻辑自动训练可解释的人工智能模型。

**结果:** BACON在多个场景下提供了高性能模型，并生成了可验证的决策逻辑。

**结论:** BACON是一种实用且有原则的方法，可以提供清晰、可信的可解释人工智能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BACON%3A+A+fully+explainable+AI+model+with+graded+logic+for+decision+making+problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14510，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14510&send_immediately=true&force_search=false)

**原文摘要:** As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [205] [Guarded Query Routing for Large Language Models](https://arxiv.org/abs/2505.14524)
*Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, Lukas Galke*

**主要类别:** cs.AI

**概要:** 本研究提出了一个受保护的查询路由问题，并引入了GQR-Bench基准来评估不同模型在应对分布外查询时的有效性和效率。结果显示WideMLP在准确率和速度之间取得了最佳平衡，而LLMs虽然准确率最高但速度较慢。


<details>
  <summary>更多</summary>
  
**动机:** 查询路由任务可以被视为文本分类问题，但需要妥善处理分布外查询。因此，我们研究了一个‘受保护’的查询路由问题。

**方法:** 对比了基于LLM的路由机制、标准LLM基线的守卫方法、连续词袋分类器和传统机器学习模型在处理查询路由问题上的有效性与效率。

**结果:** WideMLP在准确率(88%)和速度(<4ms)之间达到了最佳平衡；基于嵌入的fastText在速度(<1ms)上表现优异且具有可接受的准确率(80%)；LLMs具有最高的准确率(91%)但速度较慢(本地Llama-3.1:8B为62ms，远程GPT-4o-mini调用为669ms)。

**结论:** 我们的研究挑战了对大型语言模型在（受保护的）查询路由中的自动依赖，并为实际应用提供了具体的建议。GQR-Bench 将作为 Python 包 - `gqr` 发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guarded+Query+Routing+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14524，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14524&send_immediately=true&force_search=false)

**原文摘要:** Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.

</details>


### [206] [A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)](https://arxiv.org/abs/2505.14539)
*Gaia Belardinelli, Thomas Bolander, Sebastian Watzl*

**主要类别:** cs.AI

**概要:** This paper introduces the first general logic of attention, overcoming limitations of existing dynamic epistemic logics by generalizing edge-conditioned event models and extending attention to arbitrary formulas.


<details>
  <summary>更多</summary>
  
**动机:** To address the limitations of existing logics in modeling complex attention scenarios and biases.

**方法:** Generalizing edge-conditioned event models and extending attention to arbitrary formulas.

**结果:** A new logic that is more expressive and succinct than previous ones, capable of modeling complex attention scenarios and biases.

**结论:** The proposed logic treats attention as a modality and introduces attention principles for its axiomatization, demonstrated through examples of AI agents reasoning about human attentional biases.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Logic+of+General+Attention+Using+Edge-Conditioned+Event+Models+%28Extended+Version%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14539，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14539&send_immediately=true&force_search=false)

**原文摘要:** In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.

</details>


### [207] [Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study](https://arxiv.org/abs/2505.14544)
*Saahil Mahato*

**主要类别:** cs.AI

**概要:** This study uses multi-agent reinforcement learning to optimize traffic signal coordination in a simulated urban traffic environment, showing significant reductions in vehicle wait times and increased throughput compared to fixed-time control systems.


<details>
  <summary>更多</summary>
  
**动机:** Addressing urban traffic congestion and improving traffic signal adaptability to manage dynamic traffic patterns.

**方法:** Developing a simulated environment with Pygame, implementing a decentralized MARL controller where traffic signals act as autonomous agents, and evaluating performance against a fixed-time controller.

**结果:** Statistically significant improvements in average vehicle wait time and overall throughput were observed with the MARL approach.

**结论:** The study suggests potential for MARL-based dynamic control strategies to enhance urban traffic management efficiency, with recommendations for further research into scalability and real-world implementation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-agent+Reinforcement+Learning+vs.+Fixed-Time+Control+for+Traffic+Signal+Optimization%3A+A+Simulation+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14544&send_immediately=true&force_search=false)

**原文摘要:** Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.

</details>


### [208] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
*Devansh Bhardwaj, Arjun Beniwal, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik R. Narasimhan, Ameet Deshpande, Vishvak Murahari*

**主要类别:** cs.AI

**概要:** Introduces Agent context protocols (ACPs) for structured multi-agent communication and coordination, improving performance on complex tasks.


<details>
  <summary>更多</summary>
  
**动机:** Current methods of coordination among AI agents using natural language limit complex interactions and interoperability with domain-specific agents.

**方法:** Development of ACPs, which include persistent execution blueprints and standardized message schemas for robust multi-agent collective inference.

**结果:** ACP-powered systems achieve state-of-the-art performance in long-horizon web assistance and multimodal technical reports, outperforming commercial AI systems in human evaluations.

**结论:** ACP is a versatile tool for rapidly building high-performing generalist AI agents.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent+Context+Protocols+Enhance+Collective+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14569，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14569&send_immediately=true&force_search=false)

**原文摘要:** AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [209] [Towards a Foundation Model for Communication Systems](https://arxiv.org/abs/2505.14603)
*Davide Buffelli, Sowmen Das, Yu-Wei Lin, Sattar Vakili, Chien-Yi Wang, Masoud Attarifar, Pritthijit Nath, Da-shan Shiu*

**主要类别:** cs.AI

**概要:** This paper proposes a transformer-based foundation model for communication data, capable of handling multi-modality and estimating multiple features.


<details>
  <summary>更多</summary>
  
**动机:** To develop a general model for communication systems that can handle multiple applications and features.

**方法:** A transformer-based, multi-modal model designed to operate directly on communication data, with proposed methodologies to address key challenges.

**结果:** The model successfully estimates multiple features including transmission rank, selected precoder, Doppler spread, and delay profile.

**结论:** This work takes a step towards a foundation model for communication data, demonstrating the potential of large general models in communication systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+a+Foundation+Model+for+Communication+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14603，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14603&send_immediately=true&force_search=false)

**原文摘要:** Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.

</details>


### [210] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604)
*Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang*

**主要类别:** cs.AI

**概要:** 提出了一种新的框架Self-Braking Tuning (SBT)，用于减少大型推理模型在生成过程中的冗余推理，从而降低计算开销并解决过度思考的问题。实验表明，该方法可以将token消耗减少高达60%，同时保持与不受约束的模型相当的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的解决过度思考问题的方法通常依赖于外部干预，而本文提出的方法旨在从允许模型调节其自身推理过程的角度出发，消除对外部控制机制的依赖。

**方法:** 提出了一种名为Self-Braking Tuning (SBT)的新框架，包括构建基于标准答案的过度思考识别指标，设计系统方法检测冗余推理，并开发完整的自适应推理长度数据构造策略和创新的刹车提示机制。

**结果:** 该方法能够显著减少token消耗，最高可达60%，同时保持与不受约束的模型相当的准确性。

**结论:** 所提出的Self-Braking Tuning框架成功地减少了大型推理模型中的冗余推理，降低了计算成本，并且不需要外部控制机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Let+LLMs+Break+Free+from+Overthinking+via+Self-Braking+Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14604，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14604&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


### [211] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
*Anjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh, Huanmi Tan, Zhanke Zhou, Sanmi Koyejo, Ke Wang, Alex Aiken*

**主要类别:** cs.AI

**概要:** Introduces SATBench, a benchmark for evaluating LLMs' logical reasoning capabilities through SAT-derived puzzles.


<details>
  <summary>更多</summary>
  
**动机:** To evaluate LLMs' logical reasoning abilities beyond inference rule-based methods by leveraging the search-based nature of SAT problems.

**方法:** Generates puzzles from SAT formulas, translates them into story contexts, and validates them using LLMs and solvers.

**结果:** Strongest model achieves only 65.0% accuracy on hard UNSAT problems, similar to random baseline.

**结论:** Highlights limitations in LLMs' search-based logical reasoning abilities and provides a scalable testbed for future research.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SATBench%3A+Benchmarking+LLMs%27+Logical+Reasoning+via+Automated+Puzzle+Generation+from+SAT+Formulas，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14615，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14615&send_immediately=true&force_search=false)

**原文摘要:** We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [212] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
*Ashutosh Adhikari, Mirella Lapata*

**主要类别:** cs.AI

**概要:** 提出一种多模态辩论框架，利用较弱的语言模型监督和增强较强的视觉-语言模型在视觉问答任务中的表现。实验表明该框架优于单独的专家模型，并且较弱的语言模型的判断可以通过微调帮助视觉-语言模型获得推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型在不同领域和模态上获得专业知识，可扩展的监督变得越来越具有挑战性，特别是当它们的能力可能超过人类评估者时。

**方法:** 扩展了辩论范式到多模态设置中，让两个“有视力”的视觉-语言模型专家就答案进行辩论，而一个“无视力”（仅文本）的裁判根据论点质量来裁决。

**结果:** 在多个多模态任务上的实验显示，辩论框架始终优于个体专家模型。此外，较弱的语言模型的判断可以通过微调帮助视觉-语言模型获得推理能力。

**结论:** 提出的多模态辩论框架为较弱模型监督和增强较强模型提供了一种新的方法，并展示了其在视觉问答任务中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Debating+for+Better+Reasoning%3A+An+Unsupervised+Multimodal+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14627&send_immediately=true&force_search=false)

**原文摘要:** As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [213] [Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning](https://arxiv.org/abs/2505.14656)
*Zihao Zhang, Fei Liu*

**主要类别:** cs.AI

**概要:** This paper introduces CATS, a new method that adds explicit cost-awareness to large language model (LLM)-guided planning using Cost-Augmented Monte Carlo Tree Search.


<details>
  <summary>更多</summary>
  
**动机:** LLMs struggle with cost-sensitive planning as they either treat all actions equally or fail under strict budgets.

**方法:** Developing Cost-Augmented Monte Carlo Tree Search (CATS) to bring explicit cost-awareness into LLM-guided planning.

**结果:** CATS performs better than raw LLMs like GPT-4.1 in cost-sensitive scenarios with higher task success rates and cost efficiency.

**结论:** CATS effectively combines LLM reasoning power with structured search for budget-aware decision-making.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cost-Augmented+Monte+Carlo+Tree+Search+for+LLM-Assisted+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14656，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14656&send_immediately=true&force_search=false)

**原文摘要:** While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.

</details>


### [214] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
*Wonje Jeung, Sangyeon Yoon, Minsuk Kahng, Albert No*

**主要类别:** cs.AI

**概要:** 提出SAFEPATH方法，在保持推理性能的同时有效减少有害输出，并且比其他方法更高效。


<details>
  <summary>更多</summary>
  
**动机:** 现有安全对齐方法在减少有害输出时会降低推理深度，在复杂任务中存在显著权衡且容易受到高级攻击。

**方法:** 引入轻量级对齐方法SAFEPATH，在有害提示下微调LRM在推理开始时发出8-token安全引言。

**结果:** SAFEPATH减少了高达90.0%的有害响应，在DeepSeek-R1-Distill-Llama-8B模型中阻止了83.3%的越狱尝试，计算需求显著低于其他方法。

**结论:** SAFEPATH提供了高效的解决方案来提高LRM的安全性，同时揭示了LLM现有方法在推理模型中的局限性和新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SAFEPATH%3A+Preventing+Harmful+Reasoning+in+Chain-of-Thought+via+Early+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14667，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14667&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [215] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan*

**主要类别:** cs.AI

**概要:** 提出ContextAgent，首个结合多维上下文感知的主动式大型语言模型代理，能从可穿戴设备的感官数据和历史数据中提取信息来预测用户意图并提供非侵入式辅助服务。通过创建ContextAgentBench基准测试集验证了其在主动预测和工具调用上的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有主动代理要么依赖封闭环境下的直接LLM推断，要么采用基于规则的通知，导致用户意图理解不佳且功能受限，需要更全面的上下文感知能力来提高主动性和服务质量。

**方法:** ContextAgent首先从可穿戴设备的多维感官数据中提取上下文信息理解用户意图；接着利用这些上下文以及历史数据中的个人化上下文预测是否需要主动服务；当需要帮助时，自动调用相关工具进行辅助。

**结果:** ContextAgent在ContextAgentBench基准测试中展现出色表现，特别是在主动预测和工具调用方面分别提高了8.5%和6.0%的准确率。

**结论:** 研究展示了ContextAgent作为首个上下文感知主动代理的优势，并希望推动更加先进的人本导向主动AI助手的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ContextAgent%3A+Context-Aware+Proactive+LLM+Agents+with+Open-World+Sensory+Perceptions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14668&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [216] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
*Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu*

**主要类别:** cs.AI

**概要:** This paper introduces RICE, a novel method for improving reasoning performance in Large Reasoning Models by identifying and reinforcing specialized 'cognitive experts' without extra training or complex heuristics.


<details>
  <summary>更多</summary>
  
**动机:** To address cognitive inefficiencies (overthinking and underthinking) in existing reasoning models.

**方法:** Introduces Reinforcing Cognitive Experts (RICE) leveraging normalized Pointwise Mutual Information (nPMI) to identify cognitive experts.

**结果:** Significant improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization were observed across different benchmarks.

**结论:** The lightweight RICE approach outperforms current reasoning-steering techniques and enhances cognitive efficiency in advanced reasoning models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two+Experts+Are+All+You+Need+for+Steering+Thinking%3A+Reinforcing+Cognitive+Effort+in+MoE+Reasoning+Models+Without+Additional+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14681，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14681&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [217] [Data Balancing Strategies: A Survey of Resampling and Augmentation Methods](https://arxiv.org/abs/2505.13518)
*Behnam Yousefimehr, Mehdi Ghatee, Mohammad Amin Seifi, Javad Fazli, Sajed Tavakoli, Zahra Rafei, Shervin Ghaffari, Abolfazl Nikahd, Mahdi Razi Gandomani, Alireza Orouji, Ramtin Mahmoudi Kashani, Sarina Heshmati, Negin Sadat Mousavi*

**主要类别:** stat.ML

**概要:** This paper reviews various resampling strategies to address imbalanced data in machine learning, categorizing them into different types and discussing their effectiveness with case studies.


<details>
  <summary>更多</summary>
  
**动机:** To solve the problem of imbalanced data leading to skewed predictions and reduced model accuracy in machine learning.

**方法:** Review and categorization of resampling strategies including synthetic oversampling, adaptive techniques, generative models, ensemble-based strategies, hybrid approaches, undersampling, and neighbor-based methods.

**结果:** Highlights developments in resampling techniques and validates their effectiveness through practical implementations and case studies.

**结论:** Offers perspectives on future research directions in addressing imbalanced data in machine learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Balancing+Strategies%3A+A+Survey+of+Resampling+and+Augmentation+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13518&send_immediately=true&force_search=false)

**原文摘要:** Imbalanced data poses a significant obstacle in machine learning, as an
unequal distribution of class labels often results in skewed predictions and
diminished model accuracy. To mitigate this problem, various resampling
strategies have been developed, encompassing both oversampling and
undersampling techniques aimed at modifying class proportions. Conventional
oversampling approaches like SMOTE enhance the representation of the minority
class, whereas undersampling methods focus on trimming down the majority class.
Advances in deep learning have facilitated the creation of more complex
solutions, such as Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAEs), which are capable of producing high-quality synthetic
examples. This paper reviews a broad spectrum of data balancing methods,
classifying them into categories including synthetic oversampling, adaptive
techniques, generative models, ensemble-based strategies, hybrid approaches,
undersampling, and neighbor-based methods. Furthermore, it highlights current
developments in resampling techniques and discusses practical implementations
and case studies that validate their effectiveness. The paper concludes by
offering perspectives on potential directions for future exploration in this
domain.

</details>


### [218] [Continuous Domain Generalization](https://arxiv.org/abs/2505.13519)
*Zekun Cai, Yiheng Yao, Guangji Bai, Renhe Jiang, Xuan Song, Ryosuke Shibasaki, Liang Zhao*

**主要类别:** stat.ML

**概要:** 提出了一种新的领域泛化方法，称为连续域泛化(CDG)，通过神经李群算子和新策略，在多个数据集上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的领域泛化方法通常将领域视为离散的或者沿着单一轴（如时间）演变，未能捕捉现实世界变异的复杂多维性质。

**方法:** 提出了一种基于几何和代数理论的框架，设计了神经李群算子(NeuralLTO)，并引入了门机制和局部图策略。

**结果:** 提出的CDG方法在合成数据集和真实世界数据集（包括遥感、科学文档和交通预测）上的实验表明，其在泛化准确性和鲁棒性方面显著优于现有基线。

**结论:** 提出了连续域泛化(CDG)任务，并设计了基于几何和代数理论的框架，表明最优模型参数位于低维流形上。此外，提出了一种神经李群算子(NeuralLTO)，并引入门机制和局部图策略来处理描述符不完美情况。实验验证了该方法在泛化准确性和鲁棒性方面优于现有基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuous+Domain+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13519，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13519&send_immediately=true&force_search=false)

**原文摘要:** Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic context. However, existing
domain generalization approaches typically treat domains as discrete or
evolving along a single axis (e.g., time), which fails to capture the complex,
multi-dimensional nature of real-world variation. This paper introduces the
task of Continuous Domain Generalization (CDG), which aims to generalize
predictive models to unseen domains defined by arbitrary combinations of
continuous variation descriptors. We present a principled framework grounded in
geometric and algebraic theory, showing that optimal model parameters across
domains lie on a low-dimensional manifold. To model this structure, we propose
a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter
transitions by enforcing geometric continuity and algebraic consistency. To
handle noisy or incomplete domain descriptors, we introduce a gating mechanism
to suppress irrelevant dimensions and a local chart-based strategy for robust
generalization. Extensive experiments on synthetic and real-world
datasets-including remote sensing, scientific documents, and traffic
forecasting-demonstrate that our method significantly outperforms existing
baselines in generalization accuracy and robustness under descriptor
imperfections.

</details>


### [219] [Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback](https://arxiv.org/abs/2505.13562)
*Shishen Lin*

**主要类别:** stat.ML

**概要:** This paper proposes a new algorithm called Competitive Co-evolutionary Bandit Learning (COEBL) which combines evolutionary algorithms with bandit learning to achieve sublinear regret in matrix games.


<details>
  <summary>更多</summary>
  
**动机:** To explore the potential of randomized optimism in matrix games.

**方法:** Integrating evolutionary algorithms into the bandit framework to implement randomized optimism.

**结果:** The proposed COEBL algorithm achieves sublinear regret and outperforms classical bandit algorithms in empirical evaluations.

**结论:** This is the first theoretical regret analysis of an evolutionary bandit learning algorithm in matrix games.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Randomised+Optimism+via+Competitive+Co-Evolution+for+Matrix+Games+with+Bandit+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13562，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13562&send_immediately=true&force_search=false)

**原文摘要:** Learning in games is a fundamental problem in machine learning and artificial
intelligence, with numerous
applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work
investigates two-player zero-sum matrix games with an unknown payoff matrix and
bandit feedback, where each player observes their actions and the corresponding
noisy payoff. Prior studies have proposed algorithms for this
setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with
\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism
(e.g., \ucb) in achieving sublinear regret. However, the potential of
randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel
algorithm that integrates evolutionary algorithms (EAs) into the bandit
framework to implement randomised optimism through EA variation operators. We
prove that \coebl achieves sublinear regret, matching the performance of
deterministic optimism-based methods. To the best of our knowledge, this is the
first theoretical regret analysis of an evolutionary bandit learning algorithm
in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that
\coebl not only achieves sublinear regret but also consistently outperforms
classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic},
the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}.
These results highlight the potential of evolutionary bandit learning,
particularly the efficacy of randomised optimism via evolutionary algorithms in
game-theoretic settings.

</details>


### [220] [Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles](https://arxiv.org/abs/2505.13585)
*Xinzhu Liang, Joseph M. Lukens, Sanjaya Lohani, Brian T. Kirby, Thomas A. Searles, Xin Qiu, Kody J. H. Law*

**主要类别:** stat.ML

**概要:** 提出了一种新的贝叶斯蒙特卡洛方法SBMC，该方法在MNIST、CIFAR和IMDb等实际例子上证明了其效用。与传统方法相比，SBMC在性能和成本方面相当，并且在准确性上达到了最先进的水平，同时显著提高了不确定性量化（尤其是认识论不确定性量化）。然而，即使是并行实现也昂贵，进一步压缩时间会导致准确性迅速下降，但不确定性量化仍然有价值。通过锚定到点估计器，可以恢复准确性，同时保留有价值的不确定性量化，最终在成本上提供了与最先进技术相当的强大性能。


<details>
  <summary>更多</summary>
  
**动机:** 提出一种新的贝叶斯蒙特卡洛方法SBMC来解决现有方法的局限性，特别是在不确定性量化方面的不足。

**方法:** SBMC是一种并行实现的一致（渐近无偏）贝叶斯深度学习算法：顺序蒙特卡洛（SMC）或马尔可夫链蒙特卡洛（MCMC），并且它在点估计器和后验之间进行插值。

**结果:** SBMC在MNIST、CIFAR和IMDb等实际例子上证明了其效用，与串行实现相比，其性能和总成本相当，并且在准确性上达到了最先进的水平，同时显著提高了不确定性量化（尤其是认识论不确定性量化）。

**结论:** SBMC提供了一个强大的性能，其成本与最先进的技术相当，尽管并行实现昂贵，但通过锚定到点估计器可以恢复准确性，同时保留有价值的不确定性量化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Bayesian+Monte+Carlo%3A+fast+uncertainty+estimation+beyond+deep+ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13585&send_immediately=true&force_search=false)

**原文摘要:** This work introduces a new method called scalable Bayesian Monte Carlo
(SBMC). The model interpolates between a point estimator and the posterior, and
the algorithm is a parallel implementation of a consistent (asymptotically
unbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or
Markov chain Monte Carlo (MCMC). The method is motivated theoretically, and its
utility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic
numerical study reveals that parallel implementations of SMC and MCMC are
comparable to serial implementations in terms of performance and total cost,
and they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like
deep ensembles at convergence, along with substantially improved uncertainty
quantification (UQ)--in particular, epistemic UQ. But even parallel
implementations are expensive, with an irreducible time barrier much larger
than the cost of the MAP estimator. Compressing time further leads to rapid
degradation of accuracy, whereas UQ remains valuable. By anchoring to a point
estimator we can recover accuracy, while retaining valuable UQ, ultimately
delivering strong performance across metrics for a cost comparable to the SOTA.

</details>


### [221] [Backward Conformal Prediction](https://arxiv.org/abs/2505.13732)
*Etienne Gauthier, Francis Bach, Michael I. Jordan*

**主要类别:** stat.ML

**概要:** 提出了一种新的反向一致预测方法，它能灵活控制预测集大小并保证覆盖率，尤其适用于医疗诊断等场景。


<details>
  <summary>更多</summary>
  
**动机:** 标准一致预测方法固定覆盖率而允许预测集大小变化，本研究旨在提供更灵活的预测集大小控制。

**方法:** 提出了反向一致预测方法，该方法通过观测数据约束预测集大小的行为，并相应地调整覆盖率水平。

**结果:** 该方法在理论上和实证上都得到了验证，证明其在保持可计算覆盖保证的同时，还能确保预测集大小的可解释性和良好控制。

**结论:** 提出的方法在保持可解释性和对预测集大小的良好控制的同时，确保了可计算的覆盖保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Backward+Conformal+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13732&send_immediately=true&force_search=false)

**原文摘要:** We introduce $\textit{Backward Conformal Prediction}$, a method that
guarantees conformal coverage while providing flexible control over the size of
prediction sets. Unlike standard conformal prediction, which fixes the coverage
level and allows the conformal set size to vary, our approach defines a rule
that constrains how prediction set sizes behave based on the observed data, and
adapts the coverage level accordingly. Our method builds on two key
foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity
using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm
test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 -
\mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any
data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out
estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage
$\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the
theoretical guarantees remain computable in practice. This approach is
particularly useful in applications where large prediction sets are impractical
such as medical diagnosis. We provide theoretical results and empirical
evidence supporting the validity of our method, demonstrating that it maintains
computable coverage guarantees while ensuring interpretable, well-controlled
prediction set sizes.

</details>


### [222] [Graphon Mixtures](https://arxiv.org/abs/2505.13864)
*Sevvandi Kandanaarachchi, Cheng Soon Ong*

**主要类别:** stat.ML

**概要:** 提出一种新的生成模型来同时捕捉社会网络中的hub和密集结构，并通过新的稀疏图条件识别hub，理论上证明可以估计hub的归一化度数和稀疏组件对应的graphon，实验验证了方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的社会网络模型难以同时有效捕捉hub和密集结构。

**方法:** 提出了一种基于graphon混合的生成模型，并引入max-degree条件用于识别hub。

**结果:** 能够估计hub的归一化度数和稀疏组件对应的graphon，并在合成数据、引用图和社会网络上验证了方法的有效性。

**结论:** 所提出的模型可以有效地同时捕捉社会网络中的hub和密集结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graphon+Mixtures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13864，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13864&send_immediately=true&force_search=false)

**原文摘要:** Social networks have a small number of large hubs, and a large number of
small dense communities. We propose a generative model that captures both hub
and dense structures. Based on recent results about graphons on line graphs,
our model is a graphon mixture, enabling us to generate sequences of graphs
where each graph is a combination of sparse and dense graphs. We propose a new
condition on sparse graphs (the max-degree), which enables us to identify hubs.
We show theoretically that we can estimate the normalized degree of the hubs,
as well as estimate the graphon corresponding to sparse components of graph
mixtures. We illustrate our approach on synthetic data, citation graphs, and
social networks, showing the benefits of explicitly modeling sparse graphs.

</details>


### [223] [An Asymptotic Equation Linking WAIC and WBIC in Singular Models](https://arxiv.org/abs/2505.13902)
*Naoki Hayashi, Takuro Kutsuna, Sawa Takamuku*

**主要类别:** stat.ML

**概要:** This paper theoretically derives an asymptotic equation linking WAIC and WBIC, providing insights into their structural relationship and asymptotic behavior in singular learning theory.


<details>
  <summary>更多</summary>
  
**动机:** To address the limitations of conventional criteria in singular models where normal approximations for likelihood and posterior break down.

**方法:** Theoretical derivation of an asymptotic equation linking WAIC and WBIC despite their dependence on different posteriors.

**结果:** An asymptotically unbiased expression of WAIC in terms of the posterior distribution used for WBIC is obtained.

**结论:** This theoretical contribution enhances understanding of the structural relationship between WAIC and WBIC and provides a basis for improving computational efficiency in model selection for singular models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Asymptotic+Equation+Linking+WAIC+and+WBIC+in+Singular+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13902，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13902&send_immediately=true&force_search=false)

**原文摘要:** In statistical learning, models are classified as regular or singular
depending on whether the mapping from parameters to probability distributions
is injective. Most models with hierarchical structures or latent variables are
singular, for which conventional criteria such as the Akaike Information
Criterion and the Bayesian Information Criterion are inapplicable due to the
breakdown of normal approximations for the likelihood and posterior. To address
this, the Widely Applicable Information Criterion (WAIC) and the Widely
Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC
and WBIC are computed using posterior distributions at different temperature
settings, separate posterior sampling is generally required. In this paper, we
theoretically derive an asymptotic equation that links WAIC and WBIC, despite
their dependence on different posteriors. This equation yields an
asymptotically unbiased expression of WAIC in terms of the posterior
distribution used for WBIC. The result clarifies the structural relationship
between these criteria within the framework of singular learning theory, and
deepens understanding of their asymptotic behavior. This theoretical
contribution provides a foundation for future developments in the computational
efficiency of model selection in singular models.

</details>


### [224] [A Probabilistic Perspective on Model Collapse](https://arxiv.org/abs/2505.13947)
*Shirong Xu, Hengzhi He, Guang Cheng*

**主要类别:** stat.ML

**概要:** This paper investigates model collapse during language model training from a probabilistic perspective, providing conditions to prevent it and showing that increasing sample size is necessary.


<details>
  <summary>更多</summary>
  
**动机:** To understand the underlying mechanisms driving model collapse and how it can be mitigated.

**方法:** Investigating recursive parametric model training from a probabilistic perspective.

**结果:** Under mild conditions, progressively increasing the sample size at each training step is necessary to prevent model collapse. The required growth rate follows a superlinear pattern when the estimation is unbiased, and needs to be accelerated further in the presence of substantial estimation bias.

**结论:** Progressively increasing the sample size at each training step is necessary to prevent model collapse.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Probabilistic+Perspective+on+Model+Collapse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.13947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.13947&send_immediately=true&force_search=false)

**原文摘要:** In recent years, model collapse has become a critical issue in language model
training, making it essential to understand the underlying mechanisms driving
this phenomenon. In this paper, we investigate recursive parametric model
training from a probabilistic perspective, aiming to characterize the
conditions under which model collapse occurs and, crucially, how it can be
mitigated. We conceptualize the recursive training process as a random walk of
the model estimate, highlighting how the sample size influences the step size
and how the estimation procedure determines the direction and potential bias of
the random walk. Under mild conditions, we rigorously show that progressively
increasing the sample size at each training step is necessary to prevent model
collapse. In particular, when the estimation is unbiased, the required growth
rate follows a superlinear pattern. This rate needs to be accelerated even
further in the presence of substantial estimation bias. Building on this
probabilistic framework, we also investigate the probability that recursive
training on synthetic data yields models that outperform those trained solely
on real data. Moreover, we extend these results to general parametric model
family in an asymptotic regime. Finally, we validate our theoretical results
through extensive simulations and a real-world dataset.

</details>


### [225] [Computational Efficiency under Covariate Shift in Kernel Ridge Regression](https://arxiv.org/abs/2505.14083)
*Andrea Della Vecchia, Arnaud Mavakala Watusadisi, Ernesto De Vito, Lorenzo Rosasco*

**主要类别:** stat.ML

**概要:** This paper discusses how to handle the covariate shift problem in nonparametric regression using random projections within RKHSs, balancing computational efficiency and statistical accuracy.


<details>
  <summary>更多</summary>
  
**动机:** To deal with the computational limitations of kernel methods under covariate shift, which affects their scalability to large datasets.

**方法:** Using random projections in the RKHS to form a hypothesis space that consists of a random subspace.

**结果:** Achieving significant computational savings without reducing learning performance despite the presence of covariate shift.

**结论:** Random projections can effectively balance computational efficiency and statistical accuracy in nonparametric regression under covariate shift.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computational+Efficiency+under+Covariate+Shift+in+Kernel+Ridge+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14083&send_immediately=true&force_search=false)

**原文摘要:** This paper addresses the covariate shift problem in the context of
nonparametric regression within reproducing kernel Hilbert spaces (RKHSs).
Covariate shift arises in supervised learning when the input distributions of
the training and test data differ, presenting additional challenges for
learning. Although kernel methods have optimal statistical properties, their
high computational demands in terms of time and, particularly, memory, limit
their scalability to large datasets. To address this limitation, the main focus
of this paper is to explore the trade-off between computational efficiency and
statistical accuracy under covariate shift. We investigate the use of random
projections where the hypothesis space consists of a random subspace within a
given RKHS. Our results show that, even in the presence of covariate shift,
significant computational savings can be achieved without compromising learning
performance.

</details>


### [226] [High-dimensional Nonparametric Contextual Bandit Problem](https://arxiv.org/abs/2505.14102)
*Shogo Iwazaki, Junpei Komiyama, Masaaki Imaizumi*

**主要类别:** stat.ML

**概要:** 研究了具有大特征空间的核上下文强盗问题。提出了一种新的随机上下文分布假设，并证明即使在维度数增加到样本数时，也能实现无悔学习。还分析了宽容遗憾，并得出了宽容遗憾率关于Δ的结果。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在应用于高斯核时，对于Ω(logT)特征维度，累计遗憾界为O(T)，因此需要新的方法来解决这一问题。

**方法:** 引入了随机上下文分布假设，并对宽容遗憾进行了分析。

**结果:** 提出了新的算法，即使在特征维度增加到样本数量时，也可以实现无悔学习。并且分析了宽容遗憾率关于Δ的结果。

**结论:** 本研究提供了一种新的方法来处理具有大特征空间的核上下文强盗问题，并且对宽容遗憾进行了分析，这为在线广告和推荐系统等决策制定场景提供了重要的理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High-dimensional+Nonparametric+Contextual+Bandit+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14102&send_immediately=true&force_search=false)

**原文摘要:** We consider the kernelized contextual bandit problem with a large feature
space. This problem involves $K$ arms, and the goal of the forecaster is to
maximize the cumulative rewards through learning the relationship between the
contexts and the rewards. It serves as a general framework for various
decision-making scenarios, such as personalized online advertising and
recommendation systems. Kernelized contextual bandits generalize the linear
contextual bandit problem and offers a greater modeling flexibility. Existing
methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when
we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce
stochastic assumptions on the context distribution and show that no-regret
learning is achievable even when the number of dimensions grows up to the
number of samples. Furthermore, we analyze lenient regret, which allows a
per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret
in terms of $\Delta$.

</details>


### [227] [Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals](https://arxiv.org/abs/2505.14164)
*Marcel Arpogaus, Thomas Kneib, Thomas Nagler, David Rügamer*

**主要类别:** stat.ML

**概要:** 结合多元条件变换模型(MCTM)与归一化流(NF)，提出一种新的密度回归模型，该模型在模拟和真实数据上展示了其灵活性和解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现有统计方法在建模复杂多变量概率分布时不够灵活，而深度学习模型则难以解释。

**方法:** 将MCTM与自回归NF结合，首先利用MCTM的透明性建模可解释特征效应，然后用基于神经网络的NF技术处理联合数据分布中的复杂非线性关系。

**结果:** 该方法在多种数值实验中展现了其灵活性，并在模拟和真实数据上与其他模型进行了比较。

**结论:** 所提出的模型能够有效地结合解释性和灵活性，适用于复杂的多变量数据分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Bernstein+Normalizing+Flows+for+Flexible+Multivariate+Density+Regression+with+Interpretable+Marginals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14164，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14164&send_immediately=true&force_search=false)

**原文摘要:** Density regression models allow a comprehensive understanding of data by
modeling the complete conditional probability distribution. While flexible
estimation approaches such as normalizing flows (NF) work particularly well in
multiple dimensions, interpreting the input-output relationship of such models
is often difficult, due to the black-box character of deep learning models. In
contrast, existing statistical methods for multivariate outcomes such as
multivariate conditional transformation models (MCTM) are restricted in
flexibility and are often not expressive enough to represent complex
multivariate probability distributions. In this paper, we combine MCTM with
state-of-the-art and autoregressive NF to leverage the transparency of MCTM for
modeling interpretable feature effects on the marginal distributions in the
first step and the flexibility of neural-network-based NF techniques to account
for complex and non-linear relationships in the joint data distribution. We
demonstrate our method's versatility in various numerical experiments and
compare it with MCTM and other NF models on both simulated and real-world data.

</details>


### [228] [From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling](https://arxiv.org/abs/2505.14177)
*Marien Renaud, Valentin De Bortoli, Arthur Leclaire, Nicolas Papadakis*

**主要类别:** stat.ML

**概要:** This paper proves the stability of the Unadjusted Langevin Algorithm (ULA) for non-convex potentials and derives the first proof of convergence of the Proximal Stochastic Gradient Langevin Algorithm (PSGLA) for such potentials.


<details>
  <summary>更多</summary>
  
**动机:** To address the problem of sampling distributions from non-convex potentials using the Unadjusted Langevin Algorithm (ULA) and Proximal Stochastic Gradient Langevin Algorithm (PSGLA).

**方法:** Prove the stability of the discrete-time ULA under certain assumptions and combine it with properties of the Moreau envelope to establish the convergence of PSGLA.

**结果:** The paper proves the stability of ULA and derives the first proof of convergence of PSGLA for non-convex potentials. Empirical validation shows that PSGLA converges faster than Stochastic Gradient Langevin Algorithm while maintaining restoration properties.

**结论:** This work provides theoretical guarantees for the use of PSGLA in non-convex potential sampling problems, which has applications in imaging inverse problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+stability+of+Langevin+diffusion+to+convergence+of+proximal+MCMC+for+non-log-concave+sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14177，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14177&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of sampling distributions stemming from non-convex
potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of
the discrete-time ULA to drift approximations under the assumption that the
potential is strongly convex at infinity. In many context, e.g. imaging inverse
problems, potentials are non-convex and non-smooth. Proximal Stochastic
Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such
potentials. It combines the forward-backward optimization algorithm with a ULA
step. Our main stability result combined with properties of the Moreau envelope
allows us to derive the first proof of convergence of the PSGLA for non-convex
potentials. We empirically validate our methodology on synthetic data and in
the context of imaging inverse problems. In particular, we observe that PSGLA
exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm
for posterior sampling while preserving its restoration properties.

</details>


### [229] [A system identification approach to clustering vector autoregressive time series](https://arxiv.org/abs/2505.14421)
*Zuogong Yue, Xinyi Wang, Victor Solo*

**主要类别:** stat.ML

**概要:** This paper presents a new clustering algorithm called k-LMVAR for vector time series based on system identification approach, which performs well in simulations and scales computationally.


<details>
  <summary>更多</summary>
  
**动机:** To address the limitations of existing time series clustering methods that ignore autocorrelation patterns and rely on heuristics or domain knowledge.

**方法:** Deriving a clustering algorithm based on a mixture autoregressive model and developing a 'small-noise' limiting version called k-LMVAR.

**结果:** The algorithm performs well in comparative simulations and scales computationally.

**结论:** The proposed k-LMVAR algorithm provides a new approach for clustering vector time series based on their underlying autoregressive dynamics.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+system+identification+approach+to+clustering+vector+autoregressive+time+series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14421，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14421&send_immediately=true&force_search=false)

**原文摘要:** Clustering of time series based on their underlying dynamics is keeping
attracting researchers due to its impacts on assisting complex system
modelling. Most current time series clustering methods handle only scalar time
series, treat them as white noise, or rely on domain knowledge for high-quality
feature construction, where the autocorrelation pattern/feature is mostly
ignored. Instead of relying on heuristic feature/metric construction, the
system identification approach allows treating vector time series clustering by
explicitly considering their underlying autoregressive dynamics. We first
derive a clustering algorithm based on a mixture autoregressive model.
Unfortunately it turns out to have significant computational problems. We then
derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR
(Limiting Mixture Vector AutoRegression), that is computationally manageable.
We develop an associated BIC criterion for choosing the number of clusters and
model order. The algorithm performs very well in comparative simulations and
also scales well computationally.

</details>


### [230] [A simple estimator of the correlation kernel matrix of a determinantal point process](https://arxiv.org/abs/2505.14529)
*Christian Gouriéroux, Yang Lu*

**主要类别:** stat.ML

**概要:** 提出了一种确定性点过程核矩阵的闭式估计器，并证明了其一致性、渐近正态性和大偏差性质。


<details>
  <summary>更多</summary>
  
**动机:** 开发一种易于实现的确定性点过程核矩阵估计方法。

**方法:** 提出了一个用于确定性点过程核矩阵的闭式估计器。

**结果:** 证明了所提出的估计器的一致性、渐近正态性和大偏差性质。

**结论:** 该研究提供了一种有效的确定性点过程核矩阵估计方法，可用于学习算法的最大似然估计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+simple+estimator+of+the+correlation+kernel+matrix+of+a+determinantal+point+process，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14529&send_immediately=true&force_search=false)

**原文摘要:** The Determinantal Point Process (DPP) is a parameterized model for
multivariate binary variables, characterized by a correlation kernel matrix.
This paper proposes a closed form estimator of this kernel, which is
particularly easy to implement and can also be used as a starting value of
learning algorithms for maximum likelihood estimation. We prove the consistency
and asymptotic normality of our estimator, as well as its large deviation
properties.

</details>


### [231] [High-Dimensional Analysis of Bootstrap Ensemble Classifiers](https://arxiv.org/abs/2505.14587)
*Hamza Cherkaoui, Malik Tiomoko, Mohamed El Amine Seddik, Cosme Louart, Ekkehard Schnoor, Balazs Kegl*

**主要类别:** stat.ML

**概要:** This paper theoretically analyzes bootstrap techniques for LSSVM ensembles, offering insights into high-dimensional settings and proposing strategies to optimize performance.


<details>
  <summary>更多</summary>
  
**动机:** To understand the impact of bootstrap methods in high-dimensional settings and improve the performance of LSSVM ensembles.

**方法:** Theoretical analysis using tools from Random Matrix Theory and empirical experiments on synthetic and real-world datasets.

**结果:** Insights into the use of bootstrap methods in high-dimensional settings and proposed strategies for optimizing the number of subsets and regularization parameter.

**结论:** The study enhances understanding of bootstrap methods' impact and provides practical strategies to maximize LSSVM performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High-Dimensional+Analysis+of+Bootstrap+Ensemble+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14587，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14587&send_immediately=true&force_search=false)

**原文摘要:** Bootstrap methods have long been a cornerstone of ensemble learning in
machine learning. This paper presents a theoretical analysis of bootstrap
techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble
in the context of large and growing sample sizes and feature dimensionalities.
Leveraging tools from Random Matrix Theory, we investigate the performance of
this classifier that aggregates decision functions from multiple weak
classifiers, each trained on different subsets of the data. We provide insights
into the use of bootstrap methods in high-dimensional settings, enhancing our
understanding of their impact. Based on these findings, we propose strategies
to select the number of subsets and the regularization parameter that maximize
the performance of the LSSVM. Empirical experiments on synthetic and real-world
datasets validate our theoretical results.

</details>
