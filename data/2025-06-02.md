<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 141]
- [cs.AI](#cs.AI) [总数: 36]
- [stat.ML](#stat.ML) [总数: 15]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control](https://arxiv.org/abs/2505.23857)
*Wuhao Wang, Zhiyong Chen*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种名为DATD3的新强化学习方法，用于处理部分可观测环境下的决策问题，并且其性能优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现实应用中，强化学习经常涉及输出反馈设置，代理只能接收部分状态信息。为解决这一问题，作者提出了新的方法。

**方法:** 作者提出了Output-Feedback Markov Decision Process (OPMDP)框架，并在此基础上引入了Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient (DATD3)算法。

**结果:** 实验表明，在连续控制任务中，DATD3在部分和完全可观测条件下均优于现有基于记忆和递归的基准方法。

**结论:** 论文提出了一种新的在部分可观测环境下进行强化学习的方法DATD3，该方法表现优于现有的记忆型和递归基准方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DATD3%3A+Depthwise+Attention+Twin+Delayed+Deep+Deterministic+Policy+Gradient+For+Model+Free+Reinforcement+Learning+Under+Output+Feedback+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23857&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning in real-world applications often involves
output-feedback settings, where the agent receives only partial state
information. To address this challenge, we propose the Output-Feedback Markov
Decision Process (OPMDP), which extends the standard MDP formulation to
accommodate decision-making based on observation histories. Building on this
framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic
Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise
separable convolution and multi-head attention to encode historical
observations. DATD3 maintains policy expressiveness while avoiding the
instability of recurrent models. Extensive experiments on continuous control
tasks demonstrate that DATD3 outperforms existing memory-based and recurrent
baselines under both partial and full observability.

</details>


### [2] [Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration](https://arxiv.org/abs/2505.23859)
*Wenju Sun, Qingyong Li, Wen Wang, Yang Liu, Yangli-ao Geng, Boyang Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种高效的多任务模型合并方法LOT Merging，通过逐层最小化特征漂移，在减少性能损失的同时提高了模型合并效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多任务模型合并方法在参数级别或任务损失级别上进行优化，但存在性能差距或计算成本高的问题。作者发现性能下降与特征漂移密切相关，从而提出了新的方法。

**方法:** 提出Layer-wise Optimal Task Vector Merging (LOT Merging)，一种通过逐层最小化任务特定专家模型与统一模型之间特征漂移来实现模型合并的方法。

**结果:** LOT Merging 在多个基准测试中比现有最先进方法提升了高达4.4%（ViT-B/32），且无需昂贵的二次训练过程。

**结论:** LOT Merging通过逐层最小化特征漂移，有效减少了模型合并过程中的性能损失，并在多个视觉和视觉-语言基准测试中显著优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Minimizing+Feature+Drift+in+Model+Merging%3A+Layer-wise+Task+Vector+Fusion+for+Adaptive+Knowledge+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23859&send_immediately=true&force_search=false)

**原文摘要:** Multi-task model merging aims to consolidate knowledge from multiple
fine-tuned task-specific experts into a unified model while minimizing
performance degradation. Existing methods primarily approach this by minimizing
differences between task-specific experts and the unified model, either from a
parameter-level or a task-loss perspective. However, parameter-level methods
exhibit a significant performance gap compared to the upper bound, while
task-loss approaches entail costly secondary training procedures. In contrast,
we observe that performance degradation closely correlates with feature drift,
i.e., differences in feature representations of the same sample caused by model
merging. Motivated by this observation, we propose Layer-wise Optimal Task
Vector Merging (LOT Merging), a technique that explicitly minimizes feature
drift between task-specific experts and the unified model in a layer-by-layer
manner. LOT Merging can be formulated as a convex quadratic optimization
problem, enabling us to analytically derive closed-form solutions for the
parameters of linear and normalization layers. Consequently, LOT Merging
achieves efficient model consolidation through basic matrix operations.
Extensive experiments across vision and vision-language benchmarks demonstrate
that LOT Merging significantly outperforms baseline methods, achieving
improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.

</details>


### [3] [BiBLDR: Bidirectional Behavior Learning for Drug Repositioning](https://arxiv.org/abs/2505.23861)
*Renye Zhang, Mengyun Yang, Qichang Zhao, Jianxin Wang*

**主要类别:** cs.LG

**AI概要:** BiBLDR是一种创新性的药物重定位框架，它通过双向行为学习策略来提升药物与疾病之间交互关系的预测效果，特别是在冷启动场景下表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于图的药物重定位方法在面对新药时难以进行有效推断，因为缺乏与疾病的关联信息。因此需要提出一种新的方法来解决这一问题，提高药物重定位的效果。

**方法:** BiBLDR将药物重定位重新定义为行为序列学习任务，以捕捉药物-疾病相互作用模式。首先基于药物和疾病方面构建双向行为序列；随后采用两阶段策略进行药物重定位：第一阶段构建原型空间来表征药物和疾病的表示属性，第二阶段利用这些优化的原型和双向行为序列数据预测潜在的药物-疾病关联。

**结果:** BiBLDR在基准数据集上的实验结果显示其性能达到了最先进的水平，在冷启动场景下的表现也显著优于之前的方法。

**结论:** BiBLDR通过双向行为学习策略，能够更稳健和精确地从双向行为序列中捕捉药物与疾病的交互关系。实验结果表明，该方法在基准数据集上达到了最先进的性能，并且在冷启动场景下明显优于以往的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BiBLDR%3A+Bidirectional+Behavior+Learning+for+Drug+Repositioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23861&send_immediately=true&force_search=false)

**原文摘要:** Drug repositioning aims to identify potential new indications for existing
drugs to reduce the time and financial costs associated with developing new
drugs. Most existing deep learning-based drug repositioning methods
predominantly utilize graph-based representations. However, graph-based drug
repositioning methods struggle to perform effective inference in cold-start
scenarios involving novel drugs because of the lack of association information
with the diseases. Unlike traditional graph-based approaches, we propose a
bidirectional behavior learning strategy for drug repositioning, known as
BiBLDR. This innovative framework redefines drug repositioning as a behavior
sequential learning task to capture drug-disease interaction patterns. First,
we construct bidirectional behavioral sequences based on drug and disease
sides. The consideration of bidirectional information ensures a more meticulous
and rigorous characterization of the behavioral sequences. Subsequently, we
propose a two-stage strategy for drug repositioning. In the first stage, we
construct prototype spaces to characterize the representational attributes of
drugs and diseases. In the second stage, these refined prototypes and
bidirectional behavior sequence data are leveraged to predict potential
drug-disease associations. Based on this learning approach, the model can more
robustly and precisely capture the interactive relationships between drug and
disease features from bidirectional behavioral sequences. Extensive experiments
demonstrate that our method achieves state-of-the-art performance on benchmark
datasets. Meanwhile, BiBLDR demonstrates significantly superior performance
compared to previous methods in cold-start scenarios. Our code is published in
https://github.com/Renyeeah/BiBLDR.

</details>


### [4] [Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting](https://arxiv.org/abs/2505.23863)
*Chang Liu, Bohao Zhao, Jingtao Ding, Huandong Wang, Yong Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为PhyxMamba的新框架，该框架能够在短期观测条件下实现对混沌系统的长期预测。


<details>
  <summary>更多</summary>
  
**动机:** 由于混沌系统对初始条件的敏感性和奇异吸引子的复杂几何结构，从短期观测中长期预测混沌系统是一个重要且未被充分研究的挑战。现有方法难以在长周期内保持预测稳定性和动态一致性。

**方法:** PhyxMamba结合了基于Mamba的状态空间模型与物理信息原则，并利用时间延迟嵌入重构吸引子流形，通过生成式训练方案进行预测。

**结果:** PhyxMamba在多样化的模拟和真实世界混沌系统评估中表现出卓越的长期预测能力，并能准确捕捉关键的动力学不变量。

**结论:** PhyxMamba能够在观测数据有限的情况下可靠地预测混沌系统，为气候科学、神经科学和流行病学等领域提供了新的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mamba+Integrated+with+Physics+Principles+Masters+Long-term+Chaotic+System+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23863，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23863&send_immediately=true&force_search=false)

**原文摘要:** Long-term forecasting of chaotic systems from short-term observations remains
a fundamental and underexplored challenge due to the intrinsic sensitivity to
initial conditions and the complex geometry of strange attractors. Existing
approaches often rely on long-term training data or focus on short-term
sequence correlations, struggling to maintain predictive stability and
dynamical coherence over extended horizons. We propose PhyxMamba, a novel
framework that integrates a Mamba-based state-space model with physics-informed
principles to capture the underlying dynamics of chaotic systems. By
reconstructing the attractor manifold from brief observations using time-delay
embeddings, PhyxMamba extracts global dynamical features essential for accurate
forecasting. Our generative training scheme enables Mamba to replicate the
physical process, augmented by multi-token prediction and attractor geometry
regularization for physical constraints, enhancing prediction accuracy and
preserving key statistical invariants. Extensive evaluations on diverse
simulated and real-world chaotic systems demonstrate that PhyxMamba delivers
superior long-term forecasting and faithfully captures essential dynamical
invariants from short-term data. This framework opens new avenues for reliably
predicting chaotic systems under observation-scarce conditions, with broad
implications across climate science, neuroscience, epidemiology, and beyond.
Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.

</details>


### [5] [Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections](https://arxiv.org/abs/2505.23864)
*Wei Zhuo, Zhaohuan Zhan, Ziduo Yang, Han Yu*

**主要类别:** cs.LG

**AI概要:** 本文提出了 FedAux，一种基于辅助投影向量的联邦学习框架，用于处理图结构数据中的非 IID 挑战，通过服务器端基于 APV 相似度的参数混合策略实现了高效的跨客户端知识迁移与个性化建模。


<details>
  <summary>更多</summary>
  
**动机:** 图结构数据上的联邦学习面临非独立同分布（non-IID）挑战，尤其是在各客户端持有不同子图的情况下。为解决此问题，本文提出 FedAux 方法以实现在不共享原始数据或节点嵌入的前提下，有效对齐、比较和聚合异构分布的本地模型。

**方法:** 提出了一种名为 FedAux 的个性化子图联邦学习框架，通过辅助投影向量（APV）对节点嵌入进行一维空间的可微投影，并利用软排序和轻量级一维卷积优化嵌入，实现客户端间的知识迁移与个性化模型训练。

**结果:** 实验表明 FedAux 显著提升了联邦学习在图结构数据上的准确性和个性化性能，同时具备理论上的收敛性保障。

**结论:** FedAux 在多样化的图基准测试中显著优于现有基线方法，在准确性和个性化性能方面均有提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Subgraph+Federated+Learning+with+Differentiable+Auxiliary+Projections，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23864，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23864&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) on graph-structured data typically faces non-IID
challenges, particularly in scenarios where each client holds a distinct
subgraph sampled from a global graph. In this paper, we introduce Federated
learning with Auxiliary projections (FedAux), a personalized subgraph FL
framework that learns to align, compare, and aggregate heterogeneously
distributed local models without sharing raw data or node embeddings. In
FedAux, each client jointly trains (i) a local GNN and (ii) a learnable
auxiliary projection vector (APV) that differentiably projects node embeddings
onto a 1D space. A soft-sorting operation followed by a lightweight 1D
convolution refines these embeddings in the ordered space, enabling the APV to
effectively capture client-specific information. After local training, these
APVs serve as compact signatures that the server uses to compute inter-client
similarities and perform similarity-weighted parameter mixing, yielding
personalized models while preserving cross-client knowledge transfer. Moreover,
we provide rigorous theoretical analysis to establish the convergence and
rationality of our design. Empirical evaluations across diverse graph
benchmarks demonstrate that FedAux substantially outperforms existing baselines
in both accuracy and personalization performance.

</details>


### [6] [Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration](https://arxiv.org/abs/2505.23865)
*Emanuele Masiero, Vito Trianni, Giuseppe Vizzari, Dimitri Ognibene*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于精准农业中作物监测的不确定性感知自主探索方法，通过使用一种新的两阶段深度学习框架和三种代理架构的比较，展示了如何提高探索效率。


<details>
  <summary>更多</summary>
  
**动机:** 精准农业需要高效的自主系统进行作物监测，而代理必须在尽量减少资源消耗的同时探索大规模环境。

**方法:** 提出了一种两阶段深度学习框架，其中预训练LSTM作为信念模型，并引入了POV可见性掩码来保持马尔可夫性质和避免重复访问。通过比较三种代理架构（未训练的IG代理、使用CNN的DQN代理以及Double-CNN DQN代理）在20x20地图上的表现，评估了不同方法的探索效率。

**结果:** 实验结果表明，尽管未训练的IG代理结构简单，但其表现良好；当包含POV掩码时，DQN代理的表现与之相当；而Double-CNN DQN代理在更大的环境中始终表现出更高的探索效率。

**结论:** 论文得出，基于不确定性意识的策略在熵、信念状态和可见性跟踪的帮助下能够实现稳健且可扩展的探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Combining+Deep+Architectures+for+Information+Gain+estimation+and+Reinforcement+Learning+for+multiagent+field+exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23865，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23865&send_immediately=true&force_search=false)

**原文摘要:** Precision agriculture requires efficient autonomous systems for crop
monitoring, where agents must explore large-scale environments while minimizing
resource consumption. This work addresses the problem as an active exploration
task in a grid environment representing an agricultural field. Each cell may
contain targets (e.g., damaged crops) observable from nine predefined points of
view (POVs). Agents must infer the number of targets per cell using partial,
sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as
a belief model, updating a probabilistic map of the environment and its
associated entropy, which defines the expected information gain (IG). This
allows agents to prioritize informative regions. A key contribution is the
inclusion of a POV visibility mask in the input, preserving the Markov property
under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent
selecting actions to maximize entropy reduction; a DQN agent using CNNs over
local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent
with wider spatial context. Simulations on 20x20 maps showed that the untrained
agent performs well despite its simplicity. The DQN agent matches this
performance when the POV mask is included, while the Double-CNN agent
consistently achieves superior exploration efficiency, especially in larger
environments.
  Results show that uncertainty-aware policies leveraging entropy, belief
states, and visibility tracking lead to robust and scalable exploration. Future
work includes curriculum learning, multi-agent cooperation with shared rewards,
transformer-based models, and intrinsic motivation mechanisms to further
enhance learning efficiency and policy generalization.

</details>


### [7] [Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization](https://arxiv.org/abs/2505.23866)
*Chengli Tan, Yubo Zhou, Haishan Ye, Guang Dai, Junmin Liu, Zengjie Song, Jiangshe Zhang, Zixiang Zhao, Yunda Hao, Yong Xu*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的模型训练方法CSAM，用于减少深度神经网络的过度自信问题。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络倾向于过度自信，这可能导致灾难性的后果。

**方法:** 提出了CSAM，并通过广泛的实验评估了其性能。

**结果:** 实验证明，使用CSAM可以降低校准误差。

**结论:** CSAM在减少校准误差方面优于其他方法，包括SAM。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Understanding+The+Calibration+Benefits+of+Sharpness-Aware+Minimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23866&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks have been increasingly used in safety-critical
applications such as medical diagnosis and autonomous driving. However, many
studies suggest that they are prone to being poorly calibrated and have a
propensity for overconfidence, which may have disastrous consequences. In this
paper, unlike standard training such as stochastic gradient descent, we show
that the recently proposed sharpness-aware minimization (SAM) counteracts this
tendency towards overconfidence. The theoretical analysis suggests that SAM
allows us to learn models that are already well-calibrated by implicitly
maximizing the entropy of the predictive distribution. Inspired by this
finding, we further propose a variant of SAM, coined as CSAM, to ameliorate
model calibration. Extensive experiments on various datasets, including
ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.
Meanwhile, CSAM performs even better than SAM and consistently achieves lower
calibration error than other approaches

</details>


### [8] [Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert](https://arxiv.org/abs/2505.23868)
*Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou. Libo Qin, Wenhong Tian*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的噪声鲁棒自适应方法LoPE，仅通过生成的噪声数据增强模型鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的噪声处理方法要么依赖繁琐的数据预处理，要么采用容易产生误差积累的模型架构修改。

**方法:** LoPE通过不对称的LoRA配置集成一个专门的中毒专家，在两阶段范式中进行噪声注入，并在推理过程中选择性地屏蔽该专家。

**结果:** 实验表明，LoPE通过低成本的噪声注入实现了强大的性能和鲁棒性。

**结论:** LoPE是一种无需数据清洗的低噪声注入方法，可以提高模型的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Noise-Robustness+Through+Noise%3A+Asymmetric+LoRA+Adaption+with+Poisoning+Expert，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23868，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23868&send_immediately=true&force_search=false)

**原文摘要:** Current parameter-efficient fine-tuning methods for adapting pre-trained
language models to downstream tasks are susceptible to interference from noisy
data. Conventional noise-handling approaches either rely on laborious data
pre-processing or employ model architecture modifications prone to error
accumulation. In contrast to existing noise-process paradigms, we propose a
noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a
novel framework that enhances model robustness to noise only with generated
noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE
strategically integrates a dedicated poisoning expert in an asymmetric LoRA
configuration. Through a two-stage paradigm, LoPE performs noise injection on
the poisoning expert during fine-tuning to enhance its noise discrimination and
processing ability. During inference, we selectively mask the dedicated
poisoning expert to leverage purified knowledge acquired by normal experts for
noise-robust output. Extensive experiments demonstrate that LoPE achieves
strong performance and robustness purely through the low-cost noise injection,
which completely eliminates the requirement of data cleaning.

</details>


### [9] [MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection](https://arxiv.org/abs/2505.23870)
*Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania*

**主要类别:** cs.LG

**AI概要:** MaCP是一种用于微调大型模型的新颖有效方法，通过将权重变化投影到离散余弦空间并选取关键频率分量，实现了更高的准确性和更低的资源需求。


<details>
  <summary>更多</summary>
  
**动机:** 寻找一种参数和内存需求最小化的方法来高效微调大基础模型。

**方法:** MaCP利用余弦投影的优良能量集中和去相关特性，将低秩适应的权重变化投影到离散余弦空间，并选择每个分区最关键频率分量。

**结果:** MaCP在广泛的任务中均表现出色，包括自然语言理解和生成、文本摘要、图像分类和视频理解等，具有更高的准确性、显著降低的计算复杂性和更低的内存需求。

**结论:** MaCP是一种高效且准确的微调方法，适用于各种单模态和多模态任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MaCP%3A+Minimal+yet+Mighty+Adaptation+via+Hierarchical+Cosine+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23870，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23870&send_immediately=true&force_search=false)

**原文摘要:** We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine
Projection, that achieves exceptional performance while requiring minimal
parameters and memory for fine-tuning large foundation models. Its general idea
is to exploit the superior energy compaction and decorrelation properties of
cosine projection to improve both model efficiency and accuracy. Specifically,
it projects the weight change from the low-rank adaptation into the discrete
cosine space. Then, the weight change is partitioned over different levels of
the discrete cosine spectrum, and each partition's most critical frequency
components are selected. Extensive experiments demonstrate the effectiveness of
MaCP across a wide range of single-modality tasks, including natural language
understanding, natural language generation, text summarization, as well as
multi-modality tasks such as image classification and video understanding. MaCP
consistently delivers superior accuracy, significantly reduced computational
complexity, and lower memory requirements compared to existing alternatives.

</details>


### [10] [ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning](https://arxiv.org/abs/2505.23871)
*Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, Xiu Li*

**主要类别:** cs.LG

**AI概要:** ADG是一种利用扩散模型处理高维状态空间中数据损坏问题的新型方法，实验表明其在多种噪声设置下提高了离线RL的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的数据集容易受到噪音和错误的影响，这对应用离线强化学习构成了重大挑战，尤其是在高维状态空间和多个元素同时损坏的情况下。

**方法:** 提出了一种名为Ambient Diffusion-Guided Dataset Recovery (ADG)的方法，结合了Ambient Denoising Diffusion Probabilistic Models与标准DDPM，以识别并修复损坏的数据。

**结果:** 实验结果显示ADG在包括MuJoCo、Kitchen和Adroit在内的基准测试中有效减轻了损坏数据的影响，并达到了最先进的结果。

**结论:** ADG因其通用性可以无缝集成到任何离线RL算法中，显著提升了面对各种噪声环境时的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADG%3A+Ambient+Diffusion-Guided+Dataset+Recovery+for+Corruption-Robust+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23871&send_immediately=true&force_search=false)

**原文摘要:** Real-world datasets collected from sensors or human inputs are prone to noise
and errors, posing significant challenges for applying offline reinforcement
learning (RL). While existing methods have made progress in addressing
corrupted actions and rewards, they remain insufficient for handling corruption
in high-dimensional state spaces and for cases where multiple elements in the
dataset are corrupted simultaneously. Diffusion models, known for their strong
denoising capabilities, offer a promising direction for this problem-but their
tendency to overfit noisy samples limits their direct applicability. To
overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a
novel approach that pioneers the use of diffusion models to tackle data
corruption in offline RL. First, we introduce Ambient Denoising Diffusion
Probabilistic Models (DDPM) from approximated distributions, which enable
learning on partially corrupted datasets with theoretical guarantees. Second,
we use the noise-prediction property of Ambient DDPM to distinguish between
clean and corrupted data, and then use the clean subset to train a standard
DDPM. Third, we employ the trained standard DDPM to refine the previously
identified corrupted data, enhancing data quality for subsequent offline RL
training. A notable strength of ADG is its versatility-it can be seamlessly
integrated with any offline RL algorithm. Experiments on a range of benchmarks,
including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively
mitigates the impact of corrupted data and improves the robustness of offline
RL under various noise settings, achieving state-of-the-art results.

</details>


### [11] [A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants](https://arxiv.org/abs/2505.23875)
*Peter Samoaa, Marcus Vukojevic, Morteza Haghir Chehreghani, Antonio Longa*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的图回归数据集RelSC，并通过评估不同图神经网络架构的性能，强调了结构表示在图回归任务中的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的公共基准测试数据集主要偏向分子图和引用网络，缺乏多样性，阻碍了能够在同质和异质图结构间泛化的模型的发展。

**方法:** 作者开发了一个新的图回归数据集RelSC，包含两种变体RelSC-H和RelSC-M，并评估了一系列图神经网络架构在这两个变体上的性能差异。

**结果:** 实验结果显示，在同质与多关系设置之间存在一致的性能差异，突出了结构表示的重要性。

**结论:** RelSC的引入为图回归方法的进步提供了一个具有挑战性和多功能的基准，强调了结构表示的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Benchmark+Dataset+for+Graph+Regression+with+Homogeneous+and+Multi-Relational+Variants，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23875，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23875&send_immediately=true&force_search=false)

**原文摘要:** Graph-level regression underpins many real-world applications, yet public
benchmarks remain heavily skewed toward molecular graphs and citation networks.
This limited diversity hinders progress on models that must generalize across
both homogeneous and heterogeneous graph structures. We introduce RelSC, a new
graph-regression dataset built from program graphs that combine syntactic and
semantic information extracted from source code. Each graph is labelled with
the execution-time cost of the corresponding program, providing a continuous
target variable that differs markedly from those found in existing benchmarks.
RelSC is released in two complementary variants. RelSC-H supplies rich node
features under a single (homogeneous) edge type, while RelSC-M preserves the
original multi-relational structure, connecting nodes through multiple edge
types that encode distinct semantic relationships. Together, these variants let
researchers probe how representation choice influences model behaviour. We
evaluate a diverse set of graph neural network architectures on both variants
of RelSC. The results reveal consistent performance differences between the
homogeneous and multi-relational settings, emphasising the importance of
structural representation. These findings demonstrate RelSC's value as a
challenging and versatile benchmark for advancing graph regression methods.

</details>


### [12] [A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size](https://arxiv.org/abs/2505.23876)
*Polad Geidarov*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了利用分析公式预计算权重的多层感知机，发现其在训练速度和小数据集表现上显著优于随机初始化权重的方法。


<details>
  <summary>更多</summary>
  
**动机:** 动机在于探索提高神经网络训练效率和减少对大规模训练数据依赖性的方法。

**方法:** 该研究通过使用分析公式预先计算神经网络权重，并与随机初始化权重进行比较，在不同大小的MNIST训练数据集上进行了实验验证。

**结果:** 实验结果显示，具有预计算权重的多层感知机可以更快地训练，并且对训练数据集的减少更加鲁棒。

**结论:** 论文得出结论，具有预计算权重的多层感知机在训练速度和对训练数据集减少的鲁棒性方面优于随机初始化权重的神经网络。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+comparative+analysis+of+a+neural+network+with+calculated+weights+and+a+neural+network+with+random+generation+of+weights+based+on+the+training+dataset+size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23876，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23876&send_immediately=true&force_search=false)

**原文摘要:** The paper discusses the capabilities of multilayer perceptron neural networks
implementing metric recognition methods, for which the values of the weights
are calculated analytically by formulas. Comparative experiments in training a
neural network with pre-calculated weights and with random initialization of
weights on different sizes of the MNIST training dataset are carried out. The
results of the experiments show that a multilayer perceptron with
pre-calculated weights can be trained much faster and is much more robust to
the reduction of the training dataset.

</details>


### [13] [Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling](https://arxiv.org/abs/2505.23913)
*Gustavo Sutter Pessurno de Carvalho, Mohammed Abdulrahman, Hao Wang, Sriram Ganapathi Subramanian, Marc St-Aubin, Sharon O'Sullivan, Lawrence Wan, Luis Ricardez-Sandoval, Pascal Poupart, Agustinus Kristiadi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种无需代理拟合或获取函数优化的全新贝叶斯优化方法，通过预训练模型实现高效采样，显著提升了优化效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统的贝叶斯优化方法需要昂贵的再训练和获取函数优化步骤，因此需要一种更高效的方法。

**方法:** 使用预训练的深度生成模型直接从最优解点的后验分布中采样，从而实现汤普森抽样过程。

**结果:** 与基于高斯过程的贝叶斯优化相比，在墙钟时间方面实现了超过35倍的效率提升，并支持高效的并行和分布式贝叶斯优化。

**结论:** 本文提出了一种完全上下文内、零样本的贝叶斯优化解决方案，该方案不需要代理拟合或获取函数优化，并在实际基准测试中显示出高效性和成本效益。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Simplifying+Bayesian+Optimization+Via+In-Context+Direct+Optimum+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23913，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23913&send_immediately=true&force_search=false)

**原文摘要:** The optimization of expensive black-box functions is ubiquitous in science
and engineering. A common solution to this problem is Bayesian optimization
(BO), which is generally comprised of two components: (i) a surrogate model and
(ii) an acquisition function, which generally require expensive re-training and
optimization steps at each iteration, respectively. Although recent work
enabled in-context surrogate models that do not require re-training, virtually
all existing BO methods still require acquisition function maximization to
select the next observation, which introduces many knobs to tune, such as Monte
Carlo samplers and multi-start optimizers. In this work, we propose a
completely in-context, zero-shot solution for BO that does not require
surrogate fitting or acquisition function optimization. This is done by using a
pre-trained deep generative model to directly sample from the posterior over
the optimum point. We show that this process is equivalent to Thompson sampling
and demonstrate the capabilities and cost-effectiveness of our foundation model
on a suite of real-world benchmarks. We achieve an efficiency gain of more than
35x in terms of wall-clock time when compared with Gaussian process-based BO,
enabling efficient parallel and distributed BO, e.g., for high-throughput
optimization.

</details>


### [14] [Actor-Critic based Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2505.23878)
*Jing Ma, Chenhao Dang, Mingjie Liao*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的在线数据混合方法AC-ODM，该方法显著提高了大型语言模型预训练的效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了减少训练的碳足迹和财务成本，一些数据混合方法被提出，但这些方法未能随着训练动态演变。现有的在线数据混合（ODM）方法虽然解决了这一限制，但未考虑领域内互动。

**方法:** 开发了一种基于actor-critic的在线数据混合（AC-ODM）方法，利用辅助的actor-critic网络捕捉变化的领域权重，并通过奖励函数考虑领域内互动。

**结果:** 实验结果表明，AC-ODM-410M比ODM更快达到最佳验证困惑度，并在零样本MMLU基准测试中准确率提高了27.5%，在HumanEval基准测试中的pass@1表现也更好。

**结论:** AC-ODM方法通过捕获变化的领域权重并考虑领域内互动，显著提高了大型语言模型预训练的效率和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Actor-Critic+based+Online+Data+Mixing+For+Language+Model+Pre-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23878&send_immediately=true&force_search=false)

**原文摘要:** The coverage and composition of pretraining data significantly impacts the
generalization ability of Large Language Models (LLMs). To reduce the carbon
footprint and financial costs of training, some data mixing methods, which
applied the optimized domain weights of a small proxy model to train a larger
one, were proposed. However, these methods did not evolute with the training
dynamics. The existing online data mixing (ODM) method addressed this
limitation by applying the multi-armed bandit algorithm as data sampling
strategy. Yet, it did not consider the intra-domain interactions. In this
paper, we develop an actor-critic based online data mixing (AC-ODM) method,
which captures the varying domain weights by auxiliary actor-critic networks
and consider the intra-domain interactions with the reward function. While
constructing the dataset to pretrain a large target LLM, we directly apply the
actor, which is trained with a small proxy LLM as the environment, as the
sampling strategy. The transfer of sampling strategy can not only ensure the
efficiency of dynamical data mixing, but also expedite the convergence of
pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,
which invokes the sampling strategy obtained by a proxy LLM with 410M
parameters, reaching the optimal validation perplexity of ODM 71% faster, and
improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,
about 2.23x better on pass@1 of HumanEval benchmark.

</details>


### [15] [The Rich and the Simple: On the Implicit Bias of Adam and SGD](https://arxiv.org/abs/2505.24022)
*Bhavya Vasudeva, Jung Whan Lee, Vatsal Sharan, Mahdi Soltanolkotabi*

**主要类别:** cs.LG

**AI概要:** 本文研究了Adam和GD在训练神经网络时的隐式偏差差异，发现Adam比GD更能抵抗简单性偏差，能生成更复杂的模型并取得更好的泛化效果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Adam被广泛使用，但其隐式偏差与GD等优化算法的差异尚不明确，尤其是其对模型泛化能力的影响。

**方法:** 通过分析两层ReLU神经网络在合成数据上的二分类任务，研究Adam和GD的隐式偏差差异，并结合理论证明和实验证据进行验证。

**结果:** GD倾向于产生简单且次优的线性决策边界，而Adam能够生成更丰富的特征和非线性边界，更接近贝叶斯最优预测器，从而实现更高的测试准确率。

**结论:** Adam在训练神经网络时相比GD具有更优的泛化能力和抵抗简单性偏差的能力，尤其在分布偏移和虚假相关性存在时表现更好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Rich+and+the+Simple%3A+On+the+Implicit+Bias+of+Adam+and+SGD，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24022，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24022&send_immediately=true&force_search=false)

**原文摘要:** Adam is the de facto optimization algorithm for several deep learning
applications, but an understanding of its implicit bias and how it differs from
other algorithms, particularly standard first-order methods such as
(stochastic) gradient descent (GD), remains limited. In practice, neural
networks trained with SGD are known to exhibit simplicity bias -- a tendency to
find simple solutions. In contrast, we show that Adam is more resistant to such
simplicity bias. To demystify this phenomenon, in this paper, we investigate
the differences in the implicit biases of Adam and GD when training two-layer
ReLU neural networks on a binary classification task involving synthetic data
with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting
in a linear decision boundary with a suboptimal margin, whereas Adam leads to
much richer and more diverse features, producing a nonlinear boundary that is
closer to the Bayes' optimal predictor. This richer decision boundary also
allows Adam to achieve higher test accuracy both in-distribution and under
certain distribution shifts. We theoretically prove these results by analyzing
the population gradients. To corroborate our theoretical findings, we present
empirical results showing that this property of Adam leads to superior
generalization across datasets with spurious correlations where neural networks
trained with SGD are known to show simplicity bias and don't generalize well
under certain distributional shifts.

</details>


### [16] [CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data](https://arxiv.org/abs/2505.23879)
*Caio Cheohen, Vinnícius M. S. Gomes, Manuela L. da Silva*

**主要类别:** cs.LG

**AI概要:** This paper develops a deep learning model to predict the severity of  COVID-19 based on genetic and clinical data.


<details>
  <summary>更多</summary>
  
**动机:** The study aimed to develop a hybrid CNN-LSTM deep learning model to predict  COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients.

**方法:** A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling.

**结果:** The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance.

**结论:** The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CNN-LSTM+Hybrid+Model+for+AI-Driven+Prediction+of+COVID-19+Severity+from+Spike+Sequences+and+Clinical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23879，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23879&send_immediately=true&force_search=false)

**原文摘要:** The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need
for accurate prediction of disease severity to optimize healthcare resource
allocation and patient management. The spike protein, which facilitates viral
entry into host cells, exhibits high mutation rates, particularly in the
receptor-binding domain, influencing viral pathogenicity. Artificial
intelligence approaches, such as deep learning, offer promising solutions for
leveraging genomic and clinical data to predict disease outcomes. Objective:
This study aimed to develop a hybrid CNN-LSTM deep learning model to predict
COVID-19 severity using spike protein sequences and associated clinical
metadata from South American patients. Methods: We retrieved 9,570 spike
protein sequences from the GISAID database, of which 3,467 met inclusion
criteria after standardization. The dataset included 2,313 severe and 1,154
mild cases. A feature engineering pipeline extracted features from sequences,
while demographic and clinical variables were one-hot encoded. A hybrid
CNN-LSTM architecture was trained, combining CNN layers for local pattern
extraction and an LSTM layer for long-term dependency modeling. Results: The
model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,
and recall of 82.85%, demonstrating robust classification performance. Training
stabilized at 85% accuracy with minimal overfitting. The most prevalent
lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional
epidemiological trends, suggesting potential associations between viral
genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model
effectively predicted COVID-19 severity using spike protein sequences and
clinical data, highlighting the utility of AI in genomic surveillance and
precision public health. Despite limitations, this approach provides a
framework for early severity prediction in future outbreaks.

</details>


### [17] [Characterising the Inductive Biases of Neural Networks on Boolean Data](https://arxiv.org/abs/2505.24060)
*Chris Mingard, Lukas Seier, Niclas Göring, Andrei-Vlad Badelita, Charles London, Ard Louis*

**主要类别:** cs.LG

**AI概要:** 本文研究了深度神经网络的归纳偏置和特征学习如何影响其泛化能力，并通过一个可解析的案例提供了完整的解释。


<details>
  <summary>更多</summary>
  
**动机:** 现有的工作只能部分解释过参数化的深度神经网络为何能够很好地泛化，例如基于NTK的任务-模型对齐解释忽略了特征学习。

**方法:** 利用深度-2离散全连接网络与析取范式（DNF）公式之间的对应关系，并在布尔函数上进行训练以研究模型的学习动态和可解释特征的出现。

**结果:** 在蒙特卡洛学习算法下，模型展示了可预测的训练动态和可解释特征的出现，提供了一个从归纳先验到泛化能力的端到端解析案例研究。

**结论:** 论文得出结论，通过深度学习模型的归纳先验和特征形成可以详细解释其泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Characterising+the+Inductive+Biases+of+Neural+Networks+on+Boolean+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24060&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks are renowned for their ability to generalise well across
diverse tasks, even when heavily overparameterized. Existing works offer only
partial explanations (for example, the NTK-based task-model alignment
explanation neglects feature learning). Here, we provide an end-to-end,
analytically tractable case study that links a network's inductive prior, its
training dynamics including feature learning, and its eventual generalisation.
Specifically, we exploit the one-to-one correspondence between depth-2 discrete
fully connected networks and disjunctive normal form (DNF) formulas by training
on Boolean functions. Under a Monte Carlo learning algorithm, our model
exhibits predictable training dynamics and the emergence of interpretable
features. This framework allows us to trace, in detail, how inductive bias and
feature formation drive generalisation.

</details>


### [18] [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
*Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, Hao Tan*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种改进的测试时训练方法LaCT，它通过大规模块更新解决了传统TTT方法在处理长上下文数据时效率低下和状态容量受限的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的TTT方法由于在现代GPU上的低效表现，特别是在处理长上下文数据时效果不佳，因此需要一种更有效的方法来解决这些问题。

**方法:** 该论文采用了Large Chunk Test-Time Training (LaCT) 方法，在推理过程中对模型的部分权重进行适应性调整，并利用大规模块更新（从2K到1M个token）来提升计算效率和状态容量。

**结果:** LaCT方法实现了比现有方法高几个数量级的硬件利用率，同时能够扩展非线性状态大小，支持复杂优化器的应用，并成功应用于多种任务和模态，包括语言模型、图像集的新视角合成以及自回归视频扩散模型等。

**结论:** 论文提出了一种新的测试时训练方法LaCT，该方法通过使用大规模块更新显著提高了硬件利用率和模型的非线性状态规模，从而增强了模型处理长上下文数据的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-Time+Training+Done+Right，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23884，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23884&send_immediately=true&force_search=false)

**原文摘要:** Test-Time Training (TTT) models context dependencies by adapting part of the
model's weights (referred to as fast weights) during inference. This fast
weight, akin to recurrent states in RNNs, stores temporary memories of past
tokens in the current sequence. Existing TTT methods struggled to show
effectiveness in handling long-context data, due to their inefficiency on
modern GPUs. The TTT layers in many of these approaches operate with extremely
low FLOPs utilization (often <5%) because they deliberately apply small online
minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,
a small minibatch implies fine-grained block-wise causal dependencies in the
data, unsuitable for data beyond 1D ordered sequences, like sets or
N-dimensional grids such as images or videos. In contrast, we pursue the
opposite direction by using an extremely large chunk update, ranging from 2K to
1M tokens across tasks of varying modalities, which we refer to as Large Chunk
Test-Time Training (LaCT). It improves hardware utilization by orders of
magnitude, and more importantly, facilitates scaling of nonlinear state size
(up to 40% of model parameters), hence substantially improving state capacity,
all without requiring cumbersome and error-prone kernel implementations. It
also allows easy integration of sophisticated optimizers, e.g. Muon for online
updates. We validate our approach across diverse modalities and tasks,
including novel view synthesis with image set, language models, and
auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR
video diffusion model on sequences up to 56K tokens. In our longest sequence
experiment, we perform novel view synthesis with 1 million context length. We
hope this work will inspire and accelerate new research in the field of
long-context modeling and test-time training. Website:
https://tianyuanzhang.com/projects/ttt-done-right

</details>


### [19] [On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks](https://arxiv.org/abs/2505.24205)
*Mingze Wang, Weinan E*

**主要类别:** cs.LG

**AI概要:** 该论文系统研究了混合专家网络（MoEs）在建模复杂任务中的表达能力，表明其在处理低维和稀疏结构任务时的高效性，并揭示了网络架构关键因素的作用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管MoE网络在实际应用中表现优异，但其理论基础尚不完善，尤其是对于其建模复杂任务的能力缺乏深入理解。

**方法:** 论文通过理论分析研究了浅层和深层MoE网络的表达能力，特别是在逼近低维流形上的函数和支持组合稀疏分段函数方面的能力。

**结果:** 论文证明了浅层MoE能够高效逼近低维流形上的函数，而深度MoE则可以逼近具有组合稀疏性的大量分段函数（如$E^L$片段），且层数和专家数对其效果有重要影响。

**结论:** 论文得出MoE网络在建模复杂任务时具备高效的表达能力，尤其是在处理具有低维度和稀疏性特征的任务上表现出色，并揭示了其关键架构组件的作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Expressive+Power+of+Mixture-of-Experts+for+Structured+Complex+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24205&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in
modern deep learning. Despite their empirical success, the theoretical
foundations underlying their ability to model complex tasks remain poorly
understood. In this work, we conduct a systematic study of the expressive power
of MoEs in modeling complex tasks with two common structural priors:
low-dimensionality and sparsity. For shallow MoEs, we prove that they can
efficiently approximate functions supported on low-dimensional manifolds,
overcoming the curse of dimensionality. For deep MoEs, we show that
$\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise
functions comprising $E^L$ pieces with compositional sparsity, i.e., they can
exhibit an exponential number of structured tasks. Our analysis reveals the
roles of critical architectural components and hyperparameters in MoEs,
including the gating mechanism, expert networks, the number of experts, and the
number of layers, and offers natural suggestions for MoE variants.

</details>


### [20] [Model Informed Flows for Bayesian Inference of Probabilistic Programs](https://arxiv.org/abs/2505.24243)
*Joohwan Ko, Justin Domke*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的 Model-Informed Flow 架构，改进了变分推断方法，适用于复杂分层贝叶斯模型，并取得了优秀的表现。


<details>
  <summary>更多</summary>
  
**动机:** 变分推断在处理复杂分层贝叶斯模型的后验几何结构时常常面临挑战，而基于流的变分族和变分推断参数 (VIP) 的最新进展各自解决了这一挑战的部分问题，但它们之间的形式关系尚未被探索。

**方法:** 引入了 Model-Informed Flow (MIF) 架构，该架构增加了必要的平移机制、先验信息和层次顺序，并结合 VIP 和全秩高斯分布。

**结果:** 理论上证明了 VIP 和全秩高斯可以表示为前向自回归流并增加一个平移项和来自模型先验的输入；经验上 MIF 在多个基准测试中表现出色。

**结论:** MIF 提供了更紧密的后验近似，并在一系列分层和非分层基准测试中达到或超过了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+Informed+Flows+for+Bayesian+Inference+of+Probabilistic+Programs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24243&send_immediately=true&force_search=false)

**原文摘要:** Variational inference often struggles with the posterior geometry exhibited
by complex hierarchical Bayesian models. Recent advances in flow-based
variational families and Variationally Inferred Parameters (VIP) each address
aspects of this challenge, but their formal relationship is unexplored. Here,
we prove that the combination of VIP and a full-rank Gaussian can be
represented exactly as a forward autoregressive flow augmented with a
translation term and input from the model's prior. Guided by this theoretical
insight, we introduce the Model-Informed Flow (MIF) architecture, which adds
the necessary translation mechanism, prior information, and hierarchical
ordering. Empirically, MIF delivers tighter posterior approximations and
matches or exceeds state-of-the-art performance across a suite of hierarchical
and non-hierarchical benchmarks.

</details>


### [21] [Thompson Sampling in Online RLHF with General Function Approximation](https://arxiv.org/abs/2505.23927)
*Songtao Feng, Jie Fu*

**主要类别:** cs.LG

**AI概要:** 该论文研究了在线RLHF设置下的动作价值函数逼近问题，提出了一种无模型后验抽样算法并给出了理论分析，包括遗憾界和集中型不等式。


<details>
  <summary>更多</summary>
  
**动机:** 从实证角度看，通过人类反馈进行强化学习（RLHF）在将大型语言模型与人类偏好对齐方面取得了巨大成功，因此从理论角度研究RLHF算法的统计效率非常重要。

**方法:** 采用贝叶曼探索(Bellman eluder)维度作为函数类的复杂度度量，并设计一个受Thompson抽样启发的无模型后验抽样算法。

**结果:** 研究建立了O(√T)的遗憾界，并首次建立了基于最大似然估计(MLE)推广界的平方Bellman误差界的集中型不等式。

**结论:** 研究为在线RLHF设置设计了一个无模型后验抽样算法，并提供了理论保证，包括基于平方Bellman误差界的集中型不等式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thompson+Sampling+in+Online+RLHF+with+General+Function+Approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23927&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning from human feedback (RLHF) has achieved great
empirical success in aligning large language models (LLMs) with human
preference, and it is of great importance to study the statistical efficiency
of RLHF algorithms from a theoretical perspective. In this work, we consider
the online RLHF setting where the preference data is revealed during the
learning process and study action value function approximation. We design a
model-free posterior sampling algorithm for online RLHF inspired by Thompson
sampling and provide its theoretical guarantee. Specifically, we adopt Bellman
eluder (BE) dimension as the complexity measure of the function class and
establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other
multiplicative factor depending on the horizon, BE dimension and the
$log$-bracketing number of the function class. Further, in the analysis, we
first establish the concentration-type inequality of the squared Bellman error
bound based on the maximum likelihood estimator (MLE) generalization bound,
which plays the crucial rules in obtaining the eluder-type regret bound and may
be of independent interest.

</details>


### [22] [Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining](https://arxiv.org/abs/2505.24261)
*Weiyi Wang, Junwei Deng, Yuzheng Hu, Shiyuan Zhang, Xirui Jiang, Runting Zhang, Han Zhao, Jiaqi W. Ma*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了数据归因方法中超参数敏感性的问题，提出了无需模型重新训练的轻量级正则化值选择方法，强调了未来方法开发中超参数选择的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管数据归因方法在现代人工智能的数据中心应用中越来越受欢迎，但这些方法中超参数调优的影响仍未得到充分探索，因此需要理解这一问题以提高方法的实际应用效果。

**方法:** 该论文进行了大规模实证研究以评估数据归因方法的超参数敏感性，并通过对影响函数方法中的正则化项进行理论分析，提出了一个无需重新训练模型的轻量级正则化值选择过程。

**结果:** 研究结果显示，大多数数据归因方法确实对某些关键超参数敏感，且传统的计算验证指标用于超参数调优成本过高；为此，作者提出了有效的轻量级正则化值选择方法并验证了其有效性。

**结论:** 论文得出结论，数据归因方法在超参数选择上具有敏感性，但通过理论分析和案例研究，提出了一种轻量级的正则化值选择过程，为未来方法开发中对超参数选择的深入讨论提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Taming+Hyperparameter+Sensitivity+in+Data+Attribution%3A+Practical+Selection+Without+Costly+Retraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24261&send_immediately=true&force_search=false)

**原文摘要:** Data attribution methods, which quantify the influence of individual training
data points on a machine learning model, have gained increasing popularity in
data-centric applications in modern AI. Despite a recent surge of new methods
developed in this space, the impact of hyperparameter tuning in these methods
remains under-explored. In this work, we present the first large-scale
empirical study to understand the hyperparameter sensitivity of common data
attribution methods. Our results show that most methods are indeed sensitive to
certain key hyperparameters. However, unlike typical machine learning
algorithms -- whose hyperparameters can be tuned using computationally-cheap
validation metrics -- evaluating data attribution performance often requires
retraining models on subsets of training data, making such metrics
prohibitively costly for hyperparameter tuning. This poses a critical open
challenge for the practical application of data attribution methods. To address
this challenge, we advocate for better theoretical understandings of
hyperparameter behavior to inform efficient tuning strategies. As a case study,
we provide a theoretical analysis of the regularization term that is critical
in many variants of influence function methods. Building on this analysis, we
propose a lightweight procedure for selecting the regularization value without
model retraining, and validate its effectiveness across a range of standard
data attribution benchmarks. Overall, our study identifies a fundamental yet
overlooked challenge in the practical application of data attribution, and
highlights the importance of careful discussion on hyperparameter selection in
future method development.

</details>


### [23] [BIRD: Behavior Induction via Representation-structure Distillation](https://arxiv.org/abs/2505.23933)
*Galen Pogoncheff, Michael Beyeler*

**主要类别:** cs.LG

**AI概要:** 本论文介绍了一种名为BIRD的方法，用于将深度学习模型的行为与人类价值观保持一致，该方法通过对齐学生模型和教师模型的内部表示结构实现高效的对齐行为迁移。


<details>
  <summary>更多</summary>
  
**动机:** 将符合人类价值观的行为转移到在不同任务或数据分布上训练的模型上仍然具有挑战性，因为对齐行为容易在微调过程中被遗忘，而收集保留此行为的任务特定数据可能代价高昂。

**方法:** 研究团队引入了BIRD（Behavior Induction via Representation-structure Distillation），通过匹配学生模型与教师模型的内部表示结构进行对齐行为的迁移，并进行了超过400个教师-学生模型对的大规模研究。

**结果:** 在图像分类的分布外鲁棒性应用中，BIRD优于微调、迁移学习和持续学习方法，比次优基线提高了16%的鲁棒准确性。即使教师模型是在更简单的数据集上训练的且比学生模型小25倍，它仍然有效。

**结论:** BIRD是一个灵活的框架，可以通过匹配学生模型和教师模型的内部表示结构来传递对齐行为，从而消除部署安全AI系统的关键瓶颈。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BIRD%3A+Behavior+Induction+via+Representation-structure+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23933，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23933&send_immediately=true&force_search=false)

**原文摘要:** Human-aligned deep learning models exhibit behaviors consistent with human
values, such as robustness, fairness, and honesty. Transferring these
behavioral properties to models trained on different tasks or data
distributions remains challenging: aligned behavior is easily forgotten during
fine-tuning, and collecting task-specific data that preserves this behavior can
be prohibitively costly. We introduce BIRD (Behavior Induction via
Representation-structure Distillation), a flexible framework for transferring
aligned behavior by matching the internal representation structure of a student
model to that of a teacher. Applied to out-of-distribution robustness in image
classification, BIRD outperforms fine-tuning, transfer learning, and continual
learning methods, improving robust accuracy by up to 16% over the next
strongest baseline. It remains effective even when the teacher is trained on a
much simpler dataset and is $25 \times$ smaller than the student. In a
large-scale study of over 400 teacher-student pairs, we show that three
interpretable and computable properties of the teacher's representations (i.e.,
task relevance, behavioral relevance, and complementary knowledge) explain up
to 85% of the variance in transfer success. These insights offer practical
guidance for teacher selection and design. BIRD turns small, well-aligned
models into scalable alignment seeds, removing a key bottleneck in deploying
safe AI systems in the wild.

</details>


### [24] [GradPower: Powering Gradients for Faster Language Model Pre-Training](https://arxiv.org/abs/2505.24275)
*Mingze Wang, Jinbo Wang, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, Weinan E, Lei Wu*

**主要类别:** cs.LG

**AI概要:** GradPower是一种用于加速语言模型预训练的轻量级梯度转换技术。


<details>
  <summary>更多</summary>
  
**动机:** 需要一种有效的方法来加速语言模型的预训练，并在各种架构、参数规模、数据集和学习率计划中实现更低的终端损失。

**方法:** GradPower首先对梯度向量应用元素级别的符号幂变换，然后将变换后的梯度输入到基础优化器中。

**结果:** GradPower在多种架构、参数规模、数据集和学习率计划中一致地实现了更低的终端损失，并且可以与其它最先进的优化器无缝集成。

**结论:** GradPower是一种有效的梯度转换技术，可加速语言模型的预训练，并提供理论分析揭示其机制和梯度噪声的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GradPower%3A+Powering+Gradients+for+Faster+Language+Model+Pre-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24275，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24275&send_immediately=true&force_search=false)

**原文摘要:** We propose GradPower, a lightweight gradient-transformation technique for
accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,
GradPower first applies the elementwise sign-power transformation:
$\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds
the transformed gradient into a base optimizer. Notably, GradPower requires
only a single-line code change and no modifications to the base optimizer's
internal logic, including the hyperparameters. When applied to Adam (termed
AdamPower), GradPower consistently achieves lower terminal loss across diverse
architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,
OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The
most pronounced gains are observed when training modern mixture-of-experts
models with warmup-stable-decay schedules. GradPower also integrates seamlessly
with other state-of-the-art optimizers, such as Muon, yielding further
improvements. Finally, we provide theoretical analyses that reveal the
underlying mechanism of GradPower and highlights the influence of gradient
noise.

</details>


### [25] [Searching Neural Architectures for Sensor Nodes on IoT Gateways](https://arxiv.org/abs/2505.23939)
*Andrea Mattia Garavagno, Edoardo Ragusa, Antonio Frisoli, Paolo Gastaldo*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种基于物联网网关的自动化边缘神经网络设计方法，旨在保护数据隐私的同时实现高效能的机器学习应用。


<details>
  <summary>更多</summary>
  
**动机:** 动机是解决隐私敏感的物联网应用中机器学习访问的问题，尤其是在需要保护敏感信息如工业机密和个人数据的情况下。

**方法:** 该方法在物联网网关上运行，并通过搜索程序在本地网络内设计适用于连接传感器节点的神经网络，而无需将收集的数据分享到外部网络。

**结果:** 实验结果显示，在Visual Wake Words数据集上，该方法能够在Raspberry Pi Zero 2上运行不到10小时的搜索程序，达到最先进的结果。

**结论:** 该论文提出了一种在边缘自动设计神经网络的方法，这种方法能够在保护隐私的前提下，为医疗物联网和工业物联网提供机器学习的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Searching+Neural+Architectures+for+Sensor+Nodes+on+IoT+Gateways，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23939，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23939&send_immediately=true&force_search=false)

**原文摘要:** This paper presents an automatic method for the design of Neural Networks
(NNs) at the edge, enabling Machine Learning (ML) access even in
privacy-sensitive Internet of Things (IoT) applications. The proposed method
runs on IoT gateways and designs NNs for connected sensor nodes without sharing
the collected data outside the local network, keeping the data in the site of
collection. This approach has the potential to enable ML for Healthcare
Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing
hardware-friendly and custom NNs at the edge for personalized healthcare and
advanced industrial services such as quality control, predictive maintenance,
or fault diagnosis. By preventing data from being disclosed to cloud services,
this method safeguards sensitive information, including industrial secrets and
personal data. The outcomes of a thorough experimental session confirm that --
on the Visual Wake Words dataset -- the proposed approach can achieve
state-of-the-art results by exploiting a search procedure that runs in less
than 10 hours on the Raspberry Pi Zero 2.

</details>


### [26] [Binary Cumulative Encoding meets Time Series Forecasting](https://arxiv.org/abs/2505.24595)
*Andrei Chernov, Vitaliy Pozdnyakov, Ilya Makarov*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种基于二进制累积编码（BCE）的时间序列预测方法，利用保持序数信息的编码方式和专设的卷积神经网络架构，在减少参数的同时提高了模型的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列预测方法通常依赖于忽略潜在值固有序数结构的一热编码，因此无法在训练期间提供预测值与真实值之间的相对距离信息。这种限制是本研究的主要动机。

**方法:** 该论文提出了一种新的目标编码策略，即二进制累积编码（BCE），将标量目标转化为单调二进制向量，并设计了一个专门用于BCE的卷积神经网络架构，结合残差和扩张卷积以实现快速且富有表现力的时间建模。

**结果:** 通过广泛的实验表明，该方法在基准预测数据集上表现优异，在点预测和概率预测方面均超越了现有主流方法。

**结论:** 论文得出结论，他们提出的二进制累积编码（BCE）方法在时间序列预测任务中表现优越，既在点预测和概率预测方面都优于广泛使用的方法，同时需要更少的参数并加快训练速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Binary+Cumulative+Encoding+meets+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24595&send_immediately=true&force_search=false)

**原文摘要:** Recent studies in time series forecasting have explored formulating
regression via classification task. By discretizing the continuous target space
into bins and predicting over a fixed set of classes, these approaches benefit
from stable training, robust uncertainty modeling, and compatibility with
modern deep learning architectures. However, most existing methods rely on
one-hot encoding that ignores the inherent ordinal structure of the underlying
values. As a result, they fail to provide information about the relative
distance between predicted and true values during training. In this paper, we
propose to address this limitation by introducing binary cumulative encoding
(BCE), that represents scalar targets into monotonic binary vectors. This
encoding implicitly preserves order and magnitude information, allowing the
model to learn distance-aware representations while still operating within a
classification framework. We propose a convolutional neural network
architecture specifically designed for BCE, incorporating residual and dilated
convolutions to enable fast and expressive temporal modeling. Through extensive
experiments on benchmark forecasting datasets, we show that our approach
outperforms widely used methods in both point and probabilistic forecasting,
while requiring fewer parameters and enabling faster training.

</details>


### [27] [Vision Language Models are Biased](https://arxiv.org/abs/2505.23941)
*An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, Daeyoung Kim*

**主要类别:** cs.LG

**AI概要:** 本文探讨了视觉语言模型由于先验知识产生的偏差问题，在多种视觉计数任务中发现了显著的失败模式。


<details>
  <summary>更多</summary>
  
**动机:** 研究大型语言模型可能因互联网知识而产生偏差的问题，尤其是在视觉任务上对准确性和偏见的影响。

**方法:** 论文通过引入流行主题的知识来测试其对VLMs在计数和识别任务上的影响，并测量了模型在不同领域的准确性。

**结果:** 实验发现最先进的VLMs在计数任务中的平均准确率为17.05%，并且描述对象名称的插入文本进一步降低了准确性。

**结论:** 论文得出结论，视觉语言模型(VLMs)在标准视觉任务中表现出显著的偏差，并提出了一个测试VLM偏差的自动化框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Vision+Language+Models+are+Biased，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23941&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.

</details>


### [28] [Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms](https://arxiv.org/abs/2505.24692)
*Derek Everett, Fred Lu, Edward Raff, Fernando Camacho, James Holt*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quick-Draw+Bandits%3A+Quickly+Optimizing+in+Nonstationary+Environments+with+Extremely+Many+Arms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24692，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24692&send_immediately=true&force_search=false)

**原文摘要:** Canonical algorithms for multi-armed bandits typically assume a stationary
reward environment where the size of the action space (number of arms) is
small. More recently developed methods typically relax only one of these
assumptions: existing non-stationary bandit policies are designed for a small
number of arms, while Lipschitz, linear, and Gaussian process bandit policies
are designed to handle a large (or infinite) number of arms in stationary
reward environments under constraints on the reward function. In this
manuscript, we propose a novel policy to learn reward environments over a
continuous space using Gaussian interpolation. We show that our method
efficiently learns continuous Lipschitz reward functions with
$\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally
extends to non-stationary problems with a simple modification. We finally
demonstrate that our method is computationally favorable (100-10000x faster)
and experimentally outperforms sliding Gaussian process policies on datasets
with non-stationarity and an extremely large number of arms.

</details>


### [29] [SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations](https://arxiv.org/abs/2505.23942)
*Gaurav Sarkar, Jay Gala, Subarna Tripathi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的激活函数SG-Blend，结合了SSwish和GELU的优点，在多个任务和模型中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的激活函数如Swish和GELU通常表现出领域特定的最优性，需要一种更通用的激活函数。

**方法:** SG-Blend通过动态插值将SSwish和GELU结合，并利用可学习参数进行自适应混合。

**结果:** 实验结果显示，SG-Blend在各种模态和架构中均优于现有方法。

**结论:** SG-Blend在自然语言和计算机视觉任务中都表现出性能提升，且计算开销可以忽略不计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SG-Blend%3A+Learning+an+Interpolation+Between+Improved+Swish+and+GELU+for+Robust+Neural+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23942&send_immediately=true&force_search=false)

**原文摘要:** The design of activation functions remains a pivotal component in optimizing
deep neural networks. While prevailing choices like Swish and GELU demonstrate
considerable efficacy, they often exhibit domain-specific optima. This work
introduces SG-Blend, a novel activation function that blends our proposed
SSwish, a first-order symmetric variant of Swish and the established GELU
through dynamic interpolation. By adaptively blending these constituent
functions via learnable parameters, SG-Blend aims to harness their
complementary strengths: SSwish's controlled non-monotonicity and symmetry, and
GELU's smooth, probabilistic profile, to achieve a more universally robust
balance between model expressivity and gradient stability. We conduct
comprehensive empirical evaluations across diverse modalities and
architectures, showing performance improvements across all considered natural
language and computer vision tasks and models. These results, achieved with
negligible computational overhead, underscore SG-Blend's potential as a
versatile, drop-in replacement that consistently outperforms strong
contemporary baselines. The code is available at
https://anonymous.4open.science/r/SGBlend-6CBC.

</details>


### [30] [Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning](https://arxiv.org/abs/2505.24737)
*Erchi Wang, Yuqing Zhu, Yu-Xiang Wang*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种高效的差分隐私算法用于二元线性分类，并在离群点较少时提供了改进的风险边界结果。


<details>
  <summary>更多</summary>
  
**动机:** 旨在解决二元线性分类中的差分隐私经验风险最小化问题，并针对现有方法在离群点较少时的效果进行改进。

**方法:** 通过提出一种不需知道边缘参数γ或离群子集S_out的高效(ε,δ)-DP算法，研究者分析了其在经验零一风险上的边界。

**结果:** 获得了经验零一风险的上界为Õ(1/(γ²εn) + |S_out|/(γn))的高效(ε,δ)-DP算法，其中n是数据点数量，S_out是任意可移除的数据子集，γ是剩余数据点的线性分离边缘。

**结论:** 论文得出了一种高效的(ε,δ)-DP算法用于二元线性分类的差分隐私经验风险最小化，并改进了现有结果，尤其是在离群点数量较少的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+to+Linear+Separable+Subsets+with+Large-Margin+in+Differentially+Private+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24737&send_immediately=true&force_search=false)

**原文摘要:** This paper studies the problem of differentially private empirical risk
minimization (DP-ERM) for binary linear classification. We obtain an efficient
$(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of
$\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} +
\frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data
points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and
$\gamma$ is the margin of linear separation of the remaining data points (after
$S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic
terms. In the agnostic case, we improve the existing results when the number of
outliers is small. Our algorithm is highly adaptive because it does not require
knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We
also derive a utility bound for the advanced private hyperparameter tuning
algorithm.

</details>


### [31] [Position: The Future of Bayesian Prediction Is Prior-Fitted](https://arxiv.org/abs/2505.23947)
*Samuel Müller, Arik Reuter, Noah Hollmann, David Rügamer, Frank Hutter*

**主要类别:** cs.LG

**AI概要:** 本文探讨了 Prior-data Fitted Networks (PFNs) 在贝叶斯推断中的重要性，尤其是在数据稀缺的情况下，展示了 PFNs 的潜力以及未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 随着预训练计算资源的迅速增加，许多应用中新现实世界数据的生成几乎停滞，因此需要更加重视能够有效利用计算资源应对低数据场景的方法。

**方法:** 论文讨论了通过在随机生成的人工数据集上训练神经网络，以捕获由数据集生成分布定义的先验的方法，从而构建贝叶斯模型。

**结果:** Prior-data Fitted Networks (PFNs) 已从最初应用于小型贝叶斯建模任务扩展到更复杂的领域和更大的数据集，显示出其在广泛的应用中具有重要作用的潜力。

**结论:** Prior-data Fitted Networks (PFNs) 和其他摊销推断方法代表了贝叶斯推断的未来，它们利用摊销学习解决数据稀缺问题，并被认为是一个有前途的研究领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+The+Future+of+Bayesian+Prediction+Is+Prior-Fitted，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23947&send_immediately=true&force_search=false)

**原文摘要:** Training neural networks on randomly generated artificial datasets yields
Bayesian models that capture the prior defined by the dataset-generating
distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed
to leverage this insight. In an era of rapidly increasing computational
resources for pre-training and a near stagnation in the generation of new
real-world data in many applications, PFNs are poised to play a more important
role across a wide range of applications. They enable the efficient allocation
of pre-training compute to low-data scenarios. Originally applied to small
Bayesian modeling tasks, the field of PFNs has significantly expanded to
address more complex domains and larger datasets. This position paper argues
that PFNs and other amortized inference approaches represent the future of
Bayesian inference, leveraging amortized learning to tackle data-scarce
problems. We thus believe they are a fruitful area of research. In this
position paper, we explore their potential and directions to address their
current limitations.

</details>


### [32] [TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks](https://arxiv.org/abs/2505.23949)
*Xiang Meng, Mehdi Makni, Rahul Mazumder*

**主要类别:** cs.LG

**AI概要:** 本文提出了一種高效的可轉置N:M稀疏掩碼求解器，能夠擴展到十億參數模型，解決了現有方法無法大規模應用或限制於M=4的問題。


<details>
  <summary>更多</summary>
  
**动机:** 現有的N:M稀疏模式在訓練時因矩陣轉置而失去效率，而現有的可轉置N:M稀疏方法無法擴展到大型模型或受限於M=4，導致壓縮與準確性之間的權衡不佳。

**方法:** 將掩碼生成表述為最佳傳輸問題，並通過熵正則化和Dykstra算法求解，隨後進行捨入處理。實現基於張量的GPU加速方法以提高效率。

**结果:** 該方法在十億參數模型上實現高達100倍的速度提升，誤差僅為1-10%。LLaMA3.2-8B模型使用可轉置16:32稀疏模式時表現接近標準N:M模型，且優於標準2:4稀疏模型。

**结论:** 提出的可轉置N:M稀疏掩碼方法具有高度可擴展性和實際價值，可用於大型神經網絡訓練與推論。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TSENOR%3A+Highly-Efficient+Algorithm+for+Finding+Transposable+N%3AM+Sparse+Masks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23949&send_immediately=true&force_search=false)

**原文摘要:** Network pruning reduces the computational requirements of large neural
networks, with N:M sparsity -- retaining only N out of every M consecutive
weights -- offering a compelling balance between compressed model quality and
hardware acceleration. However, N:M sparsity only accelerates forward-pass
computations, as N:M patterns are not preserved during matrix transposition,
limiting efficiency during training where both passes are computationally
intensive. While transposable N:M sparsity has been proposed to address this
limitation, existing methods for finding transposable N:M sparse masks either
fail to scale to large models or are restricted to M=4 which results in
suboptimal compression-accuracy trade-off. We introduce an efficient solver for
transposable N:M masks that scales to billion-parameter models. We formulate
mask generation as optimal transport problems and solve through entropy
regularization and Dykstra's algorithm, followed by a rounding procedure. Our
tensor-based implementation exploits GPU parallelism, achieving up to 100x
speedup with only 1-10% error compared to existing methods. Our approach can be
integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT
and ALPS to produce transposable N:M sparse models with arbitrary N:M values.
Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains
performance close to its standard N:M counterpart and outperforms standard 2:4
sparse model, showing the practical value of our approach.

</details>


### [33] [Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective](https://arxiv.org/abs/2505.23954)
*Dylan Zapzalka, Trenton Chang, Lindsay Warrenburg, Sae-Hwan Park, Daniel K. Shenfeld, Ravi B. Parikh, Jenna Wiens, Maggie Makar*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种基于因果关系的方法，用于识别和量化机器学习模型中代理的虚假报告行为，通过区分欺骗性特征变化和真实修改，解决了资源分配决策中的战略响应问题。


<details>
  <summary>更多</summary>
  
**动机:** 在机器学习模型用于资源分配的环境中，受影响的代理可能会战略性地更改其特征以获得更好的结果，而如何区分虚假报告和真实修改是一个关键挑战。

**方法:** 利用因果推断，比较操纵数据集与非操纵数据集中虚假报告特征对其因果后代的因果效应差异，从而识别和量化平均虚假报告率。

**结果:** 理论上证明了虚假报告率的可识别性，并对估计量的方差进行了表征；在半合成和真实的Medicare数据集上验证了方法的有效性。

**结论:** 提出的因果驱动方法能够有效识别现实场景中的虚假报告行为，为资源分配决策提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimating+Misreporting+in+the+Presence+of+Genuine+Modification%3A+A+Causal+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23954&send_immediately=true&force_search=false)

**原文摘要:** In settings where ML models are used to inform the allocation of resources,
agents affected by the allocation decisions might have an incentive to
strategically change their features to secure better outcomes. While prior work
has studied strategic responses broadly, disentangling misreporting from
genuine modification remains a fundamental challenge. In this paper, we propose
a causally-motivated approach to identify and quantify how much an agent
misreports on average by distinguishing deceptive changes in their features
from genuine modification. Our key insight is that, unlike genuine
modification, misreported features do not causally affect downstream variables
(i.e., causal descendants). We exploit this asymmetry by comparing the causal
effect of misreported features on their causal descendants as derived from
manipulated datasets against those from unmanipulated datasets. We formally
prove identifiability of the misreporting rate and characterize the variance of
our estimator. We empirically validate our theoretical results using a
semi-synthetic and real Medicare dataset with misreported data, demonstrating
that our approach can be employed to identify misreporting in real-world
scenarios.

</details>


### [34] [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/abs/2505.23960)
*Henry Conklin*

**主要类别:** cs.LG

**AI概要:** 这篇论文旨在解决大规模神经网络表示空间缺乏统一描述的问题，通过引入定量方法分析模型结构与泛化能力的关系，并提出了适用于不同规模模型的新方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大规模神经网络取得了显著成功，但对其表示空间的描述缺乏统一的符号体系和方法，因此需要引入新方法来理解其结构和泛化能力。

**方法:** 论文提出了一种新的高性能估计向量空间熵的方法，并利用这些方法分析了多智能体强化学习模型、序列到序列模型以及大型语言模型。

**结果:** 论文展示了如何通过量化方法揭示大规模分布式认知模型的学习机制，并在语言结构与神经网络性能之间建立了类比关系。

**结论:** 论文得出结论，通过识别结构基本元素和信息理论量化方法，可以分析深度学习模型的学习、结构和泛化能力，并将其应用于不同规模的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Information+Structure+in+Mappings%3A+An+Approach+to+Learning%2C+Representation%2C+and+Generalisation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23960，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23960&send_immediately=true&force_search=false)

**原文摘要:** Despite the remarkable success of large large-scale neural networks, we still
lack unified notation for thinking about and describing their representational
spaces. We lack methods to reliably describe how their representations are
structured, how that structure emerges over training, and what kinds of
structures are desirable. This thesis introduces quantitative methods for
identifying systematic structure in a mapping between spaces, and leverages
them to understand how deep-learning models learn to represent information,
what representational structures drive generalisation, and how design decisions
condition the structures that emerge. To do this I identify structural
primitives present in a mapping, along with information theoretic
quantifications of each. These allow us to analyse learning, structure, and
generalisation across multi-agent reinforcement learning models,
sequence-to-sequence models trained on a single task, and Large Language
Models. I also introduce a novel, performant, approach to estimating the
entropy of vector space, that allows this analysis to be applied to models
ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models
of cognition learn, while allowing us to draw parallels between those systems
and their human analogs. They show how the structures of language and the
constraints that give rise to them in many ways parallel the kinds of
structures that drive performance of contemporary neural networks.

</details>


### [35] [Improved Approximations for Hard Graph Problems using Predictions](https://arxiv.org/abs/2505.23967)
*Anders Aamand, Justin Y. Chen, Siddharth Gollapudi, Sandeep Silwal, Hao Wu*

**主要类别:** cs.LG

**AI概要:** 本文通过引入预测机制（例如从历史数据中学习得到）设计了针对NP难解图问题的改进近似算法。


<details>
  <summary>更多</summary>
  
**动机:** 在标准设定中，许多图问题存在近似性障碍。作者旨在利用弱预测信息来突破这些障碍，并提升算法性能。

**方法:** 文章基于并扩展了Cohen-Addad等人提出的ε-预测框架，采用一种基于边的模型，每条边提供两个比特的预测信息，分别对应其端点是否属于最优解。算法统一地处理高、低度顶点约束，结合预测与非预测方法求解。

**结果:** 对于MaxCut、Vertex Cover、Set Cover和Maximum Independent Set等问题，作者成功开发出具有更好近似比的算法，并验证了预测机制的有效性。

**结论:** 该研究证明，即使使用较弱的预测信息，也可以显著改善经典近似算法的表现，为未来的研究提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+Approximations+for+Hard+Graph+Problems+using+Predictions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23967&send_immediately=true&force_search=false)

**原文摘要:** We design improved approximation algorithms for NP-hard graph problems by
incorporating predictions (e.g., learned from past data). Our prediction model
builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad,
d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based
version of this model, where each edge provides two bits of information,
corresponding to predictions about whether each of its endpoints belong to an
optimal solution. Even with weak predictions where each bit is only
$\varepsilon$-correlated with the true solution, this information allows us to
break approximation barriers in the standard setting. We develop algorithms
with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and
Maximum Independent Set problems (among others). Across these problems, our
algorithms share a unifying theme, where we separately satisfy constraints
related to high degree vertices (using predictions) and low-degree vertices
(without using predictions) and carefully combine the answers.

</details>


### [36] [Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training](https://arxiv.org/abs/2505.23971)
*William Merrill, Shane Arora, Dirk Groeneveld, Hannaneh Hajishirzi*

**主要类别:** cs.LG

**AI概要:** 该论文介绍了一种新的实证方法来测量临界批量大小（CBS），并提出了批量大小预热策略，以更高效和可靠的方式训练大规模语言模型。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决传统方法中使用梯度噪声作为CBS代理时所需的强假设问题，提供一种更实用且可信赖的方法。

**方法:** 通过对OLMo模型的实验，观察CBS随训练过程的变化趋势，并利用批量大小预热策略验证其效果。

**结果:** 发现CBS在训练初期迅速增加，随后趋于稳定；通过批量大小预热策略，用43%更少的梯度步骤实现了比原始训练更好的损失值。

**结论:** 本文提出了一种直接测量临界批量大小（CBS）的实证方法，并展示了如何在不损害性能的情况下可靠地训练大规模语言模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Critical+Batch+Size+Revisited%3A+A+Simple+Empirical+Approach+to+Large-Batch+Language+Model+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23971&send_immediately=true&force_search=false)

**原文摘要:** The right batch size is important when training language models at scale: a
large batch size is necessary for fast training, but a batch size that is too
large will harm token efficiency. To navigate this tradeoff, McCandlish et al.
(2018) suggest that a critical batch size (CBS), below which training will not
substantially degrade loss, can be estimated based on the gradient noise scale
during training. While their method has been adopted in practice, e.g., when
training GPT-3, strong assumptions are required to justify gradient noise as a
proxy for the CBS, which makes it unclear whether their approach should be
trusted in practice, limiting its applicability. In this paper, we introduce a
simple, empirical approach to directly measure the CBS and show how the CBS
evolves over training. Applying our approach to the OLMo models, we find that
CBS is near 0 at initialization, increases rapidly at first, and then plateaus
as training progresses. Furthermore, we find that this trend holds across
different model sizes (1B and 7B), suggesting CBS from small training runs can
inform larger-scale training runs. Our findings about how the CBS changes over
training motivate batch size warmup as a natural way to reliably train language
models at large batch size: start the batch size small and increase it as the
CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to
slightly better loss than the original training run with 43% fewer gradient
steps. This shows how our framework can be applied to reliably train language
models at larger batch sizes, increasing data parallelism without compromising
performance.

</details>


### [37] [Adaptive Deadline and Batch Layered Synchronized Federated Learning](https://arxiv.org/abs/2505.23973)
*Asaf Goren, Natalie Lang, Nir Shlezinger, Alejandro Cohen*

**主要类别:** cs.LG

**AI概要:** 本文提出了ADEL-FL，一种动态优化联邦学习中截止时间和批量大小的方法，显著提高了异构环境下的训练效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 同步联邦学习由于设备异构性（慢速客户端）存在延迟瓶颈，而现有解决方案将时间安排和工作负载视为静态参数，限制了其效果。

**方法:** 通过构建一个受限优化问题，最小化训练时间和全局轮次约束下的预期L2距离到全局最优解，并在指数计算模型下提供收敛性分析和证明。

**结果:** 实验表明，ADEL-FL在异构条件下比替代方法具有更好的收敛速度和最终准确率表现。

**结论:** 论文提出了一种名为ADEL-FL的新型框架，能够联合优化每轮截止时间和用户特定的批量大小，以解决边缘设备异构性导致的延迟瓶颈问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Deadline+and+Batch+Layered+Synchronized+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23973，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23973&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) enables collaborative model training across
distributed edge devices while preserving data privacy, and typically operates
in a round-based synchronous manner. However, synchronous FL suffers from
latency bottlenecks due to device heterogeneity, where slower clients
(stragglers) delay or degrade global updates. Prior solutions, such as fixed
deadlines, client selection, and layer-wise partial aggregation, alleviate the
effect of stragglers, but treat round timing and local workload as static
parameters, limiting their effectiveness under strict time constraints. We
propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines
and user-specific batch sizes for layer-wise aggregation. Our approach
formulates a constrained optimization problem minimizing the expected L2
distance to the global optimum under total training time and global rounds. We
provide a convergence analysis under exponential compute models and prove that
ADEL-FL yields unbiased updates with bounded variance. Extensive experiments
demonstrate that ADEL-FL outperforms alternative methods in both convergence
rate and final accuracy under heterogeneous conditions.

</details>


### [38] [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/abs/2505.23987)
*Vishal Dey, Xiao Hu, Xia Ning*

**主要类别:** cs.LG

**AI概要:** 本文提出C-MuMOInstruct和GeLLMO-Cs，解决了药物设计中多属性优化的问题，并展示了出色的性能和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有计算方法和指令调优大语言模型无法捕捉药物设计中多属性优化的细微目标，限制了它们的实际应用。

**方法:** 开发了一个新的指令调优数据集C-MuMOInstruct，并在此基础上构建了GeLLMO-Cs模型系列，用于执行针对特定属性的优化。

**结果:** GeLLMO-Cs在10项任务（5个分布内和5个分布外）中表现优于强基线模型，成功率最高提高126%。同时展示了对新优化任务和未见指令的强大零样本泛化能力。

**结论:** C-MuMOInstruct和GeLLMO-Cs为基于属性特定目标的分子优化提供了有效的计算方法，并展示了其在多种任务上的优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+for+Controllable+Multi-property+Multi-objective+Molecule+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23987&send_immediately=true&force_search=false)

**原文摘要:** In real-world drug design, molecule optimization requires selectively
improving multiple molecular properties up to pharmaceutically relevant levels,
while maintaining others that already meet such criteria. However, existing
computational approaches and instruction-tuned LLMs fail to capture such
nuanced property-specific objectives, limiting their practical applicability.
To address this, we introduce C-MuMOInstruct, the first instruction-tuning
dataset focused on multi-property optimization with explicit, property-specific
objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of
instruction-tuned LLMs that can perform targeted property-specific
optimization. Our experiments across 5 in-distribution and 5
out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong
baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit
impressive 0-shot generalization to novel optimization tasks and unseen
instructions. This offers a step toward a foundational LLM to support
realistic, diverse optimizations with property-specific objectives.
C-MuMOInstruct and code are accessible through
https://github.com/ninglab/GeLLMO-C.

</details>


### [39] [Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting](https://arxiv.org/abs/2505.24003)
*ChengAo Shen, Wenchao Yu, Ziming Zhao, Dongjin Song, Wei Cheng, Haifeng Chen, Jingchao Ni*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的多模态时间序列预测框架DMMV，通过分解方法整合多模态视图，有效解决了大型视觉模型在长期预测中的归纳偏差问题，并在多个数据集上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列可以通过转化为图像和文本提供多模态视角，从而揭示互补模式并利用强大的预训练模型（如大型视觉模型）进行长期预测，但直接应用这些模型存在预测周期上的归纳偏差。

**方法:** 提出了DMMV框架，结合了趋势-季节分解与基于回测残差的自适应分解方法，以解决将大型视觉模型应用于长期时间序列预测中存在的归纳偏差问题。

**结果:** 在8个基准数据集中的6个上取得了最佳均方误差（MSE）表现，并且在多个数据集上超越了14种最先进的模型。

**结论:** DMMV在LTSF任务中优于现有的单视图和多模态基线模型，通过利用趋势-季节分解和基于回测残差的自适应分解方法，有效地整合了多模态视图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+View+Enhanced+Large+Vision+Models+for+Long-Term+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24003&send_immediately=true&force_search=false)

**原文摘要:** Time series, typically represented as numerical sequences, can also be
transformed into images and texts, offering multi-modal views (MMVs) of the
same underlying signal. These MMVs can reveal complementary patterns and enable
the use of powerful pre-trained large models, such as large vision models
(LVMs), for long-term time series forecasting (LTSF). However, as we identified
in this work, applying LVMs to LTSF poses an inductive bias towards
"forecasting periods". To harness this bias, we propose DMMV, a novel
decomposition-based multi-modal view framework that leverages trend-seasonal
decomposition and a novel backcast residual based adaptive decomposition to
integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art
(SOTA) models across diverse datasets show that DMMV outperforms single-view
and existing multi-modal baselines, achieving the best mean squared error (MSE)
on 6 out of 8 benchmark datasets.

</details>


### [40] [How far away are truly hyperparameter-free learning algorithms?](https://arxiv.org/abs/2505.24005)
*Priya Kasimbeg, Vincent Roulet, Naman Agarwal, Sourabh Medapati, Fabian Pedregosa, Atish Agarwala, George E. Dahl*

**主要类别:** cs.LG

**AI概要:** 本文探讨了学习率自由方法在减少神经网络训练中超参数调优需求的潜力，结果显示这些方法仍有很大改进空间，并强调了与强基线对比测试的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 超参数调优仍然是机器学习系统开发中的关键且昂贵的部分，本文旨在探索学习率自由方法作为无超参数方法组成部分的潜力。

**方法:** 通过冻结非学习率超参数的默认值，并使用AlgoPerf: Training Algorithms基准测试来评估学习率自由方法的性能。

**结果:** 文献提供的默认设置在基准测试中表现不佳，经过搜索找到的跨所有工作负载表现良好的超参数配置显著提升了最佳AlgoPerf校准的学习率自由方法的性能，但仍略逊于类似的NadamW基线。

**结论:** 学习率自由的方法仍有很大的改进空间，与NadamW基线相比表现稍逊，表明在实现完全无需超参数调整的神经网络训练算法的梦想上还有很长的路要走。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+far+away+are+truly+hyperparameter-free+learning+algorithms%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24005&send_immediately=true&force_search=false)

**原文摘要:** Despite major advances in methodology, hyperparameter tuning remains a
crucial (and expensive) part of the development of machine learning systems.
Even ignoring architectural choices, deep neural networks have a large number
of optimization and regularization hyperparameters that need to be tuned
carefully per workload in order to obtain the best results. In a perfect world,
training algorithms would not require workload-specific hyperparameter tuning,
but would instead have default settings that performed well across many
workloads. Recently, there has been a growing literature on optimization
methods which attempt to reduce the number of hyperparameters -- particularly
the learning rate and its accompanying schedule. Given these developments, how
far away is the dream of neural network training algorithms that completely
obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as
components of hyperparameter-free methods. We freeze their (non-learning rate)
hyperparameters to default values, and score their performance using the
recently-proposed AlgoPerf: Training Algorithms benchmark. We found that
literature-supplied default settings performed poorly on the benchmark, so we
performed a search for hyperparameter configurations that performed well across
all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free
methods had much improved performance but still lagged slightly behind a
similarly calibrated NadamW baseline in overall benchmark score. Our results
suggest that there is still much room for improvement for learning-rate-free
methods, and that testing against a strong, workload-agnostic baseline is
important to improve hyperparameter reduction techniques.

</details>


### [41] [From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?](https://arxiv.org/abs/2505.24030)
*Ziming Zhao, ChengAo Shen, Hanghang Tong, Dongjin Song, Zhigang Deng, Qingsong Wen, Jingchao Ni*

**主要类别:** cs.LG

**AI概要:** 本研究系统评估了LVMs在时间序列分析中的效用，发现其在分类任务中有效，但在预测方面存在限制。


<details>
  <summary>更多</summary>
  
**动机:** 随着多模态领域的发展，大型视觉模型（LVMs）成为有前途的研究方向。鉴于Transformer和LLMs在时间序列中的有效性曾受到质疑，本文提出了一个核心问题：LVMs是否真正适用于时间序列分析？

**方法:** 研究包括4个LVM模型、8种成像方法、18个数据集和26个基线模型，涵盖了高层次（分类）和低层次（预测）任务，并进行了广泛的消融分析。

**结果:** 研究发现，LVMs确实对时间序列分类有用，但在预测任务中表现存在局限性。

**结论:** 论文得出结论，LVMs在时间序列分类中是有效的，但在预测方面面临挑战。尽管如此，当前最好的LVM预测器仅限于特定类型的LVM和成像方法，并且对预测周期表现出偏差，利用长回溯窗口的能力有限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Images+to+Signals%3A+Are+Large+Vision+Models+Useful+for+Time+Series+Analysis%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24030，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24030&send_immediately=true&force_search=false)

**原文摘要:** Transformer-based models have gained increasing attention in time series
research, driving interest in Large Language Models (LLMs) and foundation
models for time series analysis. As the field moves toward multi-modality,
Large Vision Models (LVMs) are emerging as a promising direction. In the past,
the effectiveness of Transformer and LLMs in time series has been debated. When
it comes to LVMs, a similar question arises: are LVMs truely useful for time
series analysis? To address it, we design and conduct the first principled
study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across
both high-level (classification) and low-level (forecasting) tasks, with
extensive ablation analysis. Our findings indicate LVMs are indeed useful for
time series classification but face challenges in forecasting. Although
effective, the contemporary best LVM forecasters are limited to specific types
of LVMs and imaging methods, exhibit a bias toward forecasting periods, and
have limited ability to utilize long look-back windows. We hope our findings
could serve as a cornerstone for future research on LVM- and multimodal-based
solutions to different time series tasks.

</details>


### [42] [LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin](https://arxiv.org/abs/2505.24034)
*Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个用于大规模语言模型的高效强化学习框架LlamaRL，其通过异步设计和优化技术实现了显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 为了应对大规模语言模型在延迟和内存需求上的挑战，需要开发高效的强化学习框架。

**方法:** 提出了一种名为LlamaRL的全分布式、异步强化学习框架，基于原生PyTorch构建，并进行了理论分析和实证测试。

**结果:** LlamaRL实现了高达10.7倍的速度提升，相较于类似DeepSpeed-Chat系统，并且其效率优势随着模型规模增加而增长。

**结论:** LlamaRL是一个专为大规模语言模型设计的高效强化学习框架，它通过异步设计和最佳实践实现了显著的效率提升，并且适用于未来的大型训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LlamaRL%3A+A+Distributed+Asynchronous+Reinforcement+Learning+Framework+for+Efficient+Large-scale+LLM+Trainin，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24034&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has become the most effective post-training
approach for improving the capabilities of Large Language Models (LLMs). In
practice, because of the high demands on latency and memory, it is particularly
challenging to develop an efficient RL framework that reliably manages policy
models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL
framework optimized for efficient training of large-scale LLMs with various
model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a
handful to thousands of devices. LlamaRL introduces a streamlined,
single-controller architecture built entirely on native PyTorch, enabling
modularity, ease of use, and seamless scalability to thousands of GPUs. We also
provide a theoretical analysis of LlamaRL's efficiency, including a formal
proof that its asynchronous design leads to strict RL speed-up. Empirically, by
leveraging best practices such as colocated model offloading, asynchronous
off-policy training, and distributed direct memory access for weight
synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x
speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy
model. Furthermore, the efficiency advantage continues to grow with increasing
model scale, demonstrating the framework's suitability for future large-scale
RL training.

</details>


### [43] [NeuronTune: Towards Self-Guided Spurious Bias Mitigation](https://arxiv.org/abs/2505.24048)
*Guangtao Zheng, Wenqian Ye, Aidong Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法NeuronTune，可以在不依赖外部注释的情况下，通过直接干预模型内部决策过程来减轻深度神经网络中的虚假偏差。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络经常会产生虚假偏差，依赖于非必要特征与类别之间的相关性进行预测，这会导致在缺乏这些相关性的数据上性能下降。现有的缓解方法通常依赖于对虚假相关性的外部注释，但可能难以获得且与模型中的虚假偏差无关。

**方法:** 提出了一种名为NeuronTune的后处理方法，在模型的潜在嵌入空间中探测，以识别和调节导致虚假预测行为的神经元。

**结果:** 实验结果表明，该方法能够显著减轻自导式虚假偏差，适用于不同的架构和数据模态。

**结论:** NeuronTune是一种无需依赖外部虚假相关性注释的实用且有效的方法，可以直接干预模型内部决策过程以减轻虚假偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeuronTune%3A+Towards+Self-Guided+Spurious+Bias+Mitigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24048&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks often develop spurious bias, reliance on correlations
between non-essential features and classes for predictions. For example, a
model may identify objects based on frequently co-occurring backgrounds rather
than intrinsic features, resulting in degraded performance on data lacking
these correlations. Existing mitigation approaches typically depend on external
annotations of spurious correlations, which may be difficult to obtain and are
not relevant to the spurious bias in a model. In this paper, we take a step
towards self-guided mitigation of spurious bias by proposing NeuronTune, a post
hoc method that directly intervenes in a model's internal decision process. Our
method probes in a model's latent embedding space to identify and regulate
neurons that lead to spurious prediction behaviors. We theoretically justify
our approach and show that it brings the model closer to an unbiased one.
Unlike previous methods, NeuronTune operates without requiring spurious
correlation annotations, making it a practical and effective tool for improving
model robustness. Experiments across different architectures and data
modalities demonstrate that our method significantly mitigates spurious bias in
a self-guided way.

</details>


### [44] [Differential Gated Self-Attention](https://arxiv.org/abs/2505.24054)
*Elpiniki Maria Lygizou, Mónika Farsang, Radu Grosu*

**主要类别:** cs.LG

**AI概要:** 本文提出了基于生物侧抑制机制的多头差分门控自注意力方法（M-DGSA），通过动态抑制注意力噪声提升了Transformer在噪声输入下的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统Transformer的自注意力机制对所有查询-键交互一视同仁，难以应对输入中的噪声问题；受生物神经回路中侧抑制机制的启发，需要设计更鲁棒的注意力机制。

**方法:** 提出Multihead Differential Gated Self-Attention (M-DGSA)，采用兴奋和抑制分支的双softmax映射，并通过基于token嵌入的sigmoid门控进行融合。

**结果:** M-DGSA在视觉和语言任务上均表现出优于标准Transformer、Vision Transformer和Differential Transformer的鲁棒性，同时具备较低的计算开销。

**结论:** M-DGSA方法通过引入输入依赖的门控机制，有效提升了Transformer模型在噪声输入下的鲁棒性，并成功结合了生物神经回路中的侧抑制原理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Differential+Gated+Self-Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24054&send_immediately=true&force_search=false)

**原文摘要:** Transformers excel across a large variety of tasks but remain susceptible to
corrupted inputs, since standard self-attention treats all query-key
interactions uniformly. Inspired by lateral inhibition in biological neural
circuits and building on the recent use by the Differential Transformer's use
of two parallel softmax subtraction for noise cancellation, we propose
Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head
input-dependent gating to dynamically suppress attention noise. Each head
splits into excitatory and inhibitory branches whose dual softmax maps are
fused by a sigmoid gate predicted from the token embedding, yielding a
context-aware contrast enhancement. M-DGSA integrates seamlessly into existing
Transformer stacks with minimal computational overhead. We evaluate on both
vision and language benchmarks, demonstrating consistent robustness gains over
vanilla Transformer, Vision Transformer, and Differential Transformer
baselines. Our contributions are (i) a novel input-dependent gating mechanism
for self-attention grounded in lateral inhibition, (ii) a principled synthesis
of biological contrast-enhancement and self-attention theory, and (iii)
comprehensive experiments demonstrating noise resilience and cross-domain
applicability.

</details>


### [45] [Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs](https://arxiv.org/abs/2505.24055)
*Yilong Wang, Tianxiang Zhao, Zongyu Wu, Suhang Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于链接预测和身份保持学习的新型无监督领域自适应框架，用于解决图神经网络在跨领域节点分类中标签示范分布变化的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的无监督领域自适应方法直接应用独立同分布数据的技术，未能充分考虑图结构和GNN的消息传递机制，难以应对领域间的标签分布变化。

**方法:** 采用链接预测来连接源图和目标图，修改目标图输入以减少其与源域在嵌入空间中的偏差，并结合重构、适配损失设计身份保持学习目标。

**结果:** 实验结果表明，该框架在真实世界数据集上表现有效，能够提升跨领域节点分类任务的表现。

**结论:** 论文提出了一种新的无监督领域自适应框架，通过链接预测连接源图和目标图，改进了节点分类任务的性能，并设计了身份保持学习目标以防止目标图判别信息的丢失。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Source+and+Target+Domains+via+Link+Prediction+for+Unsupervised+Domain+Adaptation+on+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24055，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24055&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) have shown great ability for node classification
on graphs. However, the success of GNNs relies on abundant labeled data, while
obtaining high-quality labels is costly and challenging, especially for newly
emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a
classifier on the labeled source graph and adapts it to the unlabeled target
graph, is attracting increasing attention. Various approaches have been
proposed to alleviate the distribution shift between the source and target
graphs to facilitate the classifier adaptation. However, most of them simply
adopt existing UDA techniques developed for independent and identically
distributed data to gain domain-invariant node embeddings for graphs, which do
not fully consider the graph structure and message-passing mechanism of GNNs
during the adaptation and will fail when label distribution shift exists among
domains. In this paper, we proposed a novel framework that adopts link
prediction to connect nodes between source and target graphs, which can
facilitate message-passing between the source and target graphs and augment the
target nodes to have ``in-distribution'' neighborhoods with the source domain.
This strategy modified the target graph on the input level to reduce its
deviation from the source domain in the embedding space and is insensitive to
disproportional label distributions across domains. To prevent the loss of
discriminative information in the target graph, we further design a novel
identity-preserving learning objective, which guides the learning of the edge
insertion module together with reconstruction and adaptation losses.
Experimental results on real-world datasets demonstrate the effectiveness of
our framework.

</details>


### [46] [Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition](https://arxiv.org/abs/2505.24059)
*Sean Foley, Hong Nguyen, Jihwan Lee, Sudarsana Reddy Kadiri, Dani Byrd, Louis Goldstein, Shrikanth Narayanan*

**主要类别:** cs.LG

**AI概要:** 本研究利用单说话人的MRI数据集开发了音频、视频及多模态模型，以更准确地解析语音与发音之间的关系，并揭示了不同模态在音素识别中的独特作用。


<details>
  <summary>更多</summary>
  
**动机:** 以往的研究受限于依赖多说话人的语料库，这阻碍了声学与发音之间详细关系的学习，因为不同说话人之间存在显著差异。本研究旨在解决这一问题。

**方法:** 使用长格式单说话人MRI语料库开发了单模态音频、视频模型以及用于音素识别的多模态模型，旨在解缠并解释每个模态的贡献。

**结果:** 音频和多模态模型在音素识别任务中表现相似，但在发音位置的预测上出现分歧；模型解释表明其在潜在空间中的编码相似，但注意力权重揭示了声学和发音时间上的差异。

**结论:** 音频和多模态模型在不同语音方式类别上表现相似，但在发音位置上存在差异。此外，模型的潜在空间显示了语音空间的相似编码，但注意力权重突出了某些音素的声学和发音时间差异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+disentangling+the+contributions+of+articulation+and+acoustics+in+multimodal+phoneme+recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24059，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24059&send_immediately=true&force_search=false)

**原文摘要:** Although many previous studies have carried out multimodal learning with
real-time MRI data that captures the audio-visual kinematics of the vocal tract
during speech, these studies have been limited by their reliance on
multi-speaker corpora. This prevents such models from learning a detailed
relationship between acoustics and articulation due to considerable
cross-speaker variability. In this study, we develop unimodal audio and video
models as well as multimodal models for phoneme recognition using a long-form
single-speaker MRI corpus, with the goal of disentangling and interpreting the
contributions of each modality. Audio and multimodal models show similar
performance on different phonetic manner classes but diverge on places of
articulation. Interpretation of the models' latent space shows similar encoding
of the phonetic space across audio and multimodal models, while the models'
attention weights highlight differences in acoustic and articulatory timing for
certain phonemes.

</details>


### [47] [Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning](https://arxiv.org/abs/2505.24061)
*Jiashun Liu, Zihao Wu, Johan Obando-Ceron, Pablo Samuel Castro, Aaron Courville, Ling Pan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的神经元学习能力度量方法GraMa及相应的优化策略ReGraMa，用于解决深度强化学习中的神经元失活问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的tau-dormant neuron ratio在复杂架构中失去统计效力，需要一种更适用于高级RL代理的方法来维持神经元的学习能力。

**方法:** 提出了一种新的神经元活动度量方法GraMa，基于梯度幅度来衡量神经元的学习能力，并在此基础上开发了重置策略ReGraMa。

**结果:** GraMa能够揭示多种复杂架构中的神经元不活跃问题，并且ReGraMa在多个深度RL算法和基准测试中均提升了学习表现。

**结论:** GraMa是一种有效的量化神经元学习能力的指标，并通过ReGraMa提升了深度强化学习的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measure+gradients%2C+not+activations%21+Enhancing+neuronal+activity+in+deep+reinforcement+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24061&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning (RL) agents frequently suffer from neuronal
activity loss, which impairs their ability to adapt to new data and learn
continually. A common method to quantify and address this issue is the
tau-dormant neuron ratio, which uses activation statistics to measure the
expressive ability of neurons. While effective for simple MLP-based agents,
this approach loses statistical power in more complex architectures. To address
this, we argue that in advanced RL agents, maintaining a neuron's learning
capacity, its ability to adapt via gradient updates, is more critical than
preserving its expressive ability. Based on this insight, we shift the
statistical objective from activations to gradients, and introduce GraMa
(Gradient Magnitude Neural Activity Metric), a lightweight,
architecture-agnostic metric for quantifying neuron-level learning capacity. We
show that GraMa effectively reveals persistent neuron inactivity across diverse
architectures, including residual networks, diffusion models, and agents with
varied activation functions. Moreover, resetting neurons guided by GraMa
(ReGraMa) consistently improves learning performance across multiple deep RL
algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.

</details>


### [48] [Primal-Dual Neural Algorithmic Reasoning](https://arxiv.org/abs/2505.24067)
*Yu He, Ellen Vitercik*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于原始-对偶方法的神经算法推理框架，结合图神经网络和小规模实例的最优解，实现对复杂问题的高效推理，并在实际应用中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的NAR方法主要关注多项式时间可解问题的精确算法学习，而如何将其扩展到更难的问题仍然是一个挑战。

**方法:** 利用图神经网络与原始-对偶算法之间的二分表示，并结合小规模实例的最优解来增强模型的推理能力。

**结果:** 实验表明该模型不仅能够模拟近似算法，还能在多个任务上超越它们，并且能很好地推广到更大和分布外的图数据。

**结论:** 论文提出了一种基于原始-对偶范式的通用NAR框架，能够有效解决更复杂的问题，并且通过实验验证了其优越的推理能力和泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Primal-Dual+Neural+Algorithmic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24067，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24067&send_immediately=true&force_search=false)

**原文摘要:** Neural Algorithmic Reasoning (NAR) trains neural networks to simulate
classical algorithms, enabling structured and interpretable reasoning over
complex data. While prior research has predominantly focused on learning exact
algorithms for polynomial-time-solvable problems, extending NAR to harder
problems remains an open challenge. In this work, we introduce a general NAR
framework grounded in the primal-dual paradigm, a classical method for
designing efficient approximation algorithms. By leveraging a bipartite
representation between primal and dual variables, we establish an alignment
between primal-dual algorithms and Graph Neural Networks. Furthermore, we
incorporate optimal solutions from small instances to greatly enhance the
model's reasoning capabilities. Our empirical results demonstrate that our
model not only simulates but also outperforms approximation algorithms for
multiple tasks, exhibiting robust generalization to larger and
out-of-distribution graphs. Moreover, we highlight the framework's practical
utility by integrating it with commercial solvers and applying it to real-world
datasets.

</details>


### [49] [DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures](https://arxiv.org/abs/2505.24069)
*Yu He, Yingxi Li, Colin White, Ellen Vitercik*

**主要类别:** cs.LG

**AI概要:** 该论文提出了DSR-Bench，用于评估大语言模型结构性推理能力的新基准，发现当前模型在复杂结构和多维数据上表现不佳，凸显了实际应用中的关键缺陷。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基准测试主要集中在高层次的应用驱动评估上，缺乏对大语言模型结构性推理能力的孤立评估。为了填补这一空白，论文提出了专门针对结构性推理能力的基准DSR-Bench。

**方法:** 论文提出了DSR-Bench，一个包含20种数据结构、35种操作和4140个问题实例的新基准，通过层次化组织实现对推理限制的细粒度分析。评估流程完全自动化且确定性，消除了主观的人工或模型判断。

**结果:** 研究结果显示，经过指令微调的模型在基本的多属性和多跳推理任务上存在困难；尽管面向推理的模型表现更好，但它们在复杂和混合结构上的表现依然脆弱，最好的模型在挑战子集上的平均得分仅为47%。

**结论:** 论文得出结论，当前最先进的大语言模型在处理涉及复杂和混合结构的任务时仍然表现脆弱，并且在多维数据和自然语言任务描述下表现较差，这突出了实际应用中存在的重要差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DSR-Bench%3A+Evaluating+the+Structural+Reasoning+Abilities+of+LLMs+via+Data+Structures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24069，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24069&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly deployed for real-world tasks
that fundamentally involve data manipulation. A core requirement across these
tasks is the ability to perform structural reasoning--that is, to understand
and reason about data relationships. For example, customer requests require a
temporal ordering, which can be represented by data structures such as queues.
However, existing benchmarks primarily focus on high-level, application-driven
evaluations without isolating this fundamental capability. To address this gap,
we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning
capabilities through data structures, which provide interpretable
representations of data relationships. DSR-Bench includes 20 data structures,
35 operations, and 4,140 problem instances, organized hierarchically for
fine-grained analysis of reasoning limitations. Our evaluation pipeline is
fully automated and deterministic, eliminating subjective human or model-based
judgments. Its synthetic nature also ensures scalability and minimizes data
contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis
shows that instruction-tuned models struggle with basic multi-attribute and
multi-hop reasoning. Furthermore, while reasoning-oriented models perform
better, they remain fragile on complex and hybrid structures, with the best
model achieving an average score of only 47% on the challenge subset.
Crucially, models often perform poorly on multi-dimensional data and natural
language task descriptions, highlighting a critical gap for real-world
deployment.

</details>


### [50] [DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals](https://arxiv.org/abs/2505.24085)
*Alireza Jafari, Fereshteh Yousefirizi, Vahid Seydi*

**主要类别:** cs.LG

**AI概要:** 这项研究开发了一个创新性的混合模型，将深度学习与梯度提升技术结合起来，实现无需手动特征提取的心房颤动自动检测。


<details>
  <summary>更多</summary>
  
**动机:** 及时检测心房颤动对于降低中风相关发病率至关重要，但存在一定的健康风险，这就需要更有效的检测方法。

**方法:** 本研究采用了19层深度卷积自编码器（DCAE）与三种增强分类器-AdaBoost、XGBoost和LightGBM（LGBM）相结合的框架。

**结果:** DCAE-LGBM模型达到了95.20%的F1分数，99.99%的敏感性以及4秒的推理延迟，优于现有方法并符合临床部署要求。

**结论:** 该研究提出了一种结合无监督深度学习和梯度提升模型的新混合方法，用于改善心房颤动（AF）的检测。这种混合系统被定位为一种可靠的自动化AF检测工具，适用于临床环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepBoost-AF%3A+A+Novel+Unsupervised+Feature+Learning+and+Gradient+Boosting+Fusion+for+Robust+Atrial+Fibrillation+Detection+in+Raw+ECG+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24085&send_immediately=true&force_search=false)

**原文摘要:** Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with
elevated health risks, where timely detection is pivotal for mitigating
stroke-related morbidity. This study introduces an innovative hybrid
methodology integrating unsupervised deep learning and gradient boosting models
to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is
coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM
(LGBM)-to harness their complementary advantages while addressing individual
limitations. The proposed framework uniquely combines DCAE with gradient
boosting, enabling end-to-end AF identification devoid of manual feature
extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of
99.99%, and inference latency of four seconds, outperforming existing methods
and aligning with clinical deployment requirements. The DCAE integration
significantly enhances boosting models, positioning this hybrid system as a
reliable tool for automated AF detection in clinical settings.

</details>


### [51] [Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting](https://arxiv.org/abs/2505.24088)
*Chen Huang, Skyler Seto, Hadi Pouransari, Mehrdad Farajtabar, Raviteja Vemulapalli, Fartash Faghri, Oncel Tuzel, Barry-John Theobald, Josh Susskind*

**主要类别:** cs.LG

**AI概要:** 本文提出Proxy-FDA方法，通过显式保留特征空间中的结构知识来减少视觉基础模型微调时的概念遗忘。


<details>
  <summary>更多</summary>
  
**动机:** 微调基础模型时会出现其他任务上的概念遗忘问题，而现有的鲁棒微调方法在保存知识方面缺乏对特征邻域结构的显式认知。

**方法:** 提出了一种新的正则化方法Proxy-FDA，该方法通过使用最近邻图在预训练和微调特征空间之间进行特征分布对齐，并通过动态生成的信息代理来提高数据多样性，从而显式保留特征空间中的结构知识。

**结果:** 实验表明，Proxy-FDA显著减少了微调过程中的概念遗忘，并揭示了遗忘与分布距离度量之间的强相关性（相较于L2距离）。

**结论:** Proxy-FDA在各种微调设置和任务中显著减少了概念遗忘，并且发现遗忘与分布距离度量之间存在强相关性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Proxy-FDA%3A+Proxy-based+Feature+Distribution+Alignment+for+Fine-tuning+Vision+Foundation+Models+without+Forgetting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24088，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24088&send_immediately=true&force_search=false)

**原文摘要:** Vision foundation models pre-trained on massive data encode rich
representations of real-world concepts, which can be adapted to downstream
tasks by fine-tuning. However, fine-tuning foundation models on one task often
leads to the issue of concept forgetting on other tasks. Recent methods of
robust fine-tuning aim to mitigate forgetting of prior knowledge without
affecting the fine-tuning performance. Knowledge is often preserved by matching
the original and fine-tuned model weights or feature pairs. However, such
point-wise matching can be too strong, without explicit awareness of the
feature neighborhood structures that encode rich knowledge as well. We propose
a novel regularization method Proxy-FDA that explicitly preserves the
structural knowledge in feature space. Proxy-FDA performs Feature Distribution
Alignment (using nearest neighbor graphs) between the pre-trained and
fine-tuned feature spaces, and the alignment is further improved by informative
proxies that are generated dynamically to increase data diversity. Experiments
show that Proxy-FDA significantly reduces concept forgetting during
fine-tuning, and we find a strong correlation between forgetting and a
distributional distance metric (in comparison to L2 distance). We further
demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,
few-shot and continual tuning) and across different tasks like image
classification, captioning and VQA.

</details>


### [52] [Practical Bayes-Optimal Membership Inference Attacks](https://arxiv.org/abs/2505.24089)
*Marcus Lassila, Johan Östman, Khac-Hoang Ngo, Alexandre Graell i Amat*

**主要类别:** cs.LG

**AI概要:** 论文提出BASE和G-BASE两种高效的成员推理攻击方法，在图结构和非图数据上均表现出优越性能，并从理论上证明了BASE与RMIA的等价性及其贝叶斯最优性。


<details>
  <summary>更多</summary>
  
**动机:** 解决图结构数据和非图数据中的成员推理攻击问题，并探索在图环境下最优查询策略的关键问题。同时，旨在降低现有攻击方法的计算成本并提高性能。

**方法:** 基于Sablayrolles等人提出的贝叶斯决策理论框架，推导了针对图神经网络的节点级成员推理攻击的贝叶斯最优规则，并提出了BASE和G-BASE作为贝叶斯最优攻击的计算高效近似方法。

**结果:** G-BASE在图结构数据上的表现优于之前提出的基于分类器的节点级成员推理攻击方法；BASE在非图数据上表现与LiRA和RMIA相当或更优，且计算成本显著更低。

**结论:** BASE和RMIA在特定超参数设置下是等价的，这为RMIA攻击提供了基于贝叶斯最优的理论依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Practical+Bayes-Optimal+Membership+Inference+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24089&send_immediately=true&force_search=false)

**原文摘要:** We develop practical and theoretically grounded membership inference attacks
(MIAs) against both independent and identically distributed (i.i.d.) data and
graph-structured data. Building on the Bayesian decision-theoretic framework of
Sablayrolles et al., we derive the Bayes-optimal membership inference rule for
node-level MIAs against graph neural networks, addressing key open questions
about optimal query strategies in the graph setting. We introduce BASE and
G-BASE, computationally efficient approximations of the Bayes-optimal attack.
G-BASE achieves superior performance compared to previously proposed
classifier-based node-level MIA attacks. BASE, which is also applicable to
non-graph data, matches or exceeds the performance of prior state-of-the-art
MIAs, such as LiRA and RMIA, at a significantly lower computational cost.
Finally, we show that BASE and RMIA are equivalent under a specific
hyperparameter setting, providing a principled, Bayes-optimal justification for
the RMIA attack.

</details>


### [53] [A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke](https://arxiv.org/abs/2505.24101)
*Zhenran Xu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种用于预测急性中风患者住院时间的可解释集成模型，该模型在缺血性中风预测中的表现优于传统方法，但在出血性中风预测中的提升不明显。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习模型在预测急性中风住院时间方面表现不佳，通用性有限，并且忽略了系统级因素。研究旨在增强模型的效率、性能和可解释性。

**方法:** 研究通过改进预测因子并开发一个可解释的多层次堆叠集成模型来提高模型效率、性能和可解释性。数据来自澳大利亚的两年期中风基金会急性病审计，分别针对缺血性和出血性中风建立模型。使用基于相关性的方法进行特征选择，并通过AUC、校准曲线和SHAP图评估模型性能。

**结果:** 对于缺血性中风（N=12,575），该集成模型表现出优越的性能[AUC: 0.824]，并统计上优于逻辑回归[AUC: 0.805; P=0.0004]。然而，在出血性中风（N=1,970）中，该模型[AUC: 0.843]与逻辑回归[AUC: 0.828; P=0.136]相比没有统计学上的显著优势。SHAP分析确定了两种中风类型的共同预测因子，包括康复评估、尿失禁、中风单元护理、无法独立行走、物理治疗和中风护理协调员的参与。

**结论:** 论文得出结论，可解释的集成模型可以有效预测缺血性中风患者的延长住院时间，但对出血性中风的预测效果未显著优于逻辑回归。需要在更大的队列中进一步验证出血性中风的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+SHAP-based+explainable+multi-level+stacking+ensemble+learning+method+for+predicting+the+length+of+stay+in+acute+stroke，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24101&send_immediately=true&force_search=false)

**原文摘要:** Length of stay (LOS) prediction in acute stroke is critical for improving
care planning. Existing machine learning models have shown suboptimal
predictive performance, limited generalisability, and have overlooked
system-level factors. We aimed to enhance model efficiency, performance, and
interpretability by refining predictors and developing an interpretable
multi-level stacking ensemble model. Data were accessed from the biennial
Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models
were developed for ischaemic and haemorrhagic stroke separately. The outcome
was prolonged LOS (the LOS above the 75th percentile). Candidate predictors
(ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical,
and system domains. Feature selection with correlation-based approaches was
used to refine key predictors. The evaluation of models included discrimination
(AUC), calibration curves, and interpretability (SHAP plots). In ischaemic
stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in
haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance
[AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic
regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However,
the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform
logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for
haemorrhagic. SHAP analysis identified shared predictors for both types of
stroke: rehabilitation assessment, urinary incontinence, stroke unit care,
inability to walk independently, physiotherapy, and stroke care coordinators
involvement. An explainable ensemble model effectively predicted the prolonged
LOS in ischaemic stroke. Further validation in larger cohorts is needed for
haemorrhagic stroke.

</details>


### [54] [Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs](https://arxiv.org/abs/2505.24110)
*Sahil Rajesh Dhayalkar*

**主要类别:** cs.LG

**AI概要:** 本研究展示了如何使用前馈ReLU神经网络精确模拟非确定性有限自动机，证明了其在多个任务中的有效性，并建立了自动机理论与深度学习之间的联系。


<details>
  <summary>更多</summary>
  
**动机:** 动机是建立非确定性有限自动机与前馈神经网络之间的等价关系，从而将自动机理论与深度学习结合。

**方法:** 通过将自动机状态编码为二进制向量并将转换编码为稀疏线性层，使用ReLU激活函数模拟非确定性分支、子集构造和ε-闭包。

**结果:** 实验结果表明，在多个验证任务中，包括并行路径跟踪、符号子集构造等，模型实现了完美或接近完美的经验对齐。

**结论:** 本文结论表明，标准的前馈ReLU神经网络可以形式化地模拟非确定性有限自动机（NFA），从而在深度学习架构中实现了符号化的NFA仿真。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Networks+as+Universal+Finite-State+Machines%3A+A+Constructive+ReLU+Simulation+Framework+for+NFAs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24110，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24110&send_immediately=true&force_search=false)

**原文摘要:** We present a formal and constructive framework establishing the equivalence
between nondeterministic finite automata (NFAs) and standard feedforward ReLU
neural networks. By encoding automaton states as binary vectors and transitions
as sparse linear layers, we show that ReLU activations simulate
nondeterministic branching, subset construction, and $\epsilon$-closures in a
mathematically precise manner. Our core theoretical results prove that a
three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any
regular language accepted by an $n$-state NFA-without recurrence, memory, or
approximation. Furthermore, we show that gradient descent over
structure-preserving networks preserves symbolic semantics and acceptance
behavior. Extensive experiments across multiple validation tasks-including
parallel path tracking, symbolic subset construction, $\epsilon$-closure
convergence, acceptance classification, structural training invariants, and
functional equivalence-achieve perfect or near-perfect empirical alignment with
ground-truth automata. This work provides the first provably complete symbolic
simulation of NFAs within standard deep learning architectures, uniting
automata theory with neural computation through ReLU dynamics.

</details>


### [55] [AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits](https://arxiv.org/abs/2505.24138)
*Yichen Shi, Ze Zhang, Hongyang Wang, Zhuofu Tao, Zhongyi Li, Bingyu Chen, Yaxin Wang, Zhiping Yu, Ting-Jung Lin, Lei He*

**主要类别:** cs.LG

**AI概要:** 本研究提出了AMSbench基准测试工具，用于评估多模态大语言模型在模拟/混合信号电路设计中的表现，结果显示现有模型在复杂任务上仍有明显局限。


<details>
  <summary>更多</summary>
  
**动机:** 自动化模拟/混合信号电路设计仍然是一个长期挑战，而多模态大语言模型在此领域展现出潜力。然而，目前的研究通常仅针对孤立的任务进行评估，缺乏全面的基准测试来系统地衡量模型在各种AMS相关挑战中的能力。

**方法:** 提出了一种名为AMSbench的基准测试套件，用于评估多模态大语言模型在模拟/混合信号电路分析和设计中的表现。

**结果:** 通过评估八种主流模型（包括开源和专有解决方案）发现，当前的多模态大语言模型在复杂多模态推理和高级电路设计任务中表现出显著局限性。

**结论:** 当前的多模态大语言模型在复杂的多模态推理和高级电路设计任务中仍存在显著不足，需要进一步提升其对电路专业知识的理解和应用能力，以缩小与人类专家之间的性能差距，并实现AMS电路设计流程的全自动化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AMSbench%3A+A+Comprehensive+Benchmark+for+Evaluating+MLLM+Capabilities+in+AMS+Circuits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24138，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24138&send_immediately=true&force_search=false)

**原文摘要:** Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated
circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit
design has remained a longstanding challenge due to its difficulty and
complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer
promising potential for supporting AMS circuit analysis and design. However,
current research typically evaluates MLLMs on isolated tasks within the domain,
lacking a comprehensive benchmark that systematically assesses model
capabilities across diverse AMS-related challenges. To address this gap, we
introduce AMSbench, a benchmark suite designed to evaluate MLLM performance
across critical tasks including circuit schematic perception, circuit analysis,
and circuit design. AMSbench comprises approximately 8000 test questions
spanning multiple difficulty levels and assesses eight prominent models,
encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and
Gemini 2.5 Pro. Our evaluation highlights significant limitations in current
MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit
design tasks. These results underscore the necessity of advancing MLLMs'
understanding and effective application of circuit-specific knowledge, thereby
narrowing the existing performance gap relative to human expertise and moving
toward fully automated AMS circuit design workflows. Our data is released at
https://huggingface.co/datasets/wwhhyy/AMSBench

</details>


### [56] [Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction](https://arxiv.org/abs/2505.24145)
*Wilfried Genuist, Éric Savin, Filippo Gatti, Didier Clouteau*

**主要类别:** cs.LG

**AI概要:** 本研究开发了一种通用、高效的流体预测模型，适用于复杂的多场景环境。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高流体流动预测的质量并减少训练需求，同时支持灵活的应用于多种场景。

**方法:** 条件分数扩散模型结合能量约束和随机微分方程，通过简单的架构设计实现高效的采样和预测。

**结果:** 模型能够在不同流动条件下生成稳定且物理一致的预测结果，并展示了对多种流体力学任务的广泛适用性。

**结论:** 论文提出了一种基于分数的扩散模型，用于多场景流体流动预测，并证明其在复杂流体动力学数据集上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Autoregressive+regularized+score-based+diffusion+models+for+multi-scenarios+fluid+flow+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24145，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24145&send_immediately=true&force_search=false)

**原文摘要:** Building on recent advances in scientific machine learning and generative
modeling for computational fluid dynamics, we propose a conditional score-based
diffusion model designed for multi-scenarios fluid flow prediction. Our model
integrates an energy constraint rooted in the statistical properties of
turbulent flows, improving prediction quality with minimal training, while
enabling efficient sampling at low cost. The method features a simple and
general architecture that requires no problem-specific design, supports
plug-and-play enhancements, and enables fast and flexible solution generation.
It also demonstrates an efficient conditioning mechanism that simplifies
training across different scenarios without demanding a redesign of existing
models. We further explore various stochastic differential equation
formulations to demonstrate how thoughtful design choices enhance performance.
We validate the proposed methodology through extensive experiments on complex
fluid dynamics datasets encompassing a variety of flow regimes and
configurations. Results demonstrate that our model consistently achieves
stable, robust, and physically faithful predictions, even under challenging
turbulent conditions. With properly tuned parameters, it achieves accurate
results across multiple scenarios while preserving key physical and statistical
properties. We present a comprehensive analysis of stochastic differential
equation impact and discuss our approach across diverse fluid mechanics tasks.

</details>


### [57] [RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget](https://arxiv.org/abs/2505.24149)
*Adam Piaseczny, Md Kamran Chowdhury Shisher, Shiqiang Wang, Christopher G. Brinton*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 RCCDA 的动态模型更新策略，能够在资源受限环境下有效应对概念漂移问题，同时提升模型推理准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的漂移检测方法在资源受限环境中会产生较高的计算开销，且无法提供对资源使用或理论性能的严格保证，因此需要提出一种新的解决方案。

**方法:** 利用过去的损失信息和可调漂移阈值，通过 Lyapunov drift-plus-penalty 框架设计了一种轻量级策略，该策略基于可测量的累积损失阈值，有效限制了模型更新频率和成本。

**结果:** 实验结果表明，在三个领域泛化数据集上，RCCDA 在概念漂移的几种调度下均优于基线方法，并能够保持严格的资源约束。

**结论:** RCCDA 是一种适用于实时机器学习部署的动态模型更新策略，能够在严格遵守资源限制的同时，提高推理准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RCCDA%3A+Adaptive+Model+Updates+in+the+Presence+of+Concept+Drift+under+a+Constrained+Resource+Budget，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24149，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24149&send_immediately=true&force_search=false)

**原文摘要:** Machine learning (ML) algorithms deployed in real-world environments are
often faced with the challenge of adapting models to concept drift, where the
task data distributions are shifting over time. The problem becomes even more
difficult when model performance must be maintained under adherence to strict
resource constraints. Existing solutions often depend on drift-detection
methods that produce high computational overhead for resource-constrained
environments, and fail to provide strict guarantees on resource usage or
theoretical performance assurances. To address these shortcomings, we propose
RCCDA: a dynamic model update policy that optimizes ML training dynamics while
ensuring strict compliance to predefined resource constraints, utilizing only
past loss information and a tunable drift threshold. In developing our policy,
we analytically characterize the evolution of model loss under concept drift
with arbitrary training update decisions. Integrating these results into a
Lyapunov drift-plus-penalty framework produces a lightweight policy based on a
measurable accumulated loss threshold that provably limits update frequency and
cost. Experimental results on three domain generalization datasets demonstrate
that our policy outperforms baseline methods in inference accuracy while
adhering to strict resource constraints under several schedules of concept
drift, making our solution uniquely suited for real-time ML deployments.

</details>


### [58] [Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning](https://arxiv.org/abs/2505.24155)
*Ehtesamul Azim, Dongjie Wang, Tae Hyun Hwang, Yanjie Fu, Wei Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的两阶段基因选择框架，结合了统计方法和生物通路知识，利用多智能体强化学习提升了基因选择的预测准确性和生物学意义。


<details>
  <summary>更多</summary>
  
**动机:** 传统的特征选择方法虽然能够识别预测性基因，但往往忽略了复杂的生物通路和调控网络，导致结果不稳定且与生物学无关。如何在保持统计严谨性的同时整合生物通路知识是当前面临的主要挑战。

**方法:** 提出了一种两阶段框架：第一阶段采用基于通路的预筛选策略，结合多种统计方法和KEGG通路信息进行初步降维；第二阶段在多智能体强化学习框架中将基因建模为协作智能体，优化预测能力和生物学相关性。

**结果:** 在多个基因表达数据集上的实验表明，与传统方法相比，该方法在预测准确性和生物学可解释性方面均有显著提升。

**结论:** 该研究提出了一种结合生物通路知识和统计选择的基因选择新框架，通过多智能体强化学习方法，在多个基因表达数据集中显著提高了预测准确性和生物学可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Biological+Pathway+Guided+Gene+Selection+Through+Collaborative+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24155，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24155&send_immediately=true&force_search=false)

**原文摘要:** Gene selection in high-dimensional genomic data is essential for
understanding disease mechanisms and improving therapeutic outcomes.
Traditional feature selection methods effectively identify predictive genes but
often ignore complex biological pathways and regulatory networks, leading to
unstable and biologically irrelevant signatures. Prior approaches, such as
Lasso-based methods and statistical filtering, either focus solely on
individual gene-outcome associations or fail to capture pathway-level
interactions, presenting a key challenge: how to integrate biological pathway
knowledge while maintaining statistical rigor in gene selection? To address
this gap, we propose a novel two-stage framework that integrates statistical
selection with biological pathway knowledge using multi-agent reinforcement
learning (MARL). First, we introduce a pathway-guided pre-filtering strategy
that leverages multiple statistical methods alongside KEGG pathway information
for initial dimensionality reduction. Next, for refined selection, we model
genes as collaborative agents in a MARL framework, where each agent optimizes
both predictive power and biological relevance. Our framework incorporates
pathway knowledge through Graph Neural Network-based state representations, a
reward mechanism combining prediction performance with gene centrality and
pathway coverage, and collaborative learning strategies using shared memory and
a centralized critic component. Extensive experiments on multiple gene
expression datasets demonstrate that our approach significantly improves both
prediction accuracy and biological interpretability compared to traditional
methods.

</details>


### [59] [Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents](https://arxiv.org/abs/2505.24157)
*Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, Jungseul Ok*

**主要类别:** cs.LG

**AI概要:** 论文提出REPOA框架，通过增强知识鲁棒性和提升探索效率，解决开放世界中多步骤任务规划的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自主智能体在复杂、多变环境中进行规划存在较大挑战，主要问题包括对LLM内部知识的过度依赖和环境假设不现实。

**方法:** 引入了REPOA框架，包括自适应依赖学习、细粒度失败感知操作记忆以及基于难度的探索机制。

**结果:** 在两个开放世界测试平台上的评估表明，REPOA具备更强的鲁棒性和高效性，可以成功获取之前方法无法实现的目标。

**结论:** REPOA框架能够有效提升开放世界环境下智能体的规划能力和效率，解决了现有方法在知识依赖和探索效率方面的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Don%27t+Just+Follow+MLLM+Plans%3A+Robust+and+Efficient+Planning+for+Open-world+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24157&send_immediately=true&force_search=false)

**原文摘要:** Developing autonomous agents capable of mastering complex, multi-step tasks
in unpredictable, interactive environments presents a significant challenge.
While Large Language Models (LLMs) offer promise for planning, existing
approaches often rely on problematic internal knowledge or make unrealistic
environmental assumptions. Although recent work explores learning planning
knowledge, they still retain limitations due to partial reliance on external
knowledge or impractical setups. Indeed, prior research has largely overlooked
developing agents capable of acquiring planning knowledge from scratch,
directly in realistic settings. While realizing this capability is necessary,
it presents significant challenges, primarily achieving robustness given the
substantial risk of incorporating LLMs' inaccurate knowledge. Moreover,
efficiency is crucial for practicality as learning can demand prohibitive
exploration. In response, we introduce Robust and Efficient Planning for
Open-world Agents (REPOA), a novel framework designed to tackle these issues.
REPOA features three key components: adaptive dependency learning and
fine-grained failure-aware operation memory to enhance robustness to knowledge
inaccuracies, and difficulty-based exploration to improve learning efficiency.
Our evaluation in two established open-world testbeds demonstrates REPOA's
robust and efficient planning, showcasing its capability to successfully obtain
challenging late-game items that were beyond the reach of prior approaches.

</details>


### [60] [Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem](https://arxiv.org/abs/2505.24178)
*Katherine Tieu, Dongqi Fu, Jun Wu, Jingrui He*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于信息瓶颈方法的误差有界不变链接选择器，用于在时间图上实现鲁棒不变学习，以解决分布外问题。


<details>
  <summary>更多</summary>
  
**动机:** 基础模型时代，分布外（OOD）问题阻碍了AI的泛化能力，尤其是在违反独立同分布（IID）条件的关系数据（如图）中，这一问题更加严重。因此，本文旨在研究时间图中哪些组件对于标签是最不变且具有代表性的。

**方法:** 利用信息瓶颈（IB）方法，提出了一个误差有界不变链接选择器，在训练过程中能够区分不变组件和可变组件，从而提高深度学习模型在不同测试场景中的泛化能力。同时推导了一系列严格的可泛化优化函数，并结合任务特定损失函数进行训练。

**结果:** 实验表明，该方法在时间链接预测等任务中表现出色，预训练模型可以应用于现实世界的问题，例如引用推荐和商品推荐，并与当前最先进的（SOTA）方法进行了比较验证。

**结论:** 通过所提出的误差有界不变链接选择器，实现了在时间图上的鲁棒不变学习，有效提升了模型在不同测试环境下的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Invariant+Link+Selector+for+Spatial-Temporal+Out-of-Distribution+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24178，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24178&send_immediately=true&force_search=false)

**原文摘要:** In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,
the data discrepancy between the training environments and testing
environments, hinder AI generalization. Further, relational data like graphs
disobeying the Independent and Identically Distributed (IID) condition makes
the problem more challenging, especially much harder when it is associated with
time. Motivated by this, to realize the robust invariant learning over temporal
graphs, we want to investigate what components in temporal graphs are most
invariant and representative with respect to labels. With the Information
Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector
that can distinguish invariant components and variant components during the
training process to make the deep learning model generalizable for different
testing scenarios. Besides deriving a series of rigorous generalizable
optimization functions, we also equip the training with task-specific loss
functions, e.g., temporal link prediction, to make pretrained models solve
real-world application tasks like citation recommendation and merchandise
recommendation, as demonstrated in our experiments with state-of-the-art (SOTA)
methods. Our code is available at https://github.com/kthrn22/OOD-Linker.

</details>


### [61] [SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](https://arxiv.org/abs/2505.24179)
*Xiaodong Ji, Hailin Zhang, Fangcheng Fu, Bin Cui*

**主要类别:** cs.LG

**AI概要:** This paper proposes SALE, a fine-grained sparse attention method for accelerating long-context prefilling in large language models (LLMs) with minimal impact on model accuracy.


<details>
  <summary>更多</summary>
  
**动机:** The self-attention module becomes a bottleneck during the prefilling stage of inference for large language models (LLMs) due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation but typically perform coarse-grained inspection of the attention map, leading to considerable loss in model accuracy.

**方法:** SALE employs 4-bit quantized query-key products for fast and accurate fine-grained attention weight estimation, followed by block-sparse attention to accelerate prefilling computations. It also introduces the Relative Attention Score metric for importance evaluation of query-key pairs and implements a custom CUDA kernel optimized for hardware efficiency.

**结果:** Experiments on long-context benchmarks demonstrate that SALE outperforms existing approaches in terms of accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. The implementation of a custom CUDA kernel reduces additional overhead to approximately 11% of the full attention latency.

**结论:** SALE is a fine-grained sparse attention method that accelerates long-context prefilling in LLMs with negligible loss in model accuracy. It achieves significant speedups while maintaining model quality and can be seamlessly integrated into existing systems with minimal code modifications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SALE+%3A+Low-bit+Estimation+for+Efficient+Sparse+Attention+in+Long-context+LLM+Prefilling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24179，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24179&send_immediately=true&force_search=false)

**原文摘要:** Many advanced Large Language Model (LLM) applications require long-context
processing, but the self-attention module becomes a bottleneck during the
prefilling stage of inference due to its quadratic time complexity with respect
to sequence length. Existing sparse attention methods accelerate attention
computation by skipping less significant regions of the attention map. However,
these approaches typically perform coarse-grained inspection of the attention
map, rendering considerable loss in model accuracy. In this paper, we propose
SALE, a fine-grained sparse attention method that accelerates the long-context
prefilling stage of LLM with negligible loss in model accuracy. SALE achieves
fast and accurate fine-grained attention weight estimation through 4-bit
quantized query-key products, followed by block-sparse attention to accelerate
prefilling computations. For importance evaluation for query-key pairs, we
adopt our Relative Attention Score metric, which offers significantly higher
efficiency within our framework. We implement a custom CUDA kernel optimized
for our approach for hardware efficiency, reducing the additional overhead to
approximately 11% of the full attention latency. Notably, SALE requires no
parameter training and can be seamlessly integrated into existing systems with
trivial code modifications. Experiments on long-context benchmarks demonstrate
that our method outperforms existing approaches in accuracy-efficiency
trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences
longer than 64K while maintaining model quality.

</details>


### [62] [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.24183)
*Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于生成Verilog代码的强化学习框架CodeV-R1，通过解决验证环境、数据质量和计算成本问题，在自然语言转硬件描述语言任务中取得了显著成果。


<details>
  <summary>更多</summary>
  
**动机:** 将强化学习与可验证奖励（RLVR）扩展到电子设计自动化（EDA），特别是从自然语言生成硬件描述语言（如Verilog），面临缺乏自动且准确的验证环境、高质量数据稀缺以及计算成本过高的挑战。

**方法:** 开发了基于规则的测试平台生成器；提出了往返数据合成方法；采用两阶段“distill-then-RL”训练流程，并引入自适应DAPO算法降低训练成本。

**结果:** 模型CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上分别达到68.6%和72.9%的pass@1，超过之前最先进的结果12~20%，性能媲美甚至超越671B的DeepSeek-R1。

**结论:** CodeV-R1为Verilog生成任务提供了一个高效的RLVR框架，解决了自动化验证环境缺失、高质量数据稀缺和计算成本高昂的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CodeV-R1%3A+Reasoning-Enhanced+Verilog+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24183&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) trained via reinforcement learning with
verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,
automatable verification, such as software programming and mathematical
problems. Extending RLVR to electronic design automation (EDA), especially
automatically generating hardware description languages (HDLs) like Verilog
from natural-language (NL) specifications, however, poses three key challenges:
the lack of automated and accurate verification environments, the scarcity of
high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To
this end, we introduce CodeV-R1, an RLVR framework for training Verilog
generation LLMs. First, we develop a rule-based testbench generator that
performs robust equivalence checking against golden references. Second, we
propose a round-trip data synthesis method that pairs open-source Verilog
snippets with LLM-generated NL descriptions, verifies code-NL-code consistency
via the generated testbench, and filters out inequivalent examples to yield a
high-quality dataset. Third, we employ a two-stage "distill-then-RL" training
pipeline: distillation for the cold start of reasoning abilities, followed by
adaptive DAPO, our novel RLVR algorithm that can reduce training cost by
adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves
68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,
surpassing prior state-of-the-art by 12~20%, while matching or even exceeding
the performance of 671B DeepSeek-R1. We will release our model, training
pipeline, and dataset to facilitate research in EDA and LLM communities.

</details>


### [63] [Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling](https://arxiv.org/abs/2505.24185)
*Yipan Wei, Yuchen Zou, Yapeng Li, Bo Du*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为FedDEA的联邦解耦聚合方法，该方法能够在不交换本地数据的情况下，有效解决联邦多任务学习中的异构任务聚合问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦多任务学习方法大多侧重于为每个客户端构建个性化模型，无法支持将多个异构任务聚合到一个统一模型中，因此在实际场景中难以实现有效的联合训练。

**方法:** FedDEA基于本地更新的响应强度动态识别任务相关维度，并通过重新缩放增强其优化效果，从而实现多任务模型的集成。

**结果:** 实验结果表明，FedDEA可以轻松集成到各种主流联邦优化算法中，并在NYUD-V2和PASCAL-Context数据集上持续提供显著的整体性能提升。

**结论:** FedDEA是一个不依赖任务标签或架构修改的联邦解耦聚合方法，能够有效抑制跨任务干扰，并在统一全局模型中实现任务级解耦聚合，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Unified+Modeling+in+Federated+Multi-Task+Learning+via+Subspace+Decoupling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24185&send_immediately=true&force_search=false)

**原文摘要:** Federated Multi-Task Learning (FMTL) enables multiple clients performing
heterogeneous tasks without exchanging their local data, offering broad
potential for privacy preserving multi-task collaboration. However, most
existing methods focus on building personalized models for each client and
unable to support the aggregation of multiple heterogeneous tasks into a
unified model. As a result, in real-world scenarios where task objectives,
label spaces, and optimization paths vary significantly, conventional FMTL
methods struggle to achieve effective joint training. To address this
challenge, we propose FedDEA (Federated Decoupled Aggregation), an
update-structure-aware aggregation method specifically designed for multi-task
model integration. Our method dynamically identifies task-relevant dimensions
based on the response strength of local updates and enhances their optimization
effectiveness through rescaling. This mechanism effectively suppresses
cross-task interference and enables task-level decoupled aggregation within a
unified global model. FedDEA does not rely on task labels or architectural
modifications, making it broadly applicable and deployment-friendly.
Experimental results demonstrate that it can be easily integrated into various
mainstream federated optimization algorithms and consistently delivers
significant overall performance improvements on widely used NYUD-V2 and
PASCAL-Context. These results validate the robustness and generalization
capabilities of FedDEA under highly heterogeneous task settings.

</details>


### [64] [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
*Orlando Marquez Ayala, Patrice Bechard, Emily Chen, Maggie Baird, Jingfei Chen*

**主要类别:** cs.LG

**AI概要:** 该论文简要指出，在需要结构化输出的领域特定任务中，微调的小型语言模型（SLMs）相比提示使用的大型语言模型（LLMs）仍具有质量优势。


<details>
  <summary>更多</summary>
  
**动机:** 随着每个token成本的降低，微调小型语言模型（SLMs）用于实际应用的优势可能不再明显，因此本文探讨了SLMs是否仍具有质量优势。

**方法:** 该论文通过比较微调小型语言模型与使用提示的大型语言模型在生成JSON格式低代码工作流程任务上的表现来进行研究。

**结果:** 论文观察到虽然良好的提示可以产生合理的结果，但微调可以平均提高10%的质量。

**结论:** 论文得出结论，对于需要结构化输出的领域特定任务，小型语言模型（SLMs）通过微调仍然具有质量优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Tune+an+SLM+or+Prompt+an+LLM%3F+The+Case+of+Generating+Low-Code+Workflows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24189&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) such as GPT-4o can handle a wide range of
complex tasks with the right prompt. As per token costs are reduced, the
advantages of fine-tuning Small Language Models (SLMs) for real-world
applications -- faster inference, lower costs -- may no longer be clear. In
this work, we present evidence that, for domain-specific tasks that require
structured outputs, SLMs still have a quality advantage. We compare fine-tuning
an SLM against prompting LLMs on the task of generating low-code workflows in
JSON form. We observe that while a good prompt can yield reasonable results,
fine-tuning improves quality by 10% on average. We also perform systematic
error analysis to reveal model limitations.

</details>


### [65] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/abs/2505.24190)
*Lan-Cuong Nguyen, Quan Nguyen-Tri, Bang Tran Khanh, Dung D. Le, Long Tran-Thanh, Khoat Than*

**主要类别:** cs.LG

**AI概要:** 本文旨在解决少样本图像分类中的合成数据与真实数据分布不一致问题，提出了一种新算法结合原型学习，有效提高了模型的泛化能力和性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于标记训练样本稀缺，少样本图像分类仍然具有挑战性，因此尝试通过添加合成数据来缓解此问题。然而，由于真实数据和合成数据分布之间存在差异，这种方法可能面临性能下降的问题。为此，需要开发一种能够量化这种分布差异影响的理论框架，并据此找到实用的方法来提高模型泛化能力。

**方法:** 提出了一种新的基于理论的算法，该算法结合了原型学习方法，以优化数据划分和模型训练过程。

**结果:** 实验结果表明，所提出的算法在多个数据集上均优于当前最先进的方法，显示出其优越的性能。

**结论:** 论文得出结论，通过理论框架指导生成优良的合成样本并结合原型学习进行模型训练，可以有效弥补真实数据与合成数据之间的差距，从而提升少样本图像分类的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provably+Improving+Generalization+of+Few-Shot+Models+with+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24190&send_immediately=true&force_search=false)

**原文摘要:** Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [66] [Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback](https://arxiv.org/abs/2505.24193)
*Ofir Schlisselberg, Tal Lancewicki, Peter Auer, Yishay Mansour*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种在延迟环境中适用于随机和对抗性环境的最佳多臂老虎机算法。


<details>
  <summary>更多</summary>
  
**动机:** 已有方法在随机环境下与已知下界存在显著差距，因此需要一种更优的解决方案。

**方法:** 研究者们在最佳双世界框架下分析了具有对抗性选择延迟的多臂老虎机问题，并提出了一种能够适应这两种环境的新算法。

**结果:** 对于对抗性情况，该算法达到了最优对数因子的遗憾；在随机情况下，提供了与已知下界匹配的遗憾界限。

**结论:** 论文得出了一种新的算法，首次在延迟环境中同时匹配随机和对抗性设置下的下界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+Best-of-Both-Worlds+Regret+for+Bandits+with+Delayed+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24193&send_immediately=true&force_search=false)

**原文摘要:** We study the multi-armed bandit problem with adversarially chosen delays in
the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal
performance in both stochastic and adversarial environments. While prior work
has made progress toward this goal, existing algorithms suffer from significant
gaps to the known lower bounds, especially in the stochastic settings. Our main
contribution is a new algorithm that, up to logarithmic factors, matches the
known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of
$\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic
terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is
the cumulative delay. In the stochastic case, we provide a regret bound which
scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum
\Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$
and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to
simultaneously match the lower bounds in both stochastic and adversarial
regimes in delayed environment. Moreover, even beyond the BoBW setting, our
stochastic regret bound is the first to match the known lower bound under
adversarial delays, improving the second term over the best known result by a
factor of $K$.

</details>


### [67] [Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion](https://arxiv.org/abs/2505.24231)
*Md Shahnawaz, Bishwajit Prasad Gond, Durga Prasad Mohapatra*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种基于动态分析和深度学习的恶意软件分类方法，它能有效对抗多态性和变形恶意软件，并实现了高达98.36%的平均准确率。


<details>
  <summary>更多</summary>
  
**动机:** 传统的静态分析无法有效应对多态性和变形恶意软件，因为它们可以在不改变其行为的情况下改变其外观，这使得仅凭代码结构进行分析变得无效。因此，需要一种能够监控恶意软件运行时行为的动态检测方法。

**方法:** 论文提出了一种动态恶意软件分类框架，该框架在Windows可移植执行文件（PE）文件运行时提取API参数调用，将原始行为数据转换为时间模式，并利用magma颜色映射将这些模式转化为灰度图像，最终使用卷积神经网络（CNN）模型进行训练以获得区分性特征。

**结果:** 实验结果显示，该方法平均准确率达到98.36%，在分类不同类别的恶意软件和良性程序方面是有效的。

**结论:** 论文得出结论，通过整合动态分析和深度学习的方法，在恶意软件分类中取得了高准确率，并且对典型的逃避策略表现出显著的抵抗力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Malware+Classification+of+Windows+PE+Files+using+CNNs+and+Greyscale+Images+Derived+from+Runtime+API+Call+Argument+Conversion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24231，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24231&send_immediately=true&force_search=false)

**原文摘要:** Malware detection and classification remains a topic of concern for
cybersecurity, since it is becoming common for attackers to use advanced
obfuscation on their malware to stay undetected. Conventional static analysis
is not effective against polymorphic and metamorphic malware as these change
their appearance without modifying their behavior, thus defying the analysis by
code structure alone. This makes it important to use dynamic detection that
monitors malware behavior at runtime. In this paper, we present a dynamic
malware categorization framework that extracts API argument calls at the
runtime execution of Windows Portable Executable (PE) files. Extracting and
encoding the dynamic features of API names, argument return values, and other
relative features, we convert raw behavioral data to temporal patterns. To
enhance feature portrayal, the generated patterns are subsequently converted
into grayscale pictures using a magma colormap. These improved photos are used
to teach a Convolutional Neural Network (CNN) model discriminative features,
which allows for reliable and accurate malware classification. Results from
experiments indicate that our method, with an average accuracy of 98.36% is
effective in classifying different classes of malware and benign by integrating
dynamic analysis and deep learning. It not only achieves high classification
accuracy but also demonstrates significant resilience against typical evasion
strategies.

</details>


### [68] [Rethinking Continual Learning with Progressive Neural Collapse](https://arxiv.org/abs/2505.24254)
*Zheng Wang, Wanhao Yu, Li Yang, Sen Lin*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的持续学习框架ProNC，它通过逐步扩展ETF目标来解决灾难性遗忘问题，并通过distillation平衡旧类和新类的目标调整。


<details>
  <summary>更多</summary>
  
**动机:** 现有的使用固定全局ETF的持续学习方法存在不切实际性和性能受限的问题。

**方法:** 开发了一个新的CL框架，利用distillation在旧类目标调整和新类目标对齐之间取得平衡。

**结果:** 实验表明，该方法显著优于相关基线方法，同时保持了更高的灵活性、简单性和效率。

**结论:** 论文提出了一种新的持续学习框架ProNC，通过逐步扩展ETF目标来有效解决灾难性遗忘问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Continual+Learning+with+Progressive+Neural+Collapse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24254&send_immediately=true&force_search=false)

**原文摘要:** Continual Learning (CL) seeks to build an agent that can continuously learn a
sequence of tasks, where a key challenge, namely Catastrophic Forgetting,
persists due to the potential knowledge interference among different tasks. On
the other hand, deep neural networks (DNNs) are shown to converge to a terminal
state termed Neural Collapse during training, where all class prototypes
geometrically form a static simplex equiangular tight frame (ETF). These
maximally and equally separated class prototypes make the ETF an ideal target
for model learning in CL to mitigate knowledge interference. Thus inspired,
several studies have emerged very recently to leverage a fixed global ETF in
CL, which however suffers from key drawbacks, such as impracticability and
limited performance.To address these challenges and fully unlock the potential
of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework
that completely removes the need of a fixed global ETF in CL. Specifically,
ProNC progressively expands the ETF target in a principled way by adding new
class prototypes as vertices for new tasks, ensuring maximal separability
across all encountered classes with minimal shifts from the previous ETF. We
next develop a new CL framework by plugging ProNC into commonly used CL
algorithm designs, where distillation is further leveraged to balance between
target shifting for old classes and target aligning for new classes. Extensive
experiments show that our approach significantly outperforms related baselines
while maintaining superior flexibility, simplicity, and efficiency.

</details>


### [69] [On Fairness of Task Arithmetic: The Role of Task Vectors](https://arxiv.org/abs/2505.24262)
*Hiroki Naganuma, Kotaro Yoshida, Laura Gomezjurado Gonzalez, Takafumi Horie, Yuji Naraki, Ryotaro Shimizu*

**主要类别:** cs.LG

**AI概要:** 本文研究了使用任务向量进行模型编辑对公平性的影响，并提出了通过调整任务向量系数实现公平性控制的可能性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管任务向量在计算上具有优势，但它们可能会无意间影响模型的公平性，这在仇恨言论检测等敏感应用中存在风险。然而，任务算术的公平性含义仍未被充分探索，因此需要这项研究。

**方法:** 论文通过基准测试任务算术与全微调和低秩适应（LoRA）进行比较，并研究从易受仇恨言论影响的人口统计子组中合并任务向量的效果。

**结果:** 研究提供了关于模型编辑公平性影响的新见解，并发现可以通过调整任务向量系数来控制公平性结果，从而实现定制化的模型行为。

**结论:** 论文得出结论，任务向量的修改对公平性有重要影响，并为实现具有公平意识和负责任的模型编辑实践奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Fairness+of+Task+Arithmetic%3A+The+Role+of+Task+Vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24262&send_immediately=true&force_search=false)

**原文摘要:** Model editing techniques, particularly task arithmetic using task vectors,
have shown promise in efficiently modifying pre-trained models through
arithmetic operations like task addition and negation. Despite computational
advantages, these methods may inadvertently affect model fairness, creating
risks in sensitive applications like hate speech detection. However, the
fairness implications of task arithmetic remain largely unexplored, presenting
a critical gap in the existing literature. We systematically examine how
manipulating task vectors affects fairness metrics, including Demographic
Parity and Equalized Odds. To rigorously assess these effects, we benchmark
task arithmetic against full fine-tuning, a costly but widely used baseline,
and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning
method. Additionally, we explore merging task vectors from models fine-tuned on
demographic subgroups vulnerable to hate speech, investigating whether fairness
outcomes can be controlled by adjusting task vector coefficients, potentially
enabling tailored model behavior. Our results offer novel insights into the
fairness implications of model editing and establish a foundation for
fairness-aware and responsible model editing practices.

</details>


### [70] [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
*James R. Golden*

**主要类别:** cs.LG

**AI概要:** 该论文展示了如何将大语言模型的推理过程转化为等效的线性系统，从而揭示其内部的语义结构和工作机制。


<details>
  <summary>更多</summary>
  
**动机:** 探索大语言模型在不修改权重或改变输出预测的情况下是否可以映射到等效线性系统，从而揭示其内部工作机制。

**方法:** 通过扩展图像扩散模型中的技术，对给定输入序列的梯度计算进行策略性修改，使得模型的雅可比矩阵几乎精确地再现前向预测的线性系统。

**结果:** 研究表明多个大语言模型（如Llama 3, Gemma 3, Qwen 3等）在低维子空间中运行，并且可以通过奇异值分解观察到与最可能输出标记相关的语义概念。

**结论:** 现代大语言模型可以通过几乎精确的局部线性分解来解释，这为理解其内部表示和语义结构提供了新方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+are+Locally+Linear+Mappings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24293，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24293&send_immediately=true&force_search=false)

**原文摘要:** We demonstrate that the inference operations of several open-weight large
language models (LLMs) can be mapped to an exactly equivalent linear system for
an input sequence without modifying the model weights or altering output
predictions. Extending techniques from image diffusion models that exhibit
local or piecewise linearity, we strategically alter the gradient computation
with respect to a given input sequence for a next-token prediction such that
the Jacobian of the model nearly exactly reproduces the forward prediction with
a linear system. We demonstrate this approach across models (Llama 3, Gemma 3,
Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show
through the singular value decomposition of the detached Jacobian that these
LLMs operate in extremely low-dimensional subspaces where many of the largest
singular vectors decode to concepts related to the most-likely output token.
This approach also allows us to examine the operation of each successive layer
(and its attention and MLP components) as nearly-exact linear systems and
observe the emergence of semantic concepts. Despite their expressive power and
global nonlinearity, modern LLMs can be interpreted through nearly-exact
locally linear decompositions that provide insights into their internal
representations and reveal interpretable semantic structures in the next-token
prediction process.

</details>


### [71] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/abs/2505.24298)
*Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu*

**主要类别:** cs.LG

**AI概要:** 本文介绍AReaL，一种用于大语言模型的完全异步强化学习系统，有效提升训练速度与GPU利用率，并在推理任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大规模RL系统由于同步机制导致严重的系统级效率低下，生成必须等待整个批次中最长输出完成，造成GPU利用率低。

**方法:** 提出了一种完全异步的RL系统，将生成与训练完全解耦，同时引入了一系列系统级优化来提高GPU利用率并采用陈旧增强的PPO变体来处理过时样本。

**结果:** 实验表明，AReaL在相同数量GPU的情况下，相比现有最佳同步系统，训练速度提高了2.57倍，且最终性能匹配甚至更优。

**结论:** AReaL通过完全异步的强化学习方法，在数学和代码推理基准上实现了高达2.57倍的训练加速，并且最终性能相当甚至更好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AReaL%3A+A+Large-Scale+Asynchronous+Reinforcement+Learning+System+for+Language+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24298，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24298&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has become a trending paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are synchronous
by alternating generation and training in a batch setting, where the rollouts
in each training batch are generated by the same (or latest) model. This
stabilizes RL training but suffers from severe system-level inefficiency.
Generation must wait until the longest output in the batch is completed before
model update, resulting in GPU underutilization. We present AReaL, a
\emph{fully asynchronous} RL system that completely decouples generation from
training. Rollout workers in AReaL continuously generate new outputs without
waiting, while training workers update the model whenever a batch of data is
collected. AReaL also incorporates a collection of system-level optimizations,
leading to substantially higher GPU utilization. To stabilize RL training,
AReaL balances the workload of rollout and training workers to control data
staleness, and adopts a staleness-enhanced PPO variant to better handle
outdated training samples. Extensive experiments on math and code reasoning
benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training
speedup} compared to the best synchronous systems with the same number of GPUs
and matched or even improved final performance. The code of AReaL is available
at https://github.com/inclusionAI/AReaL/.

</details>


### [72] [On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective](https://arxiv.org/abs/2505.24313)
*Gengze Xu, Wei Yao, Ziqiao Wang, Yong Liu*

**主要类别:** cs.LG

**AI概要:** 本文分析了弱到强泛化的理论基础，并提出了一种新的损失函数来提高学生模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 最近的研究将弱到强泛化归因于学生和教师模型之间的预测不匹配，但缺乏理论支持。

**方法:** 通过Bregman散度的广义偏差-方差分解，理论上研究学生和教师模型之间的预期总体风险差距。

**结果:** 研究表明，学生模型如果具有比教师更大的容量，可以收敛到后验均值教师，从而更容易产生弱到强泛化。

**结论:** 本文得出弱到强泛化现象的理论解释，并提出通过减少学生模型对教师监督的过拟合以及降低其预测熵可以进一步促进这一现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Emergence+of+Weak-to-Strong+Generalization%3A+A+Bias-Variance+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24313，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24313&send_immediately=true&force_search=false)

**原文摘要:** Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong
student model, trained on a dataset labeled by a weak teacher, ultimately
outperforms the teacher on the target task. Recent studies attribute this
performance gain to the prediction misfit between the student and teacher
models. In this work, we theoretically investigate the emergence of W2SG
through a generalized bias-variance decomposition of Bregman divergence.
Specifically, we show that the expected population risk gap between the student
and teacher is quantified by the expected misfit between the two models. While
this aligns with previous results, our analysis removes several restrictive
assumptions, most notably, the convexity of the student's hypothesis class,
required in earlier works. Moreover, we show that W2SG is more likely to emerge
when the student model approximates its posterior mean teacher, rather than
mimicking an individual teacher. Using a concrete example, we demonstrate that
if the student model has significantly larger capacity than the teacher, it can
indeed converge to this posterior mean. Our analysis also suggests that
avoiding overfitting to the teacher's supervision and reducing the entropy of
student's prediction further facilitate W2SG. In addition, we show that the
reverse cross-entropy loss, unlike the standard forward cross-entropy, is less
sensitive to the predictive uncertainty of the teacher. Finally, we empirically
verify our theoretical insights and demonstrate that incorporating the reverse
cross-entropy loss consistently improves student performance.

</details>


### [73] [ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving](https://arxiv.org/abs/2505.24317)
*Yongming Chen, Miner Chen, Liewen Liao, Mingyang Jiang, Xiang Zuo, Hengrui Zhang, Yuchen Xi, Songan Zhang*

**主要类别:** cs.LG

**AI概要:** 本文通过构建责任导向型奖励函数，利用知识图谱与生成技术提升自动驾驶中强化学习的效果和规则遵守能力。


<details>
  <summary>更多</summary>
  
**动机:** 传统的强化学习方法在自动驾驶中因依赖手动设计的奖励函数，在复杂场景中的效果有限，因此需要一种更有效的奖励机制。

**方法:** 引入了交通规则知识图谱，并结合视觉-语言模型和检索增强生成技术，实现奖励分配的自动化。

**结果:** 实验验证表明，该方法显著提高了事故责任分配的准确性，并有效减少了智能体在交通事故中的责任。

**结论:** 本文提出了一种基于交通规则知识图谱的责任导向型奖励函数，以增强自动驾驶中强化学习的决策能力和交通规则遵守能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ROAD%3A+Responsibility-Oriented+Reward+Design+for+Reinforcement+Learning+in+Autonomous+Driving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24317，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24317&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) in autonomous driving employs a trial-and-error
mechanism, enhancing robustness in unpredictable environments. However,
crafting effective reward functions remains challenging, as conventional
approaches rely heavily on manual design and demonstrate limited efficacy in
complex scenarios. To address this issue, this study introduces a
responsibility-oriented reward function that explicitly incorporates traffic
regulations into the RL framework. Specifically, we introduced a Traffic
Regulation Knowledge Graph and leveraged Vision-Language Models alongside
Retrieval-Augmented Generation techniques to automate reward assignment. This
integration guides agents to adhere strictly to traffic laws, thus minimizing
rule violations and optimizing decision-making performance in diverse driving
conditions. Experimental validations demonstrate that the proposed methodology
significantly improves the accuracy of assigning accident responsibilities and
effectively reduces the agent's liability in traffic incidents.

</details>


### [74] [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/abs/2505.24324)
*Ivan Petrukha, Yana Kurliak, Nataliia Stulova*

**主要类别:** cs.LG

**AI概要:** 该论文指出现有多语言代码生成基准测试在Swift上的局限性，提出新的Swift专用基准测试SwiftEval，并发现其对小型模型有显著影响。


<details>
  <summary>更多</summary>
  
**动机:** 由于现有基准测试主要面向Python，难以高质量评估其他编程语言如Swift，因此需要一个专注于Swift的基准测试。

**方法:** 通过分析现有的多语言基准测试，发现Swift部分存在不足，采用质量优先的方法创建了包含28个手工设计问题的SwiftEval基准测试，并对44种流行的代码LLM进行了评估。

**结果:** 结果显示，在需要语言特定功能的问题上，LLM得分显著下降，尤其是较小的模型更为明显。

**结论:** 论文得出结论，SwiftEval对于评估LLM在Swift编程语言方面的特定功能表现至关重要，特别是对小型模型影响显著。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwiftEval%3A+Developing+a+Language-Specific+Benchmark+for+LLM-generated+Code+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24324，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24324&send_immediately=true&force_search=false)

**原文摘要:** In recent years, large language models (LLMs) have showcased significant
advancements in code generation. However, most evaluation benchmarks are
primarily oriented towards Python, making it difficult to evaluate other
programming languages, such as Swift, with high quality. By examining widely
established multilingual benchmarks like HumanEval-XL and MultiPL-E, we
identified critical issues specific to their Swift components, making them
insufficient or even irrelevant for assessing LLM coding capabilities on Swift.
Unlike these existing approaches, which prioritize rapid scaling and
generalization by automatically translating Python-centric benchmarks with
LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the
first Swift-oriented benchmark consisting of 28 carefully hand-crafted
problems, and evaluate 44 popular Code LLMs on it. Our results show significant
LLM scores drop for problems requiring language-specific features, most
noticeable in the models of smaller sizes.

</details>


### [75] [Cartan Networks: Group theoretical Hyperbolic Deep Learning](https://arxiv.org/abs/2505.24353)
*Federico Milanesio, Matteo Santoro, Pietro G. Fré, Guido Sanguinetti*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种基于双曲空间结构的新型深度学习算法Cartan网络，并展示了其在数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 通过利用超球空间的度量特性，开发高效且信息丰富的层次数据嵌入方法。

**方法:** 利用双曲空间的可解群结构，将群同态与度量保持微分同胚交错使用，提出了一种新的超球深度学习算法。

**结果:** 提出了一种名为Cartan网络的新算法，并在多个基准数据集上展示了其潜力。

**结论:** Cartan网络为超球深度学习提供了一种新的算法类别，并在各种基准数据集上显示出有希望的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cartan+Networks%3A+Group+theoretical+Hyperbolic+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24353&send_immediately=true&force_search=false)

**原文摘要:** Hyperbolic deep learning leverages the metric properties of hyperbolic spaces
to develop efficient and informative embeddings of hierarchical data. Here, we
focus on the solvable group structure of hyperbolic spaces, which follows
naturally from their construction as symmetric spaces. This dual nature of Lie
group and Riemannian manifold allows us to propose a new class of hyperbolic
deep learning algorithms where group homomorphisms are interleaved with
metric-preserving diffeomorphisms. The resulting algorithms, which we call
Cartan networks, show promising results on various benchmark data sets and open
the way to a novel class of hyperbolic deep learning architectures.

</details>


### [76] [ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](https://arxiv.org/abs/2505.24357)
*Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, Xiaokang Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为ReCalKV的KV缓存压缩方法，通过分别对Keys和Values进行优化压缩，以最小的性能损失实现了高效的长上下文推理。


<details>
  <summary>更多</summary>
  
**动机:** 现有的KV缓存压缩方法往往引入额外计算或在高压缩比下表现显著下降，因此需要一种更高效的压缩方法。

**方法:** 开发了针对Keys和Values的不同压缩策略，包括Head-wise Similarity-aware Reordering (HSR) 和 Offline Calibration and Matrix Fusion (OCMF)。

**结果:** 实验表明，与现有的低秩压缩方法相比，ReCalKV表现更好，在保持准确性的同时实现了更高的压缩率。

**结论:** ReCalKV是一种有效的KV缓存压缩方法，能够在保持最小性能损失的同时实现高压缩比，有助于高效的长上下文推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReCalKV%3A+Low-Rank+KV+Cache+Compression+via+Head+Reordering+and+Offline+Calibration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24357，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24357&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have achieved remarkable performance, yet their
capability on long-context reasoning is often constrained by the excessive
memory required to store the Key-Value (KV) cache. This makes KV cache
compression an essential step toward enabling efficient long-context reasoning.
Recent methods have explored reducing the hidden dimensions of the KV cache,
but many introduce additional computation through projection layers or suffer
from significant performance degradation under high compression ratios. To
address these challenges, we propose ReCalKV, a post-training KV cache
compression method that reduces the hidden dimensions of the KV cache. We
develop distinct compression strategies for Keys and Values based on their
different roles and varying importance in the attention mechanism. For Keys, we
propose Head-wise Similarity-aware Reordering (HSR), which clusters similar
heads and applies grouped SVD to the key projection matrix, reducing additional
computation while preserving accuracy. For Values, we propose Offline
Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra
computational overhead. Experiments show that ReCalKV outperforms existing
low-rank compression methods, achieving high compression ratios with minimal
performance loss. Code is available at:
https://github.com/XIANGLONGYAN/ReCalKV.

</details>


### [77] [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/abs/2505.24369)
*Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Adversarial Preference Learning (APL)的迭代对抗训练方法，以解决现代语言模型在使用基于人类反馈的强化学习(RLHF)时的安全行为问题。


<details>
  <summary>更多</summary>
  
**动机:** 现代语言模型在鼓励安全行为方面通常依赖于人类反馈的强化学习(RLHF)，但由于人工标注效率低、潜在对抗攻击多样性以及反馈偏差和奖励黑客风险等原因，它们仍然容易受到攻击。

**方法:** 引入了Adversarial Preference Learning (APL)，包括三个关键创新：(1) 基于模型内在偏好概率的直接有害性度量；(2) 合成输入特定对抗变化的条件生成攻击者；(3) 具有自动化闭环反馈的迭代框架，通过漏洞发现和缓解实现持续适应。

**结果:** 实验表明，APL显著增强了Mistral-7B-Instruct-v0.3模型的鲁棒性，在GPT-4o评估中达到了83.33%的无害胜率，将LLaMA-Guard测量的有害输出从5.88%降低到0.43%，并根据HarmBench降低了65%的攻击成功率。此外，APL保持了竞争力，MT-Bench得分为6.59，LC-WinRate为46.52%对基础模型。

**结论:** APL提供了一种有效的解决方案，解决了RLHF的局限性，并提高了语言模型的安全性和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Preference+Learning+for+Robust+LLM+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24369，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24369&send_immediately=true&force_search=false)

**原文摘要:** Modern language models often rely on Reinforcement Learning from Human
Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to
adversarial attacks due to three key limitations: (1) the inefficiency and high
cost of human annotation, (2) the vast diversity of potential adversarial
attacks, and (3) the risk of feedback bias and reward hacking. To address these
challenges, we introduce Adversarial Preference Learning (APL), an iterative
adversarial training method incorporating three key innovations. First, a
direct harmfulness metric based on the model's intrinsic preference
probabilities, eliminating reliance on external assessment. Second, a
conditional generative attacker that synthesizes input-specific adversarial
variations. Third, an iterative framework with automated closed-loop feedback,
enabling continuous adaptation through vulnerability discovery and mitigation.
Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly
enhances robustness, achieving 83.33% harmlessness win rate over the base model
(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured
by LLaMA-Guard), and lowering attack success rate by up to 65% according to
HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score
of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against
the base model.

</details>


### [78] [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
*Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy*

**主要类别:** cs.LG

**AI概要:** 本研究将稀疏自编码器（SAEs）和ITDA应用于大型文本到图像扩散模型Flux 1，发现SAEs在重构和解释性上优于传统方法，并可用于引导图像生成。


<details>
  <summary>更多</summary>
  
**动机:** 探索稀疏自编码器在语言模型激活分解中的应用效果，并尝试将其与ITDA方法结合以提升模型解释性和控制能力。

**方法:** 将稀疏自编码器（SAEs）和推断时激活分解（ITDA）应用于大型文本到图像扩散模型Flux 1，并引入视觉自动化解释管道来评估嵌入的可解释性。

**结果:** SAEs能够准确地重构残差流嵌入，并在解释性方面优于MLP神经元；通过SAE特征可以实现对图像生成的引导；ITDA在解释性方面与SAEs表现相当。

**结论:** 稀疏自编码器（SAEs）和ITDA在Flux 1模型中展示了良好的可解释性和重构能力，SAEs在嵌入解释性方面优于MLP神经元，并且可以通过激活加法引导图像生成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpreting+Large+Text-to-Image+Diffusion+Models+with+Dictionary+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24360&send_immediately=true&force_search=false)

**原文摘要:** Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.

</details>


### [79] [Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer](https://arxiv.org/abs/2505.24378)
*Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao*

**主要类别:** cs.LG

**AI概要:** 本文提出了M3DT，一种基于混合专家的离线多任务强化学习框架，解决了大规模任务下的性能退化问题，并实现了高效训练和卓越的任务可扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Transformer架构已在离线多任务强化学习中得到应用，但当任务数量极大时，现有方法面临显著挑战。

**方法:** 提出了一种新的混合专家（MoE）框架，增强了决策Transformer骨干网络，并引入了一个三阶段训练机制以提高性能。

**结果:** 实验结果显示，M3DT不仅在固定任务数量下随模型扩展提升性能，而且在任务数量增加到160个时仍保持卓越表现。

**结论:** M3DT通过引入混合专家框架，有效解决了离线多任务强化学习中任务可扩展性的挑战，并展示了其在大量任务上的优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mastering+Massive+Multi-Task+Reinforcement+Learning+via+Mixture-of-Expert+Decision+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24378，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24378&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advancements in offline multi-task reinforcement learning
(MTRL) have harnessed the powerful capabilities of the Transformer
architecture, most approaches focus on a limited number of tasks, with scaling
to extremely massive tasks remaining a formidable challenge. In this paper, we
first revisit the key impact of task numbers on current MTRL method, and
further reveal that naively expanding the parameters proves insufficient to
counteract the performance degradation as the number of tasks escalates.
Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE)
framework that tackles task scalability by further unlocking the model's
parameter scalability. Specifically, we enhance both the architecture and the
optimization of the agent, where we strengthen the Decision Transformer (DT)
backbone with MoE to reduce task load on parameter subsets, and introduce a
three-stage training mechanism to facilitate efficient training with optimal
performance. Experimental results show that, by increasing the number of
experts, M3DT not only consistently enhances its performance as model expansion
on the fixed task numbers, but also exhibits remarkable task scalability,
successfully extending to 160 tasks with superior performance.

</details>


### [80] [Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm](https://arxiv.org/abs/2505.24365)
*Vardhan Shorewala, Shivam Shorewala*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种用于聚类优化和异常检测的新方法，通过迭代减少簇内方差来实现比标准k均值算法更优的性能，并在多个评估数据集上展示了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服标准k均值算法在生成更紧密的聚类方面的局限性，并提供一种统一的方法来进行聚类优化和异常检测，从而提高数据分析的准确性和效率。

**方法:** 论文提出了一种新颖的迭代算法，该算法不断减少N个聚类的簇内方差，直到达到全局最小值。使用轮廓系数、Calinski-Harabasz指数和Davies-Bouldin指数等内在度量对方法进行评估，并通过识别导致显著方差增加的点将其扩展到异常检测。外部验证采用了Jaccard相似性得分、V测度和F1测度。

**结果:** 结果表明，在合成数据集和UCI Wine Quality数据集上，方差分别减少了18.7%和88.1%，并且在UCI Wine Quality数据集上的准确性和F1分数分别提高了22.5%和20.8%。

**结论:** 论文提出了一种新颖的统一方法用于数据集中的聚类优化和异常检测。结果表明，新方法在减少方差和提高准确性和F1分数方面优于标准k均值算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Anomaly+Detection+and+Improvement+of+Clusters+using+Enhanced+K-Means+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24365&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a unified approach to cluster refinement and anomaly
detection in datasets. We propose a novel algorithm that iteratively reduces
the intra-cluster variance of N clusters until a global minimum is reached,
yielding tighter clusters than the standard k-means algorithm. We evaluate the
method using intrinsic measures for unsupervised learning, including the
silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and
extend it to anomaly detection by identifying points whose assignment causes a
significant variance increase. External validation on synthetic data and the
UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity
score, V-measure, and F1 score. Results show variance reductions of 18.7% and
88.1% on the synthetic and Wine Quality datasets, respectively, along with
accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality
dataset.

</details>


### [81] [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379)
*Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种针对精确遗忘方法的新数据提取攻击，指出即使从头开始重新训练模型也无法完全保护隐私，并建议评估遗忘方法时应考虑更广泛的威胁模型。


<details>
  <summary>更多</summary>
  
**动机:** 由于大型语言模型可能无意中包含有害或敏感个人信息，因此需要研究遗忘方法以消除特定数据的影响。

**方法:** 论文提出了一种新的数据提取攻击方法，利用预遗忘和后遗忘模型来揭示被删除数据的分布模式，并结合模型引导和令牌过滤策略提高提取成功率。

**结果:** 该论文展示了一种能够挑战当前认为最标准且最安全的精确遗忘方法的数据提取攻击，并表明其攻击方法显著提高了提取成功率。

**结论:** 论文得出结论，即即使是从头开始重新训练模型的精确遗忘方法也无法完全保护隐私，并建议在评估遗忘方法时应考虑更广泛的威胁模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+the+Gold+Standard%3A+Extracting+Forgotten+Data+under+Exact+Unlearning+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24379，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24379&send_immediately=true&force_search=false)

**原文摘要:** Large language models are typically trained on datasets collected from the
web, which may inadvertently contain harmful or sensitive personal information.
To address growing privacy concerns, unlearning methods have been proposed to
remove the influence of specific data from trained models. Of these, exact
unlearning -- which retrains the model from scratch without the target data --
is widely regarded the gold standard, believed to be robust against
privacy-related attacks. In this paper, we challenge this assumption by
introducing a novel data extraction attack that compromises even exact
unlearning. Our method leverages both the pre- and post-unlearning models: by
guiding the post-unlearning model using signals from the pre-unlearning model,
we uncover patterns that reflect the removed data distribution. Combining model
guidance with a token filtering strategy, our attack significantly improves
extraction success rates -- doubling performance in some cases -- across common
benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our
attack's effectiveness on a simulated medical diagnosis dataset to highlight
real-world privacy risks associated with exact unlearning. In light of our
findings, which suggest that unlearning may, in a contradictory way, increase
the risk of privacy leakage, we advocate for evaluation of unlearning methods
to consider broader threat models that account not only for post-unlearning
models but also for adversarial access to prior checkpoints.

</details>


### [82] [Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation](https://arxiv.org/abs/2505.24415)
*Andreas Spilz, Heiko Oppel, Michael Munz*

**主要类别:** cs.LG

**AI概要:** 本研究开发了一种创新的数据增强技术，通过肌肉骨骼模拟生成逼真的IMU运动数据，解决了深度学习在物理治疗运动评估中的数据不足问题，显著提升了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度学习模型在评估惯性测量单元（IMU）捕捉的动作时面临数据有限、类别不平衡和标签模糊的问题，因此需要一种有效的方法来改进动作质量评估。

**方法:** 提出了一种新的数据增强方法，使用肌肉骨骼模拟生成IMU数据，并结合运动轨迹的系统修改进行分析。

**结果:** 实验结果表明，该方法生成的数据与真实世界数据相似，显著提高了神经网络模型的分类准确性和泛化能力，特别是在患者特异性微调场景中效果明显。

**结论:** 论文得出结论，所提出的增强方法在物理治疗运动评估中克服了深度学习应用面临的常见挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Boosting+Automatic+Exercise+Evaluation+Through+Musculoskeletal+Simulation-Based+IMU+Data+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24415&send_immediately=true&force_search=false)

**原文摘要:** Automated evaluation of movement quality holds significant potential for
enhancing physiotherapeutic treatments and sports training by providing
objective, real-time feedback. However, the effectiveness of deep learning
models in assessing movements captured by inertial measurement units (IMUs) is
often hampered by limited data availability, class imbalance, and label
ambiguity. In this work, we present a novel data augmentation method that
generates realistic IMU data using musculoskeletal simulations integrated with
systematic modifications of movement trajectories. Crucially, our approach
ensures biomechanical plausibility and allows for automatic, reliable labeling
by combining inverse kinematic parameters with a knowledge-based evaluation
strategy. Extensive evaluations demonstrate that augmented variants closely
resembles real-world data, significantly improving the classification accuracy
and generalization capability of neural network models. Additionally, we
highlight the benefits of augmented data for patient-specific fine-tuning
scenarios, particularly when only limited subject-specific training examples
are available. Our findings underline the practicality and efficacy of this
augmentation method in overcoming common challenges faced by deep learning
applications in physiotherapeutic exercise evaluation.

</details>


### [83] [Learning Safety Constraints for Large Language Models](https://arxiv.org/abs/2505.24445)
*Xin Chen, Yarden As, Andreas Krause*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种用于增强大语言模型安全性的几何方法SaP，并展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型虽然强大，但存在产生有害输出和易受对抗攻击的安全风险，因此需要一种新的方法提高其安全性。

**方法:** 开发了一个框架，在模型的表示空间中学习和实施多个安全约束，通过多面体的面来识别安全和非安全区域。

**结果:** 实验结果显示，该方法在多个LLM上均能有效检测不道德输入、减少对抗攻击成功率，同时保持了模型的标准任务性能。

**结论:** 论文提出了一种名为SaP的方法，通过几何方法处理大语言模型的安全性问题，实验表明其能有效检测不道德输入并降低对抗攻击的成功率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Safety+Constraints+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24445&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have emerged as powerful tools but pose
significant safety risks through harmful outputs and vulnerability to
adversarial attacks. We propose SaP, short for Safety Polytope, a geometric
approach to LLM safety that learns and enforces multiple safety constraints
directly in the model's representation space. We develop a framework that
identifies safe and unsafe regions via the polytope's facets, enabling both
detection and correction of unsafe outputs through geometric steering. Unlike
existing approaches that modify model weights, SaP operates post-hoc in the
representation space, preserving model capabilities while enforcing safety
constraints. Experiments across multiple LLMs demonstrate that our method can
effectively detect unethical inputs, reduce adversarial attack success rates
while maintaining performance on standard tasks, thus highlighting the
importance of having an explicit geometric model for safety. Analysis of the
learned polytope facets reveals emergence of specialization in detecting
different semantic notions of safety, providing interpretable insights into how
safety is captured in LLMs' representation space.

</details>


### [84] [Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy](https://arxiv.org/abs/2505.24473)
*Nikita Balagansky, Yaroslav Aksenov, Daniil Laptev, Vadim Kurochkin, Gleb Gerasimov, Nikita Koryagin, Daniil Gavrilov*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新型稀疏自动编码器训练目标HierarchicalTopK，可在多种稀疏水平下实现更好的性能和解释性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的稀疏自动编码器受限于固定的稀疏水平选择，难以满足不同的稀疏需求，并且需要多个模型来增加计算负担。

**方法:** 引入了一种名为HierarchicalTopK的新训练目标，使得单个稀疏自动编码器能够在不同稀疏水平下进行优化。

**结果:** 实验结果表明，所提出的方法在稀疏性和解释方差之间实现了Pareto最优权衡，并在更高稀疏度下仍保持高可解释性得分。

**结论:** 论文提出了一种新的训练目标HierarchicalTopK，通过同时优化多个稀疏水平下的重构性能，在保持高解释性的同时提升了稀疏自动编码器的灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Train+One+Sparse+Autoencoder+Across+Multiple+Sparsity+Budgets+to+Preserve+Interpretability+and+Accuracy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24473，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24473&send_immediately=true&force_search=false)

**原文摘要:** Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting
neural networks by decomposing hidden representations into disentangled,
interpretable features via sparsity constraints. However, conventional SAEs are
constrained by the fixed sparsity level chosen during training; meeting
different sparsity requirements therefore demands separate models and increases
the computational footprint during both training and evaluation. We introduce a
novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to
optimise reconstructions across multiple sparsity levels simultaneously.
Experiments with Gemma-2 2B demonstrate that our approach achieves
Pareto-optimal trade-offs between sparsity and explained variance,
outperforming traditional SAEs trained at individual sparsity levels. Further
analysis shows that HierarchicalTopK preserves high interpretability scores
even at higher sparsity. The proposed objective thus closes an important gap
between flexibility and interpretability in SAE design.

</details>


### [85] [LightSAM: Parameter-Agnostic Sharpness-Aware Minimization](https://arxiv.org/abs/2505.24399)
*Yifei Cheng, Li Shen, Hao Sun, Nan Yin, Xiaochun Cao, Enhong Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的优化算法LightSAM，该算法通过自适应地设置Sharpness-Aware Minimization (SAM)的扰动半径和学习率，减少了对参数调节的需求，并扩展了SAM的应用范围。


<details>
  <summary>更多</summary>
  
**动机:** 尽管SAM优化器在经验上取得了成功，但它引入了一个额外的超参数——扰动半径，导致其对该参数敏感。此外，扰动半径和学习率受到问题相关参数的限制以保证收敛性。这些限制表明在实际应用中需要进行参数调优。

**方法:** LightSAM利用三种流行的自适应优化器（包括AdaGrad-Norm、AdaGrad和Adam）取代SGD优化器来进行权重扰动和模型更新，从而降低对参数的敏感度。

**结果:** 理论结果表明，在较弱的假设条件下，无论扰动半径和学习率如何选择，LightSAM都可以理想地收敛，实现了与参数无关的特性。初步实验验证了LightSAM的有效性。

**结论:** LightSAM是一种更加鲁棒且易于使用的优化方法，能够有效减少对超参数调节的依赖，拓展了SAM在深度学习任务中的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightSAM%3A+Parameter-Agnostic+Sharpness-Aware+Minimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24399&send_immediately=true&force_search=false)

**原文摘要:** Sharpness-Aware Minimization (SAM) optimizer enhances the generalization
ability of the machine learning model by exploring the flat minima landscape
through weight perturbations. Despite its empirical success, SAM introduces an
additional hyper-parameter, the perturbation radius, which causes the
sensitivity of SAM to it. Moreover, it has been proved that the perturbation
radius and learning rate of SAM are constrained by problem-dependent parameters
to guarantee convergence. These limitations indicate the requirement of
parameter-tuning in practical applications. In this paper, we propose the
algorithm LightSAM which sets the perturbation radius and learning rate of SAM
adaptively, thus extending the application scope of SAM. LightSAM employs three
popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to
replace the SGD optimizer for weight perturbation and model updating, reducing
sensitivity to parameters. Theoretical results show that under weak
assumptions, LightSAM could converge ideally with any choices of perturbation
radius and learning rate, thus achieving parameter-agnostic. We conduct
preliminary experiments on several deep learning tasks, which together with the
theoretical findings validate the the effectiveness of LightSAM.

</details>


### [86] [Object Centric Concept Bottlenecks](https://arxiv.org/abs/2505.24492)
*David Steinmann, Wolfgang Stammer, Antonia Wüst, Kristian Kersting*

**主要类别:** cs.LG

**AI概要:** 本文提出 OCB 框架，结合 CBM 和对象中心模型的优势，提升了复杂视觉任务的性能和决策可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 传统 CBM 依赖整体图像编码，限制了其在复杂、物体中心的任务中的表现和可解释性，因此需要改进方法以解决更复杂的视觉任务。

**方法:** 引入 OCB（Object-Centric Concept Bottlenecks）框架，结合 CBM 和预训练对象中心模型的优势，并通过综合消融实验分析关键组件。

**结果:** OCB 在复杂图像数据集上超越了传统 CBM，在性能和可解释性方面均表现出色。

**结论:** OCB 框架在复杂的视觉任务中优于传统 CBM，并允许进行可解释的决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Object+Centric+Concept+Bottlenecks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24492&send_immediately=true&force_search=false)

**原文摘要:** Developing high-performing, yet interpretable models remains a critical
challenge in modern AI. Concept-based models (CBMs) attempt to address this by
extracting human-understandable concepts from a global encoding (e.g., image
encoding) and then applying a linear classifier on the resulting concept
activations, enabling transparent decision-making. However, their reliance on
holistic image encodings limits their expressiveness in object-centric
real-world settings and thus hinders their ability to solve complex vision
tasks beyond single-label classification. To tackle these challenges, we
introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines
the strengths of CBMs and pre-trained object-centric foundation models,
boosting performance and interpretability. We evaluate OCB on complex image
datasets and conduct a comprehensive ablation study to analyze key components
of the framework, such as strategies for aggregating object-concept encodings.
The results show that OCB outperforms traditional CBMs and allows one to make
interpretable decisions for complex visual tasks.

</details>


### [87] [On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets](https://arxiv.org/abs/2505.24403)
*Giannis Nikolentzos, Konstantinos Skianis*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了用于处理集合数据的神经网络中的聚合函数是否具有Lipschitz连续性，并计算其Lipschitz常数，进一步推导了整个网络的Lipschitz常数上界及其稳定性与泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于神经网络的Lipschitz常数与其鲁棒性和泛化能力密切相关，因此估计模型的Lipschitz常数在许多场景中是有用的。以往的工作主要集中在多层感知机和卷积神经网络的Lipschitz常数估计上，而本文关注于能处理集合或多重集数据的神经网络。

**方法:** 分析了适用于处理集合或多重集数据的神经网络中常用的聚合函数（如sum、mean、max）是否相对于三种无序多重集距离函数具有Lipschitz连续性，并计算它们的Lipschitz常数。

**结果:** 理论上发现，在一般情况下，每种聚合函数仅相对于三种距离函数之一是Lipschitz连续的；通过实验证明了理论分析的有效性。

**结论:** 论文得出每种聚合函数仅相对于三种无序多重集距离函数之一满足Lipschitz连续，并利用这些结果推导出能够处理向量多重集的神经网络的Lipschitz常数上界，同时研究了其在扰动下的稳定性及分布偏移下的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Lipschitz+Continuity+of+Set+Aggregation+Functions+and+Neural+Networks+for+Sets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24403，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24403&send_immediately=true&force_search=false)

**原文摘要:** The Lipschitz constant of a neural network is connected to several important
properties of the network such as its robustness and generalization. It is thus
useful in many settings to estimate the Lipschitz constant of a model. Prior
work has focused mainly on estimating the Lipschitz constant of multi-layer
perceptrons and convolutional neural networks. Here we focus on data modeled as
sets or multisets of vectors and on neural networks that can handle such data.
These models typically apply some permutation invariant aggregation function,
such as the sum, mean or max operator, to the input multisets to produce a
single vector for each input sample. In this paper, we investigate whether
these aggregation functions are Lipschitz continuous with respect to three
distance functions for unordered multisets, and we compute their Lipschitz
constants. In the general case, we find that each aggregation function is
Lipschitz continuous with respect to only one of the three distance functions.
Then, we build on these results to derive upper bounds on the Lipschitz
constant of neural networks that can process multisets of vectors, while we
also study their stability to perturbations and generalization under
distribution shifts. To empirically verify our theoretical analysis, we conduct
a series of experiments on datasets from different domains.

</details>


### [88] [Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting](https://arxiv.org/abs/2505.24511)
*Jiahao Wang, Mingyue Cheng, Qi Liu*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了使用slow-thinking LLM进行时间序列预测的可能性，提出TimeReasoner方法，并发现其在零样本设置下具有一定的预测能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列预测方法通常采用快速思维模式，忽略了对时间动态和上下文依赖的明确推理。而新兴的slow-thinking LLM在多步推理方面表现出色，因此论文探讨是否能利用这类模型来进行有效的时间序列预测。

**方法:** 提出TimeReasoner方法，将时间序列预测任务重新定义为条件推理任务，并设计一系列提示策略来引导预训练的slow-thinking LLM进行推理。同时，在多个TSF基准上评估模型性能。

**结果:** 实验表明，slow-thinking LLM在零样本情况下展现出非同寻常的预测能力，尤其在识别复杂趋势和上下文变化方面。结果表明，这些模型在时间领域中具有潜力，但也存在局限性。

**结论:** 研究表明slow-thinking LLMs在时间序列预测中具有一定的零样本预测能力，尤其是在捕捉高层次趋势和上下文变化方面。论文强调了基于推理的预测范式的重要性，并指出了进一步研究的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Slow-thinking+LLMs+Reason+Over+Time%3F+Empirical+Studies+in+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24511&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting (TSF) is a fundamental and widely studied task,
spanning methods from classical statistical approaches to modern deep learning
and multimodal language modeling. Despite their effectiveness, these methods
often follow a fast thinking paradigm emphasizing pattern extraction and direct
value mapping, while overlooking explicit reasoning over temporal dynamics and
contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,
ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning
capabilities across diverse domains, suggesting a new opportunity for reframing
TSF as a structured reasoning task. This motivates a key question: can
slow-thinking LLMs effectively reason over temporal patterns to support time
series forecasting, even in zero-shot manner? To investigate this, in this
paper, we propose TimeReasoner, an extensive empirical study that formulates
TSF as a conditional reasoning task. We design a series of prompting strategies
to elicit inference-time reasoning from pretrained slow-thinking LLMs and
evaluate their performance across diverse TSF benchmarks. Our findings reveal
that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,
especially in capturing high-level trends and contextual shifts. While
preliminary, our study surfaces important insights into the reasoning behaviors
of LLMs in temporal domains highlighting both their potential and limitations.
We hope this work catalyzes further research into reasoning-based forecasting
paradigms and paves the way toward more interpretable and generalizable TSF
frameworks.

</details>


### [89] [Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data](https://arxiv.org/abs/2505.24413)
*Yang Sui, Qi Xu, Yang Bai, Annie Qu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的多任务学习方法，能同时处理多种异质性问题，在实验中表现出更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了更有效地同时解决多任务学习中的块状、分布和后验异质性问题，提出一种统一框架下的新方法。

**方法:** 提出了一种两步学习策略：首先利用跨任务的同源信息填补缺失块；然后将输入特征与响应之间的映射分解为共享成分和任务特定成分。

**结果:** 数值实验和基于ADNI数据库的实际数据分析表明，该方法在多任务学习性能上优于现有方法。

**结论:** 论文提出了一种新的多任务学习方法，能够有效处理不同任务间的异质性，并通过实验验证了其优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-task+Learning+for+Heterogeneous+Multi-source+Block-Wise+Missing+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24413&send_immediately=true&force_search=false)

**原文摘要:** Multi-task learning (MTL) has emerged as an imperative machine learning tool
to solve multiple learning tasks simultaneously and has been successfully
applied to healthcare, marketing, and biomedical fields. However, in order to
borrow information across different tasks effectively, it is essential to
utilize both homogeneous and heterogeneous information. Among the extensive
literature on MTL, various forms of heterogeneity are presented in MTL
problems, such as block-wise, distribution, and posterior heterogeneity.
Existing methods, however, struggle to tackle these forms of heterogeneity
simultaneously in a unified framework. In this paper, we propose a two-step
learning strategy for MTL which addresses the aforementioned heterogeneity.
First, we impute the missing blocks using shared representations extracted from
homogeneous source across different tasks. Next, we disentangle the mappings
between input features and responses into a shared component and a
task-specific component, respectively, thereby enabling information borrowing
through the shared component. Our numerical experiments and real-data analysis
from the ADNI database demonstrate the superior MTL performance of the proposed
method compared to other competing methods.

</details>


### [90] [Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators](https://arxiv.org/abs/2505.24533)
*Mahesh Godavarti*

**主要类别:** cs.LG

**AI概要:** 提出一种新的高维代数结构框架，统一了多种经典线性变换，并可用于开发针对特定数据的可学习变换。


<details>
  <summary>更多</summary>
  
**动机:** 将一维单类系统扩展到高维空间，并统一信号处理和数据分析中的多种线性变换。

**方法:** 从向量-矩阵对的基本情况递归定义多维方向组合的代数结构，并引入非交换性和全局交换律。

**结果:** 证明离散傅里叶变换、Walsh变换和Hadamard变换都是该代数结构的特例，并提供了这些变换的系统推导方法。

**结论:** 该框架通过统一多个经典变换，提供了一种设计可学习变换的方法，适用于特定数据模态和任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Directional+Non-Commutative+Monoidal+Structures+with+Interchange+Law+via+Commutative+Generators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24533&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel framework consisting of a class of algebraic structures
that generalize one-dimensional monoidal systems into higher dimensions by
defining per-axis composition operators subject to non-commutativity and a
global interchange law. These structures, defined recursively from a base case
of vector-matrix pairs, model directional composition in multiple dimensions
while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms
in signal processing and data analysis. In this framework, data indices are
embedded into a composite structure that decomposes into simpler components. We
show that classic transforms such as the Discrete Fourier Transform (DFT), the
Walsh transform, and the Hadamard transform are special cases of our algebraic
structure. The framework provides a systematic way to derive these transforms
by appropriately choosing vector and matrix pairs. By subsuming classical
transforms within a common structure, the framework also enables the
development of learnable transformations tailored to specific data modalities
and tasks.

</details>


### [91] [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
*Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah*

**主要类别:** cs.LG

**AI概要:** 本论文介绍 K-Steering 方法，通过非线性分类器与梯度计算，在推理时实现对多个行为属性的有效控制。


<details>
  <summary>更多</summary>
  
**动机:** 需要解决大型语言模型中多个行为属性的干扰问题，并克服线性转向方法的限制。

**方法:** 使用非线性多标签分类器和梯度计算干预方向。

**结果:** 在多个模型家族中的实证结果显示，K-Steering 在准确引导多种行为方面优于强基线方法。

**结论:** K-Steering 是一种有效的方法，用于在推理时控制多个行为属性，避免了线性假设并提高了灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Linear+Steering%3A+Unified+Multi-Attribute+Control+for+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24535&send_immediately=true&force_search=false)

**原文摘要:** Controlling multiple behavioral attributes in large language models (LLMs) at
inference time is a challenging problem due to interference between attributes
and the limitations of linear steering methods, which assume additive behavior
in activation space and require per-attribute tuning. We introduce K-Steering,
a unified and flexible approach that trains a single non-linear multi-label
classifier on hidden activations and computes intervention directions via
gradients at inference time. This avoids linearity assumptions, removes the
need for storing and tuning separate attribute vectors, and allows dynamic
composition of behaviors without retraining. To evaluate our method, we propose
two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral
control. Empirical results across 3 model families, validated by both
activation-based classifiers and LLM-based judges, demonstrate that K-Steering
outperforms strong baselines in accurately steering multiple behaviors.

</details>


### [92] [Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning](https://arxiv.org/abs/2505.24424)
*Amit Peleg, Naman Deep Singh, Matthias Hein*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 CLIC 的新方法，用于改进 CLIP 模型的组合推理能力，同时提升其检索性能。


<details>
  <summary>更多</summary>
  
**动机:** 提高视觉语言模型的组合推理能力，同时确保不损害其检索性能。

**方法:** 通过结合多张图像及其相关字幕的新训练技术对 CLIP 进行微调。

**结果:** CLIC 在多个架构和预训练模型上都提高了词汇和语义理解，并在 SugarCrepe++ 基准测试中实现了最佳性能。

**结论:** CLIC 提升了 CLIP 模型的组合推理能力，同时改善了检索性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Compositional+Awareness+in+CLIP+with+Efficient+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24424&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io

</details>


### [93] [AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams](https://arxiv.org/abs/2505.24584)
*Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种结合小型语言模型和物理模拟的闭环框架，能够高效生成符合工程约束的工业级化学工艺流程图和管道仪表图，解决了从实验室发现到工业化生产的瓶颈问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管生成式AI在新型化学品和材料的发现上取得了进展，但将其转化为工业规模生产仍然是一个关键瓶颈，因为需要开发全新的化学制造工艺。现有的AI方法无法自动生成工艺流程图和管道仪表图，而这些对于扩大化学工艺规模至关重要。

**方法:** 该框架整合了领域专用的小型语言模型与第一性原理模拟，利用了一个包含1020多种化学品的工艺流程和仪表描述的分层知识图谱，并通过多阶段训练管道对模型进行微调，同时结合DWSIM模拟器验证可行性。此外，还应用了多种推理时优化技术和结构剪枝技术以提高运行效率和模型紧凑性。

**结果:** 实验表明，该框架能够高保真地生成经过模拟器验证的工艺描述，在正确性方面优于基线方法，并且能够推广到未见过的化学品。

**结论:** 本文提出了一种闭环的、物理感知的框架，可以自动生成工业上可行的工艺流程图和管道仪表图，从而显著缩短从实验室发现到工厂部署的研发时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoChemSchematic+AI%3A+A+Closed-Loop%2C+Physics-Aware+Agentic+Framework+for+Auto-Generating+Chemical+Process+and+Instrumentation+Diagrams，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24584，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24584&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.

</details>


### [94] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/abs/2505.24434)
*Md Shahriar Rahim Siddiqui, Moshe Eliasof, Eldad Haber*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种称为Graph Flow Matching (GFM)的新方法，通过引入扩散项来改善流匹配中的速度预测，从而提高生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的点独立预测速度的方法忽略了点之间的相关性，可能影响生成质量。

**方法:** 将学习的速度场分解为反应项和扩散项，并使用图神经模块聚合邻居信息。

**结果:** 在五个图像生成基准测试中，GFM提高了FID和回忆率。

**结论:** GFM是一种轻量级增强方法，通过加入扩散项提升现有流匹配架构的生成质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Flow+Matching%3A+Enhancing+Image+Generation+with+Neighbor-Aware+Flow+Fields，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24434，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24434&send_immediately=true&force_search=false)

**原文摘要:** Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [95] [A Flat Minima Perspective on Understanding Augmentations and Model Robustness](https://arxiv.org/abs/2505.24592)
*Weebum Yoo, Sung Whan Yoon*

**主要类别:** cs.LG

**AI概要:** 这篇论文提供了一个统一的理论框架，用于理解数据增强如何提升模型的鲁棒性，并通过多个基准测试验证了其理论。


<details>
  <summary>更多</summary>
  
**动机:** 尽管数据增强在不同领域取得了巨大成功，但缺乏对其提高模型鲁棒性的效果的一般理论理解。

**方法:** 通过分析损失表面平坦性和PAC泛化界，结合CIFAR和ImageNet等现有基准上的仿真验证理论。

**结果:** 该理论涵盖了现有的大量增强方法，并且不仅限于特定类型的分布偏移，如对抗攻击。

**结论:** 论文提出了一个统一的理论框架，解释了增强数据如何通过损失表面平坦性和PAC泛化界来提升模型的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Flat+Minima+Perspective+on+Understanding+Augmentations+and+Model+Robustness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24592，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24592&send_immediately=true&force_search=false)

**原文摘要:** Model robustness indicates a model's capability to generalize well on
unforeseen distributional shifts, including data corruption, adversarial
attacks, and domain shifts. Data augmentation is one of the prevalent and
effective ways to enhance robustness. Despite the great success of
augmentations in different fields, a general theoretical understanding of their
efficacy in improving model robustness is lacking. We offer a unified
theoretical framework to clarify how augmentations can enhance model robustness
through the lens of loss surface flatness and PAC generalization bound. Our
work diverges from prior studies in that our analysis i) broadly encompasses
much of the existing augmentation methods, and ii) is not limited to specific
types of distribution shifts like adversarial attacks. We confirm our theories
through simulations on the existing common corruption and adversarial
robustness benchmarks based on the CIFAR and ImageNet datasets, as well as
domain generalization benchmarks including PACS and OfficeHome.

</details>


### [96] [Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs](https://arxiv.org/abs/2505.24438)
*Franziska Heeg, Jonas Sauer, Petra Mutzel, Ingo Scholtes*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一个能够捕捉时间图因果拓扑的新方法，并基于此开发了新的时间图神经网络消息传递方案，在实验中表现出良好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间图神经网络往往忽视了时间的方向性对因果拓扑的影响，缺乏一种能够完全捕捉这种因果拓扑的图同构推广。

**方法:** 引入了事件图同构的一致性概念，并开发了时间版本的Weisfeiler-Leman算法来区分非同构的时间图。

**结果:** 提出的方法在时间图分类任务中表现出色，强调了其优势。

**结论:** 论文得出了一种新的时间图神经网络的消息传递方案，该方案在时间图分类实验中表现良好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weisfeiler+and+Leman+Follow+the+Arrow+of+Time%3A+Expressive+Power+of+Message+Passing+in+Temporal+Event+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24438&send_immediately=true&force_search=false)

**原文摘要:** An important characteristic of temporal graphs is how the directed arrow of
time influences their causal topology, i.e., which nodes can possibly influence
each other causally via time-respecting paths. The resulting patterns are often
neglected by temporal graph neural networks (TGNNs). To formally analyze the
expressive power of TGNNs, we lack a generalization of graph isomorphism to
temporal graphs that fully captures their causal topology. Addressing this gap,
we introduce the notion of consistent event graph isomorphism, which utilizes a
time-unfolded representation of time-respecting paths in temporal graphs. We
compare this definition with existing notions of temporal graph isomorphisms.
We illustrate and highlight the advantages of our approach and develop a
temporal generalization of the Weisfeiler-Leman algorithm to heuristically
distinguish non-isomorphic temporal graphs. Building on this theoretical
foundation, we derive a novel message passing scheme for temporal graph neural
networks that operates on the event graph representation of temporal graphs. An
experimental evaluation shows that our approach performs well in a temporal
graph classification experiment.

</details>


### [97] [Hyperbolic Dataset Distillation](https://arxiv.org/abs/2505.24623)
*Wenyuan Li, Guang Li, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为HDD的超球面数据集蒸馏方法，该方法能够在保持数据层次结构和几何特性的前提下提高模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的分布匹配方法受限于欧几里得空间，无法捕捉复杂的数据几何和层次关系。

**方法:** HDD将从浅层网络提取的特征嵌入到Lorentz超球空间中，并通过优化合成数据与原始数据质心之间的测地距离来集成分层结构。

**结果:** 实验表明，HDD在不同数据集上均表现良好，同时显著提高了训练稳定性。

**结论:** HDD是一种新的超球面数据集蒸馏方法，可以有效保留原始数据的几何和层次结构特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyperbolic+Dataset+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24623，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24623&send_immediately=true&force_search=false)

**原文摘要:** To address the computational and storage challenges posed by large-scale
datasets in deep learning, dataset distillation has been proposed to synthesize
a compact dataset that replaces the original while maintaining comparable model
performance. Unlike optimization-based approaches that require costly bi-level
optimization, distribution matching (DM) methods improve efficiency by aligning
the distributions of synthetic and original data, thereby eliminating nested
optimization. DM achieves high computational efficiency and has emerged as a
promising solution. However, existing DM methods, constrained to Euclidean
space, treat data as independent and identically distributed points,
overlooking complex geometric and hierarchical relationships. To overcome this
limitation, we propose a novel hyperbolic dataset distillation method, termed
HDD. Hyperbolic space, characterized by negative curvature and exponential
volume growth with distance, naturally models hierarchical and tree-like
structures. HDD embeds features extracted by a shallow network into the Lorentz
hyperbolic space, where the discrepancy between synthetic and original data is
measured by the hyperbolic (geodesic) distance between their centroids. By
optimizing this distance, the hierarchical structure is explicitly integrated
into the distillation process, guiding synthetic samples to gravitate towards
the root-centric regions of the original data distribution while preserving
their underlying geometric characteristics. Furthermore, we find that pruning
in hyperbolic space requires only 20% of the distilled core set to retain model
performance, while significantly improving training stability. Notably, HDD is
seamlessly compatible with most existing DM methods, and extensive experiments
on different datasets validate its effectiveness.

</details>


### [98] [Stepsize anything: A unified learning rate schedule for budgeted-iteration training](https://arxiv.org/abs/2505.24452)
*Anda Tang, Yiming Dong, Yutao Zeng, zhou Xun, Zhouchen Lin*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的学习率调度方法UBA，该方法具有理论基础，并在多种任务和网络架构中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 预算迭代训练的需求日益重要，但目前缺乏理论基础的学习率调度设计。

**方法:** 构建了一个新的训练预算感知优化框架，并从中推导出UBA调度方法。

**结果:** 实验结果显示UBA调度方法在不同网络架构和任务中表现出色。

**结论:** UBA调度在各种视觉和语言任务中始终优于常用的调度方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stepsize+anything%3A+A+unified+learning+rate+schedule+for+budgeted-iteration+training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24452，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24452&send_immediately=true&force_search=false)

**原文摘要:** The expanding computational costs and limited resources underscore the
critical need for budgeted-iteration training, which aims to achieve optimal
learning within predetermined iteration budgets.While learning rate schedules
fundamentally govern the performance of different networks and tasks,
particularly in budgeted-iteration scenarios, their design remains largely
heuristic, lacking theoretical foundations.In addition, the optimal learning
rate schedule requires extensive trial-and-error selection, making the training
process inefficient.In this work, we propose the Unified Budget-Aware (UBA)
schedule, a theoretically grounded learning rate schedule that consistently
outperforms commonly-used schedules among diverse architectures and tasks under
different constrained training budgets.First, we bridge the gap by constructing
a novel training budget-aware optimization framework, which explicitly accounts
for the robustness to landscape curvature variations.From this framework, we
derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that
provides a trade-off between flexibility and simplicity, eliminating the need
for per-network numerical optimization. Moreover, we establish a theoretical
connection between $\varphi$ and the condition number, adding interpretation
and justification to our approach. Besides, we prove the convergence for
different values of $\varphi$.We offer practical guidelines for its selection
via theoretical analysis and empirical results.xtensive experimental results
show that UBA \textit{consistently surpasses} the commonly-used schedules
across diverse vision and language tasks, spanning network architectures (e.g.,
ResNet, OLMo) and scales, under different training-iteration budgets.

</details>


### [99] [Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs](https://arxiv.org/abs/2505.24684)
*Zihao Chen, Yu Xiang, Wenyong Wang*

**主要类别:** cs.LG

**AI概要:** 本文研究了因子化变分自编码器中的隐式归纳偏置，提出了\b{eta}-STCVAE模型来调整解缠粒度，从而提高解缠性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在学习语义上有意义的无监督解缠表示方面取得了成功，但VAE及其变体面临一个理论挑战：在没有隐式归纳偏置的情况下，无监督解缠是无法实现的，而这种偏置仍然难以捉摸。本文旨在探索驱动VAE解缠的隐式归纳偏置。

**方法:** 通过分析\b{eta}-TCVAE中的总相关性，作者发现了一种关键的隐式归纳偏置，称为解缠粒度，并提出了一种新的模型\b{eta}-STCVAE以调整这种粒度。

**结果:** 实验结果表明，传统的因子化VAE受限于固定的解缠粒度，倾向于解缠低复杂度特征；而通过\b{eta}-STCVAE适当调节解缠粒度可以拓宽解缠表示的范围，实现高复杂度特征的解缠。

**结论:** 本文结论指出，分解粒度作为一种隐式归纳偏置，在因子化变分自编码器（VAE）中影响了解缠性能和证据下界（ELBO）的推断，为VAE的可解释性和内在偏置提供了新的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Disentangling+Granularity%3A+An+Implicit+Inductive+Bias+in+Factorized+VAEs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24684&send_immediately=true&force_search=false)

**原文摘要:** Despite the success in learning semantically meaningful, unsupervised
disentangled representations, variational autoencoders (VAEs) and their
variants face a fundamental theoretical challenge: substantial evidence
indicates that unsupervised disentanglement is unattainable without implicit
inductive bias, yet such bias remains elusive. In this work, we focus on
exploring the implicit inductive bias that drive disentanglement in VAEs with
factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we
uncover a crucial implicit inductive bias called disentangling granularity,
which leads to the discovery of an interesting "V"-shaped optimal Evidence
Lower Bound (ELBO) trajectory within the parameter space. This finding is
validated through over 100K experiments using factorized VAEs and our newly
proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that
conventional factorized VAEs, constrained by fixed disentangling granularity,
inherently tend to disentangle low-complexity feature. Whereas, appropriately
tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the
range of disentangled representations, allowing for the disentanglement of
high-complexity features. Our findings unveil that disentangling granularity as
an implicit inductive bias in factorized VAEs influence both disentanglement
performance and the inference of the ELBO, offering fresh insights into the
interpretability and inherent biases of VAEs.

</details>


### [100] [Logits-Based Finetuning](https://arxiv.org/abs/2505.24461)
*Jingyao Li, Senqiao Yang, Sitong Wu, Han Shi, Chuanyang Zheng, Hong Xu, Jiaya Jia*

**主要类别:** cs.LG

**AI概要:** Diese arbeit stellt mood vor, ein rahmenwerk zur erkennung von ausreißern (oob) mithilfe von rekonstruktionsbasierten vorwandtaufgaben, insbesondere masked image modeling. Ohne weitere modifikationen wird eine signifikante verbesserung der erkennungsleistung im vergleich zum bisherigen stand der technik erreicht.


<details>
  <summary>更多</summary>
  
**动机:** Frühere arbeit konzentrierte sich auf erkennungsbasierte methoden zum lernen von ID-merkmalen, die jedoch neigen dazu, stattdessen abkürzungen zu lernen anstelle umfassender darstellungen. Daher war eine effektivere methode erforderlich.

**方法:** Verwendung von Masked Image Modeling als Vorwandtaufgabe für das OOD-Detektionsframework (MOOD).

**结果:** MOOD verbessert die leistung der ein-klassen-oob-erkennung um 5,7 %, die leistung der mehrklassigen oob-erkennung um 3,0 % und die leistung der nahezu verteilten oob-erkennung um 2,1 %. Es schlägt sogar die 10-shot-per-klasse outlier-exposure-oob-erkennung.

**结论:** MOOD, basierend auf rekonstruktionsbasierten Vorwandtaufgaben, verbessert die Leistung der OOD-Erkennung erheblich und übertrifft den bisherigen Stand der Technik.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Logits-Based+Finetuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24461&send_immediately=true&force_search=false)

**原文摘要:** The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection. Codes are available at
https://github.com/JulietLJY/MOOD.

</details>


### [101] [On Symmetric Losses for Robust Policy Optimization with Noisy Preferences](https://arxiv.org/abs/2505.24709)
*Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, Masashi Sugiyama*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为SymPO的新方法，用于处理语言模型中带有噪声的人类偏好数据，通过将奖励建模作为分类问题并使用对称损失函数以提高鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的偏好数据往往由于人类错误或偏见而包含噪声，传统方法通常假设标注准确，这导致了对噪声数据处理的需求。

**方法:** 将奖励建模视为分类问题，并利用对称损失函数来处理标签中的噪声。

**结果:** 实验表明，SymPO方法在合成和真实世界任务中均有效。

**结论:** 论文提出了一种在偏好数据带有噪声的情况下进行稳健策略优化的框架，即SymPO方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Symmetric+Losses+for+Robust+Policy+Optimization+with+Noisy+Preferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24709，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24709&send_immediately=true&force_search=false)

**原文摘要:** Optimizing policies based on human preferences is key to aligning language
models with human intent. This work focuses on reward modeling, a core
component in reinforcement learning from human feedback (RLHF), and offline
preference optimization, such as direct preference optimization. Conventional
approaches typically assume accurate annotations. However, real-world
preference data often contains noise due to human errors or biases. We propose
a principled framework for robust policy optimization under noisy preferences,
viewing reward modeling as a classification problem. This allows us to leverage
symmetric losses, known for their robustness to label noise in classification,
leading to our Symmetric Preference Optimization (SymPO) method. We prove that
symmetric losses enable successful policy optimization even under noisy labels,
as the resulting reward remains rank-preserving -- a property sufficient for
policy improvement. Experiments on synthetic and real-world tasks demonstrate
the effectiveness of SymPO.

</details>


### [102] [Smooth Model Compression without Fine-Tuning](https://arxiv.org/abs/2505.24469)
*Christina Runkel, Natacha Kuete Meli, Jovita Lukasik, Ander Biguri, Carola-Bibiane Schönlieb, Michael Moeller*

**主要类别:** cs.LG

**AI概要:** 该论文研究了平滑正则化对神经网络训练及压缩的影响，提出了一种基于奇异值分解的高效模型压缩方法，实现了在不牺牲性能的前提下显著减少模型参数。


<details>
  <summary>更多</summary>
  
**动机:** 标准的剪枝和压缩技术通常未考虑网络权重的结构，限制了其有效性。因此，探索平滑正则化对神经网络训练和模型压缩的影响，旨在提高模型压缩的效果。

**方法:** 应用核范数、一阶和二阶导数惩罚于神经网络训练中，以鼓励权重参数的结构化平滑性；随后使用基于奇异值分解的方法对模型权重张量进行低秩近似，实现高效的模型压缩。

**结果:** 实验表明，标准剪枝方法在应用于平滑模型时表现更好，并且所提出的压缩方法能够在无需微调的情况下达到高达70%的参数减少和91%的准确率（如CIFAR-10上的ResNet-18）。

**结论:** 本文提出了一种基于平滑正则化的神经网络训练和模型压缩方法，通过利用核范数、权重的一阶和二阶导数惩罚，在保持预测性能的同时实现了结构化的平滑性。此外，基于这一特性，采用基于奇异值分解的压缩方法，成功地在无需微调的情况下达到了最先进的压缩效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smooth+Model+Compression+without+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24469&send_immediately=true&force_search=false)

**原文摘要:** Compressing and pruning large machine learning models has become a critical
step towards their deployment in real-world applications. Standard pruning and
compression techniques are typically designed without taking the structure of
the network's weights into account, limiting their effectiveness. We explore
the impact of smooth regularization on neural network training and model
compression. By applying nuclear norm, first- and second-order derivative
penalties of the weights during training, we encourage structured smoothness
while preserving predictive performance on par with non-smooth models. We find
that standard pruning methods often perform better when applied to these smooth
models. Building on this observation, we apply a
Singular-Value-Decomposition-based compression method that exploits the
underlying smooth structure and approximates the model's weight tensors by
smaller low-rank tensors. Our approach enables state-of-the-art compression
without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18
on CIFAR-10 with $70\%$ fewer parameters.

</details>


### [103] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
*Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai*

**主要类别:** cs.LG

**AI概要:** 本文提出了Causal-aware LLMs，结合结构因果模型以提升大语言模型在动态环境中决策的能力。


<details>
  <summary>更多</summary>
  
**动机:** 预训练的大语言模型缺乏推理能力，在适应新环境方面存在困难，这阻碍了它们在复杂现实任务中的应用。

**方法:** 利用LLM提取环境特定的因果实体及其因果关系，初始化结构因果模型；通过外部反馈更新该模型；最后在决策阶段通过强化学习代理利用这些知识。

**结果:** 在开放世界游戏“Crafter”中22个不同任务上的实验结果验证了所提出方法的有效性。

**结论:** Causal-aware LLMs通过整合结构因果模型（SCM）到决策过程中，能够更准确地理解环境并做出更高效的决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal-aware+Large+Language+Models%3A+Enhancing+Decision-Making+Through+Learning%2C+Adapting+and+Acting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24710&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


### [104] [CoRet: Improved Retriever for Code Editing](https://arxiv.org/abs/2505.24715)
*Fabio Fehr, Prabhu Teja Sivaprasad, Luca Franceschi, Giovanni Zappella*

**主要类别:** cs.LG

**AI概要:** 本文提出CoRet模型，结合代码语义与结构信息，有效提升代码检索任务的召回率。


<details>
  <summary>更多</summary>
  
**动机:** 为了更有效地基于自然语言查询（如实现新功能或修复错误）检索代码存储库的相关部分，提高代码编辑任务的效率。

**方法:** 提出了一种名为CoRet的密集检索模型，以及一个专为存储库级检索设计的损失函数。

**结果:** 在SWE-bench和Long Code Arena的错误定位数据集上，CoRet显著提升了检索召回率，并通过消融实验验证了设计选择的重要性。

**结论:** CoRet通过整合代码语义、存储库结构和调用图依赖性，在代码编辑任务中的检索召回率上比现有模型提高了至少15个百分点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoRet%3A+Improved+Retriever+for+Code+Editing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24715，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24715&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we introduce CoRet, a dense retrieval model designed for
code-editing tasks that integrates code semantics, repository structure, and
call graph dependencies. The model focuses on retrieving relevant portions of a
code repository based on natural language queries such as requests to implement
new features or fix bugs. These retrieved code chunks can then be presented to
a user or to a second code-editing model or agent. To train CoRet, we propose a
loss function explicitly designed for repository-level retrieval. On SWE-bench
and Long Code Arena's bug localisation datasets, we show that our model
substantially improves retrieval recall by at least 15 percentage points over
existing models, and ablate the design choices to show their importance in
achieving these results.

</details>


### [105] [HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts](https://arxiv.org/abs/2505.24722)
*Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying*

**主要类别:** cs.LG

**AI概要:** 本文提出了基于超双曲线空间的大型语言模型HELM，通过几何重构解决了现有模型的表示灵活性不足、必要操作缺失和可扩展性差的问题，并展示了其优于传统欧几里得模型的效果。


<details>
  <summary>更多</summary>
  
**动机:** 当前的语言模型由于依赖于欧几里得运算而无法完全捕捉自然语言的语义层次和细微几何结构，这导致了训练不稳定和生成能力退化。因此，转向非欧几里得几何可以更好地使语言模型与文本的基础几何对齐。

**方法:** 论文提出了一种几何重构的Transformer基础语言模型，称为HELM，并引入了混合曲率专家模型HELM-MICE以及稠密模型HELM-D，同时开发了高效的超双曲线多头潜在注意力机制（HMLA）和必要的超双曲线等效操作。

**结果:** 论文结果显示，HELM架构在MMLU和ARC等知名基准测试中表现出了比LLaMA和DeepSeek使用的流行欧几里得架构高达4%的一致性提升。

**结论:** 论文得出的结论是，通过使用完全超双曲线空间中的语言模型HELM及其变种HELM-MICE和HELM-D，在大规模预训练中可以获得比流行的欧几里得架构更好的效果和推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HELM%3A+Hyperbolic+Large+Language+Models+via+Mixture-of-Curvature+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24722，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24722&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-KV-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.

</details>


### [106] [Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem](https://arxiv.org/abs/2505.24498)
*Andres Fernandez, Juan Azcarreta, Cagdas Bilen, Jesus Monge Alvarez*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种高效的在线语音频谱图反演方法，通过结合深度学习和梯度定理来直接从幅度预测相位导数，同时引入三种创新显著降低了计算成本，保持了高质量的重建效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的在线语音频谱图反演方法在计算成本上较高，需要寻找更高效的方法以实现高质量且低开销的重建。

**方法:** 首先设计了一个仅包含8k参数的新神经网络架构；其次通过增加1个hop大小的延迟进一步降低神经推理步骤的成本；最后利用最小二乘法中的三对角矩阵特性，提出了一个线性复杂度的求解器。

**结果:** 提出的三种创新技术使计算成本大幅下降，同时保持了高重建质量。其中新神经网络规模仅为先前技术的三十分之一，而新的求解器实现了数量级级别的加速。

**结论:** 该研究成功地将深度学习与经典数学方法相结合，在保证高质量重建的同时显著降低了在线语音频谱图反演的计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Neural+and+Numerical+Methods+for+High-Quality+Online+Speech+Spectrogram+Inversion+via+Gradient+Theorem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24498，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24498&send_immediately=true&force_search=false)

**原文摘要:** Recent work in online speech spectrogram inversion effectively combines Deep
Learning with the Gradient Theorem to predict phase derivatives directly from
magnitudes. Then, phases are estimated from their derivatives via least
squares, resulting in a high quality reconstruction. In this work, we introduce
three innovations that drastically reduce computational cost, while maintaining
high quality: Firstly, we introduce a novel neural network architecture with
just 8k parameters, 30 times smaller than previous state of the art. Secondly,
increasing latency by 1 hop size allows us to further halve the cost of the
neural inference step. Thirdly, we we observe that the least squares problem
features a tridiagonal matrix and propose a linear-complexity solver for the
least squares step that leverages tridiagonality and positive-semidefiniteness,
achieving a speedup of several orders of magnitude. We release samples online.

</details>


### [107] [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.24760)
*Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, Andreas Köpf*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的强化学习推理环境库 Reasoning Gym，它能生成大量可验证的动态训练数据，并在多个领域中实现连续难度评估。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决之前推理数据集通常是固定的、缺乏灵活性的问题，引入一个可以生成动态训练数据并支持连续评估的工具。

**方法:** 介绍了一个名为 Reasoning Gym 的库，包含超过 100 个数据生成器和验证器，并通过实验评估其效果。

**结果:** 实验结果表明，Reasoning Gym 在推理模型的评估和强化学习方面是有效的。

**结论:** Reasoning Gym 是一个创新的强化学习推理环境库，能够生成可验证奖励和几乎无限的训练数据，并具有可调整复杂性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REASONING+GYM%3A+Reasoning+Environments+for+Reinforcement+Learning+with+Verifiable+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24760&send_immediately=true&force_search=false)

**原文摘要:** We introduce Reasoning Gym (RG), a library of reasoning environments for
reinforcement learning with verifiable rewards. It provides over 100 data
generators and verifiers spanning multiple domains including algebra,
arithmetic, computation, cognition, geometry, graph theory, logic, and various
common games. Its key innovation is the ability to generate virtually infinite
training data with adjustable complexity, unlike most previous reasoning
datasets, which are typically fixed. This procedural generation approach allows
for continuous evaluation across varying difficulty levels. Our experimental
results demonstrate the efficacy of RG in both evaluating and reinforcement
learning of reasoning models.

</details>


### [108] [Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset](https://arxiv.org/abs/2505.24505)
*Ignacio Boero, Santiago Diaz, Tomás Vázquez, Enzo Coppes, Pablo Belzarena, Federico Larroca*

**主要类别:** cs.LG

**AI概要:** 本论文讨论了在电力系统操作中至关重要的最优无功功率调度（ORPD）问题，介绍了一个包含乌拉圭电网结构特征和真实运行数据的公开数据集，并评估了现实数据对基于学习的ORPD解决方案的影响。


<details>
  <summary>更多</summary>
  
**动机:** 尽管机器学习方法已在ORPD问题上取得进展，但其在真实电网条件下的有效性尚未得到充分验证。本文旨在填补这一空白，并提供一个实用数据集以促进相关研究。

**方法:** 引入一个包含乌拉圭电网结构特征和近两年真实运行数据的公开数据集，并通过合成数据与实际数据对比，评估基于学习的ORPD解决方案的表现。

**结果:** 研究发现，在从合成数据过渡到实际需求和发电输入时，预测误差显著增加，揭示了现有模型在学习复杂统计特性方面存在局限性。

**结论:** 为了提高ORPD解决方案的有效性，需要更复杂的架构来应对真实电网条件的复杂性，同时该数据集将有助于推动基于学习的电力系统管理优化技术的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Optimally+Dispatch+Power%3A+Performance+on+a+Nation-Wide+Real-World+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24505&send_immediately=true&force_search=false)

**原文摘要:** The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in
power system operations, ensuring voltage stability and minimizing power
losses. Recent advances in machine learning, particularly within the ``learning
to optimize'' framework, have enabled fast and efficient approximations of ORPD
solutions, typically by training models on precomputed optimization results.
While these approaches have demonstrated promising performance on synthetic
datasets, their effectiveness under real-world grid conditions remains largely
unexplored. This paper makes two key contributions. First, we introduce a
publicly available power system dataset that includes both the structural
characteristics of Uruguay's electrical grid and nearly two years of real-world
operational data, encompassing actual demand and generation profiles. Given
Uruguay's high penetration of renewable energy, the ORPD problem has become the
primary optimization challenge in its power network. Second, we assess the
impact of real-world data on learning-based ORPD solutions, revealing a
significant increase in prediction errors when transitioning from synthetic to
actual demand and generation inputs. Our results highlight the limitations of
existing models in learning under the complex statistical properties of real
grid conditions and emphasize the need for more expressive architectures. By
providing this dataset, we aim to facilitate further research into robust
learning-based optimization techniques for power system management.

</details>


### [109] [Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding](https://arxiv.org/abs/2505.24791)
*Jiaru Zhang, Juanwu Lu, Ziran Wang, Ruqi Zhang*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了选择性雅可比解码策略，加速自回归推理过程，显著提升了生成模型的推理速度而不影响生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是解决归一化流模型在架构约束下表达能力受限以及自回归建模推理速度慢的问题，以提高其实用性和部署效率。

**方法:** 提出了一种选择性雅可比解码（SeJD）策略，通过并行迭代优化加速自回归推理，并进行了理论分析和实证评估。

**结果:** 实验结果表明，该方法在多个数据集上验证了加速技术的有效性和通用性，推理速度提高了最高达4.7倍，同时保持了生成质量和保真度。

**结论:** 论文得出结论，严格顺序依赖在推理中对于生成高质量样本并不必要，并且通过选择性雅可比解码策略，可以在不降低生成质量的情况下显著加速自回归推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inference+Acceleration+of+Autoregressive+Normalizing+Flows+by+Selective+Jacobi+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24791&send_immediately=true&force_search=false)

**原文摘要:** Normalizing flows are promising generative models with advantages such as
theoretical rigor, analytical log-likelihood computation, and end-to-end
training. However, the architectural constraints to ensure invertibility and
tractable Jacobian computation limit their expressive power and practical
usability. Recent advancements utilize autoregressive modeling, significantly
enhancing expressive power and generation quality. However, such sequential
modeling inherently restricts parallel computation during inference, leading to
slow generation that impedes practical deployment. In this paper, we first
identify that strict sequential dependency in inference is unnecessary to
generate high-quality samples. We observe that patches in sequential modeling
can also be approximated without strictly conditioning on all preceding
patches. Moreover, the models tend to exhibit low dependency redundancy in the
initial layer and higher redundancy in subsequent layers. Leveraging these
observations, we propose a selective Jacobi decoding (SeJD) strategy that
accelerates autoregressive inference through parallel iterative optimization.
Theoretical analyses demonstrate the method's superlinear convergence rate and
guarantee that the number of iterations required is no greater than the
original sequential approach. Empirical evaluations across multiple datasets
validate the generality and effectiveness of our acceleration technique.
Experiments demonstrate substantial speed improvements up to 4.7 times faster
inference while keeping the generation quality and fidelity.

</details>


### [110] [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/abs/2505.24823)
*Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo*

**主要类别:** cs.LG

**AI概要:** PhySense 基准测试表明，当前的大型语言模型缺乏类似人类专家的基于物理原则的高效推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在解决复杂科学问题上取得了进展，但它们仍然难以模仿人类专家基于核心物理原理的简洁推理过程。

**方法:** 引入了一个新的基于物理原理的推理基准 PhySense，用于系统地评估当前最先进的 LLMs 的表现和提示类型。

**结果:** 多个先进 LLMs 和提示类型的评估结果显示，它们普遍无法与专家的推理路径保持一致，揭示了其局限性。

**结论:** 当前的大型语言模型在基于核心物理原理的高效、可解释的问题解决能力方面存在明显不足，需要进一步研究来弥补这一缺陷。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PhySense%3A+Principle-Based+Physics+Reasoning+Benchmarking+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24823&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have rapidly advanced and are increasingly
capable of tackling complex scientific problems, including those in physics.
Despite this progress, current LLMs often fail to emulate the concise,
principle-based reasoning characteristic of human experts, instead generating
lengthy and opaque solutions. This discrepancy highlights a crucial gap in
their ability to apply core physical principles for efficient and interpretable
problem solving. To systematically investigate this limitation, we introduce
PhySense, a novel principle-based physics reasoning benchmark designed to be
easily solvable by experts using guiding principles, yet deceptively difficult
for LLMs without principle-first reasoning. Our evaluation across multiple
state-of-the-art LLMs and prompt types reveals a consistent failure to align
with expert-like reasoning paths, providing insights for developing AI systems
with efficient, robust and interpretable principle-based scientific reasoning.

</details>


### [111] [Airborne Neural Network](https://arxiv.org/abs/2505.24513)
*Paritosh Ranjan, Surajit Majumder, Prodip Roy*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了将大规模深度学习系统部署到航空航天领域的一种新方法。


<details>
  <summary>更多</summary>
  
**动机:** 由于基础设施的限制，在需要实时数据处理和超低延迟的航空航天领域部署大规模深度学习系统仍然是一项挑战。

**方法:** 提出了一种分布式架构，多个空中设备各自托管神经网络的一部分神经元，并在空中网络控制器和特定层控制器的指导下协同计算。

**结果:** 该方法可以在飞行过程中实现实时学习和推理，并有可能彻底改变包括空中交通控制、实时天气和地理预测以及动态地理空间数据处理在内的航空航天应用。

**结论:** 本文提出了一种空中神经网络的新概念，为下一代人工智能驱动的航空航天系统奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Airborne+Neural+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24513，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24513&send_immediately=true&force_search=false)

**原文摘要:** Deep Learning, driven by neural networks, has led to groundbreaking
advancements in Artificial Intelligence by enabling systems to learn and adapt
like the human brain. These models have achieved remarkable results,
particularly in data-intensive domains, supported by massive computational
infrastructure. However, deploying such systems in Aerospace, where real time
data processing and ultra low latency are critical, remains a challenge due to
infrastructure limitations. This paper proposes a novel concept: the Airborne
Neural Network a distributed architecture where multiple airborne devices each
host a subset of neural network neurons. These devices compute collaboratively,
guided by an airborne network controller and layer specific controllers,
enabling real-time learning and inference during flight. This approach has the
potential to revolutionize Aerospace applications, including airborne air
traffic control, real-time weather and geographical predictions, and dynamic
geospatial data processing. By enabling large-scale neural network operations
in airborne environments, this work lays the foundation for the next generation
of AI powered Aerospace systems.

</details>


### [112] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
*Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi*

**主要类别:** cs.LG

**AI概要:** 本文提出了REDI框架，通过结合正向和负向推理轨迹显著提升小型模型在数学推理任务中的表现，并在1.5B模型上实现了新的最优结果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的模型蒸馏方法通常使用拒绝采样丢弃错误的推理示例，而这些数据可能具有重要价值。论文旨在探索如何有效利用这些数据来提升模型的推理能力。

**方法:** 提出了一种名为Reinforcement Distillation (REDI)的两阶段框架，第一阶段使用监督微调学习正向推理轨迹，第二阶段通过一种新的、无需参考的损失函数结合正向和负向推理轨迹进行优化。

**结果:** Qwen-REDI-1.5B模型在MATH-500测试中达到了83.1%的准确率（pass@1），其性能与甚至超过基于80万私有数据训练的DeepSeek-R1-Distill-Qwen-1.5B模型。

**结论:** REDI框架能够有效利用正向和负向推理轨迹，提高小型模型在离线设置下的推理性能，并在1.5B模型上使用公开数据实现了新的最优结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Harnessing+Negative+Signals%3A+Reinforcement+Distillation+from+Teacher+Data+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24850，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24850&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [113] [Transformers Are Universally Consistent](https://arxiv.org/abs/2505.24531)
*Sagar Ghosh, Kushal Bose, Swagatam Das*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了在双曲空间中使用Transformers执行普通最小二乘回归的能力，提供了理论保证并验证了其在现实数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Transformers在基础模型和大规模语言建模中取得了成功，但其理论基础仍不完善，尤其是其在序列函数回归任务中的稳健性尚未解决。考虑到现实世界数据分布的非欧几何特性，研究Transformers是否能够稳健地执行序列函数回归具有重要意义。

**方法:** 论文的方法基于理论分析，推导出经验误差的确定性上界，并通过实证评估进行验证，考虑了双曲空间和欧几里得空间中的情况。

**结果:** 论文的结果表明，当输入和输出嵌入到双曲空间时，Transformers可以均匀一致地执行OLS回归，并给出了误差上界的渐近衰减速率$O(t^{-1/2d})$。此外，将欧几里得空间作为特殊情况进行了分析，并在真实数据集上验证了理论见解。

**结论:** 论文得出结论，带有softmax非线性注意力的Transformer在输入和输出嵌入到双曲空间时，执行普通最小二乘回归是均匀一致的。分析包括了对经验误差的确定性上界以及在现实数据集上的实证验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformers+Are+Universally+Consistent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24531&send_immediately=true&force_search=false)

**原文摘要:** Despite their central role in the success of foundational models and
large-scale language modeling, the theoretical foundations governing the
operation of Transformers remain only partially understood. Contemporary
research has largely focused on their representational capacity for language
comprehension and their prowess in in-context learning, frequently under
idealized assumptions such as linearized attention mechanisms. Initially
conceived to model sequence-to-sequence transformations, a fundamental and
unresolved question is whether Transformers can robustly perform functional
regression over sequences of input tokens. This question assumes heightened
importance given the inherently non-Euclidean geometry underlying real-world
data distributions. In this work, we establish that Transformers equipped with
softmax-based nonlinear attention are uniformly consistent when tasked with
executing Ordinary Least Squares (OLS) regression, provided both the inputs and
outputs are embedded in hyperbolic space. We derive deterministic upper bounds
on the empirical error which, in the asymptotic regime, decay at a provable
rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens
and $d$ the embedding dimensionality. Notably, our analysis subsumes the
Euclidean setting as a special case, recovering analogous convergence
guarantees parameterized by the intrinsic dimensionality of the data manifold.
These theoretical insights are corroborated through empirical evaluations on
real-world datasets involving both continuous and categorical response
variables.

</details>


### [114] [HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection](https://arxiv.org/abs/2505.24534)
*Florian Frantzen, Michael T. Schaub*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为HLSAD的新方法，用于检测时间演变的单纯复形中的异常，通过利用Hodge拉普拉斯算子的谱特性来提升检测性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统图异常检测技术难以捕捉对于识别复杂结构异常至关重要的高阶相互作用，因此需要一种能够处理高阶交互的方法。

**方法:** 利用单纯复形的Hodge拉普拉斯算子谱特性来建模数据点之间的多体相互作用，并结合高维单纯结构进行异常检测。

**结果:** 通过综合实验表明，HLSAD在合成和真实世界数据集上均优于现有的图方法，在事件和变化点检测任务中表现更佳。

**结论:** HLSAD方法在检测时间演变的单纯复形中的异常方面优于现有图方法，不仅提升了检测准确性，也提高了计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HLSAD%3A+Hodge+Laplacian-based+Simplicial+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24534，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24534&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose HLSAD, a novel method for detecting anomalies in
time-evolving simplicial complexes. While traditional graph anomaly detection
techniques have been extensively studied, they often fail to capture changes in
higher-order interactions that are crucial for identifying complex structural
anomalies. These higher-order interactions can arise either directly from the
underlying data itself or through graph lifting techniques. Our approach
leverages the spectral properties of Hodge Laplacians of simplicial complexes
to effectively model multi-way interactions among data points. By incorporating
higher-dimensional simplicial structures into our method, our method enhances
both detection accuracy and computational efficiency. Through comprehensive
experiments on both synthetic and real-world datasets, we demonstrate that our
approach outperforms existing graph methods in detecting both events and change
points.

</details>


### [115] [Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems](https://arxiv.org/abs/2505.24578)
*Abhishek Chandra, Taniya Kapoor, Mitrofan Curti, Koen Tiels, Elena A. Lomonova*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种神经符号算子（NSO）框架，通过结合傅里叶神经算子与稀疏模型发现方法，有效解决了压电系统中电压-位移滞回关系建模的可解释性和泛化问题。


<details>
  <summary>更多</summary>
  
**动机:** 复杂压电系统在工业应用中至关重要，但其性能受限于电压-位移滞回关系的非线性，现有神经算子方法存在可解释性和泛化能力不足的问题。

**方法:** 提出了一种神经符号算子（NSO）框架，结合了傅里叶神经算子与基于库的稀疏模型发现方法，用于推导描述滞后关系的解析算子。

**结果:** NSO能够准确预测不同和分布外电压场的位移曲线，包括蝴蝶形状的滞回关系，并且对噪声和低保真度数据也具有鲁棒性。

**结论:** NSO相比现有的神经算子和模型发现方法在多个评估指标上表现出优势，为设计、监测和维护等实际应用提供了更好的可解释性和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neuro-Symbolic+Operator+for+Interpretable+and+Generalizable+Characterization+of+Complex+Piezoelectric+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24578&send_immediately=true&force_search=false)

**原文摘要:** Complex piezoelectric systems are foundational in industrial applications.
Their performance, however, is challenged by the nonlinear voltage-displacement
hysteretic relationships. Efficient characterization methods are, therefore,
essential for reliable design, monitoring, and maintenance. Recently proposed
neural operator methods serve as surrogates for system characterization but
face two pressing issues: interpretability and generalizability.
State-of-the-art (SOTA) neural operators are black-boxes, providing little
insight into the learned operator. Additionally, generalizing them to novel
voltages and predicting displacement profiles beyond the training domain is
challenging, limiting their practical use. To address these limitations, this
paper proposes a neuro-symbolic operator (NSO) framework that derives the
analytical operators governing hysteretic relationships. NSO first learns a
Fourier neural operator mapping voltage fields to displacement profiles,
followed by a library-based sparse model discovery method, generating white-box
parsimonious models governing the underlying hysteresis. These models enable
accurate and interpretable prediction of displacement profiles across varying
and out-of-distribution voltage fields, facilitating generalizability. The
potential of NSO is demonstrated by accurately predicting voltage-displacement
hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts
displacement profiles even for noisy and low-fidelity voltage data, emphasizing
its robustness. The results highlight the advantages of NSO compared to SOTA
neural operators and model discovery methods on several evaluation metrics.
Consequently, NSO contributes to characterizing complex piezoelectric systems
while improving the interpretability and generalizability of neural operators,
essential for design, monitoring, maintenance, and other real-world scenarios.

</details>


### [116] [Conservation-preserved Fourier Neural Operator through Adaptive Correction](https://arxiv.org/abs/2505.24579)
*Chaoyu Liu, Yangming Li, Zhongying Deng, Chris Budd, Carola-Bibiane Schönlieb*

**主要类别:** cs.LG

**AI概要:** 本文提出一种新的自适应修正方法，使傅里叶神经算子能够精确满足物理守恒定律，并在多种偏微分方程上表现出优越性。


<details>
  <summary>更多</summary>
  
**动机:** 标准FNO在建模物理系统时无法保持质量、动量和范数等关键守恒定律，而现有方法无法同时精确且自适应地修正输出以满足这些守恒定律。

**方法:** 引入一个可学习矩阵，在训练过程中自适应调整解以满足守恒定律，并从理论上证明该方法的数据损失不劣于最佳满足守恒条件的FNO。

**结果:** 实验结果表明，与现有方法相比，所提出的方法在保持守恒定律的同时具有更好的性能。

**结论:** 本文提出了一种新的自适应修正方法，确保傅里叶神经算子（FNO）在学习偏微分方程数值解时能够准确满足守恒定律，并通过实验验证了该方法优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conservation-preserved+Fourier+Neural+Operator+through+Adaptive+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24579&send_immediately=true&force_search=false)

**原文摘要:** Fourier Neural Operators (FNOs) have recently emerged as a promising and
efficient approach for learning the numerical solutions to partial differential
equations (PDEs) from data. However, standard FNO often fails to preserve key
conservation laws, such as mass conservation, momentum conservation, norm
conservation, etc., which are crucial for accurately modeling physical systems.
Existing methods for incorporating these conservation laws into Fourier neural
operators are achieved by designing related loss function or incorporating
post-processing method at the training time. None of them can both exactly and
adaptively correct the outputs to satisfy conservation laws, and our
experiments show that these methods can lead to inferior performance while
preserving conservation laws. In this work, we propose a novel adaptive
correction approach to ensure the conservation of fundamental quantities. Our
method introduces a learnable matrix to adaptively adjust the solution to
satisfy the conservation law during training. It ensures that the outputs
exactly satisfy the goal conservation law and allow for more flexibility and
adaptivity for the model to correct the outputs. We theoretically show that
applying our adaptive correction to an unconstrained FNO yields a solution with
data loss no worse than that of the best conservation-satisfying FNO. We
compare our approach with existing methods on a range of representative PDEs.
Experiment results show that our method consistently outperform other methods.

</details>


### [117] [The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches](https://arxiv.org/abs/2505.24603)
*Omri Lev, Vishwak Srinivasan, Moshe Shenfeld, Katrina Ligett, Ayush Sekhari, Ashia C. Wilson*

**主要类别:** cs.LG

**AI概要:** 本文研究了高斯草图技术在瑞丽差分隐私下的应用，提出了更精确的隐私分析方法，并证明其在线性回归任务中的性能优势。


<details>
  <summary>更多</summary>
  
**动机:** 高斯草图技术被广泛应用于多个领域，但其在差分隐私方面的潜力尚未得到充分挖掘。

**方法:** 通过瑞丽差分隐私的视角对高斯草图技术进行隐私分析，并将其应用于线性回归问题。

**结果:** 论文提供了更精确的隐私分析方法，并展示了该方法在多个数据集上的性能优势。

**结论:** 论文得出高斯草图技术在瑞丽差分隐私分析下能提升线性回归性能，并减少运行时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Gaussian+Mixing+Mechanism%3A+Renyi+Differential+Privacy+via+Gaussian+Sketches，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24603，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24603&send_immediately=true&force_search=false)

**原文摘要:** Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.

</details>


### [118] [Multi-criteria Rank-based Aggregation for Explainable AI](https://arxiv.org/abs/2505.24612)
*Sujoy Chatterjee, Everton Romanzini Colombo, Marcos Medeiros Raimundo*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的多准则决策方法来优化机器学习模型解释的质量评估和聚合，有效平衡多个冲突指标并验证了其性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决不同解释方法对同一预测可能提供相互矛盾的解释以及多质量指标之间存在权衡的问题，需要采用更有效的多准则决策方法。

**方法:** 引入一种基于排名的多准则决策加权聚合方法，并提出了XAI指标（复杂度、真实性和稳定性）的排名版本用于更好地评估特征重要性解释。

**结果:** 实验表明该方法在多种公开数据集上具有良好的鲁棒性，同时比较分析显示TOPSIS和WSUM是最佳的多准则决策和排名聚合算法。

**结论:** 本文提出了一种基于多准则决策和排名的加权聚合方法，以解决现有解释方法在平衡多个质量指标方面的不足，并证明了TOPSIS和WSUM是适用于此类任务的最佳算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-criteria+Rank-based+Aggregation+for+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24612，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24612&send_immediately=true&force_search=false)

**原文摘要:** Explainability is crucial for improving the transparency of black-box machine
learning models. With the advancement of explanation methods such as LIME and
SHAP, various XAI performance metrics have been developed to evaluate the
quality of explanations. However, different explainers can provide contrasting
explanations for the same prediction, introducing trade-offs across conflicting
quality metrics. Although available aggregation approaches improve robustness,
reducing explanations' variability, very limited research employed a
multi-criteria decision-making approach. To address this gap, this paper
introduces a multi-criteria rank-based weighted aggregation method that
balances multiple quality metrics simultaneously to produce an ensemble of
explanation models. Furthermore, we propose rank-based versions of existing XAI
metrics (complexity, faithfulness and stability) to better evaluate ranked
feature importance explanations. Extensive experiments on publicly available
datasets demonstrate the robustness of the proposed model across these metrics.
Comparative analyses of various multi-criteria decision-making and rank
aggregation algorithms showed that TOPSIS and WSUM are the best candidates for
this use case.

</details>


### [119] [Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees](https://arxiv.org/abs/2505.24627)
*Fu Luo, Yaoxin Wu, Zhi Zheng, Zhenkun Wang*

**主要类别:** cs.LG

**AI概要:** 本文研究了神经组合优化方法在不同容量约束紧度下的性能，发现其存在过拟合问题，并提出了一种新的训练方案与多专家模块来提升模型在不同约束条件下的泛化能力，实验验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 近年来的神经组合优化（NCO）方法无需领域专业知识即可展现出良好的问题求解能力。然而，大多数现有的NCO方法使用的训练和测试数据具有固定的约束值，缺乏对约束紧度变化影响的研究。因此，本文旨在研究不同约束紧度对NCO方法性能的影响，并提出改进方案。

**方法:** 本文以容量约束车辆路径问题（CVRP）为例，实证分析了不同容量约束紧度下NCO方法的性能。为了克服现有方法的局限性，作者设计了一种考虑不同约束紧度的高效训练方案，并引入多专家模块以学习通用的求解策略。

**结果:** 实验结果显示，现有的NCO方法在容量约束上存在过拟合现象，仅在特定约束值范围内表现良好。而作者提出的训练方案结合多专家模块能够有效缓解过拟合问题，在多种约束紧度下的CVRP和CVRPTW问题中均表现出优越的性能。

**结论:** 论文得出结论，现有的NCO方法在容量约束下存在过拟合问题，只能在小范围的约束值上表现良好，而在其他值上表现不佳。作者通过开发一种高效的训练方案并提出多专家模块来解决这一问题，实验结果表明该方法能有效克服过拟合问题，并在不同约束紧度下的CVRP和CVRPTW中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Neural+Combinatorial+Optimization+for+Vehicle+Routing+Problems+with+Different+Constraint+Tightness+Degrees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24627&send_immediately=true&force_search=false)

**原文摘要:** Recent neural combinatorial optimization (NCO) methods have shown promising
problem-solving ability without requiring domain-specific expertise. Most
existing NCO methods use training and testing data with a fixed constraint
value and lack research on the effect of constraint tightness on the
performance of NCO methods. This paper takes the capacity-constrained vehicle
routing problem (CVRP) as an example to empirically analyze the NCO performance
under different tightness degrees of the capacity constraint. Our analysis
reveals that existing NCO methods overfit the capacity constraint, and they can
only perform satisfactorily on a small range of the constraint values but
poorly on other values. To tackle this drawback of existing NCO methods, we
develop an efficient training scheme that explicitly considers varying degrees
of constraint tightness and proposes a multi-expert module to learn a generally
adaptable solving strategy. Experimental results show that the proposed method
can effectively overcome the overfitting issue, demonstrating superior
performances on the CVRP and CVRP with time windows (CVRPTW) with various
constraint tightness degrees.

</details>


### [120] [Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks](https://arxiv.org/abs/2505.24629)
*Lotte Bransen, Tim Janssen, Jesse Davis*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种通用的点球对抗模拟框架，用以优化守门员的应对策略，并结合实际数据验证其效果。


<details>
  <summary>更多</summary>
  
**动机:** 点球是足球比赛中具有决定性的关键时刻，但现有数据分析方法通常假设守门员和罚球者独立行动，这种简化与现实不符，因此需要一种更贴近实际的分析方法。

**方法:** 开发了一个不依赖具体球员的模拟框架，用于评估守门员策略的有效性，同时考虑了丰富的决策选择并整合了守门员的技能信息。

**结果:** 该框架能够基于大量经专家标注的数据集，对守门员的策略进行优化，并在真实场景中提供实用建议。

**结论:** 论文得出一个可行的守门员策略优化方法，通过模拟框架评估不同策略的效果，并结合守门员自身能力信息进行分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stop+Guessing%3A+Optimizing+Goalkeeper+Policies+for+Soccer+Penalty+Kicks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24629&send_immediately=true&force_search=false)

**原文摘要:** Penalties are fraught and game-changing moments in soccer games that teams
explicitly prepare for. Consequently, there has been substantial interest in
analyzing them in order to provide advice to practitioners. From a data science
perspective, such analyses suffer from a significant limitation: they make the
unrealistic simplifying assumption that goalkeepers and takers select their
action -- where to dive and where to the place the kick -- independently of
each other. In reality, the choices that some goalkeepers make depend on the
taker's movements and vice-versa. This adds substantial complexity to the
problem because not all players have the same action capacities, that is, only
some players are capable of basing their decisions on their opponent's
movements. However, the small sample sizes on the player level mean that one
may have limited insights into a specific opponent's capacities. We address
these challenges by developing a player-agnostic simulation framework that can
evaluate the efficacy of different goalkeeper strategies. It considers a rich
set of choices and incorporates information about a goalkeeper's skills. Our
work is grounded in a large dataset of penalties that were annotated by penalty
experts and include aspects of both kicker and goalkeeper strategies. We show
how our framework can be used to optimize goalkeeper policies in real-world
situations.

</details>


### [121] [WILTing Trees: Interpreting the Distance Between MPNN Embeddings](https://arxiv.org/abs/2505.24642)
*Masahiro Negishi, Thomas Gärtner, Pascal Welke*

**主要类别:** cs.LG

**AI概要:** 该论文研究了消息传递神经网络（MPNNs）在特定任务中学习到的距离函数，并提出了一种可解释的方法来分析MPNN嵌入之间的功能距离。


<details>
  <summary>更多</summary>
  
**动机:** 以前的研究将MPNN的距离与忽略任务特定信息的图结构距离联系起来，而本文旨在填补这一空白，关注MPNN在具体任务中隐式学习的功能距离。

**方法:** 通过在Weisfeiler Leman Labeling Tree (WILT)上使用最优传输方法，将MPNN嵌入之间的距离提炼为一种可解释的图距离，其中边权重揭示了对嵌入距离有显著影响的子图。

**结果:** 实验表明，MPNNs通过关注少量在领域内具有功能重要性的子图来定义嵌入的相对位置；此外，所提方法能够在线性时间内计算，并推广了两个著名的图核方法。

**结论:** MPNNs在特定任务中隐式学习嵌入距离，这种距离可以通过任务相关的子图进行解释和计算。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WILTing+Trees%3A+Interpreting+the+Distance+Between+MPNN+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24642，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24642&send_immediately=true&force_search=false)

**原文摘要:** We investigate the distance function learned by message passing neural
networks (MPNNs) in specific tasks, aiming to capture the functional distance
between prediction targets that MPNNs implicitly learn. This contrasts with
previous work, which links MPNN distances on arbitrary tasks to structural
distances on graphs that ignore task-specific information. To address this gap,
we distill the distance between MPNN embeddings into an interpretable graph
distance. Our method uses optimal transport on the Weisfeiler Leman Labeling
Tree (WILT), where the edge weights reveal subgraphs that strongly influence
the distance between embeddings. This approach generalizes two well-known graph
kernels and can be computed in linear time. Through extensive experiments, we
demonstrate that MPNNs define the relative position of embeddings by focusing
on a small set of subgraphs that are known to be functionally important in the
domain.

</details>


### [122] [Learning Distributions over Permutations and Rankings with Factorized Representations](https://arxiv.org/abs/2505.24664)
*Daniel Severo, Brian Karrer, Niklas Nolte*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于排列不同表示形式（如Lehmer代码、Fisher-Yates抽样和插入向量）的新方法，用于学习排列分布，相比现有方法更具表达能力和计算效率，实验验证了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 学习排列分布是机器学习中的一个基本问题，广泛应用于排名、组合优化、结构化预测和数据关联等领域。然而，现有的方法依赖于参数家族的混合或需要昂贵变分推断过程的神经网络，因此需要一种更高效且具有更好表达能力的方法。

**方法:** 论文提出了一种基于排列的不同表示形式（包括Lehmer代码、Fisher-Yates抽样和插入向量）的新方法，这些表示与对称群形成双射关系，从而允许使用常规深度学习技术进行无约束学习。

**结果:** 实验表明，新方法在拼图基准测试中显著优于当前方法，同时作者提出了两个新的基准测试：学习循环排列和根据用户偏好重新排序电影。结果表明，即使在最不具表达性的模式下，该方法也能学习非平凡的分布，而传统模型甚至无法生成有效的排列。

**结论:** 论文提出了一种新颖的方法来学习排列分布，通过利用排列的替代表示，如Lehmer编码、Fisher-Yates抽取和插入向量，能够使用传统的深度学习技术进行无约束学习，并且可以表示任何概率分布。此外，在最不具表达性但计算效率最高的案例中，该方法也涵盖了之前一些经典的概率模型。实验表明，该方法在多个任务上优于现有方法，尤其是在传统模型无法生成有效排列的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Distributions+over+Permutations+and+Rankings+with+Factorized+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24664&send_immediately=true&force_search=false)

**原文摘要:** Learning distributions over permutations is a fundamental problem in machine
learning, with applications in ranking, combinatorial optimization, structured
prediction, and data association. Existing methods rely on mixtures of
parametric families or neural networks with expensive variational inference
procedures. In this work, we propose a novel approach that leverages
alternative representations for permutations, including Lehmer codes,
Fisher-Yates draws, and Insertion-Vectors. These representations form a
bijection with the symmetric group, allowing for unconstrained learning using
conventional deep learning techniques, and can represent any probability
distribution over permutations. Our approach enables a trade-off between
expressivity of the model family and computational requirements. In the least
expressive and most computationally efficient case, our method subsumes
previous families of well established probabilistic models over permutations,
including Mallow's and the Repeated Insertion Model. Experiments indicate our
method significantly outperforms current approaches on the jigsaw puzzle
benchmark, a common task for permutation learning. However, we argue this
benchmark is limited in its ability to assess learning probability
distributions, as the target is a delta distribution (i.e., a single correct
solution exists). We therefore propose two additional benchmarks: learning
cyclic permutations and re-ranking movies based on user preference. We show
that our method learns non-trivial distributions even in the least expressive
mode, while traditional models fail to even generate valid permutations in this
setting.

</details>


### [123] [Learning geometry and topology via multi-chart flows](https://arxiv.org/abs/2505.24665)
*Hanlin Yu, Søren Hauberg, Marcelo Hartmann, Arto Klami, Georgios Arvanitidis*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种学习多流形映射集合的训练方案和计算流形测地线的新算法，从而显著提升了流形拓扑估计的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的数据通常位于高维空间中的低维黎曼流形上，而如果流形具有非平凡拓扑结构，则无法通过单一的归一化流正确学习，因此需要新的方法。

**方法:** 论文提出了一个学习多个归一化流（normalizing flows）集合的通用训练方案，并开发了在流形上计算测地线的第一个数值算法。

**结果:** 实验证明，所提出的方法在拓扑估计任务上带来了高度显著的改进。

**结论:** 通过组合多个流形映射并计算测地线，论文提出的方法显著改进了对流形拓扑结构的估计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+geometry+and+topology+via+multi-chart+flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24665，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24665&send_immediately=true&force_search=false)

**原文摘要:** Real world data often lie on low-dimensional Riemannian manifolds embedded in
high-dimensional spaces. This motivates learning degenerate normalizing flows
that map between the ambient space and a low-dimensional latent space. However,
if the manifold has a non-trivial topology, it can never be correctly learned
using a single flow. Instead multiple flows must be `glued together'. In this
paper, we first propose the general training scheme for learning such a
collection of flows, and secondly we develop the first numerical algorithms for
computing geodesics on such manifolds. Empirically, we demonstrate that this
leads to highly significant improvements in topology estimation.

</details>


### [124] [Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning](https://arxiv.org/abs/2505.24676)
*Mihir Bhaskar, Jun Tao Luo, Zihan Geng, Asmita Hajra, Junia Howell, Matthew R. Gormley*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种用于量化分析美国1930年代住房政策对种族财富差距影响的方法，通过OCR和回归模型建立了历史住房评估数据集。


<details>
  <summary>更多</summary>
  
**动机:** 由于历史房产估价记录难以获取，使得难以精确量化美国1930年代住房政策对种族财富差距的具体财务影响。

**方法:** 结合传统的计算机视觉技术和基于深度学习的OCR，以及基于建筑特征数据的回归模型来估计历史价值。

**结果:** 建立并发布了一个县的历史住房评估数据集，并展示了如何将这种方法推广到其他县。

**结论:** 通过使用OCR和回归模型等成本效益高的工具，学者、社区活动家和政策制定者可以更好地分析和理解红线政策的历史影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+the+Past%3A+Estimating+Historical+Appraisals+with+OCR+and+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24676，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24676&send_immediately=true&force_search=false)

**原文摘要:** Despite well-documented consequences of the U.S. government's 1930s housing
policies on racial wealth disparities, scholars have struggled to quantify its
precise financial effects due to the inaccessibility of historical property
appraisal records. Many counties still store these records in physical formats,
making large-scale quantitative analysis difficult. We present an approach
scholars can use to digitize historical housing assessment data, applying it to
build and release a dataset for one county. Starting from publicly available
scanned documents, we manually annotated property cards for over 12,000
properties to train and validate our methods. We use OCR to label data for an
additional 50,000 properties, based on our two-stage approach combining
classical computer vision techniques with deep learning-based OCR. For cases
where OCR cannot be applied, such as when scanned documents are not available,
we show how a regression model based on building feature data can estimate the
historical values, and test the generalizability of this model to other
counties. With these cost-effective tools, scholars, community activists, and
policy makers can better analyze and understand the historical impacts of
redlining.

</details>


### [125] [PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations](https://arxiv.org/abs/2505.24717)
*Benjamin Holzschuh, Qiang Liu, Georg Kohl, Nils Thuerey*

**主要类别:** cs.LG

**AI概要:** 论文提出了PDE-Transformer，这是一种专为物理模拟设计的transformer架构，其通过创新性的调整和方法，在处理多类型PDEs时展现出优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 需要一种更加可扩展和通用的transformer架构来解决物理模拟中的问题，尤其是在处理多类型PDEs时提高信息密度的一致性。

**方法:** 结合扩散transformer的最新架构改进和针对大规模模拟的调整，引入了一种新的transformer架构。此外，还采用通道自注意力机制来处理不同的物理通道。

**结果:** 实验表明，该架构在16种不同类型的PDEs数据集上优于现有的计算机视觉transformer架构；预训练模型在多个下游任务中也表现出色。

**结论:** PDE-Transformer是一种改进的基于transformer的架构，用于规则网格上的物理模拟代理建模，并且在大规模物理科学基础模型构建中具有更广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PDE-Transformer%3A+Efficient+and+Versatile+Transformers+for+Physics+Simulations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24717，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24717&send_immediately=true&force_search=false)

**原文摘要:** We introduce PDE-Transformer, an improved transformer-based architecture for
surrogate modeling of physics simulations on regular grids. We combine recent
architectural improvements of diffusion transformers with adjustments specific
for large-scale simulations to yield a more scalable and versatile
general-purpose transformer architecture, which can be used as the backbone for
building large-scale foundation models in physical sciences. We demonstrate
that our proposed architecture outperforms state-of-the-art transformer
architectures for computer vision on a large dataset of 16 different types of
PDEs. We propose to embed different physical channels individually as
spatio-temporal tokens, which interact via channel-wise self-attention. This
helps to maintain a consistent information density of tokens when learning
multiple types of PDEs simultaneously. We demonstrate that our pre-trained
models achieve improved performance on several challenging downstream tasks
compared to training from scratch and also beat other foundation model
architectures for physics simulations.

</details>


### [126] [Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach](https://arxiv.org/abs/2505.24721)
*Nick Rossenbach, Benedikt Hilmes, Leon Brackmann, Moritz Gunz, Ralf Schlüter*

**主要类别:** cs.LG

**AI概要:** 基于忆阻器的硬件提供了节能机器学习的新可能性，但目前的硬件原型无法容纳大型神经网络。我们提出了一个基于“Synaptogen”的PyTorch库来模拟具有准确捕捉的忆阻器硬件属性的神经网络执行情况。


<details>
  <summary>更多</summary>
  
**动机:** 当前硬件原型无法容纳大型神经网络，相关文献仅覆盖了用于小规模任务的小型机器学习模型。为了探索硬件属性如何影响更大的模型，需要使用仿真方法，但现有的软件假设了简化的硬件。

**方法:** 提出了一种基于PyTorch的名为“Synaptogen”的库，用于模拟带有真实忆阻器硬件属性的神经网络执行。该方法包括调整量化感知训练以适应低精度权重计算。

**结果:** 首次展示了千万级参数的ML系统在忆阻器硬件上的表现，并通过TED-LIUMv2语音识别任务验证了3位权重精度下词错误率相对退化限制在25%以内。

**结论:** 本研究表明，在模拟的忆阻器硬件上运行的神经网络可以通过适当调整训练策略来保持性能，从而为高效能机器学习提供新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Running+Conventional+Automatic+Speech+Recognition+on+Memristor+Hardware%3A+A+Simulated+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24721&send_immediately=true&force_search=false)

**原文摘要:** Memristor-based hardware offers new possibilities for energy-efficient
machine learning (ML) by providing analog in-memory matrix multiplication.
Current hardware prototypes cannot fit large neural networks, and related
literature covers only small ML models for tasks like MNIST or single word
recognition. Simulation can be used to explore how hardware properties affect
larger models, but existing software assumes simplified hardware. We propose a
PyTorch-based library based on "Synaptogen" to simulate neural network
execution with accurately captured memristor hardware properties. For the first
time, we show how an ML system with millions of parameters would behave on
memristor hardware, using a Conformer trained on the speech recognition task
TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the
relative degradation in word error rate to 25% when using a 3-bit weight
precision to execute linear operations via simulated analog computation.

</details>


### [127] [Robust Federated Learning against Model Perturbation in Edge Networks](https://arxiv.org/abs/2505.24728)
*Dongzi Jin, Yong Xiao, Yingyu Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的联邦学习方法SMRFL，用于增强模型对扰动的鲁棒性，其通过解决一个min-max优化问题实现，并且理论和实验结果均证明了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的共享模型通常假设是理想的，在实践中由于各种扰动会受到性能下降的影响。因此需要一种更鲁棒的方法。

**方法:** 提出了一种名为Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL)的新方法，通过最小化模型参数邻域内的最大损失来促进模型向平坦最小值收敛，从而提升模型鲁棒性。

**结果:** 实验结果表明，与三种基线方法相比，SMRFL在两个真实数据集和三种扰动场景下显著提升了模型的鲁棒性。

**结论:** SMRFL能够在不牺牲收敛速度的情况下提高模型对扰动的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Federated+Learning+against+Model+Perturbation+in+Edge+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24728，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24728&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) is a promising paradigm for realizing edge
intelligence, allowing collaborative learning among distributed edge devices by
sharing models instead of raw data. However, the shared models are often
assumed to be ideal, which would be inevitably violated in practice due to
various perturbations, leading to significant performance degradation. To
overcome this challenge, we propose a novel method, termed Sharpness-Aware
Minimization-based Robust Federated Learning (SMRFL), which aims to improve
model robustness against perturbations by exploring the geometrical property of
the model landscape. Specifically, SMRFL solves a min-max optimization problem
that promotes model convergence towards a flat minimum by minimizing the
maximum loss within a neighborhood of the model parameters. In this way, model
sensitivity to perturbations is reduced, and robustness is enhanced since
models in the neighborhood of the flat minimum also enjoy low loss values. The
theoretical result proves that SMRFL can converge at the same rate as FL
without perturbations. Extensive experimental results show that SMRFL
significantly enhances robustness against perturbations compared to three
baseline methods on two real-world datasets under three perturbation scenarios.

</details>


### [128] [Feature Attribution from First Principles](https://arxiv.org/abs/2505.24729)
*Magamed Taimeskhanov, Damien Garreau*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种不依赖严格公理的新特征归因框架，通过从简单模型构建复杂模型的方式推导出现有和新的归因方法。


<details>
  <summary>更多</summary>
  
**动机:** 因为现有的公理化框架往往过于限制，而经验评估这些方法仍然是一个重大挑战，因此需要一种更灵活的方法来构建特征归因框架。

**方法:** 通过从最简单的模型（即指示函数）开始定义归因，并将这些作为构建块用于更复杂模型的归因，从而提出新的特征归因框架。

**结果:** 得出了针对深度ReLU网络的归因闭式表达式，并朝着优化评价指标相对于特征归因的方向迈进了一步。

**结论:** 论文得出了一种新的特征归因框架，该框架不是基于公理而是从最简单的模型开始定义归因，并展示了如何根据原子归因的选择恢复几种现有的归因方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature+Attribution+from+First+Principles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24729&send_immediately=true&force_search=false)

**原文摘要:** Feature attribution methods are a popular approach to explain the behavior of
machine learning models. They assign importance scores to each input feature,
quantifying their influence on the model's prediction. However, evaluating
these methods empirically remains a significant challenge. To bypass this
shortcoming, several prior works have proposed axiomatic frameworks that any
feature attribution method should satisfy. In this work, we argue that such
axioms are often too restrictive, and propose in response a new feature
attribution framework, built from the ground up. Rather than imposing axioms,
we start by defining attributions for the simplest possible models, i.e.,
indicator functions, and use these as building blocks for more complex models.
We then show that one recovers several existing attribution methods, depending
on the choice of atomic attribution. Subsequently, we derive closed-form
expressions for attribution of deep ReLU networks, and take a step toward the
optimization of evaluation metrics with respect to feature attributions.

</details>


### [129] [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
*Yehonathan Refael, Guy Smorodinsky, Tom Tirer, Ofir Lindenbaum*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的优化器SUMO，通过精确的SVD正交化来提升大型语言模型训练的收敛速度和稳定性，同时减少内存消耗。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是为了解决现有的低秩梯度优化方法主要关注内存效率而忽视收敛加速的问题，尤其是在深度网络和大语言模型中高度各向异性的损失景观下表现不佳的情况。

**方法:** 论文提出了一种名为SUMO的优化器，利用精确的奇异值分解（SVD）进行动量正交化，并通过动态适应的低维子空间实现范数诱导的最速下降优化步骤。

**结果:** 结果表明，SUMO能够有效缓解近似正交化方法带来的误差，理论上建立了这些误差的上界，并通过实验证明了其在收敛性、稳定性和内存使用方面的优势。

**结论:** 论文得出结论，SUMO优化器在收敛速度、稳定性、性能以及内存需求方面相较于现有最先进的方法有显著改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SUMO%3A+Subspace-Aware+Moment-Orthogonalization+for+Accelerating+Memory-Efficient+LLM+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24749&send_immediately=true&force_search=false)

**原文摘要:** Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential acceleration in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.

</details>


### [130] [AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption](https://arxiv.org/abs/2505.24773)
*Yajie Zhou, Xiaoyi Pang, Zhibo Wang*

**主要类别:** cs.LG

**AI概要:** AFLoRA提出了一种高效的联邦微调框架，用于在资源受限和异构的现实环境中优化大型语言模型（LLMs）的适应性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法难以在保证低系统成本的同时有效聚合低秩更新，且受数据异质性和最弱客户端性能瓶颈的影响，因此需要一种更优的联邦微调方案。

**方法:** AFLoRA通过解耦共享与客户端特定更新、基于对角矩阵的秩剪枝以及结合公共数据优化的秩感知聚合，减少通信计算开销并提升模型性能。

**结果:** 实验表明，AFLoRA在准确率和效率方面均优于当前最先进的方法，为现实世界中异构环境下的LLM适配提供了实用解决方案。

**结论:** AFLoRA是一种有效的联邦学习框架，能够应对LLMs在分布式异构环境下微调所面临的挑战，具备较高的应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AFLoRA%3A+Adaptive+Federated+Fine-Tuning+of+Large+Language+Models+with+Resource-Aware+Low-Rank+Adaption，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24773&send_immediately=true&force_search=false)

**原文摘要:** Federated fine-tuning has emerged as a promising approach to adapt foundation
models to downstream tasks using decentralized data. However, real-world
deployment remains challenging due to the high computational and communication
demands of fine-tuning Large Language Models (LLMs) on clients with data and
system resources that are heterogeneous and constrained. In such settings, the
global model's performance is often bottlenecked by the weakest clients and
further degraded by the non-IID nature of local data. Although existing methods
leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to
reduce communication and computation overhead, they often fail to
simultaneously ensure accurate aggregation of low-rank updates and maintain low
system costs, thereby hindering overall performance. To address these
challenges, we propose AFLoRA, an adaptive and lightweight federated
fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific
updates to reduce overhead and improve aggregation accuracy, incorporates
diagonal matrix-based rank pruning to better utilize local resources, and
employs rank-aware aggregation with public data refinement to strengthen
generalization under data heterogeneity. Extensive experiments demonstrate that
AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,
providing a practical solution for efficient LLM adaptation in heterogeneous
environments in the real world.

</details>


### [131] [Diffusion-Based Symbolic Regression](https://arxiv.org/abs/2505.24776)
*Zachary Bastiani, Robert M. Kirby, Jacob Hochhalter, Shandian Zhe*

**主要类别:** cs.LG

**AI概要:** 该论文介绍了一种利用扩散模型解决符号回归问题的新方法，并通过实验验证其性能。


<details>
  <summary>更多</summary>
  
**动机:** 扩散模型在生成建模方面表现出色，启发作者将其应用于符号回归问题。

**方法:** 构建了基于随机掩码的扩散和去噪过程，并结合GRPO方法进行强化学习，引入长期风险寻求策略以提高性能。

**结果:** 所提出的方法在广泛实验和消融研究中证明是有效的。

**结论:** 论文提出了一种基于扩散的符号回归新方法，实验表明其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion-Based+Symbolic+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24776，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24776&send_immediately=true&force_search=false)

**原文摘要:** Diffusion has emerged as a powerful framework for generative modeling,
achieving remarkable success in applications such as image and audio synthesis.
Enlightened by this progress, we propose a novel diffusion-based approach for
symbolic regression. We construct a random mask-based diffusion and denoising
process to generate diverse and high-quality equations. We integrate this
generative processes with a token-wise Group Relative Policy Optimization
(GRPO) method to conduct efficient reinforcement learning on the given
measurement dataset. In addition, we introduce a long short-term risk-seeking
policy to expand the pool of top-performing candidates, further enhancing
performance. Extensive experiments and ablation studies have demonstrated the
effectiveness of our approach.

</details>


### [132] [EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation](https://arxiv.org/abs/2505.24779)
*Yidong Luo, Chenguang Wang, Jiahao Yang, Fanzeng Xia, Tianshu Yu*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一个全面的基准框架，用于评估混合整数线性规划实例的生成方法，重点在于质量和实用性。


<details>
  <summary>更多</summary>
  
**动机:** 由于MILP实例生成方法的发展速度超过了标准化评估技术，因此需要一种系统的方法来评估合成MILP实例的真实性和实用性。

**方法:** 通过分析求解器内部特征，包括根节点间隙、启发式成功率和割平面使用情况，提供统一且可扩展的方法来评估实例质量。

**结果:** 该框架能够有效地进行实例集保真度的系统比较，并促进不同生成技术之间的稳健比较。

**结论:** 该论文提出了一种全面的基准框架，用于系统和客观地评估MILP实例生成方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EVA-MILP%3A+Towards+Standardized+Evaluation+of+MILP+Instance+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24779，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24779&send_immediately=true&force_search=false)

**原文摘要:** Mixed-Integer Linear Programming (MILP) is fundamental to solving complex
decision-making problems. The proliferation of MILP instance generation
methods, driven by machine learning's demand for diverse optimization datasets
and the limitations of static benchmarks, has significantly outpaced
standardized evaluation techniques. Consequently, assessing the fidelity and
utility of synthetic MILP instances remains a critical, multifaceted challenge.
This paper introduces a comprehensive benchmark framework designed for the
systematic and objective evaluation of MILP instance generation methods. Our
framework provides a unified and extensible methodology, assessing instance
quality across crucial dimensions: mathematical validity, structural
similarity, computational hardness, and utility in downstream machine learning
tasks. A key innovation is its in-depth analysis of solver-internal features --
particularly by comparing distributions of key solver outputs including root
node gap, heuristic success rates, and cut plane usage -- leveraging the
solver's dynamic solution behavior as an `expert assessment' to reveal nuanced
computational resemblances. By offering a structured approach with clearly
defined solver-independent and solver-dependent metrics, our benchmark aims to
facilitate robust comparisons among diverse generation techniques, spur the
development of higher-quality instance generators, and ultimately enhance the
reliability of research reliant on synthetic MILP data. The framework's
effectiveness in systematically comparing the fidelity of instance sets is
demonstrated using contemporary generative models.

</details>


### [133] [QGAN-based data augmentation for hybrid quantum-classical neural networks](https://arxiv.org/abs/2505.24780)
*Run-Ze He, Jun-Jian Su, Su-Juan Qin, Zheng-Ping Jin, Fei Gao*

**主要类别:** cs.LG

**AI概要:** 论文研究了量子神经网络在数据增强方面的潜力，开发了一个结合量子生成对抗网络和混合量子经典神经网络的数据增强框架，并验证其在提升模型准确性和性能方面的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决量子机器学习中的数据稀缺问题，并探索数据增强在该领域中的应用。

**方法:** 将量子生成对抗网络与混合量子经典神经网络结合，提出了一种增强数据处理和分类的通用方法以及一个根据HQCNN在特定数据类别上表现动态生成样本的定制策略。

**结果:** 在MNIST数据集上的仿真实验表明，与传统数据增强方法和经典GAN相比，QGAN具有更好的性能，在参数减少一半的情况下达到了与DCGAN相当的表现。

**结论:** 量子生成对抗网络（QGANs）能够简化模型并生成高质量数据，提高混合量子经典卷积神经网络（HQCNNs）的准确性和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QGAN-based+data+augmentation+for+hybrid+quantum-classical+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24780&send_immediately=true&force_search=false)

**原文摘要:** Quantum neural networks converge faster and achieve higher accuracy than
classical models. However, data augmentation in quantum machine learning
remains underexplored. To tackle data scarcity, we integrate quantum generative
adversarial networks (QGANs) with hybrid quantum-classical neural networks
(HQCNNs) to develop an augmentation framework. We propose two strategies: a
general approach to enhance data processing and classification across HQCNNs,
and a customized strategy that dynamically generates samples tailored to the
HQCNN's performance on specific data categories, improving its ability to learn
from complex datasets. Simulation experiments on the MNIST dataset demonstrate
that QGAN outperforms traditional data augmentation methods and classical GANs.
Compared to baseline DCGAN, QGAN achieves comparable performance with half the
parameters, balancing efficiency and effectiveness. This suggests that QGANs
can simplify models and generate high-quality data, enhancing HQCNN accuracy
and performance. These findings pave the way for applying quantum data
augmentation techniques in machine learning.

</details>


### [134] [ByzFL: Research Framework for Robust Federated Learning](https://arxiv.org/abs/2505.24802)
*Marc González, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, François Taïani*

**主要类别:** cs.LG

**AI概要:** ByzFL 是一个用于开发和测试鲁棒联邦学习算法的开源框架，支持多种场景模拟和快速原型设计。


<details>
  <summary>更多</summary>
  
**动机:** 为了满足对鲁棒联邦学习算法进行系统实验的需求，提供一个集中的平台来评估不同场景下的性能。

**方法:** 提供一个统一且可扩展的框架，包括最先进的鲁棒聚合器实现、一套可配置的攻击方案以及用于模拟多种 FL 场景的工具。

**结果:** 通过单一的基于 JSON 的配置文件实现了系统的实验，并包含结果可视化的内置工具，兼容 PyTorch 张量和 NumPy 数组。

**结论:** ByzFL 是一个支持开发和基准测试鲁棒联邦学习算法的开源 Python 库，旨在促进可重复的研究和快速原型设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ByzFL%3A+Research+Framework+for+Robust+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24802&send_immediately=true&force_search=false)

**原文摘要:** We present ByzFL, an open-source Python library for developing and
benchmarking robust federated learning (FL) algorithms. ByzFL provides a
unified and extensible framework that includes implementations of
state-of-the-art robust aggregators, a suite of configurable attacks, and tools
for simulating a variety of FL scenarios, including heterogeneous data
distributions, multiple training algorithms, and adversarial threat models. The
library enables systematic experimentation via a single JSON-based
configuration file and includes built-in utilities for result visualization.
Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to
facilitate reproducible research and rapid prototyping of robust FL solutions.
ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on
GitHub: https://github.com/LPD-EPFL/byzfl.

</details>


### [135] [Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting](https://arxiv.org/abs/2505.24835)
*Fuyuan Lyu, Linfeng Du, Yunpeng Weng, Qiufang Ying, Zhiyan Xu, Wen Zou, Haolun Wu, Xiuqiang He, Xing Tang*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的资金分配框架RTS-PnO，它通过解决目标不匹配和预测不确定性来提高资金分配决策的质量，并在实际应用中显示出更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 朴素的资金分配解决方案如仅预测或预测然后优化方法存在目标不匹配的问题，同时最先进的时间序列预测模型会带来额外的不确定性。这些问题促使了RTS-PnO框架的发展。

**方法:** 论文引入了一个风险感知的时间序列预测与分配框架（RTS-PnO），包含端到端训练、自适应预测不确定性校准以及对预测模型的无依赖性三个特点。

**结果:** RTS-PnO在涉及货币、股票和加密货币的金融应用中的八个数据集上进行了评估，结果显示其表现始终优于其他竞争基线。此外，在腾讯FiT的跨境支付业务上的在线实验显示，与产品线方法相比，遗憾减少了8.4%。

**结论:** 论文提出了一个名为RTS-PnO的框架，该框架能够有效地解决资金分配问题，并在离线和在线实验中都表现出优于其他基线方法的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Timing+is+important%3A+Risk-aware+Fund+Allocation+based+on+Time-Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24835，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24835&send_immediately=true&force_search=false)

**原文摘要:** Fund allocation has been an increasingly important problem in the financial
domain. In reality, we aim to allocate the funds to buy certain assets within a
certain future period. Naive solutions such as prediction-only or
Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the
introduction of the SOTA time series forecasting model inevitably introduces
additional uncertainty in the predicted result. To solve both problems
mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate
(RTS-PnO) framework, which holds no prior assumption on the forecasting models.
Such a framework contains three features: (i) end-to-end training with
objective alignment measurement, (ii) adaptive forecasting uncertainty
calibration, and (iii) agnostic towards forecasting models. The evaluation of
RTS-PnO is conducted over both online and offline experiments. For offline
experiments, eight datasets from three categories of financial applications are
used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other
competitive baselines. The online experiment is conducted on the Cross-Border
Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed
when compared with the product-line approach. The code for the offline
experiment is available at https://github.com/fuyuanlyu/RTS-PnO.

</details>


### [136] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, Alina Oprea*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了模型蒸馏过程中对抗性偏差注入的风险，发现少量中毒样本即可导致学生模型显著放大偏差，并指出当前防御手段的不足。


<details>
  <summary>更多</summary>
  
**动机:** 模型蒸馏在创建较小、可部署的语言模型方面变得至关重要，但其在面对对抗性操纵时的韧性问题引发了关注。

**方法:** 论文研究了在训练期间通过最小数据投毒注入偏差的方式，分析了两种传播模式：无目标传播和有目标传播，并评估了当前防御方法的不足之处。

**结果:** 研究表明，对手可以通过少量中毒样本将偏差注入教师模型，这些偏差会在学生模型中显著放大。在目标场景下，学生模型生成偏差响应的概率为76.9%，而教师模型为69.4%。对于未见过的任务，学生模型中的对抗偏差出现频率是教师模型的6-29倍。

**结论:** 论文得出结论，蒸馏模型存在严重的安全漏洞，需要专门的安全保障措施，并提出了构建有效对抗偏差缓解策略的实用设计原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cascading+Adversarial+Bias+from+Injection+to+Distillation+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24842，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24842&send_immediately=true&force_search=false)

**原文摘要:** Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [137] [From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching](https://arxiv.org/abs/2505.24843)
*Ruqi Bai, Yao Ji, Zeyu Zhou, David I. Inouye*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的增强模型鲁棒性的方法，称为噪声反事实匹配（NCM），即使在小规模和有噪声的数据对中也有效。


<details>
  <summary>更多</summary>
  
**动机:** 虚假相关性会导致模型在新环境中的性能下降，而现有的因果启发方法通常表现不佳。此外，测试时数据可能不可用。

**方法:** 引入了噪声反事实匹配（NCM），这是一种基于约束的方法，通过利用具有不变性的数据对提高模型鲁棒性。

**结果:** 论文证明了某些反事实对自然满足不变性属性，并且通过合成数据集验证了所提方法的有效性，展示了在线性探测预训练主干上提升鲁棒性的能力。

**结论:** 论文提出了一种名为NCM的方法，利用不变的数据对来增强模型的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Invariant+Representations+to+Invariant+Data%3A+Provable+Robustness+to+Spurious+Correlations+via+Noisy+Counterfactual+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24843，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24843&send_immediately=true&force_search=false)

**原文摘要:** Spurious correlations can cause model performance to degrade in new
environments. Prior causality-inspired works aim to learn invariant
representations (e.g., IRM) but typically underperform empirical risk
minimization (ERM). Recent alternatives improve robustness by leveraging
test-time data, but such data may be unavailable in practice. To address these
issues, we take a data-centric approach by leveraging invariant data pairs,
pairs of samples that would have the same prediction with the optimally robust
classifier. We prove that certain counterfactual pairs will naturally satisfy
this invariance property and introduce noisy counterfactual matching (NCM), a
simple constraint-based method for leveraging invariant pairs for enhanced
robustness, even with a small set of noisy pairs-in the ideal case, each pair
can eliminate one spurious feature. For linear causal models, we prove that the
test domain error can be upper bounded by the in-domain error and a term that
depends on the counterfactuals' diversity and quality. We validate on a
synthetic dataset and demonstrate on real-world benchmarks that linear probing
on a pretrained backbone improves robustness.

</details>


### [138] [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/abs/2505.24844)
*Wanyun Xie, Francesco Tonin, Volkan Cevher*

**主要类别:** cs.LG

**AI概要:** Chameleon是一种基于嵌入空间中域表示的高效数据混合方法，可以提升大语言模型在预训练和微调过程中的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的域重加权方法通常依赖昂贵的权重计算，并且在引入新数据时需要重新训练，因此需要一种更灵活、更高效的解决方案。

**方法:** 使用杠杆得分（leverage scores）来量化域在学习嵌入空间中的重要性，并构建域亲和矩阵以确定数据混合比例。此外，该方法能够通过计算新域的嵌入直接转移到新数据上。

**结果:** 实验表明，Chameleon在多个场景中表现出色：(i) 与现有方法相比，在预训练域上的计算成本更低且性能更好；(ii) 在无需代理重新训练的情况下适应新数据变化，提高了少样本推理准确性；(iii) 在微调过程中实现了高效的域重加权，并在所有微调域上一致地改善了测试困惑度。

**结论:** Chameleon是一个灵活且高效的数据混合框架，它通过利用嵌入空间中的域表示来量化域重要性，从而提高预训练和微调期间的模型泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chameleon%3A+A+Flexible+Data-mixing+Framework+for+Language+Model+Pretraining+and+Finetuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24844&send_immediately=true&force_search=false)

**原文摘要:** Training data mixtures greatly impact the generalization performance of large
language models. Existing domain reweighting methods often rely on costly
weight computations and require retraining when new data is introduced. To this
end, we introduce a flexible and efficient data mixing framework, Chameleon,
that employs leverage scores to quantify domain importance within a learned
embedding space. We first construct a domain affinity matrix over domain
embeddings. The induced leverage scores determine a mixture that upweights
domains sharing common representations in embedding space. This formulation
allows direct transfer to new data by computing the new domain embeddings. In
experiments, we demonstrate improvements over three key scenarios: (i) our
computed weights improve performance on pretraining domains with a fraction of
the compute of existing methods; (ii) Chameleon can adapt to data changes
without proxy retraining, boosting few-shot reasoning accuracies when
transferred to new data; (iii) our method enables efficient domain reweighting
in finetuning, consistently improving test perplexity on all finetuning domains
over uniform mixture. Our code is available at
https://github.com/LIONS-EPFL/Chameleon.

</details>


### [139] [Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking](https://arxiv.org/abs/2505.24857)
*Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, Brian Karrer*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为EB-Sampler的新采样器，有效提升了当前最先进的掩码扩散模型（MDMs）的采样效率，同时保持了性能稳定。


<details>
  <summary>更多</summary>
  
**动机:** 尽管MDMs在语言建模方面表现出色，但其高效采样方法的研究相对匮乏。观察发现，部分掩码序列能够确定多个未知令牌的值，这表明标准采样程序未充分利用掩码模型中的信息。

**方法:** 引入了一种基于熵界的解掩码过程的新型采样器EB-Sampler，并将其作为现有采样器的简单替代品。

**结果:** EB-Sampler在标准编码和数学推理基准测试中将MDMs的采样速度提高了约2-3倍，并验证了该方法在小型推理任务（如迷宫导航和数独）中的有效性。

**结论:** EB-Sampler通过利用单次预测中的额外信息，提高了MDMs的采样效率，且没有性能损失。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerated+Sampling+from+Masked+Diffusion+Models+via+Entropy+Bounded+Unmasking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24857&send_immediately=true&force_search=false)

**原文摘要:** Recent masked diffusion models (MDMs) have shown competitive performance
compared to autoregressive models (ARMs) for language modeling. While most
literature has focused on performance enhancing sampling procedures, efficient
sampling from MDMs has been scarcely explored. We make the observation that
often a given sequence of partially masked tokens determines the values of
multiple unknown tokens deterministically, meaning that a single prediction of
a masked model holds additional information unused by standard sampling
procedures. Based on this observation, we introduce EB-Sampler, a simple
drop-in replacement for existing samplers, utilizing an Entropy Bounded
unmasking procedure that dynamically unmasks multiple tokens in one function
evaluation with predefined approximate error tolerance. We formulate the
EB-Sampler as part of a broad family of adaptive samplers for which we provide
an error analysis that motivates our algorithmic choices. EB-Sampler
accelerates sampling from current state of the art MDMs by roughly 2-3x on
standard coding and math reasoning benchmarks without loss in performance. We
also validate the same procedure works well on smaller reasoning tasks
including maze navigation and Sudoku, tasks ARMs often struggle with.

</details>


### [140] [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
*Joschka Braun, Carsten Eickhoff, Seyed Ali Bahrainian*

**主要类别:** cs.LG

**AI概要:** 该论文探讨了steering vectors在自由生成任务中的控制效果，并指出控制强度与文本质量之间的权衡。


<details>
  <summary>更多</summary>
  
**动机:** Steering vectors目前主要在多项选择环境中被评估，而它们在自由生成任务中的有效性尚未得到充分研究，因此需要进一步探索。

**方法:** 论文通过在NEWTS数据集的摘要上使用steering vectors来评估其对主题焦点、情感、毒性以及可读性的适应性控制效果，并与prompting方法进行比较。

**结果:** 研究发现，steering vectors能够有效控制目标摘要属性，但高强度的steering会降低文本的内在和外在质量；相比之下，prompting控制能力较弱但能保持文本质量；将两者结合可以在适度的steering强度下实现最佳控制效果和最优的质量权衡。

**结论:** 论文得出结论，在自由生成任务中应用steering vectors时，控制强度与文本质量保持之间存在实际的权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Multiple+Choice%3A+Evaluating+Steering+Vectors+for+Adaptive+Free-Form+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24859&send_immediately=true&force_search=false)

**原文摘要:** Steering vectors are a lightweight method for controlling text properties by
adding a learned bias to language model activations at inference time. So far,
steering vectors have predominantly been evaluated in multiple-choice settings,
while their effectiveness in free-form generation tasks remains understudied.
Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of
steering vectors in adaptively controlling topical focus, sentiment, toxicity,
and readability in abstractive summaries of the NEWTS dataset. We find that
steering effectively controls the targeted summary properties, but high
steering strengths consistently degrade both intrinsic and extrinsic text
quality. Compared to steering, prompting offers weaker control, while
preserving text quality. Combining steering and prompting yields the strongest
control over text properties and offers the most favorable efficacy-quality
trade-off at moderate steering strengths. Our results underscore the practical
trade-off between control strength and text quality preservation when applying
steering vectors to free-form generation tasks.

</details>


### [141] [The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models](https://arxiv.org/abs/2505.24874)
*Adam Stein, Aaditya Naik, Neelay Velingker, Mayur Naik, Eric Wong*

**主要类别:** cs.LG

**AI概要:** 这篇论文讨论了神经符号学习与基础模型的结合如何克服传统方法的局限性，实现更可靠的复杂推理任务处理。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决传统神经符号学习方法受限于简单问题，而纯神经网络基础模型存在不可靠性和缺乏可解释性的问题。

**方法:** 本文通过分析传统神经符号学习在计算、数据和程序方面的三个缺陷，以及基础模型的优势，探讨了神经符号提示方法的潜力。

**结果:** 研究表明，结合基础模型和符号程序的神经符号提示方法可以在复杂推理任务中发挥作用，并提出基础模型可能克服传统神经符号学习局限性的观点。

**结论:** 本文得出结论，基础模型能够实现可推广的神经符号解决方案，为实现神经符号学习的原始目标提供了途径，并避免了从头开始训练的缺点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Road+to+Generalizable+Neuro-Symbolic+Learning+Should+be+Paved+with+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24874&send_immediately=true&force_search=false)

**原文摘要:** Neuro-symbolic learning was proposed to address challenges with training
neural networks for complex reasoning tasks with the added benefits of
interpretability, reliability, and efficiency. Neuro-symbolic learning methods
traditionally train neural models in conjunction with symbolic programs, but
they face significant challenges that limit them to simplistic problems. On the
other hand, purely-neural foundation models now reach state-of-the-art
performance through prompting rather than training, but they are often
unreliable and lack interpretability. Supplementing foundation models with
symbolic programs, which we call neuro-symbolic prompting, provides a way to
use these models for complex reasoning tasks. Doing so raises the question:
What role does specialized model training as part of neuro-symbolic learning
have in the age of foundation models? To explore this question, we highlight
three pitfalls of traditional neuro-symbolic learning with respect to the
compute, data, and programs leading to generalization problems. This position
paper argues that foundation models enable generalizable neuro-symbolic
solutions, offering a path towards achieving the original goals of
neuro-symbolic learning without the downsides of training from scratch.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [142] [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/abs/2505.23881)
*Christopher D. Rosin*

**主要类别:** cs.AI

**AI概要:** CPro1结合推理LLMs解决了多个长期未解的组合设计问题，并生成了新解。


<details>
  <summary>更多</summary>
  
**动机:** 许多组合设计问题仍未解决，需要新的方法来构造小开放实例的解。

**方法:** 从文本定义和有效性验证器出发，通过自动超参数调整和执行反馈指导LLMs选择并实现策略。

**结果:** 解决了16个组合设计问题中的7个长期未解实例，并生成了多个新解。

**结论:** CPro1利用推理LLMs成功解决了一些长期未解的组合设计问题，并生成了新解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Using+Reasoning+Models+to+Generate+Search+Heuristics+that+Solve+Open+Instances+of+Combinatorial+Design+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23881&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) with reasoning are trained to iteratively
generate and refine their answers before finalizing them, which can help with
applications to mathematics and code generation. We apply code generation with
reasoning LLMs to a specific task in the mathematical field of combinatorial
design. This field studies diverse types of combinatorial designs, many of
which have lists of open instances for which existence has not yet been
determined. The Constructive Protocol CPro1 uses LLMs to generate search
heuristics that have the potential to construct solutions to small open
instances. Starting with a textual definition and a validity verifier for a
particular type of design, CPro1 guides LLMs to select and implement
strategies, while providing automated hyperparameter tuning and execution
feedback. CPro1 with reasoning LLMs successfully solves long-standing open
instances for 7 of 16 combinatorial design problems selected from the 2006
Handbook of Combinatorial Designs, including new solved instances for 3 of
these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary
Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves
open instances for several problems from recent (2025) literature, generating
new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform
Nested Steiner Quadruple System.

</details>


### [143] [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
*Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, Guohao Li*

**主要类别:** cs.AI

**AI概要:** Workforce是一种支持跨领域迁移的模块化多智能体框架，结合OWL训练方法显著提升了实际任务中的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 基于大语言模型的多智能体系统在跨领域迁移时面临架构重新设计和完全重训练的问题，需要更灵活的方法。

**方法:** Workforce采用分层多智能体架构，包含领域无关的Planner、子任务管理的Coordinator和具有特定功能的Workers，并引入了OWL训练方法。

**结果:** Workforce在GAIA基准测试中达到69.70%的性能，超过了商业系统OpenAI Deep Research 2.34%，并且使用OWL训练的32B模型在挑战性任务上表现接近GPT-4o。

**结论:** Workforce框架通过模块化设计和优化学习方法，实现了跨领域的可扩展泛化能力，为下一代通用AI助手奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OWL%3A+Optimized+Workforce+Learning+for+General+Multi-Agent+Assistance+in+Real-World+Task+Automation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23885，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23885&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM)-based multi-agent systems show promise for
automating real-world tasks but struggle to transfer across domains due to
their domain-specific nature. Current approaches face two critical
shortcomings: they require complete architectural redesign and full retraining
of all components when applied to new domains. We introduce Workforce, a
hierarchical multi-agent framework that decouples strategic planning from
specialized execution through a modular architecture comprising: (i) a
domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask
management, and (iii) specialized Workers with domain-specific tool-calling
capabilities. This decoupling enables cross-domain transferability during both
inference and training phases: During inference, Workforce seamlessly adapts to
new domains by adding or modifying worker agents; For training, we introduce
Optimized Workforce Learning (OWL), which improves generalization across
domains by optimizing a domain-agnostic planner with reinforcement learning
from real-world feedback. To validate our approach, we evaluate Workforce on
the GAIA benchmark, covering various realistic, multi-domain agentic tasks.
Experimental results demonstrate Workforce achieves open-source
state-of-the-art performance (69.70%), outperforming commercial systems like
OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model
achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to
GPT-4o on challenging tasks. To summarize, by enabling scalable generalization
and modular domain transfer, our work establishes a foundation for the next
generation of general-purpose AI assistants.

</details>


### [144] [Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve](https://arxiv.org/abs/2505.23946)
*Yuanzhe Liu, Ryan Deng, Tim Kaler, Xuhao Chen, Charles E. Leiserson, Yao Ma, Jie Chen*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于教训的协作框架，利用多个小型LLM代理通过相互学习来提高整体性能，优于大型LLM和其他多LLM协作方法。


<details>
  <summary>更多</summary>
  
**动机:** 观察到不同LLM在不同任务和优化类别中表现出不同的专长，没有一个模型全面占优，因此需要一种未知互补优势前提下的协作机制。

**方法:** 设计了一个教训征集-存储-选择机制，让LLM代理从彼此的成功与失败中学习并改进自身表现。

**结果:** 实验表明，通过该框架协作的小型LLM团队在编码问题解决上优于单一更大的LLM及其它多LLM协作方式。

**结论:** 基于教训的协作能够有效提升多个小型LLM的整体性能，而无需预先了解它们的优势领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lessons+Learned%3A+A+Multi-Agent+Framework+for+Code+LLMs+to+Learn+and+Improve，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23946&send_immediately=true&force_search=false)

**原文摘要:** Recent studies show that LLMs possess different skills and specialize in
different tasks. In fact, we observe that their varied performance occur in
several levels of granularity. For example, in the code optimization task, code
LLMs excel at different optimization categories and no one dominates others.
This observation prompts the question of how one leverages multiple LLM agents
to solve a coding problem without knowing their complementary strengths a
priori. We argue that a team of agents can learn from each other's successes
and failures so as to improve their own performance. Thus, a lesson is the
knowledge produced by an agent and passed on to other agents in the collective
solution process. We propose a lesson-based collaboration framework, design the
lesson solicitation--banking--selection mechanism, and demonstrate that a team
of small LLMs with lessons learned can outperform a much larger LLM and other
multi-LLM collaboration methods.

</details>


### [145] [InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback](https://arxiv.org/abs/2505.23950)
*Boyuan Chen, Donghai Hong, Jiaming Ji, Jiacheng Zheng, Bowen Dong, Jiayi Zhou, Kaile Wang, Juntao Dai, Xuyao Wang, Wenqi Chen, Qirui Zheng, Wenxin Li, Sirui Han, Yike Guo, Yaodong Yang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个新数据集InterMT，用于提升多模态大模型在多轮交互任务中的表现，并探索了其在实际应用中的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的多模态大模型缺乏复杂的多轮、多模态交互能力，需要向人类智能迈进。

**方法:** 引入了一个名为InterMT的首选数据集，并采用基于专家注释的代理工作流来构建多轮问答实例，同时提出了InterMT-Bench来评估模型能力。

**结果:** 开发了InterMT数据集，包含15.6k提示、52.6k多轮对话实例和32.4k人工标注偏好对，并展示了其在法官调解等应用中的实用性。

**结论:** 论文提出了一种新的方法和数据集InterMT，用于推动多模态大模型在多轮交互中的发展，并期望通过开源促进相关研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InterMT%3A+Multi-Turn+Interleaved+Preference+Alignment+with+Human+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23950&send_immediately=true&force_search=false)

**原文摘要:** As multimodal large models (MLLMs) continue to advance across challenging
tasks, a key question emerges: What essential capabilities are still missing? A
critical aspect of human learning is continuous interaction with the
environment -- not limited to language, but also involving multimodal
understanding and generation. To move closer to human-level intelligence,
models must similarly support multi-turn, multimodal interaction. In
particular, they should comprehend interleaved multimodal contexts and respond
coherently in ongoing exchanges. In this work, we present an initial
exploration through the InterMT -- the first preference dataset for multi-turn
multimodal interaction, grounded in real human feedback. In this exploration,
we particularly emphasize the importance of human oversight, introducing expert
annotations to guide the process, motivated by the fact that current MLLMs lack
such complex interactive capabilities. InterMT captures human preferences at
both global and local levels into nine sub-dimensions, consists of 15.6k
prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled
preference pairs. To compensate for the lack of capability for multi-modal
understanding and generation, we introduce an agentic workflow that leverages
tool-augmented MLLMs to construct multi-turn QA instances. To further this
goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting
judges with multi-turn, multimodal tasks. We demonstrate the utility of
\InterMT through applications such as judge moderation and further reveal the
multi-turn scaling law of judge model. We hope the open-source of our data can
help facilitate further research on aligning current MLLMs to the next step.
Our project website can be found at https://pku-intermt.github.io .

</details>


### [146] [MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge](https://arxiv.org/abs/2505.23982)
*Jerry Junyang Cheung, Shiyao Shen, Yuchen Zhuang, Yinghao Li, Rampi Ramprasad, Chao Zhang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了MSQA，这是一个用于评估材料科学领域LLMs的知识和复杂推理能力的新基准，发现了当前LLMs的显著性能差距。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在材料科学领域取得了最近的进展，但缺乏用于评估其领域特定知识和复杂推理能力的基准测试。

**方法:** 介绍了一个名为MSQA的综合评估基准，包含1,757个研究生级别的材料科学问题，分为详细解释性回答和二元对错评估两种格式，并对10种最先进的LLMs进行了实验。

**结果:** 通过实验发现，基于API的专有LLMs准确率高达84.5%，而开源LLMs的峰值约为60.5%。领域特定的LLMs由于过拟合和分布偏移往往表现不佳。

**结论:** MSQA是第一个联合评估LLMs在高级材料科学中的事实和推理能力的基准测试，突出了当前LLMs的显著差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MSQA%3A+Benchmarking+LLMs+on+Graduate-Level+Materials+Science+Reasoning+and+Knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23982&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances in large language models (LLMs) for materials
science, there is a lack of benchmarks for evaluating their domain-specific
knowledge and complex reasoning abilities. To bridge this gap, we introduce
MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials
science questions in two formats: detailed explanatory responses and binary
True/False assessments. MSQA distinctively challenges LLMs by requiring both
precise factual knowledge and multi-step reasoning across seven materials
science sub-fields, such as structure-property relationships, synthesis
processes, and computational modeling. Through experiments with 10
state-of-the-art LLMs, we identify significant gaps in current LLM performance.
While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source
(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform
significantly due to overfitting and distributional shifts. MSQA represents the
first benchmark to jointly evaluate the factual and reasoning capabilities of
LLMs crucial for LLMs in advanced materials science.

</details>


### [147] [Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding](https://arxiv.org/abs/2505.23990)
*Mingyang Mao, Mariela M. Perez-Cabarcas, Utteja Kallakuri, Nicholas R. Waytowich, Xiaomin Lin, Tinoosh Mohsenin*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为 Multi-RAG 的多模态检索增强生成系统，旨在通过减少认知负担来提升人类在动态环境中的决策能力。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器人和智能代理越来越多地融入人类生活，需要减轻人类的认知负担，使其能更好地应对复杂多变的信息环境。

**方法:** 开发了 Multi-RAG 系统，并在 MMBench-Video 数据集上进行了测试，以评估其多模态视频理解能力。

**结果:** Multi-RAG 在性能上优于现有的开源视频大语言模型（Video-LLMs）和大型视觉语言模型（LVLMs），同时消耗更少资源和输入数据。

**结论:** Multi-RAG 是一个具有潜力的多模态检索增强生成系统，可在动态、信息丰富的环境中为人类提供自适应帮助。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-RAG%3A+A+Multimodal+Retrieval-Augmented+Generation+System+for+Adaptive+Video+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23990&send_immediately=true&force_search=false)

**原文摘要:** To effectively engage in human society, the ability to adapt, filter
information, and make informed decisions in ever-changing situations is
critical. As robots and intelligent agents become more integrated into human
life, there is a growing opportunity-and need-to offload the cognitive burden
on humans to these systems, particularly in dynamic, information-rich
scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal
retrieval-augmented generation system designed to provide adaptive assistance
to humans in information-intensive circumstances. Our system aims to improve
situational understanding and reduce cognitive load by integrating and
reasoning over multi-source information streams, including video, audio, and
text. As an enabling step toward long-term human-robot partnerships, Multi-RAG
explores how multimodal information understanding can serve as a foundation for
adaptive robotic assistance in dynamic, human-centered situations. To evaluate
its capability in a realistic human-assistance proxy task, we benchmarked
Multi-RAG on the MMBench-Video dataset, a challenging multimodal video
understanding benchmark. Our system achieves superior performance compared to
existing open-source video large language models (Video-LLMs) and large
vision-language models (LVLMs), while utilizing fewer resources and less input
data. The results demonstrate Multi- RAG's potential as a practical and
efficient foundation for future human-robot adaptive assistance systems in
dynamic, real-world contexts.

</details>


### [148] [GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs](https://arxiv.org/abs/2505.24036)
*Amel Gader, Alsayed Algergawy*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一种新的基于大语言模型的知识图谱实例补全过程GenIC，通过两个步骤进行属性预测和链接预测，并在多个数据集上展示了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现代知识库通常包含实体描述和类型，这些信息可以为推断缺失事实提供有价值的上下文，因此利用这些文本描述以及大语言模型的能力来提取事实并识别知识图谱模式中的模式，以解决知识图谱补全问题。

**方法:** 引入了一个名为GenIC的两步生成实例补全过程，第一步是属性预测，作为多标签分类任务；第二步是链接预测，作为生成序列到序列的任务。

**结果:** 在三个数据集上的实验结果显示了所提方法优于现有基线方法。

**结论:** 论文提出了一种基于大语言模型的端到端实例补全过程，即GenIC，并且实验结果表明该方法优于现有的基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GenIC%3A+An+LLM-Based+Framework+for+Instance+Completion+in+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24036&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graph completion aims to address the gaps of knowledge bases by
adding new triples that represent facts. The complexity of this task depends on
how many parts of a triple are already known. Instance completion involves
predicting the relation-tail pair when only the head is given (h, ?, ?).
Notably, modern knowledge bases often contain entity descriptions and types,
which can provide valuable context for inferring missing facts. By leveraging
these textual descriptions and the ability of large language models to extract
facts from them and recognize patterns within the knowledge graph schema, we
propose an LLM-powered, end-to-end instance completion approach. Specifically,
we introduce GenIC: a two-step Generative Instance Completion framework. The
first step focuses on property prediction, treated as a multi-label
classification task. The second step is link prediction, framed as a generative
sequence-to-sequence task. Experimental results on three datasets show that our
method outperforms existing baselines. Our code is available at
https://github.com/amal-gader/genic.

</details>


### [149] [Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution](https://arxiv.org/abs/2505.24037)
*Qiao Xiao, Alan Ansell, Boqian Wu, Lu Yin, Mykola Pechenizkiy, Shiwei Liu, Decebal Constantin Mocanu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的稀疏大语言模型（LLM）微调方法SEFT，该方法能够在微调过程中动态演化稀疏拓扑，同时保持整体稀疏性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的后训练剪枝方法如SparseGPT和Wanda在高稀疏水平下难以保持模型性能，而现有的微调方法如全微调和LoRA要求更新整个密集度量，不适合稀疏LLM。

**方法:** SEFT通过权重下降和增长策略进行任务特定的适应，并采用敏感性驱动的剪枝准则以保持所需的稀疏水平。

**结果:** SEFT在各种LLM上的实验结果显示了其强大的性能以及优越的内存和时间效率。

**结论:** SEFT是一种专为稀疏LLM设计的新方法，能够在微调过程中动态演化稀疏拓扑，同时保持整体稀疏性。SEFT在多个LLM上的实验表明，与现有基线相比，SEFT具有更强的性能以及优越的内存和时间效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leave+it+to+the+Specialist%3A+Repair+Sparse+LLMs+with+Sparse+Fine-Tuning+via+Sparsity+Evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24037&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have achieved remarkable success across various
tasks but face deployment challenges due to their massive computational
demands. While post-training pruning methods like SparseGPT and Wanda can
effectively reduce the model size, but struggle to maintain model performance
at high sparsity levels, limiting their utility for downstream tasks. Existing
fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve
sparsity as they require updating the whole dense metrics, not well-suited for
sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a
novel method designed specifically for sparse LLMs. SEFT dynamically evolves
the sparse topology of pruned models during fine-tuning, while preserving the
overall sparsity throughout the process. The strengths of SEFT lie in its
ability to perform task-specific adaptation through a weight drop-and-grow
strategy, enabling the pruned model to self-adapt its sparse connectivity
pattern based on the target dataset. Furthermore, a sensitivity-driven pruning
criterion is employed to ensure that the desired sparsity level is consistently
maintained throughout fine-tuning. Our experiments on various LLMs, including
LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks
demonstrate that SEFT achieves stronger performance while offering superior
memory and time efficiency compared to existing baselines. Our code is publicly
available at: https://github.com/QiaoXiao7282/SEFT.

</details>


### [150] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Zhengzhong Tu*

**主要类别:** cs.AI

**AI概要:** 这篇论文首次系统性地分析了适用于大型视觉-语言模型（LVLMs）的检索增强生成（RAG）方法，并提出了一种统一的代理框架以提升模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型视觉-语言模型（LVLMs）在多种多态任务中表现出色，但它们仍受限于静态训练数据、容易产生幻觉以及无法验证最新的外部证据。这些限制影响了其在动态现实世界应用中的表现。因此需要通过检索增强生成（RAG）来解决这些问题。

**方法:** 该研究系统地分析了多模态RAG流水线的三个主要阶段：检索阶段、重排序阶段和生成阶段，并进一步探讨了一个统一的代理框架，通过自我反思将重排序和生成集成在一起。

**结果:** 通过对RAG流水线的全栈探索，研究实现了显著的性能提升，平均提高了5%且不需要进行任何微调。

**结论:** 研究的结果表明，全面探索RAG对于LVLMs具有显著的性能提升效果，平均提升了5%而无需任何微调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是mRAG%3A+Elucidating+the+Design+Space+of+Multi-modal+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24073，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24073&send_immediately=true&force_search=false)

**原文摘要:** Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [151] [SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought](https://arxiv.org/abs/2505.24181)
*Guanghao Li, Wenhao Jiang, Mingfeng Chen, Yan Li, Hao Yu, Shuting Dong, Tao Ren, Ming Tang, Chun Yuan*

**主要类别:** cs.AI

**AI概要:** 该论文提出Flow CoT推理范式及其实现框架SCOUT，在无需人工监督或预训练的情况下有效提升大语言模型的推理能力和解释性。


<details>
  <summary>更多</summary>
  
**动机:** 传统CoT方法依赖中间推理步骤，限制了可扩展性和泛化能力；现有递归推理方法需要昂贵的预训练且缺乏理论框架指导推理迭代演化。

**方法:** 提出了SCOUT（Stepwise Cognitive Optimization Using Teachers）框架，使用渐进蒸馏和基于交叉注意力的回顾模块实现Flow CoT推理。

**结果:** 在8个推理基准实验中，SCOUT在微调条件下最高提升了1.8%的准确率，同时提升了推理深度和解释粒度。

**结论:** 论文提出了一种新的推理范式Flow CoT，并通过SCOUT框架验证了其在提升LLMs推理能力和解释质量上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCOUT%3A+Teaching+Pre-trained+Language+Models+to+Enhance+Reasoning+via+Flow+Chain-of-Thought，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24181&send_immediately=true&force_search=false)

**原文摘要:** Chain of Thought (CoT) prompting improves the reasoning performance of large
language models (LLMs) by encouraging step by step thinking. However, CoT-based
methods depend on intermediate reasoning steps, which limits scalability and
generalization. Recent work explores recursive reasoning, where LLMs reuse
internal layers across iterations to refine latent representations without
explicit CoT supervision. While promising, these approaches often require
costly pretraining and lack a principled framework for how reasoning should
evolve across iterations. We address this gap by introducing Flow Chain of
Thought (Flow CoT), a reasoning paradigm that models recursive inference as a
progressive trajectory of latent cognitive states. Flow CoT frames each
iteration as a distinct cognitive stage deepening reasoning across iterations
without relying on manual supervision. To realize this, we propose SCOUT
(Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning
framework that enables Flow CoT style reasoning without the need for
pretraining. SCOUT uses progressive distillation to align each iteration with a
teacher of appropriate capacity, and a cross attention based retrospective
module that integrates outputs from previous iterations while preserving the
models original computation flow. Experiments across eight reasoning benchmarks
show that SCOUT consistently improves both accuracy and explanation quality,
achieving up to 1.8% gains under fine tuning. Qualitative analyses further
reveal that SCOUT enables progressively deeper reasoning across iterations
refining both belief formation and explanation granularity. These results not
only validate the effectiveness of SCOUT, but also demonstrate the practical
viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.

</details>


### [152] [Learning API Functionality from Demonstrations for Tool-based Agents](https://arxiv.org/abs/2505.24197)
*Bhrij Patel, Ashish Jagmohan, Aditya Vempaty*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种在没有文档的情况下从演示中学习API功能的新方法，通过使用现有API基准测试收集演示，研究演示信息对任务完成的影响。


<details>
  <summary>更多</summary>
  
**动机:** 数字工具代理通常依赖文档来理解API功能，但这些文档经常缺失、过时、私有化或不一致，阻碍了可靠通用代理的发展。因此需要一种无需文档的新范式。

**方法:** 通过现有的API基准测试，从专家API代理和自我探索中收集演示。研究演示数量以及LLM生成的摘要和评估如何影响基于API代理的任务成功率。

**结果:** 实验显示，在3个数据集和5个模型上，即使是最先进的LLM，从演示中学习API功能仍然具有挑战性。提供明确的函数调用和自然语言批评显著提高了任务成功率。

**结论:** 学习API功能仍然是一个非平凡的挑战，即使对于最先进的LLM也是如此。提供明确的函数调用和自然语言批评可以显著提高任务成功率。论文还分析了失败模式，确定了错误来源，并强调了未来工作的主要开放性挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+API+Functionality+from+Demonstrations+for+Tool-based+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24197&send_immediately=true&force_search=false)

**原文摘要:** Digital tool-based agents that invoke external Application Programming
Interfaces (APIs) often rely on documentation to understand API functionality.
However, such documentation is frequently missing, outdated, privatized, or
inconsistent-hindering the development of reliable, general-purpose agents. In
this work, we propose learning API functionality directly from demonstrations
as a new paradigm applicable in scenarios without documentation. Using existing
API benchmarks, we collect demonstrations from both expert API-based agents and
from self-exploration. To understand what information demonstrations must
convey for successful task completion, we extensively study how the number of
demonstrations and the use of LLM-generated summaries and evaluations affect
the task success rate of the API-based agent. Our experiments across 3 datasets
and 5 models show that learning functionality from demonstrations remains a
non-trivial challenge, even for state-of-the-art LLMs. We find that providing
explicit function calls and natural language critiques significantly improves
the agent's task success rate due to more accurate parameter filling. We
analyze failure modes, identify sources of error, and highlight key open
challenges for future work in documentation-free, self-improving, API-based
agents.

</details>


### [153] [SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems](https://arxiv.org/abs/2505.24201)
*Xu He, Di Wu, Yan Zhai, Kun Sun*

**主要类别:** cs.AI

**AI概要:** 这篇论文讨论了一个针对基于大语言模型的多智能体系统的系统级异常检测框架，旨在解决安全性与可靠性问题。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLM）为基础的多智能体系统（MAS）引入了新的安全性和可靠性挑战。

**方法:** 提出了一种基于图的框架和一个可插拔的SentinelAgent来观察、分析和干预MAS执行。

**结果:** 通过两个案例研究验证了该框架的能力，包括电子邮件助手和微软的Magentic-One系统，展示了其检测隐蔽风险和提供可解释根本原因归因的能力。

**结论:** 论文提出了一个系统级的异常检测框架，为MAS提供了更可信、可监控和安全的AI生态系统基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SentinelAgent%3A+Graph-based+Anomaly+Detection+in+Multi-Agent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24201&send_immediately=true&force_search=false)

**原文摘要:** The rise of large language model (LLM)-based multi-agent systems (MAS)
introduces new security and reliability challenges. While these systems show
great promise in decomposing and coordinating complex tasks, they also face
multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent
agent miscoordination. Existing guardrail mechanisms offer only partial
protection, primarily at the input-output level, and fall short in addressing
systemic or multi-point failures in MAS. In this work, we present a
system-level anomaly detection framework tailored for MAS, integrating
structural modeling with runtime behavioral oversight. Our approach consists of
two components. First, we propose a graph-based framework that models agent
interactions as dynamic execution graphs, enabling semantic anomaly detection
at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,
an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS
execution based on security policies and contextual reasoning. By bridging
abstract detection logic with actionable enforcement, our method detects not
only single-point faults and prompt injections but also multi-agent collusion
and latent exploit paths. We validate our framework through two case studies,
including an email assistant and Microsoft's Magentic-One system, demonstrating
its ability to detect covert risks and provide explainable root-cause
attribution. Our work lays the foundation for more trustworthy, monitorable,
and secure agent-based AI ecosystems.

</details>


### [154] [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/abs/2505.24208)
*Wenhan Yang, Spencer Stice, Ali Payani, Baharan Mirzasoleiman*

**主要类别:** cs.AI

**AI概要:** 本文发现视觉-语言模型中模态差距越大，安全性越差，并提出一种新方法在预训练阶段减少这种差距，有效提高模型安全性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉-语言模型（LVLMs）相比其文本模型主干存在显著的安全性下降问题，即使面对空白或无关图像也可能生成有害响应。

**方法:** 作者通过实验证明了模态差距与模型安全性之间的负相关关系，并在预训练阶段引入正则化来缩小这种差距。

**结果:** 实验表明，该方法将不安全率降低了最多16.3%，并可进一步提升现有防御机制效果达18.2%。

**结论:** 本文提出了一种新的预训练正则化方法，以减少视觉-语言模型中的模态差距，从而显著提高其安全性对齐，并且不损害性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bootstrapping+LLM+Robustness+for+VLM+Safety+via+Reducing+the+Pretraining+Modality+Gap，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24208&send_immediately=true&force_search=false)

**原文摘要:** Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for
their reliable deployment. However, LVLMs suffer from drastic safety
degradation compared to their LLM backbone. Even blank or irrelevant images can
trigger LVLMs to generate harmful responses to prompts that would otherwise be
refused in text-only contexts. The modality gap between image and text
representations has been recently hypothesized to contribute to safety
degradation of LVLMs. However, if and how the amount of modality gap affects
LVLMs' safety is not studied. In this work, we show that the amount of modality
gap is highly inversely correlated with VLMs' safety. Then, we show that this
modality gap is introduced during pretraining LVLMs and persists through
fine-tuning. Inspired by this observation, we propose a regularization to
reduce the modality gap during pretraining. Our extensive experiments on LLaVA
v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves
safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without
compromising performance, and can further boost existing defenses by up to
18.2%.

</details>


### [155] [E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness](https://arxiv.org/abs/2505.24226)
*Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, Xiang Li*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种新的基于图的RAG框架E^2GraphRAG，其通过优化索引和检索策略，在保持良好问答性能的同时显著提高了处理效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于图的RAG方法（如GraphRAG）效率低下且依赖手动预定义查询模式，限制了实际应用。

**方法:** 构建基于摘要树和实体图的索引结构，并设计了双向索引与自适应检索策略。

**结果:** 实验表明，E^2GraphRAG的索引速度比GraphRAG快10倍，检索速度比LightRAG快100倍。

**结论:** E^2GraphRAG在保持竞争力QA性能的同时，通过改进索引和检索阶段的方法，实现了比现有方法更快的处理速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是E%5E2GraphRAG%3A+Streamlining+Graph-based+RAG+for+High+Efficiency+and+Effectiveness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24226，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24226&send_immediately=true&force_search=false)

**原文摘要:** Graph-based RAG methods like GraphRAG have shown promising global
understanding of the knowledge base by constructing hierarchical entity graphs.
However, they often suffer from inefficiency and rely on manually pre-defined
query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a
streamlined graph-based RAG framework that improves both Efficiency and
Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree
with large language models and an entity graph with SpaCy based on document
chunks. We then construct bidirectional indexes between entities and chunks to
capture their many-to-many relationships, enabling fast lookup during both
local and global retrieval. For the retrieval stage, we design an adaptive
retrieval strategy that leverages the graph structure to retrieve and select
between local and global modes. Experiments show that E^2GraphRAG achieves up
to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG
in retrieval while maintaining competitive QA performance.

</details>


### [156] [ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction](https://arxiv.org/abs/2505.24230)
*Murari Ambati*

**主要类别:** cs.AI

**AI概要:** ProofNet++ 是一种结合神经网络与符号方法的自动化定理证明框架，通过引入符号监督、强化学习和自我修正机制，提高了证明的准确性与可验证性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于 LLM 的系统存在逻辑步骤幻觉和不可验证的推理问题，需要提高自动化定理证明的准确性和可验证性。

**方法:** 结合大型语言模型与形式化证明验证及自我修正机制，引入符号证明树监督、基于验证器的强化学习循环以及迭代自我修正模块。

**结果:** 在 miniF2F、Lean 的 mathlib 和 HOL Light 上的实验表明，ProofNet++ 显著优于之前的模型，在证明准确性和形式验证方面表现突出。

**结论:** ProofNet++ 在自动化定理证明方面显著提升了准确性、正确性和形式验证能力，为未来的研究提供了理论分析和资源支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProofNet%2B%2B%3A+A+Neuro-Symbolic+System+for+Formal+Proof+Verification+with+Self-Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24230&send_immediately=true&force_search=false)

**原文摘要:** We propose ProofNet++, a neuro-symbolic framework that enhances automated
theorem proving by combining large language models (LLMs) with formal proof
verification and self-correction mechanisms. Current LLM-based systems suffer
from hallucinated logical steps and unverifiable reasoning. ProofNet++
mitigates these limitations by integrating symbolic proof tree supervision, a
reinforcement learning loop using verifiers as reward functions, and an
iterative self-correction module. Our experiments on miniF2F, Lean's mathlib,
and HOL Light show that ProofNet++ significantly improves proof accuracy,
correctness, and formal verifiability over prior models. We provide theoretical
analysis of the convergence and stability of the verifier-guided RL framework
and release our datasets and codebase for future research.

</details>


### [157] [FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation](https://arxiv.org/abs/2505.24258)
*Vishal Pallagani, Nitin Gupta, John Aydin, Biplav Srivastava*

**主要类别:** cs.AI

**AI概要:** 本文介绍了FABLE，一个用于评估大型语言模型数据流推理能力的新基准测试，结果显示当前模型在这一领域仍有显著不足。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在自然和编程语言中表现出色，但其在程序任务中的数据流推理能力尚未得到系统评估。

**方法:** 通过改编软件工程中的八种经典数据流分析方法，构建了一个名为FABLE的扩展性基准测试。

**结果:** 推理型模型（DeepSeek-R1 8B）表现优于其他模型，但推断速度慢20多倍；通用和代码专用模型接近随机猜测水平。

**结论:** FABLE为评估大型语言模型在数据流推理方面的能力提供了系统基准，并揭示了当前模型的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FABLE%3A+A+Novel+Data-Flow+Analysis+Benchmark+on+Procedural+Text+for+Large+Language+Model+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24258，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24258&send_immediately=true&force_search=false)

**原文摘要:** Understanding how data moves, transforms, and persists, known as data flow,
is fundamental to reasoning in procedural tasks. Despite their fluency in
natural and programming languages, large language models (LLMs), although
increasingly being applied to decisions with procedural tasks, have not been
systematically evaluated for their ability to perform data-flow reasoning. We
introduce FABLE, an extensible benchmark designed to assess LLMs' understanding
of data flow using structured, procedural text. FABLE adapts eight classical
data-flow analyses from software engineering: reaching definitions, very busy
expressions, available expressions, live variable analysis, interval analysis,
type-state analysis, taint analysis, and concurrency analysis. These analyses
are instantiated across three real-world domains: cooking recipes, travel
routes, and automated plans. The benchmark includes 2,400 question-answer
pairs, with 100 examples for each domain-analysis combination. We evaluate
three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a
general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code
8B). Each model is tested using majority voting over five sampled completions
per prompt. Results show that the reasoning model achieves higher accuracy, but
at the cost of over 20 times slower inference compared to the other models. In
contrast, the general-purpose and code-specific models perform close to random
chance. FABLE provides the first diagnostic benchmark to systematically
evaluate data-flow reasoning and offers insights for developing models with
stronger procedural understanding.

</details>


### [158] [Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models](https://arxiv.org/abs/2505.24260)
*Mingyi He, Yuebing Liang, Shenhao Wang, Yunhan Zheng, Qingyi Wang, Dingyi Zhuang, Li Tian, Jinhua Zhao*

**主要类别:** cs.AI

**AI概要:** 这篇论文讨论了一种新的生成式人工智能在城市设计中的应用，这种方法结合了多模态扩散模型与人类专家的知识，使得设计过程更加灵活可控，并加强了人机协作。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现有方法并未很好地与人类设计工作流程集成。它们通常遵循端到端的流水线，控制有限，忽略了现实世界设计的迭代性质。

**方法:** 该研究提出了一种分阶段的生成式城市设计框架，将多模态扩散模型与人类专业知识相结合，以实现更适应性和可控的设计过程。

**结果:** 使用芝加哥和纽约市数据的实验表明，我们的框架在所有三个维度上均优于基线模型和端到端方法。

**结论:** 这项研究强调了多模态扩散模型和分阶段生成在保持人类控制和促进迭代改进方面的优势，为城市设计解决方案中的人机交互奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+AI+for+Urban+Design%3A+A+Stepwise+Approach+Integrating+Human+Expertise+with+Multimodal+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24260&send_immediately=true&force_search=false)

**原文摘要:** Urban design is a multifaceted process that demands careful consideration of
site-specific constraints and collaboration among diverse professionals and
stakeholders. The advent of generative artificial intelligence (GenAI) offers
transformative potential by improving the efficiency of design generation and
facilitating the communication of design ideas. However, most existing
approaches are not well integrated with human design workflows. They often
follow end-to-end pipelines with limited control, overlooking the iterative
nature of real-world design. This study proposes a stepwise generative urban
design framework that integrates multimodal diffusion models with human
expertise to enable more adaptive and controllable design processes. Instead of
generating design outcomes in a single end-to-end process, the framework
divides the process into three key stages aligned with established urban design
workflows: (1) road network and land use planning, (2) building layout
planning, and (3) detailed planning and rendering. At each stage, multimodal
diffusion models generate preliminary designs based on textual prompts and
image-based constraints, which can then be reviewed and refined by human
designers. We design an evaluation framework to assess the fidelity,
compliance, and diversity of the generated designs. Experiments using data from
Chicago and New York City demonstrate that our framework outperforms baseline
models and end-to-end approaches across all three dimensions. This study
underscores the benefits of multimodal diffusion models and stepwise generation
in preserving human control and facilitating iterative refinements, laying the
groundwork for human-AI interaction in urban design solutions.

</details>


### [159] [How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning](https://arxiv.org/abs/2505.24273)
*Hongyi James Cai, Junlin Wang, Xiaoyin Chen, Bhuwan Dhingra*

**主要类别:** cs.AI

**AI概要:** 本文研究了监督微调和强化学习在推理任务中的动态关系，揭示了带有回溯的长链式推理对训练效果的积极影响，尤其在复杂问题上更为显著。


<details>
  <summary>更多</summary>
  
**动机:** 论文的动机源于当前对于回溯在推理改进中的具体作用及其最优使用程度缺乏深入理解。尽管已有研究表明RL能有效内化搜索策略并实现长链式推理，但如何优化这种训练方式仍是一个未解的问题。

**方法:** 论文的方法包括对八个推理任务进行系统研究，并构建合成数据集以控制回溯步骤的数量。通过比较监督微调（SFT）和强化学习（RL）在不同任务上的表现，分析回溯的作用以及内容正确性和结构对训练的影响。

**结果:** 论文的结果表明：(1) 在强化学习训练中，带有回溯的较长链式推理通常能产生更好且更稳定的训练效果；(2) 对于较难、搜索空间更大的问题，在监督微调阶段需要更多回溯步骤。此外，实验显示强化学习训练受结构模式影响更大，而对长链式推理序列的内容正确性相对不敏感。

**结论:** 论文的结论是长链式推理（CoT）与回溯结合能够诱导出更好且更稳定的强化学习（RL）训练效果，尤其是在处理搜索空间较大的复杂问题时。此外，研究发现RL训练主要关注结构模式而非内容正确性，这为设计有效的LLM推理扩展训练策略提供了实用见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Much+Backtracking+is+Enough%3F+Exploring+the+Interplay+of+SFT+and+RL+in+Enhancing+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24273，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24273&send_immediately=true&force_search=false)

**原文摘要:** Recent breakthroughs in large language models (LLMs) have effectively
improved their reasoning abilities, particularly on mathematical and logical
problems that have verifiable answers, through techniques such as supervised
finetuning (SFT) and reinforcement learning (RL). Prior research indicates that
RL effectively internalizes search strategies, enabling long chain-of-thought
(CoT) reasoning, with backtracking emerging naturally as a learned capability.
However, the precise benefits of backtracking, specifically, how significantly
it contributes to reasoning improvements and the optimal extent of its use,
remain poorly understood. In this work, we systematically investigate the
dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc
1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self
Reference. Our findings highlight that short CoT sequences used in SFT as a
warm-up do have moderate contribution to RL training, compared with cold-start
RL; however such contribution diminishes when tasks become increasingly
difficult. Motivated by this observation, we construct synthetic datasets
varying systematically in the number of backtracking steps and conduct
controlled experiments to isolate the influence of either the correctness
(content) or the structure (i.e., backtrack frequency). We find that (1) longer
CoT with backtracks generally induce better and more stable RL training, (2)
more challenging problems with larger search space tend to need higher numbers
of backtracks during the SFT stage. Additionally, we demonstrate through
experiments on distilled data that RL training is largely unaffected by the
correctness of long CoT sequences, suggesting that RL prioritizes structural
patterns over content correctness. Collectively, our results offer practical
insights into designing optimal training strategies to effectively scale
reasoning in LLMs.

</details>


### [160] [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/abs/2505.24292)
*Yueqi Zhang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li*

**主要类别:** cs.AI

**AI概要:** 论文介绍了QuAda，一种针对引用感知对话的轻量级训练方法，能够有效解决大语言模型在处理引用文本时的不足。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大语言模型缺乏明确的机制来定位和利用对话中引用的文本，这限制了模型在处理涉及引用的对话时的效果。

**方法:** 提出了基于注意力机制的QuAda方法，通过附加两个瓶颈投影到每个注意力头上，动态放大或抑制对引用跨度的关注。

**结果:** QuAda满足了零开销和参数效率的要求，能够在不改变提示的情况下进行推理，并更新不到2.8%的主干权重。

**结论:** 论文提出了一种新的方法QuAda，适用于各种对话场景，并能推广到未见过的主题上，为引用感知的对话提供了一个有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mind+the+Quote%3A+Enabling+Quotation-Aware+Dialogue+in+LLMs+via+Plug-and-Play+Modules，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24292，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24292&send_immediately=true&force_search=false)

**原文摘要:** Human-AI conversation frequently relies on quoting earlier text-"check it
with the formula I just highlighted"-yet today's large language models (LLMs)
lack an explicit mechanism for locating and exploiting such spans. We formalise
the challenge as span-conditioned generation, decomposing each turn into the
dialogue history, a set of token-offset quotation spans, and an intent
utterance. Building on this abstraction, we introduce a quotation-centric data
pipeline that automatically synthesises task-specific dialogues, verifies
answer correctness through multi-stage consistency checks, and yields both a
heterogeneous training corpus and the first benchmark covering five
representative scenarios. To meet the benchmark's zero-overhead and
parameter-efficiency requirements, we propose QuAda, a lightweight
training-based method that attaches two bottleneck projections to every
attention head, dynamically amplifying or suppressing attention to quoted spans
at inference time while leaving the prompt unchanged and updating < 2.8% of
backbone weights. Experiments across models show that QuAda is suitable for all
scenarios and generalises to unseen topics, offering an effective,
plug-and-play solution for quotation-aware dialogue.

</details>


### [161] [GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments](https://arxiv.org/abs/2505.24306)
*Kechen Li, Yaotian Tao, Ximing Wen, Quanwei Sun, Zifei Gong, Chang Xu, Xizhe Zhang, Tianbo Ji*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一个新的方法Algorithm of Thought (AoT)，将传统算法与大语言模型（LLMs）相结合，以提升其在路径规划任务中的表现。研究结果显示，AoT 显著提高了LLMs在各种地图环境中的正确性、最优性和效率，尤其是在复杂环境中。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，大语言模型（LLMs）在规划和推理任务中展现了巨大潜力，但现有研究大多关注LLMs的独立推理能力，忽视了其与传统算法结合的潜力。因此，本文旨在填补这一空白，探索LLMs与传统算法协同工作的可能性。

**方法:** 本研究提出了一种新的混合提示技术Algorithm of Thought (AoT)，将传统算法的指导引入到大语言模型（LLMs）的提示中，并设计了一个全面的评估基准GridRoute，用于测试LLMs在不同规模地图环境中利用传统算法的能力。实验部分评估了从7B到72B参数范围内的六种LLMs，在正确性、最优性和效率方面的表现。

**结果:** 结果表明，提出的Algorithm of Thought (AoT) 方法在所有模型规模上均显著提升了性能，尤其在较大或更复杂的环境中效果更为明显，证明了AoT在网格环境中的有效性。

**结论:** 论文得出结论，通过引入传统算法指导提示的新型混合提示技术Algorithm of Thought (AoT)，在各种模型大小和地图复杂度上显著提升了性能，表明这是一种解决路径规划问题的有前景的方法。此外，该研究的代码已开源，以促进进一步的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GridRoute%3A+A+Benchmark+for+LLM-Based+Route+Planning+with+Cardinal+Movement+in+Grid+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24306&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models (LLMs) have demonstrated their
potential in planning and reasoning tasks, offering a flexible alternative to
classical pathfinding algorithms. However, most existing studies focus on LLMs'
independent reasoning capabilities and overlook the potential synergy between
LLMs and traditional algorithms. To fill this gap, we propose a comprehensive
evaluation benchmark GridRoute to assess how LLMs can take advantage of
traditional algorithms. We also propose a novel hybrid prompting technique
called Algorithm of Thought (AoT), which introduces traditional algorithms'
guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to
72B parameters across various map sizes, assessing their performance in
correctness, optimality, and efficiency in grid environments with varying
sizes. Our results show that AoT significantly boosts performance across all
model sizes, particularly in larger or more complex environments, suggesting a
promising approach to addressing path planning challenges. Our code is
open-sourced at https://github.com/LinChance/GridRoute.

</details>


### [162] [Three Kinds of Negation in Knowledge and Their Mathematical Foundations](https://arxiv.org/abs/2505.24422)
*Zhenghua Pan, Yong Wang*

**主要类别:** cs.AI

**AI概要:** 论文探讨了否定在知识处理中的三种类型（矛盾否定、对立否定和中介否定），并构建了反映其特性的数学框架SCOI与LCOI。


<details>
  <summary>更多</summary>
  
**动机:** 否定在知识处理和研究中是一个基本问题，理解其不同形式有助于更好地进行知识表达与计算。

**方法:** 通过哲学、逻辑学、语言学等多个领域对否定的理解和特征进行分析，基于矛盾与对立概念的区别，提出三种否定类型。

**结果:** 作者引入了SCOI和LCOI，并证明了它们的主要运算性质及形式推理关系。

**结论:** 本文提出了三种不同的否定类型，并介绍了SCOI和LCOI作为反映这些否定形式内在联系和性质的数学基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Three+Kinds+of+Negation+in+Knowledge+and+Their+Mathematical+Foundations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24422，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24422&send_immediately=true&force_search=false)

**原文摘要:** In the field of artificial intelligence, understanding, distinguishing,
expressing, and computing the negation in knowledge is a fundamental issue in
knowledge processing and research. In this paper, we examine and analyze the
understanding and characteristics of negation in various fields such as
philosophy, logic, and linguistics etc. Based on the distinction between the
concepts of contradiction and opposition, we propose that there are three
different types of negation in knowledge from a conceptual perspective:
contradictory negation, opposite negation, and intermediary negation. To
establish a mathematical foundation that fully reflects the intrinsic
connections, properties, and laws of these different forms of negation, we
introduce SCOI: sets with contradictory negation, opposite negation and
intermediary negation, and LCOI: logic with contradictory negation, opposite
negation and intermediary negation, and we proved the main operational
properties of SCOI as well as the formal inference relations in LCOI.

</details>


### [163] [P: A Universal Measure of Predictive Intelligence](https://arxiv.org/abs/2505.24426)
*David Gamez*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的智能测量方法，通过预测能力和Kolmogorov复杂性来评估代理（包括AI）的智能水平，并展示了其在虚拟迷宫和时间序列预测中的应用效果。


<details>
  <summary>更多</summary>
  
**动机:** 目前对智能的理解和测量落后于构建模仿人类行为系统的能力建议，且缺乏公认的AI智能定义和实用的衡量标准。

**方法:** 通过在正常环境中与代理互动，总结其预测准确性，并利用Kolmogorov复杂性考虑预测和感知环境的复杂性；并进行了两个实验验证该算法的实际可行性。

**结果:** 该算法能够测量虚拟迷宫中代理的智能水平以及预测时间序列数据的代理的智能水平。

**结论:** 论文提出了一种基于预测能力的通用智能度量方法，可作为对人类、动物和人工智能进行统一比较的起点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是P%3A+A+Universal+Measure+of+Predictive+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24426&send_immediately=true&force_search=false)

**原文摘要:** Over the last thirty years, considerable progress has been made with the
development of systems that can drive cars, play games, predict protein folding
and generate natural language. These systems are described as intelligent and
there has been a great deal of talk about the rapid increase in artificial
intelligence and its potential dangers. However, our theoretical understanding
of intelligence and ability to measure it lag far behind our capacity for
building systems that mimic intelligent human behaviour. There is no commonly
agreed definition of the intelligence that AI systems are said to possess.
No-one has developed a practical measure that would enable us to compare the
intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on
the hypothesis that prediction is the most important component of intelligence.
As an agent interacts with its normal environment, the accuracy of its
predictions is summed up and the complexity of its predictions and perceived
environment is accounted for using Kolmogorov complexity. Two experiments were
carried out to evaluate the practical feasibility of the algorithm. These
demonstrated that it could measure the intelligence of an agent embodied in a
virtual maze and an agent that makes predictions about time-series data. This
universal measure could be the starting point for a new comparative science of
intelligence that ranks humans, animals and AIs on a single ratio scale.

</details>


### [164] [AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models](https://arxiv.org/abs/2505.24784)
*Conor Heins, Toon Van de Maele, Alexander Tschantz, Hampus Linander, Dimitrije Markovic, Tommaso Salvatori, Corrado Pezzato, Ozan Catal, Ran Wei, Magnus Koudahl, Marco Perin, Karl Friston, Tim Verbelen, Christopher Buckley*

**主要类别:** cs.AI

**AI概要:** AXIOM是一种结合贝叶斯方法与深度强化学习优势的新架构，在低数据环境下显著提升学习效率。


<details>
  <summary>更多</summary>
  
**动机:** 当前的深度强化学习在多领域表现优异，但在数据效率上不如人类学习；而主动推理模型通常为单一任务设计，缺乏DRL的领域灵活性。因此需要一种兼顾两者优势的新方法。

**方法:** AXIOM使用一个最小但具有表达性的关于对象动力学的核心先验知识集合，将生成模型结构在线扩展，并通过贝叶斯模型简化诱导泛化。

**结果:** AXIOM在仅10,000次交互步骤内掌握多种游戏，参数数量少于DRL，且无需基于梯度优化的计算开销。

**结论:** AXIOM成功结合了贝叶斯方法的数据效率和可解释性以及深度强化学习的跨任务泛化能力，通过对象中心的动力学模型加速低数据环境下的学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AXIOM%3A+Learning+to+Play+Games+in+Minutes+with+Expanding+Object-Centric+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24784&send_immediately=true&force_search=false)

**原文摘要:** Current deep reinforcement learning (DRL) approaches achieve state-of-the-art
performance in various domains, but struggle with data efficiency compared to
human learning, which leverages core priors about objects and their
interactions. Active inference offers a principled framework for integrating
sensory information with prior knowledge to learn a world model and quantify
the uncertainty of its own beliefs and predictions. However, active inference
models are usually crafted for a single task with bespoke knowledge, so they
lack the domain flexibility typical of DRL approaches. To bridge this gap, we
propose a novel architecture that integrates a minimal yet expressive set of
core priors about object-centric dynamics and interactions to accelerate
learning in low-data regimes. The resulting approach, which we call AXIOM,
combines the usual data efficiency and interpretability of Bayesian approaches
with the across-task generalization usually associated with DRL. AXIOM
represents scenes as compositions of objects, whose dynamics are modeled as
piecewise linear trajectories that capture sparse object-object interactions.
The structure of the generative model is expanded online by growing and
learning mixture models from single events and periodically refined through
Bayesian model reduction to induce generalization. AXIOM masters various games
within only 10,000 interaction steps, with both a small number of parameters
compared to DRL, and without the computational expense of gradient-based
optimization.

</details>


### [165] [RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation](https://arxiv.org/abs/2505.24442)
*Zhentao Xie, Chengcheng Han, Jinxin Shi, Wenjun Cui, Xin Zhao, Xingjiao Wu, Jiabao Zhao*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为Residual Mixture-of-Agents (RMoA)的新方法，通过整合残差连接和多种优化机制，提高了多智能体系统的效率和可靠性，并在多个任务中取得了优异表现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于大语言模型的多智能体系统在多个任务中表现出强大的能力，但它们仍受限于高计算开销、信息损失和鲁棒性问题。

**方法:** 通过设计基于残差连接的多智能体系统，并引入嵌入多样性选择机制、残差提取智能体、残差聚合智能体以及自适应终止机制。

**结果:** RMoA在跨对齐、数学推理、代码生成和多任务理解等基准测试中取得了最先进性能，同时提升了推理效率并减少了计算成本。

**结论:** RMoA在多个基准任务上实现了最先进的性能，同时显著降低了计算开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RMoA%3A+Optimizing+Mixture-of-Agents+through+Diversity+Maximization+and+Residual+Compensation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24442&send_immediately=true&force_search=false)

**原文摘要:** Although multi-agent systems based on large language models show strong
capabilities on multiple tasks, they are still limited by high computational
overhead, information loss, and robustness. Inspired by ResNet's residual
learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual
connections to optimize efficiency and reliability. To maximize information
utilization from model responses while minimizing computational costs, we
innovatively design an embedding-based diversity selection mechanism that
greedily selects responses via vector similarity. Furthermore, to mitigate
iterative information degradation, we introduce a Residual Extraction Agent to
preserve cross-layer incremental information by capturing inter-layer response
differences, coupled with a Residual Aggregation Agent for hierarchical
information integration. Additionally, we propose an adaptive termination
mechanism that dynamically halts processing based on residual convergence,
further improving inference efficiency. RMoA achieves state-of-the-art
performance on the benchmarks of across alignment, mathematical reasoning, code
generation, and multitasking understanding, while significantly reducing
computational overhead. Code is available at
https://github.com/mindhunter01/RMoA.

</details>


### [166] [SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors](https://arxiv.org/abs/2505.24458)
*Tianlong Yu, Chenghang Ye, Zheyu Yang, Ziyi Zhou, Cui Tang, Zui Tao, Jun Zhang, Kailong Wang, Liting Zhou, Yang Yang, Ting Bi*

**主要类别:** cs.AI

**AI概要:** SEAR数据集揭示了基于增强现实的社会工程攻击的严重性，并为相关研究提供了支持，同时强调了防御框架设计的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 随着增强现实技术和多模态大语言模型的发展，社会工程攻击正成为一种新兴威胁，因此需要专门的数据集来研究和应对这种攻击方式。

**方法:** 构建了一个包含180个注释对话的多模态数据集SEAR，覆盖60名参与者在模拟对抗场景中的交互，并同步捕获视觉和音频线索、环境背景以及社交媒体资料等信息。

**结果:** 关键发现包括SEAR在诱导用户顺从中表现出惊人的效果（如93.3%的钓鱼链接点击率，85%的接听率）以及显著影响信任度（76.7%的互动后信任激增）。

**结论:** SEAR数据集为研究增强现实驱动的社会工程攻击提供了有价值的资源，同时强调了需要制定防御策略来应对这些威胁。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SEAR%3A+A+Multimodal+Dataset+for+Analyzing+AR-LLM-Driven+Social+Engineering+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24458&send_immediately=true&force_search=false)

**原文摘要:** The SEAR Dataset is a novel multimodal resource designed to study the
emerging threat of social engineering (SE) attacks orchestrated through
augmented reality (AR) and multimodal large language models (LLMs). This
dataset captures 180 annotated conversations across 60 participants in
simulated adversarial scenarios, including meetings, classes and networking
events. It comprises synchronized AR-captured visual/audio cues (e.g., facial
expressions, vocal tones), environmental context, and curated social media
profiles, alongside subjective metrics such as trust ratings and susceptibility
assessments. Key findings reveal SEAR's alarming efficacy in eliciting
compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and
hijacking trust (76.7% post-interaction trust surge). The dataset supports
research in detecting AR-driven SE attacks, designing defensive frameworks, and
understanding multimodal adversarial manipulation. Rigorous ethical safeguards,
including anonymization and IRB compliance, ensure responsible use. The SEAR
dataset is available at https://github.com/INSLabCN/SEAR-Dataset.

</details>


### [167] [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)
*Vasilije Markovic, Lazar Obradovic, Laszlo Hajdu, Jovan Pavlovic*

**主要类别:** cs.AI

**AI概要:** 这篇论文研究了如何优化复杂系统中超参数的作用，特别是在将大型语言模型与知识图谱集成的环境中，结果显示有针对性的调优能够带来显著的性能提升，同时也指出了评估指标的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管将大型语言模型与知识图谱结合的系统在检索增强生成中越来越常见，但系统的超参数优化研究仍不足。

**方法:** 通过使用三个多跳问答基准测试（HotPotQA、TwoWikiMultiHop 和 MuSiQue）对 Cognee 框架中的块划分、图构建、检索和提示等参数进行优化，并利用精确匹配、F1 分数以及 DeepEval 的基于 LLM 的正确性度量来评分。

**结果:** 实验结果表明，有针对性的调优可以显著提高性能，但不同数据集和度量标准下的提升并不一致，这突出了调优的价值以及现有评估指标的局限性。

**结论:** 本文强调了超参数调优在将大型语言模型与知识图谱集成中的重要性，并指出未来的进展不仅依赖于架构的改进，还需要更清晰的优化和评估框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+the+Interface+Between+Knowledge+Graphs+and+LLMs+for+Complex+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24478，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24478&send_immediately=true&force_search=false)

**原文摘要:** Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results
in complex systems with numerous hyperparameters that directly affect
performance. While such systems are increasingly common in retrieval-augmented
generation, the role of systematic hyperparameter optimization remains
underexplored. In this paper, we study this problem in the context of Cognee, a
modular framework for end-to-end KG construction and retrieval. Using three
multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize
parameters related to chunking, graph construction, retrieval, and prompting.
Each configuration is scored using established metrics (exact match, F1, and
DeepEval's LLM-based correctness metric). Our results demonstrate that
meaningful gains can be achieved through targeted tuning. While the gains are
consistent, they are not uniform, with performance varying across datasets and
metrics. This variability highlights both the value of tuning and the
limitations of standard evaluation measures. While demonstrating the immediate
potential of hyperparameter tuning, we argue that future progress will depend
not only on architectural advances but also on clearer frameworks for
optimization and evaluation in complex, modular systems.

</details>


### [168] [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/abs/2505.24479)
*Sania Nayab, Marco Simoni, Giulio Rossolini*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一种使用知识图谱与大型语言模型生成虚假信息的新技术，并指出当前检测方法的不足之处。


<details>
  <summary>更多</summary>
  
**动机:** 为了应对由于生成式人工智能的发展而加剧的虚假信息威胁，需要探索能够实现结构化和可扩展虚假信息生成的方法。

**方法:** 这篇论文提出了一种利用知识图谱（KGs）作为结构化语义资源来系统生成虚假三元组的新方法，并使用大型语言模型（LLMs）生成具有不同可信度的虚假信息。

**结果:** 通过分析知识图谱的结构性质，识别出可能错误的关系，并生成用于指导大型语言模型制造虚假信息的三元组。此外，还发现了当前基于大型语言模型的检测方法存在显著局限性。

**结论:** 该论文得出结论，利用知识图谱和大型语言模型生成虚假信息的方法极具挑战性，并强调需要改进检测策略及深入研究生成模型中的固有偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Knowledge+Graphs+and+LLMs+for+Structured+Generation+of+Misinformation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24479，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24479&send_immediately=true&force_search=false)

**原文摘要:** The rapid spread of misinformation, further amplified by recent advances in
generative AI, poses significant threats to society, impacting public opinion,
democratic stability, and national security. Understanding and proactively
assessing these threats requires exploring methodologies that enable structured
and scalable misinformation generation. In this paper, we propose a novel
approach that leverages knowledge graphs (KGs) as structured semantic resources
to systematically generate fake triplets. By analyzing the structural
properties of KGs, such as the distance between entities and their predicates,
we identify plausibly false relationships. These triplets are then used to
guide large language models (LLMs) in generating misinformation statements with
varying degrees of credibility. By utilizing structured semantic relationships,
our deterministic approach produces misinformation inherently challenging for
humans to detect, drawing exclusively upon publicly available KGs (e.g.,
WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing
between genuine and artificially generated misinformation. Our analysis
highlights significant limitations in current LLM-based detection methods,
underscoring the necessity for enhanced detection strategies and a deeper
exploration of inherent biases in generative models.

</details>


### [169] [MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge](https://arxiv.org/abs/2505.24493)
*Xin Jing, Jiadong Wang, Iosif Tsangko, Andreas Triantafyllopoulos, Björn W. Schuller*

**主要类别:** cs.AI

**AI概要:** 本论文研究了使用GPT-4o对语音情感识别进行自动标注的可能性，并提出了一个完全由GPT-4o标注的多模态情感数据集MELT。


<details>
  <summary>更多</summary>
  
**动机:** 语音情感识别（SER）依赖于人工标注，成本高且存在不一致性，而大语言模型（LLMs）可能提供一种可扩展的替代方式。因此，本文旨在探索无需人工监督的GPT-4o在情感语音数据标注中的潜力。

**方法:** 通过构造结构化文本提示，利用GPT-4o生成准确且与上下文相关的标注，仅使用文本线索作为输入来标注来自情景喜剧Friends的多模态数据集。

**结果:** 提出了MELT数据集，并通过微调四种自监督学习（SSL）骨干网络评估其在情感识别任务上的性能。主观实验结果显示，在SER任务上有一致性的性能提升。

**结论:** 研究表明，GPT-4o能够在没有直接多模态输入的情况下有效进行情感语音数据标注，MELT数据集展示了良好的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MELT%3A+Towards+Automated+Multimodal+Emotion+Data+Annotation+by+Leveraging+LLM+Embedded+Knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24493，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24493&send_immediately=true&force_search=false)

**原文摘要:** Although speech emotion recognition (SER) has advanced significantly with
deep learning, annotation remains a major hurdle. Human annotation is not only
costly but also subject to inconsistencies annotators often have different
preferences and may lack the necessary contextual knowledge, which can lead to
varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have
emerged as a scalable alternative for annotating text data. However, the
potential of LLMs to perform emotional speech data annotation without human
supervision has yet to be thoroughly investigated. To address these problems,
we apply GPT-4o to annotate a multimodal dataset collected from the sitcom
Friends, using only textual cues as inputs. By crafting structured text
prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated
during its training, showcasing that it can generate accurate and contextually
relevant annotations without direct access to multimodal inputs. Therefore, we
propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We
demonstrate the effectiveness of MELT by fine-tuning four self-supervised
learning (SSL) backbones and assessing speech emotion recognition performance
across emotion datasets. Additionally, our subjective experiments\' results
demonstrate a consistence performance improvement on SER.

</details>


### [170] [Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction](https://arxiv.org/abs/2505.24597)
*Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong*

**主要类别:** cs.AI

**AI概要:** 本文提出了NextLocMoE，一种基于大语言模型和混合专家结构的新型框架，用于提升用户下一位址预测的效果和适应性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法无法很好地捕捉现实世界地点的复杂多义性和不同用户群体的行为差异。

**方法:** 引入NextLocMoE，包含Location Semantics MoE和Personalized MoE，并结合历史感知路由机制。

**结果:** 在多个真实城市数据集上的实验表明，NextLocMoE在预测准确性、跨域泛化能力和可解释性方面表现优异。

**结论:** 论文提出NextLocMoE框架，基于大语言模型和双层混合专家设计，有效解决现有方法在捕捉地点语义和建模用户行为动态方面的不足。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixture-of-Experts+for+Personalized+and+Semantic-Aware+Next+Location+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24597&send_immediately=true&force_search=false)

**原文摘要:** Next location prediction plays a critical role in understanding human
mobility patterns. However, existing approaches face two core limitations: (1)
they fall short in capturing the complex, multi-functional semantics of
real-world locations; and (2) they lack the capacity to model heterogeneous
behavioral dynamics across diverse user groups. To tackle these challenges, we
introduce NextLocMoE, a novel framework built upon large language models (LLMs)
and structured around a dual-level Mixture-of-Experts (MoE) design. Our
architecture comprises two specialized modules: a Location Semantics MoE that
operates at the embedding level to encode rich functional semantics of
locations, and a Personalized MoE embedded within the Transformer backbone to
dynamically adapt to individual user mobility patterns. In addition, we
incorporate a history-aware routing mechanism that leverages long-term
trajectory data to enhance expert selection and ensure prediction stability.
Empirical evaluations across several real-world urban datasets show that
NextLocMoE achieves superior performance in terms of predictive accuracy,
cross-domain generalization, and interpretability

</details>


### [171] [Taxonomic Networks: A Representation for Neuro-Symbolic Pairing](https://arxiv.org/abs/2505.24601)
*Zekun Wang, Ethan L. Haarer, Nicki Barari, Christopher J. MacLellan*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的神经符号对方法，能够在不同情境下互换使用，并在必要时实现无缝转换。


<details>
  <summary>更多</summary>
  
**动机:** 探索神经和符号方法在知识表示上的结合，以提高数据和计算资源的利用效率。

**方法:** 提出了一种新的神经符号对方法，该方法通过共同的知识表示将神经和符号方法联系起来，并构建了新型的分类网络进行评估。

**结果:** 符号方法在较少的数据和计算资源下能更高效地学习分类网络，而神经方法在提供更多资源时能找到更高准确率的分类网络。

**结论:** 神经符号对方法可以根据情境需求互换使用，并且在必要时可以无缝转换，为未来更深入整合神经和符号计算的系统奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Taxonomic+Networks%3A+A+Representation+for+Neuro-Symbolic+Pairing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24601&send_immediately=true&force_search=false)

**原文摘要:** We introduce the concept of a \textbf{neuro-symbolic pair} -- neural and
symbolic approaches that are linked through a common knowledge representation.
Next, we present \textbf{taxonomic networks}, a type of discrimination network
in which nodes represent hierarchically organized taxonomic concepts. Using
this representation, we construct a novel neuro-symbolic pair and evaluate its
performance. We show that our symbolic method learns taxonomic nets more
efficiently with less data and compute, while the neural method finds
higher-accuracy taxonomic nets when provided with greater resources. As a
neuro-symbolic pair, these approaches can be used interchangeably based on
situational needs, with seamless translation between them when necessary. This
work lays the foundation for future systems that more fundamentally integrate
neural and symbolic computation.

</details>


### [172] [Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success](https://arxiv.org/abs/2505.24622)
*Ben Griffin, Joseph Ternasky, Fuat Alican, Yigit Ihlamur*

**主要类别:** cs.AI

**AI概要:** 提出一种透明的轻量级集成框架，利用大语言模型生成的问题进行预测，显著提高初创企业成功预测的精度。


<details>
  <summary>更多</summary>
  
**动机:** 需要既准确又可解释的模型来预测初创企业的成功，以支持风险投资等高风险决策。

**方法:** 使用大语言模型生成YES/NO问题，并通过基于阈值的投票机制过滤、排序和聚合，构建集成预测器；引入专家启发式方法提升性能。

**结果:** 在测试集中，10%初创企业被分类为成功的情况下，该方法达到50%的精确率，比随机选择提高了5倍；加入专家指导后提升至54%。

**结论:** 结合LLM推理与人类洞察力的轻量级集成框架在预测初创企业成功方面表现出色，展示了可解释模型在高风险决策中的价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Random+Rule+Forest+%28RRF%29%3A+Interpretable+Ensembles+of+LLM-Generated+Questions+for+Predicting+Startup+Success，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24622&send_immediately=true&force_search=false)

**原文摘要:** Predicting startup success requires models that are both accurate and
interpretable. We present a lightweight ensemble framework that combines YES/NO
questions generated by large language models (LLMs), forming a transparent
decision-making system. Each question acts as a weak heuristic, and by
filtering, ranking, and aggregating them through a threshold-based voting
mechanism, we construct a strong ensemble predictor. On a test set where 10% of
startups are classified as successful, our approach achieves a precision rate
of 50%, representing a 5x improvement over random selection, while remaining
fully transparent. When we incorporate expert-guided heuristics into the
generation process, performance improves further to 54% precision. These
results highlight the value of combining LLM reasoning with human insight and
demonstrate that simple, interpretable ensembles can support high-stakes
decisions in domains such as venture capital (VC).

</details>


### [173] [Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models](https://arxiv.org/abs/2505.24655)
*Frederike Lübeck, Jonas Wildberger, Frederik Träuble, Maximilian Mordig, Sergios Gatidis, Andreas Krause, Bernhard Schölkopf*

**主要类别:** cs.AI

**AI概要:** 本文提出AdaCVD，一种基于AI的心血管疾病风险预测工具，解决了传统模型在灵活性、数据整合和适应性方面的不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有的心血管疾病风险预测模型难以应对现实世界临床实践中患者资料复杂性、输入模式僵化以及对分布变化敏感的问题。

**方法:** 开发了基于大规模语言模型的AdaCVD框架，并在超过50万名参与者的数据上进行了微调。

**结果:** AdaCVD在基准测试中超越了现有风险评分和传统机器学习方法，实现了最先进的性能，并能够快速适应新人群，同时表现出稳定的分层分析结果。

**结论:** AdaCVD为心血管疾病风险预测提供了一种灵活、高效的新方法，适用于异构和动态的医疗环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptable+Cardiovascular+Disease+Risk+Prediction+from+Heterogeneous+Data+using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24655&send_immediately=true&force_search=false)

**原文摘要:** Cardiovascular disease (CVD) risk prediction models are essential for
identifying high-risk individuals and guiding preventive actions. However,
existing models struggle with the challenges of real-world clinical practice as
they oversimplify patient profiles, rely on rigid input schemas, and are
sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk
prediction framework built on large language models extensively fine-tuned on
over half a million participants from the UK Biobank. In benchmark comparisons,
AdaCVD surpasses established risk scores and standard machine learning
approaches, achieving state-of-the-art performance. Crucially, for the first
time, it addresses key clinical challenges across three dimensions: it flexibly
incorporates comprehensive yet variable patient information; it seamlessly
integrates both structured data and unstructured text; and it rapidly adapts to
new patient populations using minimal additional data. In stratified analyses,
it demonstrates robust performance across demographic, socioeconomic, and
clinical subgroups, including underrepresented cohorts. AdaCVD offers a
promising path toward more flexible, AI-driven clinical decision support tools
suited to the realities of heterogeneous and dynamic healthcare environments.

</details>


### [174] [EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/abs/2505.24785)
*Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Matei Zaharia, Ang Chen*

**主要类别:** cs.AI

**AI概要:** EXP-Bench是一个新的基准测试，用于系统评估AI代理在完整AI研究实验中的表现，揭示了当前技术的瓶颈。


<details>
  <summary>更多</summary>
  
**动机:** 自动化AI研究具有加速科学进步的潜力，但当前AI代理难以处理完整的实验流程。

**方法:** 设计了一个半自动管道来从研究论文及其相关开源代码中提取和构建实验细节，并创建了461个AI研究任务。

**结果:** 对OpenHands和IterativeAgent等LLM代理的评估显示，它们在个别实验方面得分达到20-35%，但在完整实验的成功率仅为0.5%。

**结论:** EXP-Bench 是一个用于评估AI代理进行端到端实验能力的新基准测试，同时指出了当前LLM代理在执行完整实验任务方面的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EXP-Bench%3A+Can+AI+Conduct+AI+Research+Experiments%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24785，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24785&send_immediately=true&force_search=false)

**原文摘要:** Automating AI research holds immense potential for accelerating scientific
progress, yet current AI agents struggle with the complexities of rigorous,
end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed
to systematically evaluate AI agents on complete research experiments sourced
from influential AI publications. Given a research question and incomplete
starter code, EXP-Bench challenges AI agents to formulate hypotheses, design
and implement experimental procedures, execute them, and analyze results. To
enable the creation of such intricate and authentic tasks with high-fidelity,
we design a semi-autonomous pipeline to extract and structure crucial
experimental details from these research papers and their associated
open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks
from 51 top-tier AI research papers. Evaluations of leading LLM-based agents,
such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial
capabilities: while scores on individual experimental aspects such as design or
implementation correctness occasionally reach 20-35%, the success rate for
complete, executable experiments was a mere 0.5%. By identifying these
bottlenecks and providing realistic step-by-step experiment procedures,
EXP-Bench serves as a vital tool for future AI agents to improve their ability
to conduct AI research experiments. EXP-Bench is open-sourced at
https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.

</details>


### [175] [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
*Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, Han Zhao*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的两阶段个性化偏好学习框架MiCRo，无需显式的细粒度注释即可提升大语言模型对多样化人类偏好的适应能力。


<details>
  <summary>更多</summary>
  
**动机:** 基于Bradley-Terry模型的奖励建模假设了一个全局奖励函数，无法捕捉本质上多样化和异质化的人类偏好，从而限制了LLMs支持个性化和多元化对齐的能力。

**方法:** MiCRo采用了一个两阶段的框架，第一阶段引入上下文感知的混合建模方法来捕捉多样化的人类偏好，第二阶段集成了在线路由策略，根据特定上下文动态调整混合权重以解决歧义。

**结果:** 实验表明，MiCRo能够有效捕捉多样化的用户偏好，并在多个偏好数据集中显著提升下游个性化任务的表现。

**结论:** MiCRo有效地捕捉了多样化的用户偏好，并在下游个性化任务中显著提升了性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MiCRo%3A+Mixture+Modeling+and+Context-aware+Routing+for+Personalized+Preference+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24846，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24846&send_immediately=true&force_search=false)

**原文摘要:** Reward modeling is a key step in building safe foundation models when
applying reinforcement learning from human feedback (RLHF) to align Large
Language Models (LLMs). However, reward modeling based on the Bradley-Terry
(BT) model assumes a global reward function, failing to capture the inherently
diverse and heterogeneous human preferences. Hence, such oversimplification
limits LLMs from supporting personalization and pluralistic alignment.
Theoretically, we show that when human preferences follow a mixture
distribution of diverse subgroups, a single BT model has an irreducible error.
While existing solutions, such as multi-objective learning with fine-grained
annotations, help address this issue, they are costly and constrained by
predefined attributes, failing to fully capture the richness of human values.
In this work, we introduce MiCRo, a two-stage framework that enhances
personalized preference learning by leveraging large-scale binary preference
datasets without requiring explicit fine-grained annotations. In the first
stage, MiCRo introduces context-aware mixture modeling approach to capture
diverse human preferences. In the second stage, MiCRo integrates an online
routing strategy that dynamically adapts mixture weights based on specific
context to resolve ambiguity, allowing for efficient and scalable preference
adaptation with minimal additional supervision. Experiments on multiple
preference datasets demonstrate that MiCRo effectively captures diverse human
preferences and significantly improves downstream personalization.

</details>


### [176] [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
*Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen*

**主要类别:** cs.AI

**AI概要:** 本研究介绍了一个名为 Open CaptchaWorld 的新基准测试平台，用以评估多模态代理解决 CAPTCHA 难题的能力，结果显示现有代理与人类表现存在显著差距。


<details>
  <summary>更多</summary>
  
**动机:** 现代多模态 LLM 代理在静态感知任务中表现出色，但它们处理像 CAPTCHAs 这样的交互式、多步骤推理挑战的能力很大程度上未经检验。

**方法:** 引入了 Open CaptchaWorld，这是一个专门设计的基于网络的基准测试平台，通过多样化和动态的 CAPTCHA 难题来评估 MLLM 支持的代理的视觉推理和交互能力。

**结果:** 实验结果表明，人类得分接近完美，而最先进的 MLLM 代理表现显著挣扎，最高成功率仅为 Browser-Use Openai-o3 的 40.0%，远低于人类水平的 93.3%。

**结论:** Open CaptchaWorld 是一个重要的基准，用于诊断当前多模态代理的局限性，并指导更强大的多模态推理系统的开发。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Open+CaptchaWorld%3A+A+Comprehensive+Web-based+Platform+for+Testing+and+Benchmarking+Multimodal+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24878&send_immediately=true&force_search=false)

**原文摘要:** CAPTCHAs have been a critical bottleneck for deploying web agents in
real-world applications, often blocking them from completing end-to-end
automation tasks. While modern multimodal LLM agents have demonstrated
impressive performance in static perception tasks, their ability to handle
interactive, multi-step reasoning challenges like CAPTCHAs is largely untested.
To address this gap, we introduce Open CaptchaWorld, the first web-based
benchmark and platform specifically designed to evaluate the visual reasoning
and interaction capabilities of MLLM-powered agents through diverse and dynamic
CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225
CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,
which quantifies the number of cognitive and motor steps required to solve each
puzzle. Experimental results show that humans consistently achieve near-perfect
scores, state-of-the-art MLLM agents struggle significantly, with success rates
at most 40.0% by Browser-Use Openai-o3, far below human-level performance,
93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing
the limits of current multimodal agents and guiding the development of more
robust multimodal reasoning systems. Code and Data are available at this https
URL.

</details>


### [177] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/abs/2505.05880)
*Bettina Fazzinga, Sergio Flesca, Filippo Furfaro, Luigi Pontieri, Francesco Scala*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种结合基于示例的序列标注模型和基于抽象论证框架（AAF）的推理器的神经符号方法，用于解决过程跟踪中事件到活动映射的高度不确定性问题，从而在数据稀缺的情况下提供高效的解释方案。


<details>
  <summary>更多</summary>
  
**动机:** 现代公司和组织需要监控和分析过程跟踪，但在跟踪事件与参考业务活动之间存在差距时，这会成为一个解释问题。为了解决这一问题，论文提出了一个高效的方法，旨在减少劳动和计算成本以及碳足迹，同时满足绿色人工智能的需求。

**方法:** 论文的方法包括使用抽象论证框架（AAF）来分析可能的事件解释，并提出了一种序列标注模型，该模型以情境感知的方式建议高概率的候选事件解释。此外，该方法结合了基于示例的序列标注模型和基于AAF的推理器，以提高效率并减少对大量手动注释数据的依赖。

**结果:** 实验结果显示，所提出的神经符号方法在事件到活动映射高度不确定的情况下能够有效工作，它通过结合基于示例的序列标注器和基于AAF的推理器，提供了高概率的候选解释，并且利用先验知识弥补了示例数据的不足。

**结论:** 论文得出结论，通过将基于示例的序列标注器与基于AAF的推理器结合，可以实现数据和计算高效的神经符号方法，这种方法能够在事件到活动映射高度不确定的情况下提供高概率的候选解释，并利用先验知识补偿示例数据的稀缺性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Combining+Abstract+Argumentation+and+Machine+Learning+for+Efficiently+Analyzing+Low-Level+Process+Event+Streams，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.05880，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.05880&send_immediately=true&force_search=false)

**原文摘要:** Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [178] [Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning](https://arxiv.org/abs/2505.23783)
*Korel Gundem, Juncheng Dong, Dennis Zhang, Vahid Tarokh, Zhengling Qi*

**主要类别:** stat.ML

**AI概要:** 本文提出 Supervised Calibration（SC）框架，通过在 logit 空间中进行最优仿射变换并引入新型正则化策略，显著提升 In-Context Learning 的校准效果和稳定性。


<details>
  <summary>更多</summary>
  
**动机:** In-Context Learning (ICL) 的预测存在系统性偏差，传统校准方法只能调整决策边界的位置，无法改变其方向，导致在严重偏差情况下效果不佳。

**方法:** 提出 Supervised Calibration (SC)，基于损失最小化的方法，在 logit 空间中学习每个类别的仿射变换，同时引入两种正则化技术：context-invariance 和 directional trust-region。

**结果:** SC 能够改变 LLM 决策边界的取向，甚至完全反转；在 4-shot、8-shot 和 16-shot 设置下，SC 在所有九个数据集上均优于现有校准方法。

**结论:** Supervised Calibration (SC) 框架能够有效解决 ICL 中的系统偏差问题，提供统一且更具表达能力的校准方法，并在多个数据集和模型上实现了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Boosting+In-Context+Learning+in+LLMs+Through+the+Lens+of+Classical+Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23783，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23783&send_immediately=true&force_search=false)

**原文摘要:** In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new
tasks with just a few examples, but their predictions often suffer from
systematic biases, leading to unstable performances in classification. While
calibration techniques are proposed to mitigate these biases, we show that, in
the logit space, many of these methods are equivalent to merely shifting the
LLM's decision boundary without having the ability to alter its orientation.
This proves inadequate when biases cause the LLM to be severely misdirected. To
address these limitations and provide a unifying framework, we propose
Supervised Calibration (SC), a loss-minimization based framework which learns
an optimal, per-class affine transformation of the LLM's predictive
probabilities in the logit space without requiring external data beyond the
context. By using a more expressive functional class, SC not only subsumes many
existing calibration methods in ICL as special cases, but also enables the
ability to alter and even completely reverse the orientation of the LLM's
decision boundary. Furthermore, SC's loss-based nature facilitates the seamless
integration of two purpose-built regularization techniques: context-invariance
and directional trust-region. The former is designed to tackle the instability
issue in ICL, while the latter controls the degree of calibration. Finally, SC
delivers state-of-the-art performance over calibration baselines in the 4-shot,
8-shot, and 16-shot settings across all nine datasets for
Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.

</details>


### [179] [Gibbs randomness-compression proposition: An efficient deep learning](https://arxiv.org/abs/2505.23869)
*M. Süzen*

**主要类别:** stat.ML

**AI概要:** 本文提出了基于Gibbs熵的随机性与压缩关系理论，并通过新的Dual Tomographic Compression (DTC) 框架在训练中实现高效压缩与高性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 作者试图揭示随机性和压缩之间的联系，并开发一种更高效、节能的深度学习模型压缩与训练方法。受彩票假设启发，他们希望通过迭代方式实现智能神经网络架构搜索。

**方法:** 论文提出了“Dual Tomographic Compression (DTC) 压缩-训练框架”，该框架通过构建压缩感知投影（称为权重射线）进行层权重矩阵的断层重建，并以双模式应用于前一层和下一层，从而触发神经元级别的剪枝。

**结果:** 实验表明，DTC框架在训练过程中实现了高效的压缩并取得了最先进的性能；同时，从统计物理的角度观察到随机压缩迭代具有相似性能，验证了随机性与压缩之间的关系。

**结论:** 论文提出了一种名为“Gibbs randomness-compression proposition”的新命题，通过Gibbs熵将随机性和压缩联系起来，并表明DTC框架能够提供一种大规模节能和资源高效的深度学习训练方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gibbs+randomness-compression+proposition%3A+An+efficient+deep+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23869，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23869&send_immediately=true&force_search=false)

**原文摘要:** A proposition that connects randomness and compression put forward via Gibbs
entropy over set of measurement vectors associated with a compression process.
The proposition states that a lossy compression process is equivalent to {\it
directed randomness} that preserves information content. The proposition
originated from the observed behaviour in newly proposed {\it Dual Tomographic
Compression} (DTC) compress-train framework. This is akin to tomographic
reconstruction of layer weight matrices via building compressed sensed
projections, so called {\it weight rays}. This tomographic approach is applied
to previous and next layers in a dual fashion, that triggers neuronal-level
pruning. This novel model compress-train scheme appear in iterative fashion and
act as smart neural architecture search, Experiments demonstrated utility of
this dual-tomography producing state-of-the-art performance with efficient
compression during training, accelerating and supporting lottery ticket
hypothesis. However, random compress-train iterations having similar
performance demonstrated the connection between randomness and compression from
statistical physics perspective, we formulated so called {\it Gibbs
randomness-compression proposition}, signifying randomness-compression
relationship via Gibbs entropy. Practically, DTC framework provides a promising
approach for massively energy and resource efficient deep learning training
approach.

</details>


### [180] [Conformal Object Detection by Sequential Risk Control](https://arxiv.org/abs/2505.24038)
*Léo Andéol, Luca Mossina, Adrien Mazoyer, Sébastien Gerchinovitz*

**主要类别:** stat.ML

**AI概要:** 这篇论文介绍了Conformal Prediction在目标检测中的应用，提出了新的方法SeqCRC，并开发了相关工具包以增强模型可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 目标检测模型在关键工业应用中的部署受到神经网络固有不可靠性和模型结构复杂性的阻碍，因此需要一种可靠的后处理方法。

**方法:** 论文提出了一种新方法Sequential Conformal Risk Control (SeqCRC)，并设计了适用于不同应用场景和认证需求的损失函数与预测集。

**结果:** 论文展示了广泛的实验结果，建立了一个基准，验证了所研究的方法，并强调了其权衡和实际影响。

**结论:** 论文得出结论，SeqCRC方法和提出的损失函数与预测集能够有效提升目标检测模型的可靠性，并通过工具包促进了方法的复现与进一步研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conformal+Object+Detection+by+Sequential+Risk+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24038&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in object detectors have led to their adoption for industrial
uses. However, their deployment in critical applications is hindered by the
inherent lack of reliability of neural networks and the complex structure of
object detection models. To address these challenges, we turn to Conformal
Prediction, a post-hoc procedure which offers statistical guarantees that are
valid for any dataset size, without requiring prior knowledge on the model or
data distribution. Our contribution is manifold: first, we formally define the
problem of Conformal Object Detection (COD) and introduce a novel method,
Sequential Conformal Risk Control (SeqCRC), that extends the statistical
guarantees of Conformal Risk Control (CRC) to two sequential tasks with two
parameters, as required in the COD setting. Then, we propose loss functions and
prediction sets suited to applying CRC to different applications and
certification requirements. Finally, we present a conformal toolkit, enabling
replication and further exploration of our methods. Using this toolkit, we
perform extensive experiments, yielding a benchmark that validates the
investigated methods and emphasizes trade-offs and other practical
consequences.

</details>


### [181] [Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity](https://arxiv.org/abs/2505.24097)
*Victor Li, Baiting Chen, Yuzhen Mao, Qi Lei, Zhun Deng*

**主要类别:** stat.ML

**AI概要:** 这篇论文介绍了Performative Risk Control框架，用于在校准机器学习模型时处理预测影响实际结果的问题，确保决策可靠。


<details>
  <summary>更多</summary>
  
**动机:** 确保黑箱机器学习模型的预测满足有限样本的统计保证至关重要，但预测本身可能影响其试图预测的结果，这种现象在社会科学和经济学中很常见，因此需要新的方法来应对这一问题。

**方法:** 作者介绍了一种迭代优化的校准过程，通过这个过程可以在预测结果影响实际情况的前提下持续改进预测并进行风险控制。

**结果:** 该研究提供了具有理论保障的风险控制方法，并探讨了不同类型的风险度量和尾部边界选择的影响，同时通过信用违约风险预测任务的数值实验验证了该框架的有效性。

**结论:** 这篇论文得出的结论是，他们提出了Performative Risk Control框架，这是首个在预测结果会影响实际结果（performativity）的情况下，实现统计上严格风险控制的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performative+Risk+Control%3A+Calibrating+Models+for+Reliable+Deployment+under+Performativity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24097，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24097&send_immediately=true&force_search=false)

**原文摘要:** Calibrating blackbox machine learning models to achieve risk control is
crucial to ensure reliable decision-making. A rich line of literature has been
studying how to calibrate a model so that its predictions satisfy explicit
finite-sample statistical guarantees under a fixed, static, and unknown
data-generating distribution. However, prediction-supported decisions may
influence the outcome they aim to predict, a phenomenon named performativity of
predictions, which is commonly seen in social science and economics. In this
paper, we introduce Performative Risk Control, a framework to calibrate models
to achieve risk control under performativity with provable theoretical
guarantees. Specifically, we provide an iteratively refined calibration
process, where we ensure the predictions are improved and risk-controlled
throughout the process. We also study different types of risk measures and
choices of tail bounds. Lastly, we demonstrate the effectiveness of our
framework by numerical experiments on the task of predicting credit default
risk. To the best of our knowledge, this work is the first one to study
statistically rigorous risk control under performativity, which will serve as
an important safeguard against a wide range of strategic manipulation in
decision-making processes.

</details>


### [182] [A Mathematical Perspective On Contrastive Learning](https://arxiv.org/abs/2505.24134)
*Ricardo Baptista, Andrew M. Stuart, Son Tran*

**主要类别:** stat.ML

**AI概要:** 本文研究了多模态对比学习，将其解释为优化参数化编码器以定义条件概率分布，并提出了跨模态生成模型及新的对比学习泛化方法。


<details>
  <summary>更多</summary>
  
**动机:** 多模态对比学习通常用于连接不同数据模态（如图像和文本），但其理论框架仍有待扩展，特别是在生成模型和概率损失函数方面。

**方法:** 作者从概率角度出发，将对比学习视为优化定义条件概率分布的编码器问题，并在多元高斯环境中研究潜在空间识别作为低秩矩阵近似问题。

**结果:** 提出了一种新的多模态对比学习框架，包括新的概率损失函数、对齐度量以及适用于特定任务的生成模型变体，并通过数值实验验证了该框架的有效性。

**结论:** 该研究为多模态对比学习提供了新的概率视角和理论基础，并展示了其在检索、分类和生成任务中的广泛应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Mathematical+Perspective+On+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24134&send_immediately=true&force_search=false)

**原文摘要:** Multimodal contrastive learning is a methodology for linking different data
modalities; the canonical example is linking image and text data. The
methodology is typically framed as the identification of a set of encoders, one
for each modality, that align representations within a common latent space. In
this work, we focus on the bimodal setting and interpret contrastive learning
as the optimization of (parameterized) encoders that define conditional
probability distributions, for each modality conditioned on the other,
consistent with the available data. This provides a framework for multimodal
algorithms such as crossmodal retrieval, which identifies the mode of one of
these conditional distributions, and crossmodal classification, which is
similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This
probabilistic perspective suggests two natural generalizations of contrastive
learning: the introduction of novel probabilistic loss functions, and the use
of alternative metrics for measuring alignment in the common latent space. We
study these generalizations of the classical approach in the multivariate
Gaussian setting. In this context we view the latent space identification as a
low-rank matrix approximation problem. This allows us to characterize the
capabilities of loss functions and alignment metrics to approximate natural
statistics, such as conditional means and covariances; doing so yields novel
variants on contrastive learning algorithms for specific mode-seeking and for
generative tasks. The framework we introduce is also studied through numerical
experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data
assimilation application arising in oceanography.

</details>


### [183] [Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings](https://arxiv.org/abs/2505.24281)
*Yang Sui, Qi Xu, Yang Bai, Annie Qu*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的多任务学习框架，能够更好地处理任务间的共享与异质信息，在理论和实际应用中均表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 多任务学习中需要有效利用共享和异质信息来提高效率，但现有方法难以在统一框架中解决异质性问题。

**方法:** 该方法结合了任务共享编码器和任务特定编码器，并探索了潜在因子系数的内在相似性结构，以统一算法交替学习编码器和系数。

**结果:** 理论分析显示了所提方法的过量风险界，并通过模拟研究和PDX数据中的肿瘤倍增时间预测验证了其优越性能。

**结论:** 该论文提出了一种双编码器框架，用于构建异质性潜在因子空间，从而更好地处理多任务学习中的共享和特异性信息。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-task+Learning+for+Heterogeneous+Data+via+Integrating+Shared+and+Task-Specific+Encodings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24281&send_immediately=true&force_search=false)

**原文摘要:** Multi-task learning (MTL) has become an essential machine learning tool for
addressing multiple learning tasks simultaneously and has been effectively
applied across fields such as healthcare, marketing, and biomedical research.
However, to enable efficient information sharing across tasks, it is crucial to
leverage both shared and heterogeneous information. Despite extensive research
on MTL, various forms of heterogeneity, including distribution and posterior
heterogeneity, present significant challenges. Existing methods often fail to
address these forms of heterogeneity within a unified framework. In this paper,
we propose a dual-encoder framework to construct a heterogeneous latent factor
space for each task, incorporating a task-shared encoder to capture common
information across tasks and a task-specific encoder to preserve unique task
characteristics. Additionally, we explore the intrinsic similarity structure of
the coefficients corresponding to learned latent factors, allowing for adaptive
integration across tasks to manage posterior heterogeneity. We introduce a
unified algorithm that alternately learns the task-specific and task-shared
encoders and coefficients. In theory, we investigate the excess risk bound for
the proposed MTL method using local Rademacher complexity and apply it to a new
but related task. Through simulation studies, we demonstrate that the proposed
method outperforms existing data integration methods across various settings.
Furthermore, the proposed method achieves superior predictive performance for
time to tumor doubling across five distinct cancer types in PDX data.

</details>


### [184] [Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels](https://arxiv.org/abs/2505.24311)
*Yi Gu*

**主要类别:** stat.ML

**AI概要:** 本文研究了t-SNE算法在广义核下的收敛性，证明其在数据点数量增加时仍能收敛到平衡分布。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地理解t-SNE算法在不同核函数下的行为和收敛性。

**方法:** 通过给出广义输入和输出核的具体形式，并证明其收敛性。

**结果:** 论文扩展了Auffinger和Fletcher在2023年的结果，表明在一定条件下，即使数据点数量发散，t-SNE算法仍能收敛到平衡分布。

**结论:** t-SNE算法在广义核下会收敛到一个平衡分布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Equilibrium+Distribution+for+t-Distributed+Stochastic+Neighbor+Embedding+with+Generalized+Kernels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24311，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24311&send_immediately=true&force_search=false)

**原文摘要:** T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm
for visualizing high-dimensional data by finding low-dimensional
representations. In this paper, we study the convergence of t-SNE with
generalized kernels and extend the results of Auffinger and Fletcher in 2023.
Our work starts by giving a concrete formulation of generalized input and
output kernels. Then we prove that under certain conditions, the t-SNE
algorithm converges to an equilibrium distribution for a wide range of input
and output kernels as the number of data points diverges.

</details>


### [185] [Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation](https://arxiv.org/abs/2505.24333)
*Alessio Giorlandino, Sebastian Goldt*

**主要类别:** stat.ML

**AI概要:** 本研究提出了一个用于分析Transformer架构中信号传播的理论框架，揭示了初始化不当导致的问题并提供了改进方案。


<details>
  <summary>更多</summary>
  
**动机:** 找到神经网络合适的初始化对于确保训练平稳和良好性能至关重要，而在Transformer中错误的初始化会导致自注意力层的两种失败模式：秩崩溃和熵崩溃。

**方法:** 通过借鉴统计物理学中的随机能量模型，对包含自注意力层、层归一化、跳跃连接和ReLU MLP的Transformer块进行解析分析。

**结果:** 识别出自注意力层中查询和键的初始方差所主导的两种状态：低方差状态下会出现秩崩溃；高方差状态下会出现熵崩溃。同时，计算出了残差连接强度的临界值以确保信号传播。

**结论:** 该论文提出了一种分析信号在普通Transformer块中传播的理论，为选择合适的初始化超参数提供了理论支持，并统一了对自注意力两种失败模式的认识。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two+failure+modes+of+deep+transformers+and+how+to+avoid+them%3A+a+unified+theory+of+signal+propagation+at+initialisation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24333&send_immediately=true&force_search=false)

**原文摘要:** Finding the right initialisation for neural networks is crucial to ensure
smooth training and good performance. In transformers, the wrong initialisation
can lead to one of two failure modes of self-attention layers: rank collapse,
where all tokens collapse into similar representations, and entropy collapse,
where highly concentrated attention scores lead to training instability. While
the right initialisation has been extensively studied in feed-forward networks,
an exact description of signal propagation through a full transformer block has
so far been lacking. Here, we provide an analytical theory of signal
propagation through vanilla transformer blocks with self-attention layers,
layer normalisation, skip connections and ReLU MLP. To treat the self-attention
layer, we draw on a formal parallel with the Random Energy Model from
statistical physics. We identify and characterise two regimes governed by the
variance of the query and key initialisations: a low-variance regime, where we
recover the known rank collapse behaviour; and a previously unexplored
high-variance regime, where signal is preserved but \textit{entropy collapse}
occurs. In the low-variance regime, we calculate the critical strength for the
residual connection to ensure signal propagation. Our theory yields
trainability diagrams that identify the correct choice of initialisation
hyper-parameters for a given architecture. Experiments with BERT-style models
trained on TinyStories validate our predictions. Our theoretical framework
gives a unified perspective on the two failure modes of self-attention and
gives quantitative predictions on the scale of both weights and residual
connections that guarantees smooth training.

</details>


### [186] [Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data](https://arxiv.org/abs/2505.24556)
*Gabriel V Cardoso, Mike Pereira*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种基于扩散生成模型（DGMs）的两步方法，用于模拟与非平稳高斯过程（GP）先验相关的预测后验分布（PPD），从而解决了计算上不可行的问题。


<details>
  <summary>更多</summary>
  
**动机:** 非平稳先验在捕捉复杂空间模式时往往是必要的，但在贝叶斯高斯过程模型中，这使得从预测后验分布中采样变得计算上不可行。因此需要一种新的方法来解决这个问题。

**方法:** 用扩散生成模型（DGM）替代高斯过程（GP）先验，并利用最近开发的无需训练的DGM指导算法从期望的后验分布中采样。

**结果:** 该方法成功应用于一个复杂的非平稳GP先验模型，并通过多个统计指标验证了所得分布与其对应的GP模型相近。此外，还展示了如何微调DGM以针对GP先验的特定部分，并将该方法应用于环境科学中的逆问题求解。

**结论:** 本文提出的基于扩散生成模型的方法为从非平稳高斯过程先验中进行后验采样提供了一个有效的解决方案，实现了最先进的预测能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predictive+posterior+sampling+from+non-stationnary+Gaussian+process+priors+via+Diffusion+models+with+application+to+climate+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24556&send_immediately=true&force_search=false)

**原文摘要:** Bayesian models based on Gaussian processes (GPs) offer a flexible framework
to predict spatially distributed variables with uncertainty. But the use of
nonstationary priors, often necessary for capturing complex spatial patterns,
makes sampling from the predictive posterior distribution (PPD) computationally
intractable. In this paper, we propose a two-step approach based on diffusion
generative models (DGMs) to mimic PPDs associated with non-stationary GP
priors: we replace the GP prior by a DGM surrogate, and leverage recent
advances on training-free guidance algorithms for DGMs to sample from the
desired posterior distribution. We apply our approach to a rich non-stationary
GP prior from which exact posterior sampling is untractable and validate that
the issuing distributions are close to their GP counterpart using several
statistical metrics. We also demonstrate how one can fine-tune the trained DGMs
to target specific parts of the GP prior. Finally we apply the proposed
approach to solve inverse problems arising in environmental sciences, thus
yielding state-of-the-art predictions.

</details>


### [187] [Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders](https://arxiv.org/abs/2505.24668)
*Jonghyun Ham, Maximilian Fleissner, Debarghya Ghoshdastidar*

**主要类别:** stat.ML

**AI概要:** 本文研究了带有瓶颈层和跳跃连接的两层线性去噪自动编码器，在梯度流和产品正则化下分析其测试风险，揭示了瓶颈宽度对偏差-方差的权衡及跳跃连接对降低方差的作用。


<details>
  <summary>更多</summary>
  
**动机:** 现代深度神经网络在高度过参数化的情况下表现出强泛化能力，而针对无监督任务如去噪仍存在许多未解问题。

**方法:** 通过梯度流训练两层线性去噪自动编码器，并利用乘积正则化推导出模型所有临界点的闭合形式表达式，进一步分析过参数化体制下的测试风险公式。

**结果:** 成功描述了带或不带跳跃连接的过参数化体制下的测试风险公式，并揭示了两个重要现象：瓶颈宽度与偏差-方差权衡的关系，以及跳跃连接对降低方差的影响。

**结论:** 论文得出，瓶颈层引入了一种类似于经典偏差-方差权衡的复杂性度量，并且跳跃连接可以减轻去噪自动编码器中的方差问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Impact+of+Bottleneck+Layers+and+Skip+Connections+on+the+Generalization+of+Linear+Denoising+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24668&send_immediately=true&force_search=false)

**原文摘要:** Modern deep neural networks exhibit strong generalization even in highly
overparameterized regimes. Significant progress has been made to understand
this phenomenon in the context of supervised learning, but for unsupervised
tasks such as denoising, several open questions remain. While some recent works
have successfully characterized the test error of the linear denoising problem,
they are limited to linear models (one-layer network). In this work, we focus
on two-layer linear denoising autoencoders trained under gradient flow,
incorporating two key ingredients of modern deep learning architectures: A
low-dimensional bottleneck layer that effectively enforces a rank constraint on
the learned solution, as well as the possibility of a skip connection that
bypasses the bottleneck. We derive closed-form expressions for all critical
points of this model under product regularization, and in particular describe
its global minimizer under the minimum-norm principle. From there, we derive
the test risk formula in the overparameterized regime, both for models with and
without skip connections. Our analysis reveals two interesting phenomena:
Firstly, the bottleneck layer introduces an additional complexity measure akin
to the classical bias-variance trade-off -- increasing the bottleneck width
reduces bias but introduces variance, and vice versa. Secondly, skip connection
can mitigate the variance in denoising autoencoders -- especially when the
model is mildly overparameterized. We further analyze the impact of skip
connections in denoising autoencoder using random matrix theory and support our
claims with numerical evidence.

</details>


### [188] [K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes](https://arxiv.org/abs/2505.24704)
*Hideaki Kim, Tomoharu Iwata, Akinori Fujino*

**主要类别:** stat.ML

**AI概要:** 本文提出了K²IE，这是一种结合核方法和经典KIE优势的新强度估计方法，具有良好的预测性能和更高的计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 尽管核方法和经典KIE都用于估计非齐次泊松过程的强度函数，但它们基于不同的理论原理，本文旨在建立两者之间的联系并提升计算效率。

**方法:** 提出了一种基于最小二乘损失的泊松过程正则化核方法，利用RKHS理论开发出更高效的KIE，称为K²IE。

**结果:** 实验表明，K²IE在预测性能上具有可比性，并且在计算效率上显著超越现有方法。

**结论:** K²IE提供了对经典KIE和基于核方法的强度估计量之间联系的新理论洞察，并且在计算效率方面优于最先进的基于核方法的估计器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是K%24%5E2%24IE%3A+Kernel+Method-based+Kernel+Intensity+Estimators+for+Inhomogeneous+Poisson+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24704&send_immediately=true&force_search=false)

**原文摘要:** Kernel method-based intensity estimators, formulated within reproducing
kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs)
have been among the most easy-to-implement and feasible methods for estimating
the intensity functions of inhomogeneous Poisson processes. While both
approaches share the term "kernel", they are founded on distinct theoretical
principles, each with its own strengths and limitations. In this paper, we
propose a novel regularized kernel method for Poisson processes based on the
least squares loss and show that the resulting intensity estimator involves a
specialized variant of the representer theorem: it has the dual coefficient of
unity and coincides with classical KIEs. This result provides new theoretical
insights into the connection between classical KIEs and kernel method-based
intensity estimators, while enabling us to develop an efficient KIE by
leveraging advanced techniques from RKHS theory. We refer to the proposed model
as the kernel method-based kernel intensity estimator (K$^2$IE). Through
experiments on synthetic datasets, we show that K$^2$IE achieves comparable
predictive performance while significantly surpassing the state-of-the-art
kernel method-based estimator in computational efficiency.

</details>


### [189] [Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery](https://arxiv.org/abs/2505.24727)
*Xiaochen Zhang, Haoyi Xiong*

**主要类别:** stat.ML

**AI概要:** 本文提出了\TheName{}，一个结合误发现率控制的新型压缩感知框架，在理论上和实践中均优于传统方法，尤其适用于复杂场景下的信号恢复。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法如LASSO在支持选择和信号估计中没有明确控制错误率，导致在复杂场景下难以可靠地识别真实信号支持。本文旨在通过引入FDR控制来解决这一问题，以提升信号恢复的稳定性和准确性。

**方法:** 引入一种称为\TheName{}的Knockoff引导压缩感知框架，将支持选择与信号估计分离，并利用统计Knockoff滤波器进行FDR控制，从而实现更精确的信号重建。

**结果:** 实验表明，新方法在模拟研究中比基线方法的F1分数提高了高达3.9倍，同时保持较低的重构误差和相对误差。在实际数据集上的表现也优于现有方法，甚至接近或超过未压缩信号的表现。

**结论:** 论文提出了一种新的基于Knockoff的压缩感知框架，能够通过精确控制误发现率（FDR）在支持识别阶段提高信号恢复的可靠性与准确性。该方法在理论保证和实证性能上都优于传统的LASSO和其他压缩感知技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knockoff-Guided+Compressive+Sensing%3A+A+Statistical+Machine+Learning+Framework+for+Support-Assured+Signal+Recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24727，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24727&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a novel Knockoff-guided compressive sensing framework,
referred to as \TheName{}, which enhances signal recovery by leveraging precise
false discovery rate (FDR) control during the support identification phase.
Unlike LASSO, which jointly performs support selection and signal estimation
without explicit error control, our method guarantees FDR control in finite
samples, enabling more reliable identification of the true signal support. By
separating and controlling the support recovery process through statistical
Knockoff filters, our framework achieves more accurate signal reconstruction,
especially in challenging scenarios where traditional methods fail. We
establish theoretical guarantees demonstrating how FDR control directly ensures
recovery performance under weaker conditions than traditional $\ell_1$-based
compressive sensing methods, while maintaining accurate signal reconstruction.
Extensive numerical experiments demonstrate that our proposed Knockoff-based
method consistently outperforms LASSO-based and other state-of-the-art
compressive sensing techniques. In simulation studies, our method improves
F1-score by up to 3.9x over baseline methods, attributed to principled false
discovery rate (FDR) control and enhanced support recovery. The method also
consistently yields lower reconstruction and relative errors. We further
validate the framework on real-world datasets, where it achieves top downstream
predictive performance across both regression and classification tasks, often
narrowing or even surpassing the performance gap relative to uncompressed
signals. These results establish \TheName{} as a robust and practical
alternative to existing approaches, offering both theoretical guarantees and
strong empirical performance through statistically grounded support selection.

</details>


### [190] [Generalization Dynamics of Linear Diffusion Models](https://arxiv.org/abs/2505.24769)
*Claudia Merger, Sebastian Goldt*

**主要类别:** stat.ML

**AI概要:** 该论文研究了扩散模型在有限数据集上从记忆到泛化的过渡，发现当样本数量N接近输入维度d时会发生这一过渡，并探讨了不同情况下如何优化模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 理解扩散模型从记忆训练数据到泛化生成新样本的转变对于评估生成模型的效率和可靠性至关重要，但目前对此过渡的理论理解尚不完整。

**方法:** 使用线性去噪器对扩散模型进行分析，通过显式计算测试误差、采样分布以及样本与目标分布之间的Kullback-Leibler散度来预测记忆到泛化的过渡。

**结果:** 论文预测记忆到泛化过渡发生在样本数N大约等于输入维度d的时候。当N小于d时，训练数据中仅包含部分相关变化方向，正则化和提前停止可以有效防止过拟合；而当N大于d时，采样分布接近最优状态，且这种状态与数据分布的具体形式无关。

**结论:** 论文通过研究线性去噪扩散模型，发现当样本数N小于输入维度d时，正则化和提前停止有助于防止过拟合；而当N大于d时，采样分布接近最优状态，并且这种状态与数据分布的具体形式无关。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+Dynamics+of+Linear+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24769，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24769&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models trained on finite datasets with $N$ samples from a target
distribution exhibit a transition from memorisation, where the model reproduces
training examples, to generalisation, where it produces novel samples that
reflect the underlying data distribution. Understanding this transition is key
to characterising the sample efficiency and reliability of generative models,
but our theoretical understanding of this transition is incomplete. Here, we
analytically study the memorisation-to-generalisation transition in a simple
model using linear denoisers, which allow explicit computation of test errors,
sampling distributions, and Kullback-Leibler divergences between samples and
target distribution. Using these measures, we predict that this transition
occurs roughly when $N \asymp d$, the dimension of the inputs. When $N$ is
smaller than the dimension of the inputs $d$, so that only a fraction of
relevant directions of variation are present in the training data, we
demonstrate how both regularization and early stopping help to prevent
overfitting. For $N > d$, we find that the sampling distributions of linear
diffusion models approach their optimum (measured by the Kullback-Leibler
divergence) linearly with $d/N$, independent of the specifics of the data
distribution. Our work clarifies how sample complexity governs generalisation
in a simple model of diffusion-based generative models and provides insight
into the training dynamics of linear denoisers.

</details>


### [191] [Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV](https://arxiv.org/abs/2505.24781)
*Karim Abou-Moustafa*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的高效方法来估计正则化 Tyler M-估计器的收缩系数，并证明了其在计算效率和准确性上的优势。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高正则化参数估计的准确性和计算效率。

**方法:** 通过设置一个合适的目 标函数，即留一交叉验证（LOOCV）对数似然损失，来估计最优收缩系数。然后提出了一种计算高效的近似方法来减少运行时间复杂度。

**结果:** 该方法在合成高维数据和真实高维数据集上均表现出高效性和准确性。

**结论:** 所提出的方法在计算效率和准确性上都优于文献中的其他方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Estimation+of+Regularized+Tyler%27s+M-Estimator+Using+Approximate+LOOCV，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24781&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of estimating a regularization parameter, or a
shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator
(RTME). In particular, we propose to estimate an optimal shrinkage coefficient
by setting $\alpha$ as the solution to a suitably chosen objective function;
namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since
LOOCV is computationally prohibitive even for moderate sample size $n$, we
propose a computationally efficient approximation for the LOOCV log-likelihood
loss that eliminates the need for invoking the RTME procedure $n$ times for
each sample left out during the LOOCV procedure. This approximation yields an
$O(n)$ reduction in the running time complexity for the LOOCV procedure, which
results in a significant speedup for computing the LOOCV estimate. We
demonstrate the efficiency and accuracy of the proposed approach on synthetic
high-dimensional data sampled from heavy-tailed elliptical distributions, as
well as on real high-dimensional datasets for object recognition, face
recognition, and handwritten digit's recognition. Our experiments show that the
proposed approach is efficient and consistently more accurate than other
methods in the literature for shrinkage coefficient estimation.

</details>


### [192] [Statistical mechanics of extensive-width Bayesian neural networks near interpolation](https://arxiv.org/abs/2505.24849)
*Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk*

**主要类别:** stat.ML

**AI概要:** 论文通过统计物理方法分析两层全连接网络的学习过程，揭示了特征学习的动态变化及其对数据量的依赖性。


<details>
  <summary>更多</summary>
  
**动机:** 缩小实际应用中的复杂神经网络与理论研究之间的差距，是本论文的主要动机。通过分析更接近实际应用的网络结构，提升理论理解。

**方法:** 论文采用统计物理学方法，分析了具有通用权重分布和激活函数的两层全连接网络，并聚焦贝叶斯最优学习场景。

**结果:** 发现了丰富的现象学，包括随着数据量增加出现的各种学习转换。特征学习在插值区域附近更为明显，而稀疏数据导致模型依赖非线性组合而非权重对齐。

**结论:** 通过统计物理学分析，论文揭示了在监督学习中，两层全连接网络的各种学习转换现象。当数据稀缺时，模型倾向于学习教师权重的非线性组合，而不是直接对齐权重。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Statistical+mechanics+of+extensive-width+Bayesian+neural+networks+near+interpolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.24849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.24849&send_immediately=true&force_search=false)

**原文摘要:** For three decades statistical mechanics has been providing a framework to
analyse neural networks. However, the theoretically tractable models, e.g.,
perceptrons, random features models and kernel machines, or multi-index models
and committee machines with few neurons, remained simple compared to those used
in applications. In this paper we help reducing the gap between practical
networks and their theoretical understanding through a statistical physics
analysis of the supervised learning of a two-layer fully connected network with
generic weight distribution and activation function, whose hidden layer is
large but remains proportional to the inputs dimension. This makes it more
realistic than infinitely wide networks where no feature learning occurs, but
also more expressive than narrow ones or with fixed inner weights. We focus on
the Bayes-optimal learning in the teacher-student scenario, i.e., with a
dataset generated by another network with the same architecture. We operate
around interpolation, where the number of trainable parameters and of data are
comparable and feature learning emerges. Our analysis uncovers a rich
phenomenology with various learning transitions as the number of data
increases. In particular, the more strongly the features (i.e., hidden neurons
of the target) contribute to the observed responses, the less data is needed to
learn them. Moreover, when the data is scarce, the model only learns non-linear
combinations of the teacher weights, rather than "specialising" by aligning its
weights with the teacher's. Specialisation occurs only when enough data becomes
available, but it can be hard to find for practical training algorithms,
possibly due to statistical-to-computational~gaps.

</details>
