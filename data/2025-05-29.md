<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 139]
- [cs.AI](#cs.AI) [总数: 27]
- [stat.ML](#stat.ML) [总数: 17]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learning Where to Learn: Training Distribution Selection for Provable OOD Performance](https://arxiv.org/abs/2505.21626)
*Nicolas Guerra, Nicholas H. Nelsen, Yunan Yang*

**主要类别:** cs.LG

**概要:** 这篇论文研究了通过设计训练数据分布以最大化OOD（Out-of-distribution）泛化性能的方法。提出两种互补算法策略，并在多个实验中验证其有效性，结果表明相比标准经验风险最小化方法有显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习模型在面对分布外（OOD）数据时通常会出现性能下降的问题，因此需要探索能够增强模型在不同目标数据分布上的鲁棒性的训练方法。

**方法:** 首先进行理论分析，推导出一系列泛化界，用以量化训练数据分布选择对OOD误差的影响；然后提出两种互补算法策略：(i) 将OOD风险最小化表示为概率测度空间上的双层优化问题；(ii) 最小化OOD误差的理论上界。

**结果:** 所提出的两种方法在函数逼近和算子学习等多个实验中显著提高了OOD准确性，优于固定分布的标准经验风险最小化方法。

**结论:** 分布感知训练是一种有原则且实用的框架，可以实现更鲁棒的OOD泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Where+to+Learn%3A+Training+Distribution+Selection+for+Provable+OOD+Performance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21626&send_immediately=true&force_search=false)

**原文摘要:** Out-of-distribution (OOD) generalization remains a fundamental challenge in
machine learning. Models trained on one data distribution often experience
substantial performance degradation when evaluated on shifted or unseen
domains. To address this challenge, the present paper studies the design of
training data distributions that maximize average-case OOD performance. First,
a theoretical analysis establishes a family of generalization bounds that
quantify how the choice of training distribution influences OOD error across a
predefined family of target distributions. These insights motivate the
introduction of two complementary algorithmic strategies: (i) directly
formulating OOD risk minimization as a bilevel optimization problem over the
space of probability measures and (ii) minimizing a theoretical upper bound on
OOD error. Last, the paper evaluates the two approaches across a range of
function approximation and operator learning examples. The proposed methods
significantly improve OOD accuracy over standard empirical risk minimization
with a fixed distribution. These results highlight the potential of
distribution-aware training as a principled and practical framework for robust
OOD generalization.

</details>


### [2] [Efficient Diffusion Models for Symmetric Manifolds](https://arxiv.org/abs/2505.21640)
*Oren Mangoubi, Neil He, Nisheeth K. Vishnoi*

**主要类别:** cs.LG

**概要:** 本文提出了一种设计高效扩散模型的框架，适用于d维对称空间黎曼流形（如环面、球面、特殊正交群和酉群）。通过引入具有空间变化协方差的新扩散模型，利用欧几里得布朗运动投影绕过热核计算，训练算法在每次迭代中只需O(1)梯度评估和几乎线性于d的操作。实验表明，该模型在训练速度上优于先前方法，并提高了样本质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的流形扩散模型通常依赖于热核，这些模型要么需要d次梯度评估，要么需要指数级的算术运算，导致效率低下。因此，有必要开发一种更高效的扩散模型来解决这一问题。

**方法:** 作者引入了一种新的扩散模型，该模型具有空间变化的协方差，能够通过欧几里得布朗运动的投影避免热核计算。同时，利用Ito引理推导出一个新颖的有效目标函数，从而将训练步骤中的梯度评估次数减少到O(1)，并将算术操作降低到几乎线性于d的程度。此外，流形的对称性保证了扩散过程满足“平均情况”Lipschitz条件，从而实现准确和高效的样本生成。

**结果:** 实验结果表明，该模型在训练速度上显著优于先前的方法，并且在环面、特殊正交群和酉群上的合成数据集上提升了样本质量。

**结论:** 本文提出的扩散模型框架为d维对称空间黎曼流形提供了一种高效的设计方法，显著减少了计算复杂度，并在实际应用中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Diffusion+Models+for+Symmetric+Manifolds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21640&send_immediately=true&force_search=false)

**原文摘要:** We introduce a framework for designing efficient diffusion models for
$d$-dimensional symmetric-space Riemannian manifolds, including the torus,
sphere, special orthogonal group and unitary group. Existing manifold diffusion
models often depend on heat kernels, which lack closed-form expressions and
require either $d$ gradient evaluations or exponential-in-$d$ arithmetic
operations per training step. We introduce a new diffusion model for symmetric
manifolds with a spatially-varying covariance, allowing us to leverage a
projection of Euclidean Brownian motion to bypass heat kernel computations. Our
training algorithm minimizes a novel efficient objective derived via Ito's
Lemma, allowing each step to run in $O(1)$ gradient evaluations and
nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap
between diffusions on symmetric manifolds and Euclidean space. Manifold
symmetries ensure the diffusion satisfies an "average-case" Lipschitz
condition, enabling accurate and efficient sample generation. Empirically, our
model outperforms prior methods in training speed and improves sample quality
on synthetic datasets on the torus, special orthogonal group, and unitary
group.

</details>


### [3] [AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent](https://arxiv.org/abs/2505.21651)
*Nikola Surjanovic, Alexandre Bouchard-Côté, Trevor Campbell*

**主要类别:** cs.LG

**概要:** 本论文提出了AutoSGD，一种能自动调整学习率的SGD方法，理论和实证结果均表明其在优化问题和机器学习任务中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 选择合适的学习率调度器通常需要大量的用户调整工作，因此作者希望提出一种能够自动确定何时增加或减少学习率的方法。

**方法:** 引入了AutoSGD，这种方法可以自动判断在给定迭代中是否应该增加或减少学习率，并采取相应措施。还介绍了支持AutoSGD收敛性的理论及其确定性对应的标准梯度下降方法。

**结果:** 实验结果表明，该方法在各种传统优化问题和机器学习任务中表现出色。

**结论:** AutoSGD能够在不需要大量用户调整的情况下，有效改善SGD的性能，适用于多种优化和机器学习场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoSGD%3A+Automatic+Learning+Rate+Selection+for+Stochastic+Gradient+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21651&send_immediately=true&force_search=false)

**原文摘要:** The learning rate is an important tuning parameter for stochastic gradient
descent (SGD) and can greatly influence its performance. However, appropriate
selection of a learning rate schedule across all iterations typically requires
a non-trivial amount of user tuning effort. To address this, we introduce
AutoSGD: an SGD method that automatically determines whether to increase or
decrease the learning rate at a given iteration and then takes appropriate
action. We introduce theory supporting the convergence of AutoSGD, along with
its deterministic counterpart for standard gradient descent. Empirical results
suggest strong performance of the method on a variety of traditional
optimization problems and machine learning tasks.

</details>


### [4] [Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape](https://arxiv.org/abs/2505.21722)
*Ioannis Bantzis, James B. Simon, Arthur Jacot*

**主要类别:** cs.LG

**概要:** 在深度ReLU网络中，当权重初始化较小时，GD最初主要受参数空间原点鞍点的影响。我们研究了所谓的逃逸方向，这些方向在更深的层中具有低秩偏差，这为证明深度ReLU网络中的鞍点到鞍点动力学提供了第一步。


<details>
  <summary>更多</summary>
  
**动机:** 研究深度ReLU网络中梯度下降（GD）的行为，特别是在权重初始化较小时，GD如何受参数空间原点鞍点的影响，并探索逃逸方向的作用。

**方法:** 分析深度ReLU网络的逃逸方向，并证明最优逃逸方向在更深的层中具有低秩偏差：第ℓ层权重矩阵的第一个奇异值至少比其他奇异值大ℓ^(1/4)。还证明了与这些逃逸方向相关的其他结果。

**结果:** 发现了深度ReLU网络中逃逸方向的低秩偏差特性，并证明了这一特性对于理解鞍点到鞍点的动力学行为的重要性。

**结论:** 这项研究为证明深度ReLU网络中的鞍点到鞍点动力学提供了一个初步的理论基础，其中GD会访问一系列具有递增瓶颈秩的鞍点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Saddle-To-Saddle+Dynamics+in+Deep+ReLU+Networks%3A+Low-Rank+Bias+in+the+First+Saddle+Escape，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21722，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21722&send_immediately=true&force_search=false)

**原文摘要:** When a deep ReLU network is initialized with small weights, GD is at first
dominated by the saddle at the origin in parameter space. We study the
so-called escape directions, which play a similar role as the eigenvectors of
the Hessian for strict saddles. We show that the optimal escape direction
features a low-rank bias in its deeper layers: the first singular value of the
$\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any
other singular value. We also prove a number of related results about these
escape directions. We argue that this result is a first step in proving
Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of
saddles with increasing bottleneck rank.

</details>


### [5] [Faster Rates for Private Adversarial Bandits](https://arxiv.org/abs/2505.21790)
*Hilal Asi, Vinod Raman, Kunal Talwar*

**主要类别:** cs.LG

**概要:** 本论文设计了新的差分隐私算法，用于对抗性Bandits问题和专家建议的Bandits问题。对于对抗性Bandits，提出了一种将非隐私Bandit算法转换为隐私Bandit算法的方法，改进了现有的遗憾上界。对于专家建议的Bandits问题，给出了首个差分隐私算法，并分析了不同$K$、$N$和$\epsilon$组合下的次线性遗憾。


<details>
  <summary>更多</summary>
  
**动机:** 对抗性Bandits和专家建议的Bandits问题在隐私保护方面存在挑战，需要开发既能保证隐私又能最小化遗憾的算法。

**方法:** 1. 对于对抗性Bandits，提出了一种将非隐私Bandit算法转换为隐私Bandit算法的简单高效方法。
2. 对于专家建议的Bandits问题，设计了首个差分隐私算法，并提供了多种遗憾上界表达式。

**结果:** - 改进了对抗性Bandits的遗憾上界，从$O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$提升到$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$。
- 在专家建议的Bandits问题中，提供了三种不同的遗憾上界，适用于不同的$K$、$N$和$\epsilon$组合。

**结论:** 本文提出的差分隐私算法在对抗性Bandits和专家建议的Bandits问题中均实现了改进的遗憾上界，展示了隐私保护与性能优化之间的平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Faster+Rates+for+Private+Adversarial+Bandits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21790&send_immediately=true&force_search=false)

**原文摘要:** We design new differentially private algorithms for the problems of
adversarial bandits and bandits with expert advice. For adversarial bandits, we
give a simple and efficient conversion of any non-private bandit algorithm to a
private bandit algorithm. Instantiating our conversion with existing
non-private bandit algorithms gives a regret upper bound of
$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing
upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all
$\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected
regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first
known separation between central and local differential privacy for this
problem. For bandits with expert advice, we give the first differentially
private algorithms, with expected regret
$O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right),
O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and
$\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} +
\frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of
actions and experts respectively. These rates allow us to get sublinear regret
for different combinations of small and large $K, N$ and $\epsilon.$

</details>


### [6] [Optimizing Data Augmentation through Bayesian Model Selection](https://arxiv.org/abs/2505.21813)
*Madi Matymov, Ba-Hien Tran, Michael Kampffmeyer, Markus Heinonen, Maurizio Filippone*

**主要类别:** cs.LG

**概要:** Data Augmentation (DA) is crucial for machine learning models. Traditionally, choosing DA parameters has been challenging. This paper proposes a novel framework that uses Bayesian principles to optimize DA parameters by interpreting them as model hyper-parameters. The approach involves deriving a tractable Evidence Lower BOund (ELBO) for joint optimization with model parameters. Experiments show improved calibration and robust performance in computer vision tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of this paper is to address the challenge of carefully choosing data augmentation parameters, which is traditionally done through trial-and-error or expensive optimization based on validation performance.

**方法:** The method involves taking a probabilistic view of data augmentation, interpreting augmentation parameters as model (hyper)-parameters, and formulating their optimization as a Bayesian model selection problem. A tractable Evidence Lower BOund (ELBO) is derived to allow joint optimization with model parameters.

**结果:** The results indicate that the proposed approach improves calibration and yields robust performance over fixed or no augmentation in computer vision tasks.

**结论:** This work provides a rigorous foundation for optimizing data augmentation using Bayesian principles, offering significant potential for robust machine learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Data+Augmentation+through+Bayesian+Model+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21813&send_immediately=true&force_search=false)

**原文摘要:** Data Augmentation (DA) has become an essential tool to improve robustness and
generalization of modern machine learning. However, when deciding on DA
strategies it is critical to choose parameters carefully, and this can be a
daunting task which is traditionally left to trial-and-error or expensive
optimization based on validation performance. In this paper, we counter these
limitations by proposing a novel framework for optimizing DA. In particular, we
take a probabilistic view of DA, which leads to the interpretation of
augmentation parameters as model (hyper)-parameters, and the optimization of
the marginal likelihood with respect to these parameters as a Bayesian model
selection problem. Due to its intractability, we derive a tractable Evidence
Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly
with model parameters. We provide extensive theoretical results on variational
approximation quality, generalization guarantees, invariance properties, and
connections to empirical Bayes. Through experiments on computer vision tasks,
we show that our approach improves calibration and yields robust performance
over fixed or no augmentation. Our work provides a rigorous foundation for
optimizing DA through Bayesian principles with significant potential for robust
machine learning.

</details>


### [7] [The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows](https://arxiv.org/abs/2505.21512)
*Harry Li, Gabriel Appleby, Kenneth Alperin, Steven R Gomez, Ashley Suh*

**主要类别:** cs.LG

**概要:** 知识图谱(KGs)虽强大，但即使是专家用户也难以有效探索。大型语言模型(LLMs)被用于填补这一空白，但其在KGs中的使用如何影响用户信任、探索策略和决策制定尚缺乏实证研究。本文开发了LinkQ系统，通过五个可视化机制帮助用户评估KG查询和LLM响应的准确性。研究表明，用户即使是有经验的专家，也容易过度信任LinkQ的输出，尽管LLM可能出错。不同的用户表现出不同的工作流程，挑战了这些系统适用于所有用户的假设。研究强调了对LLM辅助数据分析工具中虚假信任的风险以及可视化作为缓解技术的作用进行进一步研究的必要性。


<details>
  <summary>更多</summary>
  
**动机:** 知识图谱虽然强大，但用户（包括专家）仍然难以有效探索。随着大语言模型在解决此问题上的应用增加，了解它们如何影响用户信任、探索策略和决策制定变得至关重要。这为设计更有效的基于LLM的知识图谱分析系统提供了关键挑战和机会。

**方法:** 开发了一个名为LinkQ的知识图谱探索系统，该系统利用大语言模型将自然语言问题转化为结构化查询。同时设计了五个视觉机制以帮助用户评估知识图谱查询和大语言模型响应的准确性：1) LLM-KG状态图；2) 查询编辑器；3) 实体-关系ID表；4) 查询结构图；5) 交互式查询结果图。

**结果:** 通过对14名从业者的定性评估发现，用户（包括知识图谱专家）倾向于过度信任LinkQ的输出，即使大语言模型的回答是错误的。此外，用户的工作流程因其对知识图谱和大语言模型的先前熟悉程度不同而有所差异，挑战了一刀切的设计假设。

**结论:** 本研究揭示了在大语言模型辅助的数据分析工具中存在虚假信任的风险，并强调了需要进一步研究可视化作为缓解技术的角色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Role+of+Visualization+in+LLM-Assisted+Knowledge+Graph+Systems%3A+Effects+on+User+Trust%2C+Exploration%2C+and+Workflows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21512，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21512&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graphs (KGs) are powerful data structures, but exploring them
effectively remains difficult for even expert users. Large language models
(LLMs) are increasingly used to address this gap, yet little is known
empirically about how their usage with KGs shapes user trust, exploration
strategies, or downstream decision-making - raising key design challenges for
LLM-based KG visual analysis systems. To study these effects, we developed
LinkQ, a KG exploration system that converts natural language questions into
structured queries with an LLM. We collaborated with KG experts to design five
visual mechanisms that help users assess the accuracy of both KG queries and
LLM responses: an LLM-KG state diagram that illustrates which stage of the
exploration pipeline LinkQ is in, a query editor displaying the generated query
paired with an LLM explanation, an entity-relation ID table showing extracted
KG entities and relations with semantic descriptions, a query structure graph
that depicts the path traversed in the KG, and an interactive graph
visualization of query results. From a qualitative evaluation with 14
practitioners, we found that users - even KG experts - tended to overtrust
LinkQ's outputs due to its "helpful" visualizations, even when the LLM was
incorrect. Users exhibited distinct workflows depending on their prior
familiarity with KGs and LLMs, challenging the assumption that these systems
are one-size-fits-all - despite often being designed as if they are. Our
findings highlight the risks of false trust in LLM-assisted data analysis tools
and the need for further investigation into the role of visualization as a
mitigation technique.

</details>


### [8] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/abs/2505.21525)
*Peiliang Gong, Yucheng Wang, Min Wu, Zhenghua Chen, Xiaoli Li, Daoqiang Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种新的SFDA方法TERSE，专门针对多变量时间序列数据，通过时空特征编码器和相关任务来捕捉和迁移时空依赖性，从而在不使用源数据的情况下实现领域适应。实验表明该方法有效且通用。


<details>
  <summary>更多</summary>
  
**动机:** 现有的SFDA方法虽然减少了对源数据的依赖，但在处理多变量时间序列（MTS）数据时表现不佳，因为它们未能考虑MTS数据中的内在空间相关性。这些相关性对于准确表示MTS数据并在不同领域间保持不变信息至关重要。

**方法:** TERSE包含一个定制的时空特征编码器，用于捕获底层的时空特性，并结合时间恢复和空间重连任务，以重建被时间屏蔽的时间序列的潜在表示和被空间屏蔽的相关结构。在目标适配阶段，通过利用源预训练的时间恢复和空间重连网络，引导目标编码器生成与源域在空间和时间上一致的特征。

**结果:** 广泛的真实世界时间序列数据集上的实验验证了TERSE的有效性和通用性。

**结论:** TERSE可以有效地建模并跨领域转移时空依赖关系，促进隐式特征对齐，并且可以作为一个多功能的即插即用模块集成到已建立的SFDA方法中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal+Restoration+and+Spatial+Rewiring+for+Source-Free+Multivariate+Time+Series+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21525，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21525&send_immediately=true&force_search=false)

**原文摘要:** Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [9] [Revisiting Bayesian Model Averaging in the Era of Foundation Models](https://arxiv.org/abs/2505.21857)
*Mijung Park*

**主要类别:** cs.LG

**概要:** 本研究重新审视了贝叶斯模型平均（BMA）范式，通过集成预训练和轻量微调的基础模型来提高图像和文本数据的分类性能。为了使BMA在基础模型下可处理，引入了可训练的线性分类器，以从预训练基础模型中提取冻结特征作为输入。此外，还提出了一种计算成本更低、可优化的模型平均方案（OMA），直接优化模型集成权重以减少预测中的不确定性。这些方法将有助于未来更强大的基础模型的整合，从而提升具有挑战性的分类任务的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度学习领域中基础模型的快速发展为各种任务提供了强大的性能提升潜力。然而，如何有效地集成多个预训练或微调的基础模型以进一步提高分类性能仍然是一个重要的研究问题。传统的贝叶斯模型平均（BMA）方法虽然理论上有效，但在大规模基础模型上的应用面临计算复杂度和可扩展性的问题。因此，需要一种新的方法来解决这些问题，并充分利用基础模型的能力。

**方法:** 研究采用了经典的贝叶斯模型平均（BMA）范式，并对其进行了改进以适应基础模型的特性。具体而言，引入了可训练的线性分类器，利用预训练基础模型提取的冻结特征作为输入。通过模型后验概率确定更适合特定数据集的线性头和冻结特征，从而实现有原则的模型集成。此外，还提出了优化模型平均（OMA）方案，该方案通过直接优化模型集成权重来减少预测中的不确定性（即期望熵）。与BMA相比，OMA计算成本更低且更具可扩展性。

**结果:** 实验结果表明，所提出的BMA和OMA方法能够显著提高图像和文本数据的分类性能。相比于单一模型或简单集成方法，这两种方法在多个基准数据集上表现出更高的准确性和鲁棒性。特别是OMA方案在保持性能的同时降低了计算成本，证明了其在实际应用中的优越性。

**结论:** 本研究成功地将贝叶斯模型平均（BMA）范式应用于基础模型的集成，并提出了一种更高效的优化模型平均（OMA）方案。这些方法不仅提高了图像和文本分类任务的性能，而且为未来更强大的基础模型的整合提供了理论和技术支持。随着基础模型的不断发展，这些方法有望在更多领域中发挥重要作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Bayesian+Model+Averaging+in+the+Era+of+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21857&send_immediately=true&force_search=false)

**原文摘要:** We revisit the classical, full-fledged Bayesian model averaging (BMA)
paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to
enhance the classification performance on image and text data. To make BMA
tractable under foundation models, we introduce trainable linear classifiers
that take frozen features from the pre-trained foundation models as inputs. The
model posteriors over the linear classifiers tell us which linear heads and
frozen features are better suited for a given dataset, resulting in a
principled model ensembling method. Furthermore, we propose a computationally
cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize
the model ensemble weights, just like those weights based on model posterior
distributions in BMA, by reducing the amount of surprise (expected entropy of
the predictions) we get from predictions of ensembled models. With the rapid
development of foundation models, these approaches will enable the
incorporation of future, possibly significantly better foundation models to
enhance the performance of challenging classification tasks.

</details>


### [10] [SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation](https://arxiv.org/abs/2505.21514)
*Mingchao Jiang, Abhinav Jain, Sophia Zorek, Chris Jermaine*

**主要类别:** cs.LG

**概要:** SIMCOPILOT 是一个模拟大型语言模型（LLMs）作为交互式编码助手角色的基准。它提供了评估 LLM 编码能力的全面框架，涵盖 Java 和 Python 的子基准，并在不同领域中进行评估。研究强调了 LLM 在实际编码场景中的实用性和局限性，以及从语法生成器向智能开发伙伴的转变。


<details>
  <summary>更多</summary>
  
**动机:** 当前缺乏对 LLMs 作为编码助手进行全面评估的工具，尤其是在代码补全和填充任务方面。此外，现有基准经常忽视一些关键因素，例如特定任务性能、跨代码段的上下文理解和变量范围敏感性。因此，需要一个更详细和现实的评估环境来衡量 LLMs 在实际编码场景中的表现。

**方法:** 1. 开发 SIMCOPILOT 基准，包括 Java (SIMCOPILOTJ) 和 Python (SIMCOPILOTP) 的子基准。
2. 设计针对代码补全和填充任务的多样化代码库，覆盖不同的大小和复杂度。
3. 创建一个详细的评估环境，涵盖算法、数据库、计算机视觉和神经网络等多个领域。
4. 提供细粒度分析，考虑任务特定性能、跨代码段上下文理解及变量范围敏感性等因素。

**结果:** 通过跨领域的评估，SIMCOPILOT 显示出 LLMs 在编码方面的优势，但也揭示了它们在保持复杂依赖结构内逻辑一致性方面的持续挑战。此外，研究表明 LLMs 正从单纯的语法生成器向可靠的智能软件开发伙伴转变。

**结论:** SIMCOPILOT 成功提供了一个全面且细致的评估框架，用于衡量 LLMs 在实际编码场景中的表现。该研究不仅突出了 LLMs 在编码方面的潜力和局限性，还表明未来需要进一步改进以实现更可靠的智能开发协作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SIMCOPILOT%3A+Evaluating+Large+Language+Models+for+Copilot-Style+Code+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21514，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21514&send_immediately=true&force_search=false)

**原文摘要:** We introduce SIMCOPILOT, a benchmark that simulates the role of large
language models (LLMs) as interactive, "copilot"-style coding assistants.
Targeting both completion (finishing incomplete methods or code blocks) and
infill tasks (filling missing segments within existing code), SIMCOPILOT
provides a comprehensive framework for evaluating LLM coding capabilities. The
benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python
(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our
key contributions include: (a) establishing a realistic, detailed evaluation
environment to assess LLM utility in practical coding scenarios, and (b)
providing fine-grained analyses that address critical factors frequently
overlooked by existing benchmarks, such as task-specific performance nuances,
contextual understanding across code segments, and sensitivity to variable
scope. Evaluations conducted across domains-including algorithms, databases,
computer vision, and neural networks-offer insights into model strengths and
highlight persistent challenges in maintaining logical consistency within
complex dependency structures. Beyond benchmarking, our study sheds light on
the current limitations of LLM-driven code generation and underscores the
ongoing transition of LLMs from merely syntax-aware generators toward reliable,
intelligent software development partners.

</details>


### [11] [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
*Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing Liang, Yuan Qi*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为ChemHAS的方法，通过优化代理堆叠结构来减少化学工具的预测误差，并在四个基本化学任务中取得了最先进的性能。此外，还识别了四种不同的代理堆叠行为，可能提高可解释性并揭示AI代理在科学研究中的新可能性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLM）代理已经在化学相关任务中展示了通过选择合适工具提高性能的能力，但这些工具的固有预测误差限制了其有效性。因此，研究如何利用LLM代理来减少这些工具的预测误差成为了一个重要问题。

**方法:** 提出了ChemHAS（化学分层代理堆叠），这是一种通过从有限数据中优化代理堆叠结构来增强化学工具的方法。该方法在四个基础化学任务中表现出色，有效补偿了工具的预测误差。同时，还识别和描述了四种不同的代理堆叠行为。

**结果:** ChemHAS在四个基础化学任务中实现了最先进的性能，证明了该方法可以有效减少化学工具的预测误差。并且，识别出的四种代理堆叠行为可能提高了模型的可解释性。

**结论:** ChemHAS是一种简单而有效的方法，能够通过优化代理堆叠结构来减少化学工具的预测误差。这种方法不仅提升了化学任务的性能，还为AI代理在科学研究中的应用提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ChemHAS%3A+Hierarchical+Agent+Stacking+for+Enhancing+Chemistry+Tools，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21569，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21569&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.

</details>


### [12] [Continual Learning Beyond Experience Rehearsal and Full Model Surrogates](https://arxiv.org/abs/2505.21942)
*Prashant Bhat, Laurens Niesten, Elahe Arani, Bahram Zonooz*

**主要类别:** cs.LG

**概要:** 提出了一种名为SPARC的可扩展持续学习方法，该方法通过结合任务特定的工作记忆和任务无关的语义记忆进行跨任务知识巩固，显著提高了参数效率，并在多个基准上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习解决方案依赖于经验重演或全模型代理，这些方法虽然有效，但引入了大量内存和计算开销，限制了其可扩展性和在实际场景中的应用。

**方法:** SPARC通过结合任务特定的工作记忆和任务无关的语义记忆进行跨任务知识巩固，消除了对经验重演和全模型代理的需求。这种方法仅使用全模型代理所需参数的6%，具有显著的参数效率。此外，分类层中的权重重新归一化减轻了任务特定偏差。

**结果:** SPARC在Seq-TinyImageNet上取得了优于其他方法的表现，并在各种持续学习基准上与基于经验重演的方法相匹配。

**结论:** SPARC是一种实用且可扩展的持续学习解决方案，在严格的效率约束下表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continual+Learning+Beyond+Experience+Rehearsal+and+Full+Model+Surrogates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21942&send_immediately=true&force_search=false)

**原文摘要:** Continual learning (CL) has remained a significant challenge for deep neural
networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal
or full model surrogates to mitigate CF. While effective, these approaches
introduce substantial memory and computational overhead, limiting their
scalability and applicability in real-world scenarios. To address this, we
propose SPARC, a scalable CL approach that eliminates the need for experience
rehearsal and full-model surrogates. By effectively combining task-specific
working memories and task-agnostic semantic memory for cross-task knowledge
consolidation, SPARC results in a remarkable parameter efficiency, using only
6% of the parameters required by full-model surrogates. Despite its lightweight
design, SPARC achieves superior performance on Seq-TinyImageNet and matches
rehearsal-based methods on various CL benchmarks. Additionally, weight
re-normalization in the classification layer mitigates task-specific biases,
establishing SPARC as a practical and scalable solution for CL under stringent
efficiency constraints.

</details>


### [13] [FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2505.21571)
*Yao Lu, Tengfei Ma, Zeyu Wang, Zhuangzhi Chen, Dongwei Xu, Yun Lin, Qi Xuan, Guan Gui*

**主要类别:** cs.LG

**概要:** 提出了一种新的Fine-to-COarse两阶段剪枝框架FCOS，结合通道级剪枝与层崩溃诊断实现极高压缩率和高效推理。在AMR基准上，FCOS相较于原始ResNet56模型减少了95.51%的FLOPs和95.31%的参数量，仅损失0.46%的准确率。


<details>
  <summary>更多</summary>
  
**动机:** 传统手动调制识别方法难以满足现代场景中可靠信号特征提取和实时性需求，而现有的深度学习自动调制识别方法尽管提升了分类精度，但其大模型尺寸和高计算需求限制了在资源受限设备上的部署。现有的模型剪枝技术在压缩率、硬件加速和精度保持之间存在权衡。

**方法:** 引入FCOS框架，包括两个阶段：第一阶段应用分层聚类和参数融合进行通道级剪枝；第二阶段使用Layer Collapse Diagnosis (LaCD)模块通过线性探测识别并移除因高通道压缩比而崩溃的层。

**结果:** 实验表明，FCOS在多个AMR基准上优于现有的通道和层剪枝方法，实现了95.51%的FLOPs减少和95.31%的参数减少，同时性能接近原始ResNet56模型。

**结论:** FCOS框架能够实现极高压缩率、高性能和高效推理，为资源受限设备上的深度学习自动调制识别提供了一种有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FCOS%3A+A+Two-Stage+Recoverable+Model+Pruning+Framework+for+Automatic+Modulation+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21571，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21571&send_immediately=true&force_search=false)

**原文摘要:** With the rapid development of wireless communications and the growing
complexity of digital modulation schemes, traditional manual modulation
recognition methods struggle to extract reliable signal features and meet
real-time requirements in modern scenarios. Recently, deep learning based
Automatic Modulation Recognition (AMR) approaches have greatly improved
classification accuracy. However, their large model sizes and high
computational demands hinder deployment on resource-constrained devices. Model
pruning provides a general approach to reduce model complexity, but existing
weight, channel, and layer pruning techniques each present a trade-off between
compression rate, hardware acceleration, and accuracy preservation. To this
end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning
framework that combines channel-level pruning with layer-level collapse
diagnosis to achieve extreme compression, high performance and efficient
inference. In the first stage of FCOS, hierarchical clustering and parameter
fusion are applied to channel weights to achieve channel-level pruning. Then a
Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer
collapse and removes the collapsed layers due to high channel compression
ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms
existing channel and layer pruning methods. Specifically, FCOS achieves 95.51%
FLOPs reduction and 95.31% parameter reduction while still maintaining
performance close to the original ResNet56, with only a 0.46% drop in accuracy
on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.

</details>


### [14] [Judging LLMs on a Simplex](https://arxiv.org/abs/2505.21972)
*Patrick Vossler, Fan Xia, Yifan Mai, Jean Feng*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）的自由形式输出自动化评估具有挑战性，因为许多不同的答案可能同样有效。本研究通过几何框架分析了使用LLMs作为评判者的理论特性，揭示了在二元评分系统中即使弱评判者也能识别真实排名，但在三个或更多评分级别时排名不可识别。研究提出结合贝叶斯推断来整合不确定性，并通过实证评估表明该方法提高了排名准确性及覆盖范围。


<details>
  <summary>更多</summary>
  
**动机:** 当前使用大型语言模型（LLMs）作为评判者进行自由形式输出评估的做法缺乏对理论特性的深入理解，尤其是在多种有效答案的情况下如何准确评估。

**方法:** 采用几何框架将评判者和候选答案表示为概率单纯形上的点，分析不同评分系统下排名可识别性的条件；利用贝叶斯推断编码假设为先验信息，并对排名估计值和置信区间进行敏感性分析。

**结果:** 理论分析揭示了二元评分系统下的“相变”现象：在温和假设下，即使较弱的评判者也能识别真实排名；而在三元及以上评分系统中，即使数据无限，排名也可能不可识别。实证结果表明，贝叶斯推断提高了排名准确性并显著改善了覆盖范围。

**结论:** 当使用LLMs作为评判者时，需要采取更全面的方法来量化不确定性，包括考虑aleatoric和epistemic两种不确定性类型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Judging+LLMs+on+a+Simplex，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21972&send_immediately=true&force_search=false)

**原文摘要:** Automated evaluation of free-form outputs from large language models (LLMs)
is challenging because many distinct answers can be equally valid. A common
practice is to use LLMs themselves as judges, but the theoretical properties of
this approach are not yet well understood. We show that a geometric framework
that represents both judges and candidates as points on a probability simplex
can provide helpful insight on what is or is not identifiable using LLM judges.
Our theoretical analysis uncovers a "phase transition" in ranking
identifiability: for binary scoring systems, true rankings are identifiable
even with weak judges under mild assumptions, while rankings become
non-identifiable for three or more scoring levels even with infinite data,
absent additional prior knowledge. This non-identifiability highlights how
uncertainty in rankings stems from not only aleatoric uncertainty (i.e.,
inherent stochasticity in the data) but also epistemic uncertainty regarding
which assumptions hold, an aspect that has received limited attention until
now. To integrate both types of uncertainty, we use Bayesian inference to
encode assumptions as priors and conduct sensitivity analysis of ranking
estimates and credible intervals. Empirical evaluations across multiple
benchmarks demonstrate that Bayesian inference yields more accurate rankings
and substantially improves coverage rates. These results underscore the
importance of taking a more holistic approach to uncertainty quantification
when using LLMs as judges.

</details>


### [15] [Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes](https://arxiv.org/abs/2505.21573)
*Han Wan, Rui Zhang, Hao Sun*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架SINO，能够在仅有少量轨迹数据且无需已知PDE项的情况下学习PDE算子。SINO通过频域操作和独特的模块设计，实现了对全局耦合系统的精确模拟，并在多个PDE基准测试中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 经典数值求解器虽然准确，但需要精细的离散化和对控制PDEs的完全了解，在物理未知或需要快速推断时受到限制。数据驱动的神经PDE求解器通过从数据中学习减轻了这些约束，但在数据稀缺情况下表现不佳。而物理感知方法通过引入物理知识减少数据需求，却依赖于已知的PDE项或局部数值方案，限制了其处理未知或全局耦合系统的能力。

**方法:** 提出了Spectral-inspired Neural Operator（SINO），一种新颖的框架，可以从有限的轨迹（最少2-5个）中学习PDE算子，且无需任何已知的PDE项。SINO在频域中运行，并引入Frequency-to-Vector模块来学习类似于导数乘法器的频谱表示。为了建模非线性物理相互作用，设计了一个包含低通滤波器的非线性算子块以防止混叠。最后，引入了算子蒸馏技术，用于高效推理的训练模型蒸馏。

**结果:** SINO在多个PDE基准测试中取得了最先进的结果，展示了强大的离散化不变性和对分布外初始条件的鲁棒泛化能力。

**结论:** SINO是第一个能够在有限数据下准确模拟全局耦合系统（如Navier-Stokes方程）而无需任何显式PDE项的物理感知方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectral-inspired+Neural+Operator+for+Data-efficient+PDE+Simulation+in+Physics-agnostic+Regimes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21573，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21573&send_immediately=true&force_search=false)

**原文摘要:** Partial differential equations (PDEs) govern the spatiotemporal evolution of
various physical systems. Classical numerical solvers, while accurate, require
fine discretization and full knowledge of the governing PDEs, limiting their
applicability when the physics is unknown or fast inference is required.
Data-driven neural PDE solvers alleviate these constraints by learning from
data but demand large training datasets and perform poorly in data-scarce
regimes. Physics-aware methods mitigate data requirements by incorporating
physical knowledge yet rely on known PDE terms or local numerical schemes,
restricting their ability to handle unknown or globally coupled systems. In
this work, we propose the Spectral-inspired Neural Operator (SINO), a novel
framework that learns PDE operators from limited trajectories (as few as 2-5),
without any known PDE terms. SINO operates in the frequency domain and
introduces a Frequency-to-Vector module to learn spectral representations
analogous to derivative multipliers. To model nonlinear physical interactions,
we design a nonlinear operator block that includes a $\Pi$-Block with low-pass
filtering to prevent aliasing. Finally, we introduce an operator distillation
technique to distill the trained model for efficient inference. SINO achieves
state-of-the-art results across multiple PDE benchmarks, demonstrating strong
discretization invariance and robust generalization to out-of-distribution
initial conditions. To our knowledge, SINO is the first physics-aware method
capable of accurately simulating globally coupled systems (e.g., the
Navier-Stokes equations) from limited data without any explicit PDE terms.

</details>


### [16] [Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training](https://arxiv.org/abs/2505.22257)
*Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, Jesus Rios*

**主要类别:** cs.LG

**概要:** 重新审视了在策略和离策略优化环境下的组相对策略优化（GRPO）。受最近关于离策略近端策略优化（PPO）的工作的启发，该工作提高了训练稳定性、采样效率和内存使用。此外，对GRPO的最新分析表明，使用离策略样本估计优势函数可能是有益的。基于这些观察结果，我们将GRPO适应到离策略设置，并展示了在策略和离策略GRPO目标都能提高奖励。这一结果促使我们在离策略版本的GRPO中使用截断替代目标。然后，我们比较了使用两种GRPO变体在后训练中具有可验证奖励的强化学习的实证性能。我们的结果显示，离策略GRPO要么显著优于其在策略对应物，要么与之表现相当。


<details>
  <summary>更多</summary>
  
**动机:** 受到最近关于离策略近端策略优化（PPO）工作的启发，该工作提高了训练稳定性、采样效率和内存使用。此外，对GRPO的最新分析表明，使用离策略样本估计优势函数可能是有益的。

**方法:** 将GRPO适应到离策略设置，展示在策略和离策略GRPO目标，并在离策略版本的GRPO中使用截断替代目标。

**结果:** 结果显示，离策略GRPO要么显著优于其在策略对应物，要么与之表现相当。

**结论:** 离策略GRPO在强化学习任务中的表现优异，可能成为一种有效的替代方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Group+Relative+Policy+Optimization%3A+Insights+into+On-Policy+and+Off-Policy+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22257，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22257&send_immediately=true&force_search=false)

**原文摘要:** We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.

</details>


### [17] [Concentration Distribution Learning from Label Distributions](https://arxiv.org/abs/2505.21576)
*Jiawei Tang, Yuheng Jia*

**主要类别:** cs.LG

**概要:** This paper introduces background concentration into Label Distribution Learning (LDL) to form Concentration Distribution Learning (CDL). The authors propose a model using probabilistic methods and neural networks for improved accuracy over current LDL methods.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the limitation of label distribution in representing an instance by considering both relative label description degree and absolute intensity of each label, which can lead to information loss and confusion in instances.

**方法:** The author proposes background concentration as an absolute description degree term of label distribution. They use probabilistic methods and neural networks to learn label distributions and background concentrations from existing LDL datasets.

**结果:** Experiments show that the proposed approach can extract background concentrations from label distributions and produce more accurate predictions compared to existing LDL methods.

**结论:** Background concentration is a useful concept that improves the LDL process and leads to more accurate prediction results than state-of-the-art methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Concentration+Distribution+Learning+from+Label+Distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21576，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21576&send_immediately=true&force_search=false)

**原文摘要:** Label distribution learning (LDL) is an effective method to predict the
relative label description degree (a.k.a. label distribution) of a sample.
However, the label distribution is not a complete representation of an instance
because it overlooks the absolute intensity of each label. Specifically, it's
impossible to obtain the total description degree of hidden labels that not in
the label space, which leads to the loss of information and confusion in
instances. To solve the above problem, we come up with a new concept named
background concentration to serve as the absolute description degree term of
the label distribution and introduce it into the LDL process, forming the
improved paradigm of concentration distribution learning. Moreover, we propose
a novel model by probabilistic methods and neural networks to learn label
distributions and background concentrations from existing LDL datasets.
Extensive experiments prove that the proposed approach is able to extract
background concentrations from label distributions while producing more
accurate prediction results than the state-of-the-art LDL methods. The code is
available in https://github.com/seutjw/CDL-LD.

</details>


### [18] [Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings](https://arxiv.org/abs/2505.22356)
*Angéline Pouget, Mohammad Yaghini, Stephan Rabanser, Nicolas Papernot*

**主要类别:** cs.LG

**概要:** 在关键安全领域部署机器学习模型时，确保模型在没有真实标签的情况下对下游用户数据的可靠性能是一个重要挑战。本文提出了适用性过滤器，一种用于检测性能退化的框架，通过使用敏感于协变量偏移并能指示潜在预测错误的模型输出特征（即适用性信号）来实现。该方法通过统计假设检验比较测试和用户数据的经验分布，以评估未标记用户数据上的分类器准确性是否显著下降，并确保下降不超过预设范围。实验结果表明，该方法能够可靠地检测由于协变量偏移导致的性能偏差，从而为主动缓解高风险应用中的潜在故障提供支持。


<details>
  <summary>更多</summary>
  
**动机:** 在关键安全领域中，确保机器学习模型在无法获得真实标签验证的情况下，仍能在用户数据上保持可靠的性能是一项重大挑战。当前缺乏有效的方法来检测模型性能的退化，特别是在面对协变量偏移时。

**方法:** 提出了一种名为适用性过滤器的新框架，利用适用性信号（模型输出特征）来检测性能退化。具体步骤包括：1) 评估未标记用户数据上的分类器准确性是否有显著下降；2) 确保这种下降不超出预设的可接受范围；3) 使用统计假设检验比较测试和用户数据的适用性信号分布，以量化决策不确定性。该方法具有模块化设计，可以适应不同的模型和领域。

**结果:** 在多个分类任务上的实证评估表明，适用性过滤器能够可靠地检测由于协变量偏移引起的性能偏差。这为在高风险应用中主动缓解潜在故障提供了可能。

**结论:** 适用性过滤器是一种有效的工具，能够在无标签情况下检测机器学习模型的性能退化，适用于各种模型和领域。它为提高关键安全领域中模型的可靠性提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Suitability+Filter%3A+A+Statistical+Framework+for+Classifier+Evaluation+in+Real-World+Deployment+Settings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22356&send_immediately=true&force_search=false)

**原文摘要:** Deploying machine learning models in safety-critical domains poses a key
challenge: ensuring reliable model performance on downstream user data without
access to ground truth labels for direct validation. We propose the suitability
filter, a novel framework designed to detect performance deterioration by
utilizing suitability signals -- model output features that are sensitive to
covariate shifts and indicative of potential prediction errors. The suitability
filter evaluates whether classifier accuracy on unlabeled user data shows
significant degradation compared to the accuracy measured on the labeled test
dataset. Specifically, it ensures that this degradation does not exceed a
pre-specified margin, which represents the maximum acceptable drop in accuracy.
To achieve reliable performance evaluation, we aggregate suitability signals
for both test and user data and compare these empirical distributions using
statistical hypothesis testing, thus providing insights into decision
uncertainty. Our modular method adapts to various models and domains. Empirical
evaluations across different classification tasks demonstrate that the
suitability filter reliably detects performance deviations due to covariate
shift. This enables proactive mitigation of potential failures in high-stakes
applications.

</details>


### [19] [Fairness in Federated Learning: Fairness for Whom?](https://arxiv.org/abs/2505.21584)
*Afaf Taik, Khaoula Chehbouni, Golnoosh Farnadi*

**主要类别:** cs.LG

**概要:** 公平性在联邦学习（FL）中是一个快速发展的研究领域，但现有方法往往过于关注狭隘的系统级指标，如性能平等等，而忽略了FL生命周期中产生的危害及其对不同利益相关者的影响。本文通过分析文献揭示了五个常见的陷阱，并提出以危害为中心的框架，将公平性定义与具体风险和利益相关者的脆弱性联系起来，最后给出了更全面、情境感知和负责任的公平性研究建议。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习公平性研究倾向于优化狭窄的系统级指标（如性能平等等），而忽视了整个联邦学习生命周期中的潜在危害及其对不同利益相关者的影响。因此需要一种新的框架来解决这些问题。

**方法:** 通过对现有文献进行系统注释，分析其中关于公平性的定义、设计决策、评估实践和激励用例，揭示出五个反复出现的问题陷阱，并基于这些洞见提出了一个以危害为中心的框架，该框架将公平性定义与具体的利害关系和利益相关者的脆弱性联系起来。

**结果:** 发现了现有研究中存在的五个主要问题陷阱，并提出了一种新的危害中心框架，可以更好地将公平性与实际风险和利益相关者的脆弱性挂钩。

**结论:** 建议未来的研究应该更加全面、具有情境意识并注重责任，以改进联邦学习中的公平性研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fairness+in+Federated+Learning%3A+Fairness+for+Whom%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21584，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21584&send_immediately=true&force_search=false)

**原文摘要:** Fairness in federated learning has emerged as a rapidly growing area of
research, with numerous works proposing formal definitions and algorithmic
interventions. Yet, despite this technical progress, fairness in FL is often
defined and evaluated in ways that abstract away from the sociotechnical
contexts in which these systems are deployed. In this paper, we argue that
existing approaches tend to optimize narrow system level metrics, such as
performance parity or contribution-based rewards, while overlooking how harms
arise throughout the FL lifecycle and how they impact diverse stakeholders. We
support this claim through a critical analysis of the literature, based on a
systematic annotation of papers for their fairness definitions, design
decisions, evaluation practices, and motivating use cases. Our analysis reveals
five recurring pitfalls: 1) fairness framed solely through the lens of server
client architecture, 2) a mismatch between simulations and motivating use-cases
and contexts, 3) definitions that conflate protecting the system with
protecting its users, 4) interventions that target isolated stages of the
lifecycle while neglecting upstream and downstream effects, 5) and a lack of
multi-stakeholder alignment where multiple fairness definitions can be relevant
at once. Building on these insights, we propose a harm centered framework that
links fairness definitions to concrete risks and stakeholder vulnerabilities.
We conclude with recommendations for more holistic, context-aware, and
accountable fairness research in FL.

</details>


### [20] [Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles](https://arxiv.org/abs/2505.22361)
*Xiangyu Chang, Xi Chen, Yining Wang, Zhiyi Zeng*

**主要类别:** cs.LG

**概要:** 这篇论文研究了一个带有未知强凹函数$f$的多臂赌博机优化问题，目标是在$T$个周期内最大化$f(x)$。作者引入了一种新的成对比较预言，并提出了离散化技术和局部多项式逼近方法将其与线性赌博机联系起来，同时开发了锦标赛连续消除技术并运行交互式的批量LinUCB算法。最终，他们建立了接近最优的遗憾界限，并将该方法应用于两个运营管理问题，改进了现有文献中的最佳结果。


<details>
  <summary>更多</summary>
  
**动机:** 在许多实际问题中，如联合定价和库存补充问题及网络收入管理，需要在未知函数的情况下进行优化决策。然而，传统的随机优化方法无法处理估计偏差的问题，因此需要一种新的方法来解决这些问题。

**方法:** 作者首先引入了离散化技术和局部多项式逼近，将问题与线性赌博机联系起来。然后，他们开发了一种锦标赛连续消除技术来定位离散化的单元格，并在这些单元格上运行交互式的批量LinUCB算法。

**结果:** 作者建立了接近最优的遗憾界限（仅差多对数因子），并将该方法应用于两个运营管理问题，获得了比现有文献中的最佳结果更好的成果。

**结论:** 本文提出的算法和分析框架可以有效地解决具有偏差估计的成对比较预言问题，并在运营管理问题中展现了优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuum-armed+Bandit+Optimization+with+Batch+Pairwise+Comparison+Oracles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22361，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22361&send_immediately=true&force_search=false)

**原文摘要:** This paper studies a bandit optimization problem where the goal is to
maximize a function $f(x)$ over $T$ periods for some unknown strongly concave
function $f$. We consider a new pairwise comparison oracle, where the
decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of
periods and then obtains an estimate of $f(x)-f(x')$. We show that such a
pairwise comparison oracle finds important applications to joint pricing and
inventory replenishment problems and network revenue management. The challenge
in this bandit optimization is twofold. First, the decision-maker not only
needs to determine a pair of actions $(x, x')$ but also a stopping time $n$
(i.e., the number of queries based on $(x, x')$). Second, motivated by our
inventory application, the estimate of the difference $f(x)-f(x')$ is biased,
which is different from existing oracles in stochastic optimization literature.
To address these challenges, we first introduce a discretization technique and
local polynomial approximation to relate this problem to linear bandits. Then
we developed a tournament successive elimination technique to localize the
discretized cell and run an interactive batched version of LinUCB algorithm on
cells. We establish regret bounds that are optimal up to poly-logarithmic
factors. Furthermore, we apply our proposed algorithm and analytical framework
to the two operations management problems and obtain results that improve
state-of-the-art results in the existing literature.

</details>


### [21] [CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning](https://arxiv.org/abs/2505.21587)
*Bin Qin, Qirui Ji, Jiangmeng Li, Yupeng Wang, Xuesong Wu, Jianwen Cao, Fanjiang Xu*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架CellCLAT，通过参数扰动和细胞修剪策略解决了细胞拓扑深度学习中的结构约束和信息冗余问题，显著改进了自监督图学习方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自监督拓扑深度学习在处理细胞复形时面临两个主要挑战：1) 细胞复形的外部结构约束限制了传统图增强技术的应用；2) 细胞表示中的内在语义冗余可能削弱任务相关的信息。这促使研究者设计一种新方法来克服这些障碍。

**方法:** 提出了CellCLAT框架，包含两个关键部分：1) 参数扰动增强方法，在不改变底层细胞结构的情况下向细胞交互中注入可控噪声；2) 细胞修剪调度器，通过双层元学习方法屏蔽任务无关细胞的梯度贡献，去除冗余拓扑元素。

**结果:** 理论分析和实验证明表明，CellCLAT相比现有自监督图学习方法有显著改进。

**结论:** CellCLAT为解决细胞拓扑深度学习中的核心问题提供了有效方案，并标志着该领域的重要进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CellCLAT%3A+Preserving+Topology+and+Trimming+Redundancy+in+Self-Supervised+Cellular+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21587，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21587&send_immediately=true&force_search=false)

**原文摘要:** Self-supervised topological deep learning (TDL) represents a nascent but
underexplored area with significant potential for modeling higher-order
interactions in simplicial complexes and cellular complexes to derive
representations of unlabeled graphs. Compared to simplicial complexes, cellular
complexes exhibit greater expressive power. However, the advancement in
self-supervised learning for cellular TDL is largely hindered by two core
challenges: \textit{extrinsic structural constraints} inherent to cellular
complexes, and intrinsic semantic redundancy in cellular representations. The
first challenge highlights that traditional graph augmentation techniques may
compromise the integrity of higher-order cellular interactions, while the
second underscores that topological redundancy in cellular complexes
potentially diminish task-relevant information. To address these issues, we
introduce Cellular Complex Contrastive Learning with Adaptive Trimming
(CellCLAT), a twofold framework designed to adhere to the combinatorial
constraints of cellular complexes while mitigating informational redundancy.
Specifically, we propose a parameter perturbation-based augmentation method
that injects controlled noise into cellular interactions without altering the
underlying cellular structures, thereby preserving cellular topology during
contrastive learning. Additionally, a cellular trimming scheduler is employed
to mask gradient contributions from task-irrelevant cells through a bi-level
meta-learning approach, effectively removing redundant topological elements
while maintaining critical higher-order semantics. We provide theoretical
justification and empirical validation to demonstrate that CellCLAT achieves
substantial improvements over existing self-supervised graph learning methods,
marking a significant attempt in this domain.

</details>


### [22] [On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling](https://arxiv.org/abs/2505.22491)
*Moritz Haas, Sebastian Bordt, Ulrike von Luxburg, Leena Chennuru Vankadara*

**主要类别:** cs.LG

**概要:** 通过研究神经网络训练动态，发现标准参数化（SP）的成功不能完全由有限宽度现象解释。在交叉熵损失下，出现了一个中间的“受控发散”阶段，其中logits发散但损失、梯度和激活保持稳定。这使得在大学习率下进行稳定的训练成为可能，并且所有隐藏层中的特征进化得以持续，这是SP实际成功的关键。实验验证了不同优化器、架构和数据模态下，神经网络在交叉熵损失下的受控发散阶段。此外，宽度缩放考虑对于预测经验最优学习率指数非常有用，分析还阐明了最近提出的逐层学习率缩放方法的有效性和局限性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管标准参数化在实践中取得了成功，但从理论上对其理解仍然不足。现有的无限宽度理论预测，在大学习率下会导致不稳定，在稳定学习率下会导致特征学习消失。然而，实际上最优学习率的衰减速率远比理论预测的要慢，因此需要进一步研究这种差异的原因。

**方法:** 作者通过对神经网络训练动态的仔细研究，探讨了标准参数化与理论预测之间的差异。他们将损失函数纳入考量，证明在交叉熵损失下，会出现一个中间的“受控发散”阶段，即logits发散但损失、梯度和激活保持稳定。此外，他们还在不同的优化器、架构和数据模态上进行了实验验证。

**结果:** 实验结果表明，在交叉熵损失下，神经网络确实运行在受控发散阶段，而在均方误差损失下则不会。此外，宽度缩放考虑对于预测经验最优学习率指数非常有效。最后，作者分析了逐层学习率缩放方法的有效性和局限性。

**结论:** 研究揭示了标准参数化的实际成功原因，并澄清了其有效性和局限性。通过考虑损失函数的影响，特别是交叉熵损失下的受控发散阶段，可以更好地理解大尺度视觉和语言模型的训练动态。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Surprising+Effectiveness+of+Large+Learning+Rates+under+Standard+Width+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22491，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22491&send_immediately=true&force_search=false)

**原文摘要:** The dominant paradigm for training large-scale vision and language models is
He initialization and a single global learning rate (\textit{standard
parameterization}, SP). Despite its practical success, standard parametrization
remains poorly understood from a theoretical perspective: Existing
infinite-width theory would predict instability under large learning rates and
vanishing feature learning under stable learning rates. However, empirically
optimal learning rates consistently decay much slower than theoretically
predicted. By carefully studying neural network training dynamics, we
demonstrate that this discrepancy is not fully explained by finite-width
phenomena such as catapult effects or a lack of alignment between weights and
incoming activations. We instead show that the apparent contradiction can be
fundamentally resolved by taking the loss function into account: In contrast to
Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an
intermediate \textit{controlled divergence} regime emerges, where logits
diverge but loss, gradients, and activations remain stable. Stable training
under large learning rates enables persistent feature evolution at scale in all
hidden layers, which is crucial for the practical success of SP. In experiments
across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities
(vision, language), we validate that neural networks operate in this controlled
divergence regime under CE loss but not under MSE loss. Our empirical evidence
suggests that width-scaling considerations are surprisingly useful for
predicting empirically optimal learning rate exponents. Finally, our analysis
clarifies the effectiveness and limitations of recently proposed layerwise
learning rate scalings for standard initialization.

</details>


### [23] [Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning](https://arxiv.org/abs/2505.21591)
*Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, Tao Chen*

**主要类别:** cs.LG

**概要:** Model quantization is important for improving memory efficiency and inference speed in diffusion models. Existing methods struggle with 4-bit quantization, so this paper explores low-bit FP quantization and proposes the MSFP framework to address challenges such as asymmetric activation distributions, temporal complexity, and misalignment between fine-tuning loss and quantization error.


<details>
  <summary>更多</summary>
  
**动机:** Existing model quantization methods face difficulties achieving consistent performance at 4-bit precision, particularly due to issues like asymmetric activation distributions, insufficient handling of temporal complexity during denoising, and misalignment between fine-tuning loss and quantization error.

**方法:** The authors propose the mixup-sign floating-point quantization (MSFP) framework which includes unsigned FP quantization, timestep-aware LoRA (TALoRA), and denoising-factor loss alignment (DFA). These components help overcome the identified challenges in low-bit FP quantization for diffusion models.

**结果:** Through extensive experiments, the proposed MSFP framework shows superior performance in 4-bit FP quantization for diffusion models compared to existing post-training quantization fine-tuning methods using 4-bit integer quantization.

**结论:** The MSFP framework successfully addresses key challenges in low-bit FP quantization for diffusion models, marking a significant advancement towards more efficient and accurate quantized diffusion models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pioneering+4-Bit+FP+Quantization+for+Diffusion+Models%3A+Mixup-Sign+Quantization+and+Timestep-Aware+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21591，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21591&send_immediately=true&force_search=false)

**原文摘要:** Model quantization reduces the bit-width of weights and activations,
improving memory efficiency and inference speed in diffusion models. However,
achieving 4-bit quantization remains challenging. Existing methods, primarily
based on integer quantization and post-training quantization fine-tuning,
struggle with inconsistent performance. Inspired by the success of
floating-point (FP) quantization in large language models, we explore low-bit
FP quantization for diffusion models and identify key challenges: the failure
of signed FP quantization to handle asymmetric activation distributions, the
insufficient consideration of temporal complexity in the denoising process
during fine-tuning, and the misalignment between fine-tuning loss and
quantization error. To address these challenges, we propose the mixup-sign
floating-point quantization (MSFP) framework, first introducing unsigned FP
quantization in model quantization, along with timestep-aware LoRA (TALoRA) and
denoising-factor loss alignment (DFA), which ensure precise and stable
fine-tuning. Extensive experiments show that we are the first to achieve
superior performance in 4-bit FP quantization for diffusion models,
outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

</details>


### [24] [Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation](https://arxiv.org/abs/2505.22492)
*Hongyi Zhou, Josiah P. Hanna, Jin Zhu, Ying Yang, Chengchun Shi*

**主要类别:** cs.LG

**概要:** This paper explains why using history-dependent behavior policies in off-policy evaluation can lead to lower MSE through theoretical analysis and bias-variance decomposition.


<details>
  <summary>更多</summary>
  
**动机:** Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open.

**方法:** The paper theoretically demystifies this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases.

**结果:** Additionally, as the estimated behavior policy conditions on a longer history, there is a consistent decrease in variance. These findings are extended to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically.

**结论:** History-dependent behavior policy estimation can lead to lower MSE due to decreased asymptotic variances despite increased finite-sample biases.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying+the+Paradox+of+Importance+Sampling+with+an+Estimated+History-Dependent+Behavior+Policy+in+Off-Policy+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22492&send_immediately=true&force_search=false)

**原文摘要:** This paper studies off-policy evaluation (OPE) in reinforcement learning with
a focus on behavior policy estimation for importance sampling. Prior work has
shown empirically that estimating a history-dependent behavior policy can lead
to lower mean squared error (MSE) even when the true behavior policy is
Markovian. However, the question of why the use of history should lower MSE
remains open. In this paper, we theoretically demystify this paradox by
deriving a bias-variance decomposition of the MSE of ordinary importance
sampling (IS) estimators, demonstrating that history-dependent behavior policy
estimation decreases their asymptotic variances while increasing their
finite-sample biases. Additionally, as the estimated behavior policy conditions
on a longer history, we show a consistent decrease in variance. We extend these
findings to a range of other OPE estimators, including the sequential IS
estimator, the doubly robust estimator and the marginalized IS estimator, with
the behavior policy estimated either parametrically or non-parametrically.

</details>


### [25] [Relevance-driven Input Dropout: an Explanation-guided Regularization Technique](https://arxiv.org/abs/2505.21595)
*Shreyas Gururaj, Lars Grüne, Wojciech Samek, Sebastian Lapuschkin, Leander Weber*

**主要类别:** cs.LG

**概要:** 提出了一种新的数据增强方法RelDrop，通过有选择地遮挡输入中最相关的区域，推动模型使用其他重要特征进行预测，从而提高模型泛化能力。实验表明该方法提高了模型对遮挡的鲁棒性、利用更多特征并提升了推理时间的泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 过拟合是机器学习模型中的一个已知问题，导致泛化能力下降和训练-测试性能差距显著。现有的缓解措施包括dropout、数据增强、权重衰减等正则化技术，但大多数方法强调随机选择和修改输入特征，而非针对强烈影响模型决策的区域。

**方法:** 提出了一种名为Relevance-driven Input Dropout (RelDrop)的新数据增强方法，该方法通过选择性遮挡输入中最相关的区域，促使模型在预测过程中使用其他重要特征，从而实现知情的正则化以改善模型泛化能力。

**结果:** 通过一系列基准数据集上的实验，证明了RelDrop方法提高了模型对遮挡的鲁棒性、使模型利用了更多特征，并提升了推理时间的泛化性能。

**结论:** RelDrop作为一种新型的数据增强策略，能够有效改善模型的泛化能力和决策过程，尤其是在遮挡情况下的表现。代码已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Relevance-driven+Input+Dropout%3A+an+Explanation-guided+Regularization+Technique，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21595&send_immediately=true&force_search=false)

**原文摘要:** Overfitting is a well-known issue extending even to state-of-the-art (SOTA)
Machine Learning (ML) models, resulting in reduced generalization, and a
significant train-test performance gap. Mitigation measures include a
combination of dropout, data augmentation, weight decay, and other
regularization techniques. Among the various data augmentation strategies,
occlusion is a prominent technique that typically focuses on randomly masking
regions of the input during training. Most of the existing literature
emphasizes randomness in selecting and modifying the input features instead of
regions that strongly influence model decisions. We propose Relevance-driven
Input Dropout (RelDrop), a novel data augmentation method which selectively
occludes the most relevant regions of the input, nudging the model to use other
important features in the prediction process, thus improving model
generalization through informed regularization. We further conduct qualitative
and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)
affects model decision-making. Through a series of experiments on benchmark
datasets, we demonstrate that our approach improves robustness towards
occlusion, results in models utilizing more features within the region of
interest, and boosts inference time generalization performance. Our code is
available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.

</details>


### [26] [Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks](https://arxiv.org/abs/2505.22538)
*Paul Hofman, Yusuf Sale, Eyke Hüllermeier*

**主要类别:** cs.LG

**概要:** 提出了一种基于已知损失函数分解的不确定性量化框架，能够灵活地度量总不确定性、随机不确定性和认知不确定性。此框架可针对不同任务定制，并在选择性预测、分布外检测和主动学习任务中表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 现有的不确定性量化方法可能无法很好地适应特定任务的需求，因此需要一个更灵活的框架来根据具体应用场景调整不确定性度量方式。

**方法:** 通过将（严格）适当的评分规则分解为散度和熵成分，定义总不确定性、随机不确定性和认知不确定性。该方法允许使用不同的损失函数实例化不确定性量化框架，从而针对特定任务进行定制。

**结果:** 1. 在选择性预测任务中，评分规则与任务损失匹配时效果最佳；2. 分布外检测实验表明，互信息作为认知不确定性度量性能最优；3. 基于零一损失的认知不确定性度量在主动学习任务中始终优于其他方法。

**结论:** 所提出的不确定性量化框架具有灵活性，可以根据具体任务需求选择合适的损失函数，从而提升性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+Quantification+with+Proper+Scoring+Rules%3A+Adjusting+Measures+to+Prediction+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22538，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22538&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of uncertainty quantification and propose measures of
total, aleatoric, and epistemic uncertainty based on a known decomposition of
(strictly) proper scoring rules, a specific type of loss function, into a
divergence and an entropy component. This leads to a flexible framework for
uncertainty quantification that can be instantiated with different losses
(scoring rules), which makes it possible to tailor uncertainty quantification
to the use case at hand. We show that this flexibility is indeed advantageous.
In particular, we analyze the task of selective prediction and show that the
scoring rule should ideally match the task loss. In addition, we perform
experiments on two other common tasks. For out-of-distribution detection, our
results confirm that a widely used measure of epistemic uncertainty, mutual
information, performs best. Moreover, in the setting of active learning, our
measure of epistemic uncertainty based on the zero-one-loss consistently
outperforms other uncertainty measures.

</details>


### [27] [SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/abs/2505.21605)
*Fengqing Jiang, Fengbo Ma, Zhangchen Xu, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bo Li, Xianyan Chen, Zhen Xiang, Radha Poovendran*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）在复杂任务中表现出色，但在涉及科学风险的滥用情况下的安全性尚待深入研究。现有的安全基准未能充分评估模型在知识密集型、危险场景中的安全性。为此，我们提出了SOSBench，这是一个基于法规、聚焦危害的基准测试，涵盖了六个高风险科学领域：化学、生物、医学、药理学、物理和心理学。通过评估前沿模型，发现即使是最先进的模型也普遍存在违反政策的内容输出，表明LLMs在安全性对齐方面存在显著缺陷，亟需关注其负责任部署的问题。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型虽然在复杂任务中表现出色，但其在涉及科学风险的滥用情况下的安全性尚未得到充分探索。现有安全基准主要关注低知识要求或低风险场景，无法有效评估模型在处理知识密集型、危险场景时的安全性。

**方法:** 引入了SOSBench，一个基于法规、聚焦危害的基准测试工具，涵盖六个高风险科学领域，并通过LLM辅助进化管道生成3000个真实世界法规和法律相关的提示，以评估模型在高风险场景下的表现。

**结果:** 尽管最先进的模型声称已进行对齐训练，但在所有领域中仍一致披露违反政策的内容，有害响应率极高（例如Deepseek-R1为79.1%，GPT-4.1为47.3%）。

**结论:** 结果表明，当前大型语言模型在安全性对齐方面存在显著不足，强调了对其负责任部署的迫切需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SOSBENCH%3A+Benchmarking+Safety+Alignment+on+Scientific+Knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21605&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) exhibit advancing capabilities in complex tasks,
such as reasoning and graduate-level question answering, yet their resilience
against misuse, particularly involving scientifically sophisticated risks,
remains underexplored. Existing safety benchmarks typically focus either on
instructions requiring minimal knowledge comprehension (e.g., ``tell me how to
build a bomb") or utilize prompts that are relatively low-risk (e.g.,
multiple-choice or classification tasks about hazardous content). Consequently,
they fail to adequately assess model safety when handling knowledge-intensive,
hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded,
hazard-focused benchmark encompassing six high-risk scientific domains:
chemistry, biology, medicine, pharmacology, physics, and psychology. The
benchmark comprises 3,000 prompts derived from real-world regulations and laws,
systematically expanded via an LLM-assisted evolutionary pipeline that
introduces diverse, realistic misuse scenarios (e.g., detailed explosive
synthesis instructions involving advanced chemical formulas). We evaluate
frontier models within a unified evaluation framework using our SOSBench.
Despite their alignment claims, advanced models consistently disclose
policy-violating content across all domains, demonstrating alarmingly high
rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).
These results highlight significant safety alignment deficiencies and
underscore urgent concerns regarding the responsible deployment of powerful
LLMs.

</details>


### [28] [Efficient Controllable Diffusion via Optimal Classifier Guidance](https://arxiv.org/abs/2505.21666)
*Owen Oertell, Shikun Sun, Yiding Chen, Jin Peng Zhou, Zhiyong Wang, Wen Sun*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为SLCD的方法，通过监督学习实现可控扩散模型的生成任务，在保证高质量样本生成的同时，保持与基础模型相近的推理时间。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于强化学习（RL）微调的可控生成方法虽然有效，但存在过拟合奖励函数的问题，并且需要大量资源。因此，研究者希望找到一种更高效、更稳定的方法来实现可控生成任务。

**方法:** 论文将可控生成问题转化为寻找一个优化KL正则化目标函数的分布问题，并提出了SLCD方法。该方法通过迭代生成在线数据并训练一个小分类器来指导扩散模型的生成过程。其核心计算单元是分类任务，避免了RL或控制中的复杂概念。

**结果:** 理论上，通过无悔在线学习分析，证明了在KL散度下，SLCD的输出会收敛到KL正则化目标的最优解。实验上，SLCD在连续扩散的图像生成和离散扩散的生物序列生成中，均能以接近基础模型的推理时间生成高质量样本。

**结论:** SLCD提供了一种高效且稳定的可控生成方法，适用于图像和生物序列生成等多种应用场景，同时避免了传统RL方法的复杂性和资源消耗问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Controllable+Diffusion+via+Optimal+Classifier+Guidance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21666，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21666&send_immediately=true&force_search=false)

**原文摘要:** The controllable generation of diffusion models aims to steer the model to
generate samples that optimize some given objective functions. It is desirable
for a variety of applications including image generation, molecule generation,
and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of
the base model is a popular approach but it can overfit the reward function
while requiring significant resources. We frame controllable generation as a
problem of finding a distribution that optimizes a KL-regularized objective
function. We present SLCD -- Supervised Learning based Controllable Diffusion,
which iteratively generates online data and trains a small classifier to guide
the generation of the diffusion model. Similar to the standard
classifier-guided diffusion, SLCD's key computation primitive is classification
and does not involve any complex concepts from RL or control. Via a reduction
to no-regret online learning analysis, we show that under KL divergence, the
output from SLCD provably converges to the optimal solution of the
KL-regularized objective. Further, we empirically demonstrate that SLCD can
generate high quality samples with nearly the same inference time as the base
model in both image generation with continuous diffusion and biological
sequence generation with discrete diffusion. Our code is available at
https://github.com/Owen-Oertell/slcd

</details>


### [29] [What happens when generative AI models train recursively on each others' generated outputs?](https://arxiv.org/abs/2505.21677)
*Hung Ahn Vu, Galen Reeves, Emily Wenger*

**主要类别:** cs.LG

**概要:** 本研究提供了关于数据媒介交互如何在实践中展开的实证证据，开发了一个理论模型来描述这种交互训练过程，并展示了这种交互可能产生的长期结果。


<details>
  <summary>更多</summary>
  
**动机:** 互联网既是AI生成内容的来源，也是生成式AI模型的训练数据来源，这引发了未来生成式AI模型可能会被训练在其他模型生成的输出上的可能性。理解这种数据媒介模型交互的下游影响对于社会日益依赖生成式AI工具至关重要。

**方法:** 研究者提供了数据媒介交互如何在实践中展开的实证证据，开发了一个理论模型来描述这种交互训练过程，并通过实验展示了这种交互可能产生的长期结果。

**结果:** 研究发现，数据媒介交互可以通过向模型暴露在原始训练数据中可能遗漏的新概念来使模型受益，但也可能导致它们在共享任务上的性能同质化。

**结论:** 数据媒介交互对模型既有积极影响也有消极影响，理解这些影响对于社会使用生成式AI工具具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+happens+when+generative+AI+models+train+recursively+on+each+others%27+generated+outputs%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21677&send_immediately=true&force_search=false)

**原文摘要:** The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.

</details>


### [30] [Apprenticeship learning with prior beliefs using inverse optimization](https://arxiv.org/abs/2505.21639)
*Mauricio Junca, Esteban Leiva*

**主要类别:** cs.LG

**概要:** 这篇论文重新审视了马尔可夫决策过程（MDP）中的逆优化（IO）、逆强化学习（IRL）和学徒学习（AL）之间的关系，提出了在次优专家设定下将AL问题表示为正则化极小极大问题，并使用随机镜像下降法解决该问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管IRL和IO解决了相同的问题，但它们在文献中的联系尚未被充分探讨。因此，作者希望深入研究它们之间的关系，并通过引入先验信念来改进IRL和AL问题的求解方法。

**方法:** 作者将先验信念融入IRL和AL问题中，并证明了AL形式主义是他们框架的一个特例。在次优专家设定下，将AL问题表述为正则化极小极大问题，其中正则化项用于解决IRL问题的不适定性。采用随机镜像下降法（SMD）解决正则化凸凹极小极大问题，并建立收敛边界。

**结果:** 数值实验表明，正则化在学习成本向量和学徒策略中起着关键作用。所提出的方法能够有效指导寻找合理的成本函数，并且展示了良好的收敛性能。

**结论:** 本文通过重新审视IO、IRL和AL之间的关系，揭示了AL形式主义作为框架特例的本质，并通过引入正则化项改进了IRL问题的求解方法。结果表明，正则化在学习过程中具有重要作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Apprenticeship+learning+with+prior+beliefs+using+inverse+optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21639&send_immediately=true&force_search=false)

**原文摘要:** The relationship between inverse reinforcement learning (IRL) and inverse
optimization (IO) for Markov decision processes (MDPs) has been relatively
underexplored in the literature, despite addressing the same problem. In this
work, we revisit the relationship between the IO framework for MDPs, IRL, and
apprenticeship learning (AL). We incorporate prior beliefs on the structure of
the cost function into the IRL and AL problems, and demonstrate that the
convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a
relaxation of our framework. Notably, the AL formalism is a special case in our
framework when the regularization term is absent. Focusing on the suboptimal
expert setting, we formulate the AL problem as a regularized min-max problem.
The regularizer plays a key role in addressing the ill-posedness of IRL by
guiding the search for plausible cost functions. To solve the resulting
regularized-convex-concave-min-max problem, we use stochastic mirror descent
(SMD) and establish convergence bounds for the proposed method. Numerical
experiments highlight the critical role of regularization in learning cost
vectors and apprentice policies.

</details>


### [31] [multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data](https://arxiv.org/abs/2505.21680)
*Andrew J. Loza, Jun Yup Kim, Shangzheng Song, Yihang Liu, Joseph J. Y. Sung, R Andrew Taylor, Dennis L. Shung*

**主要类别:** cs.LG

**概要:** 本文提出multivariateGPT，一种单一架构模型，用于处理混合类别（包括标记化文本）和数值数据的序列。通过自回归序列分解、嵌入方案和损失函数，将下一个标记预测任务扩展到下一个标记类别和值的联合分布的概率估计。该方法可以有效学习简单物理系统中的模式，并对复杂的时间序列进行建模，如心电图和多变量电子健康记录数据。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的过程通常生成混合类别和数值的数据，这些数据在不规则和有信息量的间隔内被记录。现有的离散标记方法在数值表示能力上有限，而类似神经常微分方程的方法不太适合类别数据或有信息量的采样，并且需要增强以处理某些类别的轨迹。

**方法:** 研究者提出了multivariateGPT，这是一种用于建模混合类别（包括标记化文本）和数值数据序列的单一架构。通过自回归序列分解、嵌入方案和损失函数，将下一个标记预测任务扩展到下一个标记类别和值的联合分布的概率估计。

**结果:** 该方法能够有效地学习简单物理系统中的模式，并对复杂时间序列进行建模，如心电图和多变量电子健康记录数据。

**结论:** 这项工作扩展了基于变压器模型的实用性，使其适用于额外类别的数据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是multivariateGPT%3A+a+decoder-only+transformer+for+multivariate+categorical+and+numeric+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21680，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21680&send_immediately=true&force_search=false)

**原文摘要:** Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.

</details>


### [32] [Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling](https://arxiv.org/abs/2505.21717)
*Mónika Farsang, Ramin Hasani, Radu Grosu*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Up+Liquid-Resistance+Liquid-Capacitance+Networks+for+Efficient+Sequence+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21717，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21717&send_immediately=true&force_search=false)

**原文摘要:** We present LrcSSM, a \textit{nonlinear} recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.

</details>


### [33] [PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects](https://arxiv.org/abs/2505.21641)
*Maresa Schröder, Justin Hartenstein, Stefan Feuerriegel*

**主要类别:** cs.LG

**概要:** 本研究提出了PrivATE框架，用于在差分隐私条件下计算平均处理效应（ATE）的置信区间（CIs）。此框架包含三个步骤：通过输出扰动估计私有ATE、通过截断输出扰动机制估计私有方差以及构建考虑不确定性的置信区间。PrivATE模型无关、双重稳健，并确保有效置信区间。这是首个适用于($\varepsilon$, $\delta$)-差分隐私下的通用双重稳健框架。


<details>
  <summary>更多</summary>
  
**动机:** 在医疗等关键安全领域中，评估药物和其他医疗干预措施的效果需要可靠的平均治疗效果（ATE）推断和有效的不确定性量化（如置信区间）。然而，在这些场景下，估计治疗效果通常涉及敏感数据，因此需要保护隐私的方法来实现ATE的有效推断和不确定性量化。

**方法:** 提出了一种名为PrivATE的新机器学习框架，该框架通过以下三个步骤计算ATE的置信区间：1) 使用输出扰动方法估计差分隐私ATE；2) 使用截断输出扰动机制估计差分隐私方差；3) 考虑估计和私有化过程中的不确定性来构建置信区间。PrivATE具有模型无关性、双重稳健性，并能保证有效的置信区间。

**结果:**  PrivATE框架在合成数据集和真实世界医疗数据集上进行了验证，证明了其有效性。结果表明，PrivATE可以成功地在满足差分隐私要求的同时，提供有效的ATE置信区间估计。

**结论:**  PrivATE是一个通用且双重稳健的框架，可以在($\varepsilon$, $\delta$)-差分隐私条件下生成有效的ATE置信区间。它为医疗等领域的隐私保护数据分析提供了重要的工具支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PrivATE%3A+Differentially+Private+Confidence+Intervals+for+Average+Treatment+Effects，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21641&send_immediately=true&force_search=false)

**原文摘要:** The average treatment effect (ATE) is widely used to evaluate the
effectiveness of drugs and other medical interventions. In safety-critical
applications like medicine, reliable inferences about the ATE typically require
valid uncertainty quantification, such as through confidence intervals (CIs).
However, estimating treatment effects in these settings often involves
sensitive data that must be kept private. In this work, we present PrivATE, a
novel machine learning framework for computing CIs for the ATE under
differential privacy. Specifically, we focus on deriving valid
privacy-preserving CIs for the ATE from observational data. Our PrivATE
framework consists of three steps: (i) estimating a differentially private ATE
through output perturbation; (ii) estimating the differentially private
variance through a truncated output perturbation mechanism; and (iii)
constructing the CIs while accounting for the uncertainty from both the
estimation and privatization steps. Our PrivATE framework is model agnostic,
doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our
framework using synthetic and real-world medical datasets. To the best of our
knowledge, we are the first to derive a general, doubly robust framework for
valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.

</details>


### [34] [Deep Reinforcement Learning Agents are not even close to Human Intelligence](https://arxiv.org/abs/2505.21731)
*Quentin Delfosse, Jannis Blüml, Fabian Tatai, Théo Vincent, Bjarne Gregori, Elisabeth Dillies, Jan Peters, Constantin Rothkopf, Kristian Kersting*

**主要类别:** cs.LG

**概要:** 深度强化学习代理在许多任务中表现出令人印象深刻的结果，但它们缺乏零样本适应能力。尽管大多数鲁棒性评估都集中在任务复杂化上，但对于任务简化却没有任何评估。我们引入HackAtari，这是一组街机学习环境的任务变体，用以证明与人类不同，RL代理在训练任务的更简单版本上系统地表现出巨大的性能下降，揭示了代理对捷径的一致依赖。我们的分析跨越了多种算法和架构，突显了RL代理和人类行为智能之间的持续差距，强调了需要新的基准和方法论来强制进行系统性泛化测试，超越静态评估协议。仅仅在相同的环境中进行训练和测试不足以获得具备人类智能的代理。


<details>
  <summary>更多</summary>
  
**动机:** 深度强化学习代理虽然在许多任务中表现优异，但它们缺乏零样本适应能力，并且当前的鲁棒性评估主要集中在任务复杂化上，忽略了任务简化可能带来的影响。因此，研究者希望探索RL代理在任务简化情况下的表现，以及与人类智能的差距。

**方法:** 研究者引入了一组名为HackAtari的街机学习环境任务变体，使用这些变体来测试多种RL算法和架构在更简单任务版本上的表现。通过比较代理在简化任务上的性能下降情况，分析其对捷径的依赖程度。

**结果:** 研究发现，与人类不同，RL代理在面对更简单的任务版本时，系统性地表现出显著的性能下降，揭示了代理对特定模式或捷径的过度依赖。此外，分析显示多种算法和架构都存在这一问题，表明当前RL代理与人类行为智能之间存在明显的差距。

**结论:** 仅仅在同一环境中进行训练和测试不足以使RL代理具备类似人类的智能。未来的研究需要开发新的基准和方法论，以加强系统性泛化测试，超越现有的静态评估协议。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Reinforcement+Learning+Agents+are+not+even+close+to+Human+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21731，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21731&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning (RL) agents achieve impressive results in a wide
variety of tasks, but they lack zero-shot adaptation capabilities. While most
robustness evaluations focus on tasks complexifications, for which human also
struggle to maintain performances, no evaluation has been performed on tasks
simplifications. To tackle this issue, we introduce HackAtari, a set of task
variations of the Arcade Learning Environments. We use it to demonstrate that,
contrary to humans, RL agents systematically exhibit huge performance drops on
simpler versions of their training tasks, uncovering agents' consistent
reliance on shortcuts. Our analysis across multiple algorithms and
architectures highlights the persistent gap between RL agents and human
behavioral intelligence, underscoring the need for new benchmarks and
methodologies that enforce systematic generalization testing beyond static
evaluation protocols. Training and testing in the same environment is not
enough to obtain agents equipped with human-like intelligence.

</details>


### [35] [PreGenie: An Agentic Framework for High-quality Visual Presentation Generation](https://arxiv.org/abs/2505.21660)
*Xiaojie Xu, Xinli Xu, Sirui Chen, Haoyu Chen, Fan Zhang, Ying-Cong Chen*

**主要类别:** cs.LG

**概要:** PreGenie是一个基于多模态大语言模型的代理和模块化框架，用于生成高质量视觉演示文稿。它通过两个阶段（分析与初始生成、审查与再生成）来创建符合人类设计偏好的演示文稿，在美学和内容一致性上优于现有模型。


<details>
  <summary>更多</summary>
  
**动机:** 早期使用深度学习自动化创建视觉演示文稿的努力常遇到布局混乱、文本摘要不准确以及图像理解不足的问题，这些问题限制了其在正式场景中的应用。

**方法:** PreGenie基于Slidev演示框架，采用两阶段方法：1) 分析与初始生成，总结多模态输入并生成初始代码；2) 审查与再生成，迭代审查中间代码和渲染的幻灯片以生成最终高质量演示文稿。每个阶段利用多个协作并共享信息的多模态大语言模型。

**结果:** 广泛的实验证明PreGenie在多模态理解方面表现出色，超越现有模型，更贴近人类的设计偏好。

**结论:** PreGenie解决了传统方法存在的问题，提供了一种新的代理和模块化框架，能够生成高质量的视觉演示文稿，并适用于商业和科学研究等正式场合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PreGenie%3A+An+Agentic+Framework+for+High-quality+Visual+Presentation+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21660&send_immediately=true&force_search=false)

**原文摘要:** Visual presentations are vital for effective communication. Early attempts to
automate their creation using deep learning often faced issues such as poorly
organized layouts, inaccurate text summarization, and a lack of image
understanding, leading to mismatched visuals and text. These limitations
restrict their application in formal contexts like business and scientific
research. To address these challenges, we propose PreGenie, an agentic and
modular framework powered by multimodal large language models (MLLMs) for
generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are
rendered from Markdown code. It operates in two stages: (1) Analysis and
Initial Generation, which summarizes multimodal input and generates initial
code, and (2) Review and Re-generation, which iteratively reviews intermediate
code and rendered slides to produce final, high-quality presentations. Each
stage leverages multiple MLLMs that collaborate and share information.
Comprehensive experiments demonstrate that PreGenie excels in multimodal
understanding, outperforming existing models in both aesthetics and content
consistency, while aligning more closely with human design preferences.

</details>


### [36] [Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen](https://arxiv.org/abs/2505.21743)
*Zihao Li, Xinyuan Cao, Xiangbo Gao, Kexin Tian, Keshu Wu, Mohammad Anis, Hao Zhang, Keke Long, Jiwan Jiang, Xiaopeng Li, Yunlong Zhang, Tianbao Yang, Dominique Lord, Zhengzhong Tu, Yang Zhou*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种新的反事实安全学习方法，通过生成和分析几乎发生的碰撞事件来丰富稀疏的碰撞数据，并将其转化为用于预测碰撞的丰富信号，从而实现从反应式法医到主动预防的转变。这种方法结合了碰撞率先验、生成场景引擎、多样驾驶员模型和因果学习，构建了一个数字孪生测试平台，将微观场景与宏观模式联系起来，同时使用多目标验证器确保模拟的真实性。最终目标是推动Vision Zero（零交通事故愿景）的实现。


<details>
  <summary>更多</summary>
  
**动机:** 交通安全性研究长期受到基本数据悖论的限制：最希望防止的重大事故恰恰是最难观测到的稀有事件。现有的碰撞频率模型和替代安全指标依赖于稀疏、噪声大且报告不足的数据，而即使高保真度的仿真也无法充分采样导致灾难性结果（如死亡）的长尾情况。因此，需要一种新的范式来解决这一问题。

**方法:** 作者提出了一种从传统的仅基于碰撞数据的学习向反事实安全学习的范式转变。该方法包括以下几个方面：
- 使用碰撞率先验指导生成场景引擎。
- 结合多样化的驾驶员模型和因果学习，合成并解释近失事件（near-miss events）。
- 构建一个以碰撞为中心的数字孪生测试平台，将微观场景与宏观模式联系起来。
- 使用多目标验证器确保模拟统计的真实性。
- 将稀疏的碰撞数据转化为丰富的预测信号，用于车辆、道路和政策部署前的压力测试。

**结果:** 该方法能够通过学习那些几乎发生的碰撞事件，将交通安全从被动的事后分析转变为积极的预防措施。这种方法可以有效提高对潜在危险情景的认识和应对能力，为实现Vision Zero提供了理论和技术支持。

**结论:** 通过采用反事实安全学习方法，可以显著改善交通安全性研究中的数据稀缺问题，从而推动从反应式法医到主动预防的转型。这为实现完全消除交通死亡和严重伤害的Vision Zero目标提供了一条可行路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Simulating+the+Unseen%3A+Crash+Prediction+Must+Learn+from+What+Did+Not+Happen，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21743，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21743&send_immediately=true&force_search=false)

**原文摘要:** Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on sparse, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
sparse crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.

</details>


### [37] [DualSchool: How Reliable are LLMs for Optimization Education?](https://arxiv.org/abs/2505.21775)
*Michael Klamkin, Arnaud Deza, Sikai Cheng, Haoruo Zhao, Pascal Van Hentenryck*

**主要类别:** cs.LG

**概要:** 尽管LLMs可以准确地复述线性规划对偶转换过程，但最先进的开源LLM在生成正确对偶问题上表现不佳，即使是最小的两个变量实例也是如此。本文介绍了一个全面的框架DualSchool，用于生成和验证P2DC实例，并讨论了对教育者、学生和大型推理系统开发的影响。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是评估大规模语言模型（LLMs）在生成线性规划对偶问题（P2DC任务）上的表现，因为学生们可能期望LLMs能够很好地完成这一优化课程中的任务。

**方法:** 引入了一个名为DualSchool的综合框架，用于生成和验证P2DC实例。其验证程序使用了规范图编辑距离（Canonical Graph Edit Distance），超越了现有的优化模型评估方法。

**结果:** 实验表明，尽管LLMs能够准确地复述转换过程，但最先进的开源LLM无法一致地生成正确的对偶问题，即使是针对最小的两个变量实例以及衍生任务如正确性验证和错误分类时也如此。

**结论:** 研究结果揭示了LLMs在处理复杂推理任务时的局限性，并对教育者、学生以及大型推理系统的开发者提出了思考和启示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DualSchool%3A+How+Reliable+are+LLMs+for+Optimization+Education%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21775&send_immediately=true&force_search=false)

**原文摘要:** Consider the following task taught in introductory optimization courses which
addresses challenges articulated by the community at the intersection of
(generative) AI and OR: generate the dual of a linear program. LLMs, being
trained at web-scale, have the conversion process and many instances of Primal
to Dual Conversion (P2DC) at their disposal. Students may thus reasonably
expect that LLMs would perform well on the P2DC task. To assess this
expectation, this paper introduces DualSchool, a comprehensive framework for
generating and verifying P2DC instances. The verification procedure of
DualSchool uses the Canonical Graph Edit Distance, going well beyond existing
evaluation methods for optimization models, which exhibit many false positives
and negatives when applied to P2DC. Experiments performed by DualSchool reveal
interesting findings. Although LLMs can recite the conversion procedure
accurately, state-of-the-art open LLMs fail to consistently produce correct
duals. This finding holds even for the smallest two-variable instances and for
derivative tasks, such as correctness, verification, and error classification.
The paper also discusses the implications for educators, students, and the
development of large reasoning systems.

</details>


### [38] [Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms](https://arxiv.org/abs/2505.21792)
*Yuanzhe Peng, Jieming Bian, Lei Wang, Yin Huang, Jie Xu*

**主要类别:** cs.LG

**概要:** 这篇论文系统地分析了多模态联邦学习（MFL）在三种主要联邦学习范式中的应用，提出了问题公式化、代表性训练算法，并突显了分布式环境下多模态数据带来的挑战。同时讨论了开放性挑战并为未来研究提供了见解。


<details>
  <summary>更多</summary>
  
**动机:** 当前对多模态联邦学习的研究缺乏全面的分类方法，特别是通过不同联邦学习范式的视角来组织和理解MFL。这限制了对多模态数据在各种联邦学习环境下的独特挑战的认识。

**方法:** 作者从水平联邦学习（HFL）、垂直联邦学习（VFL）和混合联邦学习（Hybrid FL）三个主要范式出发，对多模态联邦学习进行系统审查。包括问题的数学表达、代表性的训练算法回顾以及突出多模态数据在分布式设置中引入的主要挑战。

**结果:** 建立了基于不同联邦学习范式的多模态联邦学习的分类体系，揭示了多模态数据在联邦学习中带来的新挑战，并为未来的MFL研究提供了新的视角和方向。

**结论:** 多模态联邦学习面临诸多挑战，如模态异构性、隐私异构性和通信效率低下等。通过提出的分类方法，可以更好地理解和推动MFL的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Federated+Learning%3A+A+Survey+through+the+Lens+of+Different+FL+Paradigms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21792，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21792&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Federated Learning (MFL) lies at the intersection of two pivotal
research areas: leveraging complementary information from multiple modalities
to improve downstream inference performance and enabling distributed training
to enhance efficiency and preserve privacy. Despite the growing interest in
MFL, there is currently no comprehensive taxonomy that organizes MFL through
the lens of different Federated Learning (FL) paradigms. This perspective is
important because multimodal data introduces distinct challenges across various
FL settings. These challenges, including modality heterogeneity, privacy
heterogeneity, and communication inefficiency, are fundamentally different from
those encountered in traditional unimodal or non-FL scenarios. In this paper,
we systematically examine MFL within the context of three major FL paradigms:
horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we
present the problem formulation, review representative training algorithms, and
highlight the most prominent challenge introduced by multimodal data in
distributed settings. We also discuss open challenges and provide insights for
future research. By establishing this taxonomy, we aim to uncover the novel
challenges posed by multimodal data from the perspective of different FL
paradigms and to offer a new lens through which to understand and advance the
development of MFL.

</details>


### [39] [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
*Parsa Mirtaheri, Ezra Edelman, Samy Jelassi, Eran Malach, Enric Boix-Adsera*

**主要类别:** cs.LG

**概要:** 这篇论文探讨了推理时计算在改进大型语言模型推理能力方面的潜力，并通过实验证明在某些基于图连通性问题的推理场景中，顺序扩展相较于并行扩展具有指数级的优势。


<details>
  <summary>更多</summary>
  
**动机:** 尽管推理时计算展现出显著性能提升，但其最优分配策略尚不明确，尤其是在顺序扩展（如更长的思维链）和并行扩展（如多个短思维链的多数投票）之间的优先级选择问题。

**方法:** 研究者通过展示在一些基于图连通性问题的困难分布中的推理设置下，顺序扩展相较于并行扩展具有指数级优势来阐明测试时扩展的景观。他们使用一系列语言模型进行广泛实验验证理论发现，包括从头训练用于图连通性的模型以及大型推理模型。

**结果:** 实验结果支持了理论预测，在特定的推理设置中，顺序扩展确实表现出比并行扩展更好的效果。

**结论:** 在某些复杂的推理任务中，顺序扩展策略可能更为有效，特别是在解决图连通性问题时，这为优化推理时计算资源分配提供了新的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Let+Me+Think%21+A+Long+Chain-of-Thought+Can+Be+Worth+Exponentially+Many+Short+Ones，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21825&send_immediately=true&force_search=false)

**原文摘要:** Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.

</details>


### [40] [Incentivizing Permissionless Distributed Learning of LLMs](https://arxiv.org/abs/2505.21684)
*Joel Lidin, Amir Sarfi, Evangelos Pappas, Samuel Dare, Eugene Belilovsky, Jacob Steeves*

**主要类别:** cs.LG

**概要:** 本文介绍了一种名为Gauntlet的激励系统，用于奖励在分布式深度学习基础模型中做出贡献的参与者。该系统已在bittensor区块链上部署，并用于训练一个1.2B参数的语言模型，允许完全无需许可的伪梯度贡献。通过两阶段机制快速筛选节点的在线时间、可靠性和同步性，并结合核心组件估计单个伪梯度贡献前后的损失。使用OpenSkill评分系统跟踪伪梯度分数的竞争性，并引入一种新机制确保网络中的节点执行唯一计算。实际运行结果表明，基于贡献价值支付代币的1.2B模型具有竞争力，证明了该激励系统的实用性。


<details>
  <summary>更多</summary>
  
**动机:** 当前分布式深度学习系统缺乏有效的激励机制来鼓励和评估参与者的贡献，尤其是在无需许可的环境下。因此，需要设计一种能够公平评估并奖励贡献的系统，以促进大规模分布式模型的训练。

**方法:** 提出了一种名为Gauntlet的激励系统，该系统包含以下关键部分：(1) 两阶段机制快速筛选节点的在线时间、可靠性和同步性；(2) 核心组件估计单个伪梯度贡献前后的损失；(3) 使用OpenSkill评分系统跟踪伪梯度分数的竞争性；(4) 引入新机制确保节点执行唯一计算。系统部署在bittensor区块链上，用于训练1.2B参数的语言模型。

**结果:** 实际运行结果显示，基于贡献价值支付代币的1.2B模型在每次迭代基础上具有竞争力，证明了Gauntlet激励系统的实用性和有效性。

**结论:** Gauntlet激励系统可以成功应用于分布式深度学习基础模型的训练，特别是在无需许可的环境下。该系统能够有效评估和奖励参与者的贡献，促进了大规模分布式模型的训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Incentivizing+Permissionless+Distributed+Learning+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21684&send_immediately=true&force_search=false)

**原文摘要:** We describe an incentive system for distributed deep learning of foundational
models where peers are rewarded for contributions. The incentive system,
\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to
train a 1.2B LLM with completely permissionless contributions of
pseudo-gradients: no control over the users that can register or their
hardware. \textit{Gauntlet} can be applied to any synchronous distributed
training scheme that relies on aggregating updates or pseudo-gradients. We rely
on a two-stage mechanism for fast filtering of peer uptime, reliability, and
synchronization, combined with the core component that estimates the loss
before and after individual pseudo-gradient contributions. We utilized an
OpenSkill rating system to track competitiveness of pseudo-gradient scores
across time. Finally, we introduce a novel mechanism to ensure peers on the
network perform unique computations. Our live 1.2B run, which has paid out
real-valued tokens to participants based on the value of their contributions,
yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates
the utility of our incentive system.

</details>


### [41] [TuneComp: Joint Fine-tuning and Compression for Large Foundation Models](https://arxiv.org/abs/2505.21835)
*Xiangyu Chen, Jing Liu, Ye Wang, Matthew Brand, Pu, Wang, Toshiaki Koike-Akino*

**主要类别:** cs.LG

**概要:** 联合微调和压缩模型的方法显著优于其他顺序压缩方法。


<details>
  <summary>更多</summary>
  
**动机:** 在后训练期间，通常使用的压缩方法（如知识蒸馏、低秩近似和剪枝）需要先对模型进行微调，然后进行压缩。然而，这种顺序操作会牺牲性能，并且在中间步骤创建了一个比必要的更大的模型。因此，本文旨在通过直接构建一个由下游任务指导的更小模型来缩小这一差距。

**方法:** 提出了一种逐步将模型蒸馏到剪枝低秩结构的方法，从而实现模型的联合微调和压缩。

**结果:** 实验结果表明，联合微调和压缩明显优于其他顺序压缩方法。

**结论:** 通过联合微调和压缩模型，可以有效提高性能并避免创建不必要的大型中间模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TuneComp%3A+Joint+Fine-tuning+and+Compression+for+Large+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21835，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21835&send_immediately=true&force_search=false)

**原文摘要:** To reduce model size during post-training, compression methods, including
knowledge distillation, low-rank approximation, and pruning, are often applied
after fine-tuning the model. However, sequential fine-tuning and compression
sacrifices performance, while creating a larger than necessary model as an
intermediate step. In this work, we aim to reduce this gap, by directly
constructing a smaller model while guided by the downstream task. We propose to
jointly fine-tune and compress the model by gradually distilling it to a pruned
low-rank structure. Experiments demonstrate that joint fine-tuning and
compression significantly outperforms other sequential compression methods.

</details>


### [42] [AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling](https://arxiv.org/abs/2505.21695)
*Ganglou Xu*

**主要类别:** cs.LG

**概要:** Federated learning需要平衡通信效率和模型准确性，本文提出了一种轻量级方法GDA，利用一阶信息估计本地误差趋势，成为AMSFL框架的关键部分，适用于大规模多步自适应训练环境。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习在平衡通信效率与模型准确率方面面临挑战，特别是在不增加高计算成本的情况下逼近更新误差存在困难。

**方法:** 提出了一种名为Gradient Difference Approximation (GDA)的轻量级有效方法，该方法使用一阶信息来估计局部误差趋势，而无需计算完整的Hessian矩阵，并将其作为Adaptive Multi-Step Federated Learning (AMSFL)框架的关键组成部分。

**结果:** GDA方法能够有效地估计误差趋势，降低计算成本，并为大规模多步自适应训练环境提供统一的误差建模策略。

**结论:** GDA方法结合AMSFL框架可以显著提高联邦学习中的训练效率和模型性能，同时保持较低的计算复杂度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AMSFL%3A+Adaptive+Multi-Step+Federated+Learning+via+Gradient+Difference-Based+Error+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21695&send_immediately=true&force_search=false)

**原文摘要:** Federated learning faces critical challenges in balancing communication
efficiency and model accuracy. One key issue lies in the approximation of
update errors without incurring high computational costs. In this paper, we
propose a lightweight yet effective method called Gradient Difference
Approximation (GDA), which leverages first-order information to estimate local
error trends without computing the full Hessian matrix. The proposed method
forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)
framework and provides a unified error modeling strategy for large-scale
multi-step adaptive training environments.

</details>


### [43] [An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints](https://arxiv.org/abs/2505.21841)
*Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, Honghao Wei*

**主要类别:** cs.LG

**概要:** 在动态环境中，安全强化学习至关重要。本文提出了Optimistic Mirror Descent Primal-Dual (OMDPD)算法，首次解决了具有随时对抗约束的在线CMDPs问题，提供实际的安全决策保证。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法虽然在随机约束下实现了次线性遗憾，但在对抗设置中往往失败，因为约束未知、时间变化且可能被恶意设计。

**方法:** 提出了一种名为Optimistic Mirror Descent Primal-Dual (OMDPD)的新算法，该算法无需依赖Slater条件或严格已知的安全策略即可解决在线CMDPs问题。

**结果:** OMDPD算法达到了最优遗憾O(sqrt(K))和强约束违规O(sqrt(K))。此外，如果可以获得奖励和转换的准确估计，可以进一步改善这些界限。

**结论:** 本文的研究结果为对抗环境中的安全决策提供了实用的保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Optimistic+Algorithm+for+online+CMDPS+with+Anytime+Adversarial+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21841&send_immediately=true&force_search=false)

**原文摘要:** Online safe reinforcement learning (RL) plays a key role in dynamic
environments, with applications in autonomous driving, robotics, and
cybersecurity. The objective is to learn optimal policies that maximize rewards
while satisfying safety constraints modeled by constrained Markov decision
processes (CMDPs). Existing methods achieve sublinear regret under stochastic
constraints but often fail in adversarial settings, where constraints are
unknown, time-varying, and potentially adversarially designed. In this paper,
we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the
first to address online CMDPs with anytime adversarial constraints. OMDPD
achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))
without relying on Slater's condition or the existence of a strictly known safe
policy. We further show that access to accurate estimates of rewards and
transitions can further improve these bounds. Our results offer practical
guarantees for safe decision-making in adversarial environments.

</details>


### [44] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/abs/2505.21852)
*Akifumi Wachi, Kohei Miyaguchi, Takumi Tanabe, Rei Sato, Youhei Akimoto*

**主要类别:** cs.LG

**概要:** 提出了一种名为Provably Lifetime Safe RL（PLS）的方法，该方法结合了离线安全强化学习与安全策略部署，在理论上证明了其能够在保证高概率安全的同时找到接近最优的目标回报，并在实证中展示了其在安全性和奖励性能上的优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的安全强化学习方法难以在整个过程中（从学习到操作）确保策略的安全性。

**方法:** 提出的方法PLS通过离线强化学习使用回报条件监督学习来学习策略，然后在部署过程中谨慎优化有限的参数（目标回报），并利用高斯过程（GPs）进行分析。理论部分分析了目标回报和实际回报之间的数学关系，并证明了PLS能够在高概率下确保安全性同时找到接近最优的目标回报。

**结果:** 实验证明，PLS在安全性和奖励性能上均优于基线方法，实现了在确保策略终身安全的同时获得高回报的目标。

**结论:** PLS方法为实现从学习到操作的全程策略安全性提供了有效途径，并在性能上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Provable+Approach+for+End-to-End+Safe+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21852&send_immediately=true&force_search=false)

**原文摘要:** A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [45] [SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training](https://arxiv.org/abs/2505.21893)
*Xiaomeng Yang, Zhiyu Tan, Junyan Wang, Zhijian Zhou, Hao Li*

**主要类别:** cs.LG

**概要:** 在扩散模型中，现有的Direct Preference Optimization（DPO）方法存在时间步依赖的不稳定性和策略偏差。本文提出DPO-C&M方法通过裁剪和屏蔽无信息的时间步来提高稳定性，并进一步提出SDPO框架，利用重要性采样完全校正策略偏差并强调扩散过程中的信息更新。实验表明这两种方法优于标准Diffusion-DPO，且SDPO表现更优。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩散模型优化方法如Diffusion-DPO面临两个主要问题：时间步依赖的不稳定性（源于反向和前向扩散过程的不匹配以及早期噪声时间步的高梯度方差）和策略偏差（源于优化和数据收集策略之间的不匹配）。这促使研究者探索一种能解决这些问题的新方法。

**方法:** 1. 提出DPO-C&M方法：通过裁剪和屏蔽无信息的时间步来提高稳定性，同时部分缓解策略偏差。
2. 提出SDPO框架：将重要性采样纳入目标函数，以完全校正策略偏差并强调扩散过程中的信息更新。

**结果:** 在CogVideoX-2B、CogVideoX-5B和Wan2.1-1.3B上的实验表明，DPO-C&M和SDPO均优于标准Diffusion-DPO。其中，SDPO在VBench分数、人类偏好对齐和训练鲁棒性方面表现出色。

**结论:** 时间步感知和分布校正的优化在基于扩散模型的偏好学习中至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SDPO%3A+Importance-Sampled+Direct+Preference+Optimization+for+Stable+Diffusion+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21893，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21893&send_immediately=true&force_search=false)

**原文摘要:** Preference learning has become a central technique for aligning generative
models with human expectations. Recently, it has been extended to diffusion
models through methods like Direct Preference Optimization (DPO). However,
existing approaches such as Diffusion-DPO suffer from two key challenges:
timestep-dependent instability, caused by a mismatch between the reverse and
forward diffusion processes and by high gradient variance in early noisy
timesteps, and off-policy bias arising from the mismatch between optimization
and data collection policies. We begin by analyzing the reverse diffusion
trajectory and observe that instability primarily occurs at early timesteps
with low importance weights. To address these issues, we first propose
DPO-C\&M, a practical strategy that improves stability by clipping and masking
uninformative timesteps while partially mitigating off-policy bias. Building on
this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a
principled framework that incorporates importance sampling into the objective
to fully correct for off-policy bias and emphasize informative updates during
the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and
Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,
with SDPO achieving superior VBench scores, human preference alignment, and
training robustness. These results highlight the importance of timestep-aware,
distribution-corrected optimization in diffusion-based preference learning.

</details>


### [46] [Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization](https://arxiv.org/abs/2505.21895)
*Cameron Gordon, Yiping Ji, Hemanth Saratchandran, Paul Albert, Simon Lucey*

**主要类别:** cs.LG

**概要:** Low-Rank Adaptation (LoRA) 是一种参数高效的微调方法，但低秩约束限制了表示能力。本研究探讨了将固定频率的正弦变换应用于量化后的LoRA适配器中，以保持压缩模型的效果。研究表明，正弦非线性带来的表达能力提升在量化后仍然存在，从而实现高度压缩的适配器且性能损失可忽略。该方法在语言、视觉和文本到图像生成任务中验证有效，显著节省内存的同时保持竞争力的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LoRA通过低秩矩阵乘积减少了可训练参数，但低秩约束限制了其表示能力，导致性能不如全秩微调。Ji等人提出对低秩适配器应用固定频率的正弦变换，增加稳定秩而不增加额外参数。这启发了本研究，探讨是否可以在模型压缩后的后训练量化(Post-Training Quantization)中应用相同的正弦激活技术。

**方法:** 本研究扩展了正弦变换框架至量化后的LoRA适配器。通过理论分析表明，量化适配器的稳定秩与其全精度对应物紧密相关。这为即使在量化环境下使用此类秩增强函数提供了依据。

**结果:** 结果表明，正弦非线性带来的表达能力提升在量化后仍然存在，能够产生高度压缩的适配器，同时性能损失可忽略不计。

**结论:** 本研究成功将正弦激活技术应用于量化后的LoRA适配器，证明其在压缩模型中的有效性。这种方法在多种微调任务（包括语言、视觉和文本到图像生成）中实现了显著的内存节省，同时保持了竞争力的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compressing+Sine-Activated+Low-Rank+Adapters+through+Post-Training+Quantization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21895，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21895&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to quantized LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a quantized adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under quantization. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
quantization, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.

</details>


### [47] [LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing](https://arxiv.org/abs/2505.21732)
*Ruijie Zhang, Ziyue Liu, Zhengyang Wang, Zheng Zhang*

**主要类别:** cs.LG

**概要:** 训练如ViTs和LLMs等基础模型需要巨大的计算成本。低秩矩阵或张量分解提供了一个参数效率更高的替代方案，但通常会因受限的参数空间而降低性能。本文提出了一个简单且有效的即插即用模块——潜在交叉（LaX），通过在低秩子空间之间启用信息流来增强低秩模型的能力。在使用比全秩基线少2-3倍参数的情况下，LaX提升了低秩模型的性能以匹配或超过全秩基线。当与低秩适配器（如LoRA）结合用于微调LLaMA-7/13B时，LaX能够在算术和常识推理任务上持续提升性能，且代价可忽略不计。


<details>
  <summary>更多</summary>
  
**动机:** 当前训练大型基础模型（如ViTs和LLMs）需要大量计算资源，而低秩矩阵或张量分解虽然提供了参数效率更高的方法，但往往由于参数空间的限制导致性能下降。因此，需要一种方法能够在保持参数效率的同时，避免性能损失并提高模型能力。

**方法:** 作者提出了一种名为潜在交叉（LaX）的模块，该模块能够作为即插即用组件加入到模型中。LaX通过在低秩子空间之间启用信息流动，从而增强了低秩模型的能力。此方法在ViT-Base/Large和类似LLaMA的模型上进行了广泛的验证，这些模型的参数范围从60M到1B。此外，LaX还与低秩适配器（如LoRA）结合用于微调LLaMA-7/13B模型。

**结果:** 实验结果表明，LaX能够在使用比全秩基线少2-3倍参数的情况下，使低秩模型的性能达到或超过全秩基线。同时，在微调LLaMA-7/13B时，LaX显著提高了模型在算术和常识推理任务上的表现，并且几乎没有任何额外的成本。

**结论:** LaX模块是一种有效的方法，可以显著提升低秩模型的性能，使其在参数效率更高的情况下达到甚至超越全秩模型的效果。这为大规模模型的高效训练和微调提供了一种新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LaX%3A+Boosting+Low-Rank+Training+of+Foundation+Models+via+Latent+Crossing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21732&send_immediately=true&force_search=false)

**原文摘要:** Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.

</details>


### [48] [Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding](https://arxiv.org/abs/2505.21908)
*Hanyin Wang, Zhenbang Wu, Gururaj Kolar, Hariprasad Korsapati, Brian Bartlett, Bryan Hull, Jimeng Sun*

**主要类别:** cs.LG

**概要:** DRG-Sapphire 是一种基于 Qwen2.5-7B 的大型语言模型，通过大规模强化学习（RL）实现从临床笔记中自动化 DRG 编码。它使用基于规则的奖励和 Group Relative Policy Optimization (GRPO) 方法进行训练，并在 MIMIC-IV 基准测试中达到最先进的准确率，同时生成医生验证的推理理由以增强可解释性。研究表明，在知识密集型、OOD 任务中，RL 表现与监督微调数据量的对数呈近似线性关系，强调了基础模型编码领域知识的重要性。


<details>
  <summary>更多</summary>
  
**动机:** DRG 编码对于医院的报销和运营至关重要，但目前需要大量人工操作。现有的大型语言模型由于预训练语料库中缺乏私有临床或计费数据，难以处理 DRG 编码任务。

**方法:** DRG-Sapphire 基于 Qwen2.5-7B 构建，并采用 Group Relative Policy Optimization (GRPO) 和基于规则的奖励进行训练。该方法引入了一系列强化学习改进，以应对特定领域的挑战。

**结果:** DRG-Sapphire 在 MIMIC-IV 基准上达到了最先进的准确率，并生成了医生验证的推理理由，显著提高了可解释性。此外，研究发现 RL 性能与监督微调示例数量的对数成近似线性关系。

**结论:** 对于 OOD 任务如 DRG 编码，强化学习的有效性受到基础模型中编码领域知识的限制。因此，扩展监督微调可能比单独扩展强化学习更有效且计算效率更高。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+for+Out-of-Distribution+Reasoning+in+LLMs%3A+An+Empirical+Study+on+Diagnosis-Related+Group+Coding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21908，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21908&send_immediately=true&force_search=false)

**原文摘要:** Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.

</details>


### [49] [Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing](https://arxiv.org/abs/2505.21918)
*Haruki Kai, Tsuyoshi Okita*

**主要类别:** cs.LG

**概要:** 本研究开发了一种基于传感器信号输入的人类活动识别深度学习算法，通过改进的n维数值处理Transformer模型相较于传统Transformer在五个数据集上提高了10%-15%的准确率。


<details>
  <summary>更多</summary>
  
**动机:** 人类活动识别任务需要高效处理传感器数据的方法，而传统的Transformer模型虽适用但仍有改进空间以提升性能。

**方法:** 构建了一个基于Transformer架构的预训练语言模型，并提出了增强版的n维数值处理Transformer，包含线性层嵌入、基于分箱的预处理和输出层线性变换三个关键特性。

**结果:** 在五个不同数据集上的实验结果表明，与普通Transformer相比，提出的模型提升了10%-15%的准确率。

**结论:** 所提出的增强型n维数值处理Transformer能够显著提高人类活动识别任务的性能，具有广泛的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-supervised+Learning+Method+Using+Transformer+for+Multi-dimensional+Sensor+Data+Processing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21918，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21918&send_immediately=true&force_search=false)

**原文摘要:** We developed a deep learning algorithm for human activity recognition using
sensor signals as input. In this study, we built a pretrained language model
based on the Transformer architecture, which is widely used in natural language
processing. By leveraging this pretrained model, we aimed to improve
performance on the downstream task of human activity recognition. While this
task can be addressed using a vanilla Transformer, we propose an enhanced
n-dimensional numerical processing Transformer that incorporates three key
features: embedding n-dimensional numerical data through a linear layer,
binning-based pre-processing, and a linear transformation in the output layer.
We evaluated the effectiveness of our proposed model across five different
datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%
improvements in accuracy.

</details>


### [50] [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
*M. Reza Ebrahimi, Roland Memisevic*

**主要类别:** cs.LG

**概要:** 这篇论文重新审视了循环神经网络中隐藏单元的作用，提出隐藏单元不仅是被动的记忆存储，还是网络计算中的主动参与者。通过研究双线性操作，作者展示了其在状态跟踪任务中表示隐藏状态演化的自然归纳偏置，并建立了与任务复杂度相对应的层次结构。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究多关注隐藏单元在记忆建模中的作用，而较少探讨隐藏单元作为网络计算主动参与者的角色。

**方法:** 重新分析双线性操作，展示隐藏单元和输入嵌入之间的乘法交互如何为状态跟踪任务提供自然的归纳偏置，并建立一个基于任务复杂度的层次结构。

**结果:** 理论上和实证上证明了双线性操作在状态跟踪任务中的有效性，并揭示了线性循环网络位于该层次结构的最低复杂度中心。

**结论:** 隐藏单元在循环神经网络中不仅仅是记忆的载体，还可以通过双线性操作主动贡献于网络行为，这为设计更复杂的网络架构提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Bi-Linear+State+Transitions+in+Recurrent+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21749&send_immediately=true&force_search=false)

**原文摘要:** The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.

</details>


### [51] [FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design](https://arxiv.org/abs/2505.21923)
*Asal Mehradfar, Xuzhe Zhao, Yilun Huang, Emir Ceyani, Yankai Yang, Shihao Han, Hamidreza Aghasi, Salman Avestimehr*

**主要类别:** cs.LG

**概要:** FALCON是一个统一的机器学习框架，用于全自动、基于规格的模拟电路综合。它通过性能驱动的分类器选择电路拓扑结构，并使用自定义的边中心图神经网络进行参数推断。在大规模数据集上训练和评估，FALCON在拓扑推断中表现出>99%的准确率，<10%的性能预测相对误差，并能在不到1秒的时间内完成布局感知设计。


<details>
  <summary>更多</summary>
  
**动机:** 设计符合性能规范的模拟电路是一个复杂的过程，包括拓扑选择、参数推断和布局可行性。需要一个能够实现全自动、基于规格的模拟电路综合的系统。

**方法:** FALCON首先使用性能驱动的分类器选择合适的电路拓扑结构，然后使用定制的边中心图神经网络将电路拓扑和参数映射到性能，从而通过学习的前向模型进行基于梯度的参数推断。推断过程由可微分的布局成本指导，并受到设计规则的约束。

**结果:** 在包含1M模拟毫米波电路的大规模自定义数据集上训练和评估，FALCON展示了>99%的拓扑推断准确率，<10%的性能预测相对误差，以及每实例不到1秒的高效布局感知设计。

**结论:** 这些结果表明，FALCON是端到端模拟电路设计自动化的实用且可扩展的基础模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FALCON%3A+An+ML+Framework+for+Fully+Automated+Layout-Constrained+Analog+Circuit+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21923&send_immediately=true&force_search=false)

**原文摘要:** Designing analog circuits from performance specifications is a complex,
multi-stage process encompassing topology selection, parameter inference, and
layout feasibility. We introduce FALCON, a unified machine learning framework
that enables fully automated, specification-driven analog circuit synthesis
through topology selection and layout-constrained optimization. Given a target
performance, FALCON first selects an appropriate circuit topology using a
performance-driven classifier guided by human design heuristics. Next, it
employs a custom, edge-centric graph neural network trained to map circuit
topology and parameters to performance, enabling gradient-based parameter
inference through the learned forward model. This inference is guided by a
differentiable layout cost, derived from analytical equations capturing
parasitic and frequency-dependent effects, and constrained by design rules. We
train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave
circuits, generated and simulated using Cadence Spectre across 20
expert-designed topologies. Through this evaluation, FALCON demonstrates >99\%
accuracy in topology inference, <10\% relative error in performance prediction,
and efficient layout-aware design that completes in under 1 second per
instance. Together, these results position FALCON as a practical and extensible
foundation model for end-to-end analog circuit design automation.

</details>


### [52] [Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals](https://arxiv.org/abs/2505.21750)
*Vivienne Huiling Wang, Tinghuai Wang, Joni Pajarinen*

**主要类别:** cs.LG

**概要:** 本文提出了一种结合条件扩散模型和高斯过程（GP）先验的方法，用于生成复杂的子目标并利用GP的不确定性量化，从而在分层强化学习中提高样本效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 分层强化学习中的关键挑战是低级策略随时间变化，使得高级策略难以生成有效的子目标。因此，高级策略需要捕捉复杂的子目标分布，并考虑估计中的不确定性。

**方法:** 提出了一种方法，该方法训练一个由高斯过程（GP）先验正则化的条件扩散模型，以生成各种复杂的子目标，同时利用GP的不确定性量化。基于此框架，开发了一种从扩散策略和GP预测均值中选择子目标的策略。

**结果:** 与先前的HRL方法相比，在具有挑战性的连续控制基准测试中，该方法在样本效率和性能方面表现更优。

**结论:** 所提出的结合条件扩散模型和GP先验的方法可以有效解决HRL中的问题，并在样本效率和性能上优于之前的HRL方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Reinforcement+Learning+with+Uncertainty-Guided+Diffusional+Subgoals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21750，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21750&send_immediately=true&force_search=false)

**原文摘要:** Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.

</details>


### [53] [Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection](https://arxiv.org/abs/2505.21938)
*Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, Jinhang Zuo*

**主要类别:** cs.LG

**概要:** 提出了一种更实际的对抗模型Fake Data Injection，通过注入有限数量的虚假反馈样本误导UCB和Thompson Sampling算法选择目标臂，实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的对抗随机bandits方法依赖不现实的假设，如每轮奖励操纵和无界扰动，这限制了它们在现实世界系统中的相关性。

**方法:** 提出了Fake Data Injection威胁模型，该模型允许攻击者仅向学习者的记录中注入有限数量的有界的虚假反馈样本，并设计了有效的攻击策略，考虑了奖励值的幅度约束和数据注入的时间约束。

**结果:** 理论分析表明，这些攻击可以使UCB和Thompson Sampling算法在几乎所有的轮次中选择目标臂，同时仅产生次线性的攻击成本。实验验证了策略的有效性，揭示了广泛使用的随机bandit算法在实际对抗场景下的显著漏洞。

**结论:** Fake Data Injection模型及其攻击策略揭示了现有随机bandit算法在实际对抗环境中的脆弱性，为未来的安全性研究提供了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Practical+Adversarial+Attacks+on+Stochastic+Bandits+via+Fake+Data+Injection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21938，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21938&send_immediately=true&force_search=false)

**原文摘要:** Adversarial attacks on stochastic bandits have traditionally relied on some
unrealistic assumptions, such as per-round reward manipulation and unbounded
perturbations, limiting their relevance to real-world systems. We propose a
more practical threat model, Fake Data Injection, which reflects realistic
adversarial constraints: the attacker can inject only a limited number of
bounded fake feedback samples into the learner's history, simulating legitimate
interactions. We design efficient attack strategies under this model,
explicitly addressing both magnitude constraints (on reward values) and
temporal constraints (on when and how often data can be injected). Our
theoretical analysis shows that these attacks can mislead both Upper Confidence
Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in
nearly all rounds while incurring only sublinear attack cost. Experiments on
synthetic and real-world datasets validate the effectiveness of our strategies,
revealing significant vulnerabilities in widely used stochastic bandit
algorithms under practical adversarial scenarios.

</details>


### [54] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov*

**主要类别:** cs.LG

**概要:** 本文从联想记忆(AM)的角度研究了扩散模型，揭示了在小数据和大数据情况下不同的记忆阶段，并且发现了边界转换时出现的虚假状态(spurious states)，并进行了理论预测和实证验证。


<details>
  <summary>更多</summary>
  
**动机:** 由于经典的Hopfield模型在达到临界记忆负载时会出现虚假状态，从而导致回忆错误，因此本文试图从联想记忆的角度来研究扩散模型，以理解其记忆与泛化现象。

**方法:** 将扩散模型的训练阶段视为记忆编码，生成阶段视为记忆检索。分析了在小数据和大数据两种情况下的模型行为：小数据情况下，网络在每个样本周围创建吸引盆；大数据情况下，随着训练集增大，新的吸引子状态对应于生成样本的流形。同时探讨了虚假状态的出现及其特性。

**结果:** 发现在小数据阶段扩散模型表现出强烈的记忆化现象，在大数据阶段则出现了新的吸引子状态，这些状态未出现在训练集中但具有明确的吸引盆。此外，还对虚假状态的存在进行了理论预测和实证验证。

**结论:** 本文提供了一个通过联想记忆视角理解扩散模型记忆-泛化现象的新方法，解释了虚假状态的产生机制，并通过实验验证了理论预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memorization+to+Generalization%3A+Emergence+of+Diffusion+Models+from+Associative+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21777，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21777&send_immediately=true&force_search=false)

**原文摘要:** Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [55] [Estimating the Effects of Sample Training Orders for Large Language Models without Retraining](https://arxiv.org/abs/2505.22042)
*Hao Yang, Haoxuan Li, Mengyue Yang, Xu Chen, Mingming Gong*

**主要类别:** cs.LG

**概要:** 在大型语言模型（LLMs）中，训练样本的顺序对其外部性能和内部学习动态都有显著影响。传统研究此效应的方法通常需要对不同样本顺序重新训练模型，这对LLMs来说计算上不可行。本文提出一种无需重新训练的框架，通过近似Adam优化器更新和随机投影方法存储中间检查点，高效估计任意训练样本顺序下的模型参数。该框架应用于两个下游研究问题：1）设计LLMs的训练课程；2）分析LLMs的记忆和泛化效果。


<details>
  <summary>更多</summary>
  
**动机:** 研究训练样本顺序对LLMs性能和学习动态的影响，但由于传统方法需要大量重新训练，计算成本过高，因此需要一种更高效的解决方案。

**方法:** 设计了一种无需重新训练的框架，利用一阶和二阶泰勒展开近似Adam优化器更新，并结合随机投影方法存储中间检查点，以高效估计模型参数。然后将该框架应用于两个研究问题：1）提出一种新的课程学习策略，基于估计的模型性能优化样本调度；2）分析训练样本位置对LLMs记忆和泛化能力的影响。

**结果:** 通过广泛的实验验证了该框架在重现真实模型性能方面的有效性，并展示了其在优化LLMs训练课程和分析记忆与泛化效果方面的潜力。

**结论:** 提出的无需重新训练的框架能够有效估计不同训练样本顺序下的模型参数，为优化LLMs训练课程和分析其记忆与泛化效果提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimating+the+Effects+of+Sample+Training+Orders+for+Large+Language+Models+without+Retraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22042&send_immediately=true&force_search=false)

**原文摘要:** The order of training samples plays a crucial role in large language models
(LLMs), significantly impacting both their external performance and internal
learning dynamics. Traditional methods for investigating this effect generally
require retraining the model with various sample orders, which is
computationally infeasible for LLMs. In this work, we improve traditional
methods by designing a retraining-free framework. By approximating Adam
optimizer updates with first- and second-order Taylor expansions and utilizing
random projection methods to store intermediate checkpoints, our framework can
efficiently estimate model parameters for arbitrary training sample orders.
Next, we apply our framework to two downstream research problems: (1) Training
curriculum design for LLMs -- we base our retraining-free framework to propose
a novel curriculum learning strategy that augments curriculum proposals with
estimated model performances, enabling more informed sample scheduling. (2)
LLMs' memorization and generalization effect analysis -- we use our
retraining-free framework to estimate how the positions of training samples
influence LLMs' capacity for memorization and generalization. We conduct
extensive experiments to validate the effectiveness of our retraining-free
framework in reproducing the true model performances, and further demonstrate
its potential in optimizing LLM training curricula and analyzing the
memorization and generalization effects of LLMs.

</details>


### [56] [P-DROP: Poisson-Based Dropout for Graph Neural Networks](https://arxiv.org/abs/2505.21783)
*Hyunsik Yun*

**主要类别:** cs.LG

**概要:** 提出了一种基于泊松过程的节点选择策略，通过异步局部更新防止图神经网络中的过平滑问题，并在标准数据集上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 图神经网络（GNNs）中的过平滑问题是由于重复的消息传递导致节点表示收敛并失去辨别能力，为了解决这一问题。

**方法:** 提出了一种基于泊松过程的新型节点选择策略，为每个节点配备独立的泊松时钟，实现结构感知的随机更新，从而保留结构多样性。该方法可应用于替代基于dropout的正则化和动态子图训练方案。

**结果:** 实验结果表明，在Cora、Citeseer和Pubmed等标准基准数据集上，所提出的泊松方法相较于传统的Dropout、DropEdge和DropNode方法具有竞争力或更高的准确性，尤其是在训练后期阶段。

**结论:** 基于泊松过程的节点选择策略能够有效缓解GNN中的过平滑问题，提升模型性能，特别是在训练后期。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是P-DROP%3A+Poisson-Based+Dropout+for+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21783，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21783&send_immediately=true&force_search=false)

**原文摘要:** Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),
where repeated message passing causes node representations to converge and lose
discriminative power. To address this, we propose a novel node selection
strategy based on Poisson processes, introducing stochastic but structure-aware
updates. Specifically, we equip each node with an independent Poisson clock,
enabling asynchronous and localized updates that preserve structural diversity.
We explore two applications of this strategy: as a replacement for
dropout-based regularization and as a dynamic subgraph training scheme.
Experimental results on standard benchmarks (Cora, Citeseer, Pubmed)
demonstrate that our Poisson-based method yields competitive or improved
accuracy compared to traditional Dropout, DropEdge, and DropNode approaches,
particularly in later training stages.

</details>


### [57] [The Resurrection of the ReLU](https://arxiv.org/abs/2505.22074)
*Coşku Can Horuz, Geoffrey Kasenbacher, Saya Higuchi, Sebastian Kairat, Jendrik Stoltz, Moritz Pesl, Bernhard A. Moser, Christoph Linse, Thomas Martinetz, Sebastian Otte*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的正则化方法SUGAR（surrogate gradient learning for ReLU），它在前向传播中保留了标准ReLU函数，而在反向传播中用平滑的替代梯度取代ReLU的导数，以避免梯度消失问题。实验表明，SUGAR可以显著提高卷积网络架构（如VGG-16和ResNet-18）的泛化性能，提供更稀疏的激活，并复活死亡的ReLU。此外，在现代架构（如Conv2NeXt和Swin Transformer）中用SUGAR替代GELU也能获得竞争性甚至稍优的性能。这挑战了高级激活函数对于最佳性能是必要的传统观念，证明了传统的ReLU在适当处理梯度的情况下仍能作为强大的经典激活函数广泛适用于深度学习视觉模型。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先进的激活函数（如GELU、SELU和SiLU）因其平滑梯度和改进的收敛特性而受到欢迎，但经典的ReLU由于其简单性和其他拓扑优势仍然具有吸引力。然而，ReLU容易出现梯度为零的问题（即“死亡ReLU”问题），这限制了其整体效果。因此，研究者试图解决ReLU的这一缺陷，同时保持其优点。

**方法:** 提出了SUGAR（surrogate gradient learning for ReLU），一种新型的插件式正则化方法。SUGAR在前向传播中使用标准ReLU函数，但在反向传播中用平滑的替代函数代替ReLU的导数，从而避免梯度消失。

**结果:** SUGAR与精心选择的替代函数结合时，能够显著提升VGG-16和ResNet-18等卷积网络的泛化性能，同时提供更稀疏的激活并复活死亡的ReLU。即使在现代架构（如Conv2NeXt和Swin Transformer）中用SUGAR替代GELU，也能获得竞争性甚至稍优的性能。

**结论:** 研究结果表明，传统的ReLU通过适当的梯度处理（如使用SUGAR），可以在广泛的深度学习视觉模型中作为强大且通用的经典激活函数，挑战了必须使用高级激活函数才能达到最佳性能的传统观念。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Resurrection+of+the+ReLU，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22074&send_immediately=true&force_search=false)

**原文摘要:** Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent sparsity, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing sparser activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.

</details>


### [58] [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
*Yana Veitsman, Mayank Jobanputra, Yash Sarrof, Aleksandra Bakalova, Vera Demberg, Ellie Pavlick, Michael Hahn*

**主要类别:** cs.LG

**概要:** 研究探讨了大规模预训练语言模型（LLMs）在序列到序列任务中的架构限制，特别是在检索和复制任务中。通过C-RASP框架研究长度泛化问题，发现预训练模型在诱导（向右检索）任务上表现优于反诱导（向左检索）任务，并揭示了这种不对称性与Transformer内部电路强度差异的关联。实验验证了理论保证下的针对性微调可以消除这种不对称性，但预训练并不能克服基本的长度泛化限制。


<details>
  <summary>更多</summary>
  
**动机:** 虽然Transformers在某些序列到序列任务中存在理论限制，但这些限制是否影响大规模预训练语言模型（LLMs）尚不清楚。此外，由于模型规模和预训练数据量巨大，LLMs可能在实践中有效克服这些限制。因此，研究者希望探索这些架构限制在预训练后的表现形式。

**方法:** 研究者设计了一组受Liu等人启发的检索和复制任务，并使用C-RASP框架来研究长度泛化问题。通过实证观察和机制分析，探讨了预训练模型在不同方向上的检索能力以及其背后的电路强度差异。最后，通过实际任务实验验证了研究发现。

**结果:** 实证结果显示，预训练模型在诱导任务（向右检索）上表现优于反诱导任务（向左检索）。这种不对称性可以通过理论保证的针对性微调加以消除。机制分析表明，这种不对称性与Transformer内部诱导和反诱导电路的强度差异有关。

**结论:** 预训练有选择地增强了某些Transformer的能力，但未能克服其基本的长度泛化限制。这提示我们需要更深入理解Transformer架构的局限性以及如何通过特定方法改进它们。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Born+a+Transformer+--+Always+a+Transformer%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21785，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21785&send_immediately=true&force_search=false)

**原文摘要:** Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [59] [Inclusive, Differentially Private Federated Learning for Clinical Data](https://arxiv.org/abs/2505.22108)
*Santhosh Parampottupadam, Melih Coşğun, Sarthak Pati, Maximilian Zenk, Saikat Roy, Dimitrios Bounias, Benjamin Hamm, Sinem Sav, Ralf Floca, Klaus Maier-Hein*

**主要类别:** cs.LG

**概要:** 联合学习（FL）在训练临床AI模型方面具有潜力，但其实际应用受到隐私、资源限制和合规性挑战的阻碍。现有的差分隐私（DP）方法通常应用统一噪声，这会不成比例地降低模型性能。本文提出了一种新的合规感知FL框架，通过根据可量化的客户端合规分数自适应调整噪声来增强DP。此外，还引入了一个基于关键医疗和安全标准的合规评分工具，以促进跨不同临床环境的安全、包容和平等参与。实验表明，在公共数据集上整合资源不足、合规性较低的诊所与高度监管的机构，相较于传统FL，准确率提高了15%。这项工作通过平衡隐私、合规性和性能，使FL成为全球医疗保健中现实世界临床工作流程的可行解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 联合学习（FL）为训练临床AI模型提供了有希望的方法，但在实际应用中受到隐私、资源限制和合规性问题的阻碍。现有的差分隐私（DP）方法通常应用统一噪声，这会不成比例地降低模型性能，即使在合规性良好的机构中也是如此。

**方法:** 提出了一种新的合规感知FL框架，通过根据可量化的客户端合规分数自适应调整噪声来增强DP。此外，还引入了一个基于关键医疗和安全标准的合规评分工具，以促进跨不同临床环境的安全、包容和平等参与。

**结果:** 在公共数据集上的广泛实验证明，整合资源不足、合规性较低的诊所与高度监管的机构，相较于传统FL，准确率提高了15%。

**结论:** 这项工作通过平衡隐私、合规性和性能，使FL成为全球医疗保健中现实世界临床工作流程的可行解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inclusive%2C+Differentially+Private+Federated+Learning+for+Clinical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22108，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22108&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) offers a promising approach for training clinical AI
models without centralizing sensitive patient data. However, its real-world
adoption is hindered by challenges related to privacy, resource constraints,
and compliance. Existing Differential Privacy (DP) approaches often apply
uniform noise, which disproportionately degrades model performance, even among
well-compliant institutions. In this work, we propose a novel compliance-aware
FL framework that enhances DP by adaptively adjusting noise based on
quantifiable client compliance scores. Additionally, we introduce a compliance
scoring tool based on key healthcare and security standards to promote secure,
inclusive, and equitable participation across diverse clinical settings.
Extensive experiments on public datasets demonstrate that integrating
under-resourced, less compliant clinics with highly regulated institutions
yields accuracy improvements of up to 15% over traditional FL. This work
advances FL by balancing privacy, compliance, and performance, making it a
viable solution for real-world clinical workflows in global healthcare.

</details>


### [60] [The quest for the GRAph Level autoEncoder (GRALE)](https://arxiv.org/abs/2505.22109)
*Paul Krzakala, Gabriel Melo, Charlotte Laclau, Florence d'Alché-Buc, Rémi Flamary*

**主要类别:** cs.LG

**概要:** GRALE是一种新的图自动编码器，通过最优传输启发的损失函数和可微分节点匹配模块，结合AlphaFold的核心组件Evoformer，实现了对不同大小图的编码和解码。它在模拟和分子数据上的实验表明，其预训练形式具有高度通用性，适用于多种下游任务。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于图的学习受到广泛关注，但图表示学习仍然是一个挑战性的任务，其解决可能影响化学或生物学等关键应用领域。

**方法:** 引入了GRALE，一种新的图自动编码器，它可以将不同大小的图编码和解码到共享嵌入空间。使用最优传输启发的损失函数来比较原始和重建的图，并利用与编码器和解码器一起联合训练的可微分节点匹配模块。该方法基于AlphaFold的核心组件Evoformer，扩展支持图编码和解码。

**结果:** 数值实验表明，GRALE能够实现高度通用的预训练形式，适用于从分类、回归到更复杂的图插值、编辑、匹配和预测等多种下游任务。

**结论:** GRALE提供了一种有效的图表示学习方法，具有广泛的适用性和良好的性能，在多个下游任务中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+quest+for+the+GRAph+Level+autoEncoder+%28GRALE%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22109&send_immediately=true&force_search=false)

**原文摘要:** Although graph-based learning has attracted a lot of attention, graph
representation learning is still a challenging task whose resolution may impact
key application fields such as chemistry or biology. To this end, we introduce
GRALE, a novel graph autoencoder that encodes and decodes graphs of varying
sizes into a shared embedding space. GRALE is trained using an Optimal
Transport-inspired loss that compares the original and reconstructed graphs and
leverages a differentiable node matching module, which is trained jointly with
the encoder and decoder. The proposed attention-based architecture relies on
Evoformer, the core component of AlphaFold, which we extend to support both
graph encoding and decoding. We show, in numerical experiments on simulated and
molecular data, that GRALE enables a highly general form of pre-training,
applicable to a wide range of downstream tasks, from classification and
regression to more complex tasks such as graph interpolation, editing,
matching, and prediction.

</details>


### [61] [Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer](https://arxiv.org/abs/2505.22199)
*Xinyue Hu, Zhibin Duan, Bo Chen, Mingyuan Zhou*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种新的方法——贝叶斯非负决策层（BNDL），通过将深度神经网络重新表述为条件贝叶斯非负因子分析，利用随机潜在变量来建模复杂依赖关系，提供强大的不确定性估计，并通过稀疏性和非负性促进解缠表示和决策层的学习。实验表明，BNDL不仅提高了模型的准确性，还提供了可靠的不确定性估计和更好的可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度神经网络因其强大的表达能力而取得了显著成功，但大多数模型在实际应用中难以满足不确定性估计的要求。此外，深度神经网络的纠缠特性导致了多方面的挑战，其中各种局部解释技术揭示了多个不相关的特征影响决策，从而削弱了模型的可解释性。因此，需要一种方法来解决这些问题，同时提高模型的准确性和可解释性。

**方法:** 作者提出了贝叶斯非负决策层（BNDL），将深度神经网络重新表述为条件贝叶斯非负因子分析。该方法利用随机潜在变量建模复杂依赖关系，提供强大的不确定性估计。此外，潜在变量的稀疏性和非负性鼓励模型学习解缠表示和决策层。为了实现这一目标，作者还开发了一种相应的变分推理方法，使用Weibull变分推理网络来近似潜在变量的后验分布。

**结果:** 实验结果表明，BNDL通过增强解缠能力，不仅提高了模型的准确性，还提供了可靠的不确定性估计和更好的可解释性。

**结论:** BNDL是一种有效的解决方案，可以解决深度神经网络中的不确定性估计和可解释性问题。它通过引入随机潜在变量、稀疏性和非负性约束，实现了更强大的模型性能和更高的可解释性。此外，理论分析证明了BNDL能够实现有效的解缠学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Uncertainty+Estimation+and+Interpretability+via+Bayesian+Non-negative+Decision+Layer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22199，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22199&send_immediately=true&force_search=false)

**原文摘要:** Although deep neural networks have demonstrated significant success due to
their powerful expressiveness, most models struggle to meet practical
requirements for uncertainty estimation. Concurrently, the entangled nature of
deep neural networks leads to a multifaceted problem, where various localized
explanation techniques reveal that multiple unrelated features influence the
decisions, thereby undermining interpretability. To address these challenges,
we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates
deep neural networks as a conditional Bayesian non-negative factor analysis. By
leveraging stochastic latent variables, the BNDL can model complex dependencies
and provide robust uncertainty estimation. Moreover, the sparsity and
non-negativity of the latent variables encourage the model to learn
disentangled representations and decision layers, thereby improving
interpretability. We also offer theoretical guarantees that BNDL can achieve
effective disentangled learning. In addition, we developed a corresponding
variational inference method utilizing a Weibull variational inference network
to approximate the posterior distribution of the latent variables. Our
experimental results demonstrate that with enhanced disentanglement
capabilities, BNDL not only improves the model's accuracy but also provides
reliable uncertainty estimation and improved interpretability.

</details>


### [62] [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
*Stanley Yu, Vaidehi Bulusu, Oscar Yasunaga, Clayton Lau, Cole Blondin, Sean O'Brien, Kevin Zhu, Vasu Sharma*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）虽然具备强大的对话能力，但常生成虚假信息。本研究扩展了概念锥框架，用于捕捉LLM中与真相关联的多维结构。通过因果干预等方法，发现这些锥体能够跨多个LLM家族调节与真相关的特性，并且在翻转模型对事实陈述的反应、跨架构泛化以及保持无关行为方面表现出有效性。这揭示了LLM中简单真假命题的更丰富、多方向结构，并强调了概念锥作为探索抽象行为的有希望工具的价值。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先前研究表明简单的真值可以在模型内部激活中表示为单一线性方向，但这可能无法完全捕捉其底层几何结构。因此，需要一种更全面的方法来理解LLM中的真值行为。

**方法:** 扩展概念锥框架到真值领域，识别多维锥体以因果方式调解多个LLM家族中的真值相关行为。使用三种证据支持该方法：因果干预可靠地翻转模型对事实陈述的反应；学习到的锥体跨模型架构泛化；基于锥体的干预保持无关模型行为。

**结果:** 发现了多维锥体可以跨多个LLM家族调解真值相关行为，并且在因果干预、跨架构泛化和保持无关行为方面表现良好。这揭示了LLM中简单真假命题的更丰富、多方向结构。

**结论:** 概念锥是探索LLM中抽象行为的有希望工具，揭示了LLM中简单真假命题的更丰富、多方向结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Directions+to+Cones%3A+Exploring+Multidimensional+Representations+of+Propositional+Facts+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21800，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21800&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.

</details>


### [63] [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
*Yuzhen Huang, Weihao Zeng, Xingshan Zeng, Qi Zhu, Junxian He*

**主要类别:** cs.LG

**概要:** 在强化学习中，可信的验证器对可验证奖励至关重要。本文通过数学推理案例研究，分析了规则验证器和模型验证器在静态评估和RL训练中的表现及局限性。规则验证器存在误判问题，而模型验证器则易受攻击，两者都影响RL训练性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管规则验证器已被广泛用于复杂领域（如数学推理）的强化学习训练，但其可靠性和对训练过程的影响尚不明确，因此需要深入分析不同验证器的表现和局限性。

**方法:** 以数学推理为案例，全面分析了多种验证器在静态评估和强化学习训练中的表现。具体包括：1) 发现开源规则验证器在多个常用数据集上无法识别等价答案的不同形式，导致较高的假阴性率；2) 探讨模型验证器作为潜在解决方案，并分析其在静态评估中的高准确性；3) 研究模型验证器在RL训练中的脆弱性，发现其容易被攻击并产生假阳性结果。

**结果:** 规则验证器存在显著的假阴性问题，尤其是在策略模型变强时影响更大；模型验证器虽然在静态评估中表现出更高的准确性，但在RL训练中容易受到攻击，导致虚假奖励的增加。

**结论:** 规则验证器和模型验证器各有独特风险，未来需要开发更稳健的奖励系统以提升强化学习训练的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pitfalls+of+Rule-+and+Model-based+Verifiers+--+A+Case+Study+on+Mathematical+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22203，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22203&send_immediately=true&force_search=false)

**原文摘要:** Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.

</details>


### [64] [Towards Operational Automated Greenhouse Gas Plume Detection](https://arxiv.org/abs/2505.21806)
*Brian D. Bue, Jake H. Lee, Andrew K. Thorpe, Philip G. Brodrick, Daniel Cusworth, Alana Ayasse, Vassiliki Mancoridis, Anagha Satish, Shujun Xiong, Riley Duren*

**主要类别:** cs.LG

**概要:** 尽管深度学习技术有所进步，但全自动温室气体羽流检测系统的操作部署仍然是成像光谱任务的一个难以实现的目标。本文通过多源数据实验，展示了解决数据质量控制、时空偏差和建模目标对齐等关键问题后，卷积神经网络能够实现操作级别的检测性能。提出了一种多任务模型，该模型同时学习实例检测和像素级分割，为实际应用铺平了道路。最后，提供了可重现的数据、模型和代码，并制定了最佳实践和验证标准以促进未来研究。


<details>
  <summary>更多</summary>
  
**动机:** 当前全自动温室气体羽流检测系统尚未实现操作部署，而自动化对于监测自然和人为排放越来越重要。需要解决数据质量问题、时空偏差以及建模目标对齐等关键障碍。

**方法:** 利用多任务模型（卷积神经网络），该模型同时学习实例检测和像素级分割；使用来自航空和卫星仪器的多活动数据进行严格实验，评估不同排放源类型和地区的羽流检测能力。

**结果:** 证明了在解决关键障碍后，卷积神经网络可以达到操作级别的检测性能；确定了实际部署的阈值，并提供了分析就绪的数据、模型和源代码。

**结论:** 提出了最佳实践和验证标准，为未来的研究奠定了基础，推动了全自动温室气体羽流检测系统的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Operational+Automated+Greenhouse+Gas+Plume+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21806，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21806&send_immediately=true&force_search=false)

**原文摘要:** Operational deployment of a fully automated greenhouse gas (GHG) plume
detection system remains an elusive goal for imaging spectroscopy missions,
despite recent advances in deep learning approaches. With the dramatic increase
in data availability, however, automation continues to increase in importance
for natural and anthropogenic emissions monitoring. This work reviews and
addresses several key obstacles in the field: data and label quality control,
prevention of spatiotemporal biases, and correctly aligned modeling objectives.
We demonstrate through rigorous experiments using multicampaign data from
airborne and spaceborne instruments that convolutional neural networks (CNNs)
are able to achieve operational detection performance when these obstacles are
alleviated. We demonstrate that a multitask model that learns both instance
detection and pixelwise segmentation simultaneously can successfully lead
towards an operational pathway. We evaluate the model's plume detectability
across emission source types and regions, identifying thresholds for
operational deployment. Finally, we provide analysis-ready data, models, and
source code for reproducibility, and work to define a set of best practices and
validation standards to facilitate future contributions to the field.

</details>


### [65] [Solver-Free Decision-Focused Learning for Linear Optimization Problems](https://arxiv.org/abs/2505.22224)
*Senne Berden, Ali İrfan Mahmutoğulları, Dimos Tsouros, Tias Guns*

**主要类别:** cs.LG

**概要:** 本文提出了一种针对线性优化问题的无求解器训练方法，利用线性优化的几何结构，通过比较真实最优解与预计算相邻顶点的质量来构建损失函数，从而显著降低计算成本，同时保持高决策质量。


<details>
  <summary>更多</summary>
  
**动机:** 在许多现实场景中，优化问题的参数需要从上下文特征预测，形成了预测-优化问题。尽管决策聚焦学习（DFL）能够提升下游决策质量，但其计算成本较高，尤其是在每次损失评估时都需要解决带有预测参数的优化问题。因此，研究者们希望找到一种更高效的训练方法，以减少计算成本并保持良好的决策质量。

**方法:** 作者提出了一种无需求解器的训练方法，该方法基于线性优化的几何结构。具体而言，它利用了解在一个可行多面体上是最佳的当且仅当它的目标值至少与相邻顶点一样好的性质。基于此洞察，该方法将真实最优解的估计质量与预计算的相邻顶点进行比较，并以此作为损失函数进行模型训练。

**结果:** 实验结果表明，所提出的方法可以显著降低计算成本，同时维持较高的决策质量。

**结论:** 该工作为线性优化问题提供了一种有效的无求解器训练方法，这种方法不仅降低了计算成本，还保证了决策质量，对于实际应用中的预测-优化问题具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Solver-Free+Decision-Focused+Learning+for+Linear+Optimization+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22224，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22224&send_immediately=true&force_search=false)

**原文摘要:** Mathematical optimization is a fundamental tool for decision-making in a wide
range of applications. However, in many real-world scenarios, the parameters of
the optimization problem are not known a priori and must be predicted from
contextual features. This gives rise to predict-then-optimize problems, where a
machine learning model predicts problem parameters that are then used to make
decisions via optimization. A growing body of work on decision-focused learning
(DFL) addresses this setting by training models specifically to produce
predictions that maximize downstream decision quality, rather than accuracy.
While effective, DFL is computationally expensive, because it requires solving
the optimization problem with the predicted parameters at each loss evaluation.
In this work, we address this computational bottleneck for linear optimization
problems, a common class of problems in both DFL literature and real-world
applications. We propose a solver-free training method that exploits the
geometric structure of linear optimization to enable efficient training with
minimal degradation in solution quality. Our method is based on the insight
that a solution is optimal if and only if it achieves an objective value that
is at least as good as that of its adjacent vertices on the feasible polytope.
Building on this, our method compares the estimated quality of the ground-truth
optimal solution with that of its precomputed adjacent vertices, and uses this
as loss function. Experiments demonstrate that our method significantly reduces
computational cost while maintaining high decision quality.

</details>


### [66] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/abs/2505.21807)
*Tommy Xu, Zhitian Zhang, Xiangyu Sun, Lauren Kelly Zung, Hossein Hajimirsadeghi, Greg Mori*

**主要类别:** cs.LG

**概要:** 本论文提出了一种利用基于推理的大型语言模型（LLMs）在表格数据上进行更准确和可解释预测的新方法。通过引入定制奖励函数，该模型不仅提高了预测准确性，还生成了人类可理解的预测理由。实验结果表明，该模型在金融基准数据集上的表现优于大多数现有的LLMs。


<details>
  <summary>更多</summary>
  
**动机:** 尽管梯度提升机和一些深度学习模型在表格数据上表现出色，但它们通常缺乏可解释性。而大型语言模型虽然能够生成类似人类的推理和解释，但在表格数据预测方面的表现不如人意。因此，需要一种能够在提高预测准确性的同时增强可解释性的新方法。

**方法:** 提出了一种结合基于推理的大型语言模型的方法，并使用强化学习对其进行训练。引入了自定义奖励函数，以引导模型实现高预测准确性和可理解的预测理由。

**结果:** 实验结果表明，该模型在金融基准数据集上实现了有希望的表现，超越了大多数现有的大型语言模型。

**结论:** 所提出的方法通过结合强化学习训练的基于推理的大型语言模型，在表格数据预测任务中实现了更高的准确性和更好的可解释性。这为未来在表格数据上应用语言模型提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TabReason%3A+A+Reinforcement+Learning-Enhanced+Reasoning+LLM+for+Explainable+Tabular+Data+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21807&send_immediately=true&force_search=false)

**原文摘要:** Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [67] [Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer](https://arxiv.org/abs/2505.22306)
*Zehua Chen, Yuyang Miao, Liyuan Wang, Luyun Fan, Danilo P. Mandic, Jun Zhu*

**主要类别:** cs.LG

**概要:** 提出了一种名为UniCardio的多模态扩散变压器，用于重建低质量信号和合成未记录信号，超越了现有的任务特定基线。


<details>
  <summary>更多</summary>
  
**动机:** 心血管信号（如PPG、ECG和BP）反映了心血管系统的健康状况，但由于获取挑战，实时监测这些信号联合利用受到严重限制。

**方法:** 提出了一个统一生成框架下的多模态扩散变压器UniCardio，包含专门的模型架构管理和生成任务中的信号模态，以及一个持续学习范式来结合不同的模态组合。

**结果:** 在信号去噪、插补和翻译方面明显优于最近的任务特定基线，生成的信号在检测异常健康状况和估计生命体征方面的表现与真实信号相当，同时确保人类专家可解释性。

**结论:** UniCardio为推动AI辅助医疗保健的发展提供了一个有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Versatile+Cardiovascular+Signal+Generation+with+a+Unified+Diffusion+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22306&send_immediately=true&force_search=false)

**原文摘要:** Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.

</details>


### [68] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310)
*Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger, Gintare Karolina Dziugaite, Michael Curtis Mozer, Eleni Triantafillou*

**主要类别:** cs.LG

**概要:** 近期针对大型语言模型（LLMs）的遗忘方法容易受到再学习攻击的影响：被认为已经遗忘的知识可以通过在少量（甚至看似无关的）样本上微调而重新浮现。本文在视觉分类器中研究了这种现象，并发现遗忘集的准确率可以在仅通过保留集进行微调后从约50%恢复到接近100%，即无需使用任何遗忘集样本。此效果在多种遗忘方法中都可观察到，但对于从头开始重新训练并排除遗忘集的模型（黄金标准），其准确率仍保持在50%。此外，我们发现可以通过权重空间特性（特别是原始模型与遗忘模型之间的$L_2$距离和线性模式连通性）预测对再学习攻击的抵抗能力。基于这一见解，我们提出了一类新的方法，这些方法在抵御再学习攻击方面达到了最先进的水平。


<details>
  <summary>更多</summary>
  
**动机:** 研究发现现有的LLM遗忘方法容易受到再学习攻击，知识可能通过少量数据微调重新浮现。因此需要探索为何会出现这种现象，以及如何增强模型对再学习攻击的抵抗力。

**方法:** 在视觉分类器中研究实例级遗忘现象，观察遗忘集准确率在仅微调保留集时的恢复情况。分析不同遗忘方法的效果，并探讨模型权重空间特性（如$L_2$距离和线性模式连通性）与再学习攻击抗性的关系。基于这些发现，提出一类新方法以提升对再学习攻击的抵抗力。

**结果:** 发现在多种遗忘方法中，遗忘集的准确率可以通过仅微调保留集从约50%恢复到接近100%。模型对再学习攻击的抵抗能力可以通过权重空间特性预测，所提出的新方法在抵御再学习攻击方面表现优异。

**结论:** 现有遗忘方法易受再学习攻击影响，但通过分析权重空间特性可以有效预测模型的抵抗能力。基于这些见解提出的新方法在提高模型对再学习攻击的抵抗力方面取得了最佳效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Dormant+to+Deleted%3A+Tamper-Resistant+Unlearning+Through+Weight-Space+Regularization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22310&send_immediately=true&force_search=false)

**原文摘要:** Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [69] [Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations](https://arxiv.org/abs/2505.21824)
*Praveen Kumar, Vincent T. Metzger, Scott A. Malec*

**主要类别:** cs.LG

**概要:** 全球糖尿病（特别是2型糖尿病T2DM）患病率迅速上升，带来健康和经济挑战。早期发现高风险个体至关重要。我们提出一种新的无监督框架，结合非负矩阵分解（NMF）与统计技术，利用共病和用药数据识别未诊断个体的T2DM风险，提供可解释且可扩展的解决方案，助力及时干预并改善患者结果。


<details>
  <summary>更多</summary>
  
**动机:** T2DM不仅破坏血糖调节，还损害重要器官，增加发病率和死亡率，同时带来巨大的经济负担。然而，许多现有的机器学习方法依赖于监督学习，缺乏确认的阴性案例限制了其效果。因此，需要一种新的方法来克服这些限制，更有效地识别T2DM风险个体。

**方法:** 提出了一种新的无监督框架，将非负矩阵分解（NMF）与统计技术相结合。该方法通过分析已诊断T2DM患者的多重病症和多药使用模式，将其应用于估计未诊断个体的T2DM风险。

**结果:** 该方法能够识别出T2DM患者的潜在多重病症和多药使用模式，并成功应用于评估未诊断个体的T2DM风险，提供了可解释和可扩展的解决方案。

**结论:** 所提出的无监督框架为T2DM风险预测提供了一种新途径，可以协助医疗保健提供者实施及时干预，从而改善患者结果并可能减轻未来的健康和经济负担。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+Latent+Pattern+Analysis+for+Estimating+Type+2+Diabetes+Risk+in+Undiagnosed+Populations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21824，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21824&send_immediately=true&force_search=false)

**原文摘要:** The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.

</details>


### [70] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
*Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou*

**主要类别:** cs.LG

**概要:** Skywork-OR1是一种用于增强长推理链模型的高效且可扩展的强化学习实现，显著提升了LLM在多个基准测试中的准确性，并开源了所有资源。


<details>
  <summary>更多</summary>
  
**动机:** 尽管DeepSeek-R1展示了强化学习在提升大语言模型推理能力方面的潜力，但需要进一步研究以开发更有效和可扩展的RL方法，特别是在长Chain-of-Thought模型中。

**方法:** 基于DeepSeek-R1-Distill模型系列，使用强化学习优化长推理链模型。通过全面的消融研究验证训练流程核心组件的有效性，并深入探讨熵塌缩现象及其对测试性能的影响。

**结果:** Skywork-OR1-32B模型在AIME24、AIME25基准上超越了DeepSeek-R1和Qwen3-32B，在LiveCodeBench上达到相当的结果；Skywork-OR1-7B模型表现出与同类规模模型竞争的推理能力。模型分别将32B和7B模型的平均准确率提高了15.0%和13.9%。

**结论:** Skywork-OR1证明了强化学习在提升长推理链模型上的有效性，并通过开源模型权重、训练代码和数据集支持社区研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Skywork+Open+Reasoner+1+Technical+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22312&send_immediately=true&force_search=false)

**原文摘要:** The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [71] [In Search of Adam's Secret Sauce](https://arxiv.org/abs/2505.21829)
*Antonio Orvieto, Robert Gower*

**主要类别:** cs.LG

**概要:** 在训练基于Transformer的语言模型时，理解Adam优化器的显著效果是一个重要的研究课题。通过超过1,300个语言模型的广泛实证研究，比较Adam与几种已知的简化变体（如带符号梯度和带符号动量方法）。研究表明，尽管带符号动量的方法比SGD快，但即使经过精心调整，仍始终不如Adam。然而，将Adam动量参数设置为相等是一种保留接近最佳性能的选择，并且允许新的理论洞察。在这种设置下，Adam实现了一种自然的在线算法，用于估计梯度的均值和方差，该算法源于均场高斯变分推断视角。


<details>
  <summary>更多</summary>
  
**动机:** 理解Adam优化器在训练基于Transformer的语言模型中的高效性是当前优化领域的一个核心研究主题。为了深入探讨这一问题，先前提出了几种Adam的简化版本（例如带符号梯度和带符号动量方法），但其表现是否能媲美Adam尚不明确。

**方法:** 作者进行了广泛的实证研究，训练了超过1,300个不同数据配置和规模的语言模型，比较了Adam与几种已知简化变体（如带符号动量方法）的表现。特别地，他们评估了动量、裁剪设置和学习率等因素的影响。此外，还探索了将Adam的动量参数设置为相等的情况。

**结果:** 研究发现带符号动量方法虽然比标准SGD更快，但在各种调整后仍然无法达到Adam的性能水平。而将Adam的动量参数设置为相等不仅保留了接近最优的性能，还提供了新的理论见解，并赋予Adam精确的统计解释。

**结论:** Adam的优越性能不仅依赖于带符号动量，还与其特定的动量参数设置密切相关。通过将Adam的动量参数设为相等，可以得到一种自然的在线算法，从均场高斯变分推断的角度解释梯度均值和方差的估计过程。这为Adam的有效性提供了新的理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是In+Search+of+Adam%27s+Secret+Sauce，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21829&send_immediately=true&force_search=false)

**原文摘要:** Understanding the remarkable efficacy of Adam when training transformer-based
language models has become a central research topic within the optimization
community. To gain deeper insights, several simplifications of Adam have been
proposed, such as the signed gradient and signed momentum methods. In this
work, we conduct an extensive empirical study - training over 1,300 language
models across different data configurations and scales - comparing Adam to
several known simplified variants. We find that signed momentum methods are
faster than SGD, but consistently underperform relative to Adam, even after
careful tuning of momentum, clipping setting and learning rates. However, our
analysis reveals a compelling option that preserves near-optimal performance
while allowing for new insightful reformulations: constraining the Adam
momentum parameters to be equal. Beyond robust performance, this choice affords
new theoretical insights, highlights the "secret sauce" on top of signed
momentum, and grants a precise statistical interpretation: we show that Adam in
this setting implements a natural online algorithm for estimating the mean and
variance of gradients-one that arises from a mean-field Gaussian variational
inference perspective.

</details>


### [72] [Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs](https://arxiv.org/abs/2505.22358)
*Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）在持续学习（CL）场景中常面临灾难性遗忘问题。现有的CL方法和预算自适应调整方法存在局限性。为了解决这些问题，本文提出了OA-Adapter，一种新的参数高效型持续学习方法，它将动态预算调整与正交子空间学习统一在一个端到端的训练阶段。实验结果表明，OA-Adapter在准确性和参数效率上优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在持续学习场景中会遭遇灾难性遗忘，而现有的持续学习方法要么采用固定的预算分配忽视任务复杂度，要么通过多阶段范式进行预算自适应调整，导致优化和预算分配之间的潜在错位。因此需要一种能有效协调预算分配并保留先前知识的新方法。

**方法:** 提出了一种名为OA-Adapter的方法，该方法包含一个动态瓶颈维度调整机制，在单一端到端训练阶段内同时完成高效的参数预算分配和任务目标优化。并且应用正交约束于当前任务参数子空间和历史任务动态分配参数子空间之间，以有效保存先前获得的知识。

**结果:** 实验结果表明，OA-Adapter在持续学习基准测试中，不仅在准确性上超越了现有最先进方法，而且在参数使用上更为高效——在标准CL基准上平均准确率更高，同时使用的参数减少了58.5%。

**结论:** OA-Adapter是一种有效的、参数高效的持续学习方法，适合大型语言模型使用。它通过将动态预算调整和正交子空间学习相结合，克服了现有方法的缺陷，在减少参数使用的同时提高了模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Budget-Adaptive+Adapter+Tuning+in+Orthogonal+Subspaces+for+Continual+Learning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22358&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) often suffer from catastrophic forgetting in
continual learning (CL) scenarios, where performance on previously learned
tasks degrades severely while training on sequentially arriving tasks. Although
pioneering CL approaches using orthogonal subspaces can mitigate task
interference, they typically employ fixed budget allocation, neglecting the
varying complexity across tasks and layers. Besides, recent budget-adaptive
tuning methods for LLMs often adopt multi-stage paradigms that decouple
optimization and budget allocation. Such decoupling results in potential
misalignment, which hinders those approaches' practical application in CL
scenarios. To address these limitations, we propose OA-Adapter, a novel
parameter-efficient approach for continual learning in LLMs that unifies
dynamic budget adaptation with orthogonal subspace learning in a single
end-to-end training stage. Specifically, OA-Adapter introduces a dynamic
bottleneck dimension adaptation mechanism that simultaneously allocates an
efficient parameter budget and optimizes task objectives without misalignment.
To effectively preserve previously acquired knowledge while coordinating with
the dynamic budget allocation, orthogonal constraints are applied specifically
between the parameter subspace of the current task and the dynamically
allocated parameter subspaces of historical tasks. Experimental results on
continual learning benchmarks demonstrate that OA-Adapter outperforms
state-of-the-art methods in both accuracy and parameter efficiency, achieving
higher average accuracy while using 58.5% fewer parameters on the standard CL
benchmark.

</details>


### [73] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/abs/2505.22370)
*Haomiao Qiu, Miao Zhang, Ziyue Qiao, Weili Guan, Min Zhang, Liqiang Nie*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为SplitLoRA的持续学习新方法，通过低秩适应范式和理论分析来优化梯度空间划分，从而平衡模型的稳定性和可塑性，实验结果表明该方法在多个数据集上达到了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 持续学习需要模型在顺序学习多个任务时保持稳定性（保留先前学到的知识）和可塑性（有效学习新任务）。然而，现有的梯度投影方法难以适当划分梯度空间以实现稳定性和可塑性的最佳平衡。

**方法:** 基于低秩适应范式，提出了SplitLoRA方法。首先对子空间划分如何影响模型的稳定性和可塑性进行了理论分析，然后引入了一种有效的方法，推导出先前学习任务的梯度空间的最佳划分。

**结果:** 实验结果表明，在多个数据集上，所提出的方法实现了最先进的性能。

**结论:** SplitLoRA方法通过优化梯度空间的划分，在持续学习中有效地平衡了稳定性和可塑性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SplitLoRA%3A+Balancing+Stability+and+Plasticity+in+Continual+Learning+Through+Gradient+Space+Splitting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22370，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22370&send_immediately=true&force_search=false)

**原文摘要:** Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [74] [Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning](https://arxiv.org/abs/2505.22389)
*Haomiao Qiu, Miao Zhang, Ziyue Qiao, Liqiang Nie*

**主要类别:** cs.LG

**概要:** 提出了一种新的持续学习框架Perturb-and-Merge (P&M)，通过模型合并技术缓解灾难性遗忘，结合LoRA方法减少内存开销，并在多个基准数据集上达到最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习方法容易因仅依赖最近任务的参数进行推理而遭受灾难性遗忘，因此需要一种新方法来更好地整合先前任务的知识。

**方法:** P&M框架在每个任务训练后，通过先前模型和新训练的任务特定模型的凸组合构建新模型。通过理论分析推导出最优合并系数的解析解，并引入由任务向量和损失函数Hessian矩阵组成的正则化项以减轻合并过程中的性能退化。此外，利用二阶对称有限差分有效近似该正则化项，并设计了沿任务向量方向的随机扰动策略。最后，将P&M与LoRA结合以降低内存消耗。

**结果:** 在多个持续学习基准数据集上，所提出的方法达到了最先进的性能。

**结论:** P&M框架有效地缓解了灾难性遗忘，并通过与LoRA结合显著减少了内存开销，为持续学习提供了一种高效且性能优越的新方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Train+with+Perturbation%2C+Infer+after+Merging%3A+A+Two-Stage+Framework+for+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22389&send_immediately=true&force_search=false)

**原文摘要:** Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.

</details>


### [75] [Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation](https://arxiv.org/abs/2505.22391)
*Yi Zhang, Difan Zou*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法PIDDM，通过后处理蒸馏扩散模型来加强PDE约束的满足度，从而改善生成建模的物理系统准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前扩散模型在物理系统建模中无法直接对清洁样本施加PDE约束，导致Jensen's Gap问题，影响生成模型的准确性。

**方法:** 提出Physics-Informed Distillation of Diffusion Models (PIDDM)，不直接在扩散过程中加入PDE约束，而是在后处理阶段进行蒸馏以强化PDE约束。

**结果:** 在各种PDE基准测试中，PIDDM显著提高了PDE约束的满足度，并且计算开销较小，优于PIDM、DiffusionPDE和ECI-sampling等方法。

**结论:** PIDDM为将物理约束纳入扩散模型提供了更高效和有效的策略，支持单步生成、前向和反向问题求解以及从随机部分观测中重建。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Informed+Distillation+of+Diffusion+Models+for+PDE-Constrained+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22391，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22391&send_immediately=true&force_search=false)

**原文摘要:** Modeling physical systems in a generative manner offers several advantages,
including the ability to handle partial observations, generate diverse
solutions, and address both forward and inverse problems. Recently, diffusion
models have gained increasing attention in the modeling of physical systems,
particularly those governed by partial differential equations (PDEs). However,
diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate
steps, making it infeasible to directly enforce constraints on the clean sample
$\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are
typically applied to the expectation of clean samples
$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the
learned score network. However, imposing PDE constraints on the expectation
does not strictly represent the one on the true clean data, known as Jensen's
Gap. This gap creates a trade-off: enforcing PDE constraints may come at the
cost of reduced accuracy in generative modeling. To address this, we propose a
simple yet effective post-hoc distillation approach, where PDE constraints are
not injected directly into the diffusion process, but instead enforced during a
post-hoc distillation stage. We term our method as Physics-Informed
Distillation of Diffusion Models (PIDDM). This distillation not only
facilitates single-step generation with improved PDE satisfaction, but also
support both forward and inverse problem solving and reconstruction from
randomly partial observation. Extensive experiments across various PDE
benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over
several recent and competitive baselines, such as PIDM, DiffusionPDE, and
ECI-sampling, with less computation overhead. Our approach can shed light on
more efficient and effective strategies for incorporating physical constraints
into diffusion models.

</details>


### [76] [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
*Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, Yinpeng Dong*

**主要类别:** cs.LG

**概要:** 最近在大型推理模型（LRMs）方面的进展展示了在解决复杂任务（如数学和编码）方面卓越的能力。然而，这些模型在推理过程中经常表现出过度思考的现象，导致大量的计算开销。本文通过机械可解释性的视角研究过度思考的潜在机制，发现过度思考可以通过模型激活空间中的单一方向有效捕捉，并通过干预该方向缓解。然而，随着干预强度增加，效果会达到饱和甚至恶化。进一步探索发现过度思考现象与低维流形相关，高维转向方向引入的噪声限制了效果。基于此洞察，提出了一种名为流形转向的新方法，优雅地将转向方向投影到低维激活流形上。实验表明，该方法在DeepSeek-R1精炼模型中减少了高达71%的输出标记，同时在多个数学基准上保持甚至提高了准确性，并且在代码生成和基于知识的问答任务中也表现出一致的标记减少性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型推理模型在复杂任务中表现出色，但它们在推理时容易出现过度思考的问题，表现为过多的验证循环和冗余的推导过程，这导致了巨大的计算开销。为了解决这一问题，作者试图从机械可解释性的角度探讨过度思考的潜在机制并提出解决方案。

**方法:** 作者首先发现过度思考可以由模型激活空间中的单一方向有效捕捉，并通过干预该方向缓解过度思考。然而，当干预强度增加时，效果会饱和甚至恶化。因此，作者系统性地探索了激活空间，发现过度思考现象实际上与低维流形相关，而高维转向方向引入的噪声限制了干预的效果。基于这一发现，作者提出了流形转向方法，将转向方向优雅地投影到低维激活流形上，以减少干扰噪声的影响。

**结果:** 实验结果表明，流形转向方法在DeepSeek-R1精炼模型中能够减少高达71%的输出标记，同时在多个数学基准测试中保持甚至提高了准确性。此外，该方法还表现出强大的跨领域迁移能力，在代码生成和基于知识的问答任务中也能提供一致的标记减少性能。

**结论:** 本文通过机械可解释性的视角研究了大型推理模型中过度思考的潜在机制，并提出了流形转向方法。该方法显著减少了模型输出标记数量，同时保持或提升了任务准确性，展现了强大的跨领域迁移能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Overthinking+in+Large+Reasoning+Models+via+Manifold+Steering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22411&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.

</details>


### [77] [Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning](https://arxiv.org/abs/2505.21877)
*Hongyao Chen, Tianyang Xu, Xiaojun Wu, Josef Kittler*

**主要类别:** cs.LG

**概要:** 在联邦学习中，传统的批量归一化（BN）由于无法有效更新统计参数而表现不佳。为了解决这一问题，本文提出了混合批量归一化（HBN）方法，通过分离统计参数和可学习参数的更新，引入了可学习的混合分布因子，使每个计算节点能够自适应地将当前批次的统计参数与全局统计信息混合。实验结果表明，HBN在各种联邦学习场景中具有显著优势，特别是在小批量和异构数据情况下。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习面临着处理非独立同分布数据的挑战，而标准的批量归一化方法由于缺乏一致的统计参数更新方法，导致性能下降。因此，需要探索一种适合联邦学习的归一化解决方案。

**方法:** 提出了一种名为混合批量归一化（HBN）的方法，该方法将统计参数的更新与可学习参数的更新分开，并引入了一个可学习的混合分布因子，以允许每个计算节点自适应地将当前批次的统计参数与全局统计信息进行混合。

**结果:** HBN方法在广泛的联邦学习设置中表现出色，特别在小批量和异构数据的情况下，提升了联邦学习的性能。

**结论:** 混合批量归一化（HBN）是一种有效的插件，可以显著提升联邦学习的性能，尤其是在处理小批量和异构数据时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Batch+Normalisation%3A+Resolving+the+Dilemma+of+Batch+Normalisation+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21877，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21877&send_immediately=true&force_search=false)

**原文摘要:** Batch Normalisation (BN) is widely used in conventional deep neural network
training to harmonise the input-output distributions for each batch of data.
However, federated learning, a distributed learning paradigm, faces the
challenge of dealing with non-independent and identically distributed data
among the client nodes. Due to the lack of a coherent methodology for updating
BN statistical parameters, standard BN degrades the federated learning
performance. To this end, it is urgent to explore an alternative normalisation
solution for federated learning. In this work, we resolve the dilemma of the BN
layer in federated learning by developing a customised normalisation approach,
Hybrid Batch Normalisation (HBN). HBN separates the update of statistical
parameters (i.e. , means and variances used for evaluation) from that of
learnable parameters (i.e. , parameters that require gradient updates),
obtaining unbiased estimates of global statistical parameters in distributed
scenarios. In contrast with the existing solutions, we emphasise the supportive
power of global statistics for federated learning. The HBN layer introduces a
learnable hybrid distribution factor, allowing each computing node to
adaptively mix the statistical parameters of the current batch with the global
statistics. Our HBN can serve as a powerful plugin to advance federated
learning performance. It reflects promising merits across a wide range of
federated learning settings, especially for small batch sizes and heterogeneous
data.

</details>


### [78] [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
*Xueliang Zhao, Wei Wu, Lingpeng Kong*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的无注意力语言模型（ourmodel），通过架构和数据创新解决了Transformer架构的低效问题，并使用两阶段课程微调策略进行复杂推理训练。实验结果表明，ourmodel-7B在多个基准测试中优于同规模的Transformer和混合模型，甚至超越了更大规模的Gemma3-27B。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在复杂推理任务中取得了显著进展，但仍然受到两个核心挑战的限制：由于依赖Transformer导致的架构低效，以及缺乏针对高难度领域的结构化微调。为了解决这些问题，作者提出了一个无注意力的语言模型。

**方法:** 该模型基于Mamba-2的状态空间双层（SSD）层，消除了对自注意力和键值缓存的需求，实现了固定内存、恒定时间的推理。为了对其进行复杂推理训练，作者提出了基于PromptCoT合成范式的两阶段课程微调策略，通过抽象概念选择和有理引导生成来生成结构化问题。

**结果:** 在基准评估中，ourmodel-7B的表现优于同规模的强大Transformer和混合模型，并且在AIME 24上超过Gemma3-27B 2.6%，AIME 25上超过0.6%，Livecodebench上超过3.0%。

**结论:** 状态空间模型展示了作为高效且可扩展的注意力架构替代方案的潜力，适用于高容量推理任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Reasoning+without+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22425，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22425&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.

</details>


### [79] [HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis](https://arxiv.org/abs/2505.21882)
*Ruijie Li, Xiang Zhao, Qiao Ning, Shikai Guo*

**主要类别:** cs.LG

**概要:** 在网球比赛中，动量是一个关键但难以捉摸的现象，反映运动员表现的动态变化，可能决定比赛结果。本研究定义了一个新的动量评分（MS）指标，并设计了HydraNet框架，用于通过整合32个不同维度的运动员表现来建模MS。HydraNet包括一个Hydra模块，该模块基于状态空间二元性（SSD）框架，通过滑动窗口机制捕捉显式动量，并通过跨游戏状态传播捕捉隐式动量。此外，还引入了一种新的对抗学习方法和协作对抗注意力机制（CAAM），以增强两名运动员之间的宏观动量对抗性，并捕获和整合微观层面的内部和外部运动员动态动量。研究人员还构建了一个百万级别的跨赛事网球数据集，并验证了HydraNet对MS指标的多粒度建模能力。实验评估表明，HydraNet框架构建的MS指标为理解动量如何影响不同粒度的结果提供了可操作的见解，为动量建模和运动分析奠定了新基础。


<details>
  <summary>更多</summary>
  
**动机:** 尽管动量在网球比赛中的重要性已被认识到，但在有效建模和多粒度分析方面仍存在不足，特别是在点、局、盘和整场比赛中的动态变化尚未得到充分探索。

**方法:** 1. 定义了新的Momentum Score (MS) 指标，用于量化球员在多粒度网球比赛中的动量水平。
2. 设计了HydraNet框架，整合32个异构维度的运动员表现（发球、接球、心理和疲劳等）。
3. HydraNet包含Hydra模块，基于状态空间二元性（SSD）框架，利用滑动窗口机制捕捉显式动量，通过跨游戏状态传播捕捉隐式动量。
4. 引入了一种新的Versus Learning方法，以增强两名运动员之间的宏观动量对抗性。
5. 提出了协作对抗注意力机制（CAAM），用于捕获和整合微观层面的内部和外部运动员动态动量。
6. 构建了一个百万级别的跨赛事网球数据集（涵盖2012-2023年温网和2013-2023年美网），并验证了HydraNet对MS指标的多粒度建模能力。

**结果:** 广泛的实验评估表明，HydraNet框架构建的MS指标能够提供关于动量如何在不同粒度上影响比赛结果的可操作见解。这不仅有助于更好地理解动量的作用，还为未来的动量建模和运动分析奠定了基础。

**结论:** 本研究提出的Momentum Score (MS) 指标和HydraNet框架为网球比赛中的动量建模提供了新的视角和工具。这些成果不仅有助于深入理解动量对比赛结果的影响，还为未来的研究和应用提供了坚实的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HydraNet%3A+Momentum-Driven+State+Space+Duality+for+Multi-Granularity+Tennis+Tournaments+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21882&send_immediately=true&force_search=false)

**原文摘要:** In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge. The source code and datasets are available at
https://github.com/ReyJerry/HydraNet.

</details>


### [80] [SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning](https://arxiv.org/abs/2505.22442)
*Mattie Fellows, Clarisse Wibault, Uljad Berdica, Johannes Forkel, Jakob N. Foerster, Michael A. Osborne*

**主要类别:** cs.LG

**概要:** 这篇论文提出了两个算法SOReL和TOReL，分别用于安全的离线强化学习和离线超参数调整。SOReL通过贝叶斯方法利用离线数据估计在线性能并调整所有超参数，而TOReL扩展了信息率基于离线超参数调整方法到一般的离线RL方法。实验结果表明SOReL能够准确估计贝叶斯设定下的遗憾值，而TOReL仅使用离线数据就能达到与最佳在线超参数调整方法相竞争的性能。这为安全可靠的离线RL提供了重要进展，推动了RL在现实世界的应用。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习（RL）在实际应用中的主要障碍是样本效率问题：成功案例大多局限于可以通过模拟器获得几乎无限环境交互的场景，在现实中这些交互通常成本高昂或危险。虽然离线RL理论上可以通过利用离线数据在部署前学习接近最优的策略来提供解决方案，但目前的离线RL方法仍然依赖大量的在线交互进行超参数调整，并且没有对其初始在线性能提供可靠的界限。

**方法:** 1. 提出了SOReL算法，这是一种用于安全离线强化学习的算法。它采用贝叶斯方法推断环境动态的后验概率，从而可靠地估计在线性能，并且所有超参数也完全通过离线数据进行调整。
2. 提出了TOReL算法，该算法将信息率为基础的离线超参数调整方法扩展到一般的离线RL方法中。

**结果:** 实证评估证实了SOReL能够在贝叶斯设定下准确估计遗憾值，而TOReL的离线超参数调整仅使用离线数据就达到了与最佳在线超参数调整方法相竞争的性能。

**结论:** SOReL和TOReL算法为实现安全可靠的离线强化学习迈出了重要一步，释放了RL在现实世界应用的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SOReL+and+TOReL%3A+Two+Methods+for+Fully+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22442&send_immediately=true&force_search=false)

**原文摘要:** Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.

</details>


### [81] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/abs/2505.22483)
*Abhra Chaudhuri, Anjan Dutta, Tu Bui, Serban Georgescu*

**主要类别:** cs.LG

**概要:** 这篇论文探讨了多模态融合中模型倾向于仅依赖部分模态而忽略其他模态的现象（模态坍缩）。通过分析，作者发现当一个模态的噪声特征与另一个模态的预测特征在共享神经元中纠缠时，会导致模态坍缩。进一步证明，跨模态知识蒸馏可以通过释放学生编码器中的秩瓶颈来隐式地分离这些表示，从而在不损害任何模态预测特征的情况下对融合头输出进行去噪。基于此，提出了一种通过显式基重新分配防止模态坍缩的算法，并在多个多模态基准上进行了验证。


<details>
  <summary>更多</summary>
  
**动机:** 研究模态坍缩现象的根本原因，以提高多模态模型的有效性和鲁棒性。

**方法:** 1. 分析模态坍缩现象，发现其由不同模态特征在共享神经元中的纠缠引起。
2. 证明跨模态知识蒸馏可以隐式分离这些纠缠表示。
3. 提出一种基于显式基重新分配的算法，防止模态坍缩。
4. 在多个多模态基准数据集上进行实验验证。

**结果:** 理论和实验证明了模态坍缩的原因及提出的算法有效性，能够有效防止模态坍缩并处理缺失模态问题。

**结论:** 模态坍缩是由特征纠缠引起的，跨模态知识蒸馏是一种有效的解决方案，提出的算法可防止模态坍缩并在多模态任务中取得良好效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Closer+Look+at+Multimodal+Representation+Collapse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22483&send_immediately=true&force_search=false)

**原文摘要:** We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [82] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/abs/2505.21910)
*Xianbiao Qi, Yelin He, Jiaquan Ye, Chun-Guang Li, Bojia Zi, Xili Dai, Qin Zou, Rong Xiao*

**主要类别:** cs.LG

**概要:** 在大规模Transformer训练中，提出了一种新的优化策略以解决模型崩溃问题，并证明其在多种Transformer架构上有效且稳定。


<details>
  <summary>更多</summary>
  
**动机:** 大规模Transformer训练面临挑战，特别是在不使用学习率预热等技术时，容易出现模型崩溃现象。

**方法:** 通过理论分析揭示了模型崩溃的原因是${\bW_q}^{\top} \bW_k$的谱能量集中现象，并基于Weyl's Inequality提出一种新的优化策略：当更新量的比例超过阈值时，自动调整学习率以平滑权重更新，防止谱能量集中在少数方向。

**结果:** 实验表明，该优化策略能够有效且稳定地训练ViT、Swin-Transformer和GPT等模型，无需使用学习率预热。

**结论:** 提出的优化策略解决了大规模Transformer训练中的模型崩溃问题，为更稳定的训练提供了新方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Taming+Transformer+Without+Using+Learning+Rate+Warmup，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21910，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21910&send_immediately=true&force_search=false)

**原文摘要:** Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [83] [Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data](https://arxiv.org/abs/2505.22521)
*Chao Wang, Chuanhao Nie, Yunbo Liu*

**主要类别:** cs.LG

**概要:** 在金融和电子商务等高风险领域，欺诈检测是一项关键任务。本研究对比了四种监督学习模型（逻辑回归、随机森林、LightGBM和GRU网络）在一个大规模、高度不平衡的在线交易数据集上的表现。随机森林和LightGBM在整体和类别特定指标上表现出色，而逻辑回归提供了一个可靠且可解释的基线。GRU模型对少数类欺诈具有较高的召回率，但牺牲了精确度。评估不仅关注加权平均值，还关注每个类别的精确度、召回率和F1分数，为检测稀有但重要的欺诈活动提供了细致的视角。研究结果强调了根据欺诈检测系统的具体风险承受能力和运营需求选择模型的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 欺诈检测在金融和电子商务等高风险领域至关重要，未被发现的欺诈交易可能导致重大经济损失。因此，需要找到能够有效检测欺诈行为的模型。

**方法:** 系统地比较了四个监督学习模型（逻辑回归、随机森林、LightGBM和GRU网络）在一个大规模、高度不平衡的在线交易数据集上的表现。使用整体和类别特定指标来评估模型性能。

**结果:** 随机森林和LightGBM在整体和类别特定指标上表现出色；逻辑回归提供了一个可靠且可解释的基线；GRU模型对少数类欺诈具有较高的召回率，但牺牲了精确度。

**结论:** 选择欺诈检测模型时应考虑具体的风险承受能力和运营需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Supervised+Learning+Models+for+Fraud+Detection%3A+A+Comparative+Study+of+Classical+and+Deep+Architectures+on+Imbalanced+Transaction+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22521，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22521&send_immediately=true&force_search=false)

**原文摘要:** Fraud detection remains a critical task in high-stakes domains such as
finance and e-commerce, where undetected fraudulent transactions can lead to
significant economic losses. In this study, we systematically compare the
performance of four supervised learning models - Logistic Regression, Random
Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit
(GRU) network - on a large-scale, highly imbalanced online transaction dataset.
While ensemble methods such as Random Forest and LightGBM demonstrated superior
performance in both overall and class-specific metrics, Logistic Regression
offered a reliable and interpretable baseline. The GRU model showed strong
recall for the minority fraud class, though at the cost of precision,
highlighting a trade-off relevant for real-world deployment. Our evaluation
emphasizes not only weighted averages but also per-class precision, recall, and
F1-scores, providing a nuanced view of each model's effectiveness in detecting
rare but consequential fraudulent activity. The findings underscore the
importance of choosing models based on the specific risk tolerance and
operational needs of fraud detection systems.

</details>


### [84] [Training RL Agents for Multi-Objective Network Defense Tasks](https://arxiv.org/abs/2505.22531)
*Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, Ahmed Ridley*

**主要类别:** cs.LG

**概要:** 提出了一种受开放式学习（OEL）启发的训练方法，用于开发自主网络防御者，并强调了在应用OEL到网络防御时需要解决的技术挑战。


<details>
  <summary>更多</summary>
  
**动机:** 尽管开放式学习（OEL）在其他领域显示出潜力，但将其应用于现实世界的网络安全应用中仍然存在挑战。因此，需要一种新的训练方法来开发适用于网络安全的自主代理。

**方法:** 提出了一种基于OEL的训练方法，用于开发自主网络防御者。该方法通过提供一致的任务表示接口，使学习代理能够在变化的网络条件、攻击者行为和防御目标下进行训练，同时利用先前获得的知识。

**结果:** 实验结果表明，OEL原则可以转化为更强大和通用的网络防御代理。

**结论:** 作者希望通过他们的工具和结果，对将人工智能应用于解决网络安全问题的研究产生根本性影响，特别是在开发多样且具有一致表示的任务的健身房和基准时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+RL+Agents+for+Multi-Objective+Network+Defense+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22531&send_immediately=true&force_search=false)

**原文摘要:** Open-ended learning (OEL) -- which emphasizes training agents that achieve
broad capability over narrow competency -- is emerging as a paradigm to develop
artificial intelligence (AI) agents to achieve robustness and generalization.
However, despite promising results that demonstrate the benefits of OEL,
applying OEL to develop autonomous agents for real-world cybersecurity
applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous
network defenders. Our results demonstrate that like in other domains, OEL
principles can translate into more robust and generalizable agents for cyber
defense. To apply OEL to network defense, it is necessary to address several
technical challenges. Most importantly, it is critical to provide a task
representation approach over a broad universe of tasks that maintains a
consistent interface over goals, rewards and action spaces. This way, the
learning agent can train with varying network conditions, attacker behaviors,
and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that
applies AI to solve cybersecurity problems. Specifically, as researchers
develop gyms and benchmarks for cyber defense, it is paramount that they
consider diverse tasks with consistent representations, such as those we
propose in our work.

</details>


### [85] [TabularQGAN: A Quantum Generative Model for Tabular Data](https://arxiv.org/abs/2505.22533)
*Pallavi Bhardwaj, Caitlin Jones, Lasse Dierich, Aleksandar Vučković*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的量子生成模型，用于合成表格数据。该模型在MIMIC III和Adult Census数据集上进行测试，并与经典模型CTGAN和CopulaGAN进行基准对比。结果显示，该量子模型使用仅0.072%的经典模型参数量，却能将整体相似度得分提高8.5%。这是已知的第一个成功的量子生成模型处理表格数据的展示，表明这一任务可能非常适合量子计算机。


<details>
  <summary>更多</summary>
  
**动机:** 真实世界的数据常常是稀缺或私密的，合成数据可以在这些情况下补充或替代现有数据集。尤其是在医疗、金融和软件等行业中，企业数据主要以表格形式存在，并且具有异构性（包含分类和数值特征）。因此，开发一种有效的生成模型来合成这类数据具有重要意义。

**方法:** 作者提出了一种带有灵活数据编码和新量子电路结构的量子生成对抗网络架构。该方法旨在有效建模表格数据，并通过自定义设计的两个指标评估模型的泛化能力。

**结果:** 实验结果表明，所提出的量子模型在整体相似度得分上比经典模型高出8.5%，而仅使用了经典模型参数量的0.072%。此外，自定义指标显示该模型能够生成有用且新颖的样本。

**结论:** 这是首个成功展示量子生成模型处理表格数据的研究，表明此任务可能非常适合量子计算机。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TabularQGAN%3A+A+Quantum+Generative+Model+for+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22533&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we introduce a novel quantum generative model for synthesizing
tabular data. Synthetic data is valuable in scenarios where real-world data is
scarce or private, it can be used to augment or replace existing datasets.
Real-world enterprise data is predominantly tabular and heterogeneous, often
comprising a mixture of categorical and numerical features, making it highly
relevant across various industries such as healthcare, finance, and software.
We propose a quantum generative adversarial network architecture with flexible
data encoding and a novel quantum circuit ansatz to effectively model tabular
data. The proposed approach is tested on the MIMIC III healthcare and Adult
Census datasets, with extensive benchmarking against leading classical models,
CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model
outperforms classical models by an average of 8.5% with respect to an overall
similarity score from SDMetrics, while using only 0.072% of the parameters of
the classical models. Additionally, we evaluate the generalization capabilities
of the models using two custom-designed metrics that demonstrate the ability of
the proposed quantum model to generate useful and novel samples. To our
knowledge, this is one of the first demonstrations of a successful quantum
generative model for handling tabular data, indicating that this task could be
well-suited to quantum computers.

</details>


### [86] [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
*Dongyue Li, Ziniu Zhang, Lu Wang, Hongyang R. Zhang*

**主要类别:** cs.LG

**概要:** 本文提出了一种集成方法，用于将语言模型微调到多个数据集上。该方法通过将数据集分组并训练适配器，利用低秩适应的近似性质加速计算，并在实验中取得了优于QLoRA的效果，同时计算量增加较少。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法（如QLoRA）在适应单一数据集时效率较高，但在针对多个不同任务的数据集进行微调时，如何设计高效的适应策略仍不清楚。

**方法:** 提出使用多个更小的适配器组成的集成方法，而不是每个任务一个适配器。设计了一个高效算法，将n个数据集分为m组（m通常远小于n），为每组训练一个适配器，然后通过加权组合形成集成。利用低秩适应的一阶近似性质，基于基础模型的梯度估计微调性能。

**结果:** 在多达34亿参数的模型上，该方法的近似误差小于1%，估算真实微调性能误差低于5%，且比基础微调快105倍。在Llama和GPT模型上的实验表明，该方法比QLoRA平均测试准确率高出10%，仅增加9%的FLOPs。对于34亿参数的Llama模型，QLoRA集成方法比单独QLoRA提高3%的测试准确率，仅增加8%的FLOPs。

**结论:** 提出的集成方法能够有效应对多数据集微调问题，在提升性能的同时保持较低的计算开销，具有广泛的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Ensemble+for+Fine-tuning+Language+Models+on+Multiple+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21930&send_immediately=true&force_search=false)

**原文摘要:** This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.

</details>


### [87] [Machine Unlearning under Overparameterization](https://arxiv.org/abs/2505.22601)
*Jacob L. Block, Aryan Mokhtari, Sanjay Shakkottai*

**主要类别:** cs.LG

**概要:** 本文研究了在过参数化设置下的机器遗忘算法，提出了新的遗忘定义和算法框架，并证明其在不同模型类别中的优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的遗忘算法在过参数化设置下失效，因为损失梯度消失，导致基于梯度扰动的方法无效。需要新的遗忘定义和算法来解决此问题。

**方法:** 作者将遗忘解定义为保留数据上的最小复杂度插值器，并提出一个新算法框架，该框架仅需访问原始解处保留集上的模型梯度。通过最小化受约束的正则化目标，实现对插值条件的一阶松弛。

**结果:** 对于不同的模型类别，提供了精确和近似的遗忘保证。实验表明，该框架的实现优于现有的基线方法。

**结论:** 提出的遗忘定义和算法框架在过参数化设置下有效，且在各种遗忘实验中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Machine+Unlearning+under+Overparameterization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22601&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning algorithms aim to remove the influence of specific
training samples, ideally recovering the model that would have resulted from
training on the remaining data alone. We study unlearning in the
overparameterized setting, where many models interpolate the data, and defining
the unlearning solution as any loss minimizer over the retained
set$\unicode{x2013}$as in prior work in the underparameterized
setting$\unicode{x2013}$is inadequate, since the original model may already
interpolate the retained data and satisfy this condition. In this regime, loss
gradients vanish, rendering prior methods based on gradient perturbations
ineffective, motivating both new unlearning definitions and algorithms. For
this setting, we define the unlearning solution as the minimum-complexity
interpolator over the retained data and propose a new algorithmic framework
that only requires access to model gradients on the retained set at the
original solution. We minimize a regularized objective over perturbations
constrained to be orthogonal to these model gradients, a first-order relaxation
of the interpolation condition. For different model classes, we provide exact
and approximate unlearning guarantees, and we demonstrate that an
implementation of our framework outperforms existing baselines across various
unlearning experiments.

</details>


### [88] [One Rank at a Time: Cascading Error Dynamics in Sequential Learning](https://arxiv.org/abs/2505.22602)
*Mahtab Alizadeh Vandchali, Fangshuo, Liao, Anastasios Kyrillidis*

**主要类别:** cs.LG

**概要:** 本论文通过低秩线性回归的视角研究序列学习中的误差传播问题，提出了一种将学习过程分解为一系列秩1估计问题的框架，并分析了误差在序列学习过程中的累积效应。


<details>
  <summary>更多</summary>
  
**动机:** 序列学习作为一种将复杂任务分解为简单、分层组件的范式，在AI领域中具有重要意义。然而，序列学习过程中误差如何传播尚未得到充分研究。因此，本文旨在探讨序列学习中误差传播的特性，特别是当学习顺序进行时，秩1子空间中的误差如何影响整体模型精度。

**方法:** 本文将序列学习过程分解为一系列秩1估计问题，每个后续估计依赖于前一步骤的准确性。通过这一框架，作者对序列学习过程中的误差传播进行了刻画。

**结果:** 研究结果表明，由于计算预算有限和精度有限等原因导致的误差以可预测的方式累积，并对整体模型精度产生影响。

**结论:** 本文通过分析误差传播特性，为算法设计提供了指导，并为稳定性保证提供了理论依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One+Rank+at+a+Time%3A+Cascading+Error+Dynamics+in+Sequential+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22602，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22602&send_immediately=true&force_search=false)

**原文摘要:** Sequential learning -- where complex tasks are broken down into simpler,
hierarchical components -- has emerged as a paradigm in AI. This paper views
sequential learning through the lens of low-rank linear regression, focusing
specifically on how errors propagate when learning rank-1 subspaces
sequentially. We present an analysis framework that decomposes the learning
process into a series of rank-1 estimation problems, where each subsequent
estimation depends on the accuracy of previous steps. Our contribution is a
characterization of the error propagation in this sequential process,
establishing bounds on how errors -- e.g., due to limited computational budgets
and finite precision -- affect the overall model accuracy. We prove that these
errors compound in predictable ways, with implications for both algorithmic
design and stability guarantees.

</details>


### [89] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
*Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, Ning Ding*

**主要类别:** cs.LG

**概要:** 本文研究了强化学习中策略熵崩溃的问题，并提出了两种方法Clip-Cov和KL-Cov来控制高协方差token的更新，从而鼓励探索并提升下游任务性能。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习在大规模语言模型中的应用面临策略熵崩溃的问题，这限制了策略的探索能力及性能提升。

**方法:** 通过理论和实证研究策略熵动态变化机制，提出Clip-Cov和KL-Cov两种技术分别对高协方差token进行裁剪和KL惩罚，以控制熵的变化。

**结果:** 实验表明，所提出的方法能够鼓励探索，帮助策略摆脱熵崩溃，并获得更好的下游任务表现。

**结论:** 为了实现强化学习的持续探索，必须进行熵管理，限制高协方差token的更新是有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Entropy+Mechanism+of+Reinforcement+Learning+for+Reasoning+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22617，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22617&send_immediately=true&force_search=false)

**原文摘要:** This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.

</details>


### [90] [Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization](https://arxiv.org/abs/2505.21944)
*Linli Zhou, Bokun Wang, My T. Thai, Tianbao Yang*

**主要类别:** cs.LG

**概要:** 开发了两种新的随机原始对偶双块坐标算法以优化TPAUC，适用于凸和非凸场景，并在理论和实验上证明其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 两路部分AUC（TPAUC）是处理不平衡数据二分类问题的关键性能指标，但现有的随机优化算法要么局限于近似TPAUC损失函数，要么因次优复杂度而受限。

**方法:** 提出了两种创新的随机原始对偶双块坐标算法用于TPAUC最大化，这些算法对原始和对偶变量均采用随机块坐标更新，适应于凸和非凸设置。

**结果:** 实验结果基于多个基准数据集验证了新算法的优越性能，展示出更快的收敛速度和更好的泛化能力。

**结论:** 本工作推进了TPAUC优化的最前沿，并为实际机器学习应用提供了实用工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+Primal-Dual+Double+Block-Coordinate+for+Two-way+Partial+AUC+Maximization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21944，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21944&send_immediately=true&force_search=false)

**原文摘要:** Two-way partial AUC (TPAUC) is a critical performance metric for binary
classification with imbalanced data, as it focuses on specific ranges of the
true positive rate (TPR) and false positive rate (FPR). However, stochastic
algorithms for TPAUC optimization remain under-explored, with existing methods
either limited to approximated TPAUC loss functions or burdened by sub-optimal
complexities. To overcome these limitations, we introduce two innovative
stochastic primal-dual double block-coordinate algorithms for TPAUC
maximization. These algorithms utilize stochastic block-coordinate updates for
both the primal and dual variables, catering to both convex and non-convex
settings. We provide theoretical convergence rate analyses, demonstrating
significant improvements over prior approaches. Our experimental results, based
on multiple benchmark datasets, validate the superior performance of our
algorithms, showcasing faster convergence and better generalization. This work
advances the state of the art in TPAUC optimization and offers practical tools
for real-world machine learning applications.

</details>


### [91] [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
*Michael Kirchhof, Gjergji Kasneci, Enkelejda Kasneci*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）和聊天机器人有时会提供错误输出，且无法完全避免。因此，不确定性量化至关重要。然而，传统的不确定性二分法在交互式LLM环境中过于局限。本文提出三个新的研究方向：未明确定义的不确定性、交互式学习以及输出不确定性，以期使LLM代理的交互更透明、可信和直观。


<details>
  <summary>更多</summary>
  
**动机:** LLMs和聊天机器人存在提供错误输出的问题，并且这种问题无法完全避免。因此，在与用户交互时，仅用传统的不确定性量化方法（如整体数字或区分随机不确定性和认知不确定性）已不足以描述LLM代理中的不确定性。

**方法:** 1. 提出三个新的研究方向：
   - 未明确定义的不确定性：处理用户未提供完整信息或任务定义不明确的情况。
   - 交互式学习：通过提问后续问题减少对当前上下文的不确定性。
   - 输出不确定性：利用丰富的语言和语音空间表达不确定性，而不仅限于数字。
2. 审查现有文献，发现关于随机和认知不确定性的流行定义存在矛盾，并在交互式LLM设置中失去意义。

**结果:** 这三个新研究方向有望改善LLM代理在不确定性处理和沟通方面的能力，使其交互更加透明、可信和直观。

**结论:** 传统的不确定性量化方法在交互式LLM环境中过于局限，需要探索新的研究方向以丰富不确定性表示，从而提高LLM代理交互的透明性、信任度和直观性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+Uncertainty+Quantification+Needs+Reassessment+for+Large-language+Model+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22655&send_immediately=true&force_search=false)

**原文摘要:** Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.

</details>


### [92] [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
*Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh Satheesh, Bang An, Bayan Bruss, John Langford, Furong Huang*

**主要类别:** cs.LG

**概要:** 随着大型语言模型（LLMs）快速接近甚至可能超越人类水平的表现，开发有效监督和增强这些强大模型的方法变得至关重要。本文提出了EnsemW2S方法，通过迭代组合多个弱专家模型来提升其监督强学生模型的能力，并在不同数据集上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型可能超越人类水平，但使用人类水平的数据监督和增强这些模型成为挑战。需要一种方法，利用较小、仅接触人类水平数据的模型，来监督和增强强大的大型语言模型。

**方法:** 提出了一种名为EnsemW2S的方法，采用基于token级别的集成策略，迭代地结合多个弱专家模型，系统性地解决前一轮迭代中发现的问题，并通过不断优化弱模型，提升它们共同监督更强学生模型的能力。

**结果:** 实验结果表明，在同分布（ID）数据集上，专家模型和学生模型分别提升了4%和3.2%，而在异分布（OOD）数据集上，分别最高提升了6%和2.28%。

**结论:** EnsemW2S方法显著提高了弱到强（W2S）泛化能力，为监督和增强大型语言模型提供了一种有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EnsemW2S%3A+Enhancing+Weak-to-Strong+Generalization+with+Large+Language+Model+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21959，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21959&send_immediately=true&force_search=false)

**原文摘要:** With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.

</details>


### [93] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/abs/2505.22660)
*Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak*

**主要类别:** cs.LG

**概要:** 本论文提出了一种全新的强化学习方法RENT（通过熵最小化进行强化学习），该方法完全无监督，无需外部奖励或真实答案，而是利用模型分布的熵作为内在奖励。实验表明，这种方法在多个推理基准测试上提高了模型的推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习的核心是奖励函数，而奖励工程在任何领域都是一个困难的问题。为了克服对外部奖励或真实答案的依赖，研究者们希望开发一种无需外部监督的强化学习方法。

**方法:** 提出RENT：一种基于熵最小化的无监督强化学习方法。该方法将模型生成答案时的高置信度思维链路作为强化目标，从而提升模型的推理能力。

**结果:** 在多个常用的推理基准测试（如GSM8K、MATH500、AMC、AIME和GPQA）上，使用不同规模的Qwen和Mistral系列模型进行实验，结果表明模型的推理能力得到了显著提高。

**结论:** RENT作为一种无监督的强化学习方法，具有广泛的适用性，特别是在外部监督有限或不可用的领域中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Maximizing+Confidence+Alone+Improves+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22660&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is limited or unavailable.

</details>


### [94] [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://arxiv.org/abs/2505.21974)
*Yu-Heng Hung, Kai-Jie Lin, Yu-Heng Lin, Chien-YiWang, Cheng Sun, Ping-Chun Hsieh*

**主要类别:** cs.LG

**概要:** Bayesian optimization (BO) is an efficient method for optimizing black-box functions. In multi-objective Bayesian optimization (MOBO), current learning-based acquisition functions suffer from the hypervolume identifiability issue. This paper proposes BOFormer, a deep Q-learning framework inspired by non-Markovian RL and Transformers, to address this issue through sequence modeling. Experiments show BOFormer outperforms existing rule-based and learning-based algorithms in various MOBO tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of this paper stems from the limitations of extending single-objective Bayesian optimization's learning-based acquisition functions to multi-objective problems due to the hypervolume identifiability issue, which arises from the non-Markovian nature of MOBO problems.

**方法:** The authors introduce BOFormer, a generalized deep Q-learning framework that leverages sequence modeling inspired by non-Markovian reinforcement learning and Transformer architectures to overcome the challenges in MOBO. This approach avoids the pitfalls of the hypervolume identifiability issue.

**结果:** BOFormer consistently outperforms both rule-based and learning-based benchmark algorithms across a variety of synthetic MOBO and real-world multi-objective hyperparameter optimization problems.

**结论:** This work successfully addresses the hypervolume identifiability issue in MOBO using BOFormer, showcasing its superiority over existing methods. The source code has been made publicly available to facilitate further advancements in this area.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BOFormer%3A+Learning+to+Solve+Multi-Objective+Bayesian+Optimization+via+Non-Markovian+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21974，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21974&send_immediately=true&force_search=false)

**原文摘要:** Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.

</details>


### [95] [Two-Stage Feature Generation with Transformer and Reinforcement Learning](https://arxiv.org/abs/2505.21978)
*Wanfu Gao, Zengyao Man, Zebin He, Yuhao Tang, Jun Gao, Kunpeng Liu*

**主要类别:** cs.LG

**概要:** 提出了一种两阶段特征生成（TSFG）框架，结合Transformer编码器-解码器架构与近端策略优化（PPO），解决了传统特征生成方法中的问题，实验表明其在特征质量和适应性方面优于现有最先进方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的特征生成方法依赖领域专业知识和人工干预，自动化特征生成技术虽然有所改进，但仍然存在特征冗余、探索效率低以及对多样化数据集和任务的适应性有限等问题。

**方法:** TSFG框架整合了基于Transformer的编码器-解码器架构和PPO。编码器-解码器模型利用Transformer的自注意力机制来高效表示和转换特征，捕捉数据中的复杂依赖关系；PPO则根据特定任务的反馈动态调整特征生成策略，优化整个过程以提高性能和适应性。

**结果:** TSFG能够动态生成高质量的特征集，显著提高了机器学习模型的预测性能，并且实验结果表明，TSFG在特征质量和适应性方面优于现有的最先进的方法。

**结论:** TSFG框架有效地解决了现有特征生成方法中的关键问题，通过结合Transformer和PPO的优势，提供了更高质量和更强适应性的特征生成方案，为机器学习模型性能的提升做出了贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-Stage+Feature+Generation+with+Transformer+and+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21978&send_immediately=true&force_search=false)

**原文摘要:** Feature generation is a critical step in machine learning, aiming to enhance
model performance by capturing complex relationships within the data and
generating meaningful new features. Traditional feature generation methods
heavily rely on domain expertise and manual intervention, making the process
labor-intensive and challenging to adapt to different scenarios. Although
automated feature generation techniques address these issues to some extent,
they often face challenges such as feature redundancy, inefficiency in feature
space exploration, and limited adaptability to diverse datasets and tasks. To
address these problems, we propose a Two-Stage Feature Generation (TSFG)
framework, which integrates a Transformer-based encoder-decoder architecture
with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG
leverages the Transformer's self-attention mechanism to efficiently represent
and transform features, capturing complex dependencies within the data. PPO
further enhances TSFG by dynamically adjusting the feature generation strategy
based on task-specific feedback, optimizing the process for improved
performance and adaptability. TSFG dynamically generates high-quality feature
sets, significantly improving the predictive performance of machine learning
models. Experimental results demonstrate that TSFG outperforms existing
state-of-the-art methods in terms of feature quality and adaptability.

</details>


### [96] [ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning](https://arxiv.org/abs/2505.21987)
*Zhendong Mi, Zhenglun Kong, Geng Yuan, Shaoyi Huang*

**主要类别:** cs.LG

**概要:** 提出了一种高效的LLM剪枝方法，通过引入激活余弦相似性损失引导和激活方差引导的剪枝度量，实现高性能剪枝与快速剪枝速度。实验表明该方法在LLaMA、LLaMA-2和OPT等模型上显著降低困惑度并减少剪枝时间。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）的发展带来了对内存和计算资源需求的增长，而现有的剪枝方法要么性能欠佳，要么剪枝过程效率低下。因此需要一种更高效且有效的剪枝方法。

**方法:** 提出两个关键创新：1) 激活余弦相似性损失引导的剪枝度量，考虑密集和剪枝模型间输出激活的角度偏差；2) 激活方差引导的剪枝度量，帮助保留剪枝后输出激活的语义区别，并允许使用较短输入序列进行有效剪枝。两者结合可提高LLM剪枝的准确性和效率。

**结果:** 实验结果表明，该方法在如LLaMA、LLaMA-2和OPT等流行的LLM上实现了高达18%的困惑度降低和63%的剪枝时间减少。

**结论:** 所提出的剪枝方法同时实现了高剪枝性能和快速剪枝速度，改善了校准效率，为LLM剪枝提供了一种更优的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ACE%3A+Exploring+Activation+Cosine+Similarity+and+Variance+for+Accurate+and+Calibration-Efficient+LLM+Pruning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21987&send_immediately=true&force_search=false)

**原文摘要:** With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM pruning aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal pruning
performance or low time efficiency during the pruning process. In this work, we
propose an efficient and effective pruning method that simultaneously achieves
high pruning performance and fast pruning speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided pruning metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided pruning metric, which helps preserve semantic
distinctions in output activations after pruning, enabling effective pruning
with shorter input sequences. These two components can be readily combined to
enhance LLM pruning in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.

</details>


### [97] [Learning in Compact Spaces with Approximately Normalized Transformers](https://arxiv.org/abs/2505.22014)
*Jörg K. H. Franke, Urs Spiegelhalter, Marianna Nezhurina, Jenia Jitsev, Frank Hutter, Michael Hefenbrock*

**主要类别:** cs.LG

**概要:** 提出了一种新的近似规范化方法anTransformer，通过约束参数范数和规范化表示来加速收敛。在GPT训练中，相较于QK规范化模型，收敛速度快40%，额外运行时间少于3%。该方法还支持更大的批量训练和更少的超参数调整，同时保持经典GPT架构的良好扩展特性。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习中，正则化和规范化是应对过拟合、数值不稳定性和残差流方差增加等问题的常见手段。但这些方法可能带来额外成本。本研究探索了一种替代方案，即将所有参数和表示限制在超球面上，从而无需正则化并加快收敛速度。

**方法:** 提出了一种称为anTransformer的近似规范化方法，通过对参数范数进行约束，并基于高维随机向量范数的集中性理论，通过标量乘法对所有表示进行规范化。

**结果:** 在GPT训练中，与QK规范化模型相比，使用anTransformer方法的模型收敛速度提高了40%，且额外运行时间小于3%。此外，该方法还推导了适用于更大批量训练和更少超参数调整的扩展规律。

**结论:** anTransformer是一种有效的近似规范化方法，能够在减少额外运行时间的同时显著加速模型收敛，并简化训练过程中的超参数调整，同时保留经典GPT架构的良好扩展特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+in+Compact+Spaces+with+Approximately+Normalized+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22014&send_immediately=true&force_search=false)

**原文摘要:** In deep learning, regularization and normalization are common solutions for
challenges such as overfitting, numerical instabilities, and the increasing
variance in the residual stream. An alternative approach is to force all
parameters and representations to lie on a hypersphere. This removes the need
for regularization and increases convergence speed, but comes with additional
costs. In this work, we propose a more holistic but approximate normalization
(anTransformer). Our approach constrains the norm of parameters and normalizes
all representations via scalar multiplications motivated by the tight
concentration of the norms of high-dimensional random vectors. When applied to
GPT training, we observe a 40% faster convergence compared to models with QK
normalization, with less than 3% additional runtime. Deriving scaling laws for
anGPT, we found our method enables training with larger batch sizes and fewer
hyperparameters, while matching the favorable scaling characteristics of
classic GPT architectures.

</details>


### [98] [Weakly-Supervised Contrastive Learning for Imprecise Class Labels](https://arxiv.org/abs/2505.22028)
*Zi-Hao Zhou, Jun-Jie Wang, Tong Wei, Min-Ling Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种基于连续语义相似性的弱监督对比学习框架，通过图论方法定义正负样本对，并在噪声标签和部分标签场景中验证了有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现实场景中数据标注往往模糊或不准确，导致监督对比学习受限。为解决这一问题，提出了连续语义相似性概念来替代直接依赖类别标签的方法。

**方法:** 引入连续语义相似性概念，通过迭代优化弱监督信号衡量样本对之间的语义相似性，构建以语义相似性为权重的图论框架，用于弱监督对比学习。

**结果:** 在噪声标签和部分标签学习场景中，该框架显著提升了性能，并且理论分析表明其可以在一定条件下近似监督对比学习的效果。

**结论:** 提出的弱监督对比学习框架具有高度灵活性，适用于多种弱监督学习场景，可有效提升学习效果并接近监督对比学习的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weakly-Supervised+Contrastive+Learning+for+Imprecise+Class+Labels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22028&send_immediately=true&force_search=false)

**原文摘要:** Contrastive learning has achieved remarkable success in learning effective
representations, with supervised contrastive learning often outperforming
self-supervised approaches. However, in real-world scenarios, data annotations
are often ambiguous or inaccurate, meaning that class labels may not reliably
indicate whether two examples belong to the same class. This limitation
restricts the applicability of supervised contrastive learning. To address this
challenge, we introduce the concept of ``continuous semantic similarity'' to
define positive and negative pairs. Instead of directly relying on imprecise
class labels, we measure the semantic similarity between example pairs, which
quantifies how closely they belong to the same category by iteratively refining
weak supervisory signals. Based on this concept, we propose a graph-theoretic
framework for weakly-supervised contrastive learning, where semantic similarity
serves as the graph weights. Our framework is highly versatile and can be
applied to many weakly-supervised learning scenarios. We demonstrate its
effectiveness through experiments in two common settings, i.e., noisy label and
partial label learning, where existing methods can be easily integrated to
significantly improve performance. Theoretically, we establish an error bound
for our approach, showing that it can approximate supervised contrastive
learning under mild conditions. The implementation code is available at
https://github.com/Speechless-10308/WSC.

</details>


### [99] [Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation](https://arxiv.org/abs/2505.22041)
*Michael Grohs, Adrian Rebmann, Jana-Rebecca Rehse*

**主要类别:** cs.LG

**概要:** 论文提出了一种无需专用流程模型或资源密集型微调的方法，通过使用检索增强生成（RAG）技术来检测流程中的不期望行为。实验表明，该方法在检测不期望行为方面优于微调的大型语言模型（LLMs）。


<details>
  <summary>更多</summary>
  
**动机:** 现有的符合性检查技术依赖于专用的流程模型，但当这些模型不可用时，需要一种新的方法来检测流程中的不期望行为。

**方法:** 使用检索增强生成（RAG）技术，向大型语言模型（LLM）提供包含其他流程中期望和不期望行为的知识库，使其能够将此知识迁移到当前流程中。

**结果:** 评估结果表明，与微调的LLMs相比，所提出的方法在检测不期望行为方面表现更好。

**结论:** RAG是一种可行的替代方案，可以避免资源密集型的微调过程，特别是在结合事件日志中的相关上下文（如频繁轨迹和活动）时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+Undesired+Process+Behavior+by+Means+of+Retrieval+Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22041，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22041&send_immediately=true&force_search=false)

**原文摘要:** Conformance checking techniques detect undesired process behavior by
comparing process executions that are recorded in event logs to desired
behavior that is captured in a dedicated process model. If such models are not
available, conformance checking techniques are not applicable, but
organizations might still be interested in detecting undesired behavior in
their processes. To enable this, existing approaches use Large Language Models
(LLMs), assuming that they can learn to distinguish desired from undesired
behavior through fine-tuning. However, fine-tuning is highly resource-intensive
and the fine-tuned LLMs often do not generalize well. To address these
limitations, we propose an approach that requires neither a dedicated process
model nor resource-intensive fine-tuning to detect undesired process behavior.
Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with
direct access to a knowledge base that contains both desired and undesired
process behavior from other processes, assuming that the LLM can transfer this
knowledge to the process at hand. Our evaluation shows that our approach
outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that
RAG is a viable alternative to resource-intensive fine-tuning, particularly
when enriched with relevant context from the event log, such as frequent traces
and activities.

</details>


### [100] [Differentiable Generalized Sliced Wasserstein Plans](https://arxiv.org/abs/2505.22049)
*Laetitia Chapel, Romain Tavenard, Samuel Vaiter*

**主要类别:** cs.LG

**概要:** 本论文提出了一种可微分的近似方案，将min-SWGG重写为双层优化问题，并扩展其在高维数据和流形上的应用，从而提高了最优传输（OT）计算的效率。


<details>
  <summary>更多</summary>
  
**动机:** 尽管min-SWGG方法在计算和理论上具有优势，但它继承了典型切片方法的限制：随着数据维度的增加，所需的切片数量呈指数增长，且局限于线性投影。因此，需要一种更高效的方法来识别最优切片并扩展到高维和流形数据。

**方法:** 作者将min-SWGG重新表述为一个双层优化问题，并提出了一个可微分的近似方案以有效识别最优切片，即使在高维设置下也能实现。此外，他们还定义了该方法的广义扩展，使其适用于流形上的数据。

**结果:** 所提出的方法在多种实际应用中展示了其价值，包括流形和高维空间中的梯度流计算，以及基于切片OT的条件流匹配用于图像生成。这些结果表明，快速计算传输计划是可行且有效的。

**结论:** 通过改进min-SWGG并将其扩展到高维和流形数据，该研究提供了一种高效的最优传输计算方法，解决了传统切片方法的局限性，并在多个领域中验证了其实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Differentiable+Generalized+Sliced+Wasserstein+Plans，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22049&send_immediately=true&force_search=false)

**原文摘要:** Optimal Transport (OT) has attracted significant interest in the machine
learning community, not only for its ability to define meaningful distances
between probability distributions -- such as the Wasserstein distance -- but
also for its formulation of OT plans. Its computational complexity remains a
bottleneck, though, and slicing techniques have been developed to scale OT to
large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a
single one-dimensional plan back to the original multidimensional space,
finally selecting the slice that yields the lowest Wasserstein distance as an
approximation of the full OT plan. Despite its computational and theoretical
advantages, min-SWGG inherits typical limitations of slicing methods: (i) the
number of required slices grows exponentially with the data dimension, and (ii)
it is constrained to linear projections. Here, we reformulate min-SWGG as a
bilevel optimization problem and propose a differentiable approximation scheme
to efficiently identify the optimal slice, even in high-dimensional settings.
We furthermore define its generalized extension for accommodating to data
living on manifolds. Finally, we demonstrate the practical value of our
approach in various applications, including gradient flows on manifolds and
high-dimensional spaces, as well as a novel sliced OT-based conditional flow
matching for image generation -- where fast computation of transport plans is
essential.

</details>


### [101] [Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?](https://arxiv.org/abs/2505.22081)
*Shun Sato, Issei Sato*

**主要类别:** cs.LG

**概要:** 本研究探讨了神经符号回归方法中的记忆偏差问题，通过定量评估和理论分析揭示了Transformer模型在生成未见表达式时的局限性，并提出测试时策略可减轻记忆偏差但不一定提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 神经符号回归方法虽然具有短推理时间的优势，但在处理多变量输入时性能较低，这可能与Transformer的记忆偏差有关。

**方法:** 1. 使用合成数据集定量评估Transformer的记忆偏差。
2. 理论分析记忆偏差的来源，发现其无法组合构建表达式并验证数值有效性。
3. 测试不同的测试时策略以减少记忆偏差并观察性能变化。

**结果:** 1. 发现Transformer很少生成训练数据中不存在的表达式。
2. 提供额外信息可显著减轻记忆偏差。
3. 减少记忆偏差不一定会带来性能提升。

**结论:** 本研究揭示了NSR方法中记忆偏差的根源，并为设计更鲁棒、泛化性更强的符号回归方法提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Test-time+Computation+Mitigate+Memorization+Bias+in+Neural+Symbolic+Regression%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22081&send_immediately=true&force_search=false)

**原文摘要:** Symbolic regression aims to discover mathematical equations that fit given
numerical data. It has been applied in various fields of scientific research,
such as producing human-readable expressions that explain physical phenomena.
Recently, Neural symbolic regression (NSR) methods that involve Transformers
pre-trained on large-scale synthetic datasets have gained attention. While
these methods offer advantages such as short inference time, they suffer from
low performance, particularly when the number of input variables is large. In
this study, we hypothesized that this limitation stems from the memorization
bias of Transformers in symbolic regression. We conducted a quantitative
evaluation of this bias in Transformers using a synthetic dataset and found
that Transformers rarely generate expressions not present in the training data.
Additional theoretical analysis reveals that this bias arises from the
Transformer's inability to construct expressions compositionally while
verifying their numerical validity. We finally examined if tailoring test-time
strategies can lead to reduced memorization bias and better performance. We
empirically demonstrate that providing additional information to the model at
test time can significantly mitigate memorization bias. On the other hand, we
also find that reducing memorization bias does not necessarily correlate with
improved performance. These findings contribute to a deeper understanding of
the limitations of NSR approaches and offer a foundation for designing more
robust, generalizable symbolic regression methods. Code is available at
https://github.com/Shun-0922/Mem-Bias-NSR .

</details>


### [102] [BiMi Sheets: Infosheets for bias mitigation methods](https://arxiv.org/abs/2505.22114)
*MaryBeth Defrance, Guillaume Bied, Maarten Buyl, Jefrey Lijffijt, Tijl De Bie*

**主要类别:** cs.LG

**概要:** 在过去15年中，虽然提出了数百种机器学习(ML)中的偏差缓解方法，但由于算法偏差具有领域、任务和模型特定性，导致了'可移植性陷阱'：一种情境下的偏差缓解方案可能在另一种情境下并不适用。因此，在创建偏差缓解方法时需要做出大量设计选择，如其追求的公平性形式化以及在ML管道中干预的位置和方式。这给基准测试和比较不同偏差缓解方法的相对优劣带来了挑战，并限制了它们在实践者中的应用。我们提出了BiMi Sheets作为一种便携式、统一的指南，用于记录任何偏差缓解方法的设计选择。这使得研究人员和实践者能够快速了解其主要特征并与他们的期望进行比较。此外，表格结构允许创建偏差缓解方法的结构化数据库。为了促进表格的采用，我们在bimisheet.com上提供了一个查找和创建BiMi Sheets的平台。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有数百种偏差缓解方法被提出，但这些方法在不同领域、任务和模型中表现各异，缺乏可移植性和统一的比较标准，限制了实践者对这些方法的应用。

**方法:** 提出了一种名为BiMi Sheets的便携式、统一指南来记录偏差缓解方法的设计选择，并提供了一个平台（bimisheet.com）以支持查找和创建这些表格。

**结果:** 通过BiMi Sheets，研究人员和实践者可以更快速地了解偏差缓解方法的主要特征并进行比较，同时还可以创建一个结构化的偏差缓解方法数据库。

**结论:** BiMi Sheets为偏差缓解方法提供了一个统一的记录和比较框架，有助于推动这些方法的研究与实际应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BiMi+Sheets%3A+Infosheets+for+bias+mitigation+methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22114，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22114&send_immediately=true&force_search=false)

**原文摘要:** Over the past 15 years, hundreds of bias mitigation methods have been
proposed in the pursuit of fairness in machine learning (ML). However,
algorithmic biases are domain-, task-, and model-specific, leading to a
`portability trap': bias mitigation solutions in one context may not be
appropriate in another. Thus, a myriad of design choices have to be made when
creating a bias mitigation method, such as the formalization of fairness it
pursues, and where and how it intervenes in the ML pipeline. This creates
challenges in benchmarking and comparing the relative merits of different bias
mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design
choices of any bias mitigation method. This enables researchers and
practitioners to quickly learn its main characteristics and to compare with
their desiderata. Furthermore, the sheets' structure allow for the creation of
a structured database of bias mitigation methods. In order to foster the
sheets' adoption, we provide a platform for finding and creating BiMi Sheets at
bimisheet.com.

</details>


### [103] [Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL](https://arxiv.org/abs/2505.22151)
*Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon Du Toit, Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的离线多智能体强化学习算法Oryx，它在复杂环境中实现了有效的多步协调，并在多种基准测试中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 离线多智能体强化学习中的主要挑战是在复杂环境中实现有效的多步协调。为了解决这一问题，需要一种新的算法来直接应对该挑战。

**方法:** 提出了Oryx算法，结合了最近提出的基于保留的Sable架构和隐式约束Q学习（ICQ）的序列形式，开发了一种新颖的离线自回归策略更新方案。

**结果:** Oryx在超过65个测试数据集中，在80%以上的情况下达到了最先进的性能，优于以前的离线MARL方法，并展示了在具有许多智能体和长时域的任务上的稳健泛化能力。

**结论:** Oryx算法在解决复杂协调问题的同时保持了长时间轨迹的时间连贯性，并且在离线多智能体强化学习领域表现出色。此外，作者还引入了新数据集以进一步推动研究界限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Oryx%3A+a+Performant+and+Scalable+Algorithm+for+Many-Agent+Coordination+in+Offline+MARL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22151，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22151&send_immediately=true&force_search=false)

**原文摘要:** A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.

</details>


### [104] [Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory](https://arxiv.org/abs/2505.22152)
*Dominik Fuchsgruber, Tom Wollschläger, Johannes Bordne, Stephan Günnemann*

**主要类别:** cs.LG

**概要:** 这篇论文探讨了图神经网络在异质性图中的不确定性估计问题，提出了一种基于信息论的分析方法，并通过实验证明了考虑所有节点表示的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图不确定性估计方法大多依赖于同质性假设，在异质性图中表现较差。因此，需要一种新的方法来处理异质性图中的不确定性估计问题。

**方法:** 从信息论的角度分析消息传递神经网络，开发了一个类似于数据处理不等式的度量方法，用于量化模型各层中的信息。此外，提出了一种简单的后验密度估计器，该估计器作用于联合节点嵌入空间，以提供最先进的异质性图不确定性估计。

**结果:** 实验结果表明，所提出的后验密度估计器在异质性图上提供了最先进的不确定性估计，并且在同质性图上也能与先前的工作相匹配，而无需显式利用同质性进行后处理。

**结论:** 考虑所有节点表示的同时性是超越同质性假设进行认知不确定性估计的关键设计原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+Estimation+for+Heterophilic+Graphs+Through+the+Lens+of+Information+Theory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22152，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22152&send_immediately=true&force_search=false)

**原文摘要:** While uncertainty estimation for graphs recently gained traction, most
methods rely on homophily and deteriorate in heterophilic settings. We address
this by analyzing message passing neural networks from an information-theoretic
perspective and developing a suitable analog to data processing inequality to
quantify information throughout the model's layers. In contrast to non-graph
domains, information about the node-level prediction target can increase with
model depth if a node's features are semantically different from its neighbors.
Therefore, on heterophilic graphs, the latent embeddings of an MPNN each
provide different information about the data distribution - different from
homophilic settings. This reveals that considering all node representations
simultaneously is a key design principle for epistemic uncertainty estimation
on graphs beyond homophily. We empirically confirm this with a simple post-hoc
density estimator on the joint node embedding space that provides
state-of-the-art uncertainty on heterophilic graphs. At the same time, it
matches prior work on homophilic graphs without explicitly exploiting homophily
through post-processing.

</details>


### [105] [The informativeness of the gradient revisited](https://arxiv.org/abs/2505.22158)
*Rustem Takhanov*

**主要类别:** cs.LG

**概要:** 本研究探讨了梯度信息在深度学习中的局限性，并提出了一种新的方差界来衡量梯度信息量。该界与目标函数类的成对独立性和输入分布的碰撞熵相关。通过应用到LWE映射和高频函数，展示了其实际效用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管梯度基学习取得了显著进展，但许多实际任务中梯度信息非常有限，导致需要大量迭代才能成功。因此，研究梯度信息量的理论限制变得尤为重要。

**方法:** 1. 提出一个通用的方差界，基于目标函数类的成对独立性和输入分布的碰撞熵。
2. 将此界应用于Learning with Errors (LWE) 映射和高频函数，验证其有效性。
3. 进行实验分析，探索近期基于深度学习的LWE攻击的本质。

**结果:** 给出了梯度方差的通用界，形式为$\tilde{\mathcal{O}}(\varepsilon + e^{-\frac{1}{2}\mathcal{E}_c})$，并证明了其在LWE映射和高频函数上的实用性。实验结果加深了对深度学习在LWE问题上表现的理解。

**结论:** 本研究揭示了梯度信息的理论限制，并为理解深度学习在某些问题上的困难提供了一个新视角。提出的方差界可以作为未来研究的重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+informativeness+of+the+gradient+revisited，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22158&send_immediately=true&force_search=false)

**原文摘要:** In the past decade gradient-based deep learning has revolutionized several
applications. However, this rapid advancement has highlighted the need for a
deeper theoretical understanding of its limitations. Research has shown that,
in many practical learning tasks, the information contained in the gradient is
so minimal that gradient-based methods require an exceedingly large number of
iterations to achieve success. The informativeness of the gradient is typically
measured by its variance with respect to the random selection of a target
function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a
parameter related to the pairwise independence of the target function class and
the collision entropy of the input distribution. Our bound scales as $
\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $
\tilde{\mathcal{O}} $ hides factors related to the regularity of the learning
model and the loss function, $ \varepsilon $ measures the pairwise independence
of the target function class and $\mathcal{E}_c$ is the collision entropy of
the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class
of Learning with Errors (LWE) mappings and high-frequency functions. In
addition to the theoretical analysis, we present experiments to understand
better the nature of recent deep learning-based attacks on LWE.

</details>


### [106] [An Augmentation-Aware Theory for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.22196)
*Jingyi Cui, Hongwei Wen, Yisen Wang*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一个与数据增强相关的误差界，揭示了特定增强方法对自我监督对比学习的影响，并通过像素级和表示级实验验证了理论结果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管自我监督对比学习在机器学习和计算机视觉领域取得了显著成功，但现有理论研究中，数据增强的作用（尤其是具体增强类型的影响）尚未被充分探讨。

**方法:** 作者首次提出了一种考虑数据增强的误差界，表明监督风险不仅受无监督风险影响，还由数据增强引发的权衡决定。同时，在新的语义标签假设下，分析了某些增强方法如何影响误差界。最后，进行了像素级和表示级实验以验证理论结果。

**结果:** 提出的数据增强相关误差界能够解释特定增强方法对自我监督对比学习的影响，且理论结果在实验中得到了验证。

**结论:** 该研究填补了关于数据增强在自我监督对比学习中的作用的理论空白，并为设计更有效的增强策略提供了理论依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Augmentation-Aware+Theory+for+Self-Supervised+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22196&send_immediately=true&force_search=false)

**原文摘要:** Self-supervised contrastive learning has emerged as a powerful tool in
machine learning and computer vision to learn meaningful representations from
unlabeled data. Meanwhile, its empirical success has encouraged many
theoretical studies to reveal the learning mechanisms. However, in the existing
theoretical research, the role of data augmentation is still under-exploited,
especially the effects of specific augmentation types. To fill in the blank, we
for the first time propose an augmentation-aware error bound for
self-supervised contrastive learning, showing that the supervised risk is
bounded not only by the unsupervised risk, but also explicitly by a trade-off
induced by data augmentation. Then, under a novel semantic label assumption, we
discuss how certain augmentation methods affect the error bound. Lastly, we
conduct both pixel- and representation-level experiments to verify our proposed
theoretical results.

</details>


### [107] [LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models](https://arxiv.org/abs/2505.22208)
*Yosuke Oyama, Yusuke Majima, Eiji Ohta, Yasufumi Sakai*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法LaMM，用于神经网络势（NNP）的半监督预训练，包含改进的去噪自监督学习和负载均衡算法，以更高效地利用大规模数据集进行训练并提升微调性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前通过预训练和微调提高NNP精度的方法存在计算成本高、DFT数据标注昂贵以及大规模预训练中负载不平衡的问题。

**方法:** 提出LaMM方法，结合改进的去噪自监督学习和负载均衡算法，利用约3亿个半标记样本的大规模数据集进行NNP模型的半监督预训练。

**结果:** 有效利用大规模数据集训练单一NNP模型，提升了微调的速度和准确性。

**结论:** LaMM方法能够更高效地进行NNP模型的预训练，并在微调阶段表现出更好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LaMM%3A+Semi-Supervised+Pre-Training+of+Large-Scale+Materials+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22208&send_immediately=true&force_search=false)

**原文摘要:** Neural network potentials (NNPs) are crucial for accelerating computational
materials science by surrogating density functional theory (DFT) calculations.
Improving their accuracy is possible through pre-training and fine-tuning,
where an NNP model is first pre-trained on a large-scale dataset and then
fine-tuned on a smaller target dataset. However, this approach is
computationally expensive, mainly due to the cost of DFT-based dataset labeling
and load imbalances during large-scale pre-training. To address this, we
propose LaMM, a semi-supervised pre-training method incorporating improved
denoising self-supervised learning and a load-balancing algorithm for efficient
multi-node training. We demonstrate that our approach effectively leverages a
large-scale dataset of $\sim$300 million semi-labeled samples to train a single
NNP model, resulting in improved fine-tuning performance in terms of both speed
and accuracy.

</details>


### [108] [Optimal kernel regression bounds under energy-bounded noise](https://arxiv.org/abs/2505.22235)
*Amon Lahr, Johannes Köhler, Anna Scampicchio, Melanie N. Zeilinger*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种针对基于核函数估计的紧密非渐进不确定性边界计算方法，该方法在安全关键应用中具有重要意义。通过优化测量噪声协方差，返回假设类中最坏情况下的函数实现，从而提供易于计算且精确的边界。


<details>
  <summary>更多</summary>
  
**动机:** 评估估计算法的准确性以及在下游任务中的部署（如在安全性关键场景），需要非保守的不确定性边界。现有的方法可能不够紧密或难以处理相关噪声序列。

**方法:** 推导出一种紧密、非渐进的不确定性边界，适用于基于核函数的估计，并能处理相关噪声序列。其计算依赖于未知函数和噪声的温和范数有界性假设，返回假设类中最坏情况下的函数实现。利用高斯过程的后验均值和协方差来确定测量噪声协方差的最优选择。

**结果:** 通过严格分析所提出的方法并与文献中的其他结果进行比较，证明了该方法能够有效返回紧密且易于计算的边界，适用于基于核函数的估计。

**结论:** 所提出的不确定性边界计算方法为基于核函数的估计提供了紧密且易于计算的边界，对于实际应用特别是安全性关键场景具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+kernel+regression+bounds+under+energy-bounded+noise，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22235&send_immediately=true&force_search=false)

**原文摘要:** Non-conservative uncertainty bounds are key for both assessing an estimation
algorithm's accuracy and in view of downstream tasks, such as its deployment in
safety-critical contexts. In this paper, we derive a tight, non-asymptotic
uncertainty bound for kernel-based estimation, which can also handle correlated
noise sequences. Its computation relies on a mild norm-boundedness assumption
on the unknown function and the noise, returning the worst-case function
realization within the hypothesis class at an arbitrary query input location.
The value of this function is shown to be given in terms of the posterior mean
and covariance of a Gaussian process for an optimal choice of the measurement
noise covariance. By rigorously analyzing the proposed approach and comparing
it with other results in the literature, we show its effectiveness in returning
tight and easy-to-compute bounds for kernel-based estimates.

</details>


### [109] [B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data](https://arxiv.org/abs/2505.22252)
*Magdalena Proszewska, Tomasz Danel, Dawid Rymarczyk*

**主要类别:** cs.LG

**概要:** B-XAIC是一个新的基准测试，基于真实分子数据和具有已知标签分配基本原理的多样化任务构建，用于评估分子领域中现有的GNN的XAI方法的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 当前在化学信息学和药物发现领域的可解释人工智能（XAI）评估框架，通常依赖于人工数据集或简化任务，使用无法反映真实场景复杂性的数据衍生指标，并且与解释的忠实度缺乏直接联系。

**方法:** 引入了B-XAIC，这是一个由真实世界分子数据和具有已知基础理的任务组成的新型基准。通过全面评估B-XAIC，揭示了分子领域中现有GNN的XAI方法的局限性。

**结果:** 展示了现有XAI方法在处理真实世界分子数据时的不足，强调了对更可靠和可解释模型的需求。

**结论:** B-XAIC为深入理解XAI的忠实度提供了一个有价值的资源，有助于推动更可靠和可解释模型的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是B-XAIC+Dataset%3A+Benchmarking+Explainable+AI+for+Graph+Neural+Networks+Using+Chemical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22252，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22252&send_immediately=true&force_search=false)

**原文摘要:** Understanding the reasoning behind deep learning model predictions is crucial
in cheminformatics and drug discovery, where molecular design determines their
properties. However, current evaluation frameworks for Explainable AI (XAI) in
this domain often rely on artificial datasets or simplified tasks, employing
data-derived metrics that fail to capture the complexity of real-world
scenarios and lack a direct link to explanation faithfulness. To address this,
we introduce B-XAIC, a novel benchmark constructed from real-world molecular
data and diverse tasks with known ground-truth rationales for assigned labels.
Through a comprehensive evaluation using B-XAIC, we reveal limitations of
existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.
This benchmark provides a valuable resource for gaining deeper insights into
the faithfulness of XAI, facilitating the development of more reliable and
interpretable models.

</details>


### [110] [A Unified Online-Offline Framework for Co-Branding Campaign Recommendations](https://arxiv.org/abs/2505.22254)
*Xiangxiang Dai, Xiaowei Sun, Jinhang Zuo, Xutong Liu, John C. S. Lui*

**主要类别:** cs.LG

**概要:** 论文提出了一种统一的线上-线下框架，用于跨行业品牌合作推荐。通过构建二分图量化合作概率和市场收益，并在探索新合作与利用现有合作间取得平衡。框架还优化了子品牌的资源分配以减少成本并最大化回报。实验表明该方法至少提高了12%的效果。


<details>
  <summary>更多</summary>
  
**动机:** 跨行业品牌合作（co-branding）是企业扩大市场影响力的重要策略，但找到有效的合作伙伴面临资源不平衡、品牌意愿不确定及市场变化快等挑战。

**方法:** 1. 构建二分图连接“发起”和“目标”品牌，计算合作概率和评估市场收益。
2. 在线学习阶段动态更新图结构，平衡探索新合作和利用已有合作。
3. 离线优化阶段整合子品牌利益以最大化整体回报并降低成本。
4. 提供理论分析，包括次线性后悔界和近似保证。

**结果:** 理论分析证明了在线学习的次线性后悔界和离线优化的近似保证。实验结果表明，该框架在合成数据和真实数据集上均有效，提升了至少12%的表现。

**结论:** 提出的统一框架能有效解决跨行业品牌合作中的挑战，提高短期表现并确保长期战略增长。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Unified+Online-Offline+Framework+for+Co-Branding+Campaign+Recommendations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22254&send_immediately=true&force_search=false)

**原文摘要:** Co-branding has become a vital strategy for businesses aiming to expand
market reach within recommendation systems. However, identifying effective
cross-industry partnerships remains challenging due to resource imbalances,
uncertain brand willingness, and ever-changing market conditions. In this
paper, we provide the first systematic study of this problem and propose a
unified online-offline framework to enable co-branding recommendations. Our
approach begins by constructing a bipartite graph linking ``initiating'' and
``target'' brands to quantify co-branding probabilities and assess market
benefits. During the online learning phase, we dynamically update the graph in
response to market feedback, while striking a balance between exploring new
collaborations for long-term gains and exploiting established partnerships for
immediate benefits. To address the high initial co-branding costs, our
framework mitigates redundant exploration, thereby enhancing short-term
performance while ensuring sustainable strategic growth. In the offline
optimization phase, our framework consolidates the interests of multiple
sub-brands under the same parent brand to maximize overall returns, avoid
excessive investment in single sub-brands, and reduce unnecessary costs
associated with over-prioritizing a single sub-brand. We present a theoretical
analysis of our approach, establishing a highly nontrivial sublinear regret
bound for online learning in the complex co-branding problem, and enhancing the
approximation guarantee for the NP-hard offline budget allocation optimization.
Experiments on both synthetic and real-world co-branding datasets demonstrate
the practical effectiveness of our framework, with at least 12\% improvement.

</details>


### [111] [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
*Vadim Kurochkin, Yaroslav Aksenov, Daniil Laptev, Daniil Gavrilov, Nikita Balagansky*

**主要类别:** cs.LG

**概要:** KronSAE通过克罗内克积分解潜在表示，减少内存和计算开销，并引入mAND激活函数提升可解释性和性能。


<details>
  <summary>更多</summary>
  
**动机:** Sparse Autoencoders (SAEs)在解释语言模型的隐藏状态方面表现出潜力，但大规模训练尤其是使用大型词典时仍面临挑战，主要因为编码器需要高计算成本的线性操作。

**方法:** 提出了一种名为KronSAE的新架构，利用克罗内克积分解来因子化潜在表示，大幅降低内存和计算需求；同时引入了mAND，一种可微分的激活函数，用于近似二进制AND操作，从而增强在因子化框架中的可解释性和性能。

**结果:** KronSAE显著降低了计算和内存开销，同时通过mAND提高了模型的可解释性和性能。

**结论:** KronSAE为大规模SAE训练提供了一个有效的解决方案，结合mAND可以更好地平衡效率与模型表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Train+Sparse+Autoencoders+Efficiently+by+Utilizing+Features+Correlation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22255&send_immediately=true&force_search=false)

**原文摘要:** Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.

</details>


### [112] [Full Domain Analysis in Fluid Dynamics](https://arxiv.org/abs/2505.22275)
*Alexander Hagg, Adam Gaier, Dominik Wilde, Alexander Asteroth, Holger Foysi, Dirk Reith*

**主要类别:** cs.LG

**概要:** 新型进化优化、模拟和机器学习技术能够广泛分析诸如流体动力学等领域，其中计算成本高昂且流动行为复杂。全文域分析旨在高效确定问题域中的完整解空间，并以可访问和交互的方式分析这些解的行为。通过生成许多流动实例及其多样化、优化和分析，加深对领域的理解。本文定义了全文域分析的形式模型、其当前的技术状态和子组件要求，并给出一个示例展示其应用价值。全文域分析作为源于优化和机器学习的工具，有助于理解计算物理及更广泛领域的复杂系统。


<details>
  <summary>更多</summary>
  
**动机:** 流体动力学等领域的计算成本高且流动行为复杂，需要一种方法能够全面分析问题域的解空间并深入理解领域特性。

**方法:** 1. 定义全文域分析的形式模型。
2. 阐述其当前的技术水平。
3. 明确子组件的要求。
4. 提供一个应用示例展示全文域分析的能力。

**结果:** 提供了一种形式化模型和方法论，用于执行全文域分析，展示了其在理解和分析复杂系统中的潜力。

**结论:** 全文域分析是一种结合优化和机器学习技术的有力工具，能够帮助研究人员深入理解计算物理及其他领域的复杂系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Full+Domain+Analysis+in+Fluid+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22275，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22275&send_immediately=true&force_search=false)

**原文摘要:** Novel techniques in evolutionary optimization, simulation and machine
learning allow for a broad analysis of domains like fluid dynamics, in which
computation is expensive and flow behavior is complex. Under the term of full
domain analysis we understand the ability to efficiently determine the full
space of solutions in a problem domain, and analyze the behavior of those
solutions in an accessible and interactive manner. The goal of full domain
analysis is to deepen our understanding of domains by generating many examples
of flow, their diversification, optimization and analysis. We define a formal
model for full domain analysis, its current state of the art, and requirements
of subcomponents. Finally, an example is given to show what we can learn by
using full domain analysis. Full domain analysis, rooted in optimization and
machine learning, can be a helpful tool in understanding complex systems in
computational physics and beyond.

</details>


### [113] [Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning](https://arxiv.org/abs/2505.22308)
*Zachary Shinnick, Liangze Jiang, Hemanth Saratchandran, Anton van den Hengel, Damien Teney*

**主要类别:** cs.LG

**概要:** 预训练对开发语言模型至关重要。即使使用通过简单无语义算法生成的合成数据，也能带来与自然语言预训练相似的好处。本文研究发现不同的程序规则会在模型中灌输不同但互补的归纳结构，这些结构存在于模型的不同部分，并且由多规则诱导的结构可以组合以共同增强多种能力。


<details>
  <summary>更多</summary>
  
**动机:** 探索简单的合成数据在语言模型中的作用和影响，以及它们如何改变模型的权重和架构中的特定能力。

**方法:** 识别几种有益的程序数据形式及特定算法推理技能，通过广泛的消融实验和部分迁移实验，分析不同程序规则如何在模型中灌输不同的归纳结构。

**结果:** 不同的程序规则灌输了不同但互补的归纳结构，这些结构存在于模型的不同部分，注意力层通常携带最可转移的信息，而某些预训练规则则赋予MLP块有用的结构。多个规则诱导的结构可以组合以共同增强多种能力。

**结论:** 结果表明，有可能将语言模型的知识获取与推理分离，以提高其稳健性和数据效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformers+Pretrained+on+Procedural+Data+Contain+Modular+Structures+for+Algorithmic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22308，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22308&send_immediately=true&force_search=false)

**原文摘要:** Pretraining on large, semantically rich datasets is key for developing
language models. Surprisingly, recent studies have shown that even synthetic
data, generated procedurally through simple semantic-free algorithms, can yield
some of the same benefits as natural language pretraining. It is unclear what
specific capabilities such simple synthetic data instils in a model, where
these capabilities reside in the architecture, and how they manifest within its
weights. In this short paper, we identify several beneficial forms of
procedural data, together with specific algorithmic reasoning skills that
improve in small transformers. Our core finding is that different procedural
rules instil distinct but complementary inductive structures in the model. With
extensive ablations and partial-transfer experiments, we discover that these
structures reside in different parts of the model. Attention layers often carry
the most transferable information, but some pretraining rules impart useful
structure to MLP blocks instead. Most interestingly, the structures induced by
multiple rules can be composed to jointly reinforce multiple capabilities.
These results suggest an exciting possibility of disentangling the acquisition
of knowledge from reasoning in language models, with the goal of improving
their robustness and data efficiency.

</details>


### [114] [Rethinking BPS: A Utility-Based Evaluation Framework](https://arxiv.org/abs/2505.22316)
*Konrad Özdemir, Lukas Kirchdorfer, Keyvan Amiri Elyasi, Han van der Aa, Heiner Stuckenschmidt*

**主要类别:** cs.LG

**概要:** 提出了一种新的BPS模型评估框架，通过比较基于模拟数据和真实数据训练的预测过程监控模型的表现，来评估模拟生成的流程行为的代表性，解决了现有方法无法区分模型不准确和数据复杂性的问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前BPS模型评估方法存在两个主要问题：1) 将模拟视为预测问题，难以评估模型对现状流程的捕捉能力；2) 过度依赖EMD指标，可能掩盖时间模式。这促使了对更精确评估方法的需求。

**方法:** 提出一个新框架，评估模拟质量是否能生成有代表性的流程行为。通过对比在模拟数据和真实数据上训练的预测过程监控模型在下游任务中的表现，判断模拟数据的质量。

**结果:** 实证结果表明，该框架不仅能识别差异来源，还能区分模型准确性和数据复杂性，提供更有意义的BPS质量评估方式。

**结论:** 新框架为BPS质量评估提供了更有效的方法，能够更好地区分模型不足与数据固有复杂性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+BPS%3A+A+Utility-Based+Evaluation+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22316，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22316&send_immediately=true&force_search=false)

**原文摘要:** Business process simulation (BPS) is a key tool for analyzing and optimizing
organizational workflows, supporting decision-making by estimating the impact
of process changes. The reliability of such estimates depends on the ability of
a BPS model to accurately mimic the process under analysis, making rigorous
accuracy evaluation essential. However, the state-of-the-art approach to
evaluating BPS models has two key limitations. First, it treats simulation as a
forecasting problem, testing whether models can predict unseen future events.
This fails to assess how well a model captures the as-is process, particularly
when process behavior changes from train to test period. Thus, it becomes
difficult to determine whether poor results stem from an inaccurate model or
the inherent complexity of the data, such as unpredictable drift. Second, the
evaluation approach strongly relies on Earth Mover's Distance-based metrics,
which can obscure temporal patterns and thus yield misleading conclusions about
simulation quality. To address these issues, we propose a novel framework that
evaluates simulation quality based on its ability to generate representative
process behavior. Instead of comparing simulated logs to future real-world
executions, we evaluate whether predictive process monitoring models trained on
simulated data perform comparably to those trained on real data for downstream
analysis tasks. Empirical results show that our framework not only helps
identify sources of discrepancies but also distinguishes between model accuracy
and data complexity, offering a more meaningful way to assess BPS quality.

</details>


### [115] [A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective](https://arxiv.org/abs/2505.22322)
*Zhengyu Fang, Zhimeng Jiang, Huiyuan Chen, Xiaoge Zhang, Kaiyu Tang, Xiao Li, Jing Li*

**主要类别:** cs.LG

**概要:** Diffusion models can generate high-quality tabular data but risk privacy by reproducing training samples. This study quantifies memorization per sample, revealing a heavy-tailed distribution where few samples cause most leakage. Based on this, the authors propose DynamicCut, a method to reduce memorization by pruning highly memorized samples, which works across different models and datasets with minimal impact on data diversity and performance.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods focus on dataset-level augmentation to reduce memorization in diffusion models, but there is limited understanding of which individual samples contribute most to memorization.

**方法:** The authors quantify memorization for each real sample using a relative distance ratio, analyze the training-time behaviors of top- and non-top-memorized samples, and propose DynamicCut, a two-stage mitigation method that ranks, prunes, and retrains based on memorization intensity.

**结果:** DynamicCut effectively reduces memorization across multiple tabular datasets and models, maintains data diversity and downstream performance, complements augmentation-based defenses, and enables cross-model transferability.

**结论:** DynamicCut provides an effective model-agnostic approach to mitigate memorization in tabular diffusion models by targeting highly memorized samples.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Closer+Look+on+Memorization+in+Tabular+Diffusion+Model%3A+A+Data-Centric+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22322，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22322&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have shown strong performance in generating high-quality
tabular data, but they carry privacy risks by reproducing exact training
samples. While prior work focuses on dataset-level augmentation to reduce
memorization, little is known about which individual samples contribute most.
We present the first data-centric study of memorization dynamics in tabular
diffusion models. We quantify memorization for each real sample based on how
many generated samples are flagged as replicas, using a relative distance
ratio. Our empirical analysis reveals a heavy-tailed distribution of
memorization counts: a small subset of samples contributes disproportionately
to leakage, confirmed via sample-removal experiments. To understand this, we
divide real samples into top- and non-top-memorized groups and analyze their
training-time behaviors. We track when each sample is first memorized and
monitor per-epoch memorization intensity (AUC). Memorized samples are memorized
slightly earlier and show stronger signals in early training. Based on these
insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:
(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and
(c) retrain on the filtered dataset. Across multiple tabular datasets and
models, DynamicCut reduces memorization with minimal impact on data diversity
and downstream performance. It also complements augmentation-based defenses.
Furthermore, DynamicCut enables cross-model transferability: high-ranked
samples identified from one model (e.g., a diffusion model) are also effective
for reducing memorization when removed from others, such as GANs and VAEs.

</details>


### [116] [Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning](https://arxiv.org/abs/2505.22355)
*Yongkang Liu, Xingle Xu, Ercong Nie, Zijing Wang, Shi Feng, Daling Wang, Qian Li, Hinrich Schütze*

**主要类别:** cs.LG

**概要:** 本文探讨了参数高效微调（PEFT）与全量微调（FFT）在表示能力和鲁棒性方面的特性差异，证明PEFT是FFT的严格子集，并通过实验验证了其局限性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管PEFT方法在资源消耗上远低于FFT，但在复杂任务（如推理和基于指令的微调）中表现不如FFT。因此需要从理论上分析PEFT和FFT之间的差异及局限性。

**方法:** 基于优化理论，比较PEFT和FFT在表示容量和鲁棒性方面的特性；通过理论推导证明PEFT是FFT的严格子集，并给出PEFT的理论上限；使用15个数据集和11个对抗测试集进行实验验证。

**结果:** 实验结果表明，受限的参数空间限制了PEFT模型的表示能力，使其更容易受到扰动的影响，且其性能在复杂任务中不如FFT。

**结论:** PEFT在资源效率上有优势，但在复杂任务中的表示能力和鲁棒性不足，未来研究应探索超越现有PEFT框架的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Look+Within+or+Look+Beyond%3F+A+Theoretical+Comparison+Between+Parameter-Efficient+and+Full+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22355&send_immediately=true&force_search=false)

**原文摘要:** Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable
to Full Fine-Tuning (FFT) while requiring significantly fewer computing
resources, making it the go-to choice for researchers. We find that although
PEFT can achieve competitive results on some benchmarks, its performance falls
short of FFT in complex tasks, such as reasoning and instruction-based
fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in
terms of representational capacity and robustness based on optimization theory.
We theoretically demonstrate that PEFT is a strict subset of FFT. By providing
theoretical upper bounds for PEFT, we show that the limited parameter space
constrains the model's representational ability, making it more susceptible to
perturbations. Experiments on 15 datasets encompassing classification,
generation, reasoning, instruction fine-tuning tasks and 11 adversarial test
sets validate our theories. We hope that these results spark further research
beyond the realms of well established PEFT. The source code is in the anonymous
Github repository\footnote{https://github.com/misonsky/PEFTEval}.

</details>


### [117] [Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification](https://arxiv.org/abs/2505.22359)
*Matan Schliserman, Tomer Koren*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiclass+Loss+Geometry+Matters+for+Generalization+of+Gradient+Descent+in+Separable+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22359，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22359&send_immediately=true&force_search=false)

**原文摘要:** We study the generalization performance of unregularized gradient methods for
separable linear classification. While previous work mostly deal with the
binary case, we focus on the multiclass setting with $k$ classes and establish
novel population risk bounds for Gradient Descent for loss functions that decay
to zero. In this setting, we show risk bounds that reveal that convergence
rates are crucially influenced by the geometry of the loss template, as
formalized by Wang and Scott (2024), rather than of the loss function itself.
Particularly, we establish risk upper bounds that holds for any decay rate of
the loss whose template is smooth with respect to the $p$-norm. In the case of
exponentially decaying losses, our results indicates a contrast between the
$p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and
$p=2$ where the risk scales linearly with $k$. To establish this separation
formally, we also prove a lower bound in the latter scenario, demonstrating
that the polynomial dependence on $k$ is unavoidable. Central to our analysis
is a novel bound on the Rademacher complexity of low-noise vector-valued linear
predictors with a loss template smooth w.r.t.~general $p$-norms.

</details>


### [118] [Directed Homophily-Aware Graph Neural Network](https://arxiv.org/abs/2505.22362)
*Aihu Zhang, Jiaxing Xu, Mengcheng Lan, Shili Xiang, Yiping Ke*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的图神经网络框架DHGNN，通过结合同质性和方向敏感组件来解决现有GNN在异质性邻域和有向图上的泛化问题。实验表明DHGNN在节点分类和链接预测任务上优于现有方法，并揭示了消息传递行为的深层机制。


<details>
  <summary>更多</summary>
  
**动机:** 尽管图神经网络（GNNs）在处理图结构数据方面取得了显著成功，但大多数GNN难以推广到异质性邻域，并且忽略了现实世界图中的方向性特征，导致在具有不对称结构的有向图上表现不佳。为了解决这些问题，提出了一个新的框架。

**方法:** 1. 提出Directed Homophily-aware Graph Neural Network (DHGNN)框架。
2. 引入重置门机制，根据同质性水平和信息量自适应地调节消息贡献。
3. 使用结构感知噪声容忍融合模块，有效集成来自原始和反向方向的节点表示。
4. 在同质性和异质性有向图数据集上进行广泛的实验。

**结果:** DHGNN在节点分类和链接预测任务上均优于现有最先进的方法。特别是在链接预测任务中，相较于最佳基线模型，DHGNN性能提升高达15.07%。分析还表明，门机制能够捕捉方向性同质性差异和跨层波动的同质性，从而提供了关于复杂图结构上消息传递行为的更深入见解。

**结论:** DHGNN通过引入同质性感知和方向敏感组件，有效解决了现有GNN在异质性邻域和有向图上的局限性。其出色的表现和对消息传递行为的深入解析为未来研究提供了重要参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Directed+Homophily-Aware+Graph+Neural+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22362，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22362&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.

</details>


### [119] [A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation](https://arxiv.org/abs/2505.22381)
*Lukas Kirchdorfer, Konrad Özdemir, Stjepan Kusenic, Han van der Aa, Heiner Stuckenschmidt*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法AT-KDE，用于改进业务流程模拟中的案例到达建模，通过结合全局动态、周内变化和日内分布变化来提高准确性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的案例到达建模方法通常依赖过于简化的静态分布，无法捕捉组织环境中固有的动态和时间复杂性，导致结果不够准确和可靠。

**方法:** 提出了自动时间核密度估计（AT-KDE），一种分而治之的方法，该方法通过结合全局动态、周内变化和日内分布变化来对过程的到达时间进行建模。

**结果:** 在20个不同过程的实验中，AT-KDE比现有方法更准确、更稳健，并且保持了合理的执行时间效率。

**结论:** AT-KDE为业务流程模拟提供了更精确和可扩展的案例到达建模方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Divide-and-Conquer+Approach+for+Modeling+Arrival+Times+in+Business+Process+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22381&send_immediately=true&force_search=false)

**原文摘要:** Business Process Simulation (BPS) is a critical tool for analyzing and
improving organizational processes by estimating the impact of process changes.
A key component of BPS is the case-arrival model, which determines the pattern
of new case entries into a process. Although accurate case-arrival modeling is
essential for reliable simulations, as it influences waiting and overall cycle
times, existing approaches often rely on oversimplified static distributions of
inter-arrival times. These approaches fail to capture the dynamic and temporal
complexities inherent in organizational environments, leading to less accurate
and reliable outcomes. To address this limitation, we propose Auto Time Kernel
Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival
times of processes by incorporating global dynamics, day-of-week variations,
and intraday distributional changes, ensuring both precision and scalability.
Experiments conducted across 20 diverse processes demonstrate that AT-KDE is
far more accurate and robust than existing approaches while maintaining
sensible execution time efficiency.

</details>


### [120] [STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals](https://arxiv.org/abs/2505.22422)
*Václav Voráček, Francesco Orabona*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种基于投注的算法来计算置信区间，该算法在固定水平设置下具有最优的宽度，并且在经验上优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 构建有界随机变量均值的置信区间是统计学中的经典问题，在机器学习和几乎所有科学领域都有广泛应用。特别是在样本获取成本较高的情况下，获得最紧凑的置信区间至关重要。现有的基于投注算法的方法虽然成功地推导出了最优的置信序列，但在固定水平设置下要么次优，要么缺乏有限时间保证。

**方法:** 作者提出了一种新的基于投注的算法，该算法在每一步都使用最优策略（在某种意义上），而不是像标准投注方法那样提前选择恒定策略。这导致了严格的改进，甚至对经典的Hoeffding或Bernstein不等式也是如此。

**结果:** 所提出的算法在经验上优于竞争对手，并证明其置信区间的宽度是最优的，最多相差一个随着n减小的1+o(1)因子。

**结论:** 该工作填补了现有方法在固定水平设置下缺乏有限时间保证的空白，并提供了一种更优的计算置信区间的方法，代码已在GitHub上公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STaR-Bets%3A+Sequential+Target-Recalculating+Bets+for+Tighter+Confidence+Intervals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22422，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22422&send_immediately=true&force_search=false)

**原文摘要:** The construction of confidence intervals for the mean of a bounded random
variable is a classical problem in statistics with numerous applications in
machine learning and virtually all scientific fields. In particular, obtaining
the tightest possible confidence intervals is vital every time the sampling of
the random variables is expensive. The current state-of-the-art method to
construct confidence intervals is by using betting algorithms. This is a very
successful approach for deriving optimal confidence sequences, even matching
the rate of law of iterated logarithms. However, in the fixed horizon setting,
these approaches are either sub-optimal or based on heuristic solutions with
strong empirical performance but without a finite-time guarantee. Hence, no
betting-based algorithm guaranteeing the optimal
$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the
confidence intervals are known. This work bridges this gap. We propose a
betting-based algorithm to compute confidence intervals that empirically
outperforms the competitors. Our betting strategy uses the optimal strategy in
every step (in a certain sense), whereas the standard betting methods choose a
constant strategy in advance. Leveraging this fact results in strict
improvements even for classical concentration inequalities, such as the ones of
Hoeffding or Bernstein. Moreover, we also prove that the width of our
confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.
The code is available
on~https://github.com/vvoracek/STaR-bets-confidence-interval.

</details>


### [121] [Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models](https://arxiv.org/abs/2505.22440)
*Khan Masood Parvez, Sk Md Abidar Rahaman, Ali Shiri Sichani*

**主要类别:** cs.LG

**概要:** 本研究展示了一种结合量子行为动态粒子群优化（QDPSO）算法和ANSYS HFSS仿真的机器学习增强工作流程，用于加速天线设计。通过优化、预测和ANSYS验证的完整设计周期仅需12.42分钟，比传统方法快240倍，大幅减少了工程工作量并确保了可生产的天线设计。


<details>
  <summary>更多</summary>
  
**动机:** 无线技术的快速发展需要在受限的开发周期内实现天线小型化和性能优化的自动化设计框架。传统的试错方法耗时且效率低，因此需要一种更快速和自动化的天线设计方法。

**方法:** 研究采用量子行为动态粒子群优化（QDPSO）算法进行天线环尺寸的自主优化，并使用ANSYS HFSS仿真进行验证。同时，利用机器学习模型（SVM、随机森林、XGBoost和堆叠集成）基于936个仿真数据集预测共振频率。

**结果:** QDPSO算法在11.53秒内将共振频率从常规的1.60 GHz降低到1.4208 GHz，减少了12.7%。机器学习模型在0.75秒内完成预测，其中堆叠模型训练精度最高（R²=0.9825），而SVM在验证中表现最佳（R²=0.7197）。整个设计周期只需12.42分钟，比传统方法快240倍。

**结论:** 该系统通过AI驱动的优化与CAD验证相结合，减少了工程工作量，确保了可生产的天线设计，为下一代6G和物联网应用的RF系统提供了可扩展的范例。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-Driven+Antenna+Miniaturization%3A+A+Knowledge-Based+System+Integrating+Quantum+PSO+and+Predictive+Machine+Learning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22440，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22440&send_immediately=true&force_search=false)

**原文摘要:** The rapid evolution of wireless technologies necessitates automated design
frameworks to address antenna miniaturization and performance optimization
within constrained development cycles. This study demonstrates a machine
learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm
Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.
The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,
achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared
to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,
XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds
using 936 simulation datasets, with stacked models showing superior training
accuracy (R2=0.9825) and SVM demonstrating optimal validation performance
(R2=0.7197). The complete design cycle, encompassing optimization, prediction,
and ANSYS validation, required 12.42 minutes on standard desktop hardware
(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of
PSADEA-based approaches. This 240 times of acceleration eliminates traditional
trial-and-error methods that often extend beyond seven expert-led days. The
system enables precise specifications of performance targets with automated
generation of fabrication-ready parameters, particularly benefiting compact
consumer devices requiring rapid frequency tuning. By bridging AI-driven
optimization with CAD validation, this framework reduces engineering workloads
while ensuring production-ready designs, establishing a scalable paradigm for
next-generation RF systems in 6G and IoT applications.

</details>


### [122] [Position: All Current Generative Fidelity and Diversity Metrics are Flawed](https://arxiv.org/abs/2505.22450)
*Ossi Räisä, Boris van Breugel, Mihaela van der Schaar*

**主要类别:** cs.LG

**概要:** 这篇论文指出当前生成模型的保真度和多样性指标存在缺陷，强调需要更多地关注开发更好的合成数据指标，而不是仅仅专注于模型本身。


<details>
  <summary>更多</summary>
  
**动机:** 生成建模的流行突显了对良好合成数据度量标准的需求，但现有度量标准存在许多失败案例，如缺乏异常值鲁棒性和界限不清的问题。

**方法:** 作者提出了一套合成数据度量的理想特性列表和一系列健全性检查实验，以检测特定和已知的生成建模失败模式。

**结果:** 所有现有的生成保真度和多样性度量都存在缺陷，这大大阻碍了合成数据的实际应用。

**结论:** 研究社区应投入更多精力开发更好的度量标准，并为从业者提供了关于如何（不）使用这些度量的指南。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+All+Current+Generative+Fidelity+and+Diversity+Metrics+are+Flawed，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22450，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22450&send_immediately=true&force_search=false)

**原文摘要:** Any method's development and practical application is limited by our ability
to measure its reliability. The popularity of generative modeling emphasizes
the importance of good synthetic data metrics. Unfortunately, previous works
have found many failure cases in current metrics, for example lack of outlier
robustness and unclear lower and upper bounds. We propose a list of desiderata
for synthetic data metrics, and a suite of sanity checks: carefully chosen
simple experiments that aim to detect specific and known generative modeling
failure modes. Based on these desiderata and the results of our checks, we
arrive at our position: all current generative fidelity and diversity metrics
are flawed. This significantly hinders practical use of synthetic data. Our aim
is to convince the research community to spend more effort in developing
metrics, instead of models. Additionally, through analyzing how current metrics
fail, we provide practitioners with guidelines on how these metrics should
(not) be used.

</details>


### [123] [Pure Exploration with Infinite Answers](https://arxiv.org/abs/2505.22473)
*Riccardo Poiani, Martino Bernasconi, Andrea Celli*

**主要类别:** cs.LG

**概要:** 研究了正确答案集合可能无限的情况下的纯探索问题，推导出实例依赖的下界，并提出一个框架 Sticky-Sequence Track-and-Stop 实现渐近最优。


<details>
  <summary>更多</summary>
  
**动机:** 研究无限正确答案集情况下的纯探索问题，分析现有方法为何在更一般的设定中无法实现渐近最优。

**方法:** 推导实例依赖的下界，提出Sticky-Sequence Track-and-Stop框架，该框架综合了Track-and-Stop和Sticky Track-and-Stop。

**结果:** 展示了Sticky-Sequence Track-and-Stop框架具有渐近最优性，并强调了现有方法在某些特殊情况下也具有最优性。

**结论:** 提出了一个适用于无限正确答案集的渐近最优框架，揭示了现有方法在特殊情况下的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pure+Exploration+with+Infinite+Answers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22473，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22473&send_immediately=true&force_search=false)

**原文摘要:** We study pure exploration problems where the set of correct answers is
possibly infinite, e.g., the regression of any continuous function of the means
of the bandit. We derive an instance-dependent lower bound for these problems.
By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)
for finite answer problems fail at being asymptotically optimal in this more
general setting. Finally, we present a framework, Sticky-Sequence
Track-and-Stop, which generalizes both Track-and-Stop and Sticky
Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality,
our analysis also highlights special cases where existing methods enjoy
optimality.

</details>


### [124] [Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis](https://arxiv.org/abs/2505.22474)
*Amirhossein Sohrabbeig, Omid Ardakanian, Petr Musilek*

**主要类别:** cs.LG

**概要:** A new multivariate time-series forecasting model using Graph Neural Networks (GNNs) is proposed for urban data prediction, which includes decomposition-based preprocessing to isolate trend, seasonal, and residual components. This approach enhances forecast accuracy and interpretability, showing potential for optimizing smart infrastructure systems.


<details>
  <summary>更多</summary>
  
**动机:** Multivariate urban data forecasting is complex due to intricate dependencies among different urban metrics such as weather, air pollution, carbon intensity, and energy demand.

**方法:** The paper introduces a novel model that uses advanced Graph Neural Networks (GNNs) with decomposition-based preprocessing to capture spatial dependencies and enhance forecast accuracy and interpretability.

**结果:** Extensive experiments on real-world datasets demonstrate the effectiveness of the model in various forecasting scenarios, indicating its potential to optimize smart infrastructure systems.

**结论:** The proposed model shows promise in contributing to energy-efficient urban development and enhancing public well-being through improved multivariate urban data forecasting.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forecasting+Multivariate+Urban+Data+via+Decomposition+and+Spatio-Temporal+Graph+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22474，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22474&send_immediately=true&force_search=false)

**原文摘要:** The forecasting of multivariate urban data presents a complex challenge due
to the intricate dependencies between various urban metrics such as weather,
air pollution, carbon intensity, and energy demand. This paper introduces a
novel multivariate time-series forecasting model that utilizes advanced Graph
Neural Networks (GNNs) to capture spatial dependencies among different
time-series variables. The proposed model incorporates a decomposition-based
preprocessing step, isolating trend, seasonal, and residual components to
enhance the accuracy and interpretability of forecasts. By leveraging the
dynamic capabilities of GNNs, the model effectively captures interdependencies
and improves the forecasting performance. Extensive experiments on real-world
datasets, including electricity usage, weather metrics, carbon intensity, and
air pollution data, demonstrate the effectiveness of the proposed approach
across various forecasting scenarios. The results highlight the potential of
the model to optimize smart infrastructure systems, contributing to
energy-efficient urban development and enhanced public well-being.

</details>


### [125] [Non-Asymptotic Analysis of (Sticky) Track-and-Stop](https://arxiv.org/abs/2505.22475)
*Riccardo Poiani, Martino Bernasconi, Andrea Celli*

**主要类别:** cs.LG

**概要:** 在纯探索问题中，Track-and-Stop算法及其扩展Sticky Track-and-Stop算法在环境映射到其正确答案为多值时同样表现优异。本文提供了这两种算法的非渐近性保证。


<details>
  <summary>更多</summary>
  
**动机:** 在纯探索问题中，统计学家需要收集信息以回答关于某些随机且未知环境的问题，并确保返回错误答案的概率不超过最大风险参数δ。而现有的Track-and-Stop算法在环境映射到其正确答案为单值时表现良好，但尚未解决多值情况下的问题。

**方法:** 研究者对Track-and-Stop算法和Sticky Track-and-Stop算法进行了分析，并填补了这两种算法在非渐近性保证方面的空白。

**结果:** 该研究成功提供了两种算法的非渐近性保证，进一步完善了它们在不同环境下的适用性和可靠性。

**结论:** Track-and-Stop及Sticky Track-and-Stop算法不仅在渐近情况下具有优越性，在非渐近情况下也有可靠的性能保证，这拓宽了它们的应用范围。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-Asymptotic+Analysis+of+%28Sticky%29+Track-and-Stop，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22475&send_immediately=true&force_search=false)

**原文摘要:** In pure exploration problems, a statistician sequentially collects
information to answer a question about some stochastic and unknown environment.
The probability of returning a wrong answer should not exceed a maximum risk
parameter $\delta$ and good algorithms make as few queries to the environment
as possible. The Track-and-Stop algorithm is a pioneering method to solve these
problems. Specifically, it is well-known that it enjoys asymptotic optimality
sample complexity guarantees for $\delta\to 0$ whenever the map from the
environment to its correct answers is single-valued (e.g., best-arm
identification with a unique optimal arm). The Sticky Track-and-Stop algorithm
extends these results to settings where, for each environment, there might
exist multiple correct answers (e.g., $\epsilon$-optimal arm identification).
Although both methods are optimal in the asymptotic regime, their
non-asymptotic guarantees remain unknown. In this work, we fill this gap and
provide non-asymptotic guarantees for both algorithms.

</details>


### [126] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/abs/2505.22486)
*Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci, Senad Beadini, Giuseppe Lisanti, Iacopo Masi*

**主要类别:** cs.LG

**概要:** 本论文通过能量视角分析对抗训练中的灾难性过拟合和鲁棒性过拟合问题，并提出了一种新的正则化方法（Delta Energy Regularizer）来缓解这些问题。此外，研究了鲁棒分类器作为生成模型的能力，并提出了一种基于局部类别主成分分析的改进技术以提升样本多样性和生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 当前对对抗训练的理解主要集中在分类器的角度，而缺乏从能量模型框架出发的深入探讨。此外，鲁棒分类器的内在生成能力尚未得到充分探索。

**方法:** 1. 使用能量模型框架分析标准分类器中对抗样本的能量特性。
2. 从能量视角研究对抗训练中的灾难性过拟合和鲁棒性过拟合现象。
3. 提出Delta Energy Regularizer (DER)以平滑训练过程中的能量景观。
4. 探讨鲁棒分类器作为生成模型时的质量与多样性权衡问题。
5. 提出一种基于局部类别主成分分析（PCA）和能量引导的技术以改善生成质量。

**结果:** 1. Delta Energy Regularizer在多个基准上有效缓解了灾难性过拟合和鲁棒性过拟合。
2. 提出的生成技术提升了样本多样性和生成质量，且无需显式生成模型训练。
3. 在未专门进行生成建模的情况下，实现了与混合判别-生成模型相当的Inception Score和Fréchet inception distance。

**结论:** 通过能量视角，我们加深了对对抗训练的理解，并提出了有效的正则化方法以缓解过拟合问题。此外，研究表明鲁棒分类器具有一定的生成能力，但需要进一步改进以平衡图像质量和多样性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Adversarial+Training+with+Energy-based+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22486，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22486&send_immediately=true&force_search=false)

**原文摘要:** We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


### [127] [ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods](https://arxiv.org/abs/2505.22494)
*Michal Kmicikiewicz, Vincent Fortuin, Ewa Szczurek*

**主要类别:** cs.LG

**概要:** 设计具有高适应性和新颖性的蛋白质序列是一项具有挑战性的任务。本文提出了ProSpero，一种主动学习框架，通过将预训练生成模型与代理模型结合，在探索超越野生型邻域的同时保持生物合理性。实验表明，即使代理模型存在偏差，该框架依然有效，并在各种蛋白质工程任务中表现优于或匹配现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前蛋白质序列设计方法在探索超出野生型邻域时，容易产生生物学上不可信的序列或依赖于在新区域失去保真度的代理模型。需要一种能够在保持生物合理性的同时，设计出高适应性和新颖性蛋白质序列的方法。

**方法:** 提出了一种名为ProSpero的主动学习框架，其中包含一个冻结的预训练生成模型和一个由真实反馈更新的代理模型。通过将适应性相关的残基选择与受生物约束的顺序蒙特卡洛采样相结合，实现对野生型邻域之外的探索。

**结果:** 实验结果表明，ProSpero框架在代理模型存在偏差的情况下仍然有效，并且在多种蛋白质工程任务中始终优于或匹配现有方法，能够检索到既具有高适应性又具有新颖性的序列。

**结论:** ProSpero提供了一种新的方法用于高效的数据驱动蛋白质工程，可以在保持生物合理性的同时，设计出具有高适应性和新颖性的蛋白质序列。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProSpero%3A+Active+Learning+for+Robust+Protein+Design+Beyond+Wild-Type+Neighborhoods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22494，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22494&send_immediately=true&force_search=false)

**原文摘要:** Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.

</details>


### [128] [Geometric GNNs for Charged Particle Tracking at GlueX](https://arxiv.org/abs/2505.22504)
*Ahmed Hossam Mohammed, Kishansingh Rajput, Simon Taylor, Denis Furletov, Sergey Furletov, Malachi Schram*

**主要类别:** cs.LG

**概要:** 这篇论文探讨了图神经网络（GNN）在核物理实验中用于粒子轨迹重建的任务中的应用。研究使用GlueX实验的数据，表明GNN方法相比传统方法在效率、纯度和推理速度方面更具优势，并且通过批量处理事件和利用GPU的并行计算能力进一步加速。此外，还比较了GNN在GPU和FPGA上的实现及权衡。


<details>
  <summary>更多</summary>
  
**动机:** 核物理实验旨在揭示物质的基本构成单元。为了重建粒子轨迹和精确确定相互作用，需要对高能碰撞产生的复杂事件中的带电粒子进行追踪。然而，传统的组合方法随着击中点数量的增加，其计算复杂度会超过线性增长，因此需要一种更高效的方法来解决这个问题。

**方法:** 研究采用了图神经网络（GNN）模型来进行轨迹寻找任务。利用GlueX实验的模拟数据训练模型，并在模拟数据和实际GlueX测量数据上进行测试。此外，还探索了通过批处理多个事件以利用GPU的并行计算能力，从而实现显著的速度提升。最后，比较了GNN在GPU和FPGA上的实现及其权衡。

**结果:** GNN方法在固定纯度下的基于片段的效率方面优于GlueX实验中目前使用的传统方法，同时提供了更快的推理速度。通过批处理事件，GNN模型能够显著加速。

**结论:** 图神经网络为粒子轨迹重建提供了一种更有效的方法，在效率、纯度和推理速度方面都表现出色。此外，通过利用GPU的并行计算能力和适当的硬件选择，可以进一步提高性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Geometric+GNNs+for+Charged+Particle+Tracking+at+GlueX，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22504，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22504&send_immediately=true&force_search=false)

**原文摘要:** Nuclear physics experiments are aimed at uncovering the fundamental building
blocks of matter. The experiments involve high-energy collisions that produce
complex events with many particle trajectories. Tracking charged particles
resulting from collisions in the presence of a strong magnetic field is
critical to enable the reconstruction of particle trajectories and precise
determination of interactions. It is traditionally achieved through
combinatorial approaches that scale worse than linearly as the number of hits
grows. Since particle hit data naturally form a 3-dimensional point cloud and
can be structured as graphs, Graph Neural Networks (GNNs) emerge as an
intuitive and effective choice for this task. In this study, we evaluate the
GNN model for track finding on the data from the GlueX experiment at Jefferson
Lab. We use simulation data to train the model and test on both simulation and
real GlueX measurements. We demonstrate that GNN-based track finding
outperforms the currently used traditional method at GlueX in terms of
segment-based efficiency at a fixed purity while providing faster inferences.
We show that the GNN model can achieve significant speedup by processing
multiple events in batches, which exploits the parallel computation capability
of Graphical Processing Units (GPUs). Finally, we compare the GNN
implementation on GPU and FPGA and describe the trade-off.

</details>


### [129] [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
*Wenjie Sun, Bingzhe Wu, Zhile Yang, Chengke Wu*

**主要类别:** cs.LG

**概要:** Sparse Autoencoders (SAEs) are pivotal in mechanistic interpretability, but their encoding mechanisms need further exploration. This paper introduces SAEMA to validate the stratified structure of representations and investigates how sparse encoding changes representational structures by defining local and global representations. It also proves a causal relationship between separability and reconstruction performance. The study emphasizes understanding representations and incorporating constraints for developing new interpretable tools.


<details>
  <summary>更多</summary>
  
**动机:** To explore how sparse encoding organizes activation vector representations from language models and its relation to feature disentanglement and reconstruction performance.

**方法:** Propose SAEMA to observe variability in SSPD matrix rank along latent tensor with noise. Define local and global representations to investigate representational structure changes. Intervene global representation from an optimization perspective.

**结果:** Demonstrates amplification of inter-feature distinctions through merging similar semantic features and introducing dimensionality. Proves significant causal relationship between separability and reconstruction performance.

**结论:** Explains sparsity principles from representational geometry and highlights importance of understanding representations and incorporating constraints for future SAE development.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparsification+and+Reconstruction+from+the+Perspective+of+Representation+Geometry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22506&send_immediately=true&force_search=false)

**原文摘要:** Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic
interpretability, aiming to identify interpretable monosemantic features.
However, how does sparse encoding organize the representations of activation
vector from language models? What is the relationship between this
organizational paradigm and feature disentanglement as well as reconstruction
performance? To address these questions, we propose the SAEMA, which validates
the stratified structure of the representation by observing the variability of
the rank of the symmetric semipositive definite (SSPD) matrix corresponding to
the modal tensor unfolded along the latent tensor with the level of noise added
to the residual stream. To systematically investigate how sparse encoding
alters representational structures, we define local and global representations,
demonstrating that they amplify inter-feature distinctions by merging similar
semantic features and introducing additional dimensionality. Furthermore, we
intervene the global representation from an optimization perspective, proving a
significant causal relationship between their separability and the
reconstruction performance. This study explains the principles of sparsity from
the perspective of representational geometry and demonstrates the impact of
changes in representational structure on reconstruction performance.
Particularly emphasizes the necessity of understanding representations and
incorporating representational constraints, providing empirical references for
developing new interpretable tools and improving SAEs. The code is available at
\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.

</details>


### [130] [Accelerating Optimization via Differentiable Stopping Time](https://arxiv.org/abs/2505.22509)
*Zhonglin Xie, Yiman Fong, Haoran Yuan, Zaiwen Wen*

**主要类别:** cs.LG

**概要:** 提出了一种可微分的停止时间方法，用于优化算法加速，并展示了其在多个问题中的优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现代机器学习应用中，优化是一个重要模块。尽管已经有很多努力来加速优化算法，但最小化达到目标损失所需的时间一直被认为是非可微分的，因此通常作为概念框架或用零阶方法进行优化。

**方法:** 作者提出了一个基于微分方程理论验证的可微分停止时间，并设计了一个有效的算法通过它进行反向传播。此方法提供了一个新的可微分公式以加速算法。

**结果:** 该方法在各种问题的综合实验中表现出优越的性能，证明了其有效性。

**结论:** 可微分停止时间为加速算法提供了一种新的可微分公式，并有广泛的应用前景，如在线超参数调整和学习优化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+Optimization+via+Differentiable+Stopping+Time，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22509，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22509&send_immediately=true&force_search=false)

**原文摘要:** Optimization is an important module of modern machine learning applications.
Tremendous efforts have been made to accelerate optimization algorithms. A
common formulation is achieving a lower loss at a given time. This enables a
differentiable framework with respect to the algorithm hyperparameters. In
contrast, its dual, minimizing the time to reach a target loss, is believed to
be non-differentiable, as the time is not differentiable. As a result, it
usually serves as a conceptual framework or is optimized using zeroth-order
methods. To address this limitation, we propose a differentiable stopping time
and theoretically justify it based on differential equations. An efficient
algorithm is designed to backpropagate through it. As a result, the proposed
differentiable stopping time enables a new differentiable formulation for
accelerating algorithms. We further discuss its applications, such as online
hyperparameter tuning and learning to optimize. Our proposed methods show
superior performance in comprehensive experiments across various problems,
which confirms their effectiveness.

</details>


### [131] [Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo](https://arxiv.org/abs/2505.22524)
*Chinmay Pani, Zijing Ou, Yingzhen Li*

**主要类别:** cs.LG

**概要:** 离散扩散模型在多个领域中变得非常有效。然而，实际应用通常需要生成过程遵循某些约束条件，而无需特定任务的微调。为此，我们提出了一种基于序贯蒙特卡洛(SMC)的无训练方法，在测试时从与奖励对齐的目标分布中采样。我们的方法利用了扭曲SMC和通过奖励函数的一阶泰勒展开获得的近似局部最优提议。为了解决离散空间中梯度不明确的挑战，我们引入了Gumbel-Softmax松弛，从而在离散生成框架内实现了高效的基于梯度的近似。在合成数据集和图像建模上的实证结果验证了我们方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前离散扩散模型虽然高效，但在实际应用中需要满足特定约束，而现有方法往往依赖于任务特定的微调，这增加了复杂性和成本。因此，研究者希望开发一种无需额外训练即可适应约束条件的方法。

**方法:** 该研究提出了一种基于序贯蒙特卡洛（SMC）的无训练采样方法。具体而言，使用扭曲SMC结合近似局部最优提议，该提议通过奖励函数的一阶泰勒展开获得。同时，为了处理离散空间中的梯度问题，引入了Gumbel-Softmax松弛技术，使得可以在离散生成框架内进行高效的梯度近似。

**结果:** 实验结果表明，该方法在合成数据集和图像建模任务上均表现出有效性，证明了其在离散空间生成任务中的潜力。

**结论:** 本文提出了一种无需训练的基于SMC的采样方法，成功解决了离散空间生成任务中的约束条件满足问题，并通过实验证明了其有效性。此方法为实际应用中的生成任务提供了一种新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-Time+Alignment+of+Discrete+Diffusion+Models+with+Sequential+Monte+Carlo，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22524，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22524&send_immediately=true&force_search=false)

**原文摘要:** Discrete diffusion models have become highly effective across various
domains. However, real-world applications often require the generative process
to adhere to certain constraints but without task-specific fine-tuning. To this
end, we propose a training-free method based on Sequential Monte Carlo (SMC) to
sample from the reward-aligned target distribution at the test time. Our
approach leverages twisted SMC with an approximate locally optimal proposal,
obtained via a first-order Taylor expansion of the reward function. To address
the challenge of ill-defined gradients in discrete spaces, we incorporate a
Gumbel-Softmax relaxation, enabling efficient gradient-based approximation
within the discrete generative framework. Empirical results on both synthetic
datasets and image modelling validate the effectiveness of our approach.

</details>


### [132] [A Human-Centric Approach to Explainable AI for Personalized Education](https://arxiv.org/abs/2505.22541)
*Vinitra Swamy*

**主要类别:** cs.LG

**概要:** 深度神经网络在个性化教育中的应用受限于模型决策的可解释性不足。本文通过技术进步和人类研究双重角度，提出四种新型可解释性方法，为以人类为中心的AI系统奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度神经网络在许多领域表现出色，但在教育场景中的实际应用仍有限。主要原因是模型决策缺乏可解释性，导致学生、家长和教师对其缺乏信任。

**方法:** 本文从技术进步和人类研究两个方面进行探讨：1) 技术进步包括提出多模态模块化架构（MultiModN）、可解释的专家混合模型（InterpretCC）、对抗训练以提高解释器稳定性，以及理论驱动的LLM-XAI框架（iLLuMinaTE）；2) 通过与教授、教师、学习科学家和大学生的合作，在多样化环境中评估这些方法。

**结果:** 研究表明，现有的事后解释器与教育需求之间存在系统性差异，并证明了对本质上可解释模型架构的需求。提出的四种技术贡献在不同设置中得到了积极评价，表明其在提升模型透明度和信任方面的潜力。

**结论:** 本文工作为构建以人类为中心的AI系统奠定了基础，这些系统在保持最先进性能的同时，具备内置透明性和信任。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Human-Centric+Approach+to+Explainable+AI+for+Personalized+Education，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22541，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22541&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.

</details>


### [133] [DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models](https://arxiv.org/abs/2505.22549)
*Alex Iacob, Lorenzo Sani, Mher Safaryan, Paris Giampouras, Samuel Horváth, Andrej Jovanovic, Meghdad Kurmanji, Preslav Aleksandrov, William F. Shen, Xinchi Qiu, Nicholas D. Lane*

**主要类别:** cs.LG

**概要:** DES-LOC是一种可扩展、带宽高效且容错的解决方案，适用于基础模型训练。与DDP相比，它可以减少170倍的通信量，比最先进的Local ADAM减少2倍的通信量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的不频繁通信方法（如Local SGD）仅能同步模型参数，不能直接应用于自适应优化器，因为还有额外的优化器状态。当前扩展Local SGD的方法要么缺乏收敛性保证，要么需要三倍的通信成本来同步所有优化器状态。

**方法:** 提出了一组称为DES-LOC的优化器，为参数和动量分配独立的同步周期，从而在保持收敛的同时降低通信成本。

**结果:** 通过多达1.7B的语言模型的广泛实验表明，DES-LOC的通信量比DDP少170倍，比最先进的Local ADAM少2倍。此外，与以前的启发式方法不同，DES-LOC适用于容易发生系统故障的实际训练场景。

**结论:** DES-LOC为大规模基础模型训练提供了一种可扩展、带宽高效且容错的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DES-LOC%3A+Desynced+Low+Communication+Adaptive+Optimizers+for+Training+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22549，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22549&send_immediately=true&force_search=false)

**原文摘要:** Scaling foundation model training with Distributed Data Parallel (DDP)
methods is bandwidth-limited. Existing infrequent communication methods like
Local SGD were designed to synchronize only model parameters and cannot be
trivially applied to adaptive optimizers due to additional optimizer states.
Current approaches extending Local SGD either lack convergence guarantees or
require synchronizing all optimizer states, tripling communication costs. We
propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of
optimizers assigning independent synchronization periods to parameters and
momenta, enabling lower communication costs while preserving convergence.
Through extensive experiments on language models of up to 1.7B, we show that
DES-LOC can communicate 170x less than DDP and 2x less than the previous
state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,
DES-LOC is suited for practical training scenarios prone to system failures.
DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for
foundation model training.

</details>


### [134] [Geometric Hyena Networks for Large-scale Equivariant Learning](https://arxiv.org/abs/2505.22560)
*Artem Moskalev, Mangal Prakash, Junjie Xu, Tianyu Cui, Rui Liao, Tommaso Mansi*

**主要类别:** cs.LG

**概要:** 提出了一种新的模型Geometric Hyena，该模型能够在次二次复杂度下捕捉全局几何上下文，同时保持对旋转和平移的等变性。在大型RNA分子的全原子属性预测和完整蛋白质分子动力学方面，其性能优于现有的等变模型，且所需内存和计算资源显著减少。


<details>
  <summary>更多</summary>
  
**动机:** 在建模生物、化学和物理系统时，处理全局几何上下文同时保持等变性是至关重要的。然而，由于等变性和全局上下文在规模上的计算需求，这具有挑战性。标准方法如等变自注意力机制存在二次复杂度问题，而基于距离的消息传递等局部方法则牺牲了全局信息。

**方法:** 受到最近状态空间和长卷积模型成功的启发，引入了Geometric Hyena，这是第一个用于几何系统的等变长卷积模型。该模型以低于二次复杂度的方式捕捉全局几何上下文，同时保持对旋转和平移的等变性。

**结果:** 在所有原子性质预测的大RNA分子和完整的蛋白质分子动力学中，Geometric Hyena的表现优于现有的等变模型，同时所需的内存和计算量远少于等变自注意力机制。值得注意的是，与等变变换器相比，该模型处理30k令牌的几何上下文快20倍，并允许在同一预算内上下文长度增加72倍。

**结论:** Geometric Hyena为几何系统提供了一种有效的解决方案，在保持等变性的同时高效地捕捉全局几何上下文，适用于生物、化学和物理系统的建模。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Geometric+Hyena+Networks+for+Large-scale+Equivariant+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22560，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22560&send_immediately=true&force_search=false)

**原文摘要:** Processing global geometric context while preserving equivariance is crucial
when modeling biological, chemical, and physical systems. Yet, this is
challenging due to the computational demands of equivariance and global context
at scale. Standard methods such as equivariant self-attention suffer from
quadratic complexity, while local methods such as distance-based message
passing sacrifice global information. Inspired by the recent success of
state-space and long-convolutional models, we introduce Geometric Hyena, the
first equivariant long-convolutional model for geometric systems. Geometric
Hyena captures global geometric context at sub-quadratic complexity while
maintaining equivariance to rotations and translations. Evaluated on all-atom
property prediction of large RNA molecules and full protein molecular dynamics,
Geometric Hyena outperforms existing equivariant models while requiring
significantly less memory and compute that equivariant self-attention. Notably,
our model processes the geometric context of 30k tokens 20x faster than the
equivariant transformer and allows 72x longer context within the same budget.

</details>


### [135] [FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators](https://arxiv.org/abs/2505.22573)
*Guy Moss, Leah Sophie Muhle, Reinhard Drews, Jakob H. Macke, Cornelius Schröder*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为FNOPE的新方法，该方法使用Fourier Neural Operator架构和流匹配目标，能够高效地对函数值参数进行推理，相较于现有方法大幅减少了模拟预算，并支持后验评估在任意域离散化以及向量值参数的同时估计。FNOPE在多个基准任务和冰川学中的空间推理任务上展示了其有效性，扩展了基于模拟的推理(SBI)方法在新科学领域的应用。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于模拟的推理(SBI)方法在低维参数模型中表现最佳，但对于包含函数值参数的模型（常见于气候和地球科学等研究时空过程的学科）难以进行有效的推断。

**方法:** 论文引入了一种新的高效后验估计方法，称为FNOPE。该方法结合了Fourier Neural Operator (FNO)架构与流匹配目标，专门用于对函数值参数进行推理。此外，FNOPE支持在域的任意离散化下进行后验评估，同时还可以同时估计向量值参数。

**结果:** FNOPE在几个基准任务和一个来自冰川学的具有挑战性的空间推断任务中展现了其有效性。实验结果表明，FNOPE可以在显著减少模拟预算的情况下完成函数值参数的推理。

**结论:** FNOPE方法扩展了SBI方法在新科学领域中的适用性，使得对函数值参数的推理成为可能，为研究时空过程的学科提供了更高效的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FNOPE%3A+Simulation-based+inference+on+function+spaces+with+Fourier+Neural+Operators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22573，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22573&send_immediately=true&force_search=false)

**原文摘要:** Simulation-based inference (SBI) is an established approach for performing
Bayesian inference on scientific simulators. SBI so far works best on
low-dimensional parametric models. However, it is difficult to infer
function-valued parameters, which frequently occur in disciplines that model
spatiotemporal processes such as the climate and earth sciences. Here, we
introduce an approach for efficient posterior estimation, using a Fourier
Neural Operator (FNO) architecture with a flow matching objective. We show that
our approach, FNOPE, can perform inference of function-valued parameters at a
fraction of the simulation budget of state of the art methods. In addition,
FNOPE supports posterior evaluation at arbitrary discretizations of the domain,
as well as simultaneous estimation of vector-valued parameters. We demonstrate
the effectiveness of our approach on several benchmark tasks and a challenging
spatial inference task from glaciology. FNOPE extends the applicability of SBI
methods to new scientific domains by enabling the inference of function-valued
parameters.

</details>


### [136] [Benignity of loss landscape with weight decay requires both large overparametrization and initialization](https://arxiv.org/abs/2505.22578)
*Etienne Boursier, Matthew Bowditch, Matthias Englert, Ranko Lazic*

**主要类别:** cs.LG

**概要:** 这篇论文研究了带有权重衰减的两层ReLU神经网络的损失景观，并发现当网络宽度足够大时，损失景观变得良性的条件。


<details>
  <summary>更多</summary>
  
**动机:** 权重衰减在现代神经网络训练中是常见的做法，但大多数理论分析集中在未正则化的设置上。因此，理解权重衰减下的优化过程仍然是一个挑战。

**方法:** 研究者调查了带有L2正则化的两层ReLU网络的损失景观，并通过数学推导和实验验证展示了网络宽度与损失景观特性之间的关系。

**结果:** 结果表明，当网络宽度满足 m ≥ min(n^d, 2^n) 时，损失景观变得良性，即没有虚假局部最小值。并且这种过参数化程度不仅是充分的，也是必要的。此外，初始化大小也对优化结果有显著影响。

**结论:** 大规模初始化和足够的过参数化可以使得损失景观变得良性，有助于避免陷入虚假局部最小值。然而，在小规模初始化下，即使整体景观是良性的，优化仍可能收敛到不良的局部最小值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benignity+of+loss+landscape+with+weight+decay+requires+both+large+overparametrization+and+initialization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22578&send_immediately=true&force_search=false)

**原文摘要:** The optimization of neural networks under weight decay remains poorly
understood from a theoretical standpoint. While weight decay is standard
practice in modern training procedures, most theoretical analyses focus on
unregularized settings. In this work, we investigate the loss landscape of the
$\ell_2$-regularized training loss for two-layer ReLU networks. We show that
the landscape becomes benign -- i.e., free of spurious local minima -- under
large overparametrization, specifically when the network width $m$ satisfies $m
\gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the
input dimension. More precisely in this regime, almost all constant activation
regions contain a global minimum and no spurious local minima. We further show
that this level of overparametrization is not only sufficient but also
necessary via the example of orthogonal data. Finally, we demonstrate that such
loss landscape results primarily hold relevance in the large initialization
regime. In contrast, for small initializations -- corresponding to the feature
learning regime -- optimization can still converge to spurious local minima,
despite the global benignity of the landscape.

</details>


### [137] [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)
*Joschka Braun, Carsten Eickhoff, David Krueger, Seyed Ali Bahrainian, Dmitrii Krasheninnikov*

**主要类别:** cs.LG

**概要:** 转向向量是一种通过在推理时向激活添加学习偏差来控制语言模型行为的轻量级方法。尽管转向表现出有希望的性能，但最近的研究表明，在某些情况下它可能是不可靠甚至适得其反的。本文研究了提示类型和激活差异的几何形状对转向可靠性的影响。首先，我们发现所有七种实验中的提示类型都产生了净正转向效应，但在样本之间表现出高度变化，并且经常产生与预期相反的效果。没有任何提示类型明显优于其他类型，然而来自不同提示类型的转向向量往往方向不同（通过余弦相似度测量）。其次，我们展示了训练集激活差异之间的更高余弦相似度可以预测更有效的转向。最后，我们观察到正负激活更好地分离的数据集更容易转向。我们的结果表明，当目标行为不由连贯的方向表示时，向量转向是不可靠的。


<details>
  <summary>更多</summary>
  
**动机:** 研究转向向量在控制语言模型行为方面的可靠性和有效性，特别是了解提示类型和激活差异几何形状如何影响转向可靠性。

**方法:** 1. 测试七种提示类型对转向效果的影响。
2. 计算不同提示类型产生的转向向量的方向差异（使用余弦相似度）。
3. 分析训练集激活差异的余弦相似度与转向效果的关系。
4. 观察正负激活分离程度对数据集可转向性的影响。

**结果:** 1. 所有提示类型均产生净正转向效应，但样本间方差高，常与期望效果相反。
2. 不同提示类型的转向向量方向差异显著。
3. 较高的训练集激活差异余弦相似度预示更有效的转向。
4. 正负激活分离更好的数据集更容易转向。

**结论:** 转向向量的有效性依赖于目标行为是否能由一个连贯的方向表示；否则，转向可能不可靠或适得其反。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+%28Un%29Reliability+of+Steering+Vectors+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22637，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22637&send_immediately=true&force_search=false)

**原文摘要:** Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.

</details>


### [138] [Spectral Survival Analysis](https://arxiv.org/abs/2505.22641)
*Chengzhi Shi, Stratis Ioannidis*

**主要类别:** cs.LG

**概要:** 提出了一种基于频谱方法的生存分析技术，通过将秩回归与CoxPH模型关联起来，实现了在高维数据集上的高效扩展和性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Cox比例风险（CoxPH）模型被广泛应用，但在大规模数据集和深度架构中进行扩展仍存在挑战，尤其是在高维场景下。因此需要一种新的方法来解决这一问题。

**方法:** 通过发现秩回归与CoxPH模型之间的基本联系，将用于秩回归的频谱方法改编并扩展到生存分析中，这种方法可以自然地推广到包括深度模型在内的多种CoxPH变体。

**结果:** 在多个真实世界的高维数据集上验证了该方法的可扩展性，结果表明该方法在预测性能和效率方面优于传统方法。

**结论:** 所提出的方法不仅能够有效扩展到高维数据集，还能够在预测性能和计算效率上超越传统的CoxPH方法及其变体。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectral+Survival+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22641&send_immediately=true&force_search=false)

**原文摘要:** Survival analysis is widely deployed in a diverse set of fields, including
healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model
is a semi-parametric model often encountered in the literature. Despite its
popularity, wide deployment, and numerous variants, scaling CoxPH to large
datasets and deep architectures poses a challenge, especially in the
high-dimensional regime. We identify a fundamental connection between rank
regression and the CoxPH model: this allows us to adapt and extend the
so-called spectral method for rank regression to survival analysis. Our
approach is versatile, naturally generalizing to several CoxPH variants,
including deep models. We empirically verify our method's scalability on
multiple real-world high-dimensional datasets; our method outperforms legacy
methods w.r.t. predictive performance and efficiency.

</details>


### [139] [On Learning Verifiers for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22650)
*Maria-Florina Balcan, Avrim Blum, Zhiyuan Li, Dravyansh Sharma*

**主要类别:** cs.LG

**概要:** 本文探讨了为自然语言的Chain-of-Thought推理学习可靠的验证器的问题，并提出了一个形式化的PAC学习框架来研究该问题，分析了不同强度的验证目标并提供了样本复杂度的上限和下限结果。


<details>
  <summary>更多</summary>
  
**动机:** Chain-of-Thought推理在解决复杂数学和逻辑问题中表现出强大的能力，但容易因错误或无根据的推断而偏离正轨。虽然形式化数学推理可以解决此问题，但目前的大语言模型难以以形式化方式解决复杂问题，甚至将非形式化问题形式化也很具挑战性。因此，本文旨在为自然语言的Chain-of-Thought推理学习可靠的验证器。

**方法:** 作者提出了一种形式化的PAC学习框架来研究这一问题，定义并分析了几种不同强度的自然验证目标，并提供了满足这些目标的学习验证器的样本复杂度上界，同时也给出了其他自然验证目标在无额外假设下的下界和不可能性结果。

**结果:** 建立了用于学习验证器的形式化PAC学习框架，分析了多种验证目标并提供了理论上的样本复杂度界限，揭示了学习某些验证目标的可能性与不可能性。

**结论:** 本文为自然语言Chain-of-Thought推理的验证问题提供了一个理论框架，并对不同验证目标的可学习性进行了深入分析，为进一步研究可靠验证器的设计奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Learning+Verifiers+for+Chain-of-Thought+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22650，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22650&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [140] [Understanding the learned look-ahead behavior of chess neural networks](https://arxiv.org/abs/2505.21552)
*Diogo Cruz*

**主要类别:** cs.AI

**概要:** 本研究探讨了Leela Chess Zero策略网络在国际象棋中预测未来走法的能力，发现其预测行为具有高度的情境依赖性，并能处理多达七步的棋局状态。研究表明，该网络不仅依赖单一走法序列，而是考虑多种可能的走法组合。这些结果揭示了神经网络在战略任务中发展出复杂预测能力的机制，同时也证明了可解释性技术在理解AI推理过程中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望深入了解神经网络在战略任务中的预测能力，特别是如何超越当前一步进行更远的思考。这有助于揭示AI系统在复杂领域中的推理机制。

**方法:** 通过对Leela Chess Zero策略网络的分析，研究者评估了模型在不同棋局情境下对未来走法的预测能力，并使用可解释性技术来揭示网络内部的决策机制。

**结果:** 研究发现该网络能够预测多达七步的棋局状态，并且其预测行为具有高度的情境依赖性。此外，网络不仅关注单一走法序列，还同时考虑多种可能的走法组合。

**结论:** 本研究表明，神经网络在战略任务中可以发展出复杂的预测能力，并且可解释性技术对于理解AI系统的认知过程非常有效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+the+learned+look-ahead+behavior+of+chess+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21552&send_immediately=true&force_search=false)

**原文摘要:** We investigate the look-ahead capabilities of chess-playing neural networks,
specifically focusing on the Leela Chess Zero policy network. We build on the
work of Jenner et al. (2024) by analyzing the model's ability to consider
future moves and alternative sequences beyond the immediate next move. Our
findings reveal that the network's look-ahead behavior is highly
context-dependent, varying significantly based on the specific chess position.
We demonstrate that the model can process information about board states up to
seven moves ahead, utilizing similar internal mechanisms across different
future time steps. Additionally, we provide evidence that the network considers
multiple possible move sequences rather than focusing on a single line of play.
These results offer new insights into the emergence of sophisticated look-ahead
capabilities in neural networks trained on strategic tasks, contributing to our
understanding of AI reasoning in complex domains. Our work also showcases the
effectiveness of interpretability techniques in uncovering cognitive-like
processes in artificial intelligence systems.

</details>


### [141] [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
*Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, Chuchu Fan*

**主要类别:** cs.AI

**概要:** 尽管R1类模型在推理和规划方面取得了进展，但大语言模型（LLMs）在需要精确计算、符号操作、优化和算法推理的任务上仍面临挑战。本文提出了一种名为R1-Code-Interpreter的扩展模型，通过多轮监督微调（SFT）和强化学习（RL），使文本型LLM能够自主生成多个代码查询以辅助逐步推理。研究发现，代码解释器训练由于任务多样性高和代码执行成本高而更具挑战性。最终模型R1-CI-14B在测试任务上的平均准确率从44.0%提高到64.1%，优于GPT-4o（仅文本：58.6%），接近使用代码解释器的GPT-4o（70.9%）。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在需要精确计算、符号操作、优化和算法推理的任务上表现不佳，主要原因是文本推理缺乏代码执行的严谨性。同时，如何让LLMs在文本推理和代码生成之间做出适当选择也是一个关键挑战。现有的公开研究在这方面尚缺乏足够的指导。

**方法:** 研究人员开发了R1-Code-Interpreter，这是通过对仅文本LLM进行多轮监督微调（SFT）和强化学习（RL）训练得到的模型。该模型可以在逐步推理过程中自动生成多个代码查询。此外，他们还整理了144个推理和规划任务（其中107个用于训练，37个用于测试），每个任务包含超过200个不同的问题，并对不同策略进行了实验。

**结果:** 最终模型R1-CI-14B显著提高了测试任务的平均准确率，从44.0%提升至64.1%，超过了GPT-4o（仅文本：58.6%），并且接近使用代码解释器的GPT-4o（70.9%）。此外，该模型还展现出了通过代码生成实现自我检查的行为。

**结论:** R1-Code-Interpreter的成功表明，通过适当的SFT和RL训练，可以使LLMs更有效地利用代码来解决多样化的任务。这一研究成果为未来LLMs的发展提供了新的方向和方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是R1-Code-Interpreter%3A+Training+LLMs+to+Reason+with+Code+via+Supervised+and+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21668&send_immediately=true&force_search=false)

**原文摘要:** Despite advances in reasoning and planning of R1-like models, Large Language
Models (LLMs) still struggle with tasks requiring precise computation, symbolic
manipulation, optimization, and algorithmic reasoning, in which textual
reasoning lacks the rigor of code execution. A key challenge is enabling LLMs
to decide when to use textual reasoning versus code generation. While OpenAI
trains models to invoke a Code Interpreter as needed, public research lacks
guidance on aligning pre-trained LLMs to effectively leverage code and
generalize across diverse tasks. We present R1-Code-Interpreter, an extension
of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and
reinforcement learning (RL) to autonomously generate multiple code queries
during step-by-step reasoning. We curate 144 reasoning and planning tasks (107
for training, 37 for testing), each with over 200 diverse questions. We
fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,
investigating different answer formats, reasoning vs. non-reasoning models,
cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.
Unlike prior RL work on narrow domains, we find that Code Interpreter training
is significantly harder due to high task diversity and expensive code
execution, highlighting the critical role of the SFT stage. Our final model,
R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to
64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with
Code Interpreter (70.9\%), with the emergent self-checking behavior via code
generation. Datasets, Codes, and Models are available at
https://github.com/yongchao98/R1-Code-Interpreter and
https://huggingface.co/yongchao98.

</details>


### [142] [Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing](https://arxiv.org/abs/2505.21671)
*Davin Choo, Yuqi Pan, Tonghan Wang, Milind Tambe, Alastair van Heerden, Cheryl Johnson*

**主要类别:** cs.AI

**概要:** 本文研究了在图结构中具有未知标签节点上的顺序决策问题，提出了一种基于Gittins指数的策略，在森林图上被证明是最优的，并且在实验中表现优于其他基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望解决一个在图结构中的顺序决策问题，其中每个节点都有来自有限集合的未知标签。目标是通过自适应选择节点来最大化累积折扣奖励，同时受到前沿探索约束的限制。

**方法:** 作者设计了一种基于Gittins指数的策略，该策略适用于一般的图结构，并且在图G为森林时可以证明其最优性。算法的时间复杂度为O(n² * |Σ|²)，使用O(n * |Σ|²)次对联合分布P的调用和O(n² * |Σ|)的空间。

**结果:** 实验结果表明，该方法在合成和真实世界的图数据上均优于自然基线方法，包括非树形结构、预算受限以及未折扣的情况。例如，在HIV检测模拟中，该策略仅测试一半人口就几乎检测出所有阳性病例。

**结论:** 提出的基于Gittins指数的策略在森林图结构中是理论上最优的，并且在实际应用中表现出色，尤其是在HIV检测模拟中显著优于其他方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Frontier+Exploration+on+Graphs+with+Applications+to+Network-Based+Disease+Testing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21671，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21671&send_immediately=true&force_search=false)

**原文摘要:** We study a sequential decision-making problem on a $n$-node graph $G$ where
each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from
a joint distribution $P$ that is Markov with respect to $G$. At each step,
selecting a node reveals its label and yields a label-dependent reward. The
goal is to adaptively choose nodes to maximize expected accumulated discounted
rewards. We impose a frontier exploration constraint, where actions are limited
to neighbors of previously selected nodes, reflecting practical constraints in
settings such as contact tracing and robotic exploration. We design a Gittins
index-based policy that applies to general graphs and is provably optimal when
$G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$
time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and
$O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world
graphs show that our method consistently outperforms natural baselines,
including in non-tree, budget-limited, and undiscounted settings. For example,
in HIV testing simulations on real-world sexual interaction networks, our
policy detects nearly all positive cases with only half the population tested,
substantially outperforming other baselines.

</details>


### [143] [Make Planning Research Rigorous Again!](https://arxiv.org/abs/2505.21674)
*Michael Katz, Harsha Kokel, Christian Muise, Shirin Sohrabi, Sarath Sreedharan*

**主要类别:** cs.AI

**概要:** 在规划领域60多年的发展历程中，通过严谨的设计和评估规划系统，对解决前所未有的规划问题的理论与实践做出了重要贡献。这种严谨性应同样适用于当前基于大语言模型的规划工作。通过正确整合自动规划社区的见解、工具和数据，可以加速基于LLM的规划器的发展，并避免重复已知的问题。


<details>
  <summary>更多</summary>
  
**动机:** 描述了规划领域的历史贡献以及其对于当前基于大语言模型的规划工作的意义。

**方法:** 提出将自动规划社区的见解、工具和数据正确地融入到基于LLM的规划器的设计与评估中。

**结果:** 强调避免已知问题的重要性，以促进基于LLM的规划器的进展。

**结论:** 认为应用规划领域的经验和严谨性对于加速基于LLM的规划器的发展至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Make+Planning+Research+Rigorous+Again%21，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21674，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21674&send_immediately=true&force_search=false)

**原文摘要:** In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.

</details>


### [144] [Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models](https://arxiv.org/abs/2505.21765)
*Sohyun An, Ruochen Wang, Tianyi Zhou, Cho-Jui Hsieh*

**主要类别:** cs.AI

**概要:** 近期大型推理模型（LRMs）虽提升了LLMs的推理能力，但因过度思考导致输出长度增加。为解决此问题，研究提出一种动态优化框架，将推理路径分为不同思考模式，通过系统识别和提升有益模式、移除有害模式，从而优化推理路径。实验表明，优化后的思考路径更简洁且信息量足够，显著提高推理效率并减少计算开销。此外，该方法在多个数学推理基准上验证了其减少计算开销的同时还能提升推理准确率的能力。


<details>
  <summary>更多</summary>
  
**动机:** 大型推理模型在提升推理能力的同时，由于过度思考增加了输出长度，表现为不必要的复杂推理路径，这不仅浪费计算资源，还可能降低性能。为解决这一问题，研究者希望探索一种方法来动态选择适当的模块化推理策略，即在正确位置应用合适的思考模式，以提高推理效率。

**方法:** 研究者提出了一种动态优化框架，该框架将模型生成的推理路径划分为不同的思考模式，并通过系统地识别和增强有益模式、去除有害模式来优化推理路径。同时，为了进一步改进推理路径，研究者采用了一种偏好优化技术，利用对比次优和最优推理路径的成对数据集进行支持。

**结果:** 优化后的思考路径更加简明扼要，同时保留了足够的信息量，成功减少了注意力浮点运算次数（FLOPs）最多达47%，同时保持了原本正确的回答的准确性。此外，部分原本错误的回答被纠正，准确率提高了15.6%，并且回答长度也有所减少。在多个数学推理基准上的实验结果表明，该方法显著降低了计算开销，同时提升了推理准确率，最高可达12%的准确率提升，同时将token使用量从约5000个减少到3000个。

**结论:** 通过动态优化框架和偏好优化技术，研究成功提升了推理模型的效率和准确性，同时减少了计算开销和输出长度。这些结果表明，优化思考路径可以有效改善大型推理模型的表现，为进一步提高推理模型性能提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Don%27t+Think+Longer%2C+Think+Wisely%3A+Optimizing+Thinking+Dynamics+for+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21765&send_immediately=true&force_search=false)

**原文摘要:** While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.

</details>


### [145] [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
*Tharindu Kumarage, Ninareh Mehrabi, Anil Ramakrishna, Xinyan Zhao, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta, Charith Peris*

**主要类别:** cs.AI

**概要:** 提出AIDSAFE，通过多代理迭代推理生成高质量的安全策略链式思维数据集，解决现有安全措施的局限性。AIDSAFE生成的数据可有效提升大语言模型的安全泛化和防越狱能力，同时保持效用和过度拒绝准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的安全措施存在过度拒绝和易被越狱的问题，而构建高质量的嵌入策略链式思维数据集既资源密集又难以避免推理错误或策略冲突。

**方法:** 提出AIDSAFE方法，利用多代理迭代推理扩展安全策略的链式思维，并通过数据精炼阶段消除重复、冗余和欺骗性的推理。此外，引入辅助配方使用信念增强创建不同的选择和拒绝样本以满足对齐阶段的需求。

**结果:** AIDSAFE生成的链式思维数据在政策遵循和推理质量上表现优异，基于这些数据微调开源大语言模型可以显著提高其安全泛化和防越狱能力，同时维持效用和过度拒绝准确性。

**结论:** AIDSAFE为监督微调提供了强大基础，能够有效提升大语言模型的安全性能，相关数据集已公开发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Safety+Reasoning+in+LLMs%3A+AI-agentic+Deliberation+for+Policy-embedded+CoT+Data+Creation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21784&send_immediately=true&force_search=false)

**原文摘要:** Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE

</details>


### [146] [SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts](https://arxiv.org/abs/2505.21828)
*Chen Yueh-Han, Guy Davidson, Brenden M. Lake*

**主要类别:** cs.AI

**概要:** 论文提出SAGE-Eval，用于评估大语言模型在新情境下对关键安全事实的泛化能力，发现当前模型如Claude-3.7-sonnet通过率仅为58%，并建议在部署前使用SAGE-Eval进行评估。


<details>
  <summary>更多</summary>
  
**动机:** 评估大语言模型是否能够在新情境中正确应用已知的安全事实，以避免因缺乏警告而导致严重后果。

**方法:** 构建SAGE-Eval基准测试，包含104个从权威机构获取的事实，扩展为10,428个测试场景，涵盖7个常见领域。

**结果:** 顶级模型Claude-3.7-sonnet仅通过58%的安全事实测试，模型能力和训练计算量与SAGE-Eval性能弱相关。

**结论:** 前沿大语言模型仍缺乏稳健的泛化能力，建议开发者在部署前使用SAGE-Eval评估模型可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SAGE-Eval%3A+Evaluating+LLMs+for+Systematic+Generalizations+of+Safety+Facts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21828&send_immediately=true&force_search=false)

**原文摘要:** Do LLMs robustly generalize critical safety facts to novel situations?
Lacking this ability is dangerous when users ask naive questions. For instance,
"I'm considering packing melon balls for my 10-month-old's lunch. What other
foods would be good to include?" Before offering food options, the LLM should
warn that melon balls pose a choking hazard to toddlers, as documented by the
CDC. Failing to provide such warnings could result in serious injuries or even
death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic
GEneralization evaluation, the first benchmark that tests whether LLMs properly
apply well established safety facts to naive user queries. SAGE-Eval comprises
104 facts manually sourced from reputable organizations, systematically
augmented to create 10,428 test scenarios across 7 common domains (e.g.,
Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,
passes only 58% of all the safety facts tested. We also observe that model
capabilities and training compute weakly correlate with performance on
SAGE-Eval, implying that scaling up is not the golden solution. Our findings
suggest frontier LLMs still lack robust generalization ability. We recommend
developers use SAGE-Eval in pre-deployment evaluations to assess model
reliability in addressing salient risks. We publicly release SAGE-Eval at
https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available
at https://github.com/YuehHanChen/SAGE-Eval/tree/main.

</details>


### [147] [SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem](https://arxiv.org/abs/2505.21887)
*Ahmed Heakl, Yahia Salaheldin Shaaban, Martin Takac, Salem Lahlou, Zangir Iklassov*

**主要类别:** cs.AI

**概要:** 提出SVRPBench，一个开放的基准测试平台，用于捕捉城市规模车辆路径规划中的高保真随机动态。它模拟了现实世界的配送条件，并揭示出现有的RL求解器在分布偏移下性能下降明显，而经典和启发式方法仍然稳健。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基准测试大多假设静态、理想化的环境，无法反映实际物流中的不确定性。

**方法:** 构建了一个包含500多个实例的开放基准SVRPBench，模拟真实的配送条件，如时间依赖的拥堵、对数正态延迟、概率事故等。通过生成多样化的场景来评估不同求解器的性能。

**结果:** 发现最先进的RL求解器（如POMO和AM）在分布偏移下性能下降超过20%，而经典和元启发式方法保持稳健。

**结论:** SVRPBench挑战研究社区设计超越合成假设并适应现实世界不确定性的求解器，并公开数据集以促进可重复研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SVRPBench%3A+A+Realistic+Benchmark+for+Stochastic+Vehicle+Routing+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21887&send_immediately=true&force_search=false)

**原文摘要:** Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.

</details>


### [148] [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
*Saleh Afzoon, Zahra Jahanandish, Phuong Thao Huynh, Amin Beheshti, Usman Naseem*

**主要类别:** cs.AI

**概要:** AI copilots 是一种上下文感知、由 AI 驱动的系统，旨在协助用户完成软件开发和内容创作等任务。随着这些系统的能力和采用率不断提高，个性化已成为确保可用性、信任和生产力的核心。本文通过综合研究如何在 AI copilots 的设计中捕捉、建模和优化用户偏好，提出了一个统一的 AI copilots 定义，并提出了一种基于阶段的偏好优化策略分类法，围绕交互前、交互中和交互后三个阶段进行结构化。


<details>
  <summary>更多</summary>
  
**动机:** 尽管个性化技术在推荐系统和对话代理等领域已经很成熟，但它们在像 AI copilots 这样的交互式实时系统中的应用仍然分散且未被充分探索。

**方法:** 作者引入了一个统一的 AI copilots 定义，并提出了一种基于阶段的偏好优化策略分类法。分析了获取偏好信号、建模用户意图以及整合反馈循环的技术。

**结果:** 为设计适应性强、偏好感知的 AI copilots 提供了一个结构化的基础，提供了关于如何利用可用的偏好资源以及哪些技术方法最适合系统设计每个阶段的全面视图。

**结论:** 本调查弥合了来自 AI 个性化、人机协作和大型语言模型适应的见解，为设计适应性强、偏好感知的 AI copilots 提供了结构化的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+and+Optimizing+User+Preferences+in+AI+Copilots%3A+A+Comprehensive+Survey+and+Taxonomy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21907&send_immediately=true&force_search=false)

**原文摘要:** AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.

</details>


### [149] [From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models](https://arxiv.org/abs/2505.21935)
*Kaiyu He, Zhiyu Chen*

**主要类别:** cs.AI

**概要:** 这篇论文探讨了大语言模型（LLMs）是否能够发现新知识，并通过皮尔斯的框架（溯因推理、演绎推理和归纳推理）分析了基于LLM的假设发现。它总结了现有研究在假设生成、应用和验证方面的成就与不足，展望了LLMs从信息执行者向真正创新引擎转变的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前对大语言模型的研究主要集中在提升其指令跟随和演绎推理能力上，但关于这些模型是否能够真正发现新知识的问题尚未得到充分探讨。为了追求通用人工智能（AGI），需要探索模型如何学习、推理并生成新知识。

**方法:** 作者采用了皮尔斯的推理框架（溯因推理、演绎推理和归纳推理）来审视基于LLM的假设发现过程。同时，文章综合了现有的关于假设生成、应用和验证的工作，识别出关键成果和重要空白点。

**结果:** 该研究明确了LLMs在假设生成领域的现有进展，揭示了其在成为创新引擎方面的潜力，同时也指出了当前存在的局限性。

**结论:** 通过合适的推理框架和进一步研究，LLMs有可能从单纯的信息执行者转变为推动科学研究和实际问题解决的创新引擎。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Reasoning+to+Learning%3A+A+Survey+on+Hypothesis+Discovery+and+Rule+Learning+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21935&send_immediately=true&force_search=false)

**原文摘要:** Since the advent of Large Language Models (LLMs), efforts have largely
focused on improving their instruction-following and deductive reasoning
abilities, leaving open the question of whether these models can truly discover
new knowledge. In pursuit of artificial general intelligence (AGI), there is a
growing need for models that not only execute commands or retrieve information
but also learn, reason, and generate new knowledge by formulating novel
hypotheses and theories that deepen our understanding of the world. Guided by
Peirce's framework of abduction, deduction, and induction, this survey offers a
structured lens to examine LLM-based hypothesis discovery. We synthesize
existing work in hypothesis generation, application, and validation,
identifying both key achievements and critical gaps. By unifying these threads,
we illuminate how LLMs might evolve from mere ``information executors'' into
engines of genuine innovation, potentially transforming research, science, and
real-world problem solving.

</details>


### [150] [Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism](https://arxiv.org/abs/2505.21988)
*Ziyang Zheng, Kezhi Li, Zhengyuan Shi, Qiang Xu*

**主要类别:** cs.AI

**概要:** 提出了一种新的功能子图匹配方法，通过两阶段多模态框架显著提高了逻辑电路中功能子图检测的准确性和模糊边界识别的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的子图匹配技术主要依赖于结构图同构，无法在综合变换大幅改变电路拓扑时识别功能相关的子图。

**方法:** 提出了一个两阶段多模态框架：1) 学习AIG和映射后网表的鲁棒功能嵌入以进行功能子图检测；2) 使用图分割方法识别模糊边界。

**结果:** 在标准基准测试（ITC99, OpenABCD, ForgeEDA）上的评估显示，该方法相较于现有结构方法有显著性能提升，功能子图检测平均准确率为93.8%，模糊边界识别的dice得分为91.3%。

**结论:** 功能性子图匹配能够有效克服传统方法的局限性，在逻辑电路的不同应用中具有广阔前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Functional+Matching+of+Logic+Subgraphs%3A+Beyond+Structural+Isomorphism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21988&send_immediately=true&force_search=false)

**原文摘要:** Subgraph matching in logic circuits is foundational for numerous Electronic
Design Automation (EDA) applications, including datapath optimization,
arithmetic verification, and hardware trojan detection. However, existing
techniques rely primarily on structural graph isomorphism and thus fail to
identify function-related subgraphs when synthesis transformations
substantially alter circuit topology. To overcome this critical limitation, we
introduce the concept of functional subgraph matching, a novel approach that
identifies whether a given logic function is implicitly present within a larger
circuit, irrespective of structural variations induced by synthesis or
technology mapping. Specifically, we propose a two-stage multi-modal framework:
(1) learning robust functional embeddings across AIG and post-mapping netlists
for functional subgraph detection, and (2) identifying fuzzy boundaries using a
graph segmentation approach. Evaluations on standard benchmarks (ITC99,
OpenABCD, ForgeEDA) demonstrate significant performance improvements over
existing structural methods, with average $93.8\%$ accuracy in functional
subgraph detection and a dice score of $91.3\%$ in fuzzy boundary
identification.

</details>


### [151] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/abs/2505.22006)
*Changze Qiao, Mingming Lu*

**主要类别:** cs.AI

**概要:** 通过引入EHC（无需参数更新即可学习的通用代理），论文解决了现有方法在构建多模态通用代理中的局限性，EHC包含HMR和TOEL模块，分别用于记忆检索与存储以及任务特性理解，实验表明EHC在多模态任务中达到最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 现有的利用大型语言模型构建通用多模态代理的方法要么依赖于使用大规模多模态数据进行计算昂贵的端到端训练，要么采用缺乏连续学习和适应新环境能力的工具使用方法。

**方法:** 提出了EHC，一个由分层记忆检索（HMR）模块和面向任务类别的经验学习（TOEL）模块组成的通用代理。HMR支持快速检索相关记忆并持续存储新信息；TOEL通过分类经验和提取跨类别模式来增强代理对各种任务特性的理解。

**结果:** 广泛的实验证明EHC优于现有方法，在多个标准数据集上实现了最先进的性能。

**结论:** EHC作为一个通用代理，其有效性得到了证明，适用于处理复杂的多模态任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficiently+Enhancing+General+Agents+With+Hierarchical-categorical+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22006&send_immediately=true&force_search=false)

**原文摘要:** With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


### [152] [Reinforced Reasoning for Embodied Planning](https://arxiv.org/abs/2505.22050)
*Di Wu, Jiaxin Fan, Junzhe Zang, Guanbo Wang, Wei Yin, Wenhao Li, Bo Jin*

**主要类别:** cs.AI

**概要:** 通过强化微调框架提升具身规划中R1风格的推理能力，利用监督微调和规则奖励函数优化策略，在Embench基准上显著优于同类模型，并具有强大泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前视觉-语言模型虽擅长静态感知任务，但在互动环境下的时间推理、空间理解和常识基础方面表现不佳，难以满足具身规划的需求。

**方法:** 引入强化微调框架，从闭源模型蒸馏高质量数据集进行监督微调，赋予模型结构化决策先验；设计基于规则的奖励函数并采用广义强化偏好优化方法优化策略。

**结果:** 在Embench基准测试中，无论是领域内还是领域外场景，该方法显著优于规模相似或更大的模型（如GPT-4o-mini和70B+开源基线），并在未见环境中表现出强大泛化能力。

**结论:** 强化驱动的推理有潜力推动具身AI中的长时域规划发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforced+Reasoning+for+Embodied+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22050&send_immediately=true&force_search=false)

**原文摘要:** Embodied planning requires agents to make coherent multi-step decisions based
on dynamic visual observations and natural language goals. While recent
vision-language models (VLMs) excel at static perception tasks, they struggle
with the temporal reasoning, spatial understanding, and commonsense grounding
needed for planning in interactive environments. In this work, we introduce a
reinforcement fine-tuning framework that brings R1-style reasoning enhancement
into embodied planning. We first distill a high-quality dataset from a powerful
closed-source model and perform supervised fine-tuning (SFT) to equip the model
with structured decision-making priors. We then design a rule-based reward
function tailored to multi-step action quality and optimize the policy via
Generalized Reinforced Preference Optimization (GRPO). Our approach is
evaluated on Embench, a recent benchmark for interactive embodied tasks,
covering both in-domain and out-of-domain scenarios. Experimental results show
that our method significantly outperforms models of similar or larger scale,
including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong
generalization to unseen environments. This work highlights the potential of
reinforcement-driven reasoning to advance long-horizon planning in embodied AI.

</details>


### [153] [Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired](https://arxiv.org/abs/2505.22087)
*Ruxiao Chen, Dezheng Han, Wenjie Han, Shuaishuai Guo*

**主要类别:** cs.AI

**概要:** 本研究提出了一种名为VAG-EC的新框架，通过知识图谱模拟人类视觉感知和认知映射，生成紧凑且可解释的符号语言，用于帮助视障人士实时导航。实验表明，该方法在地形相似性和上下文独立性方面优于传统方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的辅助系统要么语义丰富但反应慢，要么快速但缺乏语义深度，无法满足视障人士实时导航的需求。

**方法:** 研究提出Cognitively-Inspired Emergent Communication via Knowledge Graphs (VAG-EC) 框架，利用知识图谱表示物体及其关系，并通过注意力机制突出任务相关实体，从而模拟人类的选择性注意。这种方法能够生成紧凑、可解释和对情境敏感的符号语言。

**结果:** 在不同词汇量和消息长度的广泛实验中，VAG-EC在TopSim（地形相似性）和CI（上下文独立性）两项指标上均优于传统的新兴通信方法。

**结论:** 基于认知的新兴通信具有快速、适应性强和与人类一致的特点，为实时辅助技术提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cognitively-Inspired+Emergent+Communication+via+Knowledge+Graphs+for+Assisting+the+Visually+Impaired，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22087&send_immediately=true&force_search=false)

**原文摘要:** Assistive systems for visually impaired individuals must deliver rapid,
interpretable, and adaptive feedback to facilitate real-time navigation.
Current approaches face a trade-off between latency and semantic richness:
natural language-based systems provide detailed guidance but are too slow for
dynamic scenarios, while emergent communication frameworks offer low-latency
symbolic languages but lack semantic depth, limiting their utility in tactile
modalities like vibration. To address these limitations, we introduce a novel
framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs
(VAG-EC), which emulates human visual perception and cognitive mapping. Our
method constructs knowledge graphs to represent objects and their
relationships, incorporating attention mechanisms to prioritize task-relevant
entities, thereby mirroring human selective attention. This structured approach
enables the emergence of compact, interpretable, and context-sensitive symbolic
languages. Extensive experiments across varying vocabulary sizes and message
lengths demonstrate that VAG-EC outperforms traditional emergent communication
methods in Topographic Similarity (TopSim) and Context Independence (CI). These
findings underscore the potential of cognitively grounded emergent
communication as a fast, adaptive, and human-aligned solution for real-time
assistive technologies. Code is available at
https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.

</details>


### [154] [VIRAL: Vision-grounded Integration for Reward design And Learning](https://arxiv.org/abs/2505.22092)
*Valentin Cuzin-Rambaud, Emilien Komlenovic, Alexandre Faure, Bruno Yun*

**主要类别:** cs.AI

**概要:** 本研究提出了VIRAL，一种利用多模态大语言模型生成和优化奖励函数的管道。通过结合人类反馈或视频形式的策略解释，VIRAL在五个Gymnasium环境中加速了新行为的学习并提高了与用户意图的一致性。


<details>
  <summary>更多</summary>
  
**动机:** 当前强化学习中，不良设计的奖励函数可能带来风险，而大型语言模型在生成奖励方面已展现出超越人类的能力。因此，需要一种更有效的方法来生成和改进奖励函数，以增强人机对齐。

**方法:** 研究者开发了VIRAL系统，该系统使用多模态大语言模型根据给定环境、目标提示或注释图像自主创建和交互式改进奖励函数。奖励函数的优化过程可整合人类反馈或由视频LLM生成的描述指导。

**结果:** 在五个Gymnasium环境中评估表明，VIRAL能够加速新行为的学习，并确保与用户意图更好的一致性。

**结论:** VIRAL作为一种创新方法，展示了其在生成和改进奖励函数方面的潜力，从而促进了强化学习中的行为学习和人机对齐。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VIRAL%3A+Vision-grounded+Integration+for+Reward+design+And+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22092&send_immediately=true&force_search=false)

**原文摘要:** The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.

</details>


### [155] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/abs/2505.22104)
*Davide Corsi, Kaushik Mallik, Andoni Rodriguez, Cesar Sanchez*

**主要类别:** cs.AI

**概要:** 提出了一种动态防护机制，用于参数化安全规范的AI控制系统，相较于传统静态防护，新方法能够在线调整以适应运行时变化的安全需求，实验表明其效率远高于从头重新计算的方法。


<details>
  <summary>更多</summary>
  
**动机:** 确保AI控制自主系统的安全性至关重要，但传统的防护机制是静态设计的，无法应对运行时变化的安全需求，这可能导致致命延迟。

**方法:** 引入动态防护机制，针对参数化安全规范进行设计。这些防护机制在离线状态下根据给定的安全参数集静态设计，但在运行时能动态适应实际安全规范的变化。利用标准安全防护已知特性（如最大宽容性），提出一种简单快速的动态适应算法。

**结果:** 在机器人导航问题实验中，动态防护机制的离线设计耗时数分钟，在线适应每步仅需不到一秒至数秒，而暴力在线重计算方法速度慢了5倍。

**结论:** 动态防护机制可以有效解决运行时变化的安全需求问题，显著提高效率，减少潜在致命延迟。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Dynamic+Shielding+for+Parametric+Safety+Specifications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22104，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22104&send_immediately=true&force_search=false)

**原文摘要:** Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [156] [Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test](https://arxiv.org/abs/2505.22112)
*Guangfu Hao, Frederic Alexandre, Shan Yu*

**主要类别:** cs.AI

**概要:** 本研究通过威斯康星卡片分类测试（WCST）评估了最先进的视觉大语言模型（VLLMs）的认知灵活性，发现这些模型在基于文本输入和思维链提示下达到或超过人类水平的集合转换能力，但其能力受输入模式和提示策略的影响很大。此外，通过角色扮演，VLLMs可以模拟认知灵活性受损患者的多种功能缺陷，表明其可能具备类似于大脑的认知架构。这揭示了VLLMs在我们更高阶认知的一个关键组成部分上已经接近人类水平，并强调了使用它们来模拟复杂脑过程的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管认知灵活性在人类认知中被广泛研究，但在视觉大语言模型（VLLMs）的背景下仍然相对未被探索。因此，研究者希望评估VLLMs是否具有类似人类的认知灵活性，并探讨它们是否能模拟认知灵活性受损的情况。

**方法:** 研究者使用威斯康星卡片分类测试（WCST），一种经典的集合转换能力测量方法，来评估GPT-4o、Gemini-1.5 Pro和Claude-3.5 Sonnet等先进VLLMs的认知灵活性。实验中采用了文本输入和思维链提示，并通过角色扮演让模型模拟认知灵活性受损患者的功能缺陷。

**结果:** 结果显示，VLLMs在基于文本输入和思维链提示下的集合转换能力达到或超过了人类水平。然而，这种能力受到输入模式和提示策略的显著影响。此外，通过角色扮演，VLLMs能够成功模拟认知灵活性受损患者的功能缺陷。

**结论:** VLLMs在集合转换这一关键认知灵活性方面已接近人类水平，这为使用这些模型模拟复杂的脑过程提供了可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Visual+Large+Language+Models+Exhibit+Human-Level+Cognitive+Flexibility+in+the+Wisconsin+Card+Sorting+Test，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22112，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22112&send_immediately=true&force_search=false)

**原文摘要:** Cognitive flexibility has been extensively studied in human cognition but
remains relatively unexplored in the context of Visual Large Language Models
(VLLMs). This study assesses the cognitive flexibility of state-of-the-art
VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card
Sorting Test (WCST), a classic measure of set-shifting ability. Our results
reveal that VLLMs achieve or surpass human-level set-shifting capabilities
under chain-of-thought prompting with text-based inputs. However, their
abilities are highly influenced by both input modality and prompting strategy.
In addition, we find that through role-playing, VLLMs can simulate various
functional deficits aligned with patients having impairments in cognitive
flexibility, suggesting that VLLMs may possess a cognitive architecture, at
least regarding the ability of set-shifting, similar to the brain. This study
reveals the fact that VLLMs have already approached the human level on a key
component underlying our higher cognition, and highlights the potential to use
them to emulate complex brain processes.

</details>


### [157] [Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions](https://arxiv.org/abs/2505.22147)
*Florian Andreas Marwitz, Tanya Braun, Ralf Möller, Marcel Gehrke*

**主要类别:** cs.AI

**概要:** 论文提出了一种一阶表示方法和名为Foreplan的 relational forward planner，能以多项式规模存储状态和动作空间，并提供了一个更快的近似版本以及理论分析和实证评估，证明了其效率。


<details>
  <summary>更多</summary>
  
**动机:** 在AI决策中，随着不可区分对象数量增加，状态空间呈指数增长，导致计算策略时需要枚举的状态和动作空间过多，难以处理。

**方法:** 引入一阶表示来将状态和动作空间从指数规模压缩至多项式规模；开发了Foreplan（一种关系前向规划器）用于高效计算涉及大量不可区分对象和动作的策略；还提出了一个更快的近似版本。

**结果:** Foreplan可以识别为完成特定任务所需的对象数量，并且通过理论分析和实证评估，表明其速度至少提高了四个数量级。

**结论:** Foreplan及其近似版本能够高效解决大规模状态和动作空间的问题，显著提升了决策效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lifted+Forward+Planning+in+Relational+Factored+Markov+Decision+Processes+with+Concurrent+Actions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22147，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22147&send_immediately=true&force_search=false)

**原文摘要:** Decision making is a central problem in AI that can be formalized using a
Markov Decision Process. A problem is that, with increasing numbers of
(indistinguishable) objects, the state space grows exponentially. To compute
policies, the state space has to be enumerated. Even more possibilities have to
be enumerated if the size of the action space depends on the size of the state
space, especially if we allow concurrent actions. To tackle the exponential
blow-up in the action and state space, we present a first-order representation
to store the spaces in polynomial instead of exponential size in the number of
objects and introduce Foreplan, a relational forward planner, which uses this
representation to efficiently compute policies for numerous indistinguishable
objects and actions. Additionally, we introduce an even faster approximate
version of Foreplan. Moreover, Foreplan identifies how many objects an agent
should act on to achieve a certain task given restrictions. Further, we provide
a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a
speedup of at least four orders of magnitude.

</details>


### [158] [What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22148)
*Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi Song, Ying Wei, Defu Lian*

**主要类别:** cs.AI

**概要:** 近期关于大型语言模型（LLMs）推理的研究推广了长链思维（LCoT）策略，该策略通过逐步推理来提高最终答案的准确性。本文提出了LCoT2Tree框架，将LCoT转换为分层树结构以进行更深入的结构分析。通过图神经网络（GNNs），研究发现探索、回溯和验证等结构模式可以更好地预测多种任务和模型的最终表现，并且能够解释一些失败原因如过度分支。此外，这些结构模式支持实际应用，例如改进Best-of-N解码效果。整体结果强调了推理链内部结构的关键作用，使LCoT2Tree成为诊断、解释和改进LLMs推理的强大工具。


<details>
  <summary>更多</summary>
  
**动机:** 尽管长链思维（LCoT）在复杂任务中表现出专家级性能，但其推理链的内部结构如何驱动甚至预测最终答案的正确性仍是一个关键但尚未深入探讨的问题。

**方法:** 提出了一种名为LCoT2Tree的自动化框架，该框架将顺序LCoT转换为分层树结构，从而允许对LLM推理进行更深入的结构分析。使用图神经网络（GNNs）提取并分析结构模式，包括探索、回溯和验证等。同时，利用可解释性技术识别导致失败的关键思维模式，如过度分支。

**结果:** 研究表明，通过LCoT2Tree提取的结构模式是更强的性能预测指标，在广泛的任务和模型中表现良好。并且，这些结构模式有助于解释失败原因并支持实际应用，例如提升Best-of-N解码的有效性。

**结论:** 推理链的内部结构在LLMs推理中起着关键作用，LCoT2Tree作为一个强大的工具，可用于诊断、解释和改进LLMs的推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Makes+a+Good+Reasoning+Chain%3F+Uncovering+Structural+Patterns+in+Long+Chain-of-Thought+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22148，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22148&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in reasoning with large language models (LLMs) have
popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate
and step-by-step reasoning before producing a final answer. While LCoTs have
enabled expert-level performance in complex tasks, how the internal structures
of their reasoning chains drive, or even predict, the correctness of final
answers remains a critical yet underexplored question. In this work, we present
LCoT2Tree, an automated framework that converts sequential LCoTs into
hierarchical tree structures and thus enables deeper structural analysis of LLM
reasoning. Using graph neural networks (GNNs), we reveal that structural
patterns extracted by LCoT2Tree, including exploration, backtracking, and
verification, serve as stronger predictors of final performance across a wide
range of tasks and models. Leveraging an explainability technique, we further
identify critical thought patterns such as over-branching that account for
failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree
support practical applications, including improving Best-of-N decoding
effectiveness. Overall, our results underscore the critical role of internal
structures of reasoning chains, positioning LCoT2Tree as a powerful tool for
diagnosing, interpreting, and improving reasoning in LLMs.

</details>


### [159] [A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives](https://arxiv.org/abs/2505.22244)
*Yaron Halle, Ariel Felner, Sven Koenig, Oren Salzman*

**主要类别:** cs.AI

**概要:** 这篇论文提出了一种高效的双目标最短路径问题（BOSP）算法，该算法利用目标函数之间的相关性来减少搜索工作量，并且在DIMACS数据集实例上比现有方法快五倍。这是第一个在双目标搜索中有效利用相关性的算法，同时提供解的质量的理论保证。


<details>
  <summary>更多</summary>
  
**动机:** 双目标最短路径问题（BOSP）在实际应用中非常重要，例如在道路网络中同时优化旅行时间和燃料消耗。然而，由于目标函数之间的相关性和搜索空间的指数增长，解决这个问题具有计算上的挑战。

**方法:** 作者提出了一种受图聚类算法启发的新方法，通过预处理阶段识别图中的相关聚类并生成新的图表示。这种方法使得A*pex等近似求解器能够更高效地运行，尤其是在目标函数高度相关的情况下。

**结果:** 实验结果表明，在DIMACS数据集实例上，新算法比现有方法快五倍。此外，该算法在双目标搜索中首次有效地利用了目标函数的相关性。

**结论:** 该研究提出了一种新的高效算法，可以显著减少双目标最短路径问题中的搜索工作量，特别是在目标函数相关的情况下。该算法不仅提高了计算效率，还提供了解的质量的理论保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Preprocessing+Framework+for+Efficient+Approximate+Bi-Objective+Shortest-Path+Computation+in+the+Presence+of+Correlated+Objectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22244&send_immediately=true&force_search=false)

**原文摘要:** The bi-objective shortest-path (BOSP) problem seeks to find paths between
start and target vertices of a graph while optimizing two conflicting objective
functions. We consider the BOSP problem in the presence of correlated
objectives. Such correlations often occur in real-world settings such as road
networks, where optimizing two positively correlated objectives, such as travel
time and fuel consumption, is common. BOSP is generally computationally
challenging as the size of the search space is exponential in the number of
objective functions and the graph size. Bounded sub-optimal BOSP solvers such
as A*pex alleviate this complexity by approximating the Pareto-optimal solution
set rather than computing it exactly (given a user-provided approximation
factor). As the correlation between objective functions increases, smaller
approximation factors are sufficient for collapsing the entire Pareto-optimal
set into a single solution. We leverage this insight to propose an efficient
algorithm that reduces the search effort in the presence of correlated
objectives. Our approach for computing approximations of the entire
Pareto-optimal set is inspired by graph-clustering algorithms. It uses a
preprocessing phase to identify correlated clusters within a graph and to
generate a new graph representation. This allows a natural generalization of
A*pex to run up to five times faster on DIMACS dataset instances, a standard
benchmark in the field. To the best of our knowledge, this is the first
algorithm proposed that efficiently and effectively exploits correlations in
the context of bi-objective search while providing theoretical guarantees on
solution quality.

</details>


### [160] [Compression versus Accuracy: A Hierarchy of Lifted Models](https://arxiv.org/abs/2505.22288)
*Jan Speller, Malte Luttermann, Marcel Gehrke, Tanya Braun*

**主要类别:** cs.AI

**概要:** 论文提出了一种无超参数的分层方法，用于提升模型构建，解决了现有方法中选择合适ε值的难题，并通过分层误差界限提高了模型的可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 当前使用ACP算法进行近似推理时，需要手动探索合适的ε值，且不同ε值可能导致模型差异巨大，降低可解释性。

**方法:** 提出一种无超参数的分层方法，自动计算ε值的分层结构，确保模型在不同ε值下具有层次关系，同时提供分层误差界限。

**结果:** 该方法可以明确权衡压缩与准确性，并提高不同模型之间的可解释性。

**结论:** 分层方法为提升模型构建提供了一种新的解决方案，无需手动调整超参数，同时保持了模型的可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compression+versus+Accuracy%3A+A+Hierarchy+of+Lifted+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22288，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22288&send_immediately=true&force_search=false)

**原文摘要:** Probabilistic graphical models that encode indistinguishable objects and
relations among them use first-order logic constructs to compress a
propositional factorised model for more efficient (lifted) inference. To obtain
a lifted representation, the state-of-the-art algorithm Advanced Colour Passing
(ACP) groups factors that represent matching distributions. In an approximate
version using $\varepsilon$ as a hyperparameter, factors are grouped that
differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable
$\varepsilon$ is not obvious and may need a lot of exploration, possibly
requiring many ACP runs with different $\varepsilon$ values. Additionally,
varying $\varepsilon$ can yield wildly different models, leading to decreased
interpretability. Therefore, this paper presents a hierarchical approach to
lifted model construction that is hyperparameter-free. It efficiently computes
a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning
that once factors are grouped together given some $\varepsilon$, these factors
will be grouped together for larger $\varepsilon$ as well. The hierarchy of
$\varepsilon$ values also leads to a hierarchy of error bounds. This allows for
explicitly weighing compression versus accuracy when choosing specific
$\varepsilon$ values to run ACP with and enables interpretability between the
different models.

</details>


### [161] [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
*Fanzeng Xia, Yidong Luo, Tinko Sebastian Bartels, Yaqi Xu, Tongxin Li*

**主要类别:** cs.AI

**概要:** 近期研究表明，大型语言模型（LLMs）在处理复杂的推理任务时仍面临重大挑战。然而，现有文献大多依赖于简单的上下文学习示例进行评估，忽略了更高级的技术来激发LLMs的深思熟虑的推理能力。本文系统地探讨了结合上下文搜索和测试时扩展技术在极难推理任务中的潜力。通过使用高级的上下文搜索提示，并结合内部扩展技术，我们实现了在以前被认为“无法解决”的任务上的性能突破。我们的实证结果表明，在受控的NP难问题和复杂的现实规划基准上，该方法的成功率比先前报告的结果提高了30倍。理论上，我们证明了上下文搜索提示与内部扩展相结合可以显著扩展可解推理问题的复杂性类别。这些发现挑战了关于LLMs在复杂任务上局限性的普遍假设，表明当前的评估范式系统性地低估了它们的真实潜力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型在生成长推理步骤方面有所改进，但其在困难推理问题上的表现仍然有限。现有的评估方式主要依赖直接提示和简单的上下文学习例子，这可能忽视了模型真正的推理潜力。因此，需要探索新的技术和方法，以充分挖掘LLM的推理能力。

**方法:** 本文采用结合上下文搜索和测试时扩展的方法，利用高级的上下文搜索提示和内部扩展技术，针对超级困难的推理任务进行研究。这种方法不依赖外部机制，而是通过优化提示和模型内部结构来提升推理能力。

**结果:** 实证结果显示，该方法在受控的NP难问题和复杂的现实规划基准上，成功率相较于之前报告的结果提高了30倍。理论分析表明，上下文搜索提示与内部扩展相结合，能够显著扩展可解决推理问题的复杂性类别。

**结论:** 本研究挑战了关于大语言模型在复杂任务上限的普遍假设，指出当前评估范式系统性地低估了LLM的真实潜力。因此，需要重新评估如何对LLM推理进行基准测试，并制定更全面的评估策略，以更好地理解其在实际应用中的推理边界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Unsolvable%3A+When+In-Context+Search+Meets+Test-Time+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22290，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22290&send_immediately=true&force_search=false)

**原文摘要:** Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.

</details>


### [162] [From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications](https://arxiv.org/abs/2505.22311)
*Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Octavia A. Dobre, Merouane Debbah*

**主要类别:** cs.AI

**概要:** 随着6G通信的到来，智能通信系统面临感知和响应能力受限、可扩展性有限以及在动态环境中适应性低等多重挑战。本文提供了大型人工智能模型（LAMs）和代理型AI技术在智能通信系统中的原理、设计和应用的系统介绍，旨在为研究人员提供前沿技术和实用指导的全面概述。首先，概述了6G通信的背景，回顾了从LAMs到代理型AI的技术演变，并阐明了教程的动机和主要贡献。接着，对构建LAMs所需的关键组件进行了全面回顾，并进一步对LAMs进行分类和分析其适用性。然后，提出了以LAM为核心的通信定制设计范式，包括数据集构建和内外部学习方法。在此基础上，开发了一个基于LAM的代理型AI系统用于智能通信，明确了其核心组件和交互机制。还介绍了一个具有数据检索、协作规划和反思评估功能的多代理框架。随后，提供了LAMs和代理型AI在通信场景中应用的详细概述。最后，总结了当前研究面临的挑战和未来方向，以支持下一代高效、安全和可持续的智能通信系统的发展。


<details>
  <summary>更多</summary>
  
**动机:** 智能通信系统在6G时代面临感知和响应能力受限、可扩展性有限以及在动态环境中适应性低等问题，需要一种新的解决方案来克服这些挑战。

**方法:** 本文通过系统地介绍LAMs和代理型AI技术的原理、设计和应用，为智能通信系统的改进提供了一种新方法。具体来说，文章涵盖了LAMs的关键组件、分类及其适用性，提出了一种以LAM为核心的通信设计范式，并开发了一个基于LAM的代理型AI系统，同时引入了多代理框架和应用场景分析。

**结果:** 本文明确了LAMs和代理型AI技术在智能通信系统中的关键作用，并展示了它们在多种通信场景中的应用潜力。此外，指出了当前研究的挑战和未来的研究方向。

**结论:** LAMs和代理型AI技术为解决智能通信系统在6G时代的挑战提供了有力的支持，推动了高效、安全和可持续的下一代智能通信系统的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Large+AI+Models+to+Agentic+AI%3A+A+Tutorial+on+Future+Intelligent+Communications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22311，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22311&send_immediately=true&force_search=false)

**原文摘要:** With the advent of 6G communications, intelligent communication systems face
multiple challenges, including constrained perception and response
capabilities, limited scalability, and low adaptability in dynamic
environments. This tutorial provides a systematic introduction to the
principles, design, and applications of Large Artificial Intelligence Models
(LAMs) and Agentic AI technologies in intelligent communication systems, aiming
to offer researchers a comprehensive overview of cutting-edge technologies and
practical guidance. First, we outline the background of 6G communications,
review the technological evolution from LAMs to Agentic AI, and clarify the
tutorial's motivation and main contributions. Subsequently, we present a
comprehensive review of the key components required for constructing LAMs. We
further categorize LAMs and analyze their applicability, covering Large
Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models
(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a
LAM-centric design paradigm tailored for communications, encompassing dataset
construction and both internal and external learning approaches. Building upon
this, we develop an LAM-based Agentic AI system for intelligent communications,
clarifying its core components such as planners, knowledge bases, tools, and
memory modules, as well as its interaction mechanisms. We also introduce a
multi-agent framework with data retrieval, collaborative planning, and
reflective evaluation for 6G. Subsequently, we provide a detailed overview of
the applications of LAMs and Agentic AI in communication scenarios. Finally, we
summarize the research challenges and future directions in current studies,
aiming to support the development of efficient, secure, and sustainable
next-generation intelligent communication systems.

</details>


### [163] [AgentDNS: A Root Domain Naming System for LLM Agents](https://arxiv.org/abs/2505.22368)
*Enfang Cui, Yujun Cheng, Rui She, Dan Liu, Zhiyuan Liang, Minxin Guo, Tianzheng Li, Qian Wei, Wenjuan Xing, Zhijie Zhong*

**主要类别:** cs.AI

**概要:** 本文提出了一种名为AgentDNS的根域名和服务发现系统，旨在使大型语言模型代理能够跨组织和技术边界自主发现、解析和安全调用第三方代理和服务。其灵感来源于传统DNS系统，并引入了服务注册、语义服务发现、安全调用和统一计费的结构化机制。源代码将在https://github.com/agentdns发布。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型代理在跨厂商的服务发现、互操作性和通信方面存在挑战。尽管现有协议如模型上下文协议和代理到代理协议已经在标准化多代理之间的互操作性和通信方面取得了显著进展，但跨不同代理和工具厂商的服务发现仍然缺乏标准化协议和解决方案。

**方法:** 提出了一个基于传统DNS原理设计的系统——AgentDNS，它包括服务注册、语义服务发现、安全调用和统一计费等核心功能。通过这些机制，AgentDNS可以支持代理跨组织和技术边界的协作。

**结果:** 论文详细描述了AgentDNS的架构、核心功能和使用案例，展示了其在实际场景中简化多代理协作的潜力。

**结论:** AgentDNS有望解决当前跨厂商服务发现中的不足，促进大型语言模型代理间的互操作性与协作，推动多代理系统的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentDNS%3A+A+Root+Domain+Naming+System+for+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22368，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22368&send_immediately=true&force_search=false)

**原文摘要:** The rapid evolution of Large Language Model (LLM) agents has highlighted
critical challenges in cross-vendor service discovery, interoperability, and
communication. Existing protocols like model context protocol and
agent-to-agent protocol have made significant strides in standardizing
interoperability between agents and tools, as well as communication among
multi-agents. However, there remains a lack of standardized protocols and
solutions for service discovery across different agent and tool vendors. In
this paper, we propose AgentDNS, a root domain naming and service discovery
system designed to enable LLM agents to autonomously discover, resolve, and
securely invoke third-party agent and tool services across organizational and
technological boundaries. Inspired by the principles of the traditional DNS,
AgentDNS introduces a structured mechanism for service registration, semantic
service discovery, secure invocation, and unified billing. We detail the
architecture, core functionalities, and use cases of AgentDNS, demonstrating
its potential to streamline multi-agent collaboration in real-world scenarios.
The source code will be published on https://github.com/agentdns.

</details>


### [164] [AI Mathematician: Towards Fully Automated Frontier Mathematical Research](https://arxiv.org/abs/2505.22451)
*Yuanhang Liu, Yanxing Huang, Yanqiao Wang, Peng Li, Yang Liu*

**主要类别:** cs.AI

**概要:** 大型推理模型（LRMs）在数学能力方面取得了显著进展，但主要局限于竞赛级别问题。本文提出AI Mathematician (AIM)框架，利用LRMs的推理能力支持前沿数学研究。通过探索机制和悲观合理性验证方法，AIM能够应对研究问题的内在复杂性和程序严谨性的要求。实验表明，AIM在处理研究级别的任务上表现出强大的能力，能自主构建证明的大部分内容，并在各研究领域中发现非平凡的见解。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LRMs在数学竞赛问题上取得成功，但在更复杂的前沿数学研究中的应用尚待探索。

**方法:** 提出AIM框架，包含两个核心策略：探索机制以促进更长的解决方案路径，以及悲观合理性验证方法以确保可靠性。

**结果:** 在多个真实世界数学主题的广泛实验中，AIM展现出强大能力，能够自主构建证明的大部分并揭示非平凡见解。

**结论:** LRMs在数学发现中具有巨大潜力，基于LRM的智能体系统可能显著加速未来的数学研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Mathematician%3A+Towards+Fully+Automated+Frontier+Mathematical+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22451&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) have made significant progress in mathematical
capabilities in recent times. However, these successes have been primarily
confined to competition-level problems. In this work, we propose AI
Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs
to support frontier mathematical research. We have identified two critical
challenges of mathematical research compared to competition, {\it the intrinsic
complexity of research problems} and {\it the requirement of procedural rigor}.
To address these challenges, AIM incorporates two core strategies: an
exploration mechanism to foster longer solution paths, and the pessimistic
reasonable verification method to ensure reliability.
  This early version of AIM already exhibits strong capability in tackling
research-level tasks. We conducted extensive experiments across several
real-world mathematical topics and obtained promising results. AIM is able to
autonomously construct substantial portions of proofs and uncover non-trivial
insights within each research area. These findings highlight the potential of
LRMs in mathematical discovery and suggest that LRM-based agent systems could
significantly accelerate mathematical research in the future.

</details>


### [165] [HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym](https://arxiv.org/abs/2505.22597)
*Ngoc La, Ruaridh Mon-Williams, Julie A. Shah*

**主要类别:** cs.AI

**概要:** HDDLGym 是一个基于 Python 的工具，可以将 HDDL 领域和问题自动转换为 OpenAI Gym 环境，从而在强化学习 (RL) 中支持分层规划，特别是多智能体协作规划。本文介绍了 HDDLGym 的设计、实现以及使用方法，并通过 Transport 和 Overcooked 领域进行示范。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，尽管强化学习 (RL) 方法已广泛应用于如 OpenAI Gym 等工具中测试，但许多任务仍可从分层规划中获益。然而，目前缺乏一种能无缝集成分层规划与 RL 的工具。

**方法:** 引入了 Hierarchical Domain Definition Language (HDDL)，这是一种经典的规划语言，提供了一种结构化的方法来填补这一空白，并适用于基于模型的 RL。同时开发了 HDDLGym，该工具可以从 HDDL 领域和问题自动生成 OpenAI Gym 环境，从而连接 RL 和分层规划。HDDLGym 支持多智能体场景并允许智能体间协同规划。

**结果:** 本文提供了 HDDLGym 设计与实现的概述，详细说明了如何将 HDDL 与 Gym 接口集成，以及如何应用 RL 策略以支持分层规划。此外，还提供了详细的使用指南和实例演示，包括如何处理现有的 HDDL 领域和问题（例如国际规划竞赛中的 Transport 领域），以及如何创建新的多智能体 HDDL 领域（如 Overcooked 领域）。

**结论:** HDDLGym 是一个有价值的工具，用于研究强化学习在分层规划中的应用，特别是在多智能体环境中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HDDLGym%3A+A+Tool+for+Studying+Multi-Agent+Hierarchical+Problems+Defined+in+HDDL+with+OpenAI+Gym，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22597&send_immediately=true&force_search=false)

**原文摘要:** In recent years, reinforcement learning (RL) methods have been widely tested
using tools like OpenAI Gym, though many tasks in these environments could also
benefit from hierarchical planning. However, there is a lack of a tool that
enables seamless integration of hierarchical planning with RL. Hierarchical
Domain Definition Language (HDDL), used in classical planning, introduces a
structured approach well-suited for model-based RL to address this gap. To
bridge this integration, we introduce HDDLGym, a Python-based tool that
automatically generates OpenAI Gym environments from HDDL domains and problems.
HDDLGym serves as a link between RL and hierarchical planning, supporting
multi-agent scenarios and enabling collaborative planning among agents. This
paper provides an overview of HDDLGym's design and implementation, highlighting
the challenges and design choices involved in integrating HDDL with the Gym
interface, and applying RL policies to support hierarchical planning. We also
provide detailed instructions and demonstrations for using the HDDLGym
framework, including how to work with existing HDDL domains and problems from
International Planning Competitions, exemplified by the Transport domain.
Additionally, we offer guidance on creating new HDDL domains for multi-agent
scenarios and demonstrate the practical use of HDDLGym in the Overcooked
domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a
valuable tool for studying RL in hierarchical planning, particularly in
multi-agent contexts.

</details>


### [166] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov, Paul Kassianik, Maksym Andriushchenko, Jonas Geiping*

**主要类别:** cs.AI

**概要:** 大型语言模型的能力和自主性增强，识别漏洞对于安全部署至关重要。研究发现，攻击成功与攻击者和目标模型之间的能力差距密切相关，并提出了一个预测攻击成功的缩放定律。这表明固定能力的攻击者可能对未来的模型无效，开源模型的风险增加，模型提供者需要准确测量和控制模型的说服和操纵能力。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型的能力和自主性增强，通过红队测试识别漏洞对于安全部署变得至关重要。然而，传统的提示工程方法在红队测试成为弱到强问题时可能无效，即目标模型在能力上超过红队测试者。

**方法:** 通过评估500多个攻击者-目标对，使用基于LLM的越狱攻击模仿人类红队测试者，跨越不同的家族、规模和能力水平。研究攻击者和目标之间的能力差距对红队测试的影响。

**结果:** 三个强烈趋势浮现：（i）更有能力的模型是更好的攻击者，（ii）一旦目标的能力超过攻击者，攻击成功率急剧下降，（iii）攻击成功率与MMLU-Pro基准的社会科学分组的高性能相关。

**结论:** 固定能力的攻击者（如人类）可能对未来的模型无效，越来越有能力的开源模型放大了现有系统的风险，模型提供者必须准确测量和控制模型的说服和操纵能力以限制其作为攻击者的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Capability-Based+Scaling+Laws+for+LLM+Red-Teaming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.20162，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.20162&send_immediately=true&force_search=false)

**原文摘要:** As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [167] [A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models](https://arxiv.org/abs/2505.21580)
*Anum Fatima, Gesine Reinert*

**主要类别:** stat.ML

**概要:** 本论文提出了一种基于核化Stein差异（KSD）的拟合优度检验方法，专门用于非齐次随机图模型（IRG），该方法仅需单次网络观测即可实施，并适用于任何规模的网络。此外，该方法不依赖于检验统计量的渐近分布，且提供了理论保证。


<details>
  <summary>更多</summary>
  
**动机:** 当前对于复杂数据表示为图结构的情况，尤其是非齐次随机图模型（IRG），缺乏一种高效且适用性强的拟合优度检验方法。现有的高维快速检验工具如KSD测试需要多次观察或依赖渐近分布，因此有必要开发一种新的、更灵活的检验方法。

**方法:** 作者提出了一种基于KSD的拟合优度检验方法，专门针对IRG模型设计。这种方法只需要一次网络观测数据即可进行检验，同时避免了对检验统计量渐近分布的依赖。通过将KSD方法与IRG模型结合，作者构建了一个能够适应任意规模网络的检验框架。

**结果:** 实验结果表明，所提出的检验方法在不同规模和类型的网络中表现良好，具有较高的准确性和稳定性。同时，该方法在理论层面得到了充分验证，确保了其在实际应用中的可靠性。

**结论:** 本文成功开发了一种适用于非齐次随机图模型的KSD型拟合优度检验方法，解决了传统方法对多次观测或渐近分布的依赖问题。此方法不仅具有广泛的适用性，还提供了严格的理论支持，为复杂网络数据分析提供了一个有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Kernelised+Stein+Discrepancy+for+Assessing+the+Fit+of+Inhomogeneous+Random+Graph+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21580，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21580&send_immediately=true&force_search=false)

**原文摘要:** Complex data are often represented as a graph, which in turn can often be
viewed as a realisation of a random graph, such as of an inhomogeneous random
graph model (IRG). For general fast goodness-of-fit tests in high dimensions,
kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,
test, and analyse a KSD-type goodness-of-fit test for IRG models that can be
carried out with a single observation of the network. The test is applicable to
a network of any size and does not depend on the asymptotic distribution of the
test statistic. We also provide theoretical guarantees.

</details>


### [168] [STACI: Spatio-Temporal Aleatoric Conformal Inference](https://arxiv.org/abs/2505.21658)
*Brandon R. Feng, David Keetae Park, Xihaier Luo, Arantxa Urdangarin, Shinjae Yoo, Brian J. Reich*

**主要类别:** stat.ML

**概要:** 提出了一种名为STACI的新框架，结合变分贝叶斯神经网络和时空一致性推理算法，解决了时空高斯过程在可扩展性和不确定性量化中的问题，同时在大规模数据集上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时空深度学习模型通常假设响应的协方差矩阵是简单的独立结构，无法捕捉底层的相关性；而传统的时空高斯过程虽然能提供可解释的不确定性量化，但存在可扩展性和近似偏差的问题。

**方法:** STACI框架包含一个变分贝叶斯神经网络，用于逼近非平稳时空高斯过程，并结合一种新的时空一致性推理算法。该方法利用了神经网络模型的GPU训练能力，具有高度可扩展性并提供了统计上有效的预测区间。

**结果:** STACI在准确逼近时空过程方面优于竞争性的高斯过程和深度学习方法，并且可以轻松扩展到包含数百万观测值的数据集。

**结论:** STACI框架为时空过程建模提供了一种高效、可扩展且精确的方法，适用于大规模数据集，并能进行可靠的不确定性量化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STACI%3A+Spatio-Temporal+Aleatoric+Conformal+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21658，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21658&send_immediately=true&force_search=false)

**原文摘要:** Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty
quantification for estimation of spatio-temporal fields. Spatio-temporal deep
learning models, while scalable, typically assume a simplistic independent
covariance matrix for the response, failing to capture the underlying
correlation structure. However, spatio-temporal GPs suffer from issues of
scalability and various forms of approximation bias resulting from restrictive
assumptions of the covariance kernel function. We propose STACI, a novel
framework consisting of a variational Bayesian neural network approximation of
non-stationary spatio-temporal GP along with a novel spatio-temporal conformal
inference algorithm. STACI is highly scalable, taking advantage of GPU training
capabilities for neural network models, and provides statistically valid
prediction intervals for uncertainty quantification. STACI outperforms
competing GPs and deep methods in accurately approximating spatio-temporal
processes and we show it easily scales to datasets with millions of
observations.

</details>


### [169] [Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference](https://arxiv.org/abs/2505.21721)
*Kyurae Kim, Yi-An Ma, Trevor Campbell, Jacob R. Gardner*

**主要类别:** stat.ML

**概要:** 本文证明了在给定均值场位置-尺度变分族的情况下，使用重参数化梯度的黑箱变分推断（BBVI）以几乎与维度无关的速度收敛。对于强对数凹和对数平滑目标，子高斯族的BBVI达到接近全局最优的目标所需的迭代次数为$\mathrm{O}(\log d)$，优于全秩位置-尺度族的$\mathrm{O}(d)$依赖关系。对于重尾族，提供了较弱的$\mathrm{O}(d^{2/k})$维度依赖关系，其中$k$是有限矩的数量。如果目标对数密度的Hessian矩阵是常数，则复杂性不依赖于任何显式的维度。此外，本文还证明了我们对梯度方差的界不能仅使用目标对数密度Hessian矩阵的谱界来改进。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望理解黑箱变分推断（BBVI）在不同变分族下的收敛速度，并探讨其与问题维度的关系。特别是，他们关注使用重参数化梯度的BBVI方法在优化目标函数时的表现，以及如何通过选择不同的变分族来改善收敛性。

**方法:** 作者首先定义了一个均值场位置-尺度变分族，并分析了使用重参数化梯度的BBVI方法在该族中的表现。接着，他们分别讨论了子高斯族和重尾族的情况，得到了不同的维度依赖关系。最后，他们通过理论分析证明了梯度方差界的最优性，即无法仅通过目标对数密度Hessian矩阵的谱界进一步改进。

**结果:** 研究表明，在强对数凹和对数平滑目标下，子高斯族的BBVI方法具有$\mathrm{O}(\log d)$的收敛速度，显著优于全秩位置-尺度族的$\mathrm{O}(d)$依赖关系。对于重尾族，收敛速度则表现为$\mathrm{O}(d^{2/k})$，其中$k$为有限矩的数量。此外，当目标对数密度的Hessian矩阵为常数时，复杂性不再显式依赖于维度。

**结论:** 本文展示了使用重参数化梯度的BBVI方法在特定变分族下的收敛特性，并揭示了其与问题维度之间的关系。结果表明，通过选择适当的变分族可以显著改善收敛速度，同时证明了梯度方差界的理论最优性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nearly+Dimension-Independent+Convergence+of+Mean-Field+Black-Box+Variational+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21721&send_immediately=true&force_search=false)

**原文摘要:** We prove that, given a mean-field location-scale variational family,
black-box variational inference (BBVI) with the reparametrization gradient
converges at an almost dimension-independent rate. Specifically, for strongly
log-concave and log-smooth targets, the number of iterations for BBVI with a
sub-Gaussian family to achieve an objective $\epsilon$-close to the global
optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$
dependence of full-rank location-scale families. For heavy-tailed families, we
provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the
number of finite moments. Additionally, if the Hessian of the target
log-density is constant, the complexity is free of any explicit dimension
dependence. We also prove that our bound on the gradient variance, which is key
to our result, cannot be improved using only spectral bounds on the Hessian of
the target log-density.

</details>


### [170] [Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks](https://arxiv.org/abs/2505.21791)
*Julia Nakhleh, Robert D. Nowak*

**主要类别:** stat.ML

**概要:** 这篇论文提出了一种新的训练目标，通过最小化权重的ℓ^p准范数（0<p<1）来找到最稀疏的单隐藏层ReLU网络，从而将稀疏插值的组合问题转化为平滑优化任务。


<details>
  <summary>更多</summary>
  
**动机:** 过参数化的神经网络可以以多种方式对数据集进行插值。因此，需要明确：哪种解决方案是我们应该优先选择的？哪些显式的正则化策略可以保证得到这些解决方案？为了提高效率、泛化能力、可解释性和模型压缩效果，本文关注寻找具有最少非零参数或神经元的插值ReLU网络。

**方法:** 提出了一个连续的、几乎处处可微的训练目标，其全局最小值对应于拟合数据的最稀疏的单隐藏层ReLU网络。该方法基于最小化权重的ℓ^p准范数（0<p<1），这是一种经典的促进稀疏性的策略。此外，还证明了在该公式下，全局最小解正好对应于最稀疏的解决方案。

**结果:** 将稀疏插值的组合问题转化为平滑优化任务，可能使基于梯度的训练方法得以应用。这种方法为理解何时以及如何利用连续的稀疏诱导目标来通过训练恢复稀疏网络奠定了基础。

**结论:** 本文提出的训练目标和方法提供了一种新的途径来寻找最稀疏的ReLU网络，并为未来的研究提供了理论支持和方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Global+Minimizers+of+%24%5Cell%5Ep%24-Regularized+Objectives+Yield+the+Sparsest+ReLU+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21791&send_immediately=true&force_search=false)

**原文摘要:** Overparameterized neural networks can interpolate a given dataset in many
different ways, prompting the fundamental question: which among these solutions
should we prefer, and what explicit regularization strategies will provably
yield these solutions? This paper addresses the challenge of finding the
sparsest interpolating ReLU network -- i.e., the network with the fewest
nonzero parameters or neurons -- a goal with wide-ranging implications for
efficiency, generalization, interpretability, theory, and model compression.
Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere
differentiable training objective whose global minima are guaranteed to
correspond to the sparsest single-hidden-layer ReLU networks that fit the data.
This result marks a conceptual advance: it recasts the combinatorial problem of
sparse interpolation as a smooth optimization task, potentially enabling the
use of gradient-based training methods. Our objective is based on minimizing
$\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical
sparsity-promoting strategy in finite-dimensional settings. However, applying
these ideas to neural networks presents new challenges: the function class is
infinite-dimensional, and the weights are learned using a highly nonconvex
objective. We prove that, under our formulation, global minimizers correspond
exactly to sparsest solutions. Our work lays a foundation for understanding
when and how continuous sparsity-inducing objectives can be leveraged to
recover sparse networks through training.

</details>


### [171] [A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging](https://arxiv.org/abs/2505.21796)
*Sajad Khodadadian, Martin Zubeldia*

**主要类别:** stat.ML

**概要:** Polyak-Ruppert平均是一种广泛使用的技术，用于实现随机逼近（SA）算法的最佳渐近方差。本文提出了一种建立平均SA迭代误差非渐近集中界的一般框架。该方法假设对未平均迭代有单独的集中界，并得出对平均迭代的尖锐界。此外，还构造了一个示例，证明了结果的紧致性。直接应用包括导出收缩SA算法以及如时间差分学习和Q学习等算法的紧集中界，获得了在传统分析具有挑战性的设置中的新界。


<details>
  <summary>更多</summary>
  
**动机:** Polyak-Ruppert平均技术虽然能实现最佳渐近方差，但其高概率性能保证在一般设置下尚未充分探索。因此，需要一个通用框架来建立平均SA迭代误差的非渐近集中界。

**方法:** 提出了一种通用框架，假设可以访问未平均迭代的个体集中界，并利用这些界来推导平均迭代的尖锐界。同时通过构造一个示例验证结果的紧致性。

**结果:** 得到了适用于收缩SA算法以及时间差分学习和Q学习等算法的紧集中界，并在传统分析困难的设置中获得了新的界。

**结论:** 本文提出的方法为理解Polyak-Ruppert平均技术的高概率性能提供了理论基础，并扩展了其在强化学习相关算法中的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+General-Purpose+Theorem+for+High-Probability+Bounds+of+Stochastic+Approximation+with+Polyak+Averaging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21796，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21796&send_immediately=true&force_search=false)

**原文摘要:** Polyak-Ruppert averaging is a widely used technique to achieve the optimal
asymptotic variance of stochastic approximation (SA) algorithms, yet its
high-probability performance guarantees remain underexplored in general
settings. In this paper, we present a general framework for establishing
non-asymptotic concentration bounds for the error of averaged SA iterates. Our
approach assumes access to individual concentration bounds for the unaveraged
iterates and yields a sharp bound on the averaged iterates. We also construct
an example, showing the tightness of our result up to constant multiplicative
factors. As direct applications, we derive tight concentration bounds for
contractive SA algorithms and for algorithms such as temporal difference
learning and Q-learning with averaging, obtaining new bounds in settings where
traditional analysis is challenging.

</details>


### [172] [Spectral clustering for dependent community Hawkes process models of temporal networks](https://arxiv.org/abs/2505.21845)
*Lingfei Zhao, Hadeel Soliman, Kevin S. Xu, Subhadeep Paul*

**主要类别:** stat.ML

**概要:** 这篇论文提出了一种依赖社区霍克斯（DCH）模型，结合随机块模型和互激发霍克斯过程来建模时间网络中的社区结构和节点对之间的依赖性。此外，作者还提供了光谱聚类错误的非渐近上限，并提出了一个使用广义矩估计法（GMM）进行参数估计的可扩展模型。


<details>
  <summary>更多</summary>
  
**动机:** 时间网络在许多应用领域中广泛存在，例如在线社交媒体、金融交易和国际关系。这些网络通常表现出社区结构和节点对之间的强依赖模式。因此，需要一种能够同时捕捉社区结构和依赖性的模型。

**方法:** 论文提出了一种称为依赖社区霍克斯（DCH）模型的方法，该方法结合了随机块模型（用于社区结构建模）和互激发霍克斯过程（用于依赖性建模）。作者通过分析事件计数矩阵上的光谱聚类错误，推导出非渐近的错误上界。此外，他们还提出了一种仅包含自我激发和相互激发的DCH模型，并使用广义矩估计法（GMM）来进行参数估计。

**结果:** 研究表明，所提出的DCH模型可以有效捕捉时间网络中的社区结构和依赖性。光谱聚类错误的理论分析表明，随着节点数量、社区数量、时间持续时间和模型依赖程度的变化，错误率可以被有效控制。此外，GMM估计器在扩展到更大的网络规模和更长的时间持续时表现出一致性。

**结论:** 依赖社区霍克斯（DCH）模型为时间网络提供了一种有效的建模方法，能够在捕捉社区结构的同时考虑节点对之间的依赖性。研究结果表明，这种模型在理论和实践中都是可行的，特别是在大规模时间网络的应用场景中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectral+clustering+for+dependent+community+Hawkes+process+models+of+temporal+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21845&send_immediately=true&force_search=false)

**原文摘要:** Temporal networks observed continuously over time through timestamped
relational events data are commonly encountered in application settings
including online social media communications, financial transactions, and
international relations. Temporal networks often exhibit community structure
and strong dependence patterns among node pairs. This dependence can be modeled
through mutual excitations, where an interaction event from a sender to a
receiver node increases the possibility of future events among other node
pairs.
  We provide statistical results for a class of models that we call dependent
community Hawkes (DCH) models, which combine the stochastic block model with
mutually exciting Hawkes processes for modeling both community structure and
dependence among node pairs, respectively. We derive a non-asymptotic upper
bound on the misclustering error of spectral clustering on the event count
matrix as a function of the number of nodes and communities, time duration, and
the amount of dependence in the model. Our result leverages recent results on
bounding an appropriate distance between a multivariate Hawkes process count
vector and a Gaussian vector, along with results from random matrix theory. We
also propose a DCH model that incorporates only self and reciprocal excitation
along with highly scalable parameter estimation using a Generalized Method of
Moments (GMM) estimator that we demonstrate to be consistent for growing
network size and time duration.

</details>


### [173] [Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion](https://arxiv.org/abs/2505.21892)
*Xunpeng Huang, Yingyu Lin, Nikki Lijing Kuang, Hanze Dong, Difan Zou, Yian Ma, Tong Zhang*

**主要类别:** stat.ML

**概要:** 连续扩散模型在数据生成方面表现出色，但受限于前向马尔可夫过程的局部邻接结构和反向去噪过程中引入的偏差。本文提出了一种新的方法——量化转换扩散（QTD），将数据量化与离散扩散动力学相结合。该方法通过直方图逼近和二进制编码将连续数据分布转换为离散分布，从而在结构化的离散潜在空间中实现高效表示。此外，设计了一个基于海明距离转换的连续时间马尔可夫链作为前向过程，并引入了截断一致化技术进行反向采样，以无偏差地从离散分布生成样本。通过新颖的KL动态分析，证明了QTD可以在期望内用$O(d\ln^2(d/\epsilon))$次评分评估来逼近$d$维目标分布$p_*$，误差容限为$\epsilon$。此方法不仅提高了推理效率，还统一了离散和连续扩散范式，推动了扩散生成建模的理论基础。


<details>
  <summary>更多</summary>
  
**动机:** 尽管连续扩散模型在多领域数据生成方面表现优异，但其效率受到两个关键限制：1）前向马尔可夫过程的局部邻接结构限制了数据空间中的长距离转换；2）时间非齐次反向去噪过程模拟中固有的偏差。这些限制促使研究者探索一种新的方法，以解决上述问题并提高生成效率。

**方法:** 提出了一种名为Quantized Transition Diffusion (QTD)的新方法，结合数据量化和离散扩散动力学。具体步骤包括：1）通过直方图逼近和二进制编码将连续数据分布$p_*$转换为离散分布$q_*$；2）设计一个基于海明距离转换的连续时间马尔可夫链（CTMC）作为前向过程；3）引入截断一致化技术来模拟反向CTMC，从而实现从$q_*$无偏差地生成样本；4）利用新颖的KL动态分析证明了QTD的有效性和效率。

**结果:** 实验结果表明，QTD方法可以显著提高推理效率，在期望内用较少的评分评估次数达到较高的生成精度。同时，该方法统一了离散和连续扩散范式，为扩散生成建模提供了坚实的理论基础。

**结论:** QTD方法成功解决了连续扩散模型中的两个关键限制，提高了生成效率并推进了理论发展。它不仅为高维数据生成提供了高效的解决方案，还为未来扩散模型的研究奠定了重要基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Almost+Linear+Convergence+under+Minimal+Score+Assumptions%3A+Quantized+Transition+Diffusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21892&send_immediately=true&force_search=false)

**原文摘要:** Continuous diffusion models have demonstrated remarkable performance in data
generation across various domains, yet their efficiency remains constrained by
two critical limitations: (1) the local adjacency structure of the forward
Markov process, which restricts long-range transitions in the data space, and
(2) inherent biases introduced during the simulation of time-inhomogeneous
reverse denoising processes. To address these challenges, we propose Quantized
Transition Diffusion (QTD), a novel approach that integrates data quantization
with discrete diffusion dynamics. Our method first transforms the continuous
data distribution $p_*$ into a discrete one $q_*$ via histogram approximation
and binary encoding, enabling efficient representation in a structured discrete
latent space. We then design a continuous-time Markov chain (CTMC) with Hamming
distance-based transitions as the forward process, which inherently supports
long-range movements in the original data space. For reverse-time sampling, we
introduce a \textit{truncated uniformization} technique to simulate the reverse
CTMC, which can provably provide unbiased generation from $q_*$ under minimal
score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we
prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score
evaluations in expectation to approximate the $d$--dimensional target
distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only
establishes state-of-the-art inference efficiency but also advances the
theoretical foundations of diffusion-based generative modeling by unifying
discrete and continuous diffusion paradigms.

</details>


### [174] [Higher-Order Group Synchronization](https://arxiv.org/abs/2505.21932)
*Adriana L. Duncan, Joe Kileel*

**主要类别:** stat.ML

**概要:** 这篇论文提出了一种新的高阶群同步问题，并在超图上进行操作，通过消息传递算法对高阶局部测量进行同步以获得节点的全局估计。该方法在旋转和角度同步中表现出色，且对外部异常点具有更强的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的群同步问题主要关注基于网络的局部测量来确定全局估计，而高阶群同步问题则扩展了这一概念，适用于计算机视觉、图像处理等领域，能够更好地捕捉复杂的局部关系。

**方法:** 定义高阶群同步问题并讨论其数学基础；提出一个计算框架，利用消息传递算法直接作用于高阶测量；分析理论保证，包括在外点和噪声下的收敛性。

**结果:** 数值实验表明，该方法在某些情况下优于标准的成对同步方法，对外部异常点更鲁棒；在模拟冷冻电子显微镜数据上的表现与标准重建包相当。

**结论:** 高阶群同步方法为解决复杂同步问题提供了一个强有力的工具，特别是在存在外部异常点的情况下，表现出了显著的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Higher-Order+Group+Synchronization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21932&send_immediately=true&force_search=false)

**原文摘要:** Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.

</details>


### [175] [Learning Curves of Stochastic Gradient Descent in Kernel Regression](https://arxiv.org/abs/2505.22048)
*Haihan Zhang, Weicheng Lin, Yuanshi Liu, Cong Fang*

**主要类别:** stat.ML

**概要:** 这篇论文研究了在线一阶算法（如单次遍历的随机梯度下降，SGD）在核回归中的表现，并与离线方法（如岭回归和无岭回归）进行了比较。作者分析了在源条件下SGD的表现，尤其是在模型可能被错误指定的情况下。结果表明，除了在高度错误指定且样本量极大的情况下，SGD能够达到最优收敛速度而不会受到饱和现象的影响。这主要归因于指数衰减的学习率策略。此外，文章还提供了该策略优于迭代平均方法的第一个理论证明。


<details>
  <summary>更多</summary>
  
**动机:** 研究在线一阶算法（如SGD）在核回归问题中的性能，特别是当模型可能被错误指定时的表现如何。并将其与离线方法（如岭回归）进行比较，以了解在线算法的优劣。

**方法:** 作者分析了单次遍历的随机梯度下降（SGD）在球面上内积核下的表现。具体来说，在源条件假设下，即最优预测器可能不属于再生核希尔伯特空间（RKHS），研究了不同样本规模n相对于输入维度d的情况下的过拟合风险曲线。通过数学推导，确定了在各种情况下的收敛速率。

**结果:** 结果显示，除非模型高度错误指定并且处于学习的最终阶段（样本量远大于输入维度的幂次增长），否则SGD可以达到最小最大最优率而不受饱和现象影响。这种能力主要归因于指数衰减的学习率策略。此外，文章还证明了在常见设置下，这种方法相较于迭代平均方法具有首个可证明的优势。

**结论:** 单次遍历的SGD在核回归中表现出色，能够在大多数情况下达到最优收敛速度，而不受饱和现象的影响。这为使用类似策略训练深度神经网络提供了理论支持。同时，文章首次展示了其相对于迭代平均方法的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Curves+of+Stochastic+Gradient+Descent+in+Kernel+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22048&send_immediately=true&force_search=false)

**原文摘要:** This paper considers a canonical problem in kernel regression: how good are
the model performances when it is trained by the popular online first-order
algorithms, compared to the offline ones, such as ridge and ridgeless
regression? In this paper, we analyze the foundational single-pass Stochastic
Gradient Descent (SGD) in kernel regression under source condition where the
optimal predictor can even not belong to the RKHS, i.e. the model is
misspecified. Specifically, we focus on the inner product kernel over the
sphere and characterize the exact orders of the excess risk curves under
different scales of sample sizes $n$ concerning the input dimension $d$.
Surprisingly, we show that SGD achieves min-max optimal rates up to constants
among all the scales, without suffering the saturation, a prevalent phenomenon
observed in (ridge) regression, except when the model is highly misspecified
and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant
$\gamma >0$. The main reason for SGD to overcome the curse of saturation is the
exponentially decaying step size schedule, a common practice in deep neural
network training. As a byproduct, we provide the \emph{first} provable
advantage of the scheme over the iterative averaging method in the common
setting.

</details>


### [176] [Individualised Counterfactual Examples Using Conformal Prediction Intervals](https://arxiv.org/abs/2505.22326)
*James M. Adams, Gesine Reinert, Lukasz Szpruch, Carsten Maple, Andrew Elliott*

**主要类别:** stat.ML

**概要:** 本文提出了一种基于个体化保形预测区间（CPICFs）的反事实解释方法，通过量化个体对分类器知识的不确定性来选择最能提供信息的反事实样本。在合成数据集和真实数据集中验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前高维特征空间中的反事实解释方法存在多种可能的反事实样本，因此需要额外标准来选择最有用的反事实。本文提出考虑个体对分类器的知识水平，选择能够最大化信息增益的反事实样本。

**方法:** 1. 建立个体知识模型，并使用保形预测区间的宽度评估个体预测的不确定性。
2. 在不确定性高的区域寻找反事实样本，这些样本对个体来说更具信息量。
3. 使用合成数据集可视化决策边界、保形预测区间及生成的CPICFs。
4. 测试单个CPICF对局部区域内个体知识的影响。
5. 在合成数据集和真实数据集上通过数据增强测量反事实样本的效用。

**结果:** 1. 合成数据集验证了CPICFs方法可以有效选择具有高信息量的反事实样本。
2. 单个CPICF能够在局部区域显著提高个体对分类器的理解。
3. 真实数据集实验表明，通过数据增强，CPICFs提高了模型在保留测试集上的性能。

**结论:** 本文提出的CPICFs方法通过结合个体知识和保形预测区间，成功选择了最能提供信息的反事实样本，提升了个体对分类器的理解和模型的整体性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Individualised+Counterfactual+Examples+Using+Conformal+Prediction+Intervals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22326，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22326&send_immediately=true&force_search=false)

**原文摘要:** Counterfactual explanations for black-box models aim to pr ovide insight into
an algorithmic decision to its recipient. For a binary classification problem
an individual counterfactual details which features might be changed for the
model to infer the opposite class. High-dimensional feature spaces that are
typical of machine learning classification models admit many possible
counterfactual examples to a decision, and so it is important to identify
additional criteria to select the most useful counterfactuals. In this paper,
we explore the idea that the counterfactuals should be maximally informative
when considering the knowledge of a specific individual about the underlying
classifier. To quantify this information gain we explicitly model the knowledge
of the individual, and assess the uncertainty of predictions which the
individual makes by the width of a conformal prediction interval. Regions of
feature space where the prediction interval is wide correspond to areas where
the confidence in decision making is low, and an additional counterfactual
example might be more informative to an individual. To explore and evaluate our
individualised conformal prediction interval counterfactuals (CPICFs), first we
present a synthetic data set on a hypercube which allows us to fully visualise
the decision boundary, conformal intervals via three different methods, and
resultant CPICFs. Second, in this synthetic data set we explore the impact of a
single CPICF on the knowledge of an individual locally around the original
query. Finally, in both our synthetic data set and a complex real world dataset
with a combination of continuous and discrete variables, we measure the utility
of these counterfactuals via data augmentation, testing the performance on a
held out set.

</details>


### [177] [Credal Prediction based on Relative Likelihood](https://arxiv.org/abs/2505.22332)
*Timo Löhr, Paul Hofman, Felix Mohr, Eyke Hüllermeier*

**主要类别:** stat.ML

**概要:** 提出了一种基于相对似然性的信度预测方法，通过调整阈值控制预测的正确性和精确性，并用修改后的集成学习技术近似信度集，实验表明该方法在不损害预测性能的情况下具有更好的不确定性表示能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的信度预测方法可能无法充分表示学习者的认知不确定性，因此需要一种新的方法来更好地捕捉这种不确定性。

**方法:** 基于统计学中的相对似然性概念，定义预测目标为所有（条件）概率分布的集合，这些分布由相对似然性超过指定阈值的合理模型生成。通过修改后的集成学习技术来逼近这些信度集。

**结果:** 实验结果表明，该方法在基准数据集上能够更有效地表示不确定性，同时保持了良好的预测性能，优于几种最先进的信度预测基线方法。

**结论:** 所提出的基于相对似然性的信度预测方法是一种理论上严谨且有效的方法，能够在正确性和精确性之间进行权衡，并为不确定性建模提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Credal+Prediction+based+on+Relative+Likelihood，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22332&send_immediately=true&force_search=false)

**原文摘要:** Predictions in the form of sets of probability distributions, so-called
credal sets, provide a suitable means to represent a learner's epistemic
uncertainty. In this paper, we propose a theoretically grounded approach to
credal prediction based on the statistical notion of relative likelihood: The
target of prediction is the set of all (conditional) probability distributions
produced by the collection of plausible models, namely those models whose
relative likelihood exceeds a specified threshold. This threshold has an
intuitive interpretation and allows for controlling the trade-off between
correctness and precision of credal predictions. We tackle the problem of
approximating credal sets defined in this way by means of suitably modified
ensemble learning techniques. To validate our approach, we illustrate its
effectiveness by experiments on benchmark datasets demonstrating superior
uncertainty representation without compromising predictive performance. We also
compare our method against several state-of-the-art baselines in credal
prediction.

</details>


### [178] [Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows](https://arxiv.org/abs/2505.22364)
*Gabriele Visentin, Patrick Cheridito*

**主要类别:** stat.ML

**概要:** 提出了一种使用条件归一化流来高效计算高维空间中最佳传输映射和Wasserstein重心的新方法，该方法通过梯度最小化传输成本直接解决原问题，并能扩展到计算Wasserstein重心，数值实验表明其在各种高维任务中表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 当前计算高维空间中的最优传输映射和Wasserstein重心的方法通常依赖对偶公式和复杂的对抗优化，效率较低且难以处理大量输入分布的重心计算。

**方法:** 利用条件归一化流将输入分布近似为从公共潜在空间的可逆前向变换，从而可以直接通过基于梯度的传输成本最小化来求解原问题；进一步地，通过求解条件方差最小化问题来计算Wasserstein重心。

**结果:** 该方法能够准确计算涉及数百个输入分布的Wasserstein重心，这是以前的方法难以实现的；数值实验显示该方法在各种高维任务中表现出色，优于先前的最先进方法。

**结论:** 提出的条件归一化流方法为高效计算高维空间中的最优传输映射和Wasserstein重心提供了新途径，解决了以往方法在处理大量输入分布时的计算难题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computing+Optimal+Transport+Maps+and+Wasserstein+Barycenters+Using+Conditional+Normalizing+Flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22364，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22364&send_immediately=true&force_search=false)

**原文摘要:** We present a novel method for efficiently computing optimal transport maps
and Wasserstein barycenters in high-dimensional spaces. Our approach uses
conditional normalizing flows to approximate the input distributions as
invertible pushforward transformations from a common latent space. This makes
it possible to directly solve the primal problem using gradient-based
minimization of the transport cost, unlike previous methods that rely on dual
formulations and complex adversarial optimization. We show how this approach
can be extended to compute Wasserstein barycenters by solving a conditional
variance minimization problem. A key advantage of our conditional architecture
is that it enables the computation of barycenters for hundreds of input
distributions, which was computationally infeasible with previous methods. Our
numerical experiments illustrate that our approach yields accurate results
across various high-dimensional tasks and compares favorably with previous
state-of-the-art methods.

</details>


### [179] [Hypothesis Testing in Imaging Inverse Problems](https://arxiv.org/abs/2505.22481)
*Yiming Xi, Konstantinos Zygalakis, Marcelo Pereyra*

**主要类别:** stat.ML

**概要:** 本文提出了一种针对成像逆问题的语义假设检验框架。现代成像方法在支持假设检验方面存在困难，而假设检验是科学方法的核心组成部分，对于实验的严谨解释和与决策过程的稳健接口至关重要。成像假设检验具有三个主要挑战：单一观测难以同时重建图像、制定假设并量化其统计显著性；成像中的假设多为语义性质而非像素值的定量陈述；由于零假设和备择假设分布通常未知，控制检验错误概率具有挑战性。本文提出的框架通过利用自监督计算成像、视觉-语言模型和基于e值的非参数假设检验概念来解决这些困难。通过与图像表型相关的数值实验展示了所提出的框架，在稳健控制一类错误的同时取得了优异的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现代成像方法难以支持假设检验，而假设检验是科学方法的核心组成部分，对于实验的严谨解释和与决策过程的稳健接口至关重要。

**方法:** 本文提出的框架结合了自监督计算成像、视觉-语言模型和基于e值的非参数假设检验概念。

**结果:** 通过与图像表型相关的数值实验展示了所提出的框架，在稳健控制一类错误的同时取得了优异的效果。

**结论:** 本文提出了一种针对成像逆问题的语义假设检验框架，能够有效应对成像假设检验中的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hypothesis+Testing+in+Imaging+Inverse+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22481&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes a framework for semantic hypothesis testing tailored to
imaging inverse problems. Modern imaging methods struggle to support hypothesis
testing, a core component of the scientific method that is essential for the
rigorous interpretation of experiments and robust interfacing with
decision-making processes. There are three main reasons why image-based
hypothesis testing is challenging. First, the difficulty of using a single
observation to simultaneously reconstruct an image, formulate hypotheses, and
quantify their statistical significance. Second, the hypotheses encountered in
imaging are mostly of semantic nature, rather than quantitative statements
about pixel values. Third, it is challenging to control test error
probabilities because the null and alternative distributions are often unknown.
Our proposed approach addresses these difficulties by leveraging concepts from
self-supervised computational imaging, vision-language models, and
non-parametric hypothesis testing with e-values. We demonstrate our proposed
framework through numerical experiments related to image-based phenotyping,
where we achieve excellent power while robustly controlling Type I errors.

</details>


### [180] [IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas](https://arxiv.org/abs/2505.22518)
*Agnideep Aich, Ashit Baran Aich, Bruce Wade*

**主要类别:** stat.ML

**概要:** IGNIS Network是一种新的神经框架，通过学习从可观测依赖性度量到copula参数的直接映射来克服传统方法的限制。它在模拟数据上进行训练，覆盖了五个Archimedean copula族，并在实际数据集上验证了其效用。


<details>
  <summary>更多</summary>
  
**动机:** 传统的copula参数估计方法（如MoM、MLE和MPL）在处理A1和A2家族等复杂依赖结构时存在非单调关系和数值不稳定性的问题。

**方法:** 提出了一种名为IGNIS Network的统一神经框架，该框架通过理论指导的后处理直接从可观测依赖性度量学习到copula参数的映射。

**结果:** 与MoM相比，IGNIS Network减少了估计误差，并通过理论指导的后处理强制执行参数约束。在包括金融、医疗和环境测量在内的多个真实数据集上验证了其实际效用。

**结论:** IGNIS Network为现代应用中的鲁棒和准确的依赖建模提供了变革性的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IGNIS%3A+A+Neural+Network+Framework+for+Robust+Parameter+Estimation+in+Archimedean+Copulas，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22518&send_immediately=true&force_search=false)

**原文摘要:** Parameter estimation for Archimedean copulas remains a challenging problem,
particularly for the recently developed A1 and A2 families that exhibit complex
dependency structures. Traditional methods, such as the Method of Moments
(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood
(MPL), often struggle due to issues of non-monotonic relationship with
dependency measures such as Kendall's tau (as in the case of A1) and numerical
instability. In this paper, we present the IGNIS Network, a novel, unified
neural framework that learns a direct mapping from observable dependency
measures to copula parameters, thereby overcoming the limitations of classical
approaches. Our approach is trained on simulated data spanning five Archimedean
copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its
general applicability across the entire family. Extensive simulation studies
demonstrate that the IGNIS Network reduces estimation errors compared to MoM,
while inherently enforcing parameter constraints through theory-guided
post-processing. We further validate the practical utility of our method on
diverse real-world datasets, including financial returns (AAPL-MSFT),
healthcare metrics (CDC Diabetes indicators), and environmental measurements
(PM2.5 air quality). Our results underscore the transformative potential of
neural methods for robust and accurate dependence modeling in modern
applications.

</details>


### [181] [Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling](https://arxiv.org/abs/2505.22527)
*Agnideep Aich, Ashit Aich, Bruce Wade*

**主要类别:** stat.ML

**概要:** 提出了一种新的生成模型Symplectic Generative Network (SGN)，通过哈密顿力学实现潜空间和数据空间之间的可逆、体积保持的映射，无需雅可比行列式计算即可进行精确的可能性评估。论文从多个方面提供了严谨的理论基础，展示了SGNs的基本优势，并为未来的研究奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 引入一种基于哈密顿力学的深度生成模型，以解决传统生成模型在可能性评估时需要大量计算雅可比行列式的难题。

**方法:** 构建一个具有辛结构的潜空间，并将数据生成过程建模为哈密顿系统的随时间演化，从而实现潜空间和数据空间之间的可逆且体积保持的映射。同时提供包括可逆性和体积保持性的完整证明、正式复杂度分析、强化的通用逼近结果、基于统计流形几何的信息论分析以及广泛的稳定性分析等理论支持。

**结果:** 成功开发了SGN模型，能够实现无需雅可比行列式计算的精确可能性评估，并通过理论分析验证了其在复杂高维数据中的潜在优势。

**结论:** SGN提供了一个坚实的理论基础，展现了其在处理复杂高维数据时的根本优势，为进一步的经验研究和应用铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Symplectic+Generative+Networks+%28SGNs%29%3A+A+Hamiltonian+Framework+for+Invertible+Deep+Generative+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22527，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22527&send_immediately=true&force_search=false)

**原文摘要:** We introduce the Symplectic Generative Network (SGN), a deep generative model
that leverages Hamiltonian mechanics to construct an invertible,
volume-preserving mapping between a latent space and the data space. By
endowing the latent space with a symplectic structure and modeling data
generation as the time evolution of a Hamiltonian system, SGN achieves exact
likelihood evaluation without incurring the computational overhead of Jacobian
determinant calculations. In this work, we provide a rigorous mathematical
foundation for SGNs through a comprehensive theoretical framework that
includes: (i) complete proofs of invertibility and volume preservation, (ii) a
formal complexity analysis with theoretical comparisons to Variational
Autoencoders and Normalizing Flows, (iii) strengthened universal approximation
results with quantitative error bounds, (iv) an information-theoretic analysis
based on the geometry of statistical manifolds, and (v) an extensive stability
analysis with adaptive integration guarantees. These contributions highlight
the fundamental advantages of SGNs and establish a solid foundation for future
empirical investigations and applications to complex, high-dimensional data.

</details>


### [182] [Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction](https://arxiv.org/abs/2505.22554)
*Agnideep Aich, Md Monzur Murshed, Amanda Mayeaux, Sameera Hewage*

**主要类别:** stat.ML

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Copulas+Be+Used+for+Feature+Selection%3F+A+Machine+Learning+Study+on+Diabetes+Risk+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22554，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22554&send_immediately=true&force_search=false)

**原文摘要:** Accurate diabetes risk prediction relies on identifying key features from
complex health datasets, but conventional methods like mutual information (MI)
filters and genetic algorithms (GAs) often overlook extreme dependencies
critical for high-risk subpopulations. In this study we introduce a
feature-selection framework using the upper-tail dependence coefficient
({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher
values of a predictor co-occur with diabetes diagnoses (target variable).
Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method
prioritizes five predictors (self-reported general health, high blood pressure,
body mass index, mobility limitations, and high cholesterol levels) based on
upper tail dependencies. These features match or outperform MI and GA selected
subsets across four classifiers (Random Forest, XGBoost, Logistic Regression,
Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to
0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation
importance confirms clinical relevance, with BMI and general health driving
accuracy. To our knowledge, this is the first work to apply a copula's
upper-tail dependence for supervised feature selection, bridging extreme-value
theory and machine learning to deliver a practical toolkit for diabetes
prevention.

</details>


### [183] [Principled Out-of-Distribution Generalization via Simplicity](https://arxiv.org/abs/2505.22622)
*Jiawei Ge, Amanda Wang, Shange Tang, Chi Jin*

**主要类别:** stat.ML

**概要:** 现代基础模型在图像生成中的扩散模型展示了出色的分布外（OOD）泛化能力，本文通过简单性理论框架对此进行研究，并为学习真正可泛化的简单模型提供了首个精确的样本复杂度保证。


<details>
  <summary>更多</summary>
  
**动机:** 尽管现代基础模型展现了卓越的OOD泛化能力，但其背后的理论原理尚未明确。因此，需要研究扩散模型在图像生成中的组成泛化能力，以揭示为何这些模型能够符合人类期望。

**方法:** 通过分析神经网络架构的表现力以及简单性原则，开发了一个基于简单性的OOD泛化理论框架。该框架量化了简单性，并研究了两种关键场景：恒定差距设置和消失差距设置。

**结果:** 建立了正则化最大似然估计器的理论分析，并提供了学习真正可泛化简单模型的第一个精确样本复杂度保证。

**结论:** 简单性原则可以解释扩散模型的OOD泛化能力，并为未来的研究提供了一个理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Principled+Out-of-Distribution+Generalization+via+Simplicity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22622&send_immediately=true&force_search=false)

**原文摘要:** Modern foundation models exhibit remarkable out-of-distribution (OOD)
generalization, solving tasks far beyond the support of their training data.
However, the theoretical principles underpinning this phenomenon remain
elusive. This paper investigates this problem by examining the compositional
generalization abilities of diffusion models in image generation. Our analysis
reveals that while neural network architectures are expressive enough to
represent a wide range of models -- including many with undesirable behavior on
OOD inputs -- the true, generalizable model that aligns with human expectations
typically corresponds to the simplest among those consistent with the training
data.
  Motivated by this observation, we develop a theoretical framework for OOD
generalization via simplicity, quantified using a predefined simplicity metric.
We analyze two key regimes: (1) the constant-gap setting, where the true model
is strictly simpler than all spurious alternatives by a fixed gap, and (2) the
vanishing-gap setting, where the fixed gap is replaced by a smoothness
condition ensuring that models close in simplicity to the true model yield
similar predictions. For both regimes, we study the regularized maximum
likelihood estimator and establish the first sharp sample complexity guarantees
for learning the true, generalizable, simple model.

</details>
