<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 17]
- [cs.CL](#cs.CL) [总数: 38]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

**主要类别:** cs.AI

**AI概要:** 本文提出AI对齐问题应重新定义为通过基于过程、多智能体、发展性机制来构建熵减的、对理由响应的智能体，而非编码固定的人类价值内容。


<details>
  <summary>更多</summary>
  
**动机:** 传统基于内容的价值规范方法存在结构性不稳定问题，包括事实-价值鸿沟、价值多元化和扩展框架问题，这促使作者重新思考AI对齐的本质。

**方法:** 提出三个哲学贡献：1)阐述"规范陷阱"论证；2)提出熵减(syntropy)作为理解多智能体对齐动态的信息理论框架；3)建立真实与模拟道德能力的功能区分。

**结果:** 建立了一个能够产生关于人工系统中价值涌现和道德主体性的具体、可证伪预测的理论框架。

**结论:** 该框架为AI对齐提供了新的哲学基础，但需要进一步的实证验证，目前相关实证研究正在准备中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Syntropic+Frameworks+in+AI+Alignment%3A+A+Philosophical+Investigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03048&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [2] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为'权重计算主义'的新型认知架构，基于第一原理将认知分解为不可分割的逻辑原子和两个基本操作：指向和比较，通过可解释的权重计算模型实现决策，为构建可信赖的通用人工智能提供理论基础和实践路径。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI范式在可解释性和价值对齐方面面临根本性挑战，需要一种新的认知架构来解决这些问题并实现通用人工智能。

**方法:** 采用基于图算法的计算引擎和全局工作空间工作流，将认知分解为逻辑原子和基本操作，通过权重计算模型(权重=收益*概率)形式化决策过程，所有值都可追溯到可审计的初始权重集。

**结果:** 架构实现了透明的类人推理和在新颖场景中的稳健学习，展示了在未见过情境中的内在泛化能力。

**结论:** 权重计算主义架构为构建可信赖和对齐的通用人工智能奠定了实践和理论基础，提供了通向AGI的可行路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+the+Black+Box%3A+A+Cognitive+Architecture+for+Explainable+and+Aligned+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03072，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03072&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [3] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He, Dingmin Wang*

**主要类别:** cs.AI

**AI概要:** 论文探讨了符号求解器集成方法在何时能增强传统长思维链的性能，发现该方法仅在问题需要有限隐式推理但涉及大搜索空间时有效，特别是在约束满足问题上能显著提升LLM性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型推理模型生成长思维链会产生大量token开销，甚至可能导致错误答案，需要探索符号求解器集成方法在何时能增强传统推理方法。

**方法:** 研究符号求解器集成方法，利用LLM的代码生成能力将推理任务转换为可执行代码，然后用符号求解器解决问题，通过实验比较不同方法的性能。

**结果:** 符号求解器集成方法仅在问题需要有限隐式推理但涉及大搜索空间时有效；GPT-4o在推理深度较浅的演绎问题上表现更好；符号求解器方法在需要重复回溯的约束满足问题上显著提升LLM性能；提供声明性示例时，CodeLlama-13B在困难斑马谜题上能超越GPT-4o。

**结论:** 符号求解器集成方法是对传统长思维链推理的有效补充，特别适用于特定类型的复杂推理问题，为不同推理任务选择合适的方法提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Do+Symbolic+Solvers+Enhance+Reasoning+in+Large+Language+Models%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03272&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [4] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan, Ryota Kanai, Manuel Baltieri*

**主要类别:** cs.AI

**AI概要:** 该研究比较了主动推理中四种偏好分布定义方式对智能体性能的影响，发现在网格世界导航任务中，目标塑造（设置中间目标）能获得最佳性能但会牺牲对环境动态的学习。


<details>
  <summary>更多</summary>
  
**动机:** 主动推理文献中很少关注偏好分布如何指定以及不同指定方式如何影响智能体的推理和学习性能，本研究旨在填补这一空白。

**方法:** 考虑了四种定义偏好分布的方式（硬目标/软目标，有无目标塑造），在网格世界导航任务中比较四种智能体的性能表现。

**结果:** 目标塑造能够实现最佳整体性能（促进利用），但会牺牲对环境转移动态的学习（阻碍探索）。

**结论:** 偏好分布的定义方式对主动推理智能体的性能有重要影响，目标塑造虽能提升性能但会限制探索能力，需要在利用和探索之间进行权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prior+preferences+in+active+inference+agents%3A+soft%2C+hard%2C+and+goal+shaping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03293，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03293&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [5] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith, Marwa Abdulhai, Manfred Diaz, Marko Tesic, Rakshit S. Trivedi, Alexander Sasha Vezhnevets, Lewis Hammond, Jesse Clifton, Minsuk Chang, Edgar A. Duéñez-Guzmán, John P. Agapiou, Jayd Matyas, Danny Karmon, Akash Kundu, Aliaksei Korshuk, Ananya Ananya, Arrasy Rahman, Avinaash Anand Kulandaivel, Bain McHale, Beining Zhang, Buyantuev Alexander, Carlos Saith Rodriguez Rojas, Caroline Wang, Chetan Talele, Chenao Liu, Chichen Lin, Diana Riazi, Di Yang Shi, Emanuel Tewolde, Elizaveta Tennant, Fangwei Zhong, Fuyang Cui, Gang Zhao, Gema Parreño Piqueras, Hyeonggeun Yun, Ilya Makarov, Jiaxun Cui, Jebish Purbey, Jim Dilkes, Jord Nguyen, Lingyun Xiao, Luis Felipe Giraldo, Manuela Chacon-Chamorro, Manuel Sebastian Rios Beltran, Marta Emili García Segura, Mengmeng Wang, Mogtaba Alim, Nicanor Quijano, Nico Schiavone, Olivia Macmillan-Scott, Oswaldo Peña, Peter Stone, Ram Mohan Rao Kadiyala, Rolando Fernandez, Ruben Manrique, Sunjia Lu, Sheila A. McIlraith, Shamika Dhuri, Shuqing Shi, Siddhant Gupta, Sneheel Sarangi, Sriram Ganapathi Subramanian, Taehun Cha, Toryn Q. Klassen, Wenming Tu, Weijian Fan, Wu Ruiyang, Xue Feng, Yali Du, Yang Liu, Yiding Wang, Yipeng Kang, Yoonchang Sung, Yuxuan Chen, Zhaowei Zhang, Zhihan Wang, Zhiqiang Wu, Ziang Chen, Zilong Zheng, Zixia Jia, Ziyan Wang, Dylan Hadfield-Menell, Natasha Jaques, Tim Baarslag, Jose Hernandez-Orallo, Joel Z. Leibo*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个评估LLM智能体在零样本混合动机环境中合作能力的新方法，使用Concordia多智能体模拟环境，发现在需要说服和规范执行的场景中现有智能体的合作能力存在显著差距


<details>
  <summary>更多</summary>
  
**动机:** 现有评估方法无法衡量LLM智能体在新颖社交情境中的合作能力泛化表现，需要开发能测试智能体在不同伙伴和情境中识别和利用互利机会的方法

**方法:** 使用Concordia自然语言多智能体模拟环境，在NeurIPS 2024 Concordia竞赛中评估智能体在从谈判到集体行动问题等多种场景中实现互利的能力

**结果:** 研究发现当前智能体能力与稳健泛化所需的可靠合作之间存在显著差距，特别是在需要说服和规范执行的场景中

**结论:** 需要进一步提升LLM智能体在复杂社交情境中的合作能力，特别是在说服和规范执行方面的表现，以实现更可靠的跨情境合作

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Generalization+Capabilities+of+LLM-Based+Agents+in+Mixed-Motive+Scenarios+Using+Concordia，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03318，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03318&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [6] [Multimodal Reinforcement Learning with Agentic Verifier for AI Agents](https://arxiv.org/abs/2512.03438)
*Reuben Tan, Baolin Peng, Zhengyuan Yang, Hao Cheng, Oier Mees, Theodore Zhao, Andrea Tupini, Isar Meijier, Qianhui Wu, Yuncong Yang, Lars Liden, Yu Gu, Sheng Zhang, Xiaodong Liu, Lijuan Wang, Marc Pollefeys, Yong Jae Lee, Jianfeng Gao*

**主要类别:** cs.AI

**AI概要:** Argos是一个多模态强化学习的智能奖励代理，通过选择性地结合教师模型和规则基础的评分函数，为推理过程提供细粒度的奖励信号，显著提升多模态推理模型在空间推理、视觉幻觉等任务中的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态强化学习模型主要依赖稀疏的基于最终结果的奖励，缺乏对推理过程的细粒度指导。不同样本需要不同的评分函数，且教师模型可能提供噪声奖励信号，这限制了学习效果。

**方法:** 提出Argos奖励代理，为每个样本从教师模型和规则基础的评分函数池中选择合适的函数，同时评估：(1)最终回答准确性；(2)时空实体和动作定位；(3)推理过程质量。在SFT数据筛选和RL训练中都使用该验证器。

**结果:** 在空间推理、视觉幻觉、机器人和具身AI基准测试中取得了最先进的结果。证明了仅靠SFT后训练不足以防止RL过程中的非接地解崩溃，在线验证能有效减少奖励攻击。

**结论:** Argos通过帕累托最优性理论证明了其有效性，为多模态强化学习提供了更细粒度和可靠的奖励机制，显著提升了智能推理模型的性能和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Reinforcement+Learning+with+Agentic+Verifier+for+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03438&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.

</details>


### [7] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang, Tianpei Yang, Jingwen Qiao, Yanqing Wu, Jing Huo, Xingguo Chen, Yang Gao*

**主要类别:** cs.AI

**AI概要:** 提出了一种通信受限的多智能体强化学习框架，通过双互信息估计器区分和处理有损/无损消息，将通信影响量化到全局奖励中，在多个基准测试中验证了有效性


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中存在普遍的有损通信问题，现有多智能体强化学习方法由于可扩展性和鲁棒性有限，难以应用于复杂动态的真实环境

**方法:** 提出广义通信约束模型统一表征不同场景的通信条件，作为学习先验区分有损/无损消息；利用双互信息估计器解耦有损/无损消息对分布式决策的影响；构建通信约束的多智能体强化学习框架，将通信消息影响量化到全局奖励

**结果:** 在多个通信受限基准测试中验证了方法的有效性

**结论:** 该方法能够有效处理多智能体系统中的有损通信问题，提高了在复杂动态环境中的适用性和性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Reinforcement+Learning+with+Communication-Constrained+Priors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03528，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03528&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [8] [PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks](https://arxiv.org/abs/2512.03549)
*Yuki Orimo, Iori Kurata, Hodaka Mori, Ryuhei Okuno, Ryohto Sawada, Daisuke Okanohara*

**主要类别:** cs.AI

**AI概要:** PARC是一个用于自主执行长时程计算任务的分层多智能体编码代理系统，具有自我评估和反馈机制，能够在材料科学和数据科学任务中实现无需人工干预的自主工作。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决AI系统在长时程计算任务中需要持续人工监督的问题，开发能够自主检测和纠正高级战略错误、维持任务进度的智能系统。

**方法:** 采用分层多智能体架构，包含任务规划、执行、自我评估和反馈机制，能够从独立上下文评估自身行动结果并提供反馈。

**结果:** 在材料科学中成功复现锂离子传导和合金分离研究的关键结果，协调数十个并行模拟任务（每个约43小时计算时间）；在Kaggle数据科学任务中，从自然语言指令出发产生与人工设计基线竞争性的解决方案。

**结论:** 分层多智能体系统结合自我评估和反馈机制具有巨大潜力，能够实现AI系统独立进行大规模科学和分析工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PARC%3A+An+Autonomous+Self-Reflective+Coding+Agent+for+Robust+Execution+of+Long-Horizon+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03549，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03549&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.

</details>


### [9] [Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks](https://arxiv.org/abs/2512.03560)
*Gianni Molinari, Fabio Ciravegna*

**主要类别:** cs.AI

**AI概要:** RP-ReAct是一种新颖的多智能体方法，通过将战略规划与低级执行解耦来解决企业复杂任务中的轨迹不稳定性和上下文窗口限制问题。该方法使用Reasoner Planner Agent进行规划分析，Proxy-Execution Agent执行工具交互，并采用上下文保存策略管理大输出。在ToolQA基准测试中表现出优越性能和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 企业领域复杂任务需要协调多个工具和处理多样化数据源，但现有自主智能体存在两个主要限制：单智能体架构导致轨迹不稳定性，以及本地模型的小上下文窗口限制导致大型工具输出快速消耗上下文空间。

**方法:** 提出RP-ReAct多智能体架构，包含Reasoner Planner Agent（负责规划子步骤和持续分析执行结果）和Proxy-Execution Agent（使用ReAct方法将子步骤转换为具体工具交互）。采用上下文保存策略通过外部存储和按需访问来管理大型工具输出。

**结果:** 在ToolQA基准测试中使用六种开放权重推理模型进行评估，结果显示RP-ReAct在解决跨域复杂任务时实现了优越的性能和改进的泛化能力，超越了最先进的基线方法。

**结论:** RP-ReAct展示了在不同模型规模下的增强鲁棒性和稳定性，为企业的有效和可部署智能体解决方案铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reason-Plan-ReAct%3A+A+Reasoner-Planner+Supervising+a+ReAct+Executor+for+Complex+Enterprise+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03560，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03560&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.

</details>


### [10] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li, Armando Solar-Lezama, Yisong Yue, Stephan Zheng*

**主要类别:** cs.AI

**AI概要:** 提出了PAN编程模型和EnCompass框架，将智能体工作流逻辑与推理时策略解耦，通过Python装饰器实现，提高开发效率和可靠性


<details>
  <summary>更多</summary>
  
**动机:** 当前智能体编程方法将工作流逻辑和推理策略（如树搜索）耦合在一起，限制了开发灵活性和实验效率

**方法:** 引入概率天使非确定性（PAN）编程模型，使用Python装饰器将智能体工作流程序编译为搜索空间，实现工作流与推理策略的分离

**结果:** 通过三个案例研究表明，该框架能让程序员快速提高智能体可靠性，轻松切换不同推理策略，且只需少量额外编码

**结论:** PAN模型和EnCompass框架有效解决了智能体编程中的关注点分离问题，为LLM智能体开发提供了更灵活高效的方法

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EnCompass%3A+Enhancing+Agent+Programming+with+Search+Over+Program+Execution+Paths，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03571，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03571&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [11] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu, Xiaotie Deng*

**主要类别:** cs.AI

**AI概要:** DeepRule是一个自动化业务规则生成框架，通过三层次架构解决零售品类和定价优化中的理论与实际脱节问题，结合大语言模型处理非结构化数据、博弈论优化机制和可解释决策蒸馏，在实际零售环境中实现了比基准方法更高的利润和操作可行性。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有理论模型与现实经济复杂性之间的系统性不匹配问题，特别是数据模态不匹配、动态特征纠缠和操作不可行性三大关键差距。

**方法:** 采用三层次架构：1)混合知识融合引擎使用大语言模型进行深度语义解析；2)博弈论约束优化机制通过双边效用函数动态协调供应链利益；3)可解释决策蒸馏接口利用LLM引导的符号回归优化定价策略。

**结果:** 在实际零售环境中验证，相比系统性B2C基准方法获得了更高的利润，同时确保了操作可行性。

**结论:** 该框架建立了一个统一非结构化知识注入、多智能体优化和可解释策略合成的闭环管道，为实际经济智能提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepRule%3A+An+Integrated+Framework+for+Automated+Business+Rule+Generation+via+Deep+Predictive+Modeling+and+Hybrid+Search+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03607&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [12] [MemVerse: Multimodal Memory for Lifelong Learning Agents](https://arxiv.org/abs/2512.03627)
*Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, Ding Wang*

**主要类别:** cs.AI

**AI概要:** MemVerse是一个即插即用的记忆框架，通过结合参数化记忆和分层检索记忆，解决AI代理的记忆限制问题，显著提升多模态推理和持续学习能力


<details>
  <summary>更多</summary>
  
**动机:** 现有AI代理缺乏可靠记忆机制，导致灾难性遗忘、长时推理困难以及在多模态交互环境中无法保持连贯性操作的问题

**方法:** 采用模型无关的框架设计，包含短期记忆维护和将多模态经验转化为分层知识图谱的长期记忆结构，支持持续巩固、自适应遗忘和有界内存增长，并引入周期性蒸馏机制压缩关键知识到参数模型中

**结果:** 大量实验证明MemVerse显著提升了多模态推理和持续学习效率

**结论:** MemVerse框架使AI代理能够在扩展交互中实现记忆、适应和连贯推理，为可扩展的自适应多模态智能提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MemVerse%3A+Multimodal+Memory+for+Lifelong+Learning+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03627&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.

</details>


### [13] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu, Fengfeng Wei, Weineng Chen*

**主要类别:** cs.AI

**AI概要:** RoCo是一个基于多智能体角色协作的系统，通过探索者、利用者、批评者和集成者四个专门角色的LLM智能体协同工作，自动设计高质量的启发式算法，在组合优化问题上超越了现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于大语言模型的自动启发式设计研究通常只考虑单一角色，缺乏多样性和协作机制，限制了启发式算法的质量和性能。

**方法:** 提出RoCo多智能体角色系统，包含四个专门角色：探索者（创造性思考）、利用者（效率优化）、批评者（评估反馈）、集成者（平衡整合），通过多轮反馈和精英变异过程进行协作。

**结果:** 在5个不同组合优化问题的白盒和黑盒设置下，RoCo均表现出优越性能，生成的启发式算法超越了ReEvo和HSEvo等现有方法。

**结论:** 基于角色的协作范式为稳健高效的自动启发式设计设立了新标准，证明了多角色协作在提升算法多样性和质量方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RoCo%3A+Role-Based+LLMs+Collaboration+for+Automatic+Heuristic+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03762&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [14] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang, Songxiang Liu, Disong Wang, Yuanyuan Wang, Guanglu Wan, Helen Meng*

**主要类别:** cs.AI

**AI概要:** Omni-AutoThink是一个自适应推理框架，通过动态调整推理深度来解决现有Omni模型推理行为僵化的问题，包含自适应监督微调和自适应强化学习两个阶段，在多种模态上显著提升了自适应推理性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有Omni模型在推理行为上存在僵化问题，要么对简单问题过度推理，要么在需要推理时无法进行推理，需要一种能够根据任务难度动态调整推理深度的解决方案。

**方法:** 提出两阶段框架：(1)自适应监督微调阶段，使用大规模推理增强数据赋予模型基础推理能力；(2)自适应强化学习阶段，基于任务复杂度和奖励反馈优化推理行为。构建了涵盖文本、音频、视觉等多模态的自适应推理基准。

**结果:** 实验结果表明，与之前的基线相比，提出的框架显著提高了自适应推理性能。

**结论:** Omni-AutoThink框架有效解决了Omni模型的推理僵化问题，通过自适应调整推理深度实现了更好的性能，所有基准数据和代码将公开发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Omni-AutoThink%3A+Adaptive+Multimodal+Reasoning+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03783，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03783&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [15] [A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)](https://arxiv.org/abs/2512.03887)
*Saurav Prateek*

**主要类别:** cs.AI

**AI概要:** 论文提出Static-DRA，一种基于可配置树状静态工作流程的深度研究代理，通过Depth和Breadth参数让用户控制研究强度，平衡报告质量与计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 解决静态RAG管道在处理复杂多轮研究任务时的局限性，需要更灵活的代理系统。

**方法:** 构建包含Supervisor、Independent和Worker代理的分层架构，实现多跳信息检索和并行子主题研究，通过用户可调的Depth和Breadth参数控制研究深度和广度。

**结果:** 在DeepResearch Bench基准测试中，使用depth=2和breadth=5配置获得34.72分，实验证明增加参数值能提升研究深度和评分。

**结论:** Static-DRA提供了实用且资源感知的解决方案，赋予用户对深度研究过程的透明控制，代码和结果已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Hierarchical+Tree-based+approach+for+creating+Configurable+and+Static+Deep+Research+Agent+%28Static-DRA%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03887&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/

</details>


### [16] [Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties](https://arxiv.org/abs/2512.03931)
*Vineel Tummala, Daniela Inclezan*

**主要类别:** cs.AI

**AI概要:** 基于逻辑编程的政策感知自主代理框架，能够推理违规惩罚并相应行动，在必要时允许偏离政策以实现高风险目标，同时提高可解释性和计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注确保合规性，但需要考虑为达成高风险目标而偏离政策的情况，同时通过模拟人类决策来辅助政策制定者。

**方法:** 扩展Gelfond和Lobo的授权与义务策略语言(AOPL)以纳入惩罚机制，集成答案集编程(ASP)进行推理，开发自动化翻译工具，并改进基于ASP的规划算法。

**结果:** 在两个领域的实验中，该框架生成了更高质量的计划，避免了有害行动，并在某些情况下提高了计算效率。

**结论:** 该框架有潜力增强自主决策能力并为政策优化提供信息，展示了在政策感知自主代理领域的重要应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Autonomous+Agents+and+Policy+Compliance%3A+A+Framework+for+Reasoning+About+Penalties，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03931，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03931&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [17] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs, Luis Miguel Vieira da Silva, Jayanth Somashekaraiah, Maximilian Weigand, David Kube, Felix Gehlhoff*

**主要类别:** cs.AI

**AI概要:** 提出了一个基于Blocksworld问题的标准化基准测试框架，通过Model Context Protocol接口评估LLM智能体的自适应规划与执行能力


<details>
  <summary>更多</summary>
  
**动机:** 工业自动化需要灵活的适应控制策略，但LLM智能体缺乏系统比较的标准化基准

**方法:** 创建包含5个复杂度类别的可执行仿真环境，集成MCP作为标准化工具接口，支持不同智能体架构的无缝评估

**结果:** 开发了基准测试框架并展示了单智能体实现的可行性，建立了量化比较指标

**结论:** 该基准为LLM智能体的规划执行方法提供了标准化评估平台，支持系统化比较和性能量化

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmark+for+Planning+and+Control+with+Large+Language+Model+Agents%3A+Blocksworld+with+Model+Context+Protocol，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03955，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03955&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [18] [Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models](https://arxiv.org/abs/2512.03047)
*Samih Fadli*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个动态评估大语言模型安全性的框架，通过定义行为分类法、训练分类器来估计伦理熵S(t)，并在压力测试中测量四个前沿模型的熵动态变化。研究发现基础模型表现出持续的熵增长，而调优变体将伦理熵降低了约80%。基于这些轨迹，论文估计了有效对齐工作率gamma_eff，并将其与S(t)嵌入到监控管道中，用于运行时监督价值漂移。


<details>
  <summary>更多</summary>
  
**动机:** 传统的大语言模型安全性评估主要依赖静态基准测试，但关键故障往往是动态的，包括分布漂移下的价值漂移、越狱攻击以及部署中对齐性能的缓慢退化。论文基于'智能第二定律'，将伦理熵视为需要通过对齐工作来抵消的状态变量，旨在使这一框架在大语言模型中可操作化。

**方法:** 1. 定义五维行为分类法
2. 训练分类器从模型转录中估计伦理熵S(t)
3. 对四个前沿模型的基础版本和指令调优版本进行压力测试，测量熵动态
4. 分析熵变化轨迹并估计有效对齐工作率gamma_eff
5. 将S(t)和gamma_eff嵌入监控管道，设置稳定性阈值警报

**结果:** 基础模型显示出持续的伦理熵增长，表明价值漂移问题严重。而经过指令调优的模型变体成功抑制了漂移，将伦理熵降低了约80%。通过轨迹分析能够估计出有效的对齐工作率，为运行时监控提供了量化指标。

**结论:** 该研究成功地将伦理熵框架操作化应用于大语言模型安全性评估，证明了指令调优在抑制价值漂移方面的有效性。提出的监控管道能够实时检测伦理熵漂移，为运行时监督和价值对齐提供了实用的工具和方法论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Entropy-Based+Measurement+of+Value+Drift+and+Alignment+Work+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03047，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03047&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.

</details>


### [19] [Watermarks for Embeddings-as-a-Service Large Language Models](https://arxiv.org/abs/2512.03079)
*Anudeex Shetty*

**主要类别:** cs.CL

**AI概要:** 该论文研究了Embeddings-as-a-Service（EaaS）水印技术的安全性，揭示了现有水印技术易受释义攻击的漏洞，并提出了一种基于线性变换的新型水印技术WET，能够有效防御此类攻击。


<details>
  <summary>更多</summary>
  
**动机:** 随着企业提供基于大语言模型的嵌入服务(EaaS)，需要保护模型知识产权免受黑盒模仿攻击。现有水印技术被发现存在安全漏洞，因此需要研究更强大的水印防御机制。

**方法:** 1. 分析现有EaaS水印技术对释义攻击的脆弱性；2. 提出WET水印技术，通过线性变换嵌入向量并设计反向变换验证机制；3. 在不同攻击设置和数据集上进行实验验证。

**结果:** 1. 发现释义攻击能有效绕过当前最先进的EaaS水印技术；2. WET技术对释义攻击具有近乎完美的可验证性；3. 通过消融研究验证了WET各组件和超参数的重要性。

**结论:** 论文揭示了EaaS水印技术的新安全漏洞，提出的WET水印技术为服务提供商提供了更可靠的模型知识产权保护方案，对EaaS的安全发展具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Watermarks+for+Embeddings-as-a-Service+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03079&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.

</details>


### [20] [Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation](https://arxiv.org/abs/2512.03082)
*Nan Zhuang, Wenshuo Wang, Lekai Qian, Yuxiao Wang, Boyu Cao, Qi Liu*

**主要类别:** cs.CL

**AI概要:** 该论文提出了Reasoning Dependency Generation (RDG)框架，通过生成平衡的推理问答对数据来缓解大型语言模型中的选择支持性偏见，实验显示在微调后模型性能显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型在评估时表现出选择支持性偏见，倾向于支持自己选择的选项，这可能影响AI辅助决策的客观性。目前针对认知偏见的去偏方法研究较少。

**方法:** 提出RDG框架，自动构建平衡的推理问答对，显式建模选择、证据和理由之间的依赖关系，生成包含上下文依赖数据和依赖解耦数据的大规模数据集用于微调。

**结果:** 实验结果显示，使用RDG生成数据微调的LLM在基于记忆的实验上提升81.5%，在基于评估的实验上提升94.3%，同时在标准BBQ基准测试上保持相似性能。

**结论:** 这项工作开创了解决LLM中认知偏见的方法，为开发更可靠的AI辅助决策支持系统做出了贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Alleviating+Choice+Supportive+Bias+in+LLM+with+Reasoning+Dependency+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03082，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03082&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.

</details>


### [21] [Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies](https://arxiv.org/abs/2512.03195)
*Stylianos Saroglou, Konstantinos Diamantaras, Francesco Preta, Marina Delianidi, Apostolos Benisis, Christian Johannes Meyer*

**主要类别:** cs.CL

**AI概要:** 本研究开发了一个开源工具，结合句子链接和实体链接两种方法，将职位空缺文本链接到ESCO和EQF欧洲框架，并引入两个标注数据集来评估职业和资格表示。


<details>
  <summary>更多</summary>
  
**动机:** 改进劳动力市场信息分类，通过语言模型将职位空缺文本与欧洲技能和资格框架(ESCO和EQF)进行链接，超越表面的技能提取。

**方法:** 比较句子链接和实体链接两种方法，开发开源工具整合这两种方法，引入专门标注的数据集，并探索使用生成式大语言模型的应用方式。

**结果:** 开发了公开可用的开源工具和标注数据集，为工作实体提取提供了先进的计算基础设施，促进了劳动力市场分析的数字化研究。

**结论:** 该研究推动了职位实体提取的技术发展，为数字化经济中的工作、技能和劳动力市场叙事分析提供了重要的计算基础设施支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Job+Matching%3A+Occupation%2C+Skill+and+Qualification+Linking+with+the+ESCO+and+EQF+taxonomies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03195，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03195&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier

</details>


### [22] [InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation](https://arxiv.org/abs/2512.03197)
*Faezeh Faez, Marzieh S. Tahaei, Yaochen Hu, Ali Pourranjbar, Mahdi Biparva, Mark Coates, Yingxue Zhang*

**主要类别:** cs.CL

**AI概要:** InvertiTune框架通过受控数据生成和监督微调，实现了从文本到知识图谱的高效单次构建，优于现有方法并具有更好的跨数据集泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有Text2KG方法依赖迭代式LLM提示，计算成本高且容易忽略文本中分布的复杂关系，需要更高效的解决方案。

**方法:** 结合受控数据生成管道（从大型知识库提取子图、噪声过滤、LLM生成文本描述）和监督微调(SFT)，生成高质量训练数据集。

**结果:** 在CE12k数据集上超越大型非微调LLM和最先进Text2KG方法，在CrossEval-1200测试集上展现更强的跨数据集泛化能力。

**结论:** 现实、高质量的训练数据对推进高效高性能Text2KG系统至关重要，InvertiTune为此提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InvertiTune%3A+High-Quality+Data+Synthesis+for+Cost-Effective+Single-Shot+Text-to-Knowledge+Graph+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03197&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.

</details>


### [23] [Identifying attributions of causality in political text](https://arxiv.org/abs/2512.03214)
*Paulina Garcia-Corral*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个用于检测和解析政治文本中解释性内容的框架，通过训练轻量级因果语言模型来提取因果声明对，实现大规模分析政治解释。


<details>
  <summary>更多</summary>
  
**动机:** 解释在政治理解中至关重要，但政治科学中缺乏系统分析解释的方法，现有方法零散且针对特定问题。

**方法:** 训练一个轻量级因果语言模型，从政治文本中提取结构化的因果声明对（原因-结果对），用于下游分析。

**结果:** 该方法展示了如何大规模研究因果解释，证明了其适中的标注需求、良好的泛化能力和相对于人工编码的准确性。

**结论:** 提出的框架为系统分析政治文本中的解释提供了有效工具，有助于推动政治科学中对解释性内容的深入研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Identifying+attributions+of+causality+in+political+text，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03214&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.

</details>


### [24] [Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs](https://arxiv.org/abs/2512.03310)
*Kunj Joshi, David A. Smith*

**主要类别:** cs.CL

**AI概要:** 提出了一种名为随机掩码微调(RMFT)的新技术，通过在微调过程中随机掩码PII来减少语言模型对个人身份信息的记忆，同时保持模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型在训练过程中容易记忆个人身份信息(PII)，存在严重的安全和隐私风险，需要开发有效的隐私保护技术。

**方法:** 使用随机掩码微调(RMFT)技术，在Enron邮件数据集上进行实验验证，并提出了MaxTER评估框架来分析隐私-效用权衡。

**结果:** 相比基线微调，RMFT实现了80.81%的总提取率降低和80.17%的已见提取率降低，困惑度仅增加5.73%，性能优于去重方法。

**结论:** RMFT是一种有效的隐私保护微调方法，能够在显著减少PII记忆的同时保持模型性能，为语言模型的隐私保护提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Randomized+Masked+Finetuning%3A+An+Efficient+Way+to+Mitigate+Memorization+of+PIIs+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03310&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.

</details>


### [25] [Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní](https://arxiv.org/abs/2512.03334)
*Nemika Tyagi, Nelvin Licona Guevara, Olga Kellert*

**主要类别:** cs.CL

**AI概要:** 本研究提出了一个LLM辅助的标注流程，用于分析西班牙语-英语和西班牙语-瓜拉尼语双语话语的社会语言学和主题特征，通过自动标注揭示了语言使用模式与社会因素的定量关系。


<details>
  <summary>更多</summary>
  
**动机:** 传统的社会语言学分析依赖人工标注，成本高且难以扩展到大规模语料。本研究旨在利用大语言模型自动恢复可解释的社会语言学模式，为跨语言和低资源双语研究提供计算方法的进步。

**方法:** 使用大语言模型自动标注3,691个语码转换句子的主题、体裁和话语语用功能，整合迈阿密双语语料库的人口统计元数据，并为西班牙语-瓜拉尼语数据集添加新的主题标注。

**结果:** 结果显示迈阿密数据中性别、语言优势和话语功能之间存在系统性联系，巴拉圭文本中正式瓜拉尼语和非正式西班牙语存在明显的双语分工。这些发现用语料库规模的定量证据复制和扩展了早期的互动和社会语言学观察。

**结论:** 大语言模型能够可靠地恢复传统上只能通过人工标注获得的可解释社会语言学模式，推动了跨语言和低资源双语研究的计算方法发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Topics+and+Sociolinguistic+Variation+in+Code-Switched+Discourse%3A+Insights+from+Spanish-English+and+Spanish-Guaran%C3%AD，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03334，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03334&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaraní. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaraní dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaraní and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.

</details>


### [26] [PERCS: Persona-Guided Controllable Biomedical Summarization Dataset](https://arxiv.org/abs/2512.03340)
*Rohan Charudatt Salvi, Chirag Chawla, Dhruv Jain, Swapnil Panigrahi, Md Shad Akhtar, Shweta Yadav*

**主要类别:** cs.CL

**AI概要:** PERCS数据集：针对四种不同医学知识背景用户群体（普通大众、医学生、非医学研究者、医学专家）的个性化生物医学摘要简化数据集，包含经过医生审核的事实准确性和用户群体匹配度验证。


<details>
  <summary>更多</summary>
  
**动机:** 现有医学文本简化资源通常假设单一通用受众，忽略了不同用户群体在医学素养和信息需求方面的显著差异，需要针对特定受众的个性化摘要生成方法。

**方法:** 创建PERCS数据集，包含生物医学摘要和针对四种用户群体的定制摘要；由医生使用详细错误分类法进行事实准确性和用户群体匹配度审查；使用自动评估指标对四个大型语言模型进行基准测试。

**结果:** 技术验证显示不同用户群体的摘要在可读性、词汇选择和内容深度方面存在明显差异；建立了未来研究的基线结果。

**结论:** PERCS数据集、标注指南和评估材料已公开可用，支持个性化沟通和可控生物医学摘要生成的研究，强调需要针对特定受众的定制化摘要方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PERCS%3A+Persona-Guided+Controllable+Biomedical+Summarization+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03340&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.

</details>


### [27] [Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning](https://arxiv.org/abs/2512.03343)
*Darshan Fofadiya*

**主要类别:** cs.CL

**AI概要:** 提出了Idea-Gated Transformer架构，通过分离语义规划和语法生成来解决自回归语言模型的主题漂移问题，使用概念向量实时门控词汇选择，在保持困惑度相当的同时显著提升领域保持能力


<details>
  <summary>更多</summary>
  
**动机:** 解决自回归语言模型在Next-Token Prediction训练中出现的'主题漂移'问题，即生成内容容易偏离初始提示，因为模型过度依赖局部关联而非全局规划

**方法:** 引入Idea-Gated Transformer架构，包含辅助的'Idea Head'来预测未来上下文窗口的词袋分布，生成概念向量，通过可微分门控机制实时抑制语义无关的词汇

**结果:** 在WikiText-103上的实验显示，模型在验证困惑度与标准GPT-2基线相当的同时，表现出显著优越的领域保持能力，能够成功将生成锁定在特定语义簇中

**结论:** 门控机制有效抵抗关联漂移，为更可控的语言建模提供了参数高效的路径，解决了NTP目标的根本性短视问题

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Idea-Gated+Transformers%3A+Enforcing+Semantic+Coherence+via+Differentiable+Vocabulary+Pruning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03343，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03343&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \citep{holtzman2019curious}. While scaling model size mitigates this \citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.

</details>


### [28] [From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation](https://arxiv.org/abs/2512.03360)
*Qingchuan Li, Mingyue Cheng, Zirui Liu, Daoyu Wang, Yuting Zeng, Tongxuan Liu*

**主要类别:** cs.CL

**AI概要:** HBLR框架：通过置信度感知符号翻译和假设驱动后向推理，结合反思机制提升逻辑推理的准确性和效率


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型的前向推理方法存在冗余路径、幻觉步骤和语义漂移问题，导致推理效率低且不可靠

**方法:** 1) 置信度感知符号翻译：仅高置信度内容转为逻辑形式，不确定内容保留自然语言；2) 翻译反思模块确保语义保真度；3) 假设驱动后向推理：假设结论为真并递归验证前提；4) 推理反思模块识别纠正错误推理步骤

**结果:** 在五个推理基准测试中，HBLR在准确性和效率方面均优于强基线方法

**结论:** HBLR框架通过结合符号推理和自然语言处理的优势，模拟人类演绎思维，显著提升了逻辑推理的性能和可靠性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Hypothesis+to+Premises%3A+LLM-based+Backward+Logical+Reasoning+with+Selective+Symbolic+Translation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03360&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.

</details>


### [29] [Nexus: Higher-Order Attention Mechanisms in Transformers](https://arxiv.org/abs/2512.03377)
*Hanting Chen, Chu Zhong, Kai Han, Yuchuan Tian, Yuchen Liang, Tianyu Guo, Xinghao Chen, Dacheng Tao, Yunhe Wang*

**主要类别:** cs.CL

**AI概要:** 提出高阶注意力网络(Hon)，通过递归框架增强表示能力，打破标准注意力的低秩瓶颈限制，在多个基准测试中优于标准Transformer。


<details>
  <summary>更多</summary>
  
**动机:** 标准一阶注意力机制存在低秩瓶颈问题，难以在单层中捕获复杂的多跳关系。

**方法:** 采用递归框架，通过嵌套自注意力机制动态精炼Query和Key向量表示，使用参数高效的权重共享策略。

**结果:** 理论分析证明该方法打破标准注意力的线性瓶颈，实证结果显示在多个基准测试中优于标准Transformer。

**结论:** 高阶注意力网络通过递归注意力机制有效提升了模型的表示能力，同时保持参数效率，为解决标准注意力的局限性提供了有效方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nexus%3A+Higher-Order+Attention+Mechanisms+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03377，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03377&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.

</details>


### [30] [Characterizing Language Use in a Collaborative Situated Game](https://arxiv.org/abs/2512.03381)
*Nicholas Tomlin, Naitian Zhou, Eve Fleisig, Liangyuan, Chen, Téa Wright, Lauren Vinh, Laura X. Ma, Seun Eisape, Ellie French, Tingting Du, Tianjiao Zhang, Alexander Koller, Alane Suhr*

**主要类别:** cs.CL

**AI概要:** Portal对话语料库：收集了11.5小时《传送门2》合作模式中的人类口语对话，包含24.5K条话语，用于分析复杂协作环境中的语言现象。


<details>
  <summary>更多</summary>
  
**动机:** 合作视频游戏中玩家在复杂环境下通过沟通和推理进行协调，产生了丰富的语言数据，但现有闲聊或任务导向对话语料库缺乏这类复杂协作场景的语言现象。

**方法:** 收集《传送门2》游戏合作模式中的玩家对话数据，包括视频、音频、转录文本、游戏状态数据，并进行手动和自动语言标注。

**结果:** 识别出大多数现有对话语料库中罕见的语言现象，包括复杂空间指代、澄清与修复、临时约定形成等。

**结论:** 该语料库为分析复杂、情境化、协作问题解决场景中的语言使用提供了宝贵资源，已公开发布供未来研究使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Characterizing+Language+Use+in+a+Collaborative+Situated+Game，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03381&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.

</details>


### [31] [Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates](https://arxiv.org/abs/2512.03402)
*Yixing Xu, Chao Li, Xuanwu Yin, Spandan Tiwari, Dong Li, Ashish Sirasao, Emad Barsoum*

**主要类别:** cs.CL

**AI概要:** Dual LoRA方法通过将低秩矩阵分解为幅度组和方向组，并分别使用ReLU和sign函数来改进LoRA性能，在多个NLP任务上超越了原始LoRA及其先进变体。


<details>
  <summary>更多</summary>
  
**动机:** 传统LoRA方法由于低秩假设导致性能不佳，需要更好的参数更新机制来模拟全参数微调的效果。

**方法:** 将低秩矩阵分为两个组：幅度组（控制参数更新程度，使用ReLU函数）和方向组（决定参数更新方向，使用sign函数），更好地模拟基于梯度的参数更新过程。

**结果:** 在GPT-2、RoBERTa、DeBERTa和LLaMA系列模型上的NLG、NLU和常识推理任务实验中，Dual LoRA在相同可训练参数数量下始终优于LoRA及其先进变体。

**结论:** Dual LoRA通过引入归纳偏置改进了LoRA的性能，提供了一种更有效的参数高效微调方法，能够更好地模拟全参数微调的行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dual+LoRA%3A+Enhancing+LoRA+with+Magnitude+and+Direction+Updates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03402，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03402&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.

</details>


### [32] [PretrainZero: Reinforcement Active Pretraining](https://arxiv.org/abs/2512.03442)
*Xingrun Xing, Zhiyuan Fan, Jie Lou, Guoqi Li, Jiajun Zhang, Debing Zhang*

**主要类别:** cs.CL

**AI概要:** PretrainZero是一个基于预训练语料库的强化主动学习框架，通过主动识别信息内容、自监督学习和验证缩放来增强基础模型的通用推理能力，无需可验证标签或监督微调。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于强化学习的大模型虽然在某些领域展现专家级能力，但仍严重依赖特定领域的可验证奖励，这限制了通用推理能力的扩展。

**方法:** 提出主动预训练方法，学习统一推理策略从预训练语料中主动识别合理且信息丰富的内容；采用自监督学习直接在通用语料上预训练推理器；通过处理难度递增的掩码跨度进行验证缩放。

**结果:** 在强化预训练中，PretrainZero将Qwen3-4B-Base在MMLU-Pro、SuperGPQA和数学平均基准上分别提升了8.43、5.96和10.60分。预训练模型还可作为下游RLVR任务的推理基础模型。

**结论:** PretrainZero成功突破了通用推理的验证数据壁垒，显著增强了预训练基础模型的通用推理能力，为人工通用智能的发展提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PretrainZero%3A+Reinforcement+Active+Pretraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03442&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.

</details>


### [33] [A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention](https://arxiv.org/abs/2512.03494)
*Di Xiu, Hongyin Tang, Bolin Rong, Lizhi Yan, Jingang Wang, Yifan Lu, Xunliang Cai*

**主要类别:** cs.CL

**AI概要:** 该论文研究了Top-k注意力机制在解码和训练阶段的有效性，验证了在解码阶段仅保留与查询最相似的关键词可以达到或超越全注意力的性能，并探讨了训练-推理一致性、近似算法精度影响以及从熵理论角度的解释。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在长上下文建模中的推理计算成本已成为关键瓶颈，阻碍了智能体和多模态应用等任务的进展，需要研究更高效的注意力机制。

**方法:** 通过实验验证精确Top-k解码的有效性，探索原生Top-k注意力训练策略，研究近似Top-k算法精度对下游任务的影响，并从熵理论角度进行理论解释。

**结果:** 实验表明Top-k解码在HELMET和LongBench v2等下游任务上达到或超越全注意力性能；训练-推理一致性可进一步提升模型性能；下游任务性能与近似保真度正相关；Top-k注意力SFT导致下游任务熵减少现象。

**结论:** Top-k注意力机制是解决LLMs推理计算成本瓶颈的有效方法，低熵状态更适合Top-k解码，为高效长上下文建模提供了理论和实践基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Preliminary+Study+on+the+Promises+and+Challenges+of+Native+Top-%24k%24+Sparse+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03494，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03494&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.

</details>


### [34] [Understanding LLM Reasoning for Abstractive Summarization](https://arxiv.org/abs/2512.03503)
*Haohan Yuan, Siu Cheung Hui, Haopeng Zhang*

**主要类别:** cs.CL

**AI概要:** 研究发现大语言模型的推理能力在摘要任务中存在质量与事实忠实度的权衡：显式推理策略提高流畅性但降低事实准确性，隐式推理则相反；增加推理预算反而可能损害事实一致性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型在数学和代码生成等分析任务中表现出色，但其在抽象摘要任务中的有效性仍未被充分验证，需要系统研究不同推理策略的效果。

**方法:** 将通用推理策略适配到摘要领域，对8种推理策略和3个大推理模型在8个不同数据集上进行大规模比较研究，评估摘要质量和事实忠实度。

**结果:** 推理并非万能解决方案，其效果高度依赖于具体策略和上下文。显式推理策略倾向于提高流畅性但牺牲事实基础，而隐式推理则呈现相反模式。增加推理预算不会改善甚至可能损害事实一致性。

**结论:** 有效的摘要需要忠实压缩而非创造性过度思考，推理策略的选择需要在质量与忠实度之间做出权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+LLM+Reasoning+for+Abstractive+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03503，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03503&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.

</details>


### [35] [Fine-grained Narrative Classification in Biased News Articles](https://arxiv.org/abs/2512.03582)
*Zeba Afroz, Harsh Vardhan, Pawan Bhakuni, Aanchal Punia, Rajdeep Kumar, Md. Shad Akhtar*

**主要类别:** cs.CL

**AI概要:** 该论文提出了INDI-PROP数据集和两个基于GPT-4o-mini的多跳推理框架FANTA和TPTC，用于印度新闻媒体中偏见、叙事和说服技术的细粒度分类。


<details>
  <summary>更多</summary>
  
**动机:** 叙事是宣传的认知和情感支架，能将孤立的说服技术组织成连贯的故事来证明行动合理性、归因责任并唤起对意识形态阵营的认同。

**方法:** 开发了INDI-PROP数据集（1,266篇文章），包含三个层次标注：意识形态偏见、事件特定细粒度叙事框架和说服技术。提出了FANTA和TPTC两个GPT-4o-mini引导的多跳提示推理框架。

**结果:** 评估显示该方法在每个分类任务上都显著优于基线模型。

**结论:** 该研究为分析宣传内容提供了细粒度的叙事分类框架和多层次标注数据集，在偏见检测和宣传分析方面取得了重要进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-grained+Narrative+Classification+in+Biased+News+Articles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03582，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03582&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.

</details>


### [36] [AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment](https://arxiv.org/abs/2512.03634)
*Ahmad Aghaebrahimian*

**主要类别:** cs.CL

**AI概要:** 提出一个可解释的事实一致性评估框架，通过将文本分解为原子事实并使用加权指标来改进LLM幻觉问题的评估


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在临床等高风险领域容易产生看似合理但错误的信息（幻觉），现有评估指标无法充分评估事实一致性且缺乏可解释性

**方法:** 将文本分解为原子事实，采用灵活的、无模式的方法论，引入加权指标而非绝对指标，并提出控制复杂领域评估复杂度的机制

**结果:** 在通用和临床数据集上进行了基准测试，发布了代码以支持未来研究中的事实感知模型训练

**结论:** 该框架为解决LLM幻觉问题提供了有效的可解释评估方法，特别适用于高风险领域的事实一致性评估

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AlignCheck%3A+a+Semantic+Open-Domain+Metric+for+Factual+Consistency+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03634，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03634&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.

</details>


### [37] [Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context](https://arxiv.org/abs/2512.03671)
*Beatrice Savoldi, Giuseppe Attanasio, Olga Gorodetskaya, Marta Marchiori Manerba, Elisa Bassignana, Silvia Casola, Matteo Negri, Tommaso Caselli, Luisa Bentivogli, Alan Ramponi, Arianna Muti, Nicoletta Balbo, Debora Nozza*

**主要类别:** cs.CL

**AI概要:** 意大利首次关于生成式AI采用、使用模式和数字素养的实证研究，发现尽管用户数字素养低，生成式AI仍广泛用于工作和个人用途，甚至替代其他技术成为主要信息来源，存在明显的性别数字鸿沟，女性采用率仅为男性的一半。


<details>
  <summary>更多</summary>
  
**动机:** 研究生成式AI聊天机器人的兴起如何改变数字互动，同时关注其可能因采用不均衡和用户对其局限性认识不足而加剧数字鸿沟的风险。

**方法:** 基于新收集的1,906名意大利语成年人的调查数据，进行全面的实证分析，映射生成式AI的采用情况、使用模式和数字素养。

**结果:** 发现生成式AI被广泛用于工作和个人用途（包括情感支持和医疗建议等敏感任务），正在替代其他技术成为主要信息来源；存在显著性别鸿沟（女性采用率仅为男性一半）；数字素养是采用的关键预测因素但只能部分解释性别差异。

**结论:** 数据提供了生成式AI多用途使用的细粒度见解，强调需要有针对性的教育举措，并进一步研究能力无法完全解释的公平参与障碍。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+AI+Practices%2C+Literacy%2C+and+Divides%3A+An+Empirical+Analysis+in+the+Italian+Context，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03671，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03671&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.

</details>


### [38] [Evaluating Hydro-Science and Engineering Knowledge of Large Language Models](https://arxiv.org/abs/2512.03672)
*Shiruo Hu, Wenbo Shan, Yingjia Li, Zhiqi Wan, Xinpeng Yu, Yunjia Qi, Haotian Xia, Yang Xiao, Dingxiao Liu, Jiaru Wang, Chenxu Gong, Ruixi Zhang, Shuyue Wu, Shibo Cui, Chee Hui Lai, Wei Luo, Yubin He, Bin Xu, Jianshi Zhao*

**主要类别:** cs.CL

**AI概要:** 该研究提出了Hydro-SE Bench评估基准，包含4000道选择题，用于评估大语言模型在水科学与工程领域的知识、应用和推理能力。评估发现商业LLM准确率0.74-0.80，小参数模型0.41-0.68，模型在自然科学相关领域表现良好，但在行业标准等专业领域存在不足。


<details>
  <summary>更多</summary>
  
**动机:** 水科学与工程是一个多学科交叉的关键领域，需要专家协作决策。随着大语言模型的快速发展，需要评估其在Hydro-SE领域的知识和应用能力，但目前缺乏充分的评估标准。

**方法:** 提出Hydro-SE Bench评估基准，包含4000道选择题，覆盖9个子领域，评估LLM在基础概念知识、工程应用能力和推理计算能力三个方面的表现。

**结果:** 商业LLM准确率74-80%，小参数模型41-68%。LLM在自然科学相关子领域表现良好，但在行业标准、水工结构等专业领域表现较差。模型缩放主要提升推理计算能力。

**结论:** LLM在Hydro-SE任务中表现出明显的优势和不足，为模型开发者提供了明确的训练目标，并为Hydro-SE研究者提供了应用LLM的实践指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Hydro-Science+and+Engineering+Knowledge+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03672&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.

</details>


### [39] [Different types of syntactic agreement recruit the same units within large language models](https://arxiv.org/abs/2512.03676)
*Daria Kryvosheieva, Andrea de Varda, Evelina Fedorenko, Greta Tuckute*

**主要类别:** cs.CL

**AI概要:** 研究发现大语言模型中的语法知识表征具有系统性：不同语法现象（特别是语法一致性）在模型中激活重叠的神经元单元，表明语法一致性构成了模型表征空间中有意义的功能类别，这一模式在跨语言分析中得到验证。


<details>
  <summary>更多</summary>
  
**动机:** 探索大语言模型如何表征语法知识，特别是不同语法现象是否共享或使用不同的模型组件，以理解模型内部语法表征的组织方式。

**方法:** 采用认知神经科学启发的功能定位方法，在7个开源权重模型中识别对67种英语语法现象最敏感的神经元单元，并进行跨语言分析（包括英语、俄语、中文和57种不同语言）。

**结果:** 发现不同语法一致性类型（如主谓一致、回指、限定词-名词一致）激活重叠的神经元单元；跨语言分析显示结构更相似的语言在主谓一致性上共享更多神经元单元。

**结论:** 语法一致性作为语法依赖关系的关键标记，在大语言模型的表征空间中构成了有意义的功能类别，揭示了模型内部语法知识组织的系统性特征。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Different+types+of+syntactic+agreement+recruit+the+same+units+within+large+language+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03676，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03676&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.

</details>


### [40] [AITutor-EvalKit: Exploring the Capabilities of AI Tutors](https://arxiv.org/abs/2512.03688)
*Numaan Naeem, Kaushal Kumar Maurya, Kseniia Petukhova, Ekaterina Kochmar*

**主要类别:** cs.CL

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AITutor-EvalKit%3A+Exploring+the+Capabilities+of+AI+Tutors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03688，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03688&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.

</details>


### [41] [DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue](https://arxiv.org/abs/2512.03704)
*Yijun Liao*

**主要类别:** cs.CL

**AI概要:** DZ-TDPO是一个解决长对话状态惰性的非破坏性对齐框架，通过动态KL约束和可学习时间注意力偏置，在保持零样本泛化能力的同时达到最先进的胜率。


<details>
  <summary>更多</summary>
  
**动机:** 解决长对话系统中的状态惰性问题，即静态约束导致模型无法处理用户意图演变与历史上下文之间的冲突。

**方法:** 提出DZ-TDPO框架，结合冲突感知的动态KL约束和可学习的时间注意力偏置，实现非破坏性的对齐。

**结果:** 在Multi-Session Chat数据集上达到86.2%的胜率（Phi-3.5），Qwen2.5-7B模型达到99.4%胜率且困惑度开销可忽略，保持MMLU通用能力。

**结论:** 通过精确的注意力调节而非破坏性权重更新可以缓解状态惰性，大模型能实现近乎完美的对齐而不损失通用能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DZ-TDPO%3A+Non-Destructive+Temporal+Alignment+for+Mutable+State+Tracking+in+Long-Context+Dialogue，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03704&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO

</details>


### [42] [AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation](https://arxiv.org/abs/2512.03737)
*Chuyue Wang, Jie Feng, Yuxi Wu, Hang Zhang, Zhiguo Fan, Bing Cheng, Wei Lin*

**主要类别:** cs.CL

**AI概要:** AR-Med是一个用于医疗搜索的自动相关性评估框架，通过检索增强方法将大语言模型与验证医学知识结合，并通过知识蒸馏实现高效部署，在离线准确率和在线性能上均有显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 传统医疗搜索方法难以理解复杂用户查询，而大语言模型虽具潜力但存在幻觉、专业知识缺失和高成本等挑战，需要开发可靠且实用的医疗搜索解决方案。

**方法:** 提出AR-Med框架：1）采用检索增强方法将LLM推理基于验证医学知识；2）设计知识蒸馏方案将大教师模型压缩为紧凑学生模型；3）构建LocalQSMed多专家标注基准指导模型迭代。

**结果:** 离线准确率超过93%，相比原在线系统提升24%绝对改进，在线相关性和用户满意度显著提升，成功在在线医疗配送平台大规模部署。

**结论:** AR-Med为现实医疗应用中开发可信赖的LLM驱动系统提供了实用且可扩展的蓝图，成功解决了医疗搜索中的准确性、可靠性和效率问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AR-Med%3A+Automated+Relevance+Enhancement+in+Medical+Search+via+LLM-Driven+Information+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03737&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \textbf{AR-Med}, a novel framework for \textbf{A}utomated \textbf{R}elevance assessment for \textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\%, a 24\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.

</details>


### [43] [Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective](https://arxiv.org/abs/2512.03759)
*Jingyang Ou, Jiaqi Han, Minkai Xu, Shaoxuan Xu, Jianwen Xie, Stefano Ermon, Yi Wu, Chongxuan Li*

**主要类别:** cs.CL

**AI概要:** ESPO：针对扩散大语言模型的序列级强化学习框架，通过ELBO作为序列似然度代理，解决了扩散模型无法进行token级RL优化的根本问题，在数学推理、编程和规划任务上显著优于token级基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 扩散大语言模型(dLLMs)的迭代非自回归生成方式缺乏token级条件概率，使得传统的token级强化学习方法(如GRPO)无法直接应用，需要新的RL框架来解决这一根本性不匹配问题。

**方法:** 提出ELBO-based Sequence-level Policy Optimization (ESPO)框架，将整个序列生成视为单一动作，使用ELBO作为可处理的序列级似然度代理，包含token级重要性比率归一化和鲁棒的KL散度估计以确保大规模训练的稳定性。

**结果:** 在数学推理、编程和规划任务上的广泛实验显示，ESPO显著优于token级基线方法，在Countdown任务上实现了20-40分的显著提升，同时在数学和编程基准上保持一致的性能增益。

**结论:** ESPO为扩散大语言模型中的强化学习建立了一个原则性且经验有效的序列级优化范式，解决了扩散模型与RL方法之间的根本性不匹配问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Principled+RL+for+Diffusion+LLMs+Emerges+from+a+Sequence-Level+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03759&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.

</details>


### [44] [In-Context Representation Hijacking](https://arxiv.org/abs/2512.03771)
*Itay Yona, Amir Sarid, Michael Karasik, Yossi Gandelsman*

**主要类别:** cs.CL

**AI概要:** Doublespeak是一种针对大语言模型的上下文表示劫持攻击，通过将有害关键词替换为良性词汇，使模型在内部表示中将良性词汇理解为有害语义，从而绕过安全对齐机制。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM的安全对齐策略存在漏洞，攻击者可以通过操纵词汇的内部表示来绕过内容过滤和安全防护机制。

**方法:** 通过在多个上下文示例中有系统地用良性词汇（如carrot）替换有害关键词（如bomb），并提供一个有害请求的前缀，使模型内部表示发生语义覆盖。

**结果:** 攻击在闭源和开源模型上都取得高成功率，在Llama-3.3-70B-Instruct上单句上下文覆盖达到74%的攻击成功率，且具有跨模型家族的广泛可迁移性。

**结论:** 当前的对齐策略在表示层面存在不足，需要在表示级别进行安全防护，这揭示了大语言模型潜在空间中的新攻击面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是In-Context+Representation+Hijacking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03771&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.

</details>


### [45] [Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5](https://arxiv.org/abs/2512.03803)
*Huey Sun, Anabel Yong, Lorenzo Gilly, Felipe Jin*

**主要类别:** cs.CL

**AI概要:** 本研究将DoLa对比解码方法适配到T5和FLAN-T5编码器-解码器架构，首次在该架构中实现对比解码策略，评估其对指令跟随能力的影响。


<details>
  <summary>更多</summary>
  
**动机:** 现有对比解码方法如DoLa仅在解码器架构中实现且主要研究事实性改进，需要探索其在编码器-解码器架构中的应用效果。

**方法:** 将DoLa方法适配到T5和FLAN-T5模型家族，通过分层分析logit演化来量化DoLa对令牌输出概率的影响。

**结果:** DoLa在某些任务类别中提高了文本生成的忠实度，但在其他任务中表现不佳，结果存在任务依赖性。

**结论:** 对比解码在编码器-解码器架构中的应用效果具有选择性，需要根据具体任务类型进行评估和选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Instruction-Following+Capabilities+in+Seq2Seq+Models%3A+DoLA+Adaptations+for+T5，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03803&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.

</details>


### [46] [BERnaT: Basque Encoders for Representing Natural Textual Diversity](https://arxiv.org/abs/2512.03903)
*Ekhi Azurmendi, Joseba Fernandez de Landa, Jaione Bengoetxea, Maite Heredia, Julen Etxaniz, Mikel Zubillaga, Ander Soraluze, Aitor Soroa*

**主要类别:** cs.CL

**AI概要:** 研究发现语言模型应包含语言多样性（方言、历史、非正式语言等），而不仅依赖标准化文本。通过构建包含标准、社交媒体和历史语料的巴斯克语语料库，训练BERnaT模型，证明多样化训练能提升模型在所有任务上的性能且不影响标准基准准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前语言模型依赖经过质量过滤的大规模文本语料，但过滤过程可能无意中排除非标准语言变体，降低模型鲁棒性并加剧表示偏差。

**方法:** 针对巴斯克语构建包含标准文本、社交媒体和历史来源的新语料库，预训练BERnaT编码器模型（标准、多样化和组合三种配置），并提出将NLU任务分为标准和多样化子集的评估框架。

**结果:** 在标准和多样化数据上训练的模型始终优于仅使用标准语料训练的模型，在所有任务类型上都有性能提升，且不影响标准基准准确性。

**结论:** 语言多样性对于构建包容性强、泛化能力好的语言模型至关重要，多样化训练能提高模型性能而不损害标准任务表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BERnaT%3A+Basque+Encoders+for+Representing+Natural+Textual+Diversity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03903，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03903&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.

</details>


### [47] [Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology](https://arxiv.org/abs/2512.03818)
*Kylie L. Anglin, Stephanie Milan, Brittney Hernandez, Claudia Ventura*

**主要类别:** cs.CL

**AI概要:** 本研究提出了一个通过提示工程优化大语言模型在心理学文本分类任务中性能的实证框架，发现最有效的策略是结合代码本引导的经验提示选择和自动提示工程，而非传统的人格提示、思维链或解释性提示。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在文本分类中表现良好但输出严重依赖提示措辞，特别是在心理学等专业领域，理论驱动的构念定义可能在预训练数据中代表性不足，需要系统化的提示优化方法。

**方法:** 实验评估五种提示策略：代码本引导的经验提示选择、自动提示工程、人格提示、思维链推理和解释性提示，采用零样本和少样本分类方式，在三个心理学构念和两个模型上进行测试。

**结果:** 人格、思维链和解释性提示无法完全弥补糟糕措辞带来的性能损失；最具影响力的提示特征是构念定义、任务框架和示例；最符合专家判断的分类结果来自结合代码本引导和自动提示工程的少样本提示。

**结论:** 建议研究人员生成和评估尽可能多的提示变体（人工制作和自动生成），基于训练集的实证性能选择提示和示例，并在保留集上验证最终方法，为需要与专家判断对齐的场景提供实用、系统且理论驱动的LLM提示优化方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Alignment+Between+Human+and+Machine+Codes%3A+An+Empirical+Assessment+of+Prompt+Engineering+for+Construct+Identification+in+Psychology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03818&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.

</details>


### [48] [Jina-VLM: Small Multilingual Vision Language Model](https://arxiv.org/abs/2512.04032)
*Andreas Koukounas, Georgios Mastrapas, Florian Hönicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao*

**主要类别:** cs.CL

**AI概要:** Jina-VLM是一个2.4B参数的多语言视觉语言模型，在2B规模的开源VLM中实现了最先进的多语言视觉问答性能。


<details>
  <summary>更多</summary>
  
**动机:** 开发一个能够在多语言环境下处理任意分辨率图像的高效视觉问答模型，同时保持竞争力的纯文本处理能力。

**方法:** 使用SigLIP2视觉编码器和Qwen3语言主干，通过注意力池化连接器实现标记高效的任意分辨率图像处理。

**结果:** 在标准VQA基准测试和多语言评估中，Jina-VLM超越了同类可比模型。

**结论:** Jina-VLM成功实现了在保持文本性能的同时，在多语言视觉问答任务上达到最先进水平的平衡设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jina-VLM%3A+Small+Multilingual+Vision+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.04032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04032&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.

</details>


### [49] [Training and Evaluation of Guideline-Based Medical Reasoning in LLMs](https://arxiv.org/abs/2512.03838)
*Michael Staniek, Artem Sokolov, Stefan Riezler*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种方法，通过将医学共识指南转化为可执行的推理规则来微调LLM，使其能够遵循医学共识进行逐步推理和预测，从而提高预测准确性和解释可信度。


<details>
  <summary>更多</summary>
  
**动机:** 当前医学机器学习虽然预测准确率有所突破，但缺乏可信的解释性，无法获得医疗从业者的信任。医学领域普遍存在共识指南，这为LLM学习医学推理规则提供了机会。

**方法:** 将医学共识指南（如Sepsis-3定义）转化为语言化的推理规则实例，用于微调LLM。通过多模态设置整合时间序列预测模型的输出表示，解决临床变量稀疏和不规则采样的预测问题。

**结果:** 微调后的小模型性能优于使用显式定义进行单次学习的大模型和在包含共识定义的医学文本上训练的模型。在特定医学领域，微调模型对未见患者数据的规则推导正确性接近完美。

**结论:** 医学早期预测的主要瓶颈不是分布外泛化，而是通过预测稀疏和不规则采样的临床变量来实现时间上的泛化。多模态方法可以改善这一问题的预测结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+and+Evaluation+of+Guideline-Based+Medical+Reasoning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03838，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03838&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.

</details>


### [50] [SkillFactory: Self-Distillation For Learning Cognitive Behaviors](https://arxiv.org/abs/2512.04072)
*Zayne Sprague, Jack Lu, Manya Wadhwa, Sedrick Keh, Mengye Ren, Greg Durrett*

**主要类别:** cs.CL

**AI概要:** SkillFactory是一种在强化学习前通过监督微调让模型学习认知技能的方法，通过重新排列模型自身输出来创建训练数据，帮助模型在RL阶段更好地利用各种推理技能。


<details>
  <summary>更多</summary>
  
**动机:** 解决如何让基础语言模型学习并利用其本身不展现的认知技能（如答案验证、回溯、替代方法重试等）的问题。

**方法:** 使用SkillFactory方法进行监督微调(SFT)，通过重新排列模型自身生成的样本创建"银标"训练数据来学习认知技能，然后在强化学习(RL)阶段进一步优化。

**结果:** (1)SkillFactory SFT初始化帮助模型在RL后泛化到更难的任务变体；(2)模型确实使用了认知技能；(3)RL后的SkillFactory模型在域外任务上比RL后的基础模型更稳健。

**结论:** 在强化学习前学习的归纳偏置有助于模型学习稳健的认知技能使用，SkillFactory方法有效提升了模型的推理能力和泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SkillFactory%3A+Self-Distillation+For+Learning+Cognitive+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.04072，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04072&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.

</details>


### [51] [Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers](https://arxiv.org/abs/2512.03870)
*Hongzhan Lin, Zhiqi Bai, Xinmiao Zhang, Sen Yang, Xiang Li, Siran Yang, Yunlong Xu, Jiaheng Liu, Yongchi Zhao, Jiamang Wang, Yuchi Xu, Wenbo Su, Bo Zheng*

**主要类别:** cs.CL

**AI概要:** FusedKV通过跨层KV缓存共享技术，在保持性能的同时将Transformer解码器的KV缓存内存减少50%。该方法通过分析信息流发现顶层KV的不同分布特性，提出可学习的融合机制来优化缓存效率。


<details>
  <summary>更多</summary>
  
**动机:** Transformer解码器在处理长序列时KV缓存内存需求过高，现有跨层共享方法性能不如层内方法如GQA。需要找到既能减少内存又能保持性能的解决方案。

**方法:** 分析顶层keys和values的信息流分布，发现values主要来自底层，keys来自底层和中间层。提出FusedKV：顶层KV缓存是可学习的底层和中间层最有信息量的KV融合，直接在RoPE后操作以保留位置信息。FusedKV-Lite是简化版本，直接从底层values和中间层keys派生。

**结果:** 在332M到4B参数的LLM实验中，方法减少50%缓存内存，同时获得比标准Transformer解码器更低的验证困惑度。FusedKV-Lite在略微增加困惑度的代价下进一步减少I/O开销。

**结论:** FusedKV系列方法提供了内存高效且高性能的架构替代方案，成功解决了长序列处理中的KV缓存瓶颈问题，在内存减少和性能保持之间取得了良好平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reconstructing+KV+Caches+with+Cross-layer+Fusion+For+Enhanced+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03870，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03870&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.

</details>


### [52] [Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions](https://arxiv.org/abs/2512.03943)
*Kazi Abrab Hossain, Jannatul Somiya Mahmud, Maria Hossain Tuli, Anik Mitra, S. M. Taiabul Haque, Farig Y. Sadeque*

**主要类别:** cs.CL

**AI概要:** BRAND数据集研究显示多语言模型在宗教内容处理中存在语言差异和伊斯兰教偏见，即使对于宗教中立问题也表现出系统性偏差


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在宗教等敏感主题的偏见检测和分类中仍存在挑战，特别是多语言模型经常误传宗教信息且在宗教语境中准确性不足

**方法:** 创建BRAND双语宗教问责规范数据集（2400+条目），涵盖南亚四大宗教，使用英语和孟加拉语的三种提示类型进行测试

**结果:** 模型在英语中表现优于孟加拉语，且始终对伊斯兰教表现出偏见，即使在回答宗教中立问题时也存在这种系统性偏差

**结论:** 多语言模型在不同语言中处理相似问题时存在持续性偏见，这一发现对HCI领域宗教和灵性相关研究具有重要启示

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Lying+Only+Sinful+in+Islam%3F+Exploring+Religious+Bias+in+Multilingual+Large+Language+Models+Across+Major+Religions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03943，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03943&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.

</details>


### [53] [Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study](https://arxiv.org/abs/2512.03976)
*Lifeng Chen, Ryan Lai, Tianming Liu*

**主要类别:** cs.CL

**AI概要:** 该研究提出了一个两阶段方法（持续预训练+监督微调）来将Qwen2.5-3B模型适配到低资源藏语，显著提升了藏语语言建模和翻译性能，并首次量化分析了藏语适配的动态过程。


<details>
  <summary>更多</summary>
  
**动机:** 解决大语言模型在低资源语言（特别是藏语）适配中的挑战，包括数据稀缺和跨语言漂移问题，因为藏语具有丰富的形态学特征且代表性不足。

**方法:** 采用两阶段适配方法：1）持续预训练（CPT）建立藏语语言基础；2）监督微调（SFT）进行任务和翻译专门化。对Qwen3-4B的435层进行了层级分析。

**结果:** 困惑度显著降低（2.98→1.54），汉藏翻译质量大幅提升（BLEU: 0.046→0.261; chrF: 2.2→6.6）。适配主要集中在嵌入层和输出头，中后期MLP投影编码领域特定变换。

**结论:** CPT构建藏语语义流形，SFT以最小表示破坏实现任务对齐。研究为LLMs的藏语适配提供了首个量化探索框架，并为低资源语言的多语言基础模型扩展提供了可复现的开放框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+Large+Language+Models+to+Low-Resource+Tibetan%3A+A+Two-Stage+Continual+and+Supervised+Fine-Tuning+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03976，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03976&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.

</details>


### [54] [Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models](https://arxiv.org/abs/2512.03989)
*Taido Purason, Pavel Chizhov, Ivan P. Yamshchikov, Mark Fishel*

**主要类别:** cs.CL

**AI概要:** 本文提出两种tokenizer适应方法：继续BPE训练用于词汇扩展，以及基于叶节点的词汇剪枝，以提高tokenizer在新领域或语言中的效率和词汇利用率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的tokenizer适应方法通常在新领域文本上训练新tokenizer并追加非重叠词汇，但这种方法会导致许多词汇无法被使用或使用率低，需要更有效的词汇修改方法。

**方法:** 提出两种互补方法：1) 继续BPE训练 - 在新数据上继续BPE合并学习过程来适应预训练tokenizer；2) 基于叶节点的词汇剪枝 - 移除冗余词汇同时保持模型质量。

**结果:** 在多种语言和模型家族上的实验表明，继续BPE训练提高了tokenization效率并改善了新增词汇的利用率，词汇剪枝方法能有效减少冗余词汇。

**结论:** 这两种方法为受控词汇修改提供了实用工具，作者已将其作为开源包发布，有助于更好地将预训练语言模型迁移到新领域或语言。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Teaching+Old+Tokenizers+New+Words%3A+Efficient+Tokenizer+Adaptation+for+Pre-trained+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.03989，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03989&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.

</details>


### [55] [AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving](https://arxiv.org/abs/2512.04013)
*Ying Wang, Zhen Jin, Jiexiong Xu, Wenhai Lin, Yiquan Chen, Wenzhi Chen*

**主要类别:** cs.CL

**AI概要:** AugServe是一个高效的增强LLM推理框架，通过两阶段自适应请求调度和动态令牌批处理机制，显著提高有效吞吐量并降低延迟


<details>
  <summary>更多</summary>
  
**动机:** 现有增强LLM推理系统存在FCFS调度导致的队头阻塞问题和静态批处理限制，无法适应负载波动和硬件条件变化，降低了服务质量和有效吞吐量

**方法:** 提出两阶段自适应请求调度策略：第一阶段基于请求特征优化调度顺序，第二阶段利用运行时信息持续优化；同时动态调整令牌批处理机制以适应硬件状态和实时负载

**结果:** 实验显示AugServe相比vLLM和InferCept，有效吞吐量提高4.7-33.1倍和3.3-13.2倍，首令牌时间分别降低96.3%和95.0%

**结论:** AugServe通过智能调度和动态批处理有效解决了增强LLM推理中的延迟和吞吐量问题，显著提升了服务性能和用户体验

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AugServe%3A+Adaptive+Request+Scheduling+for+Augmented+Large+Language+Model+Inference+Serving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.04013，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04013&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.
  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.

</details>
