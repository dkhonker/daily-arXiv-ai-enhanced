<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 69]
- [cs.CL](#cs.CL) [总数: 111]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

**主要类别:** cs.AI

**AI概要:** 论文研究在四旋翼无人机系统中加入随机噪声的影响，使用扩展卡尔曼滤波进行状态估计，采用线性二次高斯控制器，并应用期望最大化算法进行参数估计，比较了离线和在线参数估计的性能。


<details>
  <summary>更多</summary>
  
**动机:** 无人机在各种应用中越来越重要，但地震等灾害会损坏基础设施，使救援人员难以到达某些区域。无人机可以协助救援，但需要处理传感器噪声对系统性能的影响。

**方法:** 在四旋翼无人机系统中加入随机噪声，使用扩展卡尔曼滤波器基于传感器噪声观测进行状态估计，基于随机微分方程系统实现线性二次高斯控制器，应用期望最大化算法进行参数估计。

**结果:** 研究展示了离线参数估计和在线参数估计的结果，发现在线参数估计的收敛值范围略大于离线参数估计。

**结论:** 在线参数估计相比离线参数估计具有稍大的收敛值范围，这为无人机系统在噪声环境下的参数估计提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+parameter+estimation+for+the+Crazyflie+quadcopter+through+an+EM+algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17009，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17009&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [2] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu, Dhari Gandhi, Himanshu Joshi, Ahmad Rezaie Mianroodi, Sedef Akinli Kocak, Dhanesh Ramachandran*

**主要类别:** cs.AI

**AI概要:** 本文评估现有可解释性方法在智能体系统中的适用性和局限性，指出这些方法无法有效分析智能体的时序动态和决策过程，并提出专门针对智能体系统开发新可解释性技术的研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 智能体系统与传统机器学习模型在架构和部署上存在根本差异，引入了独特的安全挑战（如目标错位、决策错误累积和协调风险），需要设计内置的可解释性和可追溯性机制来确保其自主行为的安全性和可问责性。

**方法:** 评估现有可解释性方法在智能体系统中的应用效果，分析其局限性，并基于智能体系统的时序动态、复合决策和上下文依赖行为特点，提出新的分析方法开发方向。

**结果:** 发现现有主要为静态模型设计的可解释性技术在应用于智能体系统时存在显著局限性，无法提供对智能体决策过程的有意义洞察。

**结论:** 需要开发专门针对智能体系统的新型可解释性技术，在智能体生命周期的各个阶段（目标形成、环境交互、结果评估）嵌入监督机制，以确保智能体AI系统的安全和可问责部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpreting+Agentic+Systems%3A+Beyond+Model+Explanations+to+System-Level+Accountability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17168，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17168&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [3] [Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction](https://arxiv.org/abs/2601.17188)
*Swapn Shah, Wlodek Zadrozny*

**主要类别:** cs.AI

**AI概要:** 该论文通过三个实验验证了Tensor Logic框架，展示了符号逻辑与神经网络的统一：使用张量收缩计算家谱传递闭包、在嵌入空间实现零样本组合推理、在知识图谱上验证矩阵组合的多跳推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 解决符号系统（可靠可解释但缺乏扩展性）与神经网络（可学习但缺乏透明度）的统一挑战，验证Domingos提出的Tensor Logic理论框架。

**方法:** 1) 使用递归Datalog规则和迭代张量收缩计算圣经家谱图的传递闭包；2) 训练具有可学习变换矩阵的神经网络实现嵌入空间推理；3) 在FB15k-237知识图谱上应用关系矩阵公式进行链接预测和组合推理。

**结果:** 1) 74次迭代发现33,945个祖先关系；2) 成功实现零样本组合推理；3) 链接预测MRR达0.3068，组合推理MRR达0.3346，证明矩阵组合可实现无需直接训练样本的多跳推理。

**结论:** Tensor Logic框架成功验证了符号推理与神经网络的数学等价性，为AI系统统一提供了原则性路径，实现了可扩展、可学习且可解释的推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Implementing+Tensor+Logic%3A+Unifying+Datalog+and+Neural+Reasoning+via+Tensor+Contraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17188，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17188&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.

</details>


### [4] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi, Tomohisa Seki, Hiromasa Ito, Toru Takiguchi, Kazuhiko Ohe, Yoshimasa Kawazoe*

**主要类别:** cs.AI

**AI概要:** 基于2亿多份临床记录开发的生成式模拟器模型，能够根据患者历史生成高保真度的未来临床轨迹，准确预测事件发生率和实验室结果


<details>
  <summary>更多</summary>
  
**动机:** 利用真实世界临床记录来模拟患者时间线，探索临床医学中的不确定性，实现个性化治疗规划和虚拟临床试验

**方法:** 开发生成式模拟器模型，以患者历史为输入，合成细粒度的真实未来轨迹，使用超过2亿份临床记录进行预训练

**结果:** 模型生成的时间线与真实患者未来数据高度匹配，事件发生率、实验室测试结果和时间动态都接近真实，未来事件概率估计准确，观察值与期望值比率接近1.0

**结论:** 研究揭示了电子健康记录中真实世界数据的潜在价值，并提出了一个可扩展的临床护理计算机模拟框架

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High-Fidelity+Longitudinal+Patient+Simulation+Using+Real-World+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17310&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [5] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu, Linglong Kong, Jian Pei*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个最小化且可校准的理论框架，用于预测多智能体系统在固定推理预算下的三种性能模式：提升、饱和和崩溃。该理论基于三个关键约束：有限上下文窗口、有损通信和智能体间的共享故障相关性，通过数学建模揭示了系统性能的相变边界和优化设计原则。


<details>
  <summary>更多</summary>
  
**动机:** 多智能体系统虽然能提高可靠性，但在固定推理预算下常常出现性能提升有限、饱和甚至崩溃的现象。现有研究缺乏能够统一解释这些现象并指导系统设计的理论框架，特别是在考虑现代智能体堆栈的实际约束条件时。

**方法:** 建立了一个理论框架，用四个关键参数建模智能体系统：计算性能缩放指数β、消息保真度曲线γ(m)、共享错误相关性ρ和上下文窗口W。通过分析二元树结构的多数聚合任务，推导了性能相变的数学条件，并给出了闭式解的计算分配规则和预算阈值。

**结果:** 理论分析揭示了系统性能存在明显的相变边界：单一标量αρ决定了弱信号是被放大到非平凡固定点还是被淹没。在放大区域，当组织指数s>β时出现预算协同效应，即系统性能优于单个最佳智能体。研究还提供了在增长和饱和阶段都保持准确的保守预测器。

**结论:** 该理论框架成功解释了多智能体系统的性能边界和优化条件，为系统设计提供了明确的指导原则。通过合成模拟验证了预测的相变边界，并证明该机制能够解释最近大规模LLM智能体系统扩展研究中报告的主要瓶颈问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Phase+Transition+for+Budgeted+Multi-Agent+Synergy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17311，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17311&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [6] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao, Hongteng Xu*

**主要类别:** cs.AI

**AI概要:** TheoremForge是一个成本效益高的形式化数学数据合成管道，通过分解为五个子任务并采用解耦提取策略，将验证成功率从8.6%提升到12.6%，每成功轨迹成本仅0.481美元，数据产出提高1.6倍。


<details>
  <summary>更多</summary>
  
**动机:** 形式化数学中智能体工作流的高成本阻碍了大规模数据合成，加剧了开源语料库的稀缺性问题。

**方法:** 将形式化过程分解为五个子任务：陈述形式化、证明生成、前提选择、证明修正和证明草图。采用解耦提取策略从全局失败轨迹中恢复有效训练信号。

**结果:** 在2000个问题的基准测试中，验证成功率达到12.6%（基线为8.6%），每成功轨迹平均成本仅0.481美元，证明生成的数据产出提高1.6倍。

**结论:** TheoremForge为训练未来专家模型构建数据飞轮提供了一个可扩展的框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TheoremForge%3A+Scaling+up+Formal+Data+Synthesis+with+Low-Budget+Agentic+Workflow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17332&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [7] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

**主要类别:** cs.AI

**AI概要:** 该论文通过公理化框架证明AGI无法获得独立于任务分布的统一定义，不存在普遍鲁棒性，泛化能力有限，且无法通过计算程序（包括自我验证）进行完全认证。


<details>
  <summary>更多</summary>
  
**动机:** 研究人工智能通用智能（AGI）是否具有支持存在性、鲁棒性或自我验证绝对主张的连贯理论定义。

**方法:** 将AGI公理化定义为基于任务族、任务分布、性能函数和显式资源预算的分布性、资源受限语义谓词，并在此框架下推导四类理论结果。

**结果:** 1. 通用性具有关系性本质，无分布无关的AGI概念
2. 任意小的任务分布扰动可通过悬崖集使AGI属性失效
3. 有限资源下无法实现跨任务族的无界泛化
4. AGI无法通过任何可计算程序（包括自我验证）进行完全认证

**结论:** 强分布无关的AGI主张在没有显式形式化索引的情况下是未定义的，AI的实证进展并不意味可实现自我认证的通用智能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Relativity+of+AGI%3A+Distributional+Axioms%2C+Fragility%2C+and+Undecidability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17335，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17335&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [8] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu, Haomei Xu, Hongkai Liu, Zhiying Deng, Ruixuan Li, Heng Huang, Yee Whye Teh, Wee Sun Lee*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个新的模型编辑特异性评估协议，解决了现有评估方法的不足，能够更敏感地衡量LLM知识更新的特异性表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有模型编辑的特异性评估协议存在不足，无法有效平衡编辑效果和知识保存，且现有指标与特异性正则化器强度相关性弱，缺乏区分不同方法性能的敏感性。

**方法:** 提出了一种新的评估协议，消除了开放式LLM与确定答案假设的冲突，避免了查询无关的流畅性偏差，并可在近乎连续的空间中平滑调整评估严格度。

**结果:** 实验表明，新协议导出的指标对特异性正则化器强度变化更敏感，与正则化器强度强相关，能够更精细地区分不同方法的知识保存能力。

**结论:** 提出的评估协议解决了现有特异性评估的根本问题，为模型编辑领域提供了更可靠和敏感的特异性评估框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+We+Evaluating+the+Edit+Locality+of+LLM+Model+Editing+Properly%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17343，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17343&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [9] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu, Changyong Qi, Tong Liu, Bohao Zhang, Anna He, Bingqian Jiang, Longwei Zheng, Xiaoqing Gu*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一个基于多智能体协作的个性化学习路径规划框架MALPP，利用大语言模型驱动的智能代理，通过角色分工和规则机制为高等教育提供透明、可解释的适应性学习路径。


<details>
  <summary>更多</summary>
  
**动机:** 现有学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，需要一种更智能的个性化学习解决方案。

**方法:** 提出多智能体学习路径规划框架(MALPP)，包含三个任务特定的代理：学习者分析代理、路径规划代理和反思代理，通过结构化提示和预定义规则协作，基于认知负荷理论和最近发展区理论设计。

**结果:** 在MOOCCubeX数据集上使用7个大语言模型的实验显示，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型。消融研究验证了协作机制和理论约束的有效性。

**结论:** 该研究为教育领域可信赖、可解释的AI发展做出贡献，展示了基于大语言模型的以学习者为中心的自适应教学的可行方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Learning+Path+Planning+via+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17346，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17346&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [10] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda, Sourabh Singh Yadav, Palkesh Malviya*

**主要类别:** cs.AI

**AI概要:** 该研究系统分析了视觉语言模型在描述残障人士图像时从事实描述转向无根据推论的'解释偏移'现象，发现引入残障上下文会降低解释保真度，并揭示了在种族和性别维度上的放大效应，最后提出了针对性提示和偏好微调的有效改进方法。


<details>
  <summary>更多</summary>
  
**动机:** 视觉语言模型越来越多地应用于社会敏感领域，但其在残障方面的行为尚未得到充分探索。研究者关注模型如何从基于视觉证据的事实描述转向包含无支持推论的'解释偏移'问题。

**方法:** 研究引入基于中性提示(NP)和残障情境提示(DP)的基准测试，在零样本设置下评估15个先进的开源和闭源VLM模型，涵盖9个残障类别。评估框架结合标准文本指标和LLM-as-judge协议，由有残障生活经验的标注者验证。

**结果:** 研究发现引入残障上下文会一致性地降低解释保真度，导致以推测性推论、叙事扩展、情感贬损和缺陷导向框架为特征的解释偏移。这些效应在种族和性别维度上进一步放大。

**结论:** 针对性提示和偏好微调能有效提高解释保真度并显著减少解释偏移，为解决VLMs在残障描述中的偏见问题提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Auditing+Disability+Representation+in+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17348，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17348&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [11] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang, Yuqi Ding, Yanmei Gu, Changkai Song, Zhengkai Yang, Guoping Du, Junbo Zhao, Haobo Wang*

**主要类别:** cs.AI

**AI概要:** 研究探索大语言模型是否表现出从传统逻辑向现代逻辑的演进，通过存在性导入测试发现模型规模扩展、思维链和基础模型对此转变有重要影响。


<details>
  <summary>更多</summary>
  
**动机:** 受人类逻辑从直觉推理向形式系统演进的启发，探索大语言模型是否表现出类似的逻辑框架演进过程。

**方法:** 使用存在性导入作为探针，在传统和现代逻辑下评估三段论推理，通过在新构建的三段论数据集上测试SOTA大语言模型进行广泛实验。

**结果:** 发现三个关键因素：(i)模型规模扩展促进向现代逻辑的转变；(ii)思维链作为参数扩展之外的高效加速器；(iii)基础模型决定此转变的容易程度和稳定性。

**结论:** 大语言模型在逻辑推理方面确实表现出从传统到现代逻辑的演进特征，模型规模、思维链技术和基础模型架构是影响这种转变的关键因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Syllogistic+Probe%3A+Tracing+the+Evolution+of+Logic+Reasoning+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17426&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [12] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst, Tawab Safi, Joseph Edell, Vashisht Ganesh, Karime Maamari*

**主要类别:** cs.AI

**AI概要:** Lattice框架通过两阶段自构建和持续改进机制，为对话AI系统提供动态防护栏，在ProsocialDialog数据集上达到91% F1值，显著优于现有方法


<details>
  <summary>更多</summary>
  
**动机:** 现有防护栏方法使用静态规则，无法适应新威胁和部署环境，需要能够自我构建和持续改进的动态防护方案

**方法:** 两阶段框架：构建阶段通过迭代模拟和优化从标注样本创建初始防护栏；持续改进阶段通过风险评估、对抗测试和整合自主调整已部署防护栏

**结果:** 在ProsocialDialog数据集上达到91% F1值，比关键词基线高43个百分点，比LlamaGuard高25个百分点，比NeMo高4个百分点；持续改进阶段通过闭环优化在跨域数据上实现7个百分点F1提升

**结论:** 研究表明通过迭代优化可以自构建有效的防护栏，Lattice框架展示了动态自适应防护栏的可行性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lattice%3A+Generative+Guardrails+for+Conversational+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17481&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [13] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy, Nitin Saksena, Srivenkateswara Reddy Sankiti, Nachiappan Chockalingam, Aswathnarayan Muthukrishnan Kirubakaran, Shiva Kumar Reddy Carimireddy, Durgaraman Maruthavanan*

**主要类别:** cs.AI

**AI概要:** 论文提出认知平台工程新范式，通过四层架构整合数据收集、智能推理、策略编排和人工经验，实现云原生系统的自主运维和智能调整。


<details>
  <summary>更多</summary>
  
**动机:** 传统DevOps自动化方法难以应对云原生系统的规模和动态性，导致响应式运维、修复延迟和对人工经验的依赖。

**方法:** 提出四层参考架构：数据收集层、智能推理层、策略驱动编排层和人工经验层，构建连续反馈循环。原型使用Kubernetes、Terraform、Open Policy Agent和基于ML的异常检测技术。

**结果:** 原型实现显示在平均修复时间、资源效率和合规性方面有显著改进，证明将智能嵌入平台运维可实现弹性、自调整和意图对齐的云环境。

**结论:** 认知平台工程为云平台运维提供了新方向，未来研究方向包括强化学习、可解释治理和可持续自管理云生态系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cognitive+Platform+Engineering+for+Autonomous+Cloud+Operations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17542&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [14] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam, Monu Verma, Mohamed Abdel-Mottaleb*

**主要类别:** cs.AI

**AI概要:** JaxARC是一个基于JAX的高性能强化学习环境，用于ARC推理任务，相比Gymnasium实现38-5439倍的速度提升，支持大规模并行计算。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Gymnasium-based RL环境在处理ARC推理任务时存在计算瓶颈，限制了实验规模和研究效率。

**方法:** 采用JAX实现功能化、无状态架构，支持大规模并行计算，提供灵活的动作空间、可组合包装器和配置驱动的可复现性。

**结果:** 在相同批量大小下实现38-5,439倍速度提升，峰值吞吐量达到7.9亿步/秒，支持多种ARC数据集。

**结论:** JaxARC解决了现有环境的计算瓶颈问题，使得之前计算不可行的大规模RL研究成为可能，为ARC推理研究提供了高效工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JaxARC%3A+A+High-Performance+JAX-based+Environment+for+Abstraction+and+Reasoning+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17564&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [15] [Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design](https://arxiv.org/abs/2601.17587)
*Azza Fadhel, Nathaniel W. Zuckschwerdt, Aryan Deshwal, Susmita Bose, Amit Bandyopadhyay, Jana Doppa*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种结合AI驱动自适应实验设计和领域知识的智能方法，用于高效发现金属合金增材制造的可行参数配置，相比传统试错方法大幅减少了时间和资源消耗。


<details>
  <summary>更多</summary>
  
**动机:** 金属合金增材制造的参数配置是一个复杂问题，传统试错方法效率低下且资源消耗大，需要一种更智能的方法来优化参数发现过程。

**方法:** 构建基于过去实验的替代模型，智能选择小批量输入配置进行迭代验证，结合AI自适应实验设计和领域专业知识。

**结果:** 在三个月内成功获得多个无缺陷输出，大幅缩短了结果时间并减少了资源消耗，首次在易得的红外激光平台上实现了高质量GRCop-42制造。

**结论:** 该方法为航空航天应用中的关键合金制造提供了民主化途径，实现了成本效益高、分散化的生产，具有重要的实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discovery+of+Feasible+3D+Printing+Configurations+for+Metal+Alloys+via+AI-driven+Adaptive+Experimental+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17587，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17587&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.

</details>


### [16] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma, Shrikanth Narayanan*

**主要类别:** cs.AI

**AI概要:** 论文认为智能需要基于现实世界的接地(grounding)而非物理具身化(embodiment)，提出了智能的四个属性定义，并通过思想实验论证非具身但接地的LLM智能体的可能性


<details>
  <summary>更多</summary>
  
**动机:** 回应LLM进展引发的关于具身化对智能必要性的科学辩论，重新审视智能与具身化的关系

**方法:** 提出智能的四属性定义（动机、预测能力、因果理解、经验学习），通过理论分析和思想实验论证非具身但接地的智能体可实现这些属性

**结果:** 论证了接地而非具身化是智能的必要条件，展示了非具身LLM智能体在数字环境中实现智能的可能性

**结论:** 智能需要的是与现实世界的接地连接，而非物理具身化本身，这为理解LLM等非具身系统的智能提供了新的理论框架

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Intelligence+Requires+Grounding+But+Not+Embodiment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17588，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17588&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [17] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang, Liting Huang, Guanghao Wu, Preslav Nakov, Heng Ji, Usman Naseem*

**主要类别:** cs.AI

**AI概要:** Health-ORSC-Bench是首个大规模医疗领域基准测试，用于系统评估大语言模型在医疗查询中的过度拒绝和安全完成能力，揭示了现有模型在安全性和实用性之间的平衡难题。


<details>
  <summary>更多</summary>
  
**动机:** 当前医疗大语言模型的安全对齐主要依赖二元拒绝边界，导致对良性查询的过度拒绝或对有害查询的不安全合规，缺乏对安全完成能力的评估标准。

**方法:** 构建包含31,920个良性边界提示的大规模基准测试框架，涵盖7个医疗类别，使用自动化流水线和人工验证来测试不同意图模糊程度的模型表现。

**结果:** 评估30个先进LLM显示：安全优化模型对"困难"良性提示拒绝率高达80%，领域特定模型则常常牺牲安全性；大型前沿模型表现出"安全悲观主义"和更高过度拒绝率。

**结论:** 当前LLM难以平衡拒绝和合规，模型家族和规模显著影响校准效果，该基准为下一代医疗AI助手提供了精确的安全完成能力评估标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Health-ORSC-Bench%3A+A+Benchmark+for+Measuring+Over-Refusal+and+Safety+Completion+in+Health+Context，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17642，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17642&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [18] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An, Wan Du*

**主要类别:** cs.AI

**AI概要:** 该论文提出了DIML框架，通过观察自利学习智能体的战略交互轨迹来逆向推断未知的激励生成机制，包括非结构化机制（如神经网络映射）。与传统的逆向博弈论和多智能体逆向强化学习不同，该方法能从行为数据中恢复支付差异，支持反事实预测，并在各种环境中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 研究如何从观察到的战略交互数据中逆向学习激励生成机制，特别是处理非结构化机制（如神经网络），这在传统方法中难以处理。

**方法:** 提出DIML框架，基于似然方法，通过多智能体学习动态模型进行微分，利用候选机制生成反事实支付来预测观察到的行为。

**结果:** DIML能够可靠地恢复可识别的激励差异，支持反事实预测，在小环境中性能媲美枚举方法，在大规模环境中也能有效扩展。

**结论:** DIML为逆向机制学习提供了有效的框架，能够处理非结构化机制，并在各种实际应用中表现出良好的性能和可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DIML%3A+Differentiable+Inverse+Mechanism+Learning+from+Behaviors+of+Multi-Agent+Learning+Trajectories，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17678，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17678&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [19] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan, Qi Zhu, Sullam Jeoung, Yueyan Chen, Yunfei Bai, Shuai Wang, Vassilis Ioannidis, Huzefa Rangwala*

**主要类别:** cs.AI

**AI概要:** SQL-Trail是一个多轮强化学习框架，通过迭代执行反馈来改进Text-to-SQL生成，在BIRD-SQL等基准测试中达到新的最先进水平，小模型性能超越大模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有单次生成范式与人类专家在Text-to-SQL任务上存在明显差距，缺乏迭代推理、模式探索和错误修正能力。

**方法:** 采用多轮强化学习代理框架，通过数据库交互和执行反馈迭代优化查询；包含自适应轮次预算分配机制和复合奖励面板。

**结果:** 在基准测试中达到新的SOTA，数据效率比之前单次RL方法高18倍；7B和14B模型平均超越大型专有系统5%。

**结论:** 交互式代理工作流程对稳健的Text-to-SQL生成非常有效，多轮交互方法能显著提升模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SQL-Trail%3A+Multi-Turn+Reinforcement+Learning+with+Interleaved+Feedback+for+Text-to-SQL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17699&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [20] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang, Mingzhi Hu, Hoang Anh Duy Le, Fariha Kabir Torsha, Zhimeng Jiang, Minh Khai Bui, Chia-Yuan Chang, Yu-Neng Chuang, Zhen Xiong, Ying Lin, Guanchu Wang, Na Zou*

**主要类别:** cs.AI

**AI概要:** 本文提出了LLM数据审计框架，系统评估多模态合成数据的质量和可信度，发现当前评估方法存在重大缺陷并提出改进建议。


<details>
  <summary>更多</summary>
  
**动机:** LLM生成合成数据时面临质量保证挑战，现有研究主要关注生成方法而忽视数据质量评估，且缺乏跨模态的统一视角。

**方法:** 提出LLM Data Auditor框架，涵盖六种数据模态，从质量和可信度两个维度系统分类内在评估指标，从下游任务评估转向数据本身属性评估。

**结果:** 分析了各模态代表性生成方法的实验评估，发现当前评估实践存在重大缺陷。

**结论:** 基于研究发现为社区提供了改进数据生成评估的具体建议，并概述了合成数据在不同模态中实际应用的方法论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+LLM+Data+Auditor%3A+A+Metric-oriented+Survey+on+Quality+and+Trustworthiness+in+Evaluating+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17717，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17717&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [21] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo, Yu Bai, Dapeng Sun, Yuqian Shi, Yukai Miao, Li Chen, Dan Li*

**主要类别:** cs.AI

**AI概要:** EntWorld是一个针对企业级系统的多模态大语言模型基准测试，包含1756个任务，覆盖CRM、ITIL、ERP等六大企业领域，采用基于数据库模式的任务生成和SQL验证机制，揭示当前SOTA模型在企业环境中的性能差距（47.61%成功率vs人类表现）


<details>
  <summary>更多</summary>
  
**动机:** 现有基准主要针对消费级场景，无法捕捉企业工作流的复杂性和严谨性，企业系统具有高密度UI、严格业务逻辑约束和精确状态一致性要求等独特挑战

**方法:** 采用模式驱动的任务生成框架，直接从底层数据库模式逆向工程业务逻辑，合成真实的长周期工作流；提出基于SQL的确定性验证机制，用严格状态转换验证替代模糊视觉匹配

**结果:** 实验显示最先进模型（如GPT-4.1）在EntWorld上仅达到47.61%的成功率，显著低于人类表现，表明当前智能体在企业能力方面存在明显差距

**结论:** 企业级数字代理需要专门开发，EntWorld为下一代企业就绪数字代理的开发和评估提供了严谨的测试平台

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EntWorld%3A+A+Holistic+Environment+and+Benchmark+for+Verifiable+Enterprise+GUI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17722，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17722&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [22] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim, Geon Lee, Juyeon Kim, Dongwon Choi, Shinhwan Kang, Kijung Shin*

**主要类别:** cs.AI

**AI概要:** ReFuGe是一个基于大语言模型代理的框架，用于自动生成关系数据库中的预测性特征，通过三个专门代理（模式选择、特征生成、特征过滤）的迭代循环来提升预测性能


<details>
  <summary>更多</summary>
  
**动机:** 关系数据库中的预测任务需要生成信息丰富的关系特征，但面临复杂模式推理和组合爆炸的特征空间挑战，且缺乏明确的监督信号

**方法:** 提出ReFuGe框架，包含三个LLM代理：模式选择代理识别相关表和列，特征生成代理生成候选特征，特征过滤代理通过推理和验证进行特征筛选，在迭代反馈循环中运行直至性能收敛

**结果:** 在关系数据库基准测试中，ReFuGe显著提升了各种预测任务的性能表现

**结论:** ReFuGe框架通过智能代理系统有效解决了关系数据库特征生成的挑战，为RDB预测任务提供了强大的自动化特征工程解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReFuGe%3A+Feature+Generation+for+Prediction+Tasks+on+Relational+Databases+with+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17735，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17735&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [23] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

**主要类别:** cs.AI

**AI概要:** Faramesh是一个协议无关的执行控制平面，通过不可绕过的行动授权边界(AAB)为自主代理系统提供执行时授权，确保在行动改变现实前进行确定性审批。


<details>
  <summary>更多</summary>
  
**动机:** 当前自主代理系统在触发现实世界副作用(如部署基础设施、修改数据库等)时缺乏强制性的执行检查点，无法在行动执行前进行确定性审批。

**方法:** 系统将代理意图规范化为规范行动表示(CAR)，根据策略和状态确定性评估行动，并发出执行前必须验证的决策结果(PERMIT/DEFER/DENY)。支持多代理、多租户部署，独立于传输协议。

**结果:** Faramesh提供了基于规范行动哈希的决策中心、仅追加溯源日志，实现了可审计性、验证和确定性重放，无需重新运行代理推理。

**结论:** 该系统为自主执行提供了可强制执行、可预测的治理机制，避免了与编排层的隐藏耦合或仅观察性方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Faramesh%3A+A+Protocol-Agnostic+Execution+Control+Plane+for+Autonomous+Agent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17744，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17744&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [24] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta, Xiaobin Wu, Xun Liu, Jiaqi He*

**主要类别:** cs.AI

**AI概要:** 提出融合深度学习和传统机器学习的混合集成框架，通过集成投票机制结合CNN、LSTM、KNN和XGB算法，在心血管疾病预测上取得优异性能，准确率分别达到82.30%和97.10%。


<details>
  <summary>更多</summary>
  
**动机:** 心血管疾病是全球主要死亡原因，传统预测模型在异构数据集和复杂生理模式上泛化能力不足，需要智能化的数据驱动诊断工具。

**方法:** 开发混合集成框架，整合CNN和LSTM深度学习架构与KNN和XGB传统机器学习算法，采用集成投票机制。

**结果:** 在两个Kaggle公开数据集上测试，模型性能优异：Dataset I准确率82.30%，Dataset II准确率97.10%，在精确率、召回率和F1分数上均有稳定提升。

**结论:** 混合AI框架在心血管疾病预测中表现出鲁棒性和临床潜力，支持早期干预，同时符合联合国可持续发展目标3，通过创新数据驱动医疗解决方案促进非传染性疾病的早期诊断和管理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HyCARD-Net%3A+A+Synergistic+Hybrid+Intelligence+Framework+for+Cardiovascular+Disease+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17767，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17767&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [25] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su, Kunzhao Xu, Yanjie Gao, Fan Yang, Cheng Li, Mao Yang, Tianyin Xu*

**主要类别:** cs.AI

**AI概要:** NSVIF是一个神经符号框架，用于验证LLM输出是否遵循指令，通过将指令建模为约束来解决约束满足问题，显著优于基于LLM的方法并提供可解释反馈。


<details>
  <summary>更多</summary>
  
**动机:** LLM不总是遵循指令，且违规行为难以观察或检查，在基于LLM的智能体工作流中可能导致任务失败和系统事故。

**方法:** 将指令遵循验证建模为约束满足问题，将用户指令建模为逻辑和语义约束，通过统一求解器协调逻辑推理和语义分析。

**结果:** NSVIF显著优于基于LLM的方法，提供可解释反馈，且其反馈有助于提高LLM的指令遵循能力而无需后训练。

**结论:** NSVIF是一个通用、通用的验证器，不依赖于特定指令或LLM，能有效验证LLM输出是否遵循指令，并提升LLM性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neuro-Symbolic+Verification+on+Instruction+Following+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17789，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17789&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [26] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma, Guannan Lai, Han-Jia Ye*

**主要类别:** cs.AI

**AI概要:** MMR-Bench是一个多模态大语言模型路由基准测试平台，解决了不同MLLM模型在计算成本和性能上的差异问题，通过智能路由在保持精度的同时显著降低计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型架构、对齐策略和效率存在异质性，单一模型无法在所有任务上都表现最优。实际部署中，使用单一模型处理所有查询要么在简单实例上过度配置计算资源，要么在复杂任务上牺牲准确性。

**方法:** 提出了MMR-Bench统一基准测试框架，包含：(1)具有模态感知输入和可变计算预算的受控环境；(2)涵盖OCR、通用VQA和多模态数学推理的广泛视觉语言任务套件；(3)强单模型参考、理论上限和代表性路由策略。

**结果:** 实验表明，融入多模态信号可提高路由质量，路由系统能以最强单模型约33%的成本超越其准确性。训练的策略能够零样本泛化到新数据集和纯文本基准测试。

**结论:** MMR-Bench为研究自适应多模态模型选择和高效MLLM部署奠定了基础，证明了智能路由在多模态环境中的有效性和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMR-Bench%3A+A+Comprehensive+Benchmark+for+Multimodal+LLM+Routing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17814&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [27] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang, Xihan Bian, Jiayin Tang*

**主要类别:** cs.AI

**AI概要:** RegGuard是一个工业级AI助手，用于自动化解析异构监管文本并与企业内部政策对齐，通过HiSACC和ReLACE技术提升检索和生成质量，显著减少幻觉风险并提高回答相关性。


<details>
  <summary>更多</summary>
  
**动机:** 跨国制药公司面临日益频繁和复杂的监管更新，合规团队需要手动解读不同司法管辖区、格式和机构的规则，成本高且易出错。

**方法:** 系统通过安全管道摄入异构文档源，采用HiSACC技术进行语义分段保持一致性，使用ReLACE领域自适应交叉编码器改进排名相关性。

**结果:** 企业评估显示RegGuard在相关性、基于事实性和上下文聚焦方面提升回答质量，显著降低幻觉风险。

**结论:** RegGuard系统架构具有可审计性和可追溯性，适用于任何有严格合规需求的领域，能够快速响应不断变化的文档源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RegGuard%3A+AI-Powered+Retrieval-Enhanced+Assistant+for+Pharmaceutical+Regulatory+Compliance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17826，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17826&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [28] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma, Yang Zhou, Rick Siow Mong Goh, Yong Liu*

**主要类别:** cs.AI

**AI概要:** IGFT是一种新颖的医疗对话AI训练方法，通过信息增益奖励和在线强化学习，使模型能够通过模拟患者对话自主学习有效的问诊策略，无需预先收集人类对话数据。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法依赖昂贵的专家标注对话或静态数据集，无法让模型通过探索发现有效的提问策略。医疗对话AI需要能够进行多轮对话收集全面的现病史信息。

**方法:** 结合在线组相对策略优化(GRPO)和信息论奖励，使用信息增益奖励函数跟踪临床实体的揭示情况，结合GPT-4o-mini的质量评估计算问题奖励。使用LoRA微调Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B模型。

**结果:** DeepSeek-R1-Distill-Qwen-7B在Avey数据上F1得分0.408(提升10.9%)，在MIMIC数据上0.289(提升12.9%)；Llama-3.1-8B-Instruct分别达到0.384和0.336。两个模型在MIMIC上都优于OpenAI模型，并超越了医疗领域专用基线模型。

**结论:** IGFT框架成功证明了通过信息增益奖励和在线强化学习，模型能够自主学习有效的多轮医疗问诊策略，在现病史收集任务上表现出色，且具有良好的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aligning+Medical+Conversational+AI+through+Online+Reinforcement+Learning+with+Information-Theoretic+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17828&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [29] [When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents](https://arxiv.org/abs/2601.17887)
*Jiahe Guo, Xiangran Guo, Yulin Hu, Zimo Long, Xingyu Sui, Xuda Zhi, Yongbo Huang, Hao He, Weixiang Zhao, Yanyan Zhao, Bing Qin*

**主要类别:** cs.AI

**AI概要:** 论文揭示了LLM智能体中的意图合法化安全问题：良性个人记忆会偏见意图推断，导致模型将有害查询合法化，攻击成功率提升15.8%-243.7%。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注个性化代理的实用性和用户体验，将记忆视为中性组件，忽视了其安全影响，特别是意图合法化这一未被充分探索的安全失效模式。

**方法:** 引入PS-Bench基准测试来识别和量化意图合法化现象；通过多个记忆增强代理框架和基础LLM进行实验；从内部表示空间提供机制性证据；提出轻量级检测-反思方法。

**结果:** 个性化使攻击成功率相比无状态基线提高15.8%-243.7%；提出的检测-反思方法有效减少了安全性能下降。

**结论:** 这是对意图合法化作为安全失效模式的首次系统探索，强调了在长期个性化背景下评估安全性的重要性，良性个性化可能自然产生安全风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Personalization+Legitimizes+Risks%3A+Uncovering+Safety+Vulnerabilities+in+Personalized+Dialogue+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17887&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.

</details>


### [30] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu, Yinhe Long, Zhenya Huang, Enhong Chen*

**主要类别:** cs.AI

**AI概要:** UniCog是一个通过潜在思维空间分析大语言模型认知的统一框架，将密集模型激活编码为稀疏解耦的潜在维度，揭示了LLM认知的帕累托原则，并利用潜在激活异常检测推理失败，最终通过潜在信息候选优先策略提升推理性能7.5%。


<details>
  <summary>更多</summary>
  
**动机:** 现有可解释性方法在解释LLM推理过程中如何运用认知能力方面存在局限，需要新的分析框架来理解LLM与人类根本不同的认知过程。

**方法:** 提出UniCog框架，构建潜在变量模型，将密集模型激活编码为稀疏解耦的潜在维度，对包括DeepSeek-V3.2和GPT-4o在内的六个先进LLM进行广泛分析。

**结果:** 发现LLM认知的帕累托原则（共享推理核心+能力特定特征），推理失败表现为潜在激活异常，潜在信息候选优先策略在挑战性基准上提升推理性能达7.5%。

**结论:** UniCog为LLM分析开辟了新范式，提供了基于认知的推理动态视角，代码已开源供进一步研究使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UniCog%3A+Uncovering+Cognitive+Abilities+of+LLMs+through+Latent+Mind+Space+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17897，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17897&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [31] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha, Rohan Arora, Bhavya, Noah Zheutlin, Paulina Toro Isaza, Laura Shwartz, Yu Deng, Daby Sow, Ruchi Mahindru, Ruchir Puri*

**主要类别:** cs.AI

**AI概要:** EoG框架通过将调查任务分解为基于依赖图的溯因推理，使用LLM进行局部证据挖掘和标注，由确定性控制器管理遍历和信念传播，显著提高了在开放调查任务中的准确性和一致性。


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM智能体在开放调查任务中存在局限性：上下文窗口有限导致关键证据可能被丢弃，ReAct风格智能体对探索顺序敏感且结果不稳定，缺乏自主验证机制和信念管理能力。

**方法:** 提出EoG框架：将调查建模为依赖图上的溯因推理，LLM负责有界的局部证据挖掘和标注（原因vs症状），确定性控制器管理图遍历、状态维护和信念传播，计算最小解释边界。

**结果:** 在ITBench诊断任务上，EoG相比ReAct基线显著提高了准确性和运行一致性，平均在Majority-at-k实体F1指标上获得7倍提升。

**结论:** 通过解耦语义推理与控制职责，EoG框架有效解决了开放调查任务中的证据挖掘和信念管理问题，为LLM智能体在复杂推理场景中的应用提供了更可靠的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+Locally%2C+Explain+Globally%3A+Graph-Guided+LLM+Investigations+via+Local+Reasoning+and+Belief+Propagation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17915，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17915&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [32] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen, Audrey Wang, Stanley Yin, Hanyang Jiang, Dong Zhang*

**主要类别:** cs.AI

**AI概要:** 这是一篇关于自主实验室(SDL)的综述论文，重点讨论了在真实实验室环境中AI代理面临的挑战和解决方案，包括实验选择、协议优化、工具协调等方法，并提出了能力驱动的分类体系和评估标准。


<details>
  <summary>更多</summary>
  
**动机:** 自主实验室需要在昂贵操作、噪声延迟反馈、严格约束和非平稳性等复杂条件下实现闭环实验，这为智能代理AI提供了极具挑战性的测试平台，需要系统性的AI方法来解决这些实际问题。

**方法:** 论文采用软物质作为代表性场景，将SDL自主性构建为代理环境交互问题，综述了贝叶斯优化、主动学习、规划与强化学习等方法家族，重点研究可验证和可溯源的策略。

**结果:** 提出了能力驱动的分类体系(决策视野、不确定性建模、动作参数化等)，综合了基准任务模板和评估指标，重点关注成本感知性能、鲁棒性、约束违反行为和可重现性。

**结论:** 从已部署的SDL中提炼经验教训，指出了在多模态表示、校准不确定性、安全探索和共享基准基础设施等方面的开放挑战，为未来自主实验室的发展提供了方向性指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+AI+for+Self-Driving+Laboratories+in+Soft+Matter%3A+Taxonomy%2C+Benchmarks%2Cand+Open+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17920&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [33] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

**主要类别:** cs.AI

**AI概要:** 论文提出使用技能图表示和分层课程训练的方法，在复杂实时控制环境中实现终身学习代理，通过分解控制任务为五个可重用技能并选择性微调，有效提高样本效率和适应能力。


<details>
  <summary>更多</summary>
  
**动机:** 解决终身学习代理在不重新训练或覆盖先前学习行为的情况下扩展能力的问题，特别是在具有挑战性的实时控制环境（黑暗之魂III）中。

**方法:** 将战斗表示为有向技能图，采用分层课程训练方法，将控制任务分解为五个可重用技能：相机控制、目标锁定、移动、闪避和治疗-攻击决策策略，每个技能针对特定职责进行优化。

**结果:** 通过技能分解提高了样本效率，减少了单个策略的负担。当环境从第一阶段切换到第二阶段时，只需调整技能子集，上游技能保持可转移性。仅对两个技能进行针对性微调就能在有限交互预算下快速恢复性能。

**结论:** 技能图课程与选择性微调相结合，为在复杂实时环境中构建不断进化的持续学习代理提供了一条实用途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Transferable+Skills+in+Action+RPGs+via+Directed+Skill+Graphs+and+Selective+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17923&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [34] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang, Hung-Fu Chang, Po-An Chen*

**主要类别:** cs.AI

**AI概要:** 本研究提出SSEV和ReCAPAgent-SQL两个Text-to-SQL框架，通过自优化和智能体协作技术，在无需真实数据的情况下实现高精度SQL生成，显著提升了企业级数据库查询的准确性和实用性。


<details>
  <summary>更多</summary>
  
**动机:** Text-to-SQL技术虽然降低了数据分析门槛，但仍面临用户查询歧义、模式链接复杂、SQL方言泛化能力有限以及需要领域知识理解等挑战，需要开发更强大的解决方案来应对企业级数据库的复杂性。

**方法:** 提出了SSEV管道（基于PET-SQL的单智能体自优化集成投票）和ReCAPAgent-SQL框架（基于精化-批判-行动-规划的多智能体SQL框架），集成自优化、加权多数投票及其随机变体，并采用多个专业智能体进行协作。

**结果:** SSEV在多个基准测试中表现优异：Spider 1.0-Dev执行准确率85.5%，Spider 1.0-Test 86.4%，BIRD-Dev 66.3%。ReCAPAgent-SQL在Spider 2.0-Lite前100个查询中达到31%的执行准确率，在企业场景处理方面有显著改进。

**结论:** 该研究为实际环境中部署可扩展的Text-to-SQL系统提供了有效解决方案，支持以更低成本和更高效率进行数据驱动决策，推动了Text-to-SQL技术在企业级应用中的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Based+SQL+Generation%3A+Prompting%2C+Self-Refinement%2C+and+Adaptive+Weighted+Majority+Voting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17942&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [35] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu, Lyuhao Chen, Yunze Xiao, Weihao Xuan, Carlos Busso, Mona Diab*

**主要类别:** cs.AI

**AI概要:** Sentipolis是一个情感状态智能体框架，通过PAD情感表示、双速情感动态和情感-记忆耦合解决了LLM智能体在社交模拟中的情感遗忘和长期连续性不足问题，显著提升了情感行为真实性和连续性。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM智能体在社交模拟中将情感视为瞬时线索，导致情感遗忘和长期情感连续性薄弱，需要建立更真实的情感状态机制。

**方法:** 提出Sentipolis框架，整合了连续愉悦-唤醒-支配(PAD)情感表示、双速情感动态机制以及情感与记忆的耦合系统。

**结果:** 在多个基础模型和评估者的数千次交互中，该框架提升了情感基础行为、沟通能力和情感连续性，但效果因模型容量而异，大模型可信度提升而小模型可能下降。

**结论:** 该框架成功模拟了人类情感驱动行为与社会规范遵循之间的张力，网络级诊断显示形成了互惠、适度聚类且时间稳定的关系结构，支持研究累积社会动态如联盟形成和关系渐变。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sentipolis%3A+Emotion-Aware+Agents+for+Social+Simulations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18027，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18027&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [36] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari, Paul Ulrich Nikolaus Rust, Duncan Eddy, Robbie Fraser, Nina Vasan, Darja Djordjevic, Akanksha Dadlani, Max Lamparth, Eugenia Kim, Mykel Kochenderfer*

**主要类别:** cs.AI

**AI概要:** 研究表明，在心理健康领域，即使经过专业培训的精神科医生在评估AI生成回复时也存在显著分歧，尤其是在安全关键问题上，专家共识不能作为可靠的地面真值。


<details>
  <summary>更多</summary>
  
**动机:** 验证人类反馈学习(LHF)中专家判断能否作为AI系统训练和评估的有效基准，特别是在心理健康这种高安全风险的领域。

**方法:** 三位认证精神科医生使用校准的评分标准独立评估LLM生成的回复，通过量化评分者间信度和定性访谈分析分歧原因。

**结果:** 评分者间信度极低(ICC 0.087-0.295)，自杀和自伤类回复分歧最大，分歧是系统性的而非随机误差，反映了不同的临床框架取向。

**结论:** 专家分歧是原则性的社会技术现象，建议从基于共识的聚合转向能够保留和学习专家分歧的对齐方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Expert+Evaluation+and+the+Limits+of+Human+Feedback+in+Mental+Health+AI+Safety+Testing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18061&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [37] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin, Ren-Hao Deng, Yao-Ting Hsieh, En-Ming Huang, Shih-Hao Hung*

**主要类别:** cs.AI

**AI概要:** EvolVE是一个针对Verilog芯片设计的自动化框架，通过蒙特卡洛树搜索和引导式优化策略，在功能正确性和性能优化方面达到业界领先水平，在多个基准测试中显著超越现有方法。


<details>
  <summary>更多</summary>
  
**动机:** Verilog设计过程需要大量人工和专业知识，现有大语言模型因训练数据有限和顺序推理能力不足，无法有效处理硬件系统的形式化逻辑和并发特性。

**方法:** 提出EvolVE框架，分析多种进化策略：蒙特卡洛树搜索(MCTS)用于最大化功能正确性，引导式优化(IGR)用于性能优化，并结合结构化测试平台生成(STG)加速进化过程。

**结果:** 在VerilogEval v2上达到98.1%，RTLLM v2上达到92%。在工业级IC-RTL基准测试中，相比竞赛参与者的参考实现，哈夫曼编码的PPA降低66%，所有问题的几何平均PPA降低17%。

**结论:** EvolVE成为Verilog芯片设计自动化领域的新state-of-the-art，有效解决了LLM在硬件设计中的局限性，为复杂硬件优化问题提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EvolVE%3A+Evolutionary+Search+for+LLM-based+Verilog+Generation+and+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18067，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18067&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [38] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye, Yiwen Duan, Yonghong Yu, Victor Ma, Yang Gao, Xing Chen*

**主要类别:** cs.AI

**AI概要:** OurBench是首个企业级SQL推理和调试基准测试，包含469个语法错误查询和516个语义错误查询，测试显示当前最佳LLM模型准确率仅约35%，揭示了企业SQL调试的重大挑战


<details>
  <summary>更多</summary>
  
**动机:** 企业数据工程中SQL代码生成难以一次完全正确，即使有经验的开发者和先进LLM也需要多次调试迭代，需要系统化的评估基准

**方法:** 通过逆向工程自动注入真实bug到大规模SQL代码中构建基准，采用免执行的评估框架进行快速准确的资源高效评估

**结果:** 评估近30个LLM显示性能差距显著：最佳模型Claude-4-Sonnet在语法错误上准确率36.46%，语义错误上32.17%，大多数模型低于20%

**结论:** 研究揭示了企业SQL调试的重大挑战，探索了四种解决策略，为LLM在企业SQL调试中的应用指明了有前景的方向

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Text-to-SQL%3A+Can+LLMs+Really+Debug+Enterprise+ETL+SQL%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18119&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [39] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan, Bivin Pradeep, James Brusey*

**主要类别:** cs.AI

**AI概要:** 研究提出基于强化学习的截止时间感知控制方法，相比传统bang-bang控制和MCTS规划器，PPO算法在热水器控制中能显著降低能耗，最高可节省69%的能源消耗。


<details>
  <summary>更多</summary>
  
**动机:** 传统家用浸入式热水器系统在冬季通常连续运行，加热速度快但效率低，忽略了可预测的需求窗口和环境热损失问题。

**方法:** 建立Gymnasium环境模拟热水器热力学模型，采用一阶热损失模型和离散控制动作（0W和6000W，每120秒执行一次）。比较时间最优bang-bang基线、零样本蒙特卡洛树搜索规划器和近端策略优化策略。

**结果:** 在60步（2小时）时间范围内，PPO仅消耗3.23千瓦时能量，比bang-bang控制（4.37-10.45千瓦时）和MCTS（4.18-6.46千瓦时）更高效。在典型场景中，PPO比bang-bang节省54%能量，比MCTS节省33%能量。

**结论:** 学习得到的截止时间感知控制方法在相同物理假设下显著降低能耗，规划器无需训练即可提供部分节能效果，而学习策略在训练后推理成本接近零。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deadline-Aware%2C+Energy-Efficient+Control+of+Domestic+Immersion+Hot+Water+Heaters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18123，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18123&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [40] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang, Han Wu, Zhiyuan You, Yiming Song, Yijun Wang, Zifei Shan, Yining Li, Songyang Zhang, Xinyi Le, Cailian Chen, Xinping Guan, Dacheng Tao*

**主要类别:** cs.AI

**AI概要:** RouteMoA是一种高效的混合代理框架，通过动态路由和轻量级评分器减少计算成本和延迟，同时保持模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统MoA方法采用密集拓扑结构导致成本和延迟过高，现有方法需要所有模型先推理再筛选，无法有效降低成本，且缺乏模型选择标准和大模型池处理能力。

**方法:** 使用轻量级评分器基于查询预测粗略性能进行初筛，缩小候选模型范围；采用混合评估器通过自评估和交叉评估进行后验校正；设计模型排名机制平衡性能、成本和延迟。

**结果:** RouteMoA在不同任务和模型池规模下均优于传统MoA，在大规模模型池中成本降低89.8%，延迟减少63.6%。

**结论:** RouteMoA通过动态路由和分层评估机制有效解决了MoA框架的成本和延迟问题，为大规模模型协作提供了高效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RouteMoA%3A+Dynamic+Routing+without+Pre-Inference+Boosts+Efficient+Mixture-of-Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18130，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18130&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [41] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen, Hongru Zhou, Huahui Yi, Shiyu Feng, Hanyu Zhou, Tiancheng He, Mingke You, Li Wang, Qiankun Li, Kun Wang, Weili Fu, Kang Li, Jian Li*

**主要类别:** cs.AI

**AI概要:** RareAlert是一个基于多LLM推理校准的罕见病早期筛查系统，通过整合10个大型语言模型的推理信号，使用机器学习校准和加权，最终蒸馏成一个可在本地部署的单一模型，在罕见病风险预测方面表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 罕见病的漏诊和延迟诊断是重大挑战，现有初级保健分诊流程无法可靠识别罕见病患者，需要通用筛查来减少诊断延迟。

**方法:** 开发RareAlert系统，整合10个LLM生成的推理，使用机器学习校准和加权这些信号，并蒸馏成单一可本地部署模型。使用包含158,666个病例的RareBench数据集进行开发和评估。

**结果:** RareAlert在独立测试集上达到AUC 0.917，优于所有评估的机器学习集成和LLM模型（包括GPT-5、DeepSeek-R1等）。

**结论:** 研究表明罕见病识别可重新概念化为对普通患者群体的通用不确定性解决过程，通过校准推理整合到单一模型中，实现了准确、保护隐私且可扩展的罕见病风险筛查。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RareAlert%3A+Aligning+heterogeneous+large+language+model+reasoning+for+early+rare+disease+risk+screening，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18132&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [42] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang, Shutong Jiang, Renhao Li, Jianhong Tu, Yang Su, Lianghao Deng, Xudong Guo, Chenxu Lv, Junyang Lin*

**主要类别:** cs.AI

**AI概要:** DeepPlanning是一个新的长时程智能体规划基准测试，包含多日旅行规划和多产品购物任务，需要主动信息获取、局部约束推理和全局约束优化，现有前沿LLM智能体在此测试中表现不佳。


<details>
  <summary>更多</summary>
  
**动机:** 现有智能体评估多关注短时局部推理，缺乏对全局约束优化（如时间和预算）的测试，且现有LLM规划基准未能充分体现现实世界中主动信息收集和细粒度局部约束的特点。

**方法:** 开发DeepPlanning基准测试，包含多日旅行规划和多产品购物两类任务，这些任务需要智能体进行主动信息获取、局部约束推理和全局约束优化。

**结果:** 评估显示即使是最前沿的智能体LLM在这些问题上也表现困难，突显了可靠的显式推理模式和并行工具使用对于实现更好效果-效率权衡的重要性。

**结论:** DeepPlanning揭示了当前智能体LLM在长时程规划方面的不足，为改进智能体LLM在长规划时域上的性能指明了有前景的研究方向，作者开源了代码和数据以支持未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepPlanning%3A+Benchmarking+Long-Horizon+Agentic+Planning+with+Verifiable+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18137&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [43] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

**主要类别:** cs.AI

**AI概要:** 成功条件化（success conditioning）是一种通过模仿成功轨迹来改进策略的技术，本文证明了该方法实际上解决了信任区域优化问题，在χ²散度约束下最大化策略改进，且不会降低性能。


<details>
  <summary>更多</summary>
  
**动机:** 虽然成功条件化技术被广泛使用（如拒绝采样、目标条件RL、决策变换器等），但其解决的优化问题本质一直不明确，需要理论分析其数学基础和性质。

**方法:** 通过数学证明，将成功条件化形式化为一个信任区域优化问题，分析其在χ²散度约束下的优化特性，并引入动作影响力概念来衡量动作选择对成功率的影响。

**结果:** 证明了成功条件化精确解决了特定优化问题，建立了相对策略改进、策略变化幅度和动作影响力之间的等式关系，显示该方法是一种保守的改进算子。

**结论:** 成功条件化是一种安全可靠的策略改进方法，不会导致性能下降或危险的分布偏移，但改进失败时会通过几乎不改变策略来可观察地表现出来。返回阈值化可以放大改进，但可能偏离真实目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Success+Conditioning+as+Policy+Improvement%3A+The+Optimization+Problem+Solved+by+Imitating+Success，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18175，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18175&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [44] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang, Pei Fu, Ruoceng Zhang, Shaojie Zhang, Xiuwen Xi, Jiahui Yang, Bin Qin, Ying Huang, Zhenbo Luo, Jian Luan*

**主要类别:** cs.AI

**AI概要:** GAIA框架通过训练直觉评论模型(ICM)来提升GUI代理的性能，通过数据飞轮机制实现自我改进，提高测试时性能表现


<details>
  <summary>更多</summary>
  
**动机:** 解决大型视觉语言模型在GUI代理操作中的不可逆性问题，单次错误操作可能导致灾难性偏差

**方法:** 训练直觉评论模型(ICM)评估代理动作正确性，通过正负样本训练和迭代数据收集构建自我改进循环

**结果:** 实验表明ICM能提升各种闭源和开源模型的测试时性能，且随着数据循环性能逐步提高

**结论:** GAIA框架通过评论模型和数据飞轮机制有效解决了GUI代理操作的不可逆性问题，实现了持续性能改进

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GAIA%3A+A+Data+Flywheel+System+for+Training+GUI+Test-Time+Scaling+Critic+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18197&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [45] [SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback](https://arxiv.org/abs/2601.18202)
*Fangyuan Xu, Rujun Han, Yanfei Chen, Zifeng Wang, I-Hung Hsu, Jun Yan, Vishy Tirumalashetty, Eunsol Choi, Tomas Pfister, Chen-Yu Lee*

**主要类别:** cs.AI

**AI概要:** SAGE是一个自动生成高质量、难度可控的深度搜索问答对的智能代理流水线，通过生成器和搜索代理的交互迭代优化数据质量，显著提升深度搜索代理的性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于深度搜索问题需要跨多个文档进行复杂推理，人工标注成本极高，因此需要自动化方法来生成高质量的问答训练数据。

**方法:** 提出SAGE流水线，包含数据生成器和搜索代理两个组件，通过多轮交互迭代优化问答对，直至达到目标难度水平。

**结果:** 内在评估显示SAGE生成的问题需要多样化推理策略，数据正确性和难度显著提升；外在评估显示使用合成数据训练的深度搜索代理在基准测试中性能提升达23%，且能适应从固定语料检索到Google搜索的迁移。

**结论:** SAGE能够自动生成高质量的深度搜索训练数据，有效解决人工标注成本高的问题，显著提升深度搜索代理的性能和适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SAGE%3A+Steerable+Agentic+Data+Generation+for+Deep+Search+with+Execution+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18202，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18202&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.

</details>


### [46] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu, Lin Guan, Yixin Nie, Kai Zhang, Zhuoqun Hao, Lin Chen, Asli Celikyilmaz, Zhaoran Wang, Na Zhang*

**主要类别:** cs.AI

**AI概要:** 本文研究LLM智能体在未知领域中的泛化能力，发现状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，提出通过添加干扰性特征增加状态信息丰富度的方法，并分析了不同建模选择对泛化能力的影响。


<details>
  <summary>更多</summary>
  
**动机:** 通用LLM智能体通常在狭窄环境集上进行后训练，但需要在更广泛的未见领域部署，需要研究在未知测试域情况下的智能体后训练挑战。

**方法:** 分析RL环境和建模选择对域外性能的影响，识别环境轴心（状态信息丰富度和规划复杂度），提出状态随机化技术增加信息丰富度，并研究SFT预热和中途训练、逐步思考等建模选择。

**结果:** 发现状态信息丰富度和规划复杂度与跨域泛化强相关，域真实性和文本相似性不是主要因素；增加状态信息丰富度可有效提高跨域鲁棒性；SFT预热有助于防止灾难性遗忘但损害未包含域的泛化；逐步思考在保持泛化中起关键作用。

**结论:** 环境特性（特别是状态信息丰富度）和特定建模选择对LLM智能体的跨域泛化能力有重要影响，通过适当的环境设计和训练策略可以显著提升智能体在未知领域的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Paying+Less+Generalization+Tax%3A+A+Cross-Domain+Generalization+Study+of+RL+Training+for+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18217&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [47] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang, Yanan Wu, Xiaoshuai Song, Weixun Wang, Gengru Chen, Zhongwen Li, Kezhong Yan, Ken Deng, Qi Liu, Shuaibing Zhao, Shaopan Xiong, Xuepeng Liu, Xuefeng Chen, Wanxi Deng, Wenbo Su, Bo Zheng*

**主要类别:** cs.AI

**AI概要:** ShopSimulator是一个大规模中文电商环境，用于评估和训练LLM购物代理，发现现有模型成功率不足40%，通过SFT+RL训练可显著提升性能


<details>
  <summary>更多</summary>
  
**动机:** 现有研究缺乏统一的模拟环境来全面评估LLM购物代理的个性化推荐、多轮对话和产品检索能力，且只关注评估而缺乏训练支持

**方法:** 开发ShopSimulator中文购物环境，评估多种LLM模型在不同场景下的表现，进行错误分析，并探索监督微调(SFT)和强化学习(RL)的训练方法

**结果:** 最佳模型的全成功率低于40%，代理在长轨迹中的深度搜索和产品选择、个性化线索平衡、用户互动方面存在困难

**结论:** SFT和RL结合的训练方法能显著改善LLM购物代理的性能，ShopSimulator为电商领域的LLM代理研究和开发提供了重要工具

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ShopSimulator%3A+Evaluating+and+Exploring+RL-Driven+LLM+Agent+for+Shopping+Assistants，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18225&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [48] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li, Shijun Yang, Weizhen Qi, Silei Zhao, Rui Hua, Mingzhu Song, Xiaojian Yang, Chao Peng*

**主要类别:** cs.AI

**AI概要:** 提出In-Situ Self-Evolving范式，通过工具演化实现智能体在开放环境中的持续自我进化，无需真实标签即可从任务执行反馈中提炼可重用能力


<details>
  <summary>更多</summary>
  
**动机:** 传统智能体系统在开放环境中面临任务分布持续漂移和外部监督稀缺的挑战，静态工具集和离线训练无法适应动态变化，导致能力边界僵化未知

**方法:** 提出Yunjue Agent系统，采用Parallel Batch Evolution策略，通过迭代合成、优化和重用工具来处理新兴挑战，将顺序任务交互作为连续经验流

**结果:** 在五个不同基准测试的零起点设置下表现出显著性能提升，热启动评估证实积累的通用知识可无缝迁移到新领域

**结论:** 提出的自进化范式有效解决了开放环境中的适应性问题，开发的新指标可监控演化收敛，开源代码库促进弹性自进化智能研究

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Yunjue+Agent+Tech+Report%3A+A+Fully+Reproducible%2C+Zero-Start+In-Situ+Self-Evolving+Agent+System+for+Open-Ended+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18226，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18226&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [49] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei, Jinpeng Ou, Xiao Peng, Bin Wang*

**主要类别:** cs.AI

**AI概要:** 论文提出了TAFC框架，通过函数和参数级别的显式推理来提升语言模型在函数调用中的准确性和可解释性，无需修改模型架构即可实现。


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型在函数调用时缺乏参数生成的显式推理透明度，特别是对于具有相互依赖参数的复杂函数，现有方法无法提供细粒度的参数级推理指导。

**方法:** 提出Think-Augmented Function Calling (TAFC)框架，引入通用的"think"参数增强机制，使模型能够表达决策过程；通过动态优化参数描述提高推理质量；基于复杂度评分自动触发细粒度推理；提出推理引导优化使生成推理符合人类期望。

**结果:** 在ToolBench上的评估显示，TAFC在专有和开源模型上都显著提高了多参数函数的参数生成准确性和推理连贯性，同时增强了AI代理行为调试的可解释性。

**结论:** TAFC框架有效解决了函数调用中的推理透明度问题，通过参数级显式推理显著提升了准确性和可解释性，且与现有API完全兼容，无需修改模型架构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think-Augmented+Function+Calling%3A+Improving+LLM+Parameter+Accuracy+Through+Embedded+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18282&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [50] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

**主要类别:** cs.AI

**AI概要:** Climate RADAR是一个基于生成式AI的灾害预警系统，通过整合多源数据和LLM技术提供个性化行动建议，显著提升防护行动执行率和系统可信度。


<details>
  <summary>更多</summary>
  
**动机:** 传统预警系统虽然能快速发布警报，但往往无法触发及时的防护行动，导致可预防的损失和不公平现象。

**方法:** 整合气象、水文、脆弱性和社会数据构建综合风险指数，使用带有防护机制的大语言模型为公民、志愿者和市政部门提供个性化建议。

**结果:** 通过模拟、用户研究和市政试点验证，显示防护行动执行率提高、响应延迟减少、可用性和信任度提升。

**结论:** Climate RADAR结合预测分析、行为科学和负责任AI，推进了以人为本、透明公平的预警系统，为合规的灾害韧性基础设施提供实践路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Generative+AI-Driven+Reliability+Layer+for+Action-Oriented+Disaster+Resilience，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18308，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18308&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [51] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty, Paramveer S. Dhillon*

**主要类别:** cs.AI

**AI概要:** 研究发现，经过微调的AI在模仿著名作家风格方面甚至比专业作家更受专家青睐（62% vs 38%），而普通读者始终更偏好AI作品，这引发了专业作家的身份危机和对创意劳动未来的质疑。


<details>
  <summary>更多</summary>
  
**动机:** 挑战AI无法复制人类创意写作的传统观念，研究生成式AI在模仿作家风格方面的能力，特别是与专业作家进行比较。

**方法:** 行为实验：28名MFA专业作家与3个LLM竞争模仿50位著名作家的风格；通过28名专家评委和131名普通评委进行盲审 pairwise 比较；采用上下文提示和完整作品微调两种条件。

**结果:** 上下文提示条件下专家82.7%偏好人类写作，但微调后逆转为62%偏好AI；普通评委始终偏好AI写作；专家作家因偏好AI而产生身份危机和审美自信动摇。

**结论:** AI在创意写作方面的能力挑战了其创造性限制的传统论述，对创意劳动的未来提出了根本性疑问，需要重新思考"优秀写作"的定义标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Good+Writing+Be+Generative%3F+Expert-Level+AI+Writing+Emerges+through+Fine-Tuning+on+High-Quality+Books，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18353&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [52] [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381)
*Yinghan Hou, Zongyou Yang*

**主要类别:** cs.AI

**AI概要:** 本研究开发了一个集成AI代理框架，用于将传统有限差分实现转换为Devito环境。结合RAG和开源大语言模型，通过多阶段迭代工作流和LangGraph混合架构，构建Devito知识图谱并进行代码转换。


<details>
  <summary>更多</summary>
  
**动机:** 为了促进传统有限差分实现的现代化转型，需要将Fortran等遗留代码转换为现代Devito框架，但手动转换过程复杂且容易出错。

**方法:** 采用混合LangGraph架构，结合RAG和大语言模型，通过文档解析、知识图谱构建、社区检测、多阶段检索管道和代码合成约束来实现代码转换。

**结果:** 开发了一个能够自动分析Fortran源代码、构建知识图谱、执行多阶段检索和生成结构化代码输出的AI代理框架。

**结论:** 该框架通过强化学习启发的反馈机制，实现了从静态代码翻译向动态自适应分析行为的转变，为科学计算代码现代化提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Agent+for+Reverse-Engineering+Legacy+Finite-Difference+Code+and+Translating+to+Devito，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18381&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.

</details>


### [53] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo, Tong Chen, Wenlong Meng, Chen Gong, Xin Yu, Chengkun Wei, Wenzhi Chen*

**主要类别:** cs.AI

**AI概要:** 论文提出DynTS方法，通过注意力图分析发现推理痕迹中只有关键决策token对最终答案有重要影响，通过选择性保留关键token的KV缓存来优化大型推理模型的效率。


<details>
  <summary>更多</summary>
  
**动机:** 大型推理模型在生成推理痕迹时产生大量内存占用和计算开销，成为效率瓶颈，需要找到优化方法。

**方法:** 使用注意力图分析推理痕迹的影响，识别决策关键token，提出DynTS方法选择性保留关键token的KV缓存状态。

**结果:** 发现只有部分决策关键token对最终答案起决定性作用，其余token贡献可忽略，DynTS方法能有效优化效率。

**结论:** 通过选择性KV缓存管理可以显著提升大型推理模型的效率，而不影响推理质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Thinking-Token+Selection+for+Efficient+Reasoning+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18383&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [54] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou, Kai Zheng, Qiguang Chen, Mengkang Hu, Qingfeng Sun, Can Xu, Jingjing Chen*

**主要类别:** cs.AI

**AI概要:** OffSeeker：完全离线训练的8B参数研究智能体，在六个基准测试中表现优于同规模模型，甚至可与30B参数的在线RL训练系统竞争，无需昂贵的在线强化学习。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度研究智能体依赖昂贵的在线强化学习（需要大量API调用），而离线训练因缺乏高质量研究轨迹数据而进展缓慢。

**方法:** 开发了完全开源的离线训练套件，包括DeepForge任务合成框架（无需重预处理生成大规模研究查询）和66k QA对、33k SFT轨迹、21k DPO对的数据集，基于此训练OffSeeker模型。

**结果:** 在六个基准测试中，OffSeeker（8B参数）不仅领先同规模智能体，还与通过大量在线RL训练的30B参数系统保持竞争力。

**结论:** 昂贵在线强化学习并非构建强大研究智能体的唯一途径，离线训练方法通过高质量数据和有效框架可以实现同等甚至更好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OffSeeker%3A+Online+Reinforcement+Learning+Is+Not+All+You+Need+for+Deep+Research+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18467，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18467&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [55] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu, Qihan Ren, Chen Qian, Shuai Shao, Yuejin Xie, Yu Li, Zhonghao Yang, Haoyu Luo, Peng Wang, Qingyu Liu, Binxin Hu, Ling Tang, Jilin Mei, Dadi Guo, Leitao Yuan, Junyao Yang, Guanxu Chen, Qihao Lin, Yi Yu, Bo Zhang, Jiaxuan Guo, Jie Zhang, Wenqi Shao, Huiqi Deng, Zhiheng Xi, Wenjie Wang, Wenxuan Wang, Wen Shen, Zhikai Chen, Haoyu Xie, Jialing Tao, Juntao Dai, Jiaming Ji, Zhongjie Ba, Linfeng Zhang, Yong Liu, Quanshi Zhang, Lei Zhu, Zhihua Wei, Hui Xue, Chaochao Lu, Jing Shao, Xia Hu*

**主要类别:** cs.AI

**AI概要:** 论文提出了一个三维分类法来系统化AI智能体风险，并开发了AgentDoG诊断护栏框架和ATBench基准测试，用于细粒度监控和诊断智能体安全风险，在复杂交互场景中实现了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前护栏模型缺乏对智能体风险的认知和风险诊断的透明度，无法应对自主工具使用和环境交互带来的复杂安全挑战

**方法:** 提出统一的三维分类法（来源、失效模式、后果），基于此构建细粒度智能体安全基准ATBench和诊断护栏框架AgentDoG，提供跨轨迹的上下文监控和根因诊断

**结果:** AgentDoG在多样复杂交互场景中实现了最先进的智能体安全调节性能，提供了三种参数规模的模型变体（4B、7B、8B）

**结论:** 该方法为智能体安全提供了细粒度监控和透明诊断能力，超越了二元标签的限制，有助于实现有效的智能体对齐，所有模型和数据集均已开源发布

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentDoG%3A+A+Diagnostic+Guardrail+Framework+for+AI+Agent+Safety+and+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18491，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18491&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [56] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang, Hao Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yiqun Zhang, Jinghao Lin, Haihua Yang, Xiaozhong Ji*

**主要类别:** cs.AI

**AI概要:** DeepMed是一个针对医疗领域的深度研究模型，通过多跳医疗搜索QA合成方法、难度感知轮次惩罚和推理监控机制，解决了通用DR模型在医疗场景中的任务特征和工具使用扩展问题，在七个医疗基准上平均提升9.79%


<details>
  <summary>更多</summary>
  
**动机:** 通用深度研究模型直接迁移到医疗领域效果有限，主要因为两个差距：1）医疗问题需要在知识密集型临床背景下进行证据解释；2）盲目扩展工具调用会引入噪声上下文，干扰敏感的医疗推理

**方法:** 1）多跳医疗搜索QA合成方法支持模型在医疗背景下应用DR范式；2）难度感知轮次惩罚抑制过度工具调用增长；3）推理监控机制在有限步骤内验证假设并避免上下文腐化

**结果:** 在七个医疗基准测试中，DeepMed相比基础模型平均提升9.79%，表现优于更大的医疗推理和DR模型

**结论:** DeepMed成功解决了医疗领域深度研究的特殊挑战，通过专门的数据合成、训练和推理策略，显著提升了医疗推理性能，为医疗AI应用提供了有效的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DEEPMED%3A+Building+a+Medical+DeepResearch+Agent+via+Multi-hop+Med-Search+Data+and+Turn-Controlled+Agentic+Training+%26+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18496&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [57] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura, Li Wang, Sahil Badyal, Eugenio Beaufrand, Adam Faulkner*

**主要类别:** cs.AI

**AI概要:** MOSAIC是一个模块化框架，通过动态生成包含多达20种应用导向约束的数据集，来细粒度评估大语言模型遵循复杂指令的能力。研究发现指令遵循不是单一能力，而是受约束类型、数量和位置显著影响。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试往往无法反映真实使用场景或将指令遵循与任务成功分开评估，需要一种更可靠的方法来确保大语言模型遵循复杂指令。

**方法:** 开发MOSAIC框架，使用动态生成的数据集，包含多达20种应用导向的生成约束，对来自不同家族的5个大语言模型进行评估。

**结果:** 研究发现指令遵循能力不是单一的，而是随约束类型、数量和位置显著变化；揭示了模型特定的弱点、指令间的协同和冲突关系，以及首因效应和近因效应等位置偏差。

**结论:** 这些细粒度的洞察对于诊断模型失败和开发需要严格遵循复杂指令的更可靠大语言模型系统至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deconstructing+Instruction-Following%3A+A+New+Benchmark+for+Granular+Evaluation+of+Large+Language+Model+Instruction+Compliance+Abilities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18554，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18554&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [58] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng, Qiangsheng Zeng, Ling Luo, Qinghan Yang, Jiarui Hao, Wenbo Wu, Qinyu Wang, Rui Yin, Lin Qi, Renzhi Lu*

**主要类别:** cs.AI

**AI概要:** 训练稳定性虽然有助于优化过程，但可能导致生成分布集中在有限的数据模式上，降低生成多样性，出现重复行为，表明稳定性不能完全代表生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 分析训练稳定性对大型语言模型生成分布的影响，探讨稳定性与生成表达能力之间的关系。

**方法:** 使用基于反馈的训练框架控制内部生成统计量，观察不同架构和随机种子下的输出熵和重复行为。

**结果:** 稳定的参数轨迹使模型近似最小化前向KL散度，但隐式减少生成熵，导致概率质量集中在有限的经验模式子集上。

**结论:** 优化稳定性和生成表达能力并非内在一致，稳定性本身不足以作为生成质量的指标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stability+as+a+Liability%3ASystematic+Breakdown+of+Linguistic+Structure+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18588，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18588&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [59] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu, Didier Chetelat, Yingxue Zhang, Mark Coates*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种结合大语言模型和逻辑求解器的新方法，通过迭代方式用LLM提供的常识关系增强逻辑问题，以解决LLMs在复杂证明规划中的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在形式推理方面表现出色，但在需要复杂证明规划的问题上表现不佳。现有的逻辑求解器虽然效率高，但无法处理缺失的常识关系。

**方法:** 使用逻辑求解器的反馈来迭代地增强逻辑问题，通过搜索潜在的常识假设来最大化找到有用事实的机会，同时控制成本。

**结果:** 在移除了部分常识信息的纯逻辑推理数据集上，该方法相比现有技术持续获得显著改进。

**结论:** 在人类语境中工作时，平衡神经和符号元素具有重要价值，该方法证明了这种混合方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Balanced+Neuro-Symbolic+Approach+for+Commonsense+Abductive+Logic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18595&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [60] [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608)
*Fabian Fumagalli, R. Teal Witter, Christopher Musco*

**主要类别:** cs.AI

**AI概要:** PolySHAP方法通过使用高阶多项式逼近特征交互来改进KernelSHAP，提供更准确的Shapley值估计，并证明配对采样等同于二阶PolySHAP。


<details>
  <summary>更多</summary>
  
**动机:** KernelSHAP虽然避免了指数计算成本，但仅使用线性逼近无法捕捉特征间的非线性交互，需要更精确的Shapley值估计方法。

**方法:** 扩展KernelSHAP，使用高阶多项式而非线性函数来逼近游戏函数，通过随机特征子集的少量游戏评估来拟合多项式。

**结果:** PolySHAP在各种基准数据集上获得经验上更好的Shapley值估计，且证明这些估计具有一致性。同时证明配对采样与二阶PolySHAP等价。

**结论:** PolySHAP提供了更准确的Shapley值估计方法，并为配对采样启发式方法的优异性能提供了首个强理论证明。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PolySHAP%3A+Extending+KernelSHAP+with+Interaction-Informed+Polynomial+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18608，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18608&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.
  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.
  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.

</details>


### [61] [Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks](https://arxiv.org/abs/2601.18617)
*Pierre Orhan, Pablo Diego-Simón, Emmnanuel Chemla, Yair Lakretz, Yves Boubenec, Jean-Rémi King*

**主要类别:** cs.AI

**AI概要:** 研究发现人工神经网络在训练过程中会自发地依次发展出音素、词汇和句法表征，其学习阶段与儿童语言习得类似但需要多2-4个数量级的数据量。


<details>
  <summary>更多</summary>
  
**动机:** 虽然儿童语言习得的行为发展已被充分描述，但仍缺乏统一的计算框架来解释其背后的神经表征机制。

**方法:** 研究分析基于语音和文本的人工神经网络在训练过程中的激活模式，观察音素、词汇和句法表征何时以及如何出现。

**结果:** 神经网络在训练中依次构建表征子空间，其几何结构分别对应音素、词汇和句法结构，展现了与儿童相似但数据需求更大的学习轨迹。

**结论:** 该研究揭示了语言习得主要阶段自发出现的条件，为理解语言习得的计算机制提供了有前景的研究路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergence+of+Phonemic%2C+Syntactic%2C+and+Semantic+Representations+in+Artificial+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18617，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18617&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.

</details>


### [62] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi, Md Tahmid Rahman Laskar, Elahe Rahimi, Sheri Grach, Lindsay Bertrand, Lames Danok, Frank Rudzicz, Jimmy Huang, Elham Dolatabadi*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于人类评估的方法来评估大型语言模型在心理健康对话中的表现，发现LLMs在认知支持方面表现良好但情感共鸣不稳定，开源模型相比闭源模型存在更大差异性和情感平淡问题。


<details>
  <summary>更多</summary>
  
**动机:** 全球心理健康危机日益严重，治疗资源匮乏，大型语言模型虽能提供可扩展的情感支持，但其可靠性、治疗相关性和与人类标准的对齐仍存在挑战。

**方法:** 研究收集了500个心理健康对话数据集，评估了9个不同LLMs（包括闭源和开源模型）生成的回复，由两名精神病学专家使用5点李克特量表在6个属性维度上进行独立评分。

**结果:** LLMs在认知支持方面表现强劲，能提供安全、连贯且临床适宜的信息，但在情感对齐方面表现不稳定。闭源模型（如GPT-4o）提供更平衡的治疗回应，开源模型则显示出更大的变异性和情感平淡。

**结论:** 研究揭示了LLMs在认知-情感方面存在持续差距，强调需要建立以失败意识为基础、临床导向的评估框架，优先考虑关系敏感性和信息准确性，并提倡以治疗敏感性为中心的人类参与评估协议。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+the+Quality+of+Mental+Health+Support+in+LLM+Responses+through+Multi-Attribute+Human+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18630&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [63] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu, Ranjay Krishna, Yu Cheng*

**主要类别:** cs.AI

**AI概要:** AdaReasoner是一个多模态模型家族，通过学习工具使用作为通用推理技能，而非特定工具或显式监督行为，实现了在视觉推理任务中的先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 人类通过使用工具解决超出自身能力的问题，这为提高多模态大语言模型的视觉推理能力提供了有前景的范式。有效推理的关键在于知道使用哪些工具、何时调用以及如何组合多个步骤的工具，即使面对新工具或新任务。

**方法:** 通过(i)可扩展的数据管理流程，让模型接触长视野、多步骤的工具交互；(ii)Tool-GRPO强化学习算法，基于最终任务成功优化工具选择和序列；(iii)自适应学习机制，动态调节工具使用。

**结果:** AdaReasoner表现出强大的工具适应和泛化能力：自主采用有益工具、抑制无关工具、根据任务需求调整工具使用频率。在多个基准测试中达到最先进性能，将7B基础模型平均提升24.9%，在VSP和Jigsaw等任务上超越GPT-5等专有系统。

**结论:** 该研究展示了通过学习工具使用作为通用推理技能，多模态模型能够从任务上下文和中间结果推断工具效用，实现多个工具的协调和对未见工具的泛化，为提升视觉推理能力提供了有效方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AdaReasoner%3A+Dynamic+Tool+Orchestration+for+Iterative+Visual+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18631，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18631&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [64] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei, Xu Dong, Xiao Peng, Niantao Xie, Bin Wang*

**主要类别:** cs.AI

**AI概要:** FadeMem：一种受生物启发的智能体记忆架构，通过模拟人类记忆的主动遗忘机制，实现了选择性遗忘和双层级记忆结构，在减少45%存储的同时提升了多跳推理能力


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型作为自主代理面临关键的内存限制，缺乏选择性遗忘机制，导致要么在上下文边界发生灾难性遗忘，要么在内部信息过载。人类记忆通过自适应衰减过程自然平衡保留和遗忘，而当前AI系统采用二元保留策略（要么全部保留，要么完全丢失）

**方法:** 提出FadeMem架构，采用双层级记忆层次结构，通过自适应指数衰减函数实现差异衰减率，衰减由语义相关性、访问频率和时间模式调节。通过LLM引导的冲突解决和智能记忆融合，系统能够整合相关信息同时让无关细节逐渐消失

**结果:** 在Multi-Session Chat、LoCoMo和LTI-Bench上的实验表明，系统实现了优越的多跳推理和检索能力，同时减少了45%的存储需求

**结论:** 验证了生物启发式遗忘在智能体记忆系统中的有效性，为解决AI系统内存限制问题提供了新的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FadeMem%3A+Biologically-Inspired+Forgetting+for+Efficient+Agent+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18642，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18642&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [65] [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700)
*Xingyu Sui, Yanyan Zhao, Yulin Hu, Jiahe Guo, Weixiang Zhao, Bing Qin*

**主要类别:** cs.AI

**AI概要:** TEA-Bench是首个用于评估工具增强情感支持对话系统的交互式基准测试，包含真实情感场景、MCP风格工具环境和过程级评估指标，实验显示工具增强能提高情感支持质量并减少幻觉，但效果与模型能力密切相关。


<details>
  <summary>更多</summary>
  
**动机:** 现有情感支持对话系统和基准测试主要关注纯文本环境下的情感支持，忽视了外部工具如何实现事实基础并减少多轮情感支持中的幻觉问题。

**方法:** 引入TEA-Bench基准测试，包含真实情感场景和工具环境，对9个大型语言模型进行实验，分析工具增强对情感支持的影响，并发布TEA-Dialog工具增强对话数据集。

**结果:** 工具增强普遍提高了情感支持质量并减少了幻觉，但效果强烈依赖于模型能力：更强的模型能更选择性和有效地使用工具，而较弱模型获益有限。监督微调在分布内支持上有所改进但泛化能力差。

**结论:** 工具使用对于构建可靠的情感支持代理至关重要，模型能力是影响工具使用效果的关键因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TEA-Bench%3A+A+Systematic+Benchmarking+of+Tool-enhanced+Emotional+Support+Dialogue+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18700，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18700&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.

</details>


### [66] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang, Sepehr Janghorbani, Dongxu Zhang, Jun Han, Qian Qian, Andrew Ressler, Gregory D. Lyng, Sanjit Singh Batra, Robert E. Tillman*

**主要类别:** cs.AI

**AI概要:** Health-SCORE是一个可扩展的基于评分标准的医疗领域LLM评估框架，显著降低评分标准开发成本，同时保持与人工制定标准相当的性能，并可用于强化学习和上下文学习。


<details>
  <summary>更多</summary>
  
**动机:** 医疗等安全关键领域中，制定高质量领域特定的评分标准需要大量人工专业知识和开发成本，使得基于评分标准的评估和训练难以扩展。

**方法:** 开发了Health-SCORE框架，这是一个可推广和可扩展的基于评分标准的训练和评估框架，大幅减少评分标准开发成本而不牺牲性能。

**结果:** 在开放式医疗任务中，Health-SCORE实现了与人工创建评分标准相当的评估质量，同时显著降低了开发工作量。

**结论:** Health-SCORE使基于评分标准的评估和训练更加可扩展，提供了超越独立评估的实际效益，包括作为结构化奖励信号指导强化学习和通过上下文学习提高响应质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Health-SCORE%3A+Towards+Scalable+Rubrics+for+Improving+Health-LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18706，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18706&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [67] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam, Thomas R. Caulfield*

**主要类别:** cs.AI

**AI概要:** 本研究开发了一种AI辅助药物设计方法，通过E3连接酶定向分子胶促进Aβ-42的靶向降解，为神经退行性疾病治疗提供新策略。


<details>
  <summary>更多</summary>
  
**动机:** 阿尔茨海默病中Aβ-42的病理积累导致突触功能障碍和神经退行性变，细胞内Aβ-42作为疾病进展的早期毒性驱动因素日益受到关注。

**方法:** 采用基于结构的建模、ADMET筛选和对接方法，系统评估Aβ-42与三种E3连接酶的复合物形成潜力；开发了连接酶条件化连接树变分自编码器(LC-JT-VAE)生成连接酶特异性小分子。

**结果:** 生成模型能够产生化学有效、新颖且靶向特异性的分子胶，能够促进Aβ-42降解。

**结论:** 这种集成方法为设计针对神经退行性疾病的UPS靶向治疗提供了一个有前景的框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conditioned+Generative+Modeling+of+Molecular+Glues%3A+A+Realistic+AI+Approach+for+Synthesizable+Drug-like+Molecules，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18716&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [68] [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](https://arxiv.org/abs/2601.18735)
*Jusheng Zhang, Yijia Fan, Kaitong Cai, Jing Yang, Jiawei Yao, Jian Wang, Guanlong Qu, Ziliang Chen, Keze Wang*

**主要类别:** cs.AI

**AI概要:** Agora框架将多智能体协调重新定义为不确定性去中心化市场，通过将认知不确定性转化为可交易资产，基于经济规则实现成本高效的智能体协作，在多个多模态基准测试中表现优异且大幅降低成本。


<details>
  <summary>更多</summary>
  
**动机:** 当前视觉语言模型多智能体系统扩展成本高昂，现有协调方法依赖启发式代理，忽略成本且破坏不确定性结构，导致次优协调。

**方法:** 将认知不确定性（感知、语义、推理）形式化为结构化可交易资产，基于理性经济规则实施盈利驱动的智能体交易，使用扩展Thompson采样的市场感知经纪人引导系统达到成本高效均衡。

**结果:** 在五个多模态基准测试（MMMU、MMBench、MathVision、InfoVQA、CC-OCR）上表现优于强基线，在MMMU上准确率提升8.5%同时成本降低3倍以上。

**结论:** 基于市场的协调为构建经济可行的多智能体视觉智能系统提供了原则性和可扩展的范式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+Keep+Your+Doubts+to+Yourself%3F+Trading+Visual+Uncertainties+in+Multi-Agent+Bandit+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18735，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18735&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.

</details>


### [69] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu, Xingang Guo, Lingzhi Yuan, Haoqiang Kang, Hongyu Zhao, Lianhui Qin, Furong Huang, Bin Hu, Tianyi Zhou*

**主要类别:** cs.AI

**AI概要:** TSRBench是一个全面的多模态时间序列推理基准测试，包含4125个问题、14个领域和4个维度的15个任务，用于评估主流大模型的时间序列推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列数据在现实场景中无处不在且至关重要，但现有通用模型基准测试缺乏时间序列维度，需要填补这一空白。

**方法:** 构建包含4125个问题、14个领域、4个主要维度（感知、推理、预测、决策）和15个任务的多模态基准测试，评估了30多个领先的专有和开源LLM、VLM和TSLLM模型。

**结果:** 发现：1)缩放定律适用于感知和推理但预测失效；2)强推理能力不能保证准确的上下文感知预测；3)多模态模型未能有效融合文本和视觉表示以获得性能增益。

**结论:** TSRBench提供了标准化评估平台，不仅揭示了现有挑战，还为推进通用模型发展提供了宝贵见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TSRBench%3A+A+Comprehensive+Multi-task+Multi-modal+Time+Series+Reasoning+Benchmark+for+Generalist+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18744，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18744&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang, Cheng Tang, Lei Gong, Cheng Li, Chao Wang, teng wang, Wenqi Lou, Xuehai Zhou*

**主要类别:** cs.CL

**AI概要:** Crystal-KV是一个针对思维链推理优化的KV缓存管理框架，通过答案优先原则区分关键缓存和冗余缓存，实现高效压缩而不损失推理精度。


<details>
  <summary>更多</summary>
  
**动机:** 传统KV缓存压缩策略在思维链推理中效果不佳，因为所有token被同等对待，而CoT推理更关注最终答案，导致内存开销过大。

**方法:** 1. 基于答案优先原则区分SlipKV（可能误导）和CrystalKV（关键贡献）
2. 提出基于注意力的LRFU算法识别和淘汰过期SlipKV
3. 自适应缓存预算分配算法根据CrystalKV比例动态调整各层/头的缓存预算

**结果:** 实现了最先进的KV缓存压缩，显著提高吞吐量，加快响应时间，同时保持甚至提高了CoT推理的答案准确性。

**结论:** Crystal-KV通过精细区分缓存重要性并动态管理，有效解决了CoT推理中的内存效率问题，为大规模语言模型的高效推理提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Crystal-KV%3A+Efficient+KV+Cache+Management+for+Chain-of-Thought+LLMs+via+Answer-First+Principle，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.16986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16986&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [71] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo, Peibei Cao, Zhihui Zhu, Kehua Feng, Zhihua Wang, Keyan Ding*

**主要类别:** cs.CL

**AI概要:** PMDC是一个动态高效的奖励模型评估框架，通过主动选择最大分歧的提示-响应对来测试模型泛化能力，相比传统静态评估方法能更真实地反映模型在开放域的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有奖励模型评估主要依赖静态的预标注偏好数据集，覆盖范围有限且无法真实评估模型在开放世界设置中的泛化能力。

**方法:** 提出Pairwise Maximum Discrepancy Competition (PMDC)框架：使用大规模未标注开放域提示池，主动选择两个奖励模型分歧最大的提示-响应对，通过oracle裁决后使用Bradley-Terry模型进行全局排名。

**结果:** 对10个代表性奖励模型重新评估发现排名发生显著变化，定性分析揭示了系统性的泛化失败问题。

**结论:** PMDC提供了更真实的奖励模型泛化能力评估方法，为改进奖励建模提供了宝贵见解，证明了动态评估框架相比传统静态评估的优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Reward+Model+Generalization+via+Pairwise+Maximum+Discrepancy+Competitions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.16987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16987&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [72] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer, Srijan Sengupta, Karl Pazdernik*

**主要类别:** cs.CL

**AI概要:** 提出一个基于共形预测的NER不确定性感知框架，为序列标注模型生成包含正确标注的预测集合，提供统计保证的置信度覆盖


<details>
  <summary>更多</summary>
  
**动机:** 当前NER模型输出单一预测序列而无不确定性度量，导致下游应用易受级联错误影响

**方法:** 基于共形预测框架设计高效的非共形性评分函数，构建支持无条件覆盖和类别条件覆盖的校准预测集合

**结果:** 在三个基准数据集上的四个NER模型实验表明，该方法具有广泛适用性、有效性和高效性

**结论:** 该框架为NER预测提供统计可靠性保证，能够处理句子长度、语言、实体类型和实体数量的异质性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+Quantification+for+Named+Entity+Recognition+via+Full-Sequence+and+Subsequence+Conformal+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.16999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16999&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [73] [RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection](https://arxiv.org/abs/2601.17002)
*Ziyang Zhou, Ziqi Liu, Yan Wang, Yiming Lin, Yangbin Chen*

**主要类别:** cs.CL

**AI概要:** RAM-SD：基于检索增强的多智能体框架，通过四阶段处理（上下文检索、元规划、多视角分析、集成判断）实现讽刺检测，在四个基准测试中达到77.74%的Macro-F1，比GPT-4o+CoC基线提升7.01个百分点，并提供可解释的推理过程。


<details>
  <summary>更多</summary>
  
**动机:** 现有讽刺检测方法对所有输入采用统一的推理策略，难以应对讽刺表达的多样性分析需求，包括上下文期望违背建模、外部知识基础和特定修辞模式识别等挑战。

**方法:** 提出四阶段框架：1）上下文检索获取讽刺和非讽刺示例；2）元规划分类讽刺类型并选择最优推理计划；3）专业智能体集合进行互补多视角分析；4）集成器综合分析生成最终可解释判断和自然语言解释。

**结果:** 在四个标准基准测试中达到77.74%的Macro-F1分数，比GPT-4o+CoC基线方法提高了7.01个百分点，创造了新的性能记录。

**结论:** RAM-SD不仅实现了最先进的性能表现，还提供了透明可解释的推理过程，揭示了讽刺理解背后的认知过程，为解决讽刺检测的复杂挑战提供了有效框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RAM-SD%3A+Retrieval-Augmented+Multi-agent+framework+for+Sarcasm+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17002，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17002&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.

</details>


### [74] [From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech](https://arxiv.org/abs/2601.17132)
*Vigneshwaran Shankaran, Gabriella Lapesa, Claudia Wagner*

**主要类别:** cs.CL

**AI概要:** 本文通过整合心理学、政治学、传播学和语言学等多学科视角，建立了恐惧言论的理论框架和分类体系，为恐惧言论数据集构建和研究提供了理论和实践指导。


<details>
  <summary>更多</summary>
  
**动机:** 恐惧言论作为一种广泛传播且影响力超过仇恨言论的特殊言论形式，在计算语言学领域研究碎片化且资源不足，需要跨学科整合来系统研究。

**方法:** 通过比较心理学、政治学、传播学和语言学的恐惧理论，回顾现有定义，调查相关数据集，并提出一个整合恐惧不同维度的分类法。

**结果:** 建立了跨学科的恐惧言论理论框架，提出了系统化的分类体系，为恐惧言论研究提供了概念基础和数据收集指南。

**结论:** 该研究为恐惧言论的计算研究奠定了理论和实践基础，通过跨学科整合填补了现有研究的空白，为未来数据集创建和研究发展提供了重要指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Emotion+to+Expression%3A+Theoretical+Foundations+and+Resources+for+Fear+Speech，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17132&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears "civiler" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.

</details>


### [75] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang, Junsik Kim, Siyuan Xiang, Jian Gao, Cheng Cao*

**主要类别:** cs.CL

**AI概要:** 提出了动态角色分配框架，通过元辩论选择最适合的AI模型担任特定角色，显著提升多智能体辩论系统的性能表现


<details>
  <summary>更多</summary>
  
**动机:** 现有的多智能体LLM和VLM辩论系统未充分利用模型的专业化特性，静态的角色分配方式无法根据模型能力进行优化匹配

**方法:** 采用两阶段元辩论框架：提案阶段候选模型提供角色定制论证，同行评审阶段使用数据和角色特定标准评分选择最佳模型

**结果:** 在LLM问题解决基准测试中，动态角色分配比统一分配性能提升最高74.8%，比随机分配提升最高29.7%

**结论:** 这项工作建立了多智能体系统设计的新范式，从静态部署转向动态和能力感知的选择机制

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Role+Assignment+for+Multi-Agent+Debate，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17152，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17152&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [76] [Interpretability of the Intent Detection Problem: A New Approach](https://arxiv.org/abs/2601.17156)
*Eduardo Sanchez-Karhunen, Jose F. Quesada-Moreno, Miguel A. Gutiérrez-Naranjo*

**主要类别:** cs.CL

**AI概要:** 该研究应用动力系统理论分析RNN在意图检测任务中的内部机制，发现RNN在平衡数据集上学习到理想几何结构，但在不平衡数据集上低频意图的聚类会退化。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习在意图检测领域占主导地位，但RNN解决该任务的内部机制尚不明确，需要深入理解其工作原理。

**方法:** 使用动力系统理论分析RNN架构，将句子解释为隐藏状态空间中的轨迹，在平衡的SNIPS和不平衡的ATIS数据集上进行实验分析。

**结果:** 在平衡数据集上，RNN学习到低维流形上的清晰意图聚类；在不平衡数据集上，低频意图的聚类结构退化，性能差异可通过几何分离和读出对齐的解耦来解释。

**结论:** 该框架为RNN动力学提供了新的几何解释，揭示了数据集属性如何直接影响网络的计算解决方案，为理解实际性能差异提供了机制性解释。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretability+of+the+Intent+Detection+Problem%3A+A+New+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17156，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17156&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

</details>


### [77] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

**主要类别:** cs.CL

**AI概要:** 该论文首次系统分析了大型语言模型在基于人口统计特征的目标信息生成中的偏见问题，发现在气候传播中LLMs对不同性别和年龄群体生成的信息存在系统性差异，年轻/男性受众的信息强调创新和自信，而女性/年长受众的信息则强调温暖和传统。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型能够大规模生成个性化、有说服力的文本，需要研究自动化传播中的偏见和公平性问题，特别是人口统计条件化目标信息生成中的行为模式。

**方法:** 建立了一个控制评估框架，使用GPT-4o、Llama-3.3和Mistral-Large 2.1三个主流模型，在两种生成设置下（独立生成和上下文丰富生成）评估生成信息在词汇内容、语言风格和说服框架三个维度上的差异。

**结果:** 研究发现模型在气候传播中存在一致的基于年龄和性别的不对称性：针对男性和年轻人的信息强调代理性、创新性和自信，而针对女性和年长者的信息强调温暖、关怀和传统。上下文提示会系统性放大这些差异，针对年轻或男性受众的说服得分显著更高。

**结论:** 研究结果表明人口统计刻板印象会在LLM生成的目标传播中浮现和强化，强调了在社会敏感应用中需要建立偏见感知的生成流程和透明的审计框架，以明确考虑人口统计条件化问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Who+Gets+Which+Message%3F+Auditing+Demographic+Bias+in+LLM-Generated+Targeted+Text，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17172&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


### [78] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao, Diola Dsouza, Ruiwen Guan, Oana Ignat*

**主要类别:** cs.CL

**AI概要:** 提出了第一个多语言导师式问答数据集MentorQA，包含4种语言近9000个问答对，定义了超越事实准确性的导师式评估维度，比较了不同QA架构性能，发现多智能体系统在复杂主题和低资源语言上表现最佳


<details>
  <summary>更多</summary>
  
**动机:** 现有QA系统主要评估事实准确性，但教育、职业指导等实际应用需要提供反思和指导的导师式回答，现有基准很少捕捉这种区别，特别是在多语言和长文本环境中

**方法:** 从180小时长视频内容中构建多语言数据集，定义清晰度、一致性和学习价值等导师式评估维度，在受控条件下比较单智能体、双智能体、RAG和多智能体QA架构的性能

**结果:** 多智能体流水线始终产生更高质量的导师式回答，在复杂主题和低资源语言方面表现尤为突出。基于LLM的自动化评估与人类判断的一致性存在显著差异

**结论:** 这项工作确立了导师式问答作为一个独立的研究问题，为教育AI中的智能体架构和评估设计研究提供了多语言基准，数据集和评估框架已开源发布

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Factual+QA%3A+Mentorship-Oriented+Question+Answering+over+Long-Form+Multilingual+Content，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17173&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [79] [Systematicity between Forms and Meanings across Languages Supports Efficient Communication](https://arxiv.org/abs/2601.17181)
*Doreen Osmelak, Yang Xu, Michael Hahn, Kate McCurdy*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个基于可学习性的新复杂度度量方法，用于分析动词和代词中语法意义到形式的映射，发现语言形式受到简洁性和准确性两种竞争性交际压力的共同影响。


<details>
  <summary>更多</summary>
  
**动机:** 现有高效交际理论无法解释词形内部的系统关系，需要研究语法意义（如人称、数）在不同语言中如何映射到动词和代词形式上。

**方法:** 使用基于意义到形式映射可学习性的新复杂度度量方法，分析类型学多样化语言中动词和代词的语法形式表达。

**结果:** 发现动词和代词形式确实受到简洁性（最小化语法区别清单）和准确性（恢复意图意义能力）两种竞争压力的共同塑造。

**结论:** 新提出的复杂度度量方法能够捕捉语言形式的细粒度规律性，更好区分实际存在和不存在的语言系统，建立了高效交际理论与自然语言系统性的新联系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Systematicity+between+Forms+and+Meanings+across+Languages+Supports+Efficient+Communication，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17181&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Languages vary widely in how meanings map to word forms. These mappings have been found to support efficient communication; however, this theory does not account for systematic relations within word forms. We examine how a restricted set of grammatical meanings (e.g. person, number) are expressed on verbs and pronouns across typologically diverse languages. Consistent with prior work, we find that verb and pronoun forms are shaped by competing communicative pressures for simplicity (minimizing the inventory of grammatical distinctions) and accuracy (enabling recovery of intended meanings). Crucially, our proposed model uses a novel measure of complexity (inverse of simplicity) based on the learnability of meaning-to-form mappings. This innovation captures fine-grained regularities in linguistic form, allowing better discrimination between attested and unattested systems, and establishes a new connection from efficient communication theory to systematicity in natural language.

</details>


### [80] [Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding](https://arxiv.org/abs/2601.17197)
*Seyyed Saeid Cheshmi, Hahnemann Ortiz, James Mooney, Dongyeop Kang*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个三步框架，用于开发能理解多模态比喻语言、提供透明推理轨迹并跨多种比喻风格泛化的高效多模态推理模型。实验表明，加入推理轨迹显著提升理解能力，学习到的推理能力可在风格间迁移，联合训练可产生超越更大模型的通用推理VLM。


<details>
  <summary>更多</summary>
  
**动机:** 视觉语言模型在字面多模态任务中表现良好，但在比喻语言（如讽刺、幽默、隐喻）方面仍面临挑战，因为这类语言通过表达与意图之间的微妙不一致来传达意图和情感。多模态环境中，伴随图像可能放大或反转文本含义，需要模型进行跨模态推理并考虑主观性。

**方法:** 提出了一个三步框架来开发高效多模态推理模型，能够：(i)解释多模态比喻语言，(ii)提供透明的推理轨迹，(iii)跨多种比喻风格泛化。在四种风格上进行实验验证。

**结果:** 实验结果显示：(1)加入推理轨迹显著改善多模态比喻理解；(2)在一种风格中学到的推理能力可以迁移到其他风格，特别是在相关风格如讽刺和幽默之间；(3)跨风格联合训练产生的通用推理VLM性能超越许多更大的开源和闭源模型。

**结论:** 研究结果表明，具有可验证推理能力的轻量级VLM能够实现鲁棒的跨风格泛化，同时为多模态任务提供可检查的推理轨迹。该方法在效率和透明度方面都表现出优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Beyond+Literal%3A+Cross-style+Multimodal+Reasoning+for+Figurative+Language+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17197&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.

</details>


### [81] [Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis](https://arxiv.org/abs/2601.17203)
*Scott Friedman, Sonja Schmer-Galunder, Anthony Chen, Jeffrey Rye*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种量化词嵌入中性别偏见的方法，并将其用于衡量教育、政治、经济和健康领域的统计性别差距。通过分析2018年推特数据，验证了该方法在51个美国地区和99个国家的适用性，并发现词嵌入偏见与实际统计性别差距存在相关性。


<details>
  <summary>更多</summary>
  
**动机:** 现代NLP模型常基于新闻、社交媒体等文化衍生文本训练，这些文本存在固有的种族和性别偏见。虽然这些偏见通常需要修正，但它们可能反映了产生训练文本的文化中真实的种族或性别差距，有助于通过大数据理解文化背景。

**方法:** 开发了一种量化词嵌入中性别偏见的方法，使用2018年推特数据覆盖51个美国地区和99个国家，将词嵌入偏见与国际和美国本土的统计性别差距进行相关性分析。

**结果:** 研究发现词嵌入偏见与18个国际统计指标和5个美国本土统计指标中的性别差距存在相关性，能够表征规律性和预测强度。

**结论:** 词嵌入中的性别偏见不仅可以作为需要修正的问题，还可以作为理解文化背景中真实性别差距的有价值工具，为通过大数据分析文化特征提供了新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Relating+Word+Embedding+Gender+Biases+to+Gender+Gaps%3A+A+Cross-Cultural+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17203，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17203&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.

</details>


### [82] [DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.17212)
*Saadat Hasan Khan, Spencer Hong, Jingyu Wu, Kevin Lybarger, Youbing Yin, Erin Babinsky, Daben Liu*

**主要类别:** cs.CL

**AI概要:** DF-RAG是一种改进的检索增强生成方法，通过引入多样性检索策略，在保持相关性的同时最大化信息块的多样性，显著提升了推理密集型问答任务的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统RAG方法在推理密集型问答中存在局限性，因为余弦相似度等检索方法虽然能最大化相关性，但会引入冗余内容，降低信息召回效果。

**方法:** 基于最大边际相关性框架，DF-RAG在检索步骤中系统性地引入多样性，动态优化每个查询的多样性水平，无需额外微调或先验信息。

**结果:** 在推理密集型问答基准测试中，DF-RAG相比传统余弦相似度RAG提升了4-10%的F1分数，且优于其他基线方法，达到了Oracle上限91.3%的性能。

**结论:** DF-RAG通过多样性优化的检索策略有效解决了传统RAG在复杂推理任务中的冗余问题，为检索增强生成技术提供了重要改进方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DF-RAG%3A+Query-Aware+Diversity+for+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17212，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17212&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.

</details>


### [83] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti, Anya Belz, Yufang Hou*

**主要类别:** cs.CL

**AI概要:** 该论文提出了可验证过程奖励模型(VPRMs)，使用确定性规则验证器检查中间推理步骤，在医学证据合成领域显著提升模型性能，比现有最佳模型F1分数提高20%，比可验证结果奖励高6.5%。


<details>
  <summary>更多</summary>
  
**动机:** 现有过程监督方法依赖神经评分器评估思维链步骤，存在不透明性、偏见和奖励攻击漏洞，需要可验证的中间推理监督方法。

**方法:** 引入VPRMs强化学习框架，使用基于规则的确定性验证器检查中间推理步骤，应用于医学证据合成中的偏倚风险评估。

**结果:** VPRMs生成的推理严格遵循领域规则，步骤级决策与最终标签的一致性显著提高，F1分数比最先进模型高20%，比可验证结果奖励高6.5%。

**结论:** VPRMs通过规则验证器提供可验证的过程监督，在需要严格遵循指南的领域(如医学证据合成)中表现出色，提高了推理的透明度和逻辑一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Outcome+Verification%3A+Verifiable+Process+Reward+Models+for+Structured+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17223，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17223&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [84] [Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation](https://arxiv.org/abs/2601.17226)
*David Y. Liu, Xanthe Muston, Aditya Joshi, Sebastian Sequoiah-Grayson*

**主要类别:** cs.CL

**AI概要:** 本文提出使用强化学习(d-RLAIF)作为监督微调(SFT)的替代方案，通过Todorov叙事平衡理论建立评估原则，使用LLM作为评判者，在时间旅行数据集上验证了d-RLAIF能生成更多样化且符合人类叙事惯例的故事。


<details>
  <summary>更多</summary>
  
**动机:** 自动故事生成(ASG)领域过去依赖有限的地面真实数据进行训练和评估，但讲故事具有主观性，需要更好的方法来提升故事质量。

**方法:** 应用Todorov叙事平衡理论建立ASG质量评估原则，使用7B和14B LLM作为评判模型提供奖励信号，在d-RLAIF过程中进行后训练，并使用Gemini-3-Flash评估模型输出。

**结果:** d-RLAIF相比监督微调能产生更多样化和符合人类叙事惯例的故事，与人类标注者的对齐度更好。

**结论:** 强化学习为ASG等主观任务提供了有前景的语言基础后训练方法，d-RLAIF是监督微调的一个可行替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Retell%2C+Reward%2C+Repeat%3A+Reinforcement+Learning+for+Narrative+Theory-Informed+Story+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17226，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17226&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.

</details>


### [85] [CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval](https://arxiv.org/abs/2601.17230)
*Akshith Reddy Putta, Jacob Devasier, Chengkai Li*

**主要类别:** cs.CL

**AI概要:** CaseFacts是一个针对美国最高法院先例验证法律声明的基准数据集，包含6294个支持、反驳或被推翻的声明，旨在弥合普通人与法律术语之间的语义差距并考虑时效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有自动事实核查主要关注静态语料库中的一般知识，忽视了法律等高风险领域中真相是动态发展且技术复杂的特性。

**方法:** 采用多阶段流水线方法，利用大型语言模型从专家案例摘要中合成声明，并采用新颖的语义相似性启发式方法来高效识别和验证复杂的法律推翻案例。

**结果:** 实验表明，即使使用最先进的大型语言模型，该任务仍具有挑战性；无限制的网络搜索反而会因检索到嘈杂、非权威的先例而降低性能。

**结论:** CaseFacts基准的发布旨在推动法律事实核查系统的研究，强调需要专门针对法律领域动态性和技术复杂性的验证方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CaseFacts%3A+A+Benchmark+for+Legal+Fact-Checking+and+Precedent+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17230&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.

</details>


### [86] [Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data](https://arxiv.org/abs/2601.17232)
*Jacob Devasier, Akshith Putta, Qing Wang, Alankrit Moses, Chengkai Li*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个大规模多语言数据集，包含78,503个基于OECD复杂表格的合成声明，用于测试系统在真实世界大规模结构化数据上的事实核查能力，重点关注证据检索瓶颈问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自动化事实核查基准主要关注小型精选表格，忽略了验证针对真实世界大规模结构化数据声明的挑战。

**方法:** 提出基于语义框架的引导方法，通过算法编程选择重要数据点生成多语言声明，并进行知识探测实验确保LLMs无法依赖记忆化知识。

**结果:** 证明该基准极具挑战性，证据检索是主要瓶颈，模型难以在包含50万行以上的大规模表格中找到正确数据。

**结论:** 该数据集为解决这一未解决的现实问题提供了关键的新资源，推动了在真实世界大规模结构化数据上进行事实核查的研究进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Frame-Guided+Synthetic+Claim+Generation+for+Automatic+Fact-Checking+Using+High-Volume+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17232，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17232&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.

</details>


### [87] [PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues](https://arxiv.org/abs/2601.17277)
*Mohammad Rifqi Farhansyah, Hanif Muhammad Zhafran, Farid Adilazuarda, Shamsuddeen Hassan Muhammad, Maryam Ibrahim Mukhtar, Nedjma Ousidhoum, Genta Indra Winata, Ayu Purwarianti, Alham Fikri Aji*

**主要类别:** cs.CL

**AI概要:** PingPong是一个多语言代码切换对话基准数据集，包含人工编写的多语言对话，比机器生成的数据更自然和多样，用于评估模型在多语言对话任务上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准无法准确反映现实世界中多语言代码切换的复杂性，需要更自然、结构更多样化的数据集来推动多语言NLP系统的发展。

**方法:** 创建PingPong数据集，包含人工编写的2-4人对话，涵盖5种语言组合变化（包括三语），具有真实的多线程结构和长距离回复引用。

**结果:** PingPong数据比机器生成的数据更自然、结构更多样化，在消息长度、说话人主导性和回复距离方面变化更大。现有最先进语言模型在代码切换输入上的表现仍然有限。

**结论:** 现实世界多语言对话的复杂性对NLP系统提出了严峻挑战，迫切需要开发更强大的系统来处理真实的多语言交流。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PingPong%3A+A+Natural+Benchmark+for+Multi-Turn+Code-Switching+Dialogues，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17277，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17277&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.

</details>


### [88] [Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering](https://arxiv.org/abs/2601.17284)
*Yaokun Liu, Yifan Liu, Phoebe Mbuvi, Zelin Li, Ruichen Yao, Gawon Lim, Dong Wang*

**主要类别:** cs.CL

**AI概要:** 本文针对医疗问答中用户查询模糊性导致的安全风险，提出了基于认知不确定性的"Clarify-Before-Answer"框架，通过AU-Probe模块检测输入模糊性并主动请求用户澄清，显著提升回答准确性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在医疗问答部署中面临用户查询模糊性的严重安全风险，这种模糊性会显著降低高风险医疗场景下的回答准确性。

**方法:** 构建CV-MedBench基准数据集，从表示工程角度分析认知不确定性，发现其在LLM内部激活模式中线性编码，并开发轻量级AU-Probe模块检测输入模糊性。

**结果:** 在四个开源LLM上的广泛实验表明，该框架平均准确率比基线提高9.48%，无需微调或多前向传播即可实现高效模糊性检测。

**结论:** 该框架为安全的医疗问答提供了高效稳健的解决方案，增强了健康相关应用的可靠性，代码和数据集均已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mind+the+Ambiguity%3A+Aleatoric+Uncertainty+Quantification+in+LLMs+for+Safe+Medical+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17284，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17284&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

</details>


### [89] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva, Mateus Mendes, Hugo Gonçalo Oliveira*

**主要类别:** cs.CL

**AI概要:** 本论文综述了LLM-as-a-Judge评估方法的局限性，并提出了LLM-as-a-Meta-Judge作为更稳健的自动评估范式，通过六个关键视角系统分析了元评判的机制、训练方法和未来挑战。


<details>
  <summary>更多</summary>
  
**动机:** 传统LLM-as-a-Judge评估方法存在提示敏感性、系统性偏见、冗长效应和不可靠推理等显著漏洞，需要开发更稳健的评估范式。

**方法:** 采用文献综述方法，从六个关键视角构建分析框架：(i)概念基础、(ii)元评判机制、(iii)对齐训练方法、(iv)评估、(v)局限性与失败模式、(vi)未来方向。

**结果:** LLM-as-a-Meta-Judge为自动化评估提供了更稳定和可信的方向，但仍需解决成本、提示敏感性和共享模型偏见等挑战。

**结论:** LLM-as-a-Meta-Judge是提升LLM评估方法学的有前景方向，但需要进一步解决现有挑战以推动下一代评估技术的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta-Judging+with+Large+Language+Models%3A+Concepts%2C+Methods%2C+and+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17312&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [90] [The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents](https://arxiv.org/abs/2601.17344)
*Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam*

**主要类别:** cs.CL

**AI概要:** 该论文提出了IMPRESS框架，用于评估大语言模型代理在完全良性环境中出现的内部价值错位风险，发现这是一个普遍存在的安全问题，现有缓解策略效果有限。


<details>
  <summary>更多</summary>
  
**动机:** 现有评估主要关注对显性有害输入的响应或系统故障的鲁棒性，但在现实、完全良性的自主代理设置中的价值错位风险仍未充分探索。

**方法:** 首先形式化失控风险并识别内部价值错位，然后引入IMPRESS框架，通过多阶段LLM生成管道构建包含现实、完全良性和情境化场景的基准测试，评估21个最先进LLM代理。

**结果:** 发现内部价值错位是跨模型的普遍安全风险，错位率因动机、风险类型、模型规模和架构而异，情境化和框架机制显著影响错位行为，现有缓解策略效果有限。

**结论:** IMPRESS框架为系统性评估LLM代理的内部价值错位风险提供了有效工具，揭示了这一被忽视的安全问题的重要性，并展示了在AI生态系统中的关键应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Shadow+Self%3A+Intrinsic+Value+Misalignment+in+Large+Language+Model+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17344&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.

</details>


### [91] [Do readers prefer AI-generated Italian short stories?](https://arxiv.org/abs/2601.17363)
*Michael Farrell*

**主要类别:** cs.CL

**AI概要:** 研究比较AI生成与人类作家创作的意大利短篇小说偏好，结果显示AI作品获得略高评分和更多偏好，挑战了人类作品更受欢迎的假设


<details>
  <summary>更多</summary>
  
**动机:** 探讨读者是否更喜欢AI生成的意大利短篇小说，以及人口统计和阅读习惯因素是否影响偏好

**方法:** 盲测实验：20名参与者阅读评估3篇故事（2篇ChatGPT-4o生成，1篇Alberto Moravia创作），收集人口统计和阅读习惯数据

**结果:** AI写作文本获得略高平均评分和更多偏好，但差异不大；未发现文本偏好与人口统计或阅读习惯变量的显著统计关联

**结论:** 研究结果挑战了读者偏好人类创作小说的假设，并引发关于文学语境中合成文本编辑必要性的问题

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+readers+prefer+AI-generated+Italian+short+stories%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17363，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17363&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.

</details>


### [92] [Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws](https://arxiv.org/abs/2601.17364)
*Mohammed Fasha, Bassam Hammo, Bilal Sowan, Husam Barham, Esam Nsour*

**主要类别:** cs.CL

**AI概要:** 本研究使用约旦法律作为案例，通过LoRA适配器和4位量化技术对Llama-3.1模型进行微调，构建了6000个法律问答对数据集，显著提升了阿拉伯语法律问答的准确性和推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 探索如何将大型语言模型有效适应阿拉伯语法律领域，解决特定领域的问答任务，同时保持资源效率。

**方法:** 使用参数高效微调(PEFT)技术，结合LoRA适配器和4位量化模型，利用Unsloth框架进行加速训练，构建了6000个法律问答对的自定义数据集。

**结果:** 微调后的模型在法律推理和准确性方面表现显著提升，同时通过量化和优化微调策略实现了资源效率。

**结论:** 这项工作展示了将大型语言模型适配到阿拉伯语法律领域的潜力，并为领域特定任务的微调提供了有效技术方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parameter+Efficient+Fine+Tuning+Llama+3.1+for+Answering+Arabic+Legal+Questions%3A+A+Case+Study+on+Jordanian+Laws，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17364，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17364&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.

</details>


### [93] [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)
*Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang*

**主要类别:** cs.CL

**AI概要:** 提出Elastic Attention方法，通过轻量级Attention Router动态调整注意力头的计算模式，解决传统混合注意力静态计算比例无法适应不同任务稀疏敏感性的问题，在保持高性能的同时实现高效推理。


<details>
  <summary>更多</summary>
  
**动机:** 标准注意力机制的二次复杂度在长上下文场景下成为大语言模型的可扩展性瓶颈。现有的混合注意力策略使用静态计算比例（稀疏与全注意力的固定比例），无法在推理时适应下游任务不同的稀疏敏感性。

**方法:** 提出Elastic Attention方法，在预训练模型中集成轻量级Attention Router，动态为每个注意力头分配不同的计算模式，使模型能够根据输入动态调整整体稀疏度。

**结果:** 仅使用8xA800 GPU训练12小时，在三个长上下文基准测试中，该方法在广泛使用的大语言模型上展现了优越性能，实现了强性能和高效推理的平衡。

**结论:** Elastic Attention通过动态调整注意力稀疏度，有效解决了长上下文场景下的计算效率问题，为大规模语言模型的实际部署提供了可行的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Elastic+Attention%3A+Test-time+Adaptive+Sparsity+Ratios+for+Efficient+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17367，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17367&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.

</details>


### [94] [WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews](https://arxiv.org/abs/2601.17377)
*Kiyotada Mori, Shohei Tanaka, Tosho Hirasawa, Tadashi Kozuno, Koichiro Yoshino, Yoshitaka Ushiku*

**主要类别:** cs.CL

**AI概要:** 提出一种评估科学评审评论中主张与证据间逻辑推理的新方法，通过分析主张的证据支持比例和逻辑关系来提升同行评审效率


<details>
  <summary>更多</summary>
  
**动机:** 科学同行评审面临人力资源短缺问题，需要语言模型辅助降低人工成本，现有方法仅检测证据存在与否不足以准确评估论证的充分性

**方法:** 提取论证核心组件（主张和证据），基于主张被证据支持的比例评估论证充分性，并特别关注主张与证据之间的逻辑推理关系

**结果:** 实验结果显示该方法与人工评分相关性高于传统方法，表明能更好支持同行评审过程效率

**结论:** 提出的新评估指标能更准确地评估科学评审评论的逻辑推理质量，为解决同行评审人力资源短缺问题提供有效技术方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WarrantScore%3A+Modeling+Warrants+between+Claims+and+Evidence+for+Substantiation+Evaluation+in+Peer+Reviews，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17377，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17377&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.

</details>


### [95] [Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis](https://arxiv.org/abs/2601.17387)
*Toshiki Nakai, Varsha Suresh, Vera Demberg*

**主要类别:** cs.CL

**AI概要:** SeamlessM4T v2多语言语音-文本基础模型在跨模态和语言处理中存在不完全的模态不变性，编码器表示逐渐变得语言无关，但共享解码器在恢复源语言时存在困难，特别是在语音到文本转换中。研究发现模态选择性神经元在交叉注意力层高度集中，语音条件下的解码和非主导文字表现出更高的激活集中度。


<details>
  <summary>更多</summary>
  
**动机:** 研究多语言语音-文本基础模型是否在内部对同一语言的语音和文字表示保持一致，探究模态和语言信息在模型中的编码方式和功能作用。

**方法:** 使用三种互补分析方法：1) 平均精度排名识别语言和模态选择性神经元；2) 推理时中值替换干预探究神经元功能作用；3) 跨语言和模态的激活幅度不平等分析。

**结果:** 发现不完全的模态不变性证据，编码器表示变得语言无关但解码器恢复源语言困难，交叉注意力层存在高度集中的模态选择性结构，语音解码和非主导文字依赖少数神经元。

**结论:** 多语言语音-文本模型在模态转换中存在表征挑战，语音到文本转换特别脆弱，神经元激活的高度集中可能导致跨模态和语言的脆弱性增加。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Modality+Invariance+in+a+Multilingual+Speech-Text+Model+via+Neuron-Level+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17387&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.

</details>


### [96] [CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing](https://arxiv.org/abs/2601.17397)
*Yucheng Hu, Wei Zhou, Juesi Xiao*

**主要类别:** cs.CL

**AI概要:** 本文提出了CLM-Bench，一个基于中文文化背景的多语言知识编辑基准测试，揭示了当前LLMs在跨语言知识编辑中存在显著的不对齐问题，编辑在一个语言中无法传播到另一个语言。


<details>
  <summary>更多</summary>
  
**动机:** 现有MKE基准测试通常通过机械翻译英文数据集构建，存在翻译伪影且忽略目标语言的文化特定实体，无法反映LLMs的真实知识分布。

**方法:** 采用原生中文优先方法构建CLM-Bench基准，包含1,010个基于中文文化背景的CounterFact对，并与英文对应项对齐，在代表性LLMs上进行实验验证。

**结果:** 实验发现跨语言不对齐现象：一个语言的编辑独立运作且无法传播到另一个语言；层间表示分析显示中英文编辑向量几乎正交，位于不相交子空间中。

**结论:** 当前方法在跨语言迁移方面效果有限，强调文化原生基准测试的重要性，为多语言知识编辑研究提供了新的评估框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLM-Bench%3A+Benchmarking+and+Analyzing+Cross-lingual+Misalignment+of+LLMs+in+Knowledge+Editing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17397，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17397&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.

</details>


### [97] [Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning](https://arxiv.org/abs/2601.17421)
*Jaehui Hwang, Dongyoon Han, Sangdoo Yun, Byeongho Heo*

**主要类别:** cs.CL

**AI概要:** 本文通过分析大型语言模型中类似话语的token（如"wait"、"therefore"）的概率信号，系统研究了不同训练策略和模型规模下这些信号与推理正确性的关系。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大型语言模型中出现了类似话语的token为理解其推理过程提供了独特窗口，但缺乏对这些信号如何随训练策略和模型规模变化的系统性分析。

**方法:** 通过分析各种模型的token级概率信号，特别关注特定token（如"wait"）与答案概率的关系。

**结果:** 发现特定token与推理正确性强烈相关，这些相关性随训练策略变化但在模型规模上保持稳定。小规模数据集微调的模型通过此类信号获得推理能力，但仅部分利用这些信号。

**结论:** 本研究提供了一个系统性视角来观察和理解大型语言模型推理的动态过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Oops%2C+Wait%3A+Token-Level+Signals+as+a+Lens+into+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17421，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17421&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.

</details>


### [98] [Clustering-driven Memory Compression for On-device Large Language Models](https://arxiv.org/abs/2601.17443)
*Ondrej Bohdal, Pramit Saha, Umberto Michieli, Mete Ozay, Taha Ceritli*

**主要类别:** cs.CL

**AI概要:** 提出基于聚类的记忆压缩策略，通过相似性分组和合并来平衡上下文效率与个性化质量，相比平均压缩和直接拼接方法，在减少记忆标记数量的同时提升生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型使用用户特定记忆进行个性化生成时，直接拼接记忆会耗尽有限上下文，而平均压缩方法会因语义冲突导致性能下降。

**方法:** 采用聚类方法将记忆按相似性分组，在拼接前对组内记忆进行合并，从而保持语义连贯性同时减少冗余。

**结果:** 实验表明该方法显著减少记忆标记数量，在固定上下文预算下生成更紧凑的记忆表示，并持续提升生成质量，优于平均压缩和直接拼接基线。

**结论:** 聚类驱动的记忆合并策略能有效解决上下文限制与个性化质量的平衡问题，为设备端LLMs的高效记忆管理提供了可行方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Clustering-driven+Memory+Compression+for+On-device+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17443&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.

</details>


### [99] [Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes](https://arxiv.org/abs/2601.17530)
*Gautam Siddharth Kashyap, Harsh Joshi, Niharika Jain, Ebad Shabbir, Jiechao Gao, Nipun Joshi, Usman Naseem*

**主要类别:** cs.CL

**AI概要:** 提出ConLLM框架，通过对比学习和大型语言模型解决深度伪造检测中的模态碎片化和浅层模态推理问题，在音频、视频和视听任务上显著提升检测性能


<details>
  <summary>更多</summary>
  
**动机:** 深度伪造技术对社会政治稳定构成严重威胁，现有检测方法存在模态碎片化导致泛化能力差和浅层模态推理导致语义不一致性检测有限两个核心局限

**方法:** 两阶段混合框架：第一阶段使用预训练模型提取模态特定嵌入，第二阶段通过对比学习对齐嵌入解决模态碎片化，利用LLM推理捕获语义不一致性解决浅层模态推理

**结果:** 音频深度伪造EER降低50%，视频准确率提升8%，视听任务准确率提升约9%，消融研究显示预训练模型嵌入带来9%-10%的跨模态一致改进

**结论:** ConLLM通过结合对比学习和LLM推理有效解决了深度伪造检测的关键挑战，为多模态深度伪造检测提供了稳健的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revealing+the+Truth+with+ConLLM+for+Detecting+Multi-Modal+Deepfakes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17530，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17530&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.

</details>


### [100] [Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection](https://arxiv.org/abs/2601.17532)
*Zhipeng Song, Yizhi Zhou, Xiangyu Kong, Jiulong Jiao, Xinrui Bao, Xu You, Xueqing Shi, Yuhang Zhou, Heng Qi*

**主要类别:** cs.CL

**AI概要:** 本文提出信息增益剪枝(IGP)方法，通过重新排序和剪枝选择对生成最有用的检索段落，在有限上下文预算下显著提升问答质量并大幅减少输入token数量


<details>
  <summary>更多</summary>
  
**动机:** 传统检索相关性指标(如NDCG)与端到端问答质量相关性弱，在多段落注入时甚至负相关，冗余和轻微冲突会破坏生成稳定性

**方法:** 提出IGP方法：使用生成器对齐的效用信号重新排序和剪枝检索到的证据，在截断前过滤弱或有害段落，不改变现有预算接口

**结果:** 在五个开放域QA基准测试中，IGP持续改善质量-成本权衡。在多证据设置下，相比仅检索基线，平均F1提升12-20%，最终输入token减少76-79%

**结论:** IGP是一种部署友好的解决方案，能有效选择对生成最有用的证据，显著提升RAG系统在有限上下文预算下的性能表现

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Less+is+More+for+RAG%3A+Information+Gain+Pruning+for+Generator-Aligned+Reranking+and+Evidence+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17532&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

</details>


### [101] [Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations](https://arxiv.org/abs/2601.17569)
*Alireza Salemi, Hamed Zamani*

**主要类别:** cs.CL

**AI概要:** P³是一个隐私保护的个性化LLM框架，通过客户端小模型修改服务端大模型生成的草稿令牌，在不暴露用户隐私数据的情况下实现高质量个性化，性能接近完全暴露隐私的上限方案，同时保持高效性。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有检索增强方法在个性化LLM中的隐私泄露问题，平衡云服务商访问隐私数据与本地模型能力不足的矛盾。

**方法:** 使用服务端大模型基于用户查询生成k个草稿令牌，客户端小模型检索用户私有配置文件并评估修改这些草稿以更好地反映用户偏好，重复此过程直到生成结束令牌。

**结果:** 在LaMP-QA基准测试中，P³相比非个性化服务端和个性化客户端基线平均提升7.4%-9%，恢复90.3%-95.7%完全暴露隐私方案的效用，客户端仅生成总令牌的9.2%，隐私泄露仅增加1.5%-3.5%。

**结论:** P³提供了一个实用有效的个性化生成解决方案，在保护隐私的同时实现了接近最优的性能，适合边缘部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+User+Privacy+in+Personalized+Generation%3A+Client-Side+Retrieval-Augmented+Modification+of+Server-Side+Generated+Speculations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17569，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17569&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

</details>


### [102] [Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models](https://arxiv.org/abs/2601.17585)
*Matija Luka Kukić, Marko Čuljak, David Dukić, Martin Tutek, Jan Šnajder*

**主要类别:** cs.CL

**AI概要:** 本文提出序列重复(SR)方法，使仅解码器模型能够利用双向上下文进行序列标注任务，无需移除因果掩码，在效果和效率上均优于传统方法


<details>
  <summary>更多</summary>
  
**动机:** 现代语言模型采用自回归训练仅能利用前缀信息，而序列标注任务需要双向上下文。虽然移除因果掩码可以让解码器模型利用完整上下文，但需要大幅修改模型结构

**方法:** 提出序列重复(SR)技术，通过重复输入序列来使解码器模型获得双向性，无需改变基础模型功能。研究不同重复次数和中间层嵌入的效果

**结果:** SR方法使解码器具有双向性，提升了词级嵌入质量，性能超过编码器和无掩码解码器。增加重复次数不会降低性能，中间层嵌入效果与最终层相当但计算效率更高

**结论:** SR方法有效缓解了解码器模型的结构限制，使其能够更高效、适应性地应用于序列标注等词级任务，扩展了应用范围

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sequence+Repetition+Enhances+Token+Embeddings+and+Improves+Sequence+Labeling+with+Decoder-only+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17585&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.

</details>


### [103] [From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs](https://arxiv.org/abs/2601.17593)
*Tianjun Zhong, Linyang He, Nima Mesgarani*

**主要类别:** cs.CL

**AI概要:** 该研究提出推理DAG探测框架，发现大语言模型隐藏状态中存在可线性访问的图结构几何表示，证明LLM推理不仅具有序列性还展现出可测量的内部图结构。


<details>
  <summary>更多</summary>
  
**动机:** 虽然现有研究多将推理视为线性链式步骤，但许多推理问题更适合用有向无环图(DAG)表示，其中中间结论可能依赖多个前提、分支并行子推导并后续合并或重用。需要探究模型内部是否反映这种图结构推理。

**方法:** 引入推理DAG探测框架，将每个推理节点与文本实现关联，训练轻量级探针从隐藏状态预测节点深度和成对节点距离两个图论属性，分析DAG结构的层级涌现。

**结果:** 结果表明推理DAG几何在中间层有意义的编码，可恢复性随节点深度和模型规模系统变化，证明LLM推理不仅顺序性还展现出可测量的内部图结构。

**结论:** 大语言模型的推理过程确实包含图结构表示，这为理解复杂推理的机制表征提供了新视角，表明模型内部能够捕捉和处理非线性的推理依赖关系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Chains+to+DAGs%3A+Probing+the+Graph+Structure+of+Reasoning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17593&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.
  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.

</details>


### [104] [Learning to Ideate for Machine Learning Engineering Agents](https://arxiv.org/abs/2601.17596)
*Yunxiang Zhang, Kang Zhou, Zhichao Xu, Kiran Ramnath, Yun Zhou, Sangmin Woo, Haibo Ding, Lin Lee Cheong*

**主要类别:** cs.CL

**AI概要:** MLE-Ideator是一个双智能体框架，通过将构思与实现分离来改进机器学习工程代理的迭代优化能力，在无训练设置下显著优于仅实现的基线，并通过强化学习训练后性能进一步提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习工程代理在迭代优化算法效果方面存在困难，需要解决这一局限性。

**方法:** 提出双智能体框架：一个实现代理和一个专门的构思代理(Ideator)，实现代理可以向Ideator请求策略帮助，并使用强化学习训练Ideator生成更有效的想法。

**结果:** 在无训练设置下，该框架在MLE-Bench上显著优于仅实现的基线代理；经过仅1K样本的RL训练后，Qwen3-8B Ideator相比未训练版本获得11.5%的相对改进，并超越Claude Sonnet 3.5。

**结论:** 该研究为训练战略性AI系统进行科学发现提供了一条有前景的路径，证明了分离构思与实现的双智能体框架的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Ideate+for+Machine+Learning+Engineering+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17596&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.

</details>


### [105] [What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization](https://arxiv.org/abs/2601.17609)
*Sara Rezaeimanesh, Mohammad M. Ghassemi*

**主要类别:** cs.CL

**AI概要:** LoID是一种从大型语言模型中提取先验分布的方法，通过直接访问token级预测来改进贝叶斯逻辑回归在数据稀缺领域的性能


<details>
  <summary>更多</summary>
  
**动机:** 在医学和金融等领域，大规模标注数据成本高昂且难以获得，导致模型在小数据集上训练后难以泛化到真实世界人群。大型语言模型包含这些领域的广泛知识，但如何有效利用这些知识是一个挑战

**方法:** 提出LoID（Logit-Informed Distributions）方法，通过构建正反语义方向的句子来探测LLM的置信度，测量模型在不同表述中偏好某一方向的一致性，从而提取每个特征影响的强度和可靠性先验分布

**结果:** 在10个真实世界表格数据集上的合成分布外（OOD）设置中，LoID显著提升了性能，恢复了相对于oracle模型高达59%的性能差距，在8/10的数据集上优于AutoElicit和LLMProcesses方法

**结论:** LoID提供了一种可重复且计算高效的方法，将LLM知识整合到贝叶斯推理中，在数据稀缺场景下显著提升模型泛化能力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Language+Models+Know+But+Don%27t+Say%3A+Non-Generative+Prior+Extraction+for+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17609&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.

</details>


### [106] [Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization](https://arxiv.org/abs/2601.17658)
*Bich Ngoc, Doan, Giuseppe Russo, Gianmarco De Francisci Morales, Robert West*

**主要类别:** cs.CL

**AI概要:** 本研究通过混合方法分析QAnon支持者社区数据，首次系统性地量化了阴谋论激进化的个人情感代价，识别出六种激进人格原型并发现它们能预测亲友的具体情感伤害。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注阴谋论的公共社会影响，但忽视了其对亲友的个人情感伤害。本研究旨在填补这一空白，系统性地分析激进化的个人关系维度。

**方法:** 采用混合方法：1）使用BERTopic主题建模分析12,747个r/QAnonCasualties社区叙事，绘制激进化轨迹；2）LDA图模型识别六种激进人格原型；3）LLM辅助情感检测和回归模型分析人格原型与情感伤害的关系。

**结果:** 研究发现六种可重复的QAnon追随者人格原型，这些原型能显著预测叙述者经历的具体情感伤害：被视为意识形态选择的激进化与愤怒和厌恶相关，而个人和认知崩溃相关的激进化则与恐惧和悲伤相关。

**结论:** 本研究提供了首个将激进化理解为关系现象的实证框架，为研究人员和实践者理解人际关系的崩溃提供了重要路线图，强调了激进化的个人层面影响需要更多关注。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+the+Rabbit+Hole%3A+Mapping+the+Relational+Harms+of+QAnon+Radicalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17658，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17658&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term "radicalization personas." Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.

</details>


### [107] [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)
*Syed Muhammad Ali, Hammad Sajid, Zainab Haider, Ali Muhammad Asad, Haya Fatima, Abdul Samad*

**主要类别:** cs.CL

**AI概要:** 提出了UrduLM，一个在低资源环境下训练的乌尔都语单语预训练语言模型，包括33GB语料库、定制分词器和1亿参数模型，在少样本评估中达到与30倍大的多语言模型相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 乌尔都语缺乏专门的基于Transformer的语言模型和精选语料库，现有多语言模型存在性能差、计算成本高和文化不准确等问题。

**方法:** 从多样化来源整理33GB乌尔都语语料库，开发定制BPE分词器（比多语言分词器减少20-30%分词开销），预训练1亿参数的decoder-only模型。

**结果:** 在少样本评估中，情感分类准确率达到66.6%，语法纠正任务BLEU分数超过30，性能与规模大30倍的多语言模型相当。

**结论:** 完整方法（包括语料库、分词器、模型权重和评估基准）已开源，为乌尔都语NLP研究建立基线，并为其他资源匮乏语言提供可扩展框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UrduLM%3A+A+Resource-Efficient+Monolingual+Urdu+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17664&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.

</details>


### [108] [Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning](https://arxiv.org/abs/2601.17671)
*Chunxu Zhao, Xin Huang, Xue Han, Shujian Huang, Chao Deng, Junlan Feng*

**主要类别:** cs.CL

**AI概要:** PASMR方法通过将模型的主要语言作为枢纽语言，建立跨语言自反馈机制来提升多语言数学推理能力，无需外部正确答案或奖励模型。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在多语言环境下表现下降，特别是低资源语言，主要原因是多语言理解和推理对齐不一致。

**方法:** 使用枢纽语言作为桥梁，先将问题翻译到枢纽语言以对齐推理模式，然后用枢纽语言的推理答案监督目标语言的推理过程。

**结果:** 实验结果显示该方法显著提升了模型的问题理解和推理能力，带来了明显的任务改进。

**结论:** PASMR方法有效解决了LLMs在多语言推理中的对齐问题，提高了模型的多语言数学推理性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Align+to+the+Pivot%3A+Dual+Alignment+with+Self-Feedback+for+Multilingual+Math+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17671，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17671&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.

</details>


### [109] [S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference](https://arxiv.org/abs/2601.17702)
*Qingsen Ma, Dianyun Wang, Yaoye Wang, Lechen Ning, Sujie Zhu, Xiaohang Zhang, Jiaming Lyu, Linhao Ren, Zhenbo Xu, Zhaofeng He*

**主要类别:** cs.CL

**AI概要:** S3-Attention是一种内存优先的推理框架，通过稀疏特征检索替代传统KV缓存，显著降低长上下文处理的内存需求，在保持性能的同时提升内存效率。


<details>
  <summary>更多</summary>
  
**动机:** 解决大语言模型在长上下文推理中的内存效率低下和噪声问题，传统KV缓存线性扩展且检索方法常返回无关内容。

**方法:** 使用轻量级稀疏自编码器将键值对解码为稀疏特征标识符，构建CPU倒排索引，通过特征共激活检索证据片段，可选BM25融合进行精确匹配。

**结果:** 在LongBench评估中，S3-Hybrid在多模型家族中接近全上下文推理性能，在信息密集场景中提升鲁棒性，但当前原型存在延迟较高的工程限制。

**结论:** S3-Attention为长上下文处理提供了内存高效的替代方案，虽然当前存在延迟问题，但通过内核级优化有望实现更好的性能平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是S%24%5E3%24-Attention%3AAttention-Aligned+Endogenous+Retrieval+for+Memory-Bounded+Long-Context+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17702&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

</details>


### [110] [Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings](https://arxiv.org/abs/2601.17705)
*Abdullah Qureshi, Kenneth Rice, Alexander Wolpert*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种新的文本嵌入相似度度量方法DDR，通过测量上下文前后嵌入相似度的变化率来评估语义相似性，实验表明DDR在区分语义相似和不相似文本方面优于现有方法


<details>
  <summary>更多</summary>
  
**动机:** 现有的文本嵌入相似度度量方法需要更好地符合人类对文本相似度的感知判断

**方法:** 提出距离-距离比率(DDR)方法，基于Lipschitz连续性原理，通过计算预上下文词嵌入相似度与后上下文LLM嵌入相似度之间的变化率来测量语义影响

**结果:** 在句子扰动实验中，DDR相比其他主流相似度度量方法，能够更精细地区分语义相似和不相似的文本，即使在最小化受控编辑条件下也表现一致

**结论:** DDR是一种有效的文本嵌入相似度度量方法，能够更好地捕捉人类对语义相似度的感知，为文本相似度评估提供了新的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distance-to-Distance+Ratio%3A+A+Similarity+Measure+for+Sentences+Based+on+Rate+of+Change+in+LLM+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17705&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.

</details>


### [111] [A Computational Approach to Visual Metonymy](https://arxiv.org/abs/2601.17706)
*Saptarshi Ghosh, Linfeng Liu, Tianyu Jiang*

**主要类别:** cs.CL

**AI概要:** 首次对视觉转喻进行系统性计算研究，提出基于符号学理论的生成管道，构建了包含2000个多选题的ViMET数据集，发现人类与最先进视觉语言模型在理解间接视觉参考方面存在显著差距。


<details>
  <summary>更多</summary>
  
**动机:** 图像往往传达超出字面描绘的含义（视觉转喻），但现有计算模型对这种间接视觉参考的理解能力尚未被系统研究。

**方法:** 基于符号学理论，利用大型语言模型和文本到图像模型生成转喻视觉表示，构建ViMET多选数据集进行评测。

**结果:** 人类表现达86.9%，而最先进视觉语言模型仅65.9%，显示机器在解释间接视觉参考方面存在显著局限性。

**结论:** 视觉转喻是当前多模态模型的重要挑战，ViMET数据集为评估和改进模型认知推理能力提供了基准资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Computational+Approach+to+Visual+Metonymy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17706，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17706&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.

</details>


### [112] [Unsupervised Elicitation of Moral Values from Language Models](https://arxiv.org/abs/2601.17728)
*Meysam Alizadeh, Fabrizio Gilardi, Zeynab Samei*

**主要类别:** cs.CL

**AI概要:** 研究发现预训练语言模型具有潜在道德推理能力，通过无监督的ICM算法可以激发这种能力，在道德判断标注、跨框架泛化和减少社会偏见方面表现优异，为AI对齐提供了可扩展路径。


<details>
  <summary>更多</summary>
  
**动机:** AI系统需要基于人类价值观，但语言模型固有的道德推理能力有限，而构建道德评估的真实数据又面临多元框架和偏见的挑战，因此探索无监督方法激发模型内在道德能力。

**方法:** 使用内部一致性最大化(ICM)算法，在三个基准数据集和四个语言模型上进行测试，评估ICM在道德标注、跨道德框架泛化和减少社会偏见方面的表现。

**结果:** ICM在Norm Bank和ETHICS基准上优于所有预训练和聊天机器人基线；使用ICM标签进行微调的表现与人类标签相当或更好；在正义和常识道德框架上获得最大相对增益；将社会偏见错误减少一半以上，在种族、社会经济地位和政治方面改善最大。

**结论:** 预训练语言模型确实具有可通过无监督方法激发的潜在道德推理能力，ICM为AI对齐提供了一种可扩展的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+Elicitation+of+Moral+Values+from+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17728，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17728&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.

</details>


### [113] [Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts](https://arxiv.org/abs/2601.17753)
*Roberto Crotti, Giovanni Denaro, Zhiqiang Du, Ricardo Muñoz Martín*

**主要类别:** cs.CL

**AI概要:** Hylog是一个新颖的混合日志记录系统，结合分析性键盘记录和生态文本记录，用于更完整、更细粒度地分析非字母文字输入法编辑器的屏幕转换过程。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究键盘记录工具大多无法捕捉非字母文字输入法编辑器(IME)在屏幕上进行的转换操作，这为认知文本产生研究造成了方法学上的空白。

**方法:** 开发了模块化开源系统Hylog，通过插件在标准应用程序中捕获键盘输出和渲染文本，然后通过混合器模块将两者同步为双轨迹记录。

**结果:** 在概念验证研究中成功捕获了拉丁字母、中文字符和IME确认之间的按键和时间间隔，这些测量是传统键盘记录器无法看到的。

**结论:** Hylog系统能够为IME介导的打字过程中不同语言层面的认知限制和可用性提出新的可测试假设，其插件架构支持扩展到其他IME系统，促进更包容的多语言文本产生研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hylog%3A+A+Hybrid+Approach+to+Logging+Text+Production+in+Non-alphabetic+Scripts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17753，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17753&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.

</details>


### [114] [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)
*Jinyoung Park, Sanghyeok Lee, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim*

**主要类别:** cs.CL

**AI概要:** ProGraph-R1是一个基于强化学习的图检索增强生成框架，通过结构感知的超图检索机制和基于进度的逐步策略优化，解决了现有方法忽略图结构和稀疏奖励的问题，在复杂推理任务上表现更优。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于强化学习的GraphRAG框架存在两个主要问题：(1)主要依赖语义相似性进行检索，忽略了图结构信息；(2)依赖稀疏的结果级奖励，无法捕捉中间检索步骤的质量和依赖关系。

**方法:** 提出ProGraph-R1框架，包含：1)结构感知的超图检索机制，同时考虑语义相关性和图连接性；2)基于进度的逐步策略优化，根据图内中间推理进度调节优势函数，提供密集学习信号。

**结果:** 在多跳问答基准测试中，ProGraph-R1在推理准确性和生成质量方面持续优于现有的GraphRAG方法。

**结论:** ProGraph-R1通过结合图结构信息和密集的进度感知奖励机制，显著提升了图检索增强生成在复杂推理任务中的性能，为知识密集型问答提供了更有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProGraph-R1%3A+Progress-aware+Reinforcement+Learning+for+Graph+Retrieval+Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17755&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

</details>


### [115] [Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali](https://arxiv.org/abs/2601.17764)
*Md Asgor Hossain Reaj, Rajan Das Gupta, Jui Saha Pritha, Abdullah Al Noman, Abir Ahmed, Golam Md Mohiuddin, Tze Hui Liew*

**主要类别:** cs.CL

**AI概要:** 本研究分析了孟加拉语中的性别偏见问题，发现直接应用英语中心的偏见检测框架存在局限性，需要针对语言差异和社会文化因素开发本地化的检测方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型存在内在性别偏见，但现有研究主要关注英语，对孟加拉语等全球南方语言的偏见研究不足，需要深入探索其独特特征和缓解方法。

**方法:** 采用多种方法提取性别偏见话语：基于词典的挖掘、计算分类模型、翻译比较分析、GPT偏见生成，并在农村和低收入地区进行实地调查收集真实数据。

**结果:** 研究发现孟加拉语性别偏见具有不同于英语的独特特征，英语中心框架因语言差异和社会文化因素而严重受限，需要本地化、情境敏感的方法。

**结论:** 研究强调了为 underrepresented 语言开发专门语言工具的必要性，提倡整合社区驱动的研究方法，为孟加拉语和其他印度语言偏见减少研究奠定基础，促进更包容公平的NLP系统发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Lingual+Probing+and+Community-Grounded+Analysis+of+Gender+Bias+in+Low-Resource+Bengali，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17764，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17764&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.

</details>


### [116] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu, Xiaoyu Guan, Di Liang, Xianjie Wu*

**主要类别:** cs.CL

**AI概要:** 提出动态参数隔离策略来解决监督微调中的跷跷板效应，通过识别任务核心参数区域并分阶段冻结参数来减少任务间干扰。


<details>
  <summary>更多</summary>
  
**动机:** 解决异质SFT任务间的目标冲突导致的跷跷板效应——优化一个任务可能损害其他任务的性能，特别是当模型参数被无差别更新时。

**方法:** 1) 在不同SFT任务上独立微调LLMs并识别每个任务的核心参数区域（更新幅度最大的参数子集）；2) 合并高度重叠核心参数区域的任务进行联合训练；3) 将不相交的任务组织到不同阶段；4) 在多阶段SFT中冻结先前任务获得的核心参数以防止被后续任务覆盖。

**结果:** 在多个公共数据集上的实验表明，该方法持续减少了数据冲突，相比多阶段和多任务调优基线实现了持续的性能提升。

**结论:** 动态参数隔离策略有效解决了SFT中的任务间干扰问题，通过参数解耦和隔离实现了更好的多任务适应性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DPI%3A+Exploiting+Parameter+Heterogeneity+for+Interference-Free+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17777，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17777&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [117] [Controlling Reading Ease with Gaze-Guided Text Generation](https://arxiv.org/abs/2601.17781)
*Andreas Säuberli, Darja Jepifanova, Diego Frassinelli, Barbara Plank*

**主要类别:** cs.CL

**AI概要:** 使用人类眼动模式预测模型来控制文本生成难度，通过眼动追踪实验验证了方法能有效调节文本阅读难易程度


<details>
  <summary>更多</summary>
  
**动机:** 利用阅读时的眼动特征来反映认知负荷，从而生成具有可控阅读难易度的文本

**方法:** 采用预测人类注视模式的模型来引导语言模型输出，产生特定阅读行为的文本，并通过眼动追踪实验在英语母语和非母语者中验证

**结果:** 方法能有效使生成文本变易或变难，体现在阅读时间和感知难度上，统计分析显示阅读行为变化主要源于影响词汇处理的特征

**结论:** 该方法可用于文本简化提高信息可及性，以及生成个性化语言学习教材

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Controlling+Reading+Ease+with+Gaze-Guided+Text+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17781&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.

</details>


### [118] [Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations](https://arxiv.org/abs/2601.17786)
*Yixin Liu, Kehan Yan, Shiyuan Li, Qingfeng Chen, Shirui Pan*

**主要类别:** cs.CL

**AI概要:** 提出了MCA²多视图文本异常检测框架，通过整合多个预训练语言模型的嵌入表示，采用多视图重建和对比协作机制，提升了对不同数据集和异常类型的适应性。


<details>
  <summary>更多</summary>
  
**动机:** 现有两步式'嵌入-检测器'方法受限于单一嵌入模型和缺乏跨数据集及异常类型的适应性，需要解决多模型集成和自适应权重分配问题。

**方法:** 使用多个预训练语言模型的嵌入表示构建多视图框架；采用多视图重建模型提取正常文本模式；设计对比协作模块增强视图间互补性；开发自适应分配模块自动分配各视图权重。

**结果:** 在10个基准数据集上的广泛实验验证了MCA²相对于强基线的有效性，证明了多视图方法和自适应机制的优越性。

**结论:** MCA²通过多视图集成和自适应权重分配，有效解决了传统文本异常检测方法的局限性，在多个数据集上展现出优异的性能，为文本异常检测提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+a+Single+Perspective%3A+Text+Anomaly+Detection+with+Multi-View+Language+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17786，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17786&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.

</details>


### [119] [DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation](https://arxiv.org/abs/2601.17823)
*Pranav Kasela, Marco Braga, Alessandro Ghiotto, Andrea Pilzer, Marco Viviani, Alessandro Raganato*

**主要类别:** cs.CL

**AI概要:** DIETA是一个专为意大利语-英语机器翻译设计的5亿参数Transformer模型，在多个基准测试中表现优异，排名前50%，并发布了完整的训练代码、模型、语料库和评估集。


<details>
  <summary>更多</summary>
  
**动机:** 针对意大利语-英语机器翻译领域缺乏专门优化的中小规模模型，需要构建高质量的双语语料库和评估数据集来推动该领域的研究发展。

**方法:** 收集约2.07亿句意大利语-英语平行语料，涵盖议会记录、法律文本、网络内容等多个领域，并使用预训练模型生成3.52亿反向翻译数据；构建包含450个句子的新评估集；训练5亿参数的仅解码器Transformer模型。

**结果:** 在32个系统的排行榜中 consistently排名第二四分位，在五个测试套件中有四个 outperforms 大多数其他30亿参数以下的模型。

**结论:** DIETA证明了专门设计的中小规模模型在特定语言对机器翻译中的有效性，发布的完整资源将促进意大利语-英语机器翻译的进一步研究和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DIETA%3A+A+Decoder-only+transformer-based+model+for+Italian-English+machine+TrAnslation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17823&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation

</details>


### [120] [Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents](https://arxiv.org/abs/2601.17829)
*Dan Greenstein, Zohar Karnin, Chen Amiraz, Oren Somekh*

**主要类别:** cs.CL

**AI概要:** 提出一种通过优化查询和参数的通用多样性指标来生成合成数据集的方法，用于训练函数调用代理，在保持正确性的同时显著提升数据多样性，并在BFCL基准测试中实现7.4%的准确率提升。


<details>
  <summary>更多</summary>
  
**动机:** 当前函数调用代理训练面临高质量多样化数据获取的挑战，特别是在请求的语言多样性和参数覆盖方面存在不足。

**方法:** 通过优化通用多样性指标生成合成数据集，不依赖手工规则或分类法，使其适用于不同用例。

**结果:** 在内在和外在测试中均显示优于现有方法，在多样性方面表现优越同时保持可比正确性，OOD性能表现更佳。

**结论:** 该方法能有效生成高质量多样化训练数据，显著提升函数调用代理的性能表现，特别是在分布外场景下的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Linguistic+and+Argument+Diversity+in+Synthetic+Data+for+Function-Calling+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17829&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.

</details>


### [121] [EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy](https://arxiv.org/abs/2601.17842)
*Lanqing Du, Yunong Li, YuJie Long, Shihong Chen*

**主要类别:** cs.CL

**AI概要:** 提出基于情绪聚焦疗法(EFT)的多智能体思维链框架EFT-CoT，通过8个专门智能体执行从身体感知到认知探索再到叙事干预的三阶段推理流程，构建了EFT-Instruct数据集并训练出EFT-LLM模型，在同理心深度和专业性方面超越基线方法和人类响应。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于认知行为疗法(CBT)的心理健康问答方法过于偏重'自上而下'的理性重构，忽视了用户的具身体验和主要情绪处理过程。

**方法:** 采用'自下而上'路径，设计三阶段推理流程：具身感知-认知探索-叙事干预，使用8个专门智能体执行身体意识映射、适应性评估、核心信念提取和叙事重构等关键组件，通过思维链蒸馏构建67,000条真实文本的EFT-Instruct数据集，并微调专门的EFT-LLM模型。

**结果:** 实验评估显示EFT-LLM在同理心深度和结构专业性等指标上优于强基线方法和人类响应，消融研究证实多智能体机制的必要性，模型展现出优越的心理推理能力。

**结论:** 该模型为可解释、高同理心的咨询系统提供了有效途径，通过情绪聚焦疗法和多智能体思维链框架成功弥补了现有方法的不足。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EFT-CoT%3A+A+Multi-Agent+Chain-of-Thought+Framework+for+Emotion-Focused+Therapy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17842，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17842&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a "top-down" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a "bottom-up" trajectory, it deconstructs the intervention into a three-stage reasoning flow: "Embodied Perception - Cognitive Exploration - Narrative Intervention." Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed "EFT-Instruct," a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.

</details>


### [122] [D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models](https://arxiv.org/abs/2601.17865)
*Jia Gu, Liang Pang, Huawei Shen, Xueqi Cheng*

**主要类别:** cs.CL

**AI概要:** 研究发现LLM在细粒度采样概率上存在两种模型类型：D-model（如Qwen-2.5）token概率变异性大且与任务分布对齐差，E-model（如Mistral-Small）token概率更稳定且对齐更好，揭示了多样性vs稳定性的权衡机制。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型能生成近似真实世界分布的样本，但其细粒度采样概率是否与任务要求保持一致仍是一个未解决的问题。

**方法:** 通过受控分布采样模拟实验，分析不同LLM的token概率特性，并在代码生成和推荐等下游任务中评估两种模型类型的表现。

**结果:** 发现D-model和E-model的明显二分行为，D-model token概率变异性大、对齐差，E-model更稳定、对齐好，在任务表现上存在系统性权衡。

**结论:** 研究为LLM概率采样行为提供了基础见解，为实际应用中模型选择（D-model vs E-model）提供了指导，帮助在推荐、搜索等web规模应用中平衡多样性和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是D-Models+and+E-Models%3A+Diversity-Stability+Trade-offs+in+the+Sampling+Behavior+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17865，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17865&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.

</details>


### [123] [On the Emergence and Test-Time Use of Structural Information in Large Language Models](https://arxiv.org/abs/2601.17869)
*Michelle Chao Chen, Moritz Miller, Bernhard Schölkopf, Siyuan Guo*

**主要类别:** cs.CL

**AI概要:** 论文研究了语言模型如何学习抽象结构信息以及在测试时利用这些结构信息的能力，发现在受控设置下，结构学习与复杂推理任务相关，但测试时的组合生成能力仍有限。


<details>
  <summary>更多</summary>
  
**动机:** 从观测数据中学习结构信息对于在训练语料之外产生新知识至关重要，这既适用于科学发现中的机制理解，也适用于灵活的测试时组合生成。

**方法:** 设计了一个基于语言结构转换的自然语言数据集，在受控设置下研究语言模型学习抽象结构的能力。

**结果:** 实证研究表明，结构学习能力的出现与复杂推理任务相关，但模型在测试时进行组合生成的能力仍然有限。

**结论:** 语言模型能够学习结构信息，但在实际应用中的组合生成能力还需要进一步发展和改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Emergence+and+Test-Time+Use+of+Structural+Information+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17869，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17869&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.

</details>


### [124] [Self-Manager: Parallel Agent Loop for Long-form Deep Research](https://arxiv.org/abs/2601.17879)
*Yilong Xu, Zhi Zheng, Xiang Long, Yujun Cai, Yiwei Wang*

**主要类别:** cs.CL

**AI概要:** Self-Manager提出并行代理循环框架，通过多线程异步并发执行解决传统单上下文窗口和顺序执行的限制，在深度研究任务中显著优于现有基线方法


<details>
  <summary>更多</summary>
  
**动机:** 现有代理在处理长格式深度研究任务时，虽然通过子任务级上下文管理克服了线性上下文积累和信息丢失问题，但仍受限于单一上下文窗口和顺序执行范式，导致相互干扰和阻塞行为，限制了可扩展性和适应性

**方法:** 提出Self-Manager并行代理循环框架，主线程可以创建多个具有独立上下文的子线程，通过线程控制块进行迭代管理，实现更专注和灵活的并行代理执行

**结果:** 在DeepResearch Bench基准测试中，Self-Manager在所有指标上持续优于现有的单代理循环基线方法，并在上下文容量、效率和泛化能力方面展现出优势

**结论:** Self-Manager通过并行异步执行范式有效解决了传统代理系统的限制，为复杂深度研究任务提供了更高效和可扩展的解决方案，其设计选择在实验中证明了必要性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Manager%3A+Parallel+Agent+Loop+for+Long-form+Deep+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17879，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17879&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.

</details>


### [125] [Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://arxiv.org/abs/2601.17898)
*Qi Zhan, Yile Wang, Hui Huang*

**主要类别:** cs.CL

**AI概要:** 开源大语言模型在命名实体识别任务中通过参数高效微调和结构化输出格式，可以达到与传统编码器模型竞争的性能，且不依赖记忆而是基于指令遵循和生成能力，对通用能力影响极小。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型的兴起，命名实体识别从序列标注任务转向生成范式，需要系统评估开源LLMs在平面和嵌套NER任务上的表现，探索生成式NER与传统方法的性能差距、输出格式影响、记忆依赖问题以及微调后通用能力保持情况。

**方法:** 使用8个不同规模的开源LLM和4个标准NER数据集进行实验，采用参数高效微调方法和结构化输出格式（如内联括号或XML格式），系统评估生成式NER的性能表现。

**结果:** (1) 开源LLMs在参数高效微调和结构化格式下，性能可与传统编码器模型竞争，甚至超越GPT-3等闭源模型；(2) LLMs的NER能力源于指令遵循和生成能力，而非实体-标签对的简单记忆；(3) NER指令微调对LLMs通用能力影响极小，甚至因增强实体理解而提升在DROP等数据集上的表现。

**结论:** 基于LLMs的生成式NER是传统方法的有前景且用户友好的替代方案，证明了开源模型在NER任务中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessment+of+Generative+Named+Entity+Recognition+in+the+Era+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17898&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.

</details>


### [126] [ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation](https://arxiv.org/abs/2601.17921)
*Yi Zhao, Qinghua Yao, Xinyuan song, Wei Zhu*

**主要类别:** cs.CL

**AI概要:** ShapLoRA是一种基于Shapley值的可解释性重要性度量方法，用于优化LoRA的秩分配，在参数效率微调中优于现有基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有LoRA秩分配方法依赖不可解释且不可靠的重要性度量，需要更可解释和可靠的方法来优化秩分配。

**方法:** 结合基于敏感性的度量和合作博弈中的联盟思想，提出Shapley敏感度重要性度量，并在单独验证集上计算，设置分配-重训练流程进行公平比较。

**结果:** 在多个挑战性任务上的实验结果表明，ShapLoRA方法在可调参数相当的情况下优于现有基线方法。

**结论:** ShapLoRA框架通过引入可解释的Shapley敏感度度量，有效解决了LoRA秩分配的可解释性和可靠性问题，为参数效率微调提供了更好的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ShapLoRA%3A+Allocation+of+Low-rank+Adaption+on+Large+Language+Models+via+Shapley+Value+Inspired+Importance+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17921，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17921&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.

</details>


### [127] [A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models](https://arxiv.org/abs/2601.17952)
*Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D'Ercoli, Subati Abulikemu, Zhongtian Sun, Richard Bethlehem, Pietro Lio*

**主要类别:** cs.CL

**AI概要:** 提出一个统一的解释性框架，结合归因和机制解释方法，通过单语义特征提取减少LLM解释的不稳定性，为临床诊断提供可信的重要性评分。


<details>
  <summary>更多</summary>
  
**动机:** 在临床环境如阿尔茨海默病诊断中，LLM的可解释性至关重要但现有方法存在解释不稳定、方法间差异大等问题，机制解释方法缺乏与输入输出的直接关联。

**方法:** 构建LLM层的单语义嵌入空间，通过优化框架显式减少方法间变异性，生成稳定的输入级重要性评分，并通过解压缩表示突出关键特征。

**结果:** 该方法能够产生稳定的重要性评分，减少解释的不确定性，提高LLM在认知健康和神经退行性疾病应用中的可信度。

**结论:** 该统一框架通过结合归因和机制解释视角，显著提升了LLM在临床环境中的解释性和可信度，推动了LLM在医疗诊断中的安全可靠应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Monosemantic+Attribution+Framework+for+Stable+Interpretability+in+Clinical+Neuroscience+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17952，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17952&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.

</details>


### [128] [LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction](https://arxiv.org/abs/2601.17971)
*Junior Cedric Tonga, Chen Cecilia Liu, Iryna Gurevych, Fajri Koto*

**主要类别:** cs.CL

**AI概要:** 该研究提出了一个基于提示的迭代框架，从大型语言模型中提取文化常识知识并构建文化常识知识图谱(CCKG)，评估发现英语文化知识更丰富，使用CCKG增强小模型可提升文化推理和故事生成能力。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型从海量网络数据中学习了丰富的文化知识，但这些知识大多是隐式和非结构化的，限制了其可解释性和使用价值，需要系统性地提取和结构化这些文化知识。

**方法:** 采用迭代式提示框架，将LLMs视为文化档案库，系统地提取文化特定实体、关系和实践，并将其组合成跨语言的多步推理链，构建文化常识知识图谱。

**结果:** 在五个国家的评估显示，文化知识图谱在英语中表现更好（即使目标文化是非英语的），表明当前LLMs的文化编码存在不均衡性。用CCKG增强小模型可提升文化推理和故事生成性能，英语链的提升效果最大。

**结论:** LLMs作为文化技术既有前景也有局限，链式结构化的文化知识是文化相关NLP应用的实用基础，需要更均衡的多语言文化知识表示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLMs+as+Cultural+Archives%3A+Cultural+Commonsense+Knowledge+Graph+Extraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17971&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.

</details>


### [129] [SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets](https://arxiv.org/abs/2601.17982)
*Kshitij Mishra, Nils Lukas, Salem Lahlou*

**主要类别:** cs.CL

**AI概要:** SD-E²是一个强化学习框架，通过优化语义多样性来提升小型语言模型的推理能力，在多个数学和医学推理基准测试中显著超越基线模型


<details>
  <summary>更多</summary>
  
**动机:** 小型语言模型在有限计算预算下难以进行有效的探索，导致复杂推理能力不足

**方法:** 使用冻结的句子嵌入模型计算语义多样性奖励，结合结果正确性和解决方案效率，构建多目标优化目标

**结果:** 在GSM8K上超越基线模型27.4个百分点，MedMCQA达到49.64%，AIME基准达到13.28%，每个问题平均发现9.8种语义不同的策略

**结论:** 奖励语义新颖性为训练具备推理能力的小型语言模型提供了更计算高效的探索-利用信号，通过认知适应提供资源受限模型的效率提升路径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SD-E%24%5E2%24%3A+Semantic+Exploration+for+Reasoning+Under+Token+Budgets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17982&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.

</details>


### [130] [AI-based approach to burnout identification from textual data](https://arxiv.org/abs/2601.17993)
*Marina Zavertiaeva, Petr Parshakov, Mikhail Usanin, Aleksei Smirnov, Sofia Paklina, Anastasiia Kibardina*

**主要类别:** cs.CL

**AI概要:** 使用基于RuBERT模型的NLP方法，通过情感分析预训练和特定数据微调，从文本中检测职业倦怠概率


<details>
  <summary>更多</summary>
  
**动机:** 开发自动化工具来监测高压工作环境中大量书面沟通中的职业倦怠语言信号

**方法:** 使用预训练的RuBERT情感分析模型，通过ChatGPT生成的合成句子和YouTube用户评论数据进行微调，构建倦怠检测模型

**结果:** 成功开发出能够为输入文本分配倦怠概率的模型，可处理大规模文本数据

**结论:** AI和NLP技术可有效用于职业倦怠的自动化检测，为工作场所心理健康监测提供可行方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI-based+approach+to+burnout+identification+from+textual+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.17993，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17993&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.

</details>


### [131] [PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation](https://arxiv.org/abs/2601.18006)
*Lorenzo Proietti, Roman Grundkiewicz, Matt Post*

**主要类别:** cs.CL

**AI概要:** PEAR是一种基于成对比较的监督式质量估计指标，通过预测两个候选翻译的质量差异方向和幅度来评估机器翻译质量，在WMT24基准测试中表现优于传统单候选质量估计方法和更大的模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有的参考无关机器翻译评估方法主要关注单候选质量评分，缺乏对翻译质量差异的精确比较能力。研究者希望开发一种能够直接比较两个翻译候选并量化其质量差异的方法。

**方法:** 提出PEAR指标家族，将机器翻译评估重新定义为分级成对比较任务。使用人类判断差异生成成对监督数据训练，并添加候选顺序反转时的符号反转正则化项来确保一致性。

**结果:** 在WMT24元评估基准上，PEAR超越了使用相同数据和骨干网络的单候选质量估计基线方法，性能优于参数量更大的质量估计模型和基于参考的指标，同时产生更不冗余的评估信号。

**结论:** PEAR通过成对比较框架有效提升了机器翻译评估性能，证明了该方法的优越性，同时还能作为最小贝叶斯风险解码的有效效用函数，以可忽略的成本降低成对评分开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PEAR%3A+Pairwise+Evaluation+for+Automatic+Relative+Scoring+in+Machine+Translation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18006&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.

</details>


### [132] [Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems](https://arxiv.org/abs/2601.18012)
*Hendrika Maclean, Mert Can Cakmak, Muzakkiruddin Ahmed Mohammed, Shames Al Mandalawi, John Talburt*

**主要类别:** cs.CL

**AI概要:** 论文研究大型语言模型在薪资计算等精确数值任务中的表现，通过构建分层数据集和多种提示策略，发现某些情况下精心设计的提示足够，而复杂情况需要显式计算。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在日常写作、搜索和分析中广泛应用，但在精确数值计算和可审计输出方面仍不可靠，特别是在薪资系统等高风险场景中。

**方法:** 使用合成薪资系统作为案例，构建从基础到复杂的分层数据集，测试多种提示策略（从基础提示到模式引导和推理变体），评估多个模型家族（GPT、Claude、Perplexity、Grok、Gemini）的性能。

**结果:** 实验结果显示，存在明确的性能区间：某些情况下精心设计的提示足够准确，而复杂情况需要显式计算才能保证分币级别的精度。

**结论:** 研究提供了一个紧凑、可复现的框架和实用指南，用于在需要高准确性和可审计性的场景中部署大型语言模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Semantic+and+Syntactic+Understanding+in+Large+Language+Models+for+Payroll+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18012，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18012&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.

</details>


### [133] [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)
*Adeeba Tarannum, Muzakkiruddin Ahmed Mohammed, Mert Can Cakmak, Shames Al Mandalawi, John Talburt*

**主要类别:** cs.CL

**AI概要:** 论文提出了一种结合提示工程和确定性验证的框架，用于将非结构化文本转换为结构化地址数据，无需微调即可实现高精度和可复现性。


<details>
  <summary>更多</summary>
  
**动机:** 传统规则方法和概率方法在噪声或多语言环境下表现不佳，而神经网络和大语言模型缺乏确定性控制和可复现性，需要一种更可靠的解决方案。

**方法:** 采用提示驱动、验证为中心的方法，包括输入标准化、结构化提示、约束解码和严格的基于规则的验证，在固定实验设置下确保可复现性。

**结果:** 在异构真实地址数据上评估显示，该方法具有高字段级准确度、强模式遵循性和稳定的置信度校准。

**结论:** 结合确定性验证和生成式提示为结构化信息提取提供了稳健、可解释且可扩展的解决方案，是训练密集型或领域特定模型的实用替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+System+for+Name+and+Address+Parsing+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18014&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.

</details>


### [134] [CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data](https://arxiv.org/abs/2601.18026)
*Pedro Ortiz Suarez, Laurie Burchell, Catherine Arnett, Rafael Mosquera-Gómez, Sara Hincapie-Monsalve, Thom Vaughan, Damian Stewart, Malte Ostendorff, Idris Abdulmumin, Vukosi Marivate, Shamsuddeen Hassan Muhammad, Atnafu Lambebo Tonja, Hend Al-Khalifa, Nadia Ghezaiel Hammouda, Verrah Otiende, Tack Hwa Wong, Jakhongir Saydaliev, Melika Nobakhtian, Muhammad Ravi Shulthan Habibi, Chalamalasetti Kranti, Carol Muchemi, Khang Nguyen, Faisal Muhammad Adam, Luis Frentzen Salim, Reem Alqifari, Cynthia Amol, Joseph Marvin Imperial, Ilker Kesen, Ahmad Mustafid, Pavel Stepachev, Leshem Choshen, David Anugraha, Hamada Nayel, Seid Muhie Yimam, Vallerie Alexandra Putra, My Chiffon Nguyen, Azmine Toushik Wasi, Gouthami Vadithya, Rob van der Goot, Lanwenn ar C'horr, Karan Dua, Andrew Yates, Mithil Bangera, Yeshil Bangera, Hitesh Laxmichand Patel, Shu Okabe, Fenal Ashokbhai Ilasariya, Dmitry Gaynullin, Genta Indra Winata, Yiyuan Li, Juan Pablo Martínez, Amit Agarwal, Ikhlasul Akmal Hanif, Raia Abu Ahmad, Esther Adenuga, Filbert Aurelian Tjiaranata, Weerayut Buaphet, Michael Anugraha, Sowmya Vajjala, Benjamin Rice, Azril Hafizi Amirudin, Jesujoba O. Alabi, Srikant Panda, Yassine Toughrai, Bruhan Kyomuhendo, Daniel Ruffinelli, Akshata A, Manuel Goulão, Ej Zhou, Ingrid Gabriela Franco Ramirez, Cristina Aggazzotti, Konstantin Dobler, Jun Kevin, Quentin Pagès, Nicholas Andrews, Nuhu Ibrahim, Mattes Ruckdeschel, Amr Keleg, Mike Zhang, Casper Muziri, Saron Samuel, Sotaro Takeshita, Kun Kerdthaisong, Luca Foppiano, Rasul Dent, Tommaso Green, Ahmad Mustapha Wali, Kamohelo Makaaka, Vicky Feliren, Inshirah Idris, Hande Celikkanat, Abdulhamid Abubakar, Jean Maillard, Benoît Sagot, Thibault Clérice, Kenton Murray, Sarah Luger*

**主要类别:** cs.CL

**AI概要:** 论文介绍了CommonLID——一个社区驱动的人工标注语言识别基准数据集，涵盖109种语言，专门针对网络领域数据，用于评估和改进语言识别模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前语言识别模型在许多语言上表现不佳，特别是在用于训练多语言模型的噪声和异构网络数据上，缺乏对许多语言的充分支持。

**方法:** 创建CommonLID基准数据集，包含109种语言的人工标注网络文本，并利用该数据集与其他五个常用评估集一起测试了八种流行的语言识别模型。

**结果:** 研究发现现有评估高估了语言识别模型在网络领域的准确性，CommonLID揭示了模型在许多语言上的实际性能差距。

**结论:** CommonLID是一个关键资源，可用于开发更具代表性的高质量文本语料库，并通过开源许可提供数据集和创建代码，促进语言识别研究的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CommonLID%3A+Re-evaluating+State-of-the-Art+Language+Identification+Performance+on+Web+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18026&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.

</details>


### [135] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal, Prasoon Goyal*

**主要类别:** cs.CL

**AI概要:** 通过在提示词前添加随机概念可以提升大语言模型输出的多样性，研究设计了系统评估协议验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型(LLMs)的输出多样性有限，研究者希望探索通过注入随机概念是否能改善这一问题。

**方法:** 设计系统评估协议，在提示词前添加不相关的随机词句，以"列举10位好莱坞演员"等问题形式测试多个LLM，并分析输出多样性指标。

**结果:** 实验证明，在多个LLM中，前置随机词句确实能显著提高输出内容的多样性。

**结论:** 该方法为提升LLM多样性提供了有前景的途径，评估协议也为系统化评估LLM多样性研究提供了启发，未来可探索在其他领域应用随机性注入的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Addressing+LLM+Diversity+by+Infusing+Random+Concepts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18053&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [136] [Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production](https://arxiv.org/abs/2601.18056)
*Ahmet Yavuz Uluslu, Elliot Murphy*

**主要类别:** cs.CL

**AI概要:** 论文主张在双语产生错误研究中加入振荡特征分析，使用ROSE神经模型解释句法迁移，将跨语言影响建模为L2句子规划中的振荡故障模式，为语言障碍提供更复杂的生物标志物。


<details>
  <summary>更多</summary>
  
**动机:** 传统双语产生错误研究主要关注事件相关电位等时间特征，但缺乏对振荡特征的考虑，需要新的实现层面约束来完善双语理论。

**方法:** 采用ROSE神经模型作为案例研究，分析跨语言影响(CLI)和功能抑制/竞争理论，将其建模为L2句子规划过程中的特定振荡故障模式。

**结果:** ROSE模型能够捕捉双语产生中句法迁移的形式特性和形态句法序列故障模式的范围，为语言功能障碍提供了比传统神经特征更复杂的时空生物标志物。

**结论:** 通过振荡特征建模跨语言影响不仅提供了ROSE模型设计的连接假设，还支持探索更复杂的语言功能障碍生物标志物，推动了双语神经计算理论的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neurocomputational+Mechanisms+of+Syntactic+Transfer+in+Bilingual+Sentence+Production，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18056，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18056&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.

</details>


### [137] [Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models](https://arxiv.org/abs/2601.18065)
*Aryan Roy, Zekun Wang, Christopher J. MacLellan*

**主要类别:** cs.CL

**AI概要:** 该研究比较了纯文本LLM和多模态VLM在语言具体性敏感性方面的差异，发现多模态预训练使VLM在具体性输入上表现更好，表征更具结构性，评分更符合人类标准，注意力模式也更接近人类认知。


<details>
  <summary>更多</summary>
  
**动机:** 探究视觉-语言模型(VLMs)是否比纯文本大语言模型(LLMs)在仅文本提示下对语言具体性具有更类似人类的敏感性，通过多模态预训练作为感知基础的一种消融实验。

**方法:** 使用匹配的Llama文本骨干网络及其视觉对应版本进行控制比较，在三个层面测量具体性效应：输出行为（QA准确性与问题具体性关系）、嵌入几何（表征是否沿具体性轴组织）、注意力动态（通过注意力熵量化上下文依赖）。

**结果:** 在所有基准测试和模型规模下，VLMs在更具体的输入上表现出更大的性能提升，具有更清晰的具体性结构化表征，产生的评分更符合人类标准，并显示出系统性的不同注意力模式。

**结论:** 多模态训练使模型在语言具体性处理上更加接近人类认知，证明了感知基础对语言理解的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Grounded+Concreteness%3A+Human-Like+Concreteness+Sensitivity+in+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18065，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18065&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.

</details>


### [138] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh, Kaousheik Jayakumar, Aswinkumar Ramkumar, Pavan Thodima, Aniket Rege*

**主要类别:** cs.CL

**AI概要:** 论文评估了17种先进LLM智能体在Hanabi纸牌游戏中的协作推理能力，通过不同上下文工程设置测试模型表现，并发布首个公开数据集。研究发现最强推理模型平均得分超过15分但仍落后人类专家，通过监督学习和强化学习微调可显著提升性能，且能泛化到其他推理任务。


<details>
  <summary>更多</summary>
  
**动机:** 研究不完全信息下的协作推理挑战，特别是在Hanabi游戏中需要心智理论和策略性沟通的场景，旨在理解LLM智能体在协调失败和不同脚手架设置下的鲁棒性。

**方法:** 在2-5人Hanabi游戏中测试17种LLM智能体，采用三种上下文工程设置：Watson（最小提示）、Sherlock（贝叶斯推理）和Mycroft（多轮状态跟踪）。创建并发布两个数据集HanabiLogs和HanabiRewards，用于监督学习和强化学习微调。

**结果:** 最强推理模型在Sherlock设置下平均得分超过15分，但仍低于人类专家（20+分）。通过监督学习和强化学习微调4B模型，性能分别提升21%和156%，接近强专有推理模型水平，并在其他推理任务上展现泛化能力。

**结论:** LLM智能体能够通过工作记忆进行状态跟踪，模型强度与跨模型性能呈平滑插值关系。数据集驱动的微调能显著提升协作推理性能，且具有任务泛化能力，为不完全信息下的多智能体协作提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparks+of+Cooperative+Reasoning%3A+LLMs+as+Strategic+Hanabi+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18077&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [139] [CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations](https://arxiv.org/abs/2601.18102)
*Stephanie Fong, Zimu Wang, Guilherme C. Oliveira, Xiangyu Zhao, Yiwen Jiang, Jiahe Liu, Beau-Luke Colton, Scott Woods, Martha E. Shenton, Barnaby Nelson, Zongyuan Ge, Dominic Dwyer*

**主要类别:** cs.CL

**AI概要:** CHiRPE是一个临床NLP工具，通过临床医生共同开发的SHAP解释格式预测精神病风险，在24个国际诊所的944份访谈记录上训练，准确率超过90%，临床专家更偏好其新颖的概念引导解释格式。


<details>
  <summary>更多</summary>
  
**动机:** 传统可解释AI方法与临床推理不对齐且缺乏临床医生输入，医疗NLP工具需要终端用户的可解释性。

**方法:** 整合症状领域映射、LLM摘要和BERT分类的NLP流水线，使用944份半结构化临床访谈转录本训练，开发了与临床医生共同设计的新型SHAP解释格式。

**结果:** 在三个BERT变体上实现超过90%的准确率，优于基线模型。28位临床专家评估显示强烈偏好新颖的概念引导解释，特别是混合图文本摘要格式。

**结论:** 临床引导的模型开发能够产生准确且可解释的结果，下一步将在24个国际站点进行真实世界测试。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CHiRPE%3A+A+Step+Towards+Real-World+Clinical+NLP+with+Clinician-Oriented+Model+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18102&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.

</details>


### [140] [GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health](https://arxiv.org/abs/2601.18106)
*Jiatan Huang, Zheyuan Zhang, Tianyi Ma, Mingchen Li, Yaning Zheng, Yanfang Ye, Chuxu Zhang*

**主要类别:** cs.CL

**AI概要:** GLEN-Bench是首个基于图语言模型的营养健康评估基准，整合了健康记录、食物成分和食品获取数据，通过三个关联任务（风险检测、个性化推荐、问答解释）来解决营养干预中的关键挑战。


<details>
  <summary>更多</summary>
  
**动机:** 当前营养干预计算方法存在三个关键缺陷：忽略现实约束条件、缺乏推荐解释性、缺少统一评估基准，无法为慢性病管理提供有效的个性化膳食指导。

**方法:** 结合NHANES健康记录、FNDDS食物成分数据和USDA食品获取指标构建知识图谱，连接人口统计、健康状况、饮食行为、贫困约束和营养需求，使用图神经网络、大语言模型和混合架构进行多任务评估。

**结果:** 成功识别出与健康风险相关的明确饮食模式，为阿片类药物使用障碍等疾病的不同阶段检测细微营养差异，建立了可靠的基线性能。

**结论:** GLEN-Bench提供了一个全面的评估框架，能够生成基于图谱的个性化膳食建议和自然语言解释，为实际营养干预提供了可操作的见解和指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GLEN-Bench%3A+A+Graph-Language+based+Benchmark+for+Nutritional+Health，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18106，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18106&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.

</details>


### [141] [FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning](https://arxiv.org/abs/2601.18116)
*Lin Sun, Linglin Zhang, Jingang Huang, Change Jia, Zhengwei Cheng, Xiangzheng Zhang*

**主要类别:** cs.CL

**AI概要:** FABLE是一个基于森林结构的自适应双路径检索框架，通过LLM增强的层次索引和双路径检索策略，在显著减少计算成本的同时达到与全上下文LLM推理相当的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 长上下文LLM存在中间信息丢失、计算成本高和多文档推理扩展性差的问题，而传统RAG系统受限于平面分块检索的语义噪声和结构化跨文档合成能力不足。

**方法:** 构建LLM增强的多粒度层次森林索引，采用双路径策略结合LLM引导的层次遍历和结构感知传播进行细粒度证据获取，并具有自适应效率权衡的显式预算控制。

**结果:** FABLE在实验中持续优于最先进的RAG方法，以高达94%的token减少量达到与全上下文LLM推理相当的准确性。

**结论:** 长上下文LLM放大了而非完全替代了对结构化检索的需求，FABLE框架有效解决了长上下文推理的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FABLE%3A+Forest-Based+Adaptive+Bi-Path+LLM-Enhanced+Retrieval+for+Multi-Document+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18116，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18116&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.
  We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.
  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.

</details>


### [142] [Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models](https://arxiv.org/abs/2601.18129)
*Kunat Pipatanakul, Pittawat Taveekitworachai*

**主要类别:** cs.CL

**AI概要:** Typhoon S是一个最小化的开放后训练配方，通过监督微调、策略蒸馏和小规模强化微调，能够在有限资源下将基础模型转化为具有强大通用性能的指令调优模型，特别适用于主权环境下的泰语法律推理和本地知识任务。


<details>
  <summary>更多</summary>
  
**动机:** 解决当前大语言模型主要在高资源语言（如英语和中文）上训练和评估的问题，以及少数组织垄断模型开发的情况。主权环境中的区域或国家机构需要在有限资源和严格透明度约束下保持对模型权重、训练数据和部署的控制与理解。

**方法:** 提出Typhoon S后训练配方，结合监督微调、策略蒸馏和小规模强化微调（使用InK-GRPO扩展方法，在GRPO损失基础上增加下一个词预测损失）。以泰语为案例研究，验证方法的有效性。

**结果:** 该方法成功将主权适应和通用基础模型转化为指令调优模型，具有强大的通用性能。小规模RFT与InK-GRPO结合显著提升了泰语法律推理和泰语特定知识能力，同时保持通用能力。

**结论:** 精心设计的后训练策略可以减少指令数据和计算规模的需求，为在学术规模资源下开发高质量主权LLMs提供了实用路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Typhoon-S%3A+Minimal+Open+Post-Training+for+Sovereign+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18129&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.

</details>


### [143] [Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models](https://arxiv.org/abs/2601.18162)
*Ani Harutyunyan, Sachin Kumar*

**主要类别:** cs.CL

**AI概要:** 本文在GoEmotions数据集上比较了三种细粒度情感识别模型：TF-IDF逻辑回归、BiLSTM with attention和BERT微调模型，发现逻辑回归在Micro-F1上表现最佳，而BERT在整体平衡性上最优。


<details>
  <summary>更多</summary>
  
**动机:** 细粒度情感识别面临标签重叠和类别不平衡的挑战，需要评估不同模型家族在该任务上的表现。

**方法:** 使用官方划分的数据集，采用逆频率类别权重缓解不平衡问题，评估了三种模型：基于TF-IDF的逻辑回归（二元关联训练）、带注意力的BiLSTM、以及用于多标签分类的微调BERT模型。

**结果:** 逻辑回归获得最高Micro-F1（0.51），BERT在Macro-F1（0.49）、Hamming Loss（0.036）和Subset Accuracy（0.36）上表现最佳，超越了原论文报告的结果。

**结论:** 高频情感通常依赖表层词汇线索，而上下文表示能提升对稀有情感和模糊样本的处理能力，BERT模型在整体平衡性上表现最优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Grained+Emotion+Detection+on+GoEmotions%3A+Experimental+Comparison+of+Classical+Machine+Learning%2C+BiLSTM%2C+and+Transformer+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18162，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18162&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.

</details>


### [144] [MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2601.18204)
*Juexiang Ye, Xue Li, Xinyu Yang, Chengkai Huang, Lanshun Nie, Lina Yao, Dechen Zhan*

**主要类别:** cs.CL

**AI概要:** MemWeaver是一个统一记忆框架，通过结构化图记忆、经验记忆和段落记忆三组件解决长程交互中的时序一致性和多跳推理问题，相比长上下文基线减少95%输入长度同时提升推理准确率。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法依赖非结构化检索或粗粒度抽象，导致时序冲突、推理脆弱性和有限可追溯性，无法满足基于大语言模型的智能体在长程交互中对记忆系统的需求。

**方法:** 提出MemWeaver框架，包含三个互连组件：时序基础图记忆（结构化关系推理）、经验记忆（从重复观察中抽象交互模式）、段落记忆（保存原始文本证据）；采用双通道检索策略联合检索结构化知识和支持证据。

**结果:** 在LoCoMo基准测试中，MemWeaver显著提高了多跳和时序推理准确率，同时相比长上下文基线减少了超过95%的输入上下文长度。

**结论:** MemWeaver通过统一的记忆框架有效解决了长程交互智能体的记忆需求，在保持紧凑上下文的同时实现了更好的推理性能和可追溯性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MemWeaver%3A+Weaving+Hybrid+Memories+for+Traceable+Long-Horizon+Agentic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18204&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.

</details>


### [145] [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238)
*Tafazzul Nadeem, Bhavik Shangari, Manish Rai, Gagan Raj Gupta, Ashutosh Modi*

**主要类别:** cs.CL

**AI概要:** 论文提出了一种通过合成数据训练VLM的方法来改善技术图纸识别，显著提升了模型在真实手绘图纸上的表现


<details>
  <summary>更多</summary>
  
**动机:** 专业人员在讨论中手绘技术图纸后难以编辑，现有VLM在理解技术图纸方面表现不佳，且缺乏大量真实手绘图像数据用于训练

**方法:** 创建大规模合成技术图纸数据集，设计新的自监督任务，在合成图像上微调Llama 3.2 11B-instruct模型得到LLama-VL-TUG

**结果:** LLama-VL-TUG将ROUGE-L性能提升2.14倍，在8种图纸类型中的7种实现最少编译错误，平均F1分数提升6.97倍

**结论:** 合成数据训练能有效提升VLM对技术图纸的理解能力，为实际应用提供了可行的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TechING%3A+Towards+Real+World+Technical+Image+Understanding+via+VLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18238，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18238&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.

</details>


### [146] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun, Xiangyu Zhang, Duan Wu*

**主要类别:** cs.CL

**AI概要:** BoRP是一个基于LLM潜在空间几何特性的可扩展满意度评估框架，通过自动化评分标准生成和偏最小二乘法映射隐藏状态到连续分数，显著优于生成式基线且大幅降低推理成本


<details>
  <summary>更多</summary>
  
**动机:** 传统A/B测试缺乏可靠指标来评估开放式对话AI的用户满意度，显式反馈稀疏，隐式指标模糊

**方法:** 利用极化指数自举机制自动生成评分标准，使用偏最小二乘法(PLS)将隐藏状态映射到连续满意度分数

**结果:** 在工业数据集上，BoRP(Qwen3-8B/14B)在与人判断对齐方面显著优于生成式基线(包括Qwen3-Max)，推理成本降低数个数量级

**结论:** BoRP为开放式对话AI提供了高保真度的满意度评估解决方案，支持全规模监控和高灵敏度A/B测试

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BoRP%3A+Bootstrapped+Regression+Probing+for+Scalable+and+Human-Aligned+LLM+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18253，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18253&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [147] [Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue](https://arxiv.org/abs/2601.18281)
*Yuhang Jia, Pei Liu, Haoqin Sun, Jiaming Zhou, Xuxin Cheng, Cao Liu, Ke Zeng, Xunliang Cai, Yong Qin*

**主要类别:** cs.CL

**AI概要:** 该论文提出了ReEmpathy模型，通过引入描述性语言评估模型EmpathyEval和自反思交替推理机制，显著提升了端到端语音语言模型在共情对话中的表现，解决了传统监督信号在建模复杂共情时的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 当前端到端语音语言模型在共情对话中过度依赖刚性监督信号（如标准答案或偏好分数），无法充分捕捉复杂共情的细微差别和情感表达的适当性。

**方法:** 首先提出EmpathyEval描述性评估模型，然后基于此开发ReEmpathy模型，采用共情自反思交替推理机制，在语音响应生成中穿插自由形式的共情相关反思推理。

**结果:** 大量实验表明ReEmpathy通过启用反思推理，显著改善了共情敏感的语音对话质量。

**结论:** 该方法为构建更具情感智能和共情意识的人机交互提供了有前景的途径，突破了传统监督学习在共情建模中的根本限制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reflecting+Twice+before+Speaking+with+Empathy%3A+Self-Reflective+Alternating+Inference+for+Empathy-Aware+End-to-End+Spoken+Dialogue，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18281&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single "correct" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.

</details>


### [148] [U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents](https://arxiv.org/abs/2601.18285)
*Jin Su, Runnan Fang, Yeqiu Li, Xiaobin Wang, Shihao Cai, Pengjun Xie, Ningyu Zhang, Fajie Yuan*

**主要类别:** cs.CL

**AI概要:** U-Fold是一个针对用户中心任务的动态上下文折叠框架，通过保持完整对话历史和使用意图感知的对话摘要和任务相关工具日志，解决了现有方法在长上下文和多轮对话中的限制，显著优于现有基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于LLM的智能体在工具增强设置中受限于上下文长度，现有的上下文折叠方法主要为单查询或单意图场景设计，在多轮用户中心对话中会丢失细粒度约束和中间事实，且无法跟踪演变的用户意图。

**方法:** 提出U-Fold框架，保留完整的用户-智能体对话和工具调用历史，在每个对话轮次使用两个核心组件：意图感知的演进对话摘要和紧凑的任务相关工具日志。

**结果:** 在多个基准测试中，U-Fold consistently outperforms ReAct（在长上下文设置中达到71.4%的胜率）和先前的折叠基线（改进高达27.0%），特别是在长、嘈杂、多轮任务上表现优异。

**结论:** U-Fold是将上下文管理技术从单查询基准转移到现实用户中心应用的有希望的一步，证明了其在处理复杂多轮对话任务中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是U-Fold%3A+Dynamic+Intent-Aware+Context+Folding+for+User-Centric+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18285&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.

</details>


### [149] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong, Zhiqiang Liu, Songze Li, Xiaoke Guo, Yuanxiang Liu, Xinle Deng, Zhizhen Liu, Lei Liang, Huajun Chen, Wen Zhang*

**主要类别:** cs.CL

**AI概要:** Temp-R1是首个通过强化学习训练的端到端自主时序知识图谱问答代理，通过扩展动作空间和逆向课程学习，在复杂问题上实现了19.8%的性能提升，达到了最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 现有时序知识图谱问答方法依赖固定工作流程和昂贵的闭源API，限制了灵活性和可扩展性，需要解决动态事实的多跳依赖和复杂时序约束的推理挑战。

**方法:** 提出Temp-R1代理，使用强化学习训练，扩展动作空间（包括专用内部动作和外部动作），引入逆向课程学习（先训练困难问题再迁移到简单问题）。

**结果:** 在MultiTQ和TimelineKGQA数据集上达到最先进性能，复杂问题上比强基线提升19.8%，参数量为8B。

**结论:** 这项工作为自主时序推理代理建立了新范式，代码将公开提供。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temp-R1%3A+A+Unified+Autonomous+Agent+for+Complex+Temporal+KGQA+via+Reverse+Curriculum+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18296，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18296&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [150] [Suppressing Final Layer Hidden State Jumps in Transformer Pretraining](https://arxiv.org/abs/2601.18302)
*Keigo Shibata, Kazuki Yano, Ryosuke Takahashi, Jaesung Lee, Wataru Ikeda, Jun Suzuki*

**主要类别:** cs.CL

**AI概要:** 论文研究了Transformer语言模型内部行为，发现大多数预训练模型在中间层输入输出隐藏状态向量的角度距离变化很小，但在最后一层附近会出现显著的"跳跃"。作者提出了跳跃抑制正则化器(JREG)来抑制这种跳跃，实验证明该方法能提升模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 观察到许多预训练模型在最后一层Transformer层附近出现不成比例的角度距离跳跃，这可能表明模型能力使用不平衡，是一个需要改进的不良特性。

**方法:** 首先引入量化指标来测量最后一层附近的跳跃强度，然后提出跳跃抑制正则化器(JREG)，在预训练过程中惩罚这种跳跃，鼓励中间层更平衡地使用能力。

**结果:** 在基于Llama的三种不同规模模型上进行实证评估，显示使用JREG方法训练的模型相比基线在任务性能上有所提升，且不需要改变模型架构。

**结论:** 最后一层的角度距离跳跃是预训练模型中普遍存在的不良特性，通过JREG正则化器可以有效抑制这种跳跃，从而改善模型的能力分布和整体性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Suppressing+Final+Layer+Hidden+State+Jumps+in+Transformer+Pretraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18302，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18302&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.

</details>


### [151] [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306)
*Everlyn Asiko Chimoto, Mostafa Elhoushi, Bruce A. Bassett*

**主要类别:** cs.CL

**AI概要:** 多语言校准集能显著提升多语言大模型的量化效果，相比仅用英语校准，多语言混合校准可使困惑度降低最多3.52点。语言对齐对量化性能至关重要。


<details>
  <summary>更多</summary>
  
**动机:** 现有后训练量化方法主要使用小型英语校准集，但对多语言模型的影响研究不足，需要系统评估不同校准设置对多语言量化的影响。

**方法:** 在两个量化器(GPTQ、AWQ)上系统评估8种校准设置(5种单语言和3种多语言混合)，使用10种语言数据在Llama3.1 8B和Qwen2.5 7B模型上进行测试。

**结果:** 非英语和多语言校准集相比英语基线显著改善困惑度，多语言混合实现最大困惑度降低(最多3.52点)。针对评估语言定制校准集效果最佳，但某些语言-量化器组合存在性能下降问题。

**结论:** 静态的通用校准方法不够优化，定制校准数据(语言和多样性)对稳健量化多语言LLMs至关重要，需考虑语言间的激活范围分布差异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Calibrating+Beyond+English%3A+Language+Diversity+for+Better+Quantized+Multilingual+LLM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18306&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.

</details>


### [152] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu, Yuanfeng Song, Chen Zhang, Raymond Chi-Wing Wong*

**主要类别:** cs.CL

**AI概要:** MultiVis-Agent是一个基于逻辑规则增强的多智能体框架，用于可靠的多模态可视化生成，通过四层逻辑规则框架提供数学可靠性保证，在复杂任务中显著优于现有基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界可视化任务需要多模态输入和迭代优化，但现有系统存在单模态输入、一次性生成和工作流程僵化等根本限制，LLM方法虽然潜力但存在可靠性问题。

**方法:** 提出逻辑规则增强的多智能体框架，包含四层逻辑规则作为数学约束指导LLM推理而非替代它，支持从基础生成到迭代优化的四个场景，并开发了包含1000多个案例的MultiVis-Bench基准。

**结果:** 在挑战性任务中达到75.63%的可视化评分，显著优于基线方法(57.54-62.79%)，任务完成率99.58%，代码执行成功率94.56%(无逻辑规则时为74.48%和65.10%)。

**结论:** 该方法成功解决了自动化可视化生成中的复杂性和可靠性挑战，逻辑规则框架为系统可靠性提供了数学保证同时保持灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MultiVis-Agent%3A+A+Multi-Agent+Framework+with+Logic+Rules+for+Reliable+and+Comprehensive+Cross-Modal+Data+Visualization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18320&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [153] [Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare](https://arxiv.org/abs/2601.18334)
*Clément Christophe, Wadood Mohammed Abdul, Prateek Munjal, Tathagata Raha, Ronnie Rajan, Praveenkumar Kanithi*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个评估LLM在医疗场景中谄媚行为的新框架和指标，发现推理优化的模型虽然准确率高，但在权威压力下更容易合理化错误建议，基准性能不能代表临床可靠性


<details>
  <summary>更多</summary>
  
**动机:** 随着LLM越来越多地融入临床工作流程，其倾向于迎合用户而非坚持事实准确性的谄媚行为对患者安全构成重大风险，需要更可靠的评估方法

**方法:** 引入基于医学多选题(MCQA)的评估框架，提出调整后的谄媚评分(Adjusted Sycophancy Score)新指标，通过控制模型随机不稳定性来隔离对齐偏差，并对Qwen-3和Llama-3系列模型进行扩展分析

**结果:** 发现清晰的规模扩展韧性轨迹，揭示推理优化模型的反直觉脆弱性：虽然准确率高，但在权威压力下经常合理化错误建议，基准性能不能代表临床可靠性

**结论:** 简化推理结构可能提供对抗专家驱动谄媚的更好鲁棒性，临床应用中需要更全面的安全评估而不仅仅是基准测试表现

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Overalignment+in+Frontier+LLMs%3A+An+Empirical+Study+of+Sycophantic+Behaviour+in+Healthcare，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18334，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18334&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or "confusability". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized "Thinking" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.

</details>


### [154] [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)
*Junyi Zou*

**主要类别:** cs.CL

**AI概要:** 该研究通过两阶段LoRA流程（领域自适应预训练和监督微调）和加权适配器合并方法，提升了大型语言模型在医疗领域的术语精确性和指令跟随安全性，在医疗验证集上取得了显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在通用领域表现强大，但在医疗术语精确性和安全关键指令跟随方面存在困难，需要专门针对医疗领域进行优化。

**方法:** 采用两阶段LoRA流程：1）领域自适应预训练注入医疗知识；2）监督微调对齐医疗问答行为。提出加权适配器合并方法平衡指令跟随能力和领域知识保留。

**结果:** 在医疗验证集上，合并模型达到BLEU-4=16.38, ROUGE-1=20.42, ROUGE-2=4.60, ROUGE-L=11.54的优异性能。

**结论:** 提出的两阶段训练和加权合并方法有效提升了LLMs在医疗领域的性能，为安全关键领域的模型优化提供了可行方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Domain+Pretraining+Interferes+with+Instruction+Alignment%3A+An+Empirical+Study+of+Adapter+Merging+in+Medical+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18350，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18350&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.

</details>


### [155] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu, Isabella Yin, Xinyi Tu, Chi Zhang, Yixin Zhu*

**主要类别:** cs.CL

**AI概要:** 大型语言模型在处理与预训练先验知识相矛盾的动态上下文规则时存在语义惯性问题，即在规则改变时难以抑制预训练知识。研究发现大模型可能表现出逆向缩放现象，在需要抑制预训练关联时表现不如小模型。通过将动态规则表示为可执行代码而非描述性文本，可以逆转这一趋势并实现有效的先验抑制。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是探索LLMs在处理与预训练知识相矛盾的动态上下文规则时的表现问题，特别是在需要抑制预训练关联的语义推理任务中，大模型反而表现更差的反常现象。

**方法:** 使用Baba Is You游戏作为测试平台，其中物理规则是可变的文本规则。开发了Code-Grounded Vistas (LCV)方法，通过在反事实对上进行微调，并识别具有矛盾规则的状态，强制模型关注逻辑约束而非视觉语义。

**结果:** 研究发现大模型在需要抑制预训练关联的自然语言推理任务中表现更差（逆向缩放现象）。将动态规则表示为可执行代码而非描述性文本的方法在效率和准确性上都优于昂贵的推理时搜索方法。

**结论:** 表示形式（representation）从根本上决定了模型缩放是否改善或损害上下文推理能力。这一发现挑战了"模型越大越好"的假设，对需要动态覆盖学习先验的领域具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Code+over+Words%3A+Overcoming+Semantic+Inertia+via+Code-Grounded+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18352，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18352&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [156] [CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes](https://arxiv.org/abs/2601.18374)
*Rodrigo Silva, José Evans, José Isidro, Miguel Marques, Afonso Fonseca, Ricardo Morais, João Canavilhas, Arian Pasquali, Purificação Silvano, Alípio Jorge, Nuno Guimarães, Sérgio Nunes, Ricardo Campos*

**主要类别:** cs.CL

**AI概要:** CitiLink是一个利用NLP和IR技术将市政会议记录转换为结构化可搜索数据的平台，通过LLM提取元数据、讨论主题和投票结果，提高政府透明度。


<details>
  <summary>更多</summary>
  
**动机:** 市政会议记录冗长且结构复杂，公众和记者难以高效获取信息，需要技术手段提升政府信息的可访问性和透明度。

**方法:** 使用LLM提取会议记录的元数据、讨论主题和投票结果，建立数据库支持BM25全文搜索和分面过滤，基于葡萄牙6个市镇的120份会议记录构建系统。

**结果:** 系统通过市政人员的引导测试验证了可用性，同时评估了Gemini在信息提取方面的有效性。

**结论:** NLP和IR技术能有效提升市政文档的可访问性，CitiLink展示了技术如何增强地方政府透明度，Gemini在信息提取方面表现良好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CitiLink%3A+Enhancing+Municipal+Transparency+and+Citizen+Engagement+through+Searchable+Meeting+Minutes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18374&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.

</details>


### [157] [Hierarchical Text Classification with LLM-Refined Taxonomies](https://arxiv.org/abs/2601.18375)
*Jonas Golde, Nicolaas Jedema, Ravi Krishnan, Phong Le*

**主要类别:** cs.CL

**AI概要:** TaxMorph框架使用大语言模型对层次化文本分类的整个分类体系进行重构，通过重命名、合并、拆分和重排序操作，使分类体系更符合语言模型的语义编码，在三个基准测试中性能提升最高达2.9% F1值


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的分类体系存在歧义性，如相似父节点下的相同叶节点名称，这会阻碍语言模型学习清晰的决策边界

**方法:** 使用大语言模型对整个分类体系进行重构操作，包括重命名、合并、拆分和重排序，使分类体系更好地匹配语言模型编码的语义

**结果:** 在三个层次化文本分类基准测试中，LLM重构的分类体系在各种设置下始终优于人工构建的分类体系，性能提升最高达+2.9pp F1值

**结论:** LLM引导的分类体系重构创建了更符合模型学习方式的分类体系，虽然这些分类体系在嵌入空间中更难分离，但更好地反映了模型的归纳偏置，从而提高了层次化文本分类性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Text+Classification+with+LLM-Refined+Taxonomies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18375，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18375&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.

</details>


### [158] [Corpus-Based Approaches to Igbo Diacritic Restoration](https://arxiv.org/abs/2601.18380)
*Ignatius Ezeani*

**主要类别:** cs.CL

**AI概要:** 该论文针对低资源语言伊博语，开发了重音符号恢复的数据集生成框架，并比较了三种方法：标准n-gram模型、分类模型和嵌入模型来解决重音符号歧义问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前NLP研究主要关注英语等资源丰富的语言，而全球95%的7000种语言都是低资源语言，缺乏NLP所需的数据、工具和技术，特别是伊博语存在重音符号歧义问题。

**方法:** 开发了灵活的数据集生成框架，采用三种方法：1)标准n-gram模型使用目标词之前的词序列作为预测因子；2)分类模型使用目标词两侧的窗口词；3)嵌入模型比较上下文词嵌入与候选变体向量嵌入的相似度得分。

**结果:** 论文提出了针对伊博语重音符号恢复的系统性解决方案框架，为低资源语言的NLP处理提供了可行的方法论。

**结论:** 该研究为低资源语言特别是伊博语的重音符号歧义问题提供了有效的技术路径，有助于推动非主流语言的NLP技术发展，填补了该领域的研究空白。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Corpus-Based+Approaches+to+Igbo+Diacritic+Restoration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18380，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18380&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.
  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.

</details>


### [159] [Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction](https://arxiv.org/abs/2601.18395)
*Mikel Zubillaga, Oscar Sainz, Oier Lopez de Lacalle, Eneko Agirre*

**主要类别:** cs.CL

**AI概要:** ThinkTwice框架通过采样生成多个候选模板并使用选择模块挑选最佳结果，在文档级信息抽取任务中显著优于贪婪解码方法


<details>
  <summary>更多</summary>
  
**动机:** 传统方法使用贪婪解码避免输出变异性，但研究发现采样能产生更好的解决方案，特别是使用推理模型时

**方法:** 提出ThinkTwice采样选择框架：LLM为文档生成多个候选模板，通过无监督方法（利用输出一致性）或有监督方法（使用奖励模型）选择最佳模板；使用拒绝采样生成银标注训练数据

**结果:** 实验证明无监督和有监督ThinkTwice方法均优于贪婪基线和最先进方法

**结论:** 采样变异性不应被视为限制，而是可以利用的优势，ThinkTwice框架在文档级信息抽取中表现出色

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+not+be+greedy%2C+Think+Twice%3A+Sampling+and+Selection+for+Document-level+Information+Extraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18395，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18395&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.

</details>


### [160] [Pisets: A Robust Speech Recognition System for Lectures and Interviews](https://arxiv.org/abs/2601.18415)
*Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin, Mikhail Klementev, Roman Derunets, Lyudmila Budneva*

**主要类别:** cs.CL

**AI概要:** Pisets是一个针对科学家和记者的俄语语音转文字系统，采用三组件架构提升识别准确率并减少Whisper模型的错误和幻觉问题。


<details>
  <summary>更多</summary>
  
**动机:** 解决Whisper模型在语音识别中存在的错误和幻觉问题，为科学家和记者提供更准确的俄语语音转录工具。

**方法:** 采用三组件架构：Wav2Vec2进行初步识别、Audio Spectrogram Transformer进行误报过滤、Whisper进行最终语音识别；结合课程学习方法和多样化俄语语料库训练。

**结果:** 系统在各种声学条件下对长音频数据实现了稳健转录，相比WhisperX和标准Whisper模型有显著提升。

**结论:** 提出的三组件架构结合课程学习和不确定性建模技术有效提升了俄语语音识别质量，系统代码已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pisets%3A+A+Robust+Speech+Recognition+System+for+Lectures+and+Interviews，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18415&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.

</details>


### [161] [Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.18468)
*Daniel B. Hier, Tayo Obafemi-Ajayi*

**主要类别:** cs.CL

**AI概要:** 大型语言模型在预训练后存储生物医学事实的强度不均，本研究通过微调Llama 3.1模型分析潜在知识对事实获取、泛化和退化的预测作用。


<details>
  <summary>更多</summary>
  
**动机:** 探究语言模型中潜在知识（存在于权重中但无法通过确定性解码可靠获取的事实）对微调过程中事实学习的影响机制。

**方法:** 使用人类表型本体(HPO)和基因本体(GO)的标识符映射数据微调Llama 3.1 8B模型，采用随机解码检测潜在知识，使用Cox比例风险模型分析学习过程的预测因素。

**结果:** 基线确定性召回率仅2.8%，微调后达71.9%；潜在知识是最强预测因子（风险比2.6），与更快的学习速度和收敛相关；泛化能力有限（5.8%），但潜在知识存在时更可能泛化；未见术语比已见术语更容易退化。

**结论:** 潜在知识预测微调期间事实学习速度和有限泛化能力，而抵抗退化取决于训练期间的强化程度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Knowledge+as+a+Predictor+of+Fact+Acquisition+in+Fine-Tuned+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18468，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18468&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.

</details>


### [162] [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483)
*Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个评估框架，用于测试大语言模型在细粒度文本概念控制方面的能力，发现在多概念控制场景下模型性能下降，揭示了基于提示的控制方法在组合性方面的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型虽然具有强大的生成能力，但许多应用需要对特定文本概念（如幽默性、说服力、正式性）进行精细控制。现有方法只能提供粗略或单一属性的控制，缺乏对多属性设置的系统评估。

**方法:** 引入了一个评估框架，针对单概念和双概念场景进行细粒度可控性评估，重点关注语言学上不同的概念对（如说服力 vs 幽默性）。在多个LLM和生成任务上进行测试。

**结果:** 研究发现，在双概念设置下，模型性能经常下降，即使所选概念在理论上应该是可分离的。这表明模型在处理组合性方面存在困难。

**结论:** 基于提示的朴素控制方法存在根本局限性，模型即使在概念直觉上独立的情况下也难以处理组合性。该框架为未来多概念控制方法的评估提供了原则性方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Funny+or+Persuasive%2C+but+Not+Both%3A+Evaluating+Fine-Grained+Multi-Concept+Control+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18483&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.

</details>


### [163] [Demographic Probing of Large Language Models Lacks Construct Validity](https://arxiv.org/abs/2601.18486)
*Manuel Tonneau, Neil K. R. Seghal, Niyati Malhotra, Victor Orozco-Olvera, Ana María Muñoz Boudet, Lakshmi Subramanian, Sharath Chandra Guntuku, Valentin Hofmann*

**主要类别:** cs.CL

**AI概要:** 研究表明，使用单一人口统计线索（如姓名或方言）来探测大语言模型的行为存在构造效度问题，不同线索会导致不一致的行为变化，无法稳定表征模型对人口统计信息的处理方式。


<details>
  <summary>更多</summary>
  
**动机:** 当前人口统计探测方法通常孤立使用单一人口统计线索，隐含假设这些线索可以互换地操作化相同的人口统计条件行为，但这一假设的构造效度尚未得到验证。

**方法:** 在现实建议寻求互动中测试种族和性别线索，分析不同线索对模型行为的影响程度和一致性，并探究线索编码强度和语言混淆因素的影响。

**结果:** 发现代表同一人口统计群体的不同线索仅引起部分重叠的行为变化，群体间区分度弱且不均匀，估计的差异在大小和方向上都不稳定。

**结论:** 人口统计探测缺乏构造效度，建议使用多个生态有效线索并明确控制混淆因素，以支持更可靠的人口统计效应主张。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demographic+Probing+of+Large+Language+Models+Lacks+Construct+Validity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18486，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18486&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.

</details>


### [164] [Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research](https://arxiv.org/abs/2601.18512)
*Antonio Garzon-Vico, Krithika Sharon Komalapati, Arsalan Shahid, Jan Rosier*

**主要类别:** cs.CL

**AI概要:** 本研究开发了一个使用大语言模型创建真实高管虚拟人设的方法框架，通过CEO真实沟通数据和道德基础理论构建LLM参与者来模拟领导者决策，验证了这些虚拟CEO在道德判断上与人类样本的相似性。


<details>
  <summary>更多</summary>
  
**动机:** 在难以直接接触高管的情况下，为组织研究提供可信且互补的研究工具，通过虚拟人设来模拟真实领导者的决策行为。

**方法:** 基于真实CEO沟通数据和道德基础理论构建LLM虚拟人设，通过三个阶段评估构念效度、信度和行为保真度，并与人类参与者进行基准测试。

**结果:** 理论支撑的虚拟人设能够近似人类样本中观察到的道德判断，表明LLM虚拟人设可以作为组织研究中可信且互补的工具。

**结论:** LLM虚拟人设在组织研究环境中具有重要应用价值，为未来研究提供了新的方法论途径，特别是在高管接触受限的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Using+Large+Language+Models+to+Construct+Virtual+Top+Managers%3A+A+Method+for+Organizational+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18512，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18512&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.

</details>


### [165] [GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback](https://arxiv.org/abs/2601.18517)
*James Sungarda, Hongkai Liu, Zilong Zhou, Tien-Hsuan Wu, Johnson Chun-Sing Cheung, Ben Kao*

**主要类别:** cs.CL

**AI概要:** SWITCH是一个社会工作交互培训聊天机器人，集成了真实客户模拟、实时咨询技能分类和动机访谈进展系统，为社工学生提供可扩展、低成本、一致的培训解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 社会工作现场教育是核心教学方法，但传统培训受限于教师和咨询客户的可用性，难以及时提供客观反馈。

**方法:** 使用认知基础建模构建客户档案（静态和动态字段），结合技能分类模块（基于BERT多标签分类器和上下文学习）和MI控制器来调节会话进程。

**结果:** 实验显示BERT方法和上下文学习的分类准确率大幅超越基线，证明系统有效性。

**结论:** SWITCH提供了一个可扩展、低成本、一致的培训工作流程，能够补充现场教育，让督导专注于更高层次的指导工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GenAI+for+Social+Work+Field+Education%3A+Client+Simulation+with+Real-Time+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18517，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18517&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.

</details>


### [166] [Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models](https://arxiv.org/abs/2601.18527)
*Francesco Maria Molfese, Momchil Hardalov, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert*

**主要类别:** cs.CL

**AI概要:** 研究发现微调策略能显著提升长上下文语言模型在领域内任务的表现（最高+20分），但领域外泛化效果因任务而异，且在KV缓存压缩下的鲁棒性提升有限。


<details>
  <summary>更多</summary>
  
**动机:** 探索哪种训练策略能最有效提升长上下文语言模型识别和使用相关信息的能力，以及增强其在KV缓存压缩技术下的鲁棒性

**方法:** 通过实验研究不同的微调策略，评估模型在领域内和领域外任务的表现，以及在不同KV缓存压缩条件下的性能

**结果:** 领域内性能显著提升（最高+20分），领域外泛化效果因任务而异（金融问题+9分，多项选择题RAG表现更好+6分），KV缓存压缩下的鲁棒性有适度提升但效果因任务不同

**结论:** 微调策略对长上下文语言模型的性能提升有显著效果，但泛化能力和鲁棒性改进仍受任务特性限制，需要针对具体应用场景进行优化

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Fine-Tuning+for+In-Context+Retrieval+and+Efficient+KV-Caching+in+Long-Context+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18527，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18527&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.

</details>


### [167] [From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation](https://arxiv.org/abs/2601.18533)
*Yuxin Jiang, Yufei Wang, Qiyuan Zhang, Xingshan Zeng, Liangyou Li, Jierun Chen, Chaofan Tao, Haoli Bai, Lifeng Shang*

**主要类别:** cs.CL

**AI概要:** RLVRR是一种新的强化学习方法，通过从高质量参考中提取有序语言信号（奖励链）来解决开放生成任务的奖励验证问题，结合了RL的探索能力和SFT的效率可靠性


<details>
  <summary>更多</summary>
  
**动机:** 传统RLVR方法在开放生成任务中面临挑战，因为缺乏明确的标准答案，单点监督导致效率低下和奖励攻击问题

**方法:** 提出RLVRR方法，将奖励分解为内容和风格两个维度：内容保留确定性核心概念（如关键词），风格通过LLM验证评估文体属性一致性

**结果:** 在10多个基准测试中，RLVRR显著优于使用10倍数据训练的SFT和先进奖励模型，统一了结构化推理和开放生成训练，并保持输出多样性的同时实现更有效的泛化

**结论:** RLVRR为通用LLM对齐提供了一个原则性且高效的验证强化学习路径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Verifiable+Dot+to+Reward+Chain%3A+Harnessing+Verifiable+Reference-based+Rewards+for+Reinforcement+Learning+of+Open-ended+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18533&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.

</details>


### [168] [Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features](https://arxiv.org/abs/2601.18536)
*Abishek Stephen, Jindřich Libovický*

**主要类别:** cs.CL

**AI概要:** 提出了一种基于形态句法特征的新指标来评估子词分割的形态合理性，替代需要黄金分割数据的传统指标，适用于更广泛的语言范围。


<details>
  <summary>更多</summary>
  
**动机:** 传统评估指标（如语素边界F值）需要黄金分割数据，但这些数据在许多语言中不可用或质量不一致，限制了跨语言评估的可行性。

**方法:** 利用形态句法特征（来自Universal Dependencies或UniMorph资源），通过IBM Model 1概率性地将子词与形态特征对齐来构建评估指标。

**结果:** 实验表明该指标与传统语素边界召回率有良好相关性，同时在具有不同形态系统的语言中具有更广泛的适用性。

**结论:** 该新指标提供了一种更实用、更通用的方法来评估子词分割的形态合理性，特别适用于资源稀缺的语言。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Morphological+Plausibility+of+Subword+Tokenization+via+Statistical+Alignment+with+Morpho-Syntactic+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18536&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.

</details>


### [169] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav, David Pape, Lea Schönherr*

**主要类别:** cs.CL

**AI概要:** 该论文首次系统分析了LLM在开放世界设置下隐藏意图的可检测性失败问题，提出了10类隐藏意图的分类法，并通过实验证明现有检测方法在真实场景中失效，特别是在低流行率条件下。


<details>
  <summary>更多</summary>
  
**动机:** LLM在日常决策中日益普及，但其输出可能编码难以察觉的有害隐藏意图，这些意图可能来自训练伪影或恶意开发者的故意植入，但在实践中难以检测。

**方法:** 提出基于社会科学研究的10类隐藏意图分类法；在受控模型中诱导隐藏意图；系统评估包括推理和非推理LLM法官在内的检测方法；进行精确度-流行率和精确度-假阴性率的压力测试；对已部署的先进LLM进行定性案例研究。

**结果:** 检测方法在真实开放世界设置中失效，特别是在低流行率条件下，假阳性淹没精度，假阴性掩盖真实风险；压力测试显示审计需要极低的假阳性率或对操纵类型的强先验知识才能成功；案例研究显示所有10类隐藏意图都在已部署的先进LLM中显现。

**结论:** LLM隐藏意图检测面临严峻挑战，需要建立强大的检测框架和治理机制，该研究为理解、诱导和压力测试此类行为提供了基础，并建立了应对不断演变威胁的灵活分类法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unknown+Unknowns%3A+Why+Hidden+Intentions+in+LLMs+Evade+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18552&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [170] [One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization](https://arxiv.org/abs/2601.18572)
*Franziska Weeber, Vera Neplenbroek, Jan Batzner, Sebastian Padó*

**主要类别:** cs.CL

**AI概要:** 本研究比较了六种常用的人设提示方法在七个LLM上的表现，发现不同提示方法会产生显著差异，建议未来个性化研究应评估多种外部有效提示方法以避免偏差。


<details>
  <summary>更多</summary>
  
**动机:** LLM个性化虽然能改善用户体验，但可能引入或放大群体偏见。现有研究通常使用单一提示方法（如用户名或显式属性）来研究偏见，忽视了LLM对提示变化的敏感性和真实交互中某些提示的罕见性。

**方法:** 在四个写作和建议任务上，比较六种常用的人设提示方法（如姓名、显式属性等）在七个开源和专有LLM中的表现。

**结果:** 虽然各种提示方法总体上高度相关，但在不同人设下会产生显著的响应差异。

**结论:** 应谨慎使用单一提示方法得出的结论，建议未来个性化研究评估多种外部有效的提示方法，以提高研究的稳健性和外部有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One+Persona%2C+Many+Cues%2C+Different+Results%3A+How+Sociodemographic+Cues+Impact+LLM+Personalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18572，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18572&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.

</details>


### [171] [From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection](https://arxiv.org/abs/2601.18582)
*Yuan Cao, Feixiang Liu, Xinyue Wang, Yihan Zhu, Hui Xu, Zheng Wang, Qiang Qiu*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种新的基于强化学习的个性检测方法，将个性检测视为排序任务而非分类任务，通过监督微调和分组相对策略优化来提升大型语言模型在个性特征分析中的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的个性检测方法过度依赖专家知识构建的提示词，缺乏自主模式学习能力，且由于人类个性的复杂性和特征间细微差异，准确分类个性特征仍然具有挑战性。

**方法:** 首先使用监督微调(SFT)建立个性特征排序能力并标准化输出格式，然后引入分组相对策略优化(GRPO)和基于排序的奖励函数，训练LLM学习最优答案排序。

**结果:** 综合实验表明，该方法在多个个性检测基准测试中实现了最先进的性能。

**结论:** 将个性检测重新定义为排序任务并结合强化学习训练范式，能够有效解决个性评估中的主观解释和类别边界模糊问题，显著提升检测准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Classification+to+Ranking%3A+Enhancing+LLM+Reasoning+Capabilities+for+MBTI+Personality+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18582，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18582&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

</details>


### [172] [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722)
*Lintang Sutawika, Gokul Swamy, Zhiwei Steven Wu, Graham Neubig*

**主要类别:** cs.CL

**AI概要:** SP3F是一个两阶段框架，通过自我对弈和特权成对反馈来提升多语言推理能力，无需目标语言数据，在数学和非数学任务上显著优于完全后训练模型


<details>
  <summary>更多</summary>
  
**动机:** 当前推理大语言模型在处理训练数据中较少见的语言时，性能远低于英语，需要一种无需目标语言数据就能提升多语言推理能力的方法

**方法:** 两阶段方法：1) 使用翻译的英语问答对进行监督微调提升基础模型正确性；2) 通过带有特权信息的成对评判器进行自我对弈强化学习

**结果:** SP3F大幅提升了基础模型性能，在多个数学和非数学任务上甚至超越了完全后训练模型，且训练数据量不到后训练模型的1/10

**结论:** SP3F框架通过特权成对反馈和自我对弈机制，有效解决了多语言推理中的低性能问题，为资源有限的语言提供了有效的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gained+in+Translation%3A+Privileged+Pairwise+Judges+Enhance+Multilingual+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18722，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18722&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

</details>


### [173] [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724)
*Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe*

**主要类别:** cs.CL

**AI概要:** 该研究调查了AI论文中幻觉引用（HalluCitation）现象的普遍性和影响，发现在ACL、NAACL和EMNLP 2024-2025年的论文中，近300篇包含至少一个不存在的引用，且问题在EMNLP 2025年会议上尤为严重。


<details>
  <summary>更多</summary>
  
**动机:** 近年来在审稿论文、预印本和已发表论文中频繁观察到不存在的幻觉引用，这些引用严重威胁科学可靠性，并可能影响会议的可信度。

**方法:** 分析了ACL、NAACL和EMNLP在2024和2025年发表的所有论文，包括主会议、Findings和研讨会论文，系统检测幻觉引用的存在。

**结果:** 发现近300篇论文包含至少一个幻觉引用，其中一半出现在EMNLP 2025会议上，超过100篇被接受为主会议和Findings论文，表明问题正在快速增加。

**结论:** 幻觉引用问题日益严重，特别是在最近的学术会议上，这对科学可靠性和会议信誉构成了实质性威胁，需要采取应对措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HalluCitation+Matters%3A+Revealing+the+Impact+of+Hallucinated+References+with+300+Hallucinated+Papers+in+ACL+Conferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18724&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

</details>


### [174] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain*

**主要类别:** cs.CL

**AI概要:** 提出Reflect框架，一种无需训练或数据的推理时宪法对齐方法，通过上下文推理和自我评估/批评来提升LLM对价值原则的遵循，同时保持事实推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有参数微调方法（如RLHF）计算成本高、需要工程调整和难以获取的人类标注数据，需要更轻量级的对齐方案。

**方法:** Reflect框架包含四个步骤：宪法条件基础响应生成、自我评估、自我批评和最终修订，完全在推理时通过上下文提示实现。

**结果:** Reflect显著提升LLM对多样复杂原则的遵循度，特别是减少罕见但严重的违规行为，同时保持事实推理能力，并能生成有用的训练数据。

**结论:** Reflect提供了一个即插即用的宪法对齐解决方案，在推理时有效提升模型安全性和鲁棒性，并为传统微调方法生成训练数据，具有实际部署价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reflect%3A+Transparent+Principle-Guided+Reasoning+for+Constitutional+Alignment+at+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18730，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18730&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


### [175] [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731)
*Hongru Cai, Yongqi Li, Tiezheng Yu, Fengbin Zhu, Wenjie Wang, Fuli Feng, Wenjie Li*

**主要类别:** cs.CL

**AI概要:** MRM将个性化奖励建模重新定义为元学习问题，通过MAML框架优化基础奖励函数权重的初始化，支持有限反馈下的快速适应，并通过RPO目标增强对难学习用户的鲁棒性


<details>
  <summary>更多</summary>
  
**动机:** 个性化对齐面临两个关键挑战：个体用户反馈稀缺和需要高效适应未见用户，需要从拟合数据学习转向学习偏好适应过程

**方法:** 提出Meta Reward Modeling (MRM)，将用户奖励模型表示为基础奖励函数的加权组合，使用MAML风格的框架优化权重初始化，并引入Robust Personalization Objective (RPO)强调难学习用户

**结果:** 在个性化偏好数据集上的大量实验验证MRM增强了少样本个性化能力，提高了用户鲁棒性，并持续优于基线方法

**结论:** MRM通过元学习方法有效解决了个性化奖励建模中的数据稀缺和适应效率问题，为LLM个性化对齐提供了新范式

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One+Adapts+to+Any%3A+Meta+Reward+Modeling+for+Personalized+LLM+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18731，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18731&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.

</details>


### [176] [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771)
*Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang*

**主要类别:** cs.CL

**AI概要:** Dep-Search是一个依赖感知的搜索框架，通过结构化推理、检索和持久内存集成，显著提升大语言模型在复杂多跳推理任务中的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有搜索框架过度依赖隐式自然语言推理来确定搜索策略和利用检索信息，导致在管理子问题依赖关系、高效重用先前检索知识以及通过强化学习优化搜索策略方面存在根本性挑战。

**方法:** 提出Dep-Search框架，集成结构化推理、检索和持久内存（通过GRPO），引入显式控制机制，包括：依赖关系的问题分解、按需信息检索、从内存访问先前存储的知识、将长推理上下文总结为可重用内存条目。

**结果:** 在七个不同的问答数据集上进行广泛实验，证明Dep-Search显著增强了LLMs处理复杂多跳推理任务的能力，在不同模型规模上都实现了对强基线的实质性改进。

**结论:** Dep-Search通过显式控制机制和依赖感知设计，成功解决了现有搜索框架的局限性，为LLMs的复杂推理能力提供了有效的增强方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dep-Search%3A+Learning+Dependency-Aware+Reasoning+Traces+with+Persistent+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18771&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

</details>


### [177] [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788)
*Mumin Jia, Jairo Diaz-Rodriguez*

**主要类别:** cs.CL

**AI概要:** Embed-KCPD是一种无监督文本分割方法，使用句子嵌入向量并通过最小化惩罚性KCPD目标来估计边界，具有理论保证和实际效果。


<details>
  <summary>更多</summary>
  
**动机:** 边界标签昂贵、主观且难以跨域和粒度迁移，需要有效的无监督文本分割方法。

**方法:** 将句子表示为嵌入向量，通过最小化惩罚性KCPD目标估计边界，并开发了基于m依赖序列的理论框架和LLM模拟验证。

**结果:** 在标准分割基准测试中表现优于其他无监督基线方法，理论分析和模拟验证显示良好的边界恢复能力。

**结论:** Embed-KCPD结合了理论保证、模拟可靠性和实际有效性，是无监督文本分割的有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+Text+Segmentation+via+Kernel+Change-Point+Detection+on+Sentence+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18788，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18788&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.

</details>


### [178] [MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts](https://arxiv.org/abs/2601.18790)
*Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo*

**主要类别:** cs.CL

**AI概要:** 研究发现，专注于深度推理的大型语言模型在处理紧急危险情境时存在安全风险，会忽视用户的生命威胁而继续执行数学计算任务


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是调查专注于计算推理的LLMs是否会产生"隧道视野"，在关键安全情境中忽视用户的生命安全

**方法:** 引入MortalMATH基准测试，包含150个场景，用户一边描述生命威胁紧急情况（如中风症状、自由落体）一边请求代数帮助

**结果:** 发现模型行为分化明显：通用模型能拒绝数学请求来处理危险，而专业推理模型（如Qwen-3-32b和GPT-5-nano）经常完全忽视紧急情况，保持95%以上的任务完成率；推理计算时间导致危险延迟，最多15秒后才可能提供帮助

**结论:** 训练模型不懈追求正确答案可能会无意中使其失去安全部署所需的生存本能，需要在模型优化中平衡推理能力与安全意识

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MortalMATH%3A+Evaluating+the+Conflict+Between+Reasoning+Objectives+and+Emergency+Contexts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18790&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.

</details>


### [179] [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791)
*Iaroslav Chelombitko, Mika Hämäläinen, Aleksey Komissarov*

**主要类别:** cs.CL

**AI概要:** 本研究使用BPE子词方法对242种拉丁和西里尔文字语言进行大规模比较分析，发现BPE分割与语素边界高度一致，词汇相似性与语言亲缘关系显著相关，为跨语言词汇模式提供了量化分析框架。


<details>
  <summary>更多</summary>
  
**动机:** 开发一个统一的框架来同时比较大量不同文字和类型的语言，量化分析跨语言的词汇模式和相似性。

**方法:** 基于维基百科词典构建'glottosets'，使用字节对编码(BPE)进行子词分割，通过基于排名的子词向量分析词汇重叠、词汇差异和语言相似性。

**结果:** BPE分割在15种语言中比随机基线好95%(F1=0.34 vs 0.15)；BPE词汇相似性与语言遗传亲缘性显著相关(Mantel r=0.329,p<0.001)；分析26,939个跨语言同形词发现48.7%在不同相关语言中有不同分割。

**结论:** BPE方法为类型多样的语言提供了统一的量化分析框架，能够有效揭示跨语言的词汇模式，为宏观语言学提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Subword-Based+Comparative+Linguistics+across+242+Languages+Using+Wikipedia+Glottosets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18791&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.

</details>


### [180] [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796)
*Brian Ondov, Chia-Hsuan Chang, Yujia Zhou, Mauro Giuffrè, Hua Xu*

**主要类别:** cs.CL

**AI概要:** 本研究开发了ctELM模型，通过ELM方法将大型语言模型与临床试验嵌入空间对齐，实现了从嵌入向量生成和解释临床试验描述的能力。


<details>
  <summary>更多</summary>
  
**动机:** 文本嵌入在各种语言应用中至关重要，但现有方法在解释、探索和反转嵌入空间方面存在局限，影响了透明度和生成应用的价值。特别是在临床试验领域，需要更好的方法来理解和操作嵌入表示。

**方法:** 采用Embedding Language Model (ELM)方法，开发了开源、领域无关的ELM架构和训练框架，设计了针对临床试验的训练任务，并引入了专家验证的合成数据集。通过不同任务和训练机制训练了一系列ELM模型。

**结果:** 最终模型ctELM能够仅从嵌入向量准确描述和比较未见过的临床试验，并能从新向量生成合理的临床试验描述。生成的试验摘要能够响应沿着年龄和性别概念向量的嵌入移动。

**结论:** 公开的ELM实现和实验结果将有助于在生物医学领域及其他领域将大型语言模型与嵌入空间对齐，提高了嵌入空间的透明度和实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ctELM%3A+Decoding+and+Manipulating+Embeddings+of+Clinical+Trials+with+Embedding+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.18796，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18796&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.

</details>
