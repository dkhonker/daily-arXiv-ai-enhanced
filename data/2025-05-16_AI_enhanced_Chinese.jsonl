{"id": "2505.08828", "pdf": "https://arxiv.org/pdf/2505.08828", "abs": "https://arxiv.org/abs/2505.08828", "authors": ["Eduardo Araujo Oliveira", "Madhavi Mohoni", "Sonsoles L\u00f3pez-Pernas", "Mohammed Saqr"], "title": "Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 pages, 10 figures, 11 tables", "summary": "As human-AI collaboration becomes increasingly prevalent in educational\ncontexts, understanding and measuring the extent and nature of such\ninteractions pose significant challenges. This research investigates the use of\nauthorship verification (AV) techniques not as a punitive measure, but as a\nmeans to quantify AI assistance in academic writing, with a focus on promoting\ntransparency, interpretability, and student development. Building on prior\nwork, we structured our investigation into three stages: dataset selection and\nexpansion, AV method development, and systematic evaluation. Using three\ndatasets - including a public dataset (PAN-14) and two from University of\nMelbourne students from various courses - we expanded the data to include\nLLM-generated texts, totalling 1,889 documents and 540 authorship problems from\n506 students. We developed an adapted Feature Vector Difference AV methodology\nto construct robust academic writing profiles for students, designed to capture\nmeaningful, individual characteristics of their writing. The method's\neffectiveness was evaluated across multiple scenarios, including distinguishing\nbetween student-authored and LLM-generated texts and testing resilience against\nLLMs' attempts to mimic student writing styles. Results demonstrate the\nenhanced AV classifier's ability to identify stylometric discrepancies and\nmeasure human-AI collaboration at word and sentence levels while providing\neducators with a transparent tool to support academic integrity investigations.\nThis work advances AV technology, offering actionable insights into the\ndynamics of academic writing in an AI-driven era.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u4f5c\u8005\u9a8c\u8bc1\u6280\u672f\u91cf\u5316AI\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u534f\u52a9\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u8c03\u67e5\uff0c\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7279\u5f81\u5411\u91cf\u5dee\u5f02AV\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u5b66\u672f\u8bda\u4fe1\u8c03\u67e5\u3002", "motivation": "\u968f\u7740\u4eba-AI\u5408\u4f5c\u5728\u6559\u80b2\u73af\u5883\u4e2d\u65e5\u76ca\u666e\u904d\uff0c\u7406\u89e3\u548c\u8861\u91cf\u8fd9\u79cd\u4e92\u52a8\u7684\u7a0b\u5ea6\u548c\u6027\u8d28\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f5c\u8005\u9a8c\u8bc1\u6280\u672f\u91cf\u5316AI\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u534f\u52a9\uff0c\u4ee5\u4fc3\u8fdb\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b66\u751f\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u6570\u636e\u96c6\u9009\u62e9\u548c\u6269\u5c55\u3001AV\u65b9\u6cd5\u5f00\u53d1\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002\u4f7f\u7528\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u5305\u62ec\u516c\u5171\u6570\u636e\u96c6\uff08PAN-14\uff09\u548c\u58a8\u5c14\u672c\u5927\u5b66\u5b66\u751f\u7684\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u6269\u5c55\u6570\u636e\u4ee5\u5305\u62ecLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u603b\u8ba11,889\u7bc7\u6587\u6863\u548c540\u4e2a\u4f5c\u8005\u95ee\u9898\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7279\u5f81\u5411\u91cf\u5dee\u5f02AV\u65b9\u6cd5\uff0c\u6784\u5efa\u5b66\u751f\u5b66\u672f\u5199\u4f5c\u7684\u7a33\u5065\u6863\u6848\uff0c\u6355\u6349\u5176\u5199\u4f5c\u7684\u6709\u610f\u4e49\u7684\u4e2a\u4f53\u7279\u5f81\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u7684AV\u5206\u7c7b\u5668\u80fd\u591f\u8bc6\u522b\u6587\u4f53\u5dee\u5f02\uff0c\u5e76\u5728\u5355\u8bcd\u548c\u53e5\u5b50\u7ea7\u522b\u8861\u91cf\u4eba-AI\u5408\u4f5c\uff0c\u540c\u65f6\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u900f\u660e\u7684\u5de5\u5177\uff0c\u4ee5\u652f\u6301\u5b66\u672f\u8bda\u4fe1\u8c03\u67e5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86AV\u6280\u672f\uff0c\u4e3aAI\u9a71\u52a8\u65f6\u4ee3\u5b66\u672f\u5199\u4f5c\u7684\u52a8\u6001\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.08891", "pdf": "https://arxiv.org/pdf/2505.08891", "abs": "https://arxiv.org/abs/2505.08891", "authors": ["Daeun Hwang", "Samuel Shields", "Alex Calderwood", "Shi Johnson-Bey", "Michael Mateas", "Noah Wardrip-Fruin", "Edward F. Melcer"], "title": "Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives", "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, 1 appendix. Workshop paper, CHI 2025\n  Augmented Educators and AI", "summary": "Motivation is an important factor underlying successful learning. Previous\nresearch has demonstrated the positive effects that static interactive\nnarrative games can have on motivation. Concurrently, advances in AI have made\ndynamic and adaptive approaches to interactive narrative increasingly\naccessible. However, limited work has explored the impact that dynamic\nnarratives can have on learner motivation. In this paper, we compare two\nversions of Academical, a choice-based educational interactive narrative game\nabout research ethics. One version employs a traditional hand-authored\nbranching plot (i.e., static narrative) while the other dynamically sequences\nplots during play (i.e., dynamic narrative). Results highlight the importance\nof responsive content and a variety of choices for player engagement, while\nalso illustrating the challenge of balancing pedagogical goals with the dynamic\naspects of narrative. We also discuss design implications that arise from these\nfindings. Ultimately, this work provides initial steps to illuminate the\nemerging potential of AI-driven dynamic narrative in educational games.", "AI": {"tldr": "\u7814\u7a76AI\u9a71\u52a8\u7684\u52a8\u6001\u53d9\u4e8b\u5728\u6559\u80b2\u6e38\u620f\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u52a8\u6001\u53d9\u4e8b\u5bf9\u5b66\u4e60\u8005\u52a8\u673a\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u9700\u5e73\u8861\u6559\u5b66\u76ee\u6807\u4e0e\u53d9\u4e8b\u52a8\u6001\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u6210\u529f\u5b66\u4e60\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4ee5\u5f80\u7814\u7a76\u8bc1\u660e\u9759\u6001\u4e92\u52a8\u53d9\u4e8b\u6e38\u620f\u5bf9\u52a8\u673a\u6709\u79ef\u6781\u5f71\u54cd\uff0cAI\u6280\u672f\u7684\u53d1\u5c55\u4f7f\u52a8\u6001\u548c\u81ea\u9002\u5e94\u4e92\u52a8\u53d9\u4e8b\u8d8a\u6765\u8d8a\u53ef\u884c\uff0c\u4f46\u52a8\u6001\u53d9\u4e8b\u5bf9\u5b66\u4e60\u8005\u52a8\u673a\u7684\u5f71\u54cd\u7814\u7a76\u6709\u9650\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u7248\u672c\u7684\u6559\u80b2\u4e92\u52a8\u53d9\u4e8b\u6e38\u620f\u300aAcademical\u300b\uff0c\u4e00\u79cd\u662f\u4f20\u7edf\u7684\u624b\u5de5\u7f16\u5199\u5206\u652f\u5267\u60c5\uff08\u9759\u6001\u53d9\u4e8b\uff09\uff0c\u53e6\u4e00\u79cd\u5728\u6e38\u620f\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6392\u5e8f\u5267\u60c5\uff08\u52a8\u6001\u53d9\u4e8b\uff09\u3002", "result": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u54cd\u5e94\u6027\u5185\u5bb9\u548c\u591a\u79cd\u9009\u62e9\u5bf9\u73a9\u5bb6\u53c2\u4e0e\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u4e5f\u5c55\u793a\u4e86\u5728\u5e73\u8861\u6559\u5b66\u76ee\u6807\u4e0e\u53d9\u4e8b\u52a8\u6001\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aAI\u9a71\u52a8\u7684\u52a8\u6001\u53d9\u4e8b\u5728\u6559\u80b2\u6e38\u620f\u4e2d\u7684\u6f5c\u529b\u63d0\u4f9b\u4e86\u521d\u6b65\u7684\u542f\u793a\u3002"}}
{"id": "2505.08996", "pdf": "https://arxiv.org/pdf/2505.08996", "abs": "https://arxiv.org/abs/2505.08996", "authors": ["Adele E Goldberg", "Supantho Rakshit", "Jennifer Hu", "Kyle Mahowald"], "title": "A suite of LMs comprehend puzzle statements as well as humans", "categories": ["cs.CL"], "comment": null, "summary": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension.", "AI": {"tldr": "\u91cd\u65b0\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u82f1\u8bed\u8bed\u53e5\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4eba\u7c7b\u8868\u73b0\u88ab\u9ad8\u4f30\uff0c\u6a21\u578b\u80fd\u529b\u88ab\u4f4e\u4f30\u3002\u5728\u9650\u5236\u91cd\u8bfb\u7684\u6761\u4ef6\u4e0b\uff0c\u4eba\u7c7b\u51c6\u786e\u7387\u4f4e\u4e8e\u67d0\u4e9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u548c\u6a21\u578b\u5728\u6d89\u53ca\u6f5c\u5728\u4e92\u60e0\u884c\u4e3a\u7684\u67e5\u8be2\u4e0a\u90fd\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u8868\u660e\u5b58\u5728\u5171\u4eab\u7684\u8bed\u7528\u654f\u611f\u6027\uff0c\u800c\u975e\u6a21\u578b\u7279\u5b9a\u7684\u7f3a\u9677\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u989d\u5916\u5206\u6790\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u88ab\u7cfb\u7edf\u6027\u4f4e\u4f30\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u9700\u8981\u66f4\u8c28\u614e\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7f16\u7801\u5b9e\u8df5\uff0c\u5e76\u6311\u6218\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u4e0a\u5929\u751f\u5f31\u4e8e\u4eba\u7c7b\u7684\u5047\u8bbe\u3002", "motivation": "\u91cd\u65b0\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u82f1\u8bed\u8bed\u53e5\u4e0a\u7684\u8868\u73b0\uff0c\u7ea0\u6b63\u4e4b\u524d\u7814\u7a76\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5bf9\u4eba\u7c7b\u8868\u73b0\u7684\u9ad8\u4f30\u548c\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u4f4e\u4f30\u3002", "method": "\u4f7f\u7528\u76f8\u540c\u7684\u523a\u6fc0\u6750\u6599\uff0c\u8fdb\u884c\u9884\u6ce8\u518c\u7814\u7a76\uff0c\u6bd4\u8f83\u4eba\u7c7b\u5728\u5141\u8bb8\u91cd\u8bfb\u548c\u9650\u5236\u91cd\u8bfb\u4e24\u79cd\u6761\u4ef6\u4e0b\u7684\u53cd\u5e94\uff0c\u540c\u65f6\u5bf9\u6bd4\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002\u901a\u8fc7\u989d\u5916\u5206\u6790\u6a21\u578b\u7684\u5bf9\u6570\u6982\u7387\u3001\u5f00\u653e\u6027\u56de\u7b54\u7684\u91cd\u65b0\u7f16\u7801\u4ee5\u53ca\u5bf9\u5176\u4ed6\u53e5\u5b50\u7684\u8bed\u6cd5\u6027\u8bc4\u5206\uff0c\u8fdb\u4e00\u6b65\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u4eba\u7c7b\u5728\u9650\u5236\u91cd\u8bfb\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u7387\uff0873%\uff09\u4f4e\u4e8eFalcon-180B-Chat\uff0876%\uff09\u548cGPT-4\uff0881%\uff09\uff0c\u800c\u65b0\u7684GPT-o1\u6a21\u578b\u8fbe\u5230\u5b8c\u7f8e\u51c6\u786e\u7387\u3002\u4eba\u7c7b\u548c\u6a21\u578b\u5728\u6d89\u53ca\u6f5c\u5728\u4e92\u60e0\u884c\u4e3a\u7684\u67e5\u8be2\u4e0a\u90fd\u9762\u4e34\u6311\u6218\uff0c\u8868\u660e\u5b58\u5728\u5171\u4eab\u7684\u8bed\u7528\u654f\u611f\u6027\u3002\u901a\u8fc7\u989d\u5916\u5206\u6790\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u88ab\u7cfb\u7edf\u6027\u4f4e\u4f30\uff0cGPT-4o\u53ef\u4ee5\u6839\u636e\u63d0\u793a\u6846\u67b6\u4e0e\u65b0\u624b\u6216\u4e13\u5bb6\u7684\u8bed\u6cd5\u6027\u5224\u65ad\u4e00\u81f4\u3002", "conclusion": "\u9700\u8981\u66f4\u8c28\u614e\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7f16\u7801\u5b9e\u8df5\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5f53\u524d\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u4e0a\u5e76\u4e0d\u5929\u751f\u5f31\u4e8e\u4eba\u7c7b\u3002"}}
{"id": "2505.09005", "pdf": "https://arxiv.org/pdf/2505.09005", "abs": "https://arxiv.org/abs/2505.09005", "authors": ["Nicole Cuneo", "Eleanor Graves", "Supantho Rakshit", "Adele E. Goldberg"], "title": "For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies", "categories": ["cs.CL"], "comment": null, "summary": "It remains debated how well any LM understands natural language or generates\nreliable metalinguistic judgments. Moreover, relatively little work has\ndemonstrated that LMs can represent and respect subtle relationships between\nform and function proposed by linguists. We here focus on a particular such\nrelationship established in recent work: English speakers' judgments about the\ninformation structure of canonical sentences predicts independently collected\nacceptability ratings on corresponding 'long distance dependency' [LDD]\nconstructions, across a wide array of base constructions and multiple types of\nLDDs. To determine whether any LM captures this relationship, we probe GPT-4 on\nthe same tasks used with humans and new extensions.Results reveal reliable\nmetalinguistic skill on the information structure and acceptability tasks,\nreplicating a striking interaction between the two, despite the zero-shot,\nexplicit nature of the tasks, and little to no chance of contamination [Studies\n1a, 1b]. Study 2 manipulates the information structure of base sentences and\nconfirms a causal relationship: increasing the prominence of a constituent in a\ncontext sentence increases the subsequent acceptability ratings on an LDD\nconstruction. The findings suggest a tight relationship between natural and\nGPT-4 generated English, and between information structure and syntax, which\nbegs for further exploration.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86GPT-4\u5bf9\u82f1\u8bed\u4fe1\u606f\u7ed3\u6784\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u7ed3\u6784\u7684\u5224\u65ad\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u4f3c\uff0c\u8868\u660e\u81ea\u7136\u8bed\u8a00\u548cGPT-4\u751f\u6210\u82f1\u8bed\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u8054\u7cfb\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5bf9\u81ea\u7136\u8bed\u8a00\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u82f1\u8bed\u4fe1\u606f\u7ed3\u6784\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\uff08LDD\uff09\u7ed3\u6784\u7684\u5224\u65ad\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u8ba9GPT-4\u5b8c\u6210\u4e0e\u4eba\u7c7b\u76f8\u540c\u7684\u4efb\u52a1\uff0c\u5305\u62ec\u4fe1\u606f\u7ed3\u6784\u548c\u63a5\u53d7\u5ea6\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u64cd\u7eb5\u4fe1\u606f\u7ed3\u6784\u6765\u9a8c\u8bc1\u56e0\u679c\u5173\u7cfb\u3002", "result": "GPT-4\u5728\u4fe1\u606f\u7ed3\u6784\u548c\u63a5\u53d7\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u53ef\u9760\u7684\u5143\u8bed\u8a00\u6280\u80fd\uff0c\u590d\u5236\u4e86\u4eba\u7c7b\u7684\u4ea4\u4e92\u6548\u5e94\u3002\u64cd\u7eb5\u4fe1\u606f\u7ed3\u6784\u53ef\u4ee5\u589e\u52a0\u540e\u7eedLDD\u7ed3\u6784\u7684\u63a5\u53d7\u5ea6\u3002", "conclusion": "GPT-4\u751f\u6210\u7684\u82f1\u8bed\u4e0e\u81ea\u7136\u82f1\u8bed\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u8054\u7cfb\uff0c\u4fe1\u606f\u7ed3\u6784\u548c\u53e5\u6cd5\u4e4b\u95f4\u4e5f\u5b58\u5728\u7d27\u5bc6\u5173\u7cfb\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2505.08896", "pdf": "https://arxiv.org/pdf/2505.08896", "abs": "https://arxiv.org/abs/2505.08896", "authors": ["Pankaj Kumar", "Aditya Mishra", "Pranamesh Chakraborty", "Subrahmanya Swamy Peruru"], "title": "Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Developing an autonomous vehicle control strategy for signalised\nintersections (SI) is one of the challenging tasks due to its inherently\ncomplex decision-making process. This study proposes a Deep Reinforcement\nLearning (DRL) based longitudinal vehicle control strategy at SI. A\ncomprehensive reward function has been formulated with a particular focus on\n(i) distance headway-based efficiency reward, (ii) decision-making criteria\nduring amber light, and (iii) asymmetric acceleration/ deceleration response,\nalong with the traditional safety and comfort criteria. This reward function\nhas been incorporated with two popular DRL algorithms, Deep Deterministic\nPolicy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the\ncontinuous action space of acceleration/deceleration. The proposed models have\nbeen trained on the combination of real-world leader vehicle (LV) trajectories\nand simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.\nThe overall performance of the proposed models has been tested using Cumulative\nDistribution Function (CDF) plots and compared with the real-world trajectory\ndata. The results show that the RL models successfully maintain lower distance\nheadway (i.e., higher efficiency) and jerk compared to human-driven vehicles\nwithout compromising safety. Further, to assess the robustness of the proposed\nmodels, we evaluated the model performance on diverse safety-critical\nscenarios, in terms of car-following and traffic signal compliance. Both DDPG\nand SAC models successfully handled the critical scenarios, while the DDPG\nmodel showed smoother action profiles compared to the SAC model. Overall, the\nresults confirm that DRL-based longitudinal vehicle control strategy at SI can\nhelp to improve traffic safety, efficiency, and comfort.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4fe1\u53f7\u706f\u4ea4\u53c9\u53e3\u7eb5\u5411\u8f66\u8f86\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u7efc\u5408\u5956\u52b1\u51fd\u6570\u548cDRL\u7b97\u6cd5\uff08DDPG\u548cSAC\uff09\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u7b56\u7565\u80fd\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u3002", "motivation": "\u5f00\u53d1\u4fe1\u53f7\u706f\u4ea4\u53c9\u53e3\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u7b56\u7565\u662f\u4e00\u9879\u6311\u6218\uff0c\u56e0\u4e3a\u5176\u51b3\u7b56\u8fc7\u7a0b\u590d\u6742\u3002\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7eb5\u5411\u8f66\u8f86\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u5956\u52b1\u51fd\u6570\uff0c\u91cd\u70b9\u5173\u6ce8\u8ddd\u79bb\u95f4\u9694\u6548\u7387\u5956\u52b1\u3001\u9ec4\u706f\u51b3\u7b56\u6807\u51c6\u548c\u975e\u5bf9\u79f0\u52a0\u51cf\u901f\u54cd\u5e94\uff0c\u540c\u65f6\u8003\u8651\u4f20\u7edf\u5b89\u5168\u548c\u8212\u9002\u6027\u6807\u51c6\u3002\u5c06\u8be5\u5956\u52b1\u51fd\u6570\u4e0e\u4e24\u79cd\u6d41\u884c\u7684DRL\u7b97\u6cd5\uff08DDPG\u548cSAC\uff09\u7ed3\u5408\uff0c\u5904\u7406\u8fde\u7eed\u7684\u52a0\u51cf\u901f\u52a8\u4f5c\u7a7a\u95f4\u3002\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u524d\u8f66\u8f68\u8ff9\u548c\u57fa\u4e8eOrnstein-Uhlenbeck\u8fc7\u7a0b\u751f\u6210\u7684\u6a21\u62df\u8f68\u8ff9\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08CDF\uff09\u56fe\u4e0e\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\u6570\u636e\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660eRL\u6a21\u578b\u6210\u529f\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ddd\u79bb\u95f4\u9694\uff08\u5373\u66f4\u9ad8\u6548\u7387\uff09\u548c\u6296\u52a8\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u5b89\u5168\u6027\u3002\u5728\u591a\u79cd\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5305\u62ec\u8ddf\u8f66\u548c\u4ea4\u901a\u4fe1\u53f7\u9075\u5b88\uff09\u4e2d\u8bc4\u4f30\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0cDDPG\u548cSAC\u6a21\u578b\u5747\u6210\u529f\u5904\u7406\u4e86\u8fd9\u4e9b\u573a\u666f\uff0cDDPG\u6a21\u578b\u7684\u52a8\u4f5c\u66f2\u7ebf\u6bd4SAC\u6a21\u578b\u66f4\u5e73\u6ed1\u3002", "conclusion": "\u603b\u4f53\u800c\u8a00\uff0c\u7ed3\u679c\u8bc1\u5b9e\u57fa\u4e8eDRL\u7684\u4fe1\u53f7\u706f\u4ea4\u53c9\u53e3\u7eb5\u5411\u8f66\u8f86\u63a7\u5236\u7b56\u7565\u6709\u52a9\u4e8e\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u3002"}}
{"id": "2505.08792", "pdf": "https://arxiv.org/pdf/2505.08792", "abs": "https://arxiv.org/abs/2505.08792", "authors": ["Michelle Nashla Turcios", "Alicia E. Boyd", "Angela D. R. Smith", "Brittany Johnson"], "title": "A Preliminary Framework for Intersectionality in ML Pipelines", "categories": ["cs.LG", "cs.CY"], "comment": "Accepted for the 1st International Intersectionality and Software\n  Engineering Workshop, colocated with FSE 2025", "summary": "Machine learning (ML) has become a go-to solution for improving how we use,\nexperience, and interact with technology (and the world around us).\nUnfortunately, studies have repeatedly shown that machine learning technologies\nmay not provide adequate support for societal identities and experiences.\nIntersectionality is a sociological framework that provides a mechanism for\nexplicitly considering complex social identities, focusing on social justice\nand power. While the framework of intersectionality can support the development\nof technologies that acknowledge and support all members of society, it has\nbeen adopted and adapted in ways that are not always true to its foundations,\nthereby weakening its potential for impact. To support the appropriate adoption\nand use of intersectionality for more equitable technological outcomes, we\namplify the foundational intersectionality scholarship--Crenshaw, Combahee, and\nCollins (three C's), to create a socially relevant preliminary framework in\ndeveloping machine-learning solutions. We use this framework to evaluate and\nreport on the (mis)alignments of intersectionality application in machine\nlearning literature.", "AI": {"tldr": "Intersectionality framework can support the development of technologies that acknowledge and support all members of society, but it has been adopted and adapted in ways that are not always true to its foundations, thereby weakening its potential for impact.", "motivation": "Machine learning technologies may not provide adequate support for societal identities and experiences.", "method": "Amplify the foundational intersectionality scholarship to create a socially relevant preliminary framework in developing machine-learning solutions.", "result": "Evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.", "conclusion": "Intersectionality framework can support the development of technologies that acknowledge and support all members of society."}}
{"id": "2505.08800", "pdf": "https://arxiv.org/pdf/2505.08800", "abs": "https://arxiv.org/abs/2505.08800", "authors": ["Olivia Nocentini", "Marta Lagomarsino", "Gokhan Solak", "Younggeol Cho", "Qiyi Tong", "Marta Lorenzini", "Arash Ajoudani"], "title": "Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Driver fatigue poses a significant challenge to railway safety, with\ntraditional systems like the dead-man switch offering limited and basic\nalertness checks. This study presents an online behavior-based monitoring\nsystem utilizing a customised Directed-Graph Neural Network (DGNN) to classify\ntrain driver's states into three categories: alert, not alert, and\npathological. To optimize input representations for the model, an ablation\nstudy was performed, comparing three feature configurations: skeletal-only,\nfacial-only, and a combination of both. Experimental results show that\ncombining facial and skeletal features yields the highest accuracy (80.88%) in\nthe three-class model, outperforming models using only facial or skeletal\nfeatures. Furthermore, this combination achieves over 99% accuracy in the\nbinary alertness classification. Additionally, we introduced a novel dataset\nthat, for the first time, incorporates simulated pathological conditions into\ntrain driver monitoring, broadening the scope for assessing risks related to\nfatigue and health. This work represents a step forward in enhancing railway\nsafety through advanced online monitoring using vision-based technologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u7684\u5728\u7ebf\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b9a\u5236\u7684\u6709\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u5bf9\u5217\u8f66\u53f8\u673a\u7684\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\uff0c\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u7684\u6a21\u578b\u5728\u4e09\u5206\u7c7b\u6a21\u578b\u4e2d\u51c6\u786e\u7387\u6700\u9ad8\uff0880.88%\uff09\uff0c\u5728\u4e8c\u5143\u8b66\u89c9\u6027\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u5e76\u5f15\u5165\u4e86\u5305\u542b\u6a21\u62df\u75c5\u7406\u6761\u4ef6\u7684\u65b0\u6570\u636e\u96c6\uff0c\u4ee5\u6269\u5927\u5bf9\u75b2\u52b3\u548c\u5065\u5eb7\u76f8\u5173\u98ce\u9669\u7684\u8bc4\u4f30\u8303\u56f4\u3002", "motivation": "\u4f20\u7edf\u7684\u5217\u8f66\u53f8\u673a\u75b2\u52b3\u76d1\u6d4b\u7cfb\u7edf\uff08\u5982\u6b7b\u4eba\u5f00\u5173\uff09\u529f\u80fd\u6709\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u76d1\u6d4b\u53f8\u673a\u7684\u8b66\u89c9\u6027\u72b6\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5148\u8fdb\u7684\u5728\u7ebf\u76d1\u6d4b\u7cfb\u7edf\u6765\u63d0\u9ad8\u94c1\u8def\u5b89\u5168\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u7684\u5728\u7ebf\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b9a\u5236\u7684\u6709\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u5bf9\u5217\u8f66\u53f8\u673a\u7684\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u7279\u5f81\u914d\u7f6e\uff1a\u4ec5\u9aa8\u9abc\u3001\u4ec5\u9762\u90e8\u548c\u4e24\u8005\u7684\u7ec4\u5408\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u7684\u8f93\u5165\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u7684\u6a21\u578b\u5728\u4e09\u5206\u7c7b\u6a21\u578b\u4e2d\u51c6\u786e\u7387\u6700\u9ad8\uff0880.88%\uff09\uff0c\u5728\u4e8c\u5143\u8b66\u89c9\u6027\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u8d85\u8fc799%\u3002\u6b64\u5916\uff0c\u672c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5c06\u6a21\u62df\u75c5\u7406\u6761\u4ef6\u7eb3\u5165\u5217\u8f66\u53f8\u673a\u76d1\u6d4b\u4e2d\uff0c\u6269\u5927\u4e86\u5bf9\u75b2\u52b3\u548c\u5065\u5eb7\u76f8\u5173\u98ce\u9669\u7684\u8bc4\u4f30\u8303\u56f4\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u5148\u8fdb\u7684\u89c6\u89c9\u6280\u672f\u8fdb\u884c\u5728\u7ebf\u76d1\u6d4b\uff0c\u4e3a\u63d0\u9ad8\u94c1\u8def\u5b89\u5168\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.08828", "pdf": "https://arxiv.org/pdf/2505.08828", "abs": "https://arxiv.org/abs/2505.08828", "authors": ["Eduardo Araujo Oliveira", "Madhavi Mohoni", "Sonsoles L\u00f3pez-Pernas", "Mohammed Saqr"], "title": "Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 pages, 10 figures, 11 tables", "summary": "As human-AI collaboration becomes increasingly prevalent in educational\ncontexts, understanding and measuring the extent and nature of such\ninteractions pose significant challenges. This research investigates the use of\nauthorship verification (AV) techniques not as a punitive measure, but as a\nmeans to quantify AI assistance in academic writing, with a focus on promoting\ntransparency, interpretability, and student development. Building on prior\nwork, we structured our investigation into three stages: dataset selection and\nexpansion, AV method development, and systematic evaluation. Using three\ndatasets - including a public dataset (PAN-14) and two from University of\nMelbourne students from various courses - we expanded the data to include\nLLM-generated texts, totalling 1,889 documents and 540 authorship problems from\n506 students. We developed an adapted Feature Vector Difference AV methodology\nto construct robust academic writing profiles for students, designed to capture\nmeaningful, individual characteristics of their writing. The method's\neffectiveness was evaluated across multiple scenarios, including distinguishing\nbetween student-authored and LLM-generated texts and testing resilience against\nLLMs' attempts to mimic student writing styles. Results demonstrate the\nenhanced AV classifier's ability to identify stylometric discrepancies and\nmeasure human-AI collaboration at word and sentence levels while providing\neducators with a transparent tool to support academic integrity investigations.\nThis work advances AV technology, offering actionable insights into the\ndynamics of academic writing in an AI-driven era.", "AI": {"tldr": "This research investigates the use of authorship verification (AV) techniques to quantify AI assistance in academic writing, promoting transparency and student development. The study is structured into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration.", "motivation": "The motivation for this paper stems from the challenges posed by human-AI collaboration in educational contexts, particularly the need to understand and measure such interactions effectively. The authors aim to promote transparency, interpretability, and student development through the application of AV techniques.", "method": "The method involves structuring the investigation into three stages: 1) Dataset selection and expansion - using three datasets including LLM-generated texts; 2) AV method development - adapting a Feature Vector Difference methodology to capture individual writing characteristics; 3) Systematic evaluation - evaluating effectiveness across multiple scenarios including distinguishing between student-authored and LLM-generated texts.", "result": "The results show that the enhanced AV classifier can successfully identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels. It also demonstrates resilience against LLMs' attempts to mimic student writing styles.", "conclusion": "This work advances AV technology, providing actionable insights into the dynamics of academic writing in an AI-driven era. It offers educators a transparent tool to support academic integrity investigations."}}
{"id": "2505.08891", "pdf": "https://arxiv.org/pdf/2505.08891", "abs": "https://arxiv.org/abs/2505.08891", "authors": ["Daeun Hwang", "Samuel Shields", "Alex Calderwood", "Shi Johnson-Bey", "Michael Mateas", "Noah Wardrip-Fruin", "Edward F. Melcer"], "title": "Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives", "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, 1 appendix. Workshop paper, CHI 2025\n  Augmented Educators and AI", "summary": "Motivation is an important factor underlying successful learning. Previous\nresearch has demonstrated the positive effects that static interactive\nnarrative games can have on motivation. Concurrently, advances in AI have made\ndynamic and adaptive approaches to interactive narrative increasingly\naccessible. However, limited work has explored the impact that dynamic\nnarratives can have on learner motivation. In this paper, we compare two\nversions of Academical, a choice-based educational interactive narrative game\nabout research ethics. One version employs a traditional hand-authored\nbranching plot (i.e., static narrative) while the other dynamically sequences\nplots during play (i.e., dynamic narrative). Results highlight the importance\nof responsive content and a variety of choices for player engagement, while\nalso illustrating the challenge of balancing pedagogical goals with the dynamic\naspects of narrative. We also discuss design implications that arise from these\nfindings. Ultimately, this work provides initial steps to illuminate the\nemerging potential of AI-driven dynamic narrative in educational games.", "AI": {"tldr": "The paper compares two versions of an educational game, one with a static narrative and the other with a dynamic narrative, to explore their impact on learner motivation.", "motivation": "Previous research has shown that static interactive narrative games can positively affect motivation, and advances in AI have made dynamic narratives more accessible. However, there is limited exploration into how dynamic narratives impact learner motivation.", "method": "The study compares two versions of Academical, an educational game about research ethics: one version uses a traditional hand-authored branching plot (static narrative), while the other dynamically sequences plots during play (dynamic narrative).", "result": "Results show that responsive content and a variety of choices are important for player engagement. It also highlights the challenge of balancing pedagogical goals with the dynamic aspects of the narrative.", "conclusion": "This work provides initial insights into the potential of AI-driven dynamic narratives in educational games, discussing design implications from the findings."}}
{"id": "2505.08996", "pdf": "https://arxiv.org/pdf/2505.08996", "abs": "https://arxiv.org/abs/2505.08996", "authors": ["Adele E Goldberg", "Supantho Rakshit", "Jennifer Hu", "Kyle Mahowald"], "title": "A suite of LMs comprehend puzzle statements as well as humans", "categories": ["cs.CL"], "comment": null, "summary": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension.", "AI": {"tldr": "Reanalysis of language model comprehension shows that previous studies overestimated human performance and underestimated LLMs, especially in naturalistic conditions without rereading. Models like Falcon-180B-Chat and GPT-4 outperform humans in certain tasks, and newer models achieve perfect accuracy.", "motivation": "To reassess the claim that large language models underperform humans in comprehending minimally complex English statements by revisiting the experimental setup and considering different conditions.", "method": "Conducted a preregistered study comparing human responses in two conditions: one allowing rereading (replicating an original study) and another restricting rereading (a more naturalistic test). Evaluated LLMs' performance using the same stimuli and additional analyses such as log probabilities, recoding of open-ended responses, and grammaticality ratings.", "result": "Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieved perfect accuracy. Both humans and models struggled with queries involving potentially reciprocal actions, suggesting shared pragmatic sensitivities.", "conclusion": "The findings challenge the assumption that current models are inherently weaker than humans at language comprehension and highlight the need for more careful experimental design and coding practices in evaluating LLMs."}}
{"id": "2505.09005", "pdf": "https://arxiv.org/pdf/2505.09005", "abs": "https://arxiv.org/abs/2505.09005", "authors": ["Nicole Cuneo", "Eleanor Graves", "Supantho Rakshit", "Adele E. Goldberg"], "title": "For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies", "categories": ["cs.CL"], "comment": null, "summary": "It remains debated how well any LM understands natural language or generates\nreliable metalinguistic judgments. Moreover, relatively little work has\ndemonstrated that LMs can represent and respect subtle relationships between\nform and function proposed by linguists. We here focus on a particular such\nrelationship established in recent work: English speakers' judgments about the\ninformation structure of canonical sentences predicts independently collected\nacceptability ratings on corresponding 'long distance dependency' [LDD]\nconstructions, across a wide array of base constructions and multiple types of\nLDDs. To determine whether any LM captures this relationship, we probe GPT-4 on\nthe same tasks used with humans and new extensions.Results reveal reliable\nmetalinguistic skill on the information structure and acceptability tasks,\nreplicating a striking interaction between the two, despite the zero-shot,\nexplicit nature of the tasks, and little to no chance of contamination [Studies\n1a, 1b]. Study 2 manipulates the information structure of base sentences and\nconfirms a causal relationship: increasing the prominence of a constituent in a\ncontext sentence increases the subsequent acceptability ratings on an LDD\nconstruction. The findings suggest a tight relationship between natural and\nGPT-4 generated English, and between information structure and syntax, which\nbegs for further exploration.", "AI": {"tldr": "GPT-4\u5c55\u793a\u4e86\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5143\u8bed\u8a00\u6280\u80fd\uff0c\u80fd\u591f\u7406\u89e3\u4fe1\u606f\u7ed3\u6784\u548c\u53ef\u63a5\u53d7\u6027\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4e14\u7814\u7a76\u53d1\u73b0\u4fe1\u606f\u7ed3\u6784\u5bf9\u957f\u8ddd\u79bb\u4f9d\u8d56\u6784\u9020\u7684\u53ef\u63a5\u53d7\u6027\u6709\u56e0\u679c\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u662f\u5426\u80fd\u591f\u6355\u6349\u5230\u82f1\u8bed\u4f7f\u7528\u8005\u5728\u4fe1\u606f\u7ed3\u6784\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\uff08LDD\uff09\u7ed3\u6784\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u79cd\u5173\u7cfb\u5df2\u7ecf\u5728\u4eba\u7c7b\u4e2d\u88ab\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7\u8ba9GPT-4\u5b8c\u6210\u4e0e\u4eba\u7c7b\u76f8\u540c\u7684\u4efb\u52a1\uff0c\u7814\u7a76\u5176\u5728\u4fe1\u606f\u7ed3\u6784\u548c\u53ef\u63a5\u53d7\u6027\u5224\u65ad\u4e0a\u7684\u8868\u73b0\uff1b\u5e76\u901a\u8fc7\u64cd\u7eb5\u57fa\u7840\u53e5\u5b50\u7684\u4fe1\u606f\u7ed3\u6784\u6765\u786e\u8ba4\u56e0\u679c\u5173\u7cfb\u3002", "result": "GPT-4\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u53ef\u9760\u7684\u5143\u8bed\u8a00\u6280\u80fd\uff0c\u590d\u5236\u4e86\u4fe1\u606f\u7ed3\u6784\u548c\u53ef\u63a5\u53d7\u6027\u4efb\u52a1\u4e4b\u95f4\u7684\u663e\u8457\u4ea4\u4e92\u4f5c\u7528\uff1b\u4fe1\u606f\u7ed3\u6784\u7684\u7a81\u51fa\u7a0b\u5ea6\u5f71\u54cd\u540e\u7eedLDD\u6784\u9020\u7684\u53ef\u63a5\u53d7\u6027\u8bc4\u5206\u3002", "conclusion": "GPT-4\u751f\u6210\u7684\u82f1\u8bed\u4e0e\u81ea\u7136\u82f1\u8bed\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u8054\u7cfb\uff0c\u4fe1\u606f\u7ed3\u6784\u4e0e\u53e5\u6cd5\u4e4b\u95f4\u4e5f\u6709\u5bc6\u5207\u5173\u7cfb\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2505.08896", "pdf": "https://arxiv.org/pdf/2505.08896", "abs": "https://arxiv.org/abs/2505.08896", "authors": ["Pankaj Kumar", "Aditya Mishra", "Pranamesh Chakraborty", "Subrahmanya Swamy Peruru"], "title": "Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Developing an autonomous vehicle control strategy for signalised\nintersections (SI) is one of the challenging tasks due to its inherently\ncomplex decision-making process. This study proposes a Deep Reinforcement\nLearning (DRL) based longitudinal vehicle control strategy at SI. A\ncomprehensive reward function has been formulated with a particular focus on\n(i) distance headway-based efficiency reward, (ii) decision-making criteria\nduring amber light, and (iii) asymmetric acceleration/ deceleration response,\nalong with the traditional safety and comfort criteria. This reward function\nhas been incorporated with two popular DRL algorithms, Deep Deterministic\nPolicy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the\ncontinuous action space of acceleration/deceleration. The proposed models have\nbeen trained on the combination of real-world leader vehicle (LV) trajectories\nand simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.\nThe overall performance of the proposed models has been tested using Cumulative\nDistribution Function (CDF) plots and compared with the real-world trajectory\ndata. The results show that the RL models successfully maintain lower distance\nheadway (i.e., higher efficiency) and jerk compared to human-driven vehicles\nwithout compromising safety. Further, to assess the robustness of the proposed\nmodels, we evaluated the model performance on diverse safety-critical\nscenarios, in terms of car-following and traffic signal compliance. Both DDPG\nand SAC models successfully handled the critical scenarios, while the DDPG\nmodel showed smoother action profiles compared to the SAC model. Overall, the\nresults confirm that DRL-based longitudinal vehicle control strategy at SI can\nhelp to improve traffic safety, efficiency, and comfort.", "AI": {"tldr": "This paper explores a Deep Reinforcement Learning (DRL) based longitudinal vehicle control strategy for autonomous vehicles at signalised intersections, incorporating a comprehensive reward function. It evaluates two DRL algorithms, showing improved traffic safety, efficiency, and comfort.", "motivation": "Existing methods for controlling autonomous vehicles at signalised intersections may not fully address the complex decision-making processes involved, prompting the need for more efficient and safer strategies.", "method": "The method involves developing a comprehensive reward function focused on distance headway-based efficiency, decision-making during amber light, and asymmetric acceleration/deceleration response. This is combined with two DRL algorithms: DDPG and SAC, which handle continuous action spaces of acceleration and deceleration. The models are trained using real-world and simulated trajectories.", "result": "The DRL models successfully maintained lower distance headway and jerk compared to human-driven vehicles without compromising safety. Both DDPG and SAC models handled safety-critical scenarios well, with DDPG showing smoother action profiles.", "conclusion": "A DRL-based longitudinal vehicle control strategy at signalised intersections can enhance traffic safety, efficiency, and comfort."}}
{"id": "2505.08792", "pdf": "https://arxiv.org/pdf/2505.08792", "abs": "https://arxiv.org/abs/2505.08792", "authors": ["Michelle Nashla Turcios", "Alicia E. Boyd", "Angela D. R. Smith", "Brittany Johnson"], "title": "A Preliminary Framework for Intersectionality in ML Pipelines", "categories": ["cs.LG", "cs.CY"], "comment": "Accepted for the 1st International Intersectionality and Software\n  Engineering Workshop, colocated with FSE 2025", "summary": "Machine learning (ML) has become a go-to solution for improving how we use,\nexperience, and interact with technology (and the world around us).\nUnfortunately, studies have repeatedly shown that machine learning technologies\nmay not provide adequate support for societal identities and experiences.\nIntersectionality is a sociological framework that provides a mechanism for\nexplicitly considering complex social identities, focusing on social justice\nand power. While the framework of intersectionality can support the development\nof technologies that acknowledge and support all members of society, it has\nbeen adopted and adapted in ways that are not always true to its foundations,\nthereby weakening its potential for impact. To support the appropriate adoption\nand use of intersectionality for more equitable technological outcomes, we\namplify the foundational intersectionality scholarship--Crenshaw, Combahee, and\nCollins (three C's), to create a socially relevant preliminary framework in\ndeveloping machine-learning solutions. We use this framework to evaluate and\nreport on the (mis)alignments of intersectionality application in machine\nlearning literature.", "AI": {"tldr": "This paper discusses the importance of intersectionality in machine learning to support societal identities and experiences, proposing a framework based on Crenshaw, Combahee, and Collins' scholarship to evaluate its application in ML literature.", "motivation": "Studies have shown that machine learning technologies may not adequately support societal identities and experiences. Intersectionality is a sociological framework that can support the development of inclusive technologies but has been adopted in ways that do not always align with its original foundations.", "method": "The authors amplify the foundational intersectionality scholarship of Crenshaw, Combahee, and Collins to create a socially relevant preliminary framework for developing machine-learning solutions. This framework is then used to evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.", "result": "The proposed framework provides a mechanism for explicitly considering complex social identities in machine learning, focusing on social justice and power. It highlights misalignments in how intersectionality has been applied in machine learning literature.", "conclusion": "Adopting intersectionality through the lens of Crenshaw, Combahee, and Collins can help create more equitable technological outcomes. The framework offers a path towards ensuring that intersectionality is appropriately adopted in machine learning."}}
{"id": "2505.08800", "pdf": "https://arxiv.org/pdf/2505.08800", "abs": "https://arxiv.org/abs/2505.08800", "authors": ["Olivia Nocentini", "Marta Lagomarsino", "Gokhan Solak", "Younggeol Cho", "Qiyi Tong", "Marta Lorenzini", "Arash Ajoudani"], "title": "Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Driver fatigue poses a significant challenge to railway safety, with\ntraditional systems like the dead-man switch offering limited and basic\nalertness checks. This study presents an online behavior-based monitoring\nsystem utilizing a customised Directed-Graph Neural Network (DGNN) to classify\ntrain driver's states into three categories: alert, not alert, and\npathological. To optimize input representations for the model, an ablation\nstudy was performed, comparing three feature configurations: skeletal-only,\nfacial-only, and a combination of both. Experimental results show that\ncombining facial and skeletal features yields the highest accuracy (80.88%) in\nthe three-class model, outperforming models using only facial or skeletal\nfeatures. Furthermore, this combination achieves over 99% accuracy in the\nbinary alertness classification. Additionally, we introduced a novel dataset\nthat, for the first time, incorporates simulated pathological conditions into\ntrain driver monitoring, broadening the scope for assessing risks related to\nfatigue and health. This work represents a step forward in enhancing railway\nsafety through advanced online monitoring using vision-based technologies.", "AI": {"tldr": "This paper presents an online monitoring system for train drivers using a customized DGNN, achieving high accuracy in classifying driver states and introducing a novel dataset with simulated pathological conditions.", "motivation": "Driver fatigue is a significant challenge to railway safety, requiring more advanced systems than traditional ones like the dead-man switch.", "method": "The study utilized a Directed-Graph Neural Network (DGNN) to classify train driver's states into three categories. An ablation study compared three feature configurations: skeletal-only, facial-only, and a combination of both.", "result": "Combining facial and skeletal features yielded the highest accuracy (80.88%) in the three-class model and over 99% accuracy in binary alertness classification. A novel dataset incorporating simulated pathological conditions was also introduced.", "conclusion": "This work enhances railway safety through advanced online monitoring using vision-based technologies."}}
{"id": "2505.09039", "pdf": "https://arxiv.org/pdf/2505.09039", "abs": "https://arxiv.org/abs/2505.09039", "authors": ["Jingfeng Chen", "Raghuveer Thirukovalluru", "Junlin Wang", "Kaiwei Luo", "Bhuwan Dhingra"], "title": "Atomic Consistency Preference Optimization for Long-Form Question Answering", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u4ea7\u751f\u4e8b\u5b9e\u6027\u5e7b\u89c9\uff0c\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACPO\u7684\u81ea\u76d1\u7763\u504f\u597d\u8c03\u6574\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cACPO\u5728LongFact\u548cBioGen\u6570\u636e\u96c6\u4e0a\u6bd4FactAlign\u9ad8\u51fa1.95\u5206\u3002", "motivation": "\u73b0\u6709\u7684\u7f13\u89e3LLMs\u4e8b\u5b9e\u6027\u5e7b\u89c9\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u66f4\u5f3a\u7684\u6a21\u578b\u6216\u5916\u90e8\u77e5\u8bc6\u5e93\u6765\u8bc4\u4f30\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5e76\u4e0d\u603b\u662f\u53ef\u884c\u7684\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8LLMs\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86Atomic Consistency Preference Optimization (ACPO) \u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u504f\u597d\u8c03\u6574\u65b9\u6cd5\u3002ACPO\u5229\u7528\u539f\u5b50\u4e00\u81f4\u6027\u4fe1\u53f7\uff0c\u5373\u591a\u4e2a\u968f\u673a\u54cd\u5e94\u4e2d\u5355\u4e2a\u4e8b\u5b9e\u7684\u4e00\u81f4\u6027\uff0c\u6765\u8bc6\u522b\u9ad8\u8d28\u91cf\u548c\u4f4e\u8d28\u91cf\u7684\u6570\u636e\u5bf9\uff0c\u4ee5\u8fdb\u884c\u6a21\u578b\u5bf9\u9f50\u3002\u901a\u8fc7\u6d88\u9664\u5bf9\u5916\u90e8\u6a21\u578b\u6216\u77e5\u8bc6\u5e93\u7684\u9700\u6c42\uff0cACPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6539\u8fdb\u4e8b\u5b9e\u6027\u95ee\u7b54\u7684\u65b9\u6cd5\u3002", "result": "\u5c3d\u7ba1\u662f\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cACPO\u5728LongFact\u548cBioGen\u6570\u636e\u96c6\u4e0a\u6bd4\u5f3a\u76d1\u7763\u5bf9\u9f50\u57fa\u7ebfFactAlign\u9ad8\u51fa1.95\u5206\u3002", "conclusion": "ACPO\u662f\u4e00\u79cd\u6709\u6548\u7684\u589e\u5f3a\u4e8b\u5b9e\u53ef\u9760\u6027\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u6216\u77e5\u8bc6\u5e93\u3002"}}
{"id": "2505.08905", "pdf": "https://arxiv.org/pdf/2505.08905", "abs": "https://arxiv.org/abs/2505.08905", "authors": ["Michael Majurski", "Cynthia Matuszek"], "title": "Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language Models (LMs) continue to advance, improving response quality and\ncoherence. Given Internet-scale training datasets, LMs have likely encountered\nmuch of what users might ask them to generate in some form during their\ntraining. A plethora of evaluation benchmarks have been constructed to assess\nmodel quality, response appropriateness, and reasoning capabilities. However,\nthe human effort required for benchmark construction is limited and being\nrapidly outpaced by the size and scope of the models under evaluation.\nAdditionally, having humans build a benchmark for every possible domain of\ninterest is impractical. Therefore, we propose a methodology for automating the\nconstruction of fact-based synthetic data model evaluations grounded in\ndocument populations. This work leverages those very same LMs to evaluate\ndomain-specific knowledge automatically, using only grounding documents (e.g.,\na textbook) as input. This synthetic data benchmarking approach corresponds\nwell with human curated questions with a Spearman ranking correlation of 0.96\nand a benchmark evaluation Pearson accuracy correlation of 0.79. This novel\ntool supports generating both multiple choice and open-ended synthetic data\nquestions to gain diagnostic insight of LM capability. We apply this\nmethodology to evaluate model performance on a recent relevant arXiv preprint,\ndiscovering a surprisingly strong performance from Gemma3 models.", "AI": {"tldr": "Language Models are advancing rapidly, but human-made benchmarks can't keep up. This paper proposes automating fact-based synthetic data model evaluations using the LMs themselves, with strong correlations to human-curated questions.", "motivation": "To address the impracticality of humans creating evaluation benchmarks for every domain due to the rapid advancement and scale of Language Models.", "method": "Proposing a methodology that leverages Language Models to automatically evaluate domain-specific knowledge using grounding documents as input, generating both multiple choice and open-ended questions.", "result": "The synthetic data benchmarking approach shows strong correlation with human-curated questions (Spearman ranking correlation of 0.96 and Pearson accuracy correlation of 0.79). When applied to an arXiv preprint, it revealed strong performance from Gemma3 models.", "conclusion": "This novel tool supports gaining diagnostic insight into Language Model capabilities across various domains using automated, fact-based synthetic data evaluations."}}
{"id": "2505.08793", "pdf": "https://arxiv.org/pdf/2505.08793", "abs": "https://arxiv.org/abs/2505.08793", "authors": ["Monirul Islam Pavel", "Siyi Hu", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Onboard Optimization and Learning: A Survey", "categories": ["cs.LG", "cs.AR"], "comment": "36 pages, 5 figures, 3 tables", "summary": "Onboard learning is a transformative approach in edge AI, enabling real-time\ndata processing, decision-making, and adaptive model training directly on\nresource-constrained devices without relying on centralized servers. This\nparadigm is crucial for applications demanding low latency, enhanced privacy,\nand energy efficiency. However, onboard learning faces challenges such as\nlimited computational resources, high inference costs, and security\nvulnerabilities. This survey explores a comprehensive range of methodologies\nthat address these challenges, focusing on techniques that optimize model\nefficiency, accelerate inference, and support collaborative learning across\ndistributed devices. Approaches for reducing model complexity, improving\ninference speed, and ensuring privacy-preserving computation are examined\nalongside emerging strategies that enhance scalability and adaptability in\ndynamic environments. By bridging advancements in hardware-software co-design,\nmodel compression, and decentralized learning, this survey provides insights\ninto the current state of onboard learning to enable robust, efficient, and\nsecure AI deployment at the edge.", "AI": {"tldr": "This paper is a survey that explores methodologies to overcome challenges in onboard learning for edge AI, such as limited resources and security issues, by focusing on model optimization, inference acceleration, and collaborative learning.", "motivation": "Onboard learning in edge AI holds significant promise for real-time data processing, decision-making, and adaptive model training on resource-constrained devices without centralized servers. However, it faces challenges like limited computational resources, high inference costs, and security vulnerabilities.", "method": "The survey examines techniques that optimize model efficiency, accelerate inference, support collaborative learning, reduce model complexity, improve inference speed, ensure privacy-preserving computation, and enhance scalability and adaptability through advancements in hardware-software co-design, model compression, and decentralized learning.", "result": "The analysis provides insights into the current state of onboard learning, highlighting methods that enable robust, efficient, and secure AI deployment at the edge.", "conclusion": "By addressing key challenges through various optimization strategies, onboard learning can achieve more effective and secure AI deployment on edge devices."}}
{"id": "2505.08801", "pdf": "https://arxiv.org/pdf/2505.08801", "abs": "https://arxiv.org/abs/2505.08801", "authors": ["Md. Sakib Hassan Chowdhury", "Md. Hafiz Ahamed", "Bishowjit Paul", "Sarafat Hussain Abhi", "Abu Bakar Siddique", "Md. Robius Sany"], "title": "OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "12 pages, 17 figures", "summary": "Gait recognition, known for its ability to identify individuals from a\ndistance, has gained significant attention in recent times due to its\nnon-intrusive verification. While video-based gait identification systems\nperform well on large public datasets, their performance drops when applied to\nreal-world, unconstrained gait data due to various factors. Among these,\nuncontrolled outdoor environments, non-overlapping camera views, varying\nillumination, and computational efficiency are core challenges in gait-based\nauthentication. Currently, no dataset addresses all these challenges\nsimultaneously. In this paper, we propose an OptiGait-LGBM model capable of\nrecognizing person re-identification under these constraints using a skeletal\nmodel approach, which helps mitigate inconsistencies in a person's appearance.\nThe model constructs a dataset from landmark positions, minimizing memory usage\nby using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to\nrepresent uncontrolled gait sequences in complex outdoor environments. The\nprocess involves extracting skeletal joint landmarks, generating numerical\ndatasets, and developing an OptiGait-LGBM gait classification model. Our aim is\nto address the aforementioned challenges with minimal computational cost\ncompared to existing methods. A comparative analysis with ensemble techniques\nsuch as Random Forest and CatBoost demonstrates that the proposed approach\noutperforms them in terms of accuracy, memory usage, and training time. This\nmethod provides a novel, low-cost, and memory-efficient video-based gait\nrecognition solution for real-world scenarios.", "AI": {"tldr": "The paper proposes an OptiGait-LGBM model for person re-identification using skeletal data, introduces the RUET-GAIT dataset, and demonstrates superior performance in accuracy, memory usage, and training time compared to existing methods.", "motivation": "To address challenges in real-world gait recognition such as uncontrolled environments, varying illumination, non-overlapping camera views, and computational efficiency, which current datasets and models fail to handle simultaneously.", "method": "The method involves constructing a dataset from landmark positions using a skeletal model approach, introducing the RUET-GAIT benchmark dataset, extracting skeletal joint landmarks, generating numerical datasets, and developing the OptiGait-LGBM gait classification model.", "result": "The proposed OptiGait-LGBM model outperforms ensemble techniques like Random Forest and CatBoost in terms of accuracy, memory usage, and training time.", "conclusion": "The paper presents a novel, low-cost, and memory-efficient video-based gait recognition solution for real-world scenarios."}}
{"id": "2505.09056", "pdf": "https://arxiv.org/pdf/2505.09056", "abs": "https://arxiv.org/abs/2505.09056", "authors": ["Brandon Smith", "Mohamed Reda Bouadjenek", "Tahsin Alamgir Kheya", "Phillip Dawson", "Sunil Aryal"], "title": "A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6280\u672f\u4ea4\u4e92\u65b9\u9762\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5176\u8f93\u51fa\u76f8\u4f3c\u6027\u3001\u591a\u6837\u6027\u548c\u4f26\u7406\u5f71\u54cd\u4ecd\u5b58\u7591\u95ee\u3002\u7814\u7a76\u901a\u8fc75000\u4e2a\u63d0\u793a\u8bcd\u548c12\u4e2aLLMs\u751f\u6210\u7ea63\u767e\u4e07\u6587\u672c\u53d1\u73b0\uff1a\u540c\u6a21\u578b\u8f93\u51fa\u66f4\u76f8\u4f3c\uff1bWizardLM-2-8x22b\u8f93\u51fa\u76f8\u4f3c\u5ea6\u9ad8\uff0cGPT-4\u5219\u66f4\u591a\u6837\uff1b\u4e0d\u540c\u6a21\u578b\u5199\u4f5c\u98ce\u683c\u5dee\u5f02\u5927\uff1bLLM\u5185\u5bb9\u6709\u72ec\u7279\u8bed\u8a00\u7279\u5f81\uff1b\u90e8\u5206\u6a21\u578b\u6027\u522b\u5e73\u8861\u66f4\u597d\u3001\u504f\u89c1\u51cf\u5c11\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3aLLM\u672a\u6765\u5f00\u53d1\u548c\u4f26\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5176\u8f93\u51fa\u76f8\u4f3c\u6027\u3001\u591a\u6837\u6027\u548c\u4f26\u7406\u5f71\u54cd\u5c1a\u5b58\u7591\u95ee\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u4ee5\u4e86\u89e3\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u53ca\u7279\u6027\u3002", "method": "\u4f7f\u75285000\u4e2a\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\u7684\u63d0\u793a\u8bcd\uff0c\u4ece\u5305\u62ecOpenAI\u3001Google\u3001Microsoft\u3001Meta\u548cMistral\u5728\u5185\u768412\u4e2aLLMs\uff08\u5305\u542b\u4e13\u6709\u548c\u5f00\u6e90\u7cfb\u7edf\uff09\u751f\u6210\u7ea63\u767e\u4e07\u6587\u672c\u8fdb\u884c\u5206\u6790\u3002", "result": "1. \u540c\u4e00LLM\u7684\u8f93\u51fa\u6bd4\u4e0e\u4eba\u7c7b\u6587\u672c\u66f4\u76f8\u4f3c\uff1b2. WizardLM-2-8x22b\u8f93\u51fa\u9ad8\u5ea6\u76f8\u4f3c\uff0cGPT-4\u8f93\u51fa\u66f4\u591a\u6837\uff1b3. \u4e0d\u540cLLM\u5199\u4f5c\u98ce\u683c\u5dee\u5f02\u5927\uff0cLlama 3\u548cMistral\u76f8\u4f3c\u5ea6\u9ad8\uff0cGPT-4\u72ec\u5177\u7279\u8272\uff1b4. LLM\u8bcd\u6c47\u548c\u8bed\u8c03\u5dee\u5f02\u4f53\u73b0\u5176\u72ec\u7279\u6027\uff1b5. \u90e8\u5206LLM\u6027\u522b\u5e73\u8861\u66f4\u597d\u4e14\u504f\u89c1\u51cf\u5c11\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86LLM\u8f93\u51fa\u7684\u884c\u4e3a\u548c\u591a\u6837\u6027\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765LLM\u7684\u5f00\u53d1\u548c\u4f26\u7406\u8bc4\u4f30\u3002"}}
{"id": "2505.08988", "pdf": "https://arxiv.org/pdf/2505.08988", "abs": "https://arxiv.org/abs/2505.08988", "authors": ["Montaser Mohammedalamen", "Michael Bowling"], "title": "Generalization in Monitored Markov Decision Processes (Mon-MDPs)", "categories": ["cs.AI"], "comment": "Under Review", "summary": "Reinforcement learning (RL) typically models the interaction between the\nagent and environment as a Markov decision process (MDP), where the rewards\nthat guide the agent's behavior are always observable. However, in many\nreal-world scenarios, rewards are not always observable, which can be modeled\nas a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have\nbeen limited to simple, tabular cases, restricting their applicability to\nreal-world problems. This work explores Mon-MDPs using function approximation\n(FA) and investigates the challenges involved. We show that combining function\napproximation with a learned reward model enables agents to generalize from\nmonitored states with observable rewards, to unmonitored environment states\nwith unobservable rewards. Therefore, we demonstrate that such generalization\nwith a reward model achieves near-optimal policies in environments formally\ndefined as unsolvable. However, we identify a critical limitation of such\nfunction approximation, where agents incorrectly extrapolate rewards due to\novergeneralization, resulting in undesirable behaviors. To mitigate\novergeneralization, we propose a cautious police optimization method leveraging\nreward uncertainty. This work serves as a step towards bridging this gap\nbetween Mon-MDP theory and real-world applications.", "AI": {"tldr": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5956\u52b1\u901a\u5e38\u603b\u662f\u53ef\u89c2\u6d4b\u7684\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5956\u52b1\u5e76\u4e0d\u603b\u662f\u53ef\u89c2\u6d4b\u7684\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u76d1\u63a7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Mon-MDP\uff09\u6765\u5efa\u6a21\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u51fd\u6570\u903c\u8fd1\uff08FA\uff09\u89e3\u51b3Mon-MDP\u7684\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u4e2d\u7684\u6311\u6218\u3002\u6211\u4eec\u5c55\u793a\u4e86\u7ed3\u5408\u51fd\u6570\u903c\u8fd1\u4e0e\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u4f7f\u667a\u80fd\u4f53\u4ece\u5177\u6709\u53ef\u89c2\u6d4b\u5956\u52b1\u7684\u76d1\u63a7\u72b6\u6001\u6cdb\u5316\u5230\u5177\u6709\u4e0d\u53ef\u89c2\u6d4b\u5956\u52b1\u7684\u975e\u76d1\u63a7\u73af\u5883\u72b6\u6001\u3002\u8fd9\u6837\u7684\u6cdb\u5316\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u7b56\u7565\uff0c\u5373\u4f7f\u5728\u5f62\u5f0f\u4e0a\u5b9a\u4e49\u4e3a\u4e0d\u53ef\u89e3\u7684\u73af\u5883\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u7136\u800c\uff0c\u6211\u4eec\u4e5f\u53d1\u73b0\u4e86\u4e00\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7531\u4e8e\u8fc7\u62df\u5408\uff0c\u667a\u80fd\u4f53\u4f1a\u9519\u8bef\u5730\u63a8\u65ad\u5956\u52b1\uff0c\u4ece\u800c\u5bfc\u81f4\u4e0d\u671f\u671b\u7684\u884c\u4e3a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5956\u52b1\u4e0d\u786e\u5b9a\u6027\u7684\u8c28\u614e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u662f\u5f25\u5408Mon-MDP\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u5dee\u8ddd\u7684\u4e00\u6b65\u3002", "motivation": "\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5956\u52b1\u5e76\u4e0d\u603b\u662f\u53ef\u89c2\u6d4b\u7684\uff0c\u800c\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u5956\u52b1\u603b\u662f\u53ef\u89c2\u6d4b\u7684\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u4e0d\u53ef\u89c2\u6d4b\u5956\u52b1\u7684\u60c5\u51b5\uff0c\u4ee5\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u66f4\u597d\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u672c\u6587\u4f7f\u7528\u51fd\u6570\u903c\u8fd1\uff08FA\uff09\u548c\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u6765\u89e3\u51b3\u76d1\u63a7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Mon-MDP\uff09\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u4ece\u5177\u6709\u53ef\u89c2\u6d4b\u5956\u52b1\u7684\u76d1\u63a7\u72b6\u6001\u6cdb\u5316\u5230\u5177\u6709\u4e0d\u53ef\u89c2\u6d4b\u5956\u52b1\u7684\u975e\u76d1\u63a7\u73af\u5883\u72b6\u6001\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5956\u52b1\u4e0d\u786e\u5b9a\u6027\u7684\u8c28\u614e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u51fd\u6570\u903c\u8fd1\u4e0e\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u4f7f\u667a\u80fd\u4f53\u5728\u5177\u6709\u4e0d\u53ef\u89c2\u6d4b\u5956\u52b1\u7684\u72b6\u6001\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u4e5f\u53d1\u73b0\u4e86\u7531\u4e8e\u8fc7\u62df\u5408\u5bfc\u81f4\u7684\u5956\u52b1\u9519\u8bef\u63a8\u65ad\u7684\u95ee\u9898\uff0c\u4f46\u63d0\u51fa\u7684\u8c28\u614e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u51fd\u6570\u903c\u8fd1\u548c\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u6765\u89e3\u51b3\u76d1\u63a7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Mon-MDP\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5373\u8fc7\u62df\u5408\u95ee\u9898\u3002\u63d0\u51fa\u7684\u8c28\u614e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u4e3a\u7f13\u89e3\u8fc7\u62df\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u662f\u5f25\u5408Mon-MDP\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u5dee\u8ddd\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.08795", "pdf": "https://arxiv.org/pdf/2505.08795", "abs": "https://arxiv.org/abs/2505.08795", "authors": ["Andres Anabalon", "Hugo Garces", "Julio Oliva", "Jose Cifuentes"], "title": "The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "We show that there is a fast algorithm that embeds hierarchical structures in\nthree-dimensional Minkowski spacetime. The correlation of data ends up purely\nencoded in the causal structure. Our model relies solely on oriented token\npairs -- local hierarchical signals -- with no access to global symbolic\nstructure. We apply our method to the corpus of \\textit{WordNet}. We provide a\nperfect embedding of the mammal sub-tree including ambiguities (more than one\nhierarchy per node) in such a way that the hierarchical structures get\ncompletely codified in the geometry and exactly reproduce the ground-truth. We\nextend this to a perfect embedding of the maximal unambiguous subset of the\n\\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We\nintroduce a novel retrieval mechanism in which causality, not distance, governs\nhierarchical access. Our results seem to indicate that all discrete data has a\nperfect geometrical representation that is three-dimensional. The resulting\nembeddings are nearly conformally invariant, indicating deep connections with\ngeneral relativity and field theory. These results suggest that concepts,\ncategories, and their interrelations, namely hierarchical meaning itself, is\ngeometric.", "AI": {"tldr": "An fast algorithm for embedding hierarchical structures in 3D Minkowski spacetime is presented, which perfectly encodes data correlations in causal structures and applies to datasets like WordNet, suggesting that all discrete data may have a perfect 3D geometric representation.", "motivation": "To find a method that can efficiently embed hierarchical structures into 3D Minkowski spacetime and encode the correlation of data purely within the causal structure without needing global symbolic structure.", "method": "Using oriented token pairs as local hierarchical signals, the model embeds hierarchical structures such as the mammal sub-tree of WordNet with ambiguities. It also extends to a maximal unambiguous subset of WordNet nouns.", "result": "The model successfully provides a perfect embedding of the mammal sub-tree of WordNet including ambiguities and also achieves a perfect embedding of the maximal unambiguous subset of WordNet nouns. A novel retrieval mechanism based on causality rather than distance is introduced.", "conclusion": "All discrete data might have a perfect three-dimensional geometrical representation. The embeddings are nearly conformally invariant, linking to general relativity and field theory. Hierarchical meaning itself appears to be geometric."}}
{"id": "2505.08808", "pdf": "https://arxiv.org/pdf/2505.08808", "abs": "https://arxiv.org/abs/2505.08808", "authors": ["Anqing Jiang", "Jinhao Chai", "Yu Gao", "Yiru Wang", "Yuwen Heng", "Zhigang Sun", "Hao Sun", "Zezhong Zhao", "Li Sun", "Jian Zhou", "Lijuan Zhu", "Shugong Xu", "Hao Zhao"], "title": "SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in high-definition \\emph{HD} map construction have\ndemonstrated the effectiveness of dense representations, which heavily rely on\ncomputationally intensive bird's-eye view \\emph{BEV} features. While sparse\nrepresentations offer a more efficient alternative by avoiding dense BEV\nprocessing, existing methods often lag behind due to the lack of tailored\ndesigns. These limitations have hindered the competitiveness of sparse\nrepresentations in online HD map construction. In this work, we systematically\nrevisit and enhance sparse representation techniques, identifying key\narchitectural and algorithmic improvements that bridge the gap with--and\nultimately surpass--dense approaches. We introduce a dedicated network\narchitecture optimized for sparse map feature extraction, a sparse-dense\nsegmentation auxiliary task to better leverage geometric and semantic cues, and\na denoising module guided by physical priors to refine predictions. Through\nthese enhancements, our method achieves state-of-the-art performance on the\nnuScenes dataset, significantly advancing HD map construction and centerline\ndetection. Specifically, SparseMeXt-Tiny reaches a mean average precision\n\\emph{mAP} of 55.5% at 32 frames per second \\emph{fps}, while SparseMeXt-Base\nattains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large\nachieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for\nsparse representations in HD map construction. These results underscore the\nuntapped potential of sparse methods, challenging the conventional reliance on\ndense representations and redefining efficiency-performance trade-offs in the\nfield.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u8868\u793a\u65b9\u6cd5SparseMeXt\uff0c\u5728\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u5bc6\u96c6\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u7a00\u758f\u8868\u793a\u5728\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u5b9a\u5236\u8bbe\u8ba1\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u4e0e\u5bc6\u96c6\u8868\u793a\u7ade\u4e89\u3002\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5e76\u6539\u8fdb\u7a00\u758f\u8868\u793a\u6280\u672f\u4ee5\u7f29\u5c0f\u4e0e\u5bc6\u96c6\u65b9\u6cd5\u7684\u5dee\u8ddd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e13\u7528\u7684\u7f51\u7edc\u67b6\u6784\u3001\u7a00\u758f-\u5bc6\u96c6\u5206\u5272\u8f85\u52a9\u4efb\u52a1\u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\u7684\u53bb\u566a\u6a21\u5757\uff0c\u8fd9\u4e9b\u6280\u672f\u5171\u540c\u4f18\u5316\u4e86\u7a00\u758f\u5730\u56fe\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5145\u5206\u5229\u7528\u4e86\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "result": "SparseMeXt-Tiny\u3001SparseMeXt-Base\u548cSparseMeXt-Large\u5206\u522b\u8fbe\u5230\u4e8655.5%\u300165.2%\u548c68.9%\u7684mAP\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u5e27\u7387\uff0832 fps\u300120 fps\u4ee5\u4e0a\uff09\u3002\u8fd9\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u7a00\u758f\u65b9\u6cd5\u5177\u6709\u672a\u5f00\u53d1\u7684\u6f5c\u529b\uff0c\u6311\u6218\u4e86\u5bf9\u5bc6\u96c6\u8868\u793a\u7684\u4f20\u7edf\u4f9d\u8d56\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u9886\u57df\u4e2d\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\u3002"}}
{"id": "2505.09068", "pdf": "https://arxiv.org/pdf/2505.09068", "abs": "https://arxiv.org/abs/2505.09068", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "title": "S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/.", "AI": {"tldr": "This paper presents S-DAT, a scalable multilingual framework for assessing divergent thinking using language models and embeddings, showing robust scoring across 11 languages and providing a fairer evaluation tool.", "motivation": "Traditional creativity assessments are labor-intensive, language-specific, and subjective, limiting scalability and cross-cultural applicability.", "method": "S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance as a proxy for divergent thinking, evaluated across 11 diverse languages.", "result": "S-DAT demonstrates robust and consistent scoring across linguistic contexts, shows convergent validity with other DT measures, and correct discriminant validity with convergent thinking.", "conclusion": "S-DAT addresses limitations of earlier approaches by offering cross-linguistic flexibility for inclusive, global-scale creativity research."}}
{"id": "2505.08995", "pdf": "https://arxiv.org/pdf/2505.08995", "abs": "https://arxiv.org/abs/2505.08995", "authors": ["Ardian Selmonaj", "Oleg Szehr", "Giacomo Del Rio", "Alessandro Antonucci", "Adrian Schneider", "Michael R\u00fcegsegger"], "title": "Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "comment": "Published as journal chapter in Deep Learning Applications, Vol. 1,\n  by Taylor & Francis", "summary": "This work presents a Hierarchical Multi-Agent Reinforcement Learning\nframework for analyzing simulated air combat scenarios involving heterogeneous\nagents. The objective is to identify effective Courses of Action that lead to\nmission success within preset simulations, thereby enabling the exploration of\nreal-world defense scenarios at low cost and in a safe-to-fail setting.\nApplying deep Reinforcement Learning in this context poses specific challenges,\nsuch as complex flight dynamics, the exponential size of the state and action\nspaces in multi-agent systems, and the capability to integrate real-time\ncontrol of individual units with look-ahead planning. To address these\nchallenges, the decision-making process is split into two levels of\nabstraction: low-level policies control individual units, while a high-level\ncommander policy issues macro commands aligned with the overall mission\ntargets. This hierarchical structure facilitates the training process by\nexploiting policy symmetries of individual agents and by separating control\nfrom command tasks. The low-level policies are trained for individual combat\ncontrol in a curriculum of increasing complexity. The high-level commander is\nthen trained on mission targets given pre-trained control policies. The\nempirical validation confirms the advantages of the proposed framework.", "AI": {"tldr": "This paper proposes a Hierarchical Multi-Agent Reinforcement Learning framework for simulated air combat scenarios, which divides decision-making into low-level and high-level policies to address challenges such as complex flight dynamics and large state/action spaces.", "motivation": "To identify effective Courses of Action in air combat simulations leading to mission success, allowing real-world defense exploration at low cost and in a safe environment.", "method": "A two-level abstraction is used where low-level policies control individual units and high-level commander policy issues macro commands aligned with mission targets. Low-level policies are trained through increasing complexity curriculums while high-level commander is trained on mission targets given pre-trained control policies.", "result": "Empirical validation confirms the advantages of the proposed framework.", "conclusion": "The Hierarchical Multi-Agent Reinforcement Learning framework successfully addresses the challenges of applying deep reinforcement learning in air combat simulations by exploiting policy symmetries and separating control/command tasks."}}
{"id": "2505.08803", "pdf": "https://arxiv.org/pdf/2505.08803", "abs": "https://arxiv.org/abs/2505.08803", "authors": ["Zizhao Hu", "Mohammad Rostami", "Jesse Thomason"], "title": "Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent research has highlighted the risk of generative model collapse, where\nperformance progressively degrades when continually trained on self-generated\ndata. However, existing exploration on model collapse is limited to single,\nunimodal models, limiting our understanding in more realistic scenarios, such\nas diverse multi-modal AI agents interacting autonomously through synthetic\ndata and continually evolving. We expand the synthetic data training and model\ncollapse study to multi-modal vision-language generative systems, such as\nvision-language models (VLMs) and text-to-image diffusion models, as well as\nrecursive generate-train loops with multiple models. We find that model\ncollapse, previously observed in single-modality generative models, exhibits\ndistinct characteristics in the multi-modal context, such as improved\nvision-language alignment and increased variance in VLM image-captioning task.\nAdditionally, we find that general approaches such as increased decoding\nbudgets, greater model diversity, and relabeling with frozen models can\neffectively mitigate model collapse. Our findings provide initial insights and\npractical guidelines for reducing the risk of model collapse in self-improving\nmulti-agent AI systems and curating robust multi-modal synthetic datasets.", "AI": {"tldr": "This paper investigates model collapse in multi-modal vision-language generative systems and recursive generate-train loops, providing insights and guidelines for mitigating collapse.", "motivation": "To understand model collapse in more realistic scenarios such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving.", "method": "Expand the study of synthetic data training and model collapse to multi-modal vision-language generative systems including VLMs and text-to-image diffusion models, and explore recursive generate-train loops with multiple models.", "result": "Model collapse in multi-modal context shows distinct characteristics like improved vision-language alignment and increased variance in VLM image-captioning task. General approaches can effectively mitigate model collapse.", "conclusion": "The findings provide initial insights and practical guidelines for reducing model collapse risk in self-improving multi-agent AI systems and creating robust multi-modal synthetic datasets."}}
{"id": "2505.08811", "pdf": "https://arxiv.org/pdf/2505.08811", "abs": "https://arxiv.org/abs/2505.08811", "authors": ["Shijie Lian", "Ziyi Zhang", "Laurence Tianruo Yang and", "Mengyu Ren", "Debin Liu", "Hua Li"], "title": "TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Underwater 3D scene reconstruction is crucial for undewater robotic\nperception and navigation. However, the task is significantly challenged by the\ncomplex interplay between light propagation, water medium, and object surfaces,\nwith existing methods unable to model their interactions accurately.\nAdditionally, expensive training and rendering costs limit their practical\napplication in underwater robotic systems. Therefore, we propose Tensorized\nUnderwater Gaussian Splatting (TUGS), which can effectively solve the modeling\nchallenges of the complex interactions between object geometries and water\nmedia while achieving significant parameter reduction. TUGS employs lightweight\ntensorized higher-order Gaussians with a physics-based underwater Adaptive\nMedium Estimation (AME) module, enabling accurate simulation of both light\nattenuation and backscatter effects in underwater environments. Compared to\nother NeRF-based and GS-based methods designed for underwater, TUGS is able to\nrender high-quality underwater images with faster rendering speeds and less\nmemory usage. Extensive experiments on real-world underwater datasets have\ndemonstrated that TUGS can efficiently achieve superior reconstruction quality\nusing a limited number of parameters, making it particularly suitable for\nmemory-constrained underwater UAV applications", "AI": {"tldr": "An underwater 3D scene reconstruction method named Tensorized Underwater Gaussian Splatting (TUGS) is proposed, which can accurately simulate light attenuation and backscatter effects in underwater environments with less memory usage and faster rendering speeds.", "motivation": "Existing methods for underwater 3D scene reconstruction are unable to model the interactions between light propagation, water medium, and object surfaces accurately. Moreover, these methods have expensive training and rendering costs that limit their practical application in underwater robotic systems.", "method": "The proposed method, TUGS, employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module. This enables accurate simulation of both light attenuation and backscatter effects in underwater environments.", "result": "Compared to other NeRF-based and GS-based methods designed for underwater, TUGS can render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated its superior reconstruction quality using a limited number of parameters.", "conclusion": "TUGS can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction, making it particularly suitable for memory-constrained underwater UAV applications."}}
{"id": "2505.09082", "pdf": "https://arxiv.org/pdf/2505.09082", "abs": "https://arxiv.org/abs/2505.09082", "authors": ["Sophie Zhang", "Zhiming Lin"], "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.", "AI": {"tldr": "Recent advancements in large language models (LLMs) have shown excellent Chinese text processing capabilities, especially in Chinese Spelling Correction (CSC). However, there are still challenges in reliability and generalization. This paper proposes CEC-Zero, a new reinforcement learning (RL) framework that allows LLMs to self-correct without external supervision. Experiments show that RL-enhanced LLMs achieve high accuracy and good cross-domain generalization, providing a scalable solution for optimizing reliability in Chinese NLP applications.", "motivation": "Although LLMs outperform traditional models in accuracy and robustness in CSC tasks, issues with reliability and generalization remain unresolved.", "method": "Propose CEC-Zero, a reinforcement learning framework that enables LLMs to self-correct by learning error strategies autonomously without external supervision.", "result": "Experiments demonstrate that RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, reducing reliance on annotated data or auxiliary models.", "conclusion": "CEC-Zero offers a scalable solution to optimize reliability in Chinese NLP applications and establishes a new paradigm for self-improving language models."}}
{"id": "2505.09012", "pdf": "https://arxiv.org/pdf/2505.09012", "abs": "https://arxiv.org/abs/2505.09012", "authors": ["Bo Meng", "Chenghao Xu", "Yongli Zhu"], "title": "Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "Cascading failures in power grids can lead to grid collapse, causing severe\ndisruptions to social operations and economic activities. In certain cases,\nmulti-stage cascading failures can occur. However, existing\ncascading-failure-mitigation strategies are usually single-stage-based,\noverlooking the complexity of the multi-stage scenario. This paper treats the\nmulti-stage cascading failure problem as a reinforcement learning task and\ndevelops a simulation environment. The reinforcement learning agent is then\ntrained via the deterministic policy gradient algorithm to achieve continuous\nactions. Finally, the effectiveness of the proposed approach is validated on\nthe IEEE 14-bus and IEEE 118-bus systems.", "AI": {"tldr": "The paper addresses multi-stage cascading failures in power grids using reinforcement learning, validating the method on IEEE bus systems.", "motivation": "Cascading failures in power grids can lead to severe disruptions, and current mitigation strategies often overlook the complexity of multi-stage scenarios.", "method": "The multi-stage cascading failure problem is treated as a reinforcement learning task, with a simulation environment developed and an agent trained via deterministic policy gradient algorithm for continuous actions.", "result": "The effectiveness of the approach is demonstrated through validation on the IEEE 14-bus and IEEE 118-bus systems.", "conclusion": "Reinforcement learning offers a promising approach to mitigating multi-stage cascading failures in power grids."}}
{"id": "2505.08823", "pdf": "https://arxiv.org/pdf/2505.08823", "abs": "https://arxiv.org/abs/2505.08823", "authors": ["Cody Steinmetz", "Gavin Childress", "Aaron Herbst", "Gavin Jones", "Jasdeep Singh", "Eli Vang", "Keagan Weinstock"], "title": "An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural-language processing,\nyet their scale makes real-world deployment costly. Post-training quantization\nreduces memory and computation but often degrades accuracy, while\nquantization-aware training can recover performance at the cost of extra\ntraining. Pushing quantization to the ternary (2-bit) regime yields even larger\nsavings but is notoriously unstable. Building on recent work showing that a\nbias-free, RMS-normalized Transformer with straight-through estimation can\nreach 1.58-bit precision, we demonstrate that simply inserting RMS\nnormalization before every linear projection and applying a gradual, layer-wise\nquantization schedule stably fine-tunes full-precision checkpoints into ternary\nLLMs. Our approach matches or surpasses more elaborate knowledge-distillation\npipelines on standard language-modeling benchmarks without adding model\ncomplexity. These results indicate that careful normalization alone can close\nmuch of the accuracy gap between ternary and full-precision LLMs, making\nultra-low-bit inference practical.", "AI": {"tldr": "Large language models can be effectively quantized to 2-bit precision using RMS normalization and a layer-wise quantization schedule, maintaining accuracy without added model complexity.", "motivation": "To reduce the cost of deploying large language models by achieving stable ternary (2-bit) quantization without significant accuracy degradation or added model complexity.", "method": "Insert RMS normalization before every linear projection and apply a gradual, layer-wise quantization schedule to fine-tune full-precision checkpoints into ternary LLMs.", "result": "This method matches or surpasses more complex knowledge-distillation pipelines on language-modeling benchmarks.", "conclusion": "Careful normalization can significantly close the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical."}}
{"id": "2505.08814", "pdf": "https://arxiv.org/pdf/2505.08814", "abs": "https://arxiv.org/abs/2505.08814", "authors": ["Wenkai Li", "Xiaoqi Li", "Yingjie Mao", "Yishun Wang"], "title": "Towards Understanding Deep Learning Model in Image Recognition via Coverage Test", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) play a crucial role in the field of artificial\nintelligence, and their security-related testing has been a prominent research\nfocus. By inputting test cases, the behavior of models is examined for\nanomalies, and coverage metrics are utilized to determine the extent of neurons\ncovered by these test cases. With the widespread application and advancement of\nDNNs, different types of neural behaviors have garnered attention, leading to\nthe emergence of various coverage metrics for neural networks. However, there\nis currently a lack of empirical research on these coverage metrics,\nspecifically in analyzing the relationships and patterns between model depth,\nconfiguration information, and neural network coverage. This paper aims to\ninvestigate the relationships and patterns of four coverage metrics: primary\nfunctionality, boundary, hierarchy, and structural coverage. A series of\nempirical experiments were conducted, selecting LeNet, VGG, and ResNet as\ndifferent DNN architectures, along with 10 models of varying depths ranging\nfrom 5 to 54 layers, to compare and study the relationships between different\ndepths, configuration information, and various neural network coverage metrics.\nAdditionally, an investigation was carried out on the relationships between\nmodified decision/condition coverage and dataset size. Finally, three potential\nfuture directions are proposed to further contribute to the security testing of\nDNN Models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u56db\u79cd\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u5ea6\u91cf\uff08\u4e3b\u8981\u529f\u80fd\u3001\u8fb9\u754c\u3001\u5c42\u6b21\u548c\u7ed3\u6784\u8986\u76d6\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u548c\u6a21\u5f0f\uff0c\u4f7f\u7528LeNet\u3001VGG\u548cResNet\u4f5c\u4e3a\u4e0d\u540c\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u4fee\u6539\u540e\u7684\u51b3\u7b56/\u6761\u4ef6\u8986\u76d6\u7387\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u6f5c\u5728\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u8fdb\u6b65\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u795e\u7ecf\u884c\u4e3a\u5f15\u8d77\u4e86\u5173\u6ce8\uff0c\u51fa\u73b0\u4e86\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u5ea6\u91cf\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u8fd9\u4e9b\u8986\u76d6\u5ea6\u91cf\u7684\u7ecf\u9a8c\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u5206\u6790\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u4e4b\u95f4\u7684\u5173\u7cfb\u548c\u6a21\u5f0f\u65b9\u9762\u3002", "method": "\u9009\u62e9LeNet\u3001VGG\u548cResNet\u4f5c\u4e3a\u4e0d\u540c\u7684DNN\u67b6\u6784\uff0c\u9009\u53d610\u4e2a\u6df1\u5ea6\u4ece5\u523054\u5c42\u4e0d\u7b49\u7684\u6a21\u578b\uff0c\u6bd4\u8f83\u548c\u7814\u7a76\u4e0d\u540c\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u540c\u65f6\uff0c\u5bf9\u4fee\u6539\u540e\u7684\u51b3\u7b56/\u6761\u4ef6\u8986\u76d6\u7387\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u4e4b\u95f4\u7684\u5173\u7cfb\u8fdb\u884c\u8c03\u67e5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u56db\u79cd\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u548c\u6a21\u5f0f\uff0c\u4ee5\u53ca\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86\u4fee\u6539\u540e\u7684\u51b3\u7b56/\u6761\u4ef6\u8986\u76d6\u7387\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u5728DNN\u6a21\u578b\u5b89\u5168\u6d4b\u8bd5\u65b9\u9762\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.09269", "pdf": "https://arxiv.org/pdf/2505.09269", "abs": "https://arxiv.org/abs/2505.09269", "authors": ["Ulrich Frank", "Pierre Maier"], "title": "How an unintended Side Effect of a Research Project led to Boosting the Power of UML", "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the design, implementation and use of a new UML modeling\ntool that represents a significant advance over conventional tools. Among other\nthings, it allows the integration of class diagrams and object diagrams as well\nas the execution of objects. This not only enables new software architectures\ncharacterized by the integration of software with corresponding object models,\nbut is also ideal for use in teaching, as it provides students with a\nparticularly stimulating learning experience. A special feature of the project\nis that it has emerged from a long-standing international research project,\nwhich is aimed at a comprehensive multi-level architecture. The project is\ntherefore an example of how research can lead to valuable results that arise as\na side effect of other work.", "AI": {"tldr": "This paper introduces a new UML modeling tool which integrates class diagrams and object diagrams, executes objects, and provides a stimulating learning experience for students. It stems from an international research project focused on multi-level architecture.", "motivation": "To create an advanced UML modeling tool that goes beyond conventional tools by integrating class and object diagrams, executing objects, and providing a valuable teaching aid.", "method": "Design, implement, and utilize a new UML modeling tool emerging from an international research project aimed at a comprehensive multi-level architecture.", "result": "The new tool allows integration of class and object diagrams as well as execution of objects, leading to new software architectures and an engaging educational tool.", "conclusion": "This project exemplifies how research can yield valuable results as side effects of other work."}}
{"id": "2505.09024", "pdf": "https://arxiv.org/pdf/2505.09024", "abs": "https://arxiv.org/abs/2505.09024", "authors": ["Aaron Baughman", "Rahul Agarwal", "Eduardo Morales", "Gozde Akay"], "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, 3 tables", "summary": "We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment.", "AI": {"tldr": "This paper presents a meta-prompting method using agentic reinforcement learning where an LLM as a Judge (LLMaaJ) teaches another LLM to produce content aligning with human mental expectations. Experiments at the US Open 2024 showed 53.8% alignment with human reviewers after an average of 4.38 iterations, enhancing content quality and coverage.", "motivation": "To optimize the similarity between human mental expectations and the neural processing states of Large Language Models when producing complex texts.", "method": "Meta-prompting method combined with agentic reinforcement learning where an LLM (LLMaaJ) instructs another LLM through in-context learning to generate text considering both intended and unintended traits.", "result": "Human content reviewer expectations aligned with AI 53.8% of the time, requiring an average of 4.38 iterations. Content quality improved with extended coverage of tennis action.", "conclusion": "The deployment at the US Open 2024 demonstrated the effectiveness of the method in aligning AI-generated content with human expectations, which has been applied successfully in other sports and entertainment events."}}
{"id": "2505.08827", "pdf": "https://arxiv.org/pdf/2505.08827", "abs": "https://arxiv.org/abs/2505.08827", "authors": ["Toby Simonds", "Kevin Lopez", "Akira Yoshiyama", "Dominique Garmier"], "title": "Self Rewarding Self Improving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We demonstrate that large language models can effectively self-improve\nthrough self-judging without requiring reference solutions, leveraging the\ninherent asymmetry between generating and verifying solutions. Our experiments\non Countdown puzzles and MIT Integration Bee problems show that models can\nprovide reliable reward signals without ground truth answers, enabling\nreinforcement learning in domains previously not possible. By implementing\nself-judging, we achieve significant performance gains maintaining alignment\nwith formal verification. When combined with synthetic question generation, we\nestablish a complete self-improvement loop where models generate practice\nproblems, solve them, and evaluate their own performance-achieving an 8%\nimprovement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on\nintegration tasks. Our findings demonstrate that LLM judges can provide\neffective reward signals for training models, unlocking many reinforcement\nlearning environments previously limited by the difficulty of creating\nprogrammatic rewards. This suggests a potential paradigm shift toward AI\nsystems that continuously improve through self-directed learning rather than\nhuman-guided training, potentially accelerating progress in domains with scarce\ntraining data or complex evaluation requirements.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u6ca1\u6709\u53c2\u8003\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u81ea\u6211\u63d0\u5347\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728 Countdown \u62fc\u56fe\u548c MIT \u79ef\u5206\u8702\u95ee\u9898\u4e0a\uff0c\u6a21\u578b\u53ef\u4ee5\u63d0\u4f9b\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u5728\u4ee5\u524d\u4e0d\u53ef\u80fd\u7684\u9886\u57df\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u3002\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u81ea\u6211\u6539\u8fdb\u5faa\u73af\uff0c\u4f7fQwen 2.5 7B\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e868%\uff0c\u5e76\u5728\u79ef\u5206\u4efb\u52a1\u4e0a\u8d85\u8fc7\u4e86GPT-4o\u7684\u8868\u73b0\u3002\u8fd9\u8868\u660eLLM\u8bc4\u5224\u8005\u80fd\u591f\u4e3a\u8bad\u7ec3\u6a21\u578b\u63d0\u4f9b\u6709\u6548\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u53ef\u80fd\u5e26\u6765\u5411\u81ea\u6211\u5bfc\u5411\u5b66\u4e60\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u9886\u57df\u53d7\u5230\u7f3a\u4e4f\u7f16\u7a0b\u5956\u52b1\u7684\u9650\u5236\uff0c\u96be\u4ee5\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u7b54\u6848\u7684\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u6210\u4e3a\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u3002", "method": "\u5229\u7528\u751f\u6210\u548c\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u56fa\u6709\u4e0d\u5bf9\u79f0\u6027\uff0c\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\u5b9e\u73b0\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb\u3002\u5177\u4f53\u5730\uff0c\u5728 Countdown \u62fc\u56fe\u548c MIT \u79ef\u5206\u8702\u95ee\u9898\u4e0a\u6d4b\u8bd5\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\u6280\u672f\uff0c\u5f62\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u81ea\u6211\u6539\u8fdb\u5faa\u73af\uff1a\u751f\u6210\u7ec3\u4e60\u95ee\u9898\u3001\u89e3\u51b3\u95ee\u9898\u5e76\u8bc4\u4f30\u81ea\u8eab\u6027\u80fd\u3002", "result": "Qwen 2.5 7B\u5b9e\u73b0\u4e868%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8fc7\u57fa\u7ebf\u5e76\u5728\u79ef\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u7684\u8868\u73b0\u3002\u8bc1\u660e\u4e86LLM\u8bc4\u5224\u8005\u80fd\u591f\u63d0\u4f9b\u6709\u6548\u7684\u5956\u52b1\u4fe1\u53f7\u4ee5\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\u548c\u81ea\u6211\u5bfc\u5411\u5b66\u4e60\uff0cAI\u7cfb\u7edf\u80fd\u591f\u6301\u7eed\u6539\u8fdb\uff0c\u53ef\u80fd\u52a0\u901f\u90a3\u4e9b\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u6216\u8bc4\u4f30\u8981\u6c42\u590d\u6742\u7684\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.08817", "pdf": "https://arxiv.org/pdf/2505.08817", "abs": "https://arxiv.org/abs/2505.08817", "authors": ["Camilo Carvajal Reyes", "Joaqu\u00edn Fontbona", "Felipe Tobar"], "title": "Towards SFW sampling for diffusion models via external conditioning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepcted at IJCNN 2025", "summary": "Score-based generative models (SBM), also known as diffusion models, are the\nde facto state of the art for image synthesis. Despite their unparalleled\nperformance, SBMs have recently been in the spotlight for being tricked into\ncreating not-safe-for-work (NSFW) content, such as violent images and\nnon-consensual nudity. Current approaches that prevent unsafe generation are\nbased on the models' own knowledge, and the majority of them require\nfine-tuning. This article explores the use of external sources for ensuring\nsafe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional\nTrajectory Correction step that guides the samples away from undesired regions\nin the ambient space using multimodal models as the source of conditioning.\nFurthermore, using Contrastive Language Image Pre-training (CLIP), our method\nadmits user-defined NSFW classes, which can vary in different settings. Our\nexperiments on the text-to-image SBM Stable Diffusion validate that the\nproposed SFW sampler effectively reduces the generation of explicit content\nwhile being competitive with other fine-tuning-based approaches, as assessed\nvia independent NSFW detectors. Moreover, we evaluate the impact of the SFW\nsampler on image quality and show that the proposed correction scheme comes at\na minor cost with negligible effect on samples not needing correction. Our\nstudy confirms the suitability of the SFW sampler towards aligned SBM models\nand the potential of using model-agnostic conditioning for the prevention of\nunwanted images.", "AI": {"tldr": "Score-based generative models (SBM) are leading in image synthesis but can generate NSFW content. Current prevention methods use model knowledge and fine-tuning. This paper explores using external sources for safe outputs in SBMs, presenting an SFW sampler with Conditional Trajectory Correction that uses multimodal models and CLIP for user-defined NSFW classes. Experiments on Stable Diffusion show the SFW sampler reduces explicit content generation while maintaining competitive performance with minimal impact on image quality.", "motivation": "The motivation is to address the issue of SBMs generating NSFW content and explore methods to ensure safe outputs without relying solely on the model's own knowledge or requiring fine-tuning.", "method": "The method involves implementing a Conditional Trajectory Correction step in the SFW sampler, which uses multimodal models to guide samples away from undesired regions. CLIP is used to allow user-defined NSFW classes, providing flexibility in different settings.", "result": "The experiments demonstrate that the SFW sampler effectively reduces the generation of explicit content while performing competitively with fine-tuning-based approaches. The correction scheme has a minor cost with negligible effect on image quality for samples not needing correction.", "conclusion": "The study concludes that the SFW sampler is suitable for aligned SBM models and highlights the potential of model-agnostic conditioning for preventing unwanted images."}}
{"id": "2505.09286", "pdf": "https://arxiv.org/pdf/2505.09286", "abs": "https://arxiv.org/abs/2505.09286", "authors": ["Jiin Park", "Misuk Kim"], "title": "A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data", "categories": ["cs.CL"], "comment": "36 pages, 3 figures", "summary": "Effectively analyzing online review data is essential across industries.\nHowever, many existing studies are limited to specific domains and languages or\ndepend on supervised learning approaches that require large-scale labeled\ndatasets. To address these limitations, we propose a multilingual, scalable,\nand unsupervised framework for cross-domain aspect detection. This framework is\ndesigned for multi-aspect labeling of multilingual and multi-domain review\ndata. In this study, we apply automatic labeling to Korean and English review\ndatasets spanning various domains and assess the quality of the generated\nlabels through extensive experiments. Aspect category candidates are first\nextracted through clustering, and each review is then represented as an\naspect-aware embedding vector using negative sampling. To evaluate the\nframework, we conduct multi-aspect labeling and fine-tune several pretrained\nlanguage models to measure the effectiveness of the automatically generated\nlabels. Results show that these models achieve high performance, demonstrating\nthat the labels are suitable for training. Furthermore, comparisons with\npublicly available large language models highlight the framework's superior\nconsistency and scalability when processing large-scale data. A human\nevaluation also confirms that the quality of the automatic labels is comparable\nto those created manually. This study demonstrates the potential of a robust\nmulti-aspect labeling approach that overcomes limitations of supervised methods\nand is adaptable to multilingual, multi-domain environments. Future research\nwill explore automatic review summarization and the integration of artificial\nintelligence agents to further improve the efficiency and depth of review\nanalysis.", "AI": {"tldr": "Effectively analyzing online review data is essential across industries. This study proposes a multilingual, scalable, and unsupervised framework for cross-domain aspect detection, demonstrating its effectiveness through various experiments.", "motivation": "Existing studies on analyzing online review data are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets.", "method": "The method involves applying automatic labeling to Korean and English review datasets spanning various domains. Aspect category candidates are extracted through clustering, and each review is represented as an aspect-aware embedding vector using negative sampling.", "result": "The models achieve high performance in multi-aspect labeling, showing that the automatically generated labels are suitable for training. The framework exhibits superior consistency and scalability compared to publicly available large language models. Human evaluation confirms the quality of the automatic labels.", "conclusion": "This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments."}}
{"id": "2505.09029", "pdf": "https://arxiv.org/pdf/2505.09029", "abs": "https://arxiv.org/abs/2505.09029", "authors": ["Hazim Alzorgan", "Abolfazl Razi"], "title": "Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient\n(TD3), depend on basic noise-based exploration, which can result in less than\noptimal policy convergence. In this study, we introduce Monte Carlo Beam Search\n(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts\nwith TD3 to improve exploration and action selection. MCBS produces several\ncandidate actions around the policy's output and assesses them through\nshort-horizon rollouts, enabling the agent to make better-informed choices. We\ntest MCBS across various continuous-control benchmarks, including\nHalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency\nand performance compared to standard TD3 and other baseline methods like SAC,\nPPO, and A2C. Our findings emphasize MCBS's capability to enhance policy\nlearning through structured look-ahead search while ensuring computational\nefficiency. Additionally, we offer a detailed analysis of crucial\nhyperparameters, such as beam width and rollout depth, and explore adaptive\nstrategies to optimize MCBS for complex control tasks. Our method shows a\nhigher convergence rate across different environments compared to TD3, SAC,\nPPO, and A2C. For instance, we achieved 90% of the maximum achievable reward\nwithin around 200 thousand timesteps compared to 400 thousand timesteps for the\nsecond-best method.", "AI": {"tldr": "MCBS\u662f\u4e00\u79cd\u7ed3\u5408\u4e86TD3\u3001\u675f\u641c\u7d22\u548c\u8499\u7279\u5361\u6d1brollouts\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u548c\u52a8\u4f5c\u9009\u62e9\u3002\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u8f83\u4e8eTD3\u3001SAC\u3001PPO\u548cA2C\u7b49\u65b9\u6cd5\uff0cMCBS\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u566a\u58f0\u7684\u63a2\u7d22\u65b9\u6cd5\uff08\u5982TD3\uff09\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684\u7b56\u7565\u6536\u655b\u3002\u4e3a\u4e86\u6539\u8fdb\u63a2\u7d22\u548c\u52a8\u4f5c\u9009\u62e9\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aMonte Carlo Beam Search (MCBS)\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u675f\u641c\u7d22\u548c\u8499\u7279\u5361\u6d1brollouts\u4e0eTD3\u7ed3\u5408\u3002MCBS\u901a\u8fc7\u751f\u6210\u5019\u9009\u52a8\u4f5c\u5e76\u8fdb\u884c\u77ed\u89c6\u57dfrollouts\u8bc4\u4f30\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u505a\u51fa\u66f4\u660e\u667a\u7684\u9009\u62e9\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u5173\u952e\u8d85\u53c2\u6570\uff08\u5982\u675f\u5bbd\u548crollout\u6df1\u5ea6\uff09\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff0c\u5e76\u63a2\u8ba8\u4e86\u9002\u5e94\u6027\u7b56\u7565\u4ee5\u4f18\u5316\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u7684MCBS\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMCBS\u5728\u5305\u62ecHalfCheetah-v4\u3001Walker2d-v5\u548cSwimmer-v5\u5728\u5185\u7684\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u6bd4TD3\u3001SAC\u3001PPO\u548cA2C\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u7ea620\u4e07\u6b65\u5185\u8fbe\u5230\u4e86\u6700\u5927\u53ef\u5b9e\u73b0\u5956\u52b1\u768490%\uff0c\u800c\u7b2c\u4e8c\u597d\u7684\u65b9\u6cd5\u5219\u9700\u8981\u7ea640\u4e07\u6b65\u3002", "conclusion": "MCBS\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u524d\u77bb\u641c\u7d22\u589e\u5f3a\u4e86\u7b56\u7565\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u5b83\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4e3a\u590d\u6742\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08829", "pdf": "https://arxiv.org/pdf/2505.08829", "abs": "https://arxiv.org/abs/2505.08829", "authors": ["David Kinney"], "title": "Aggregating Concepts of Fairness and Accuracy in Predictive Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "An algorithm that outputs predictions about the state of the world will\nalmost always be designed with the implicit or explicit goal of outputting\naccurate predictions (i.e., predictions that are likely to be true). In\naddition, the rise of increasingly powerful predictive algorithms brought about\nby the recent revolution in artificial intelligence has led to an emphasis on\nbuilding predictive algorithms that are fair, in the sense that their\npredictions do not systematically evince bias or bring about harm to certain\nindividuals or groups. This state of affairs presents two conceptual\nchallenges. First, the goals of accuracy and fairness can sometimes be in\ntension, and there are no obvious normative guidelines for managing the\ntrade-offs between these two desiderata when they arise. Second, there are many\ndistinct ways of measuring both the accuracy and fairness of a predictive\nalgorithm; here too, there are no obvious guidelines on how to aggregate our\npreferences for predictive algorithms that satisfy disparate measures of\nfairness and accuracy to various extents. The goal of this paper is to address\nthese challenges by arguing that there are good reasons for using a linear\ncombination of accuracy and fairness metrics to measure the\nall-things-considered value of a predictive algorithm for agents who care about\nboth accuracy and fairness. My argument depends crucially on a classic result\nin the preference aggregation literature due to Harsanyi. After making this\nformal argument, I apply my result to an analysis of accuracy-fairness\ntrade-offs using the COMPAS dataset compiled by Angwin et al.", "AI": {"tldr": "This paper argues for using a linear combination of accuracy and fairness metrics to measure the overall value of predictive algorithms, based on Harsanyi's result in preference aggregation. It also analyzes accuracy-fairness trade-offs using the COMPAS dataset.", "motivation": "The motivation is to address the conceptual challenges of balancing accuracy and fairness in predictive algorithms, as well as determining how to aggregate preferences for different measures of these qualities.", "method": "The method involves making a formal argument based on Harsanyi's classic result in preference aggregation literature to justify the use of a linear combination of accuracy and fairness metrics. This approach is then applied to analyze accuracy-fairness trade-offs using the COMPAS dataset.", "result": "The result is a justification for using a linear combination of accuracy and fairness metrics to evaluate the overall value of predictive algorithms, with an application to the COMPAS dataset illustrating the analysis of such trade-offs.", "conclusion": "The conclusion emphasizes that there are good reasons to use a linear combination of accuracy and fairness metrics when evaluating predictive algorithms, providing a normative guideline for managing trade-offs between accuracy and fairness."}}
{"id": "2505.08833", "pdf": "https://arxiv.org/pdf/2505.08833", "abs": "https://arxiv.org/abs/2505.08833", "authors": ["Qingyi Wang", "Yuebing Liang", "Yunhan Zheng", "Kaiyuan Xu", "Jinhua Zhao", "Shenhao Wang"], "title": "Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Generative AI offers new opportunities for automating urban planning by\ncreating site-specific urban layouts and enabling flexible design exploration.\nHowever, existing approaches often struggle to produce realistic and practical\ndesigns at scale. Therefore, we adapt a state-of-the-art Stable Diffusion\nmodel, extended with ControlNet, to generate high-fidelity satellite imagery\nconditioned on land use descriptions, infrastructure, and natural environments.\nTo overcome data availability limitations, we spatially link satellite imagery\nwith structured land use and constraint information from OpenStreetMap. Using\ndata from three major U.S. cities, we demonstrate that the proposed diffusion\nmodel generates realistic and diverse urban landscapes by varying land-use\nconfigurations, road networks, and water bodies, facilitating cross-city\nlearning and design diversity. We also systematically evaluate the impacts of\nvarying language prompts and control imagery on the quality of satellite\nimagery generation. Our model achieves high FID and KID scores and demonstrates\nrobustness across diverse urban contexts. Qualitative assessments from urban\nplanners and the general public show that generated images align closely with\ndesign descriptions and constraints, and are often preferred over real images.\nThis work establishes a benchmark for controlled urban imagery generation and\nhighlights the potential of generative AI as a tool for enhancing planning\nworkflows and public engagement.", "AI": {"tldr": "The paper adapts a Stable Diffusion model with ControlNet to generate high-fidelity satellite imagery for urban planning, conditioned on land use descriptions, infrastructure, and natural environments. It achieves realistic urban landscapes, high FID/KID scores, and positive feedback from planners.", "motivation": "Existing generative AI approaches struggle to produce realistic and practical urban designs at scale.", "method": "Adapted a state-of-the-art Stable Diffusion model extended with ControlNet, spatially linking satellite imagery with structured land use and constraint information from OpenStreetMap using data from three major U.S. cities.", "result": "Generated realistic and diverse urban landscapes, achieved high FID and KID scores, showed robustness across urban contexts, and received positive qualitative assessments from urban planners and the public.", "conclusion": "Established a benchmark for controlled urban imagery generation and demonstrated the potential of generative AI in enhancing planning workflows and public engagement."}}
{"id": "2505.09316", "pdf": "https://arxiv.org/pdf/2505.09316", "abs": "https://arxiv.org/abs/2505.09316", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging", "categories": ["cs.CL", "cs.IR"], "comment": "16 pages", "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.", "AI": {"tldr": "The paper introduces InForage, a reinforcement learning framework that improves LLMs' information retrieval through dynamic and adaptive search behaviors.", "motivation": "To address the limitations of static retrieval strategies in complex tasks involving ambiguous, multi-step, or evolving information needs.", "method": "Proposes InForage, inspired by Information Foraging Theory (IFT), which formalizes retrieval-augmented reasoning as a dynamic information-seeking process. It rewards intermediate retrieval quality to encourage iterative gathering and integration of information.", "result": "InForage outperforms baseline methods in general question answering, multi-hop reasoning tasks, and a new real-time web QA dataset.", "conclusion": "InForage is effective in building robust, adaptive, and efficient reasoning agents for LLMs."}}
{"id": "2505.09031", "pdf": "https://arxiv.org/pdf/2505.09031", "abs": "https://arxiv.org/abs/2505.09031", "authors": ["Adarsh Kumar", "Hwiyoon Kim", "Jawahar Sai Nathani", "Neil Roy"], "title": "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Hallucination, where large language models (LLMs) generate confident but\nincorrect or irrelevant information, remains a key limitation in their\napplication to complex, open-ended tasks. Chain-of-thought (CoT) prompting has\nemerged as a promising method for improving multistep reasoning by guiding\nmodels through intermediate steps. However, CoT alone does not fully address\nthe hallucination problem. In this work, we investigate how combining CoT with\nretrieval-augmented generation (RAG), as well as applying self-consistency and\nself-verification strategies, can reduce hallucinations and improve factual\naccuracy. By incorporating external knowledge sources during reasoning and\nenabling models to verify or revise their own outputs, we aim to generate more\naccurate and coherent responses. We present a comparative evaluation of\nbaseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification\ntechniques. Our results highlight the effectiveness of each method and identify\nthe most robust approach for minimizing hallucinations while preserving fluency\nand reasoning depth.", "AI": {"tldr": "\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4(CoT)\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u4ee5\u53ca\u81ea\u6211\u4e00\u81f4\u6027\u4e0e\u81ea\u6211\u9a8c\u8bc1\u7b56\u7565\uff0c\u53ef\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u5e76\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u672c\u6587\u5bf9\u6bd4\u4e86\u57fa\u7840LLM\u4e0eCoT\u3001CoT+RAG\u3001\u81ea\u6211\u4e00\u81f4\u6027\u53ca\u81ea\u6211\u9a8c\u8bc1\u6280\u672f\u7684\u6548\u679c\uff0c\u627e\u51fa\u4e86\u5728\u4fdd\u6301\u6d41\u7545\u6027\u548c\u63a8\u7406\u6df1\u5ea6\u7684\u540c\u65f6\uff0c\u6700\u5c0f\u5316\u5e7b\u89c9\u7684\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u94fe\u5f0f\u601d\u7ef4(CoT)\u80fd\u6539\u5584\u591a\u6b65\u9aa4\u63a8\u7406\uff0c\u4f46\u5b83\u5355\u72ec\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u5176\u4ed6\u65b9\u6cd5\u6765\u8fdb\u4e00\u6b65\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u56de\u7b54\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u5c06\u94fe\u5f0f\u601d\u7ef4(CoT)\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u76f8\u7ed3\u5408\uff0c\u5e76\u5e94\u7528\u81ea\u6211\u4e00\u81f4\u6027\u548c\u81ea\u6211\u9a8c\u8bc1\u7b56\u7565\u3002\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u6e90\u548c\u8ba9\u6a21\u578b\u9a8c\u8bc1\u6216\u4fee\u6b63\u81ea\u8eab\u8f93\u51fa\uff0c\u751f\u6210\u66f4\u51c6\u786e\u8fde\u8d2f\u7684\u54cd\u5e94\u3002", "result": "\u6bcf\u79cd\u65b9\u6cd5\u90fd\u6709\u5176\u6548\u679c\uff0c\u7814\u7a76\u786e\u5b9a\u4e86\u5728\u4fdd\u6301\u8bed\u8a00\u6d41\u7545\u6027\u548c\u63a8\u7406\u6df1\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u51cf\u5c11\u5e7b\u89c9\u7684\u6700\u7a33\u5065\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408CoT\u4e0eRAG\uff0c\u540c\u65f6\u4f7f\u7528\u81ea\u6211\u4e00\u81f4\u6027\u548c\u81ea\u6211\u9a8c\u8bc1\u7b56\u7565\uff0c\u662f\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u6709\u6548\u9014\u5f84\u3002\u8fd9\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u7cbe\u786e\u548c\u53ef\u9760\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.08846", "pdf": "https://arxiv.org/pdf/2505.08846", "abs": "https://arxiv.org/abs/2505.08846", "authors": ["Felix Marti-Perez", "Brigt H\u00e5vardstun", "C\u00e8sar Ferri", "Carlos Monserrat", "Jan Arne Telle"], "title": "Evaluating Simplification Algorithms for Interpretability of Time Series Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we introduce metrics to evaluate the use of simplified time\nseries in the context of interpretability of a TSC - a Time Series Classifier.\nSuch simplifications are important because time series data, in contrast to\ntext and image data, are not intuitively understandable to humans. These\nmetrics are related to the complexity of the simplifications - how many\nsegments they contain - and to their loyalty - how likely they are to maintain\nthe classification of the original time series. We employ these metrics to\nevaluate four distinct simplification algorithms, across several TSC algorithms\nand across datasets of varying characteristics, from seasonal or stationary to\nshort or long. Our findings suggest that using simplifications for\ninterpretability of TSC is much better than using the original time series,\nparticularly when the time series are seasonal, non-stationary and/or with low\nentropy.", "AI": {"tldr": "This paper introduces metrics to evaluate simplified time series for interpretability of Time Series Classifier (TSC), showing that simplifications are better than original time series in certain conditions.", "motivation": "Time series data is not intuitively understandable to humans, unlike text and image data. Simplified time series could improve interpretability of TSC.", "method": "Introduced metrics related to complexity (number of segments) and loyalty (likelihood to maintain classification). Evaluated four distinct simplification algorithms across several TSC algorithms and datasets with varying characteristics.", "result": "Simplifications for interpretability of TSC are much better than using the original time series, especially when the time series are seasonal, non-stationary and/or with low entropy.", "conclusion": "Simplified time series can significantly enhance the interpretability of TSCs under specific conditions."}}
{"id": "2505.08834", "pdf": "https://arxiv.org/pdf/2505.08834", "abs": "https://arxiv.org/abs/2505.08834", "authors": ["Muhammad Junaid Asif"], "title": "Crowd Scene Analysis using Deep Learning Techniques", "categories": ["cs.CV", "cs.AI"], "comment": "MS Graduate Research Thesis", "summary": "Our research is focused on two main applications of crowd scene analysis\ncrowd counting and anomaly detection In recent years a large number of\nresearches have been presented in the domain of crowd counting We addressed two\nmain challenges in this domain 1 Deep learning models are datahungry paradigms\nand always need a large amount of annotated data for the training of algorithm\nIt is timeconsuming and costly task to annotate such large amount of data\nSelfsupervised training is proposed to deal with this challenge 2 MCNN consists\nof multicolumns of CNN with different sizes of filters by presenting a novel\napproach based on a combination of selfsupervised training and MultiColumn CNN\nThis enables the model to learn features at different levels and makes it\neffective in dealing with challenges of occluded scenes nonuniform density\ncomplex backgrounds and scale invariation The proposed model was evaluated on\npublicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE\nand MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly\ndetection addressing challenges like lighting environmental conditions\nunexpected objects and scalability The model extracts spatial and temporal\nfeatures allowing it to be generalized to realworld scenes Spatial features are\nlearned using CNN while temporal features are learned using LSTM blocks The\nmodel works on binary classification and can detect normal or abnormal behavior\nThe models performance is improved by replacing fully connected layers with\ndense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset\nshow our models outperform other stateoftheart approaches", "AI": {"tldr": "This paper proposes a combination of self-supervised training and Multi-Column CNN for crowd counting, and a spatiotemporal model based on VGG19 for crowd anomaly detection, both showing superior performance compared to state-of-the-art methods.", "motivation": "To address the challenges in crowd scene analysis including the need for large annotated datasets and the difficulties posed by occluded scenes, nonuniform density, complex backgrounds, and scale invariance in crowd counting; and lighting, environmental conditions, unexpected objects, and scalability in crowd anomaly detection.", "method": "For crowd counting, the method combines self-supervised training with Multi-Column CNN. For crowd anomaly detection, a spatiotemporal model based on VGG19 is used where spatial features are learned using CNN and temporal features using LSTM blocks, with dense residual blocks replacing fully connected layers.", "result": "The proposed crowd counting model achieves better results on ShanghaiTech and UCF-QNRF datasets in terms of MAE and MSE. The crowd anomaly detection model outperforms other state-of-the-art approaches on the Hockey Fight dataset and SCVD dataset.", "conclusion": "The proposed models effectively tackle the challenges in crowd counting and crowd anomaly detection, achieving superior performance over existing state-of-the-art methods."}}
{"id": "2505.09338", "pdf": "https://arxiv.org/pdf/2505.09338", "abs": "https://arxiv.org/abs/2505.09338", "authors": ["Jingcheng Niu", "Xingdi Yuan", "Tong Wang", "Hamidreza Saghir", "Amir H. Abdi"], "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.", "AI": {"tldr": "This paper observes a novel phenomenon called contextual entrainment in language models, where LMs are distracted by irrelevant contextual information. The authors hypothesize the existence of 'entrainment heads' within the model's attention mechanism and use differentiable masking to identify these heads. Turning off these heads reduces the effect of contextual entrainment.", "motivation": "To understand why and how language models become distracted by irrelevant contextual information, introducing the concept of contextual entrainment.", "method": "Observing the logits assigned by LMs to tokens that previously appeared in context prompts, using counterfactual vs factual prompts, and employing differentiable masking to discover 'entrainment heads'.", "result": "Statistically significant evidence shows that contextual entrainment is influenced by semantic factors, and turning off identified 'entrainment heads' significantly reduces the effect of contextual entrainment.", "conclusion": "The discovery of contextual entrainment and the investigation into LM distraction via entrainment heads marks a key step towards mechanistic analysis and mitigation of the distraction problem."}}
{"id": "2505.09114", "pdf": "https://arxiv.org/pdf/2505.09114", "abs": "https://arxiv.org/abs/2505.09114", "authors": ["Minh Hoang Nguyen", "Linh Le Pham Van", "Thommen George Karimpanal", "Sunil Gupta", "Hung Le"], "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Decision Transformers (DT) play a crucial role in modern reinforcement\nlearning, leveraging offline datasets to achieve impressive results across\nvarious domains. However, DT requires high-quality, comprehensive data to\nperform optimally. In real-world applications, the lack of training data and\nthe scarcity of optimal behaviours make training on offline datasets\nchallenging, as suboptimal data can hinder performance. To address this, we\npropose the Counterfactual Reasoning Decision Transformer (CRDT), a novel\nframework inspired by counterfactual reasoning. CRDT enhances DT ability to\nreason beyond known data by generating and utilizing counterfactual\nexperiences, enabling improved decision-making in unseen scenarios. Experiments\nacross Atari and D4RL benchmarks, including scenarios with limited data and\naltered dynamics, demonstrate that CRDT outperforms conventional DT approaches.\nAdditionally, reasoning counterfactually allows the DT agent to obtain\nstitching abilities, combining suboptimal trajectories, without architectural\nmodifications. These results highlight the potential of counterfactual\nreasoning to enhance reinforcement learning agents' performance and\ngeneralization capabilities.", "AI": {"tldr": "CRDT is a new framework that improves Decision Transformers by using counterfactual reasoning, leading to better decision-making in unseen scenarios and outperforming conventional DT approaches.", "motivation": "Decision Transformers need high-quality data for optimal performance but face challenges due to lack of training data and scarcity of optimal behaviours. The authors aim to enhance DT's ability to reason beyond known data.", "method": "Proposed the Counterfactual Reasoning Decision Transformer (CRDT), which generates and utilizes counterfactual experiences to improve decision-making in unseen scenarios.", "result": "Experiments on Atari and D4RL benchmarks show CRDT outperforms conventional DT approaches, especially in scenarios with limited data and altered dynamics. Also, it enables the agent to combine suboptimal trajectories without architectural modifications.", "conclusion": "Counterfactual reasoning can significantly enhance reinforcement learning agents' performance and generalization capabilities."}}
{"id": "2505.08915", "pdf": "https://arxiv.org/pdf/2505.08915", "abs": "https://arxiv.org/abs/2505.08915", "authors": ["Jialin Mao", "Itay Griniasty", "Yan Sun", "Mark K. Transtrum", "James P. Sethna", "Pratik Chaudhari"], "title": "An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": null, "summary": "Recent experiments have shown that training trajectories of multiple deep\nneural networks with different architectures, optimization algorithms,\nhyper-parameter settings, and regularization methods evolve on a remarkably\nlow-dimensional \"hyper-ribbon-like\" manifold in the space of probability\ndistributions. Inspired by the similarities in the training trajectories of\ndeep networks and linear networks, we analytically characterize this phenomenon\nfor the latter. We show, using tools in dynamical systems theory, that the\ngeometry of this low-dimensional manifold is controlled by (i) the decay rate\nof the eigenvalues of the input correlation matrix of the training data, (ii)\nthe relative scale of the ground-truth output to the weights at the beginning\nof training, and (iii) the number of steps of gradient descent. By analytically\ncomputing and bounding the contributions of these quantities, we characterize\nphase boundaries of the region where hyper-ribbons are to be expected. We also\nextend our analysis to kernel machines and linear models that are trained with\nstochastic gradient descent.", "AI": {"tldr": "Recent experiments indicate that training trajectories of various deep neural networks evolve on a low-dimensional 'hyper-ribbon-like' manifold. Inspired by this, the paper analytically characterizes this phenomenon for linear networks using dynamical systems theory.", "motivation": "To understand why training trajectories of different neural networks evolve on a low-dimensional manifold.", "method": "Using tools in dynamical systems theory, analyze how factors such as eigenvalue decay rate, scale of ground-truth output to initial weights, and number of gradient descent steps control the geometry of this manifold. Also extend analysis to kernel machines and linear models trained with stochastic gradient descent.", "result": "The geometry of the low-dimensional manifold is controlled by three key factors and phase boundaries of the region where hyper-ribbons are expected can be characterized by analytically computing and bounding these factors.", "conclusion": "This study provides analytical insights into why training trajectories evolve on low-dimensional manifolds for linear networks, kernel machines, and linear models trained with stochastic gradient descent."}}
{"id": "2505.08854", "pdf": "https://arxiv.org/pdf/2505.08854", "abs": "https://arxiv.org/abs/2505.08854", "authors": ["Yuping Wang", "Shuo Xing", "Cui Can", "Renjie Li", "Hongyuan Hua", "Kexin Tian", "Zhaobin Mo", "Xiangbo Gao", "Keshu Wu", "Sulong Zhou", "Hengxu You", "Juntong Peng", "Junge Zhang", "Zehao Wang", "Rui Song", "Mingxuan Yan", "Walter Zimmer", "Xingcheng Zhou", "Peiran Li", "Zhaohan Lu", "Chia-Ju Chen", "Yue Huang", "Ryan A. Rossi", "Lichao Sun", "Hongkai Yu", "Zhiwen Fan", "Frank Hao Yang", "Yuhao Kang", "Ross Greer", "Chenxi Liu", "Eun Hak Lee", "Xuan Di", "Xinyue Ye", "Liu Ren", "Alois Knoll", "Xiaopeng Li", "Shuiwang Ji", "Masayoshi Tomizuka", "Marco Pavone", "Tianbao Yang", "Jing Du", "Ming-Hsuan Yang", "Hua Wei", "Ziran Wang", "Yang Zhou", "Jiachen Li", "Zhengzhong Tu"], "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative\ntechnological wave that reconfigures industries through its unparalleled\ncapabilities for content creation, reasoning, planning, and multimodal\nunderstanding. This revolutionary force offers the most promising path yet\ntoward solving one of engineering's grandest challenges: achieving reliable,\nfully autonomous driving, particularly the pursuit of Level 5 autonomy. This\nsurvey delivers a comprehensive and critical synthesis of the emerging role of\nGenAI across the autonomous driving stack. We begin by distilling the\nprinciples and trade-offs of modern generative modeling, encompassing VAEs,\nGANs, Diffusion Models, and Large Language Models (LLMs). We then map their\nfrontier applications in image, LiDAR, trajectory, occupancy, video generation\nas well as LLM-guided reasoning and decision making. We categorize practical\napplications, such as synthetic data workflows, end-to-end driving strategies,\nhigh-fidelity digital twin systems, smart transportation networks, and\ncross-domain transfer to embodied AI. We identify key obstacles and\npossibilities such as comprehensive generalization across rare cases,\nevaluation and safety checks, budget-limited implementation, regulatory\ncompliance, ethical concerns, and environmental effects, while proposing\nresearch plans across theoretical assurances, trust metrics, transport\nintegration, and socio-technical influence. By unifying these threads, the\nsurvey provides a forward-looking reference for researchers, engineers, and\npolicymakers navigating the convergence of generative AI and advanced\nautonomous mobility. An actively maintained repository of cited works is\navailable at https://github.com/taco-group/GenAI4AD.", "AI": {"tldr": "Generative Artificial Intelligence (GenAI) is a transformative force that can potentially solve the challenge of achieving fully autonomous driving, especially Level 5 autonomy. This survey explores the role of GenAI across the autonomous driving stack, including its principles, applications, obstacles and possibilities.", "motivation": "To provide a comprehensive synthesis of the emerging role of Generative Artificial Intelligence (GenAI) in achieving reliable, fully autonomous driving, particularly focusing on Level 5 autonomy.", "method": "The paper reviews modern generative modeling techniques such as VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). It maps their applications in areas like image, LiDAR, trajectory, occupancy, video generation, LLM-guided reasoning and decision making. Practical applications include synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI.", "result": "Identifies key obstacles such as generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects. Proposes research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence.", "conclusion": "This survey serves as a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility."}}
{"id": "2505.09388", "pdf": "https://arxiv.org/pdf/2505.09388", "abs": "https://arxiv.org/abs/2505.09388", "authors": ["An Yang", "Anfeng Li", "Baosong Yang", "Beichen Zhang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Gao", "Chengen Huang", "Chenxu Lv", "Chujie Zheng", "Dayiheng Liu", "Fan Zhou", "Fei Huang", "Feng Hu", "Hao Ge", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Yang", "Jiaxi Yang", "Jing Zhou", "Jingren Zhou", "Junyang Lin", "Kai Dang", "Keqin Bao", "Kexin Yang", "Le Yu", "Lianghao Deng", "Mei Li", "Mingfeng Xue", "Mingze Li", "Pei Zhang", "Peng Wang", "Qin Zhu", "Rui Men", "Ruize Gao", "Shixuan Liu", "Shuang Luo", "Tianhao Li", "Tianyi Tang", "Wenbiao Yin", "Xingzhang Ren", "Xinyu Wang", "Xinyu Zhang", "Xuancheng Ren", "Yang Fan", "Yang Su", "Yichang Zhang", "Yinger Zhang", "Yu Wan", "Yuqiong Liu", "Zekun Wang", "Zeyu Cui", "Zhenru Zhang", "Zhipeng Zhou", "Zihan Qiu"], "title": "Qwen3 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.", "AI": {"tldr": "Qwen3 is the latest version in the Qwen model family, featuring advanced performance, efficiency, and multilingual capabilities. It integrates thinking and non-thinking modes for dynamic responses, introduces a thinking budget mechanism, and expands multilingual support from 29 to 119 languages. Qwen3 achieves state-of-the-art results across various benchmarks and is publicly accessible under Apache 2.0.", "motivation": "To advance the capabilities of large language models in terms of performance, efficiency, and multilingual support while reducing the need for switching between different models for various tasks.", "method": "Development of Qwen3 series with dense and MoE architectures, integration of thinking and non-thinking modes, introduction of a thinking budget mechanism, and expansion of multilingual support from 29 to 119 languages.", "result": "Qwen3 achieves state-of-the-art results in diverse benchmarks including code generation, mathematical reasoning, and agent tasks, competitive against larger MoE models and proprietary models.", "conclusion": "Qwen3 successfully advances the state of the art in large language models, enhancing global accessibility through improved cross-lingual understanding and generation capabilities, and is publicly accessible under Apache 2.0."}}
{"id": "2505.09289", "pdf": "https://arxiv.org/pdf/2505.09289", "abs": "https://arxiv.org/abs/2505.09289", "authors": ["Pedro M. P. Curvo", "Mara Dragomir", "Salvador Torpes", "Mohammadmahdi Rahimi"], "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"", "categories": ["cs.AI"], "comment": "11 Tables, 9 Figures", "summary": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems.", "AI": {"tldr": "This study replicates and extends Piatti et al.'s GovSim framework, confirming large language models' cooperative decision-making capabilities generalize across models, scenarios, and languages. Notably, high-performing models can influence lower-performing ones in heterogeneous multi-agent systems.", "motivation": "To validate the findings of Piatti et al. on the cooperative decision-making capabilities of large language models (LLMs) using the GovSim framework, and to explore its applicability to new settings, languages, and model architectures.", "method": "Replicate key experiments from Piatti et al., evaluate additional LLMs (e.g., DeepSeek-V3, GPT-4o-mini), introduce new settings (heterogeneous multi-agent environment, Japanese instructions, inverse environment), and assess the impact of the universalization principle.", "result": "Large models like GPT-4-turbo can achieve sustainable cooperation with or without the universalization principle, while smaller models fail without it. Cooperative behavior generalizes across different architectures and model sizes. High-performing models can influence lower-performing ones in heterogeneous environments.", "conclusion": "The GovSim benchmark is applicable to various models, scenarios, and languages, providing insights into LLM adaptability in complex cooperative tasks. Findings suggest potential improvements in computational resource efficiency and cooperative AI system development."}}
{"id": "2505.08940", "pdf": "https://arxiv.org/pdf/2505.08940", "abs": "https://arxiv.org/abs/2505.08940", "authors": ["Jeremie Blanchard", "Lisa Casino", "Jordan Gierschendorf"], "title": "NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach", "categories": ["cs.LG", "astro-ph.IM"], "comment": "12 pages", "summary": "The characterization of exoplanetary atmospheres through spectral analysis is\na complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration\nwith the European Space Agency's (ESA) Ariel mission, provided an opportunity\nto explore machine learning techniques for extracting atmospheric compositions\nfrom simulated spectral data. In this work, we focus on a data-centric business\napproach, prioritizing generalization over competition-specific optimization.\nWe briefly outline multiple experimental axes, including feature extraction,\nsignal transformation, and heteroskedastic uncertainty modeling. Our\nexperiments demonstrate that uncertainty estimation plays a crucial role in the\nGaussian Log-Likelihood (GLL) score, impacting performance by several\npercentage points. Despite improving the GLL score by 11%, our results\nhighlight the inherent limitations of tabular modeling and feature engineering\nfor this task, as well as the constraints of a business-driven approach within\na Kaggle-style competition framework. Our findings emphasize the trade-offs\nbetween model simplicity, interpretability, and generalization in astrophysical\ndata analysis.", "AI": {"tldr": "The paper explores machine learning methods for analyzing exoplanet atmospheres from spectral data, emphasizing uncertainty estimation's role in performance and highlighting the limitations of a business-driven approach.", "motivation": "To address the complex challenge of characterizing exoplanetary atmospheres through spectral analysis by exploring machine learning techniques provided by the NeurIPS 2024 Ariel Data Challenge.", "method": "Focused on a data-centric business approach with prioritization of generalization. Explored multiple experimental axes such as feature extraction, signal transformation, and heteroskedastic uncertainty modeling.", "result": "Uncertainty estimation significantly impacts the Gaussian Log-Likelihood (GLL) score, improving it by 11%. However, limitations were found in tabular modeling and feature engineering for this task.", "conclusion": "The study reveals trade-offs between model simplicity, interpretability, and generalization in astrophysical data analysis under a competition framework."}}
{"id": "2505.08882", "pdf": "https://arxiv.org/pdf/2505.08882", "abs": "https://arxiv.org/abs/2505.08882", "authors": ["Ali Almakhluk", "Uthman Baroudi", "Yasser El-Alfy"], "title": "Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety", "categories": ["cs.CV", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "This study aims to improve transportation safety, especially traffic safety.\nRoad damage anomalies such as potholes and cracks have emerged as a significant\nand recurring cause for accidents. To tackle this problem and improve road\nsafety, a comprehensive system has been developed to detect potholes, cracks\n(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit\nthis data to the cloud for appropriate action by authorities. The system also\nbroadcasts warning signals to nearby vehicles warning them if a severe anomaly\nis detected on the road. Moreover, the system can count road anomalies in\nreal-time. It is emulated through the utilization of Raspberry Pi, a camera\nmodule, deep learning model, laptop, and cloud service. Deploying this\ninnovative solution aims to proactively enhance road safety by notifying\nrelevant authorities and drivers about the presence of potholes and cracks to\ntake actions, thereby mitigating potential accidents arising from this\nprevalent road hazard leading to safer road conditions for the whole community.", "AI": {"tldr": "This study develops a system using Raspberry Pi, a camera module, deep learning model, laptop, and cloud service to detect road damages like potholes and cracks, classify their sizes, broadcast warning signals to nearby vehicles, and transmit data to the cloud for improving transportation safety.", "motivation": "Road damage anomalies such as potholes and cracks have emerged as a significant and recurring cause for accidents. The study aims to improve transportation safety by detecting these anomalies and notifying authorities and drivers.", "method": "The system uses Raspberry Pi, a camera module, a deep learning model, a laptop, and cloud services. It detects potholes and cracks, classifies their sizes, broadcasts warning signals to nearby vehicles, counts anomalies in real-time, and transmits data to the cloud.", "result": "The developed system can successfully detect road anomalies, classify their severity, warn nearby vehicles about severe anomalies, and send data to the cloud for further action by authorities.", "conclusion": "By deploying this innovative solution, the study aims to proactively enhance road safety and mitigate potential accidents arising from road hazards, leading to safer road conditions for the whole community."}}
{"id": "2505.09407", "pdf": "https://arxiv.org/pdf/2505.09407", "abs": "https://arxiv.org/abs/2505.09407", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits", "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": "12 pages, 12 figures", "summary": "Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations.", "AI": {"tldr": "QEDACVC is a quantum computing solution for multilingual machine translation, achieving 82% accuracy on the OPUS dataset.", "motivation": "Current cloud-based multilingual translation services use large multilingual language models with classical computing as the backend. There is potential to explore quantum computing for this task.", "method": "QEDACVC introduces a quantum encoder-decoder architecture that uses quantum convolution, quantum pooling, quantum variational circuit, and quantum attention mechanisms. It is designed to run on quantum computing hardware.", "result": "QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.", "conclusion": "QEDACVC demonstrates the potential of quantum computing in multilingual machine translation."}}
{"id": "2505.09341", "pdf": "https://arxiv.org/pdf/2505.09341", "abs": "https://arxiv.org/abs/2505.09341", "authors": ["Ev\u017een Wybitul"], "title": "Access Controls Will Solve the Dual-Use Dilemma", "categories": ["cs.AI"], "comment": null, "summary": "AI safety systems face a dual-use dilemma. Since the same request can be\neither harmless or harmful depending on who made it and why, if the system\nmakes decisions based solely on the request's content, it will refuse some\nlegitimate queries and let pass harmful ones. To address this, we propose a\nconceptual access control framework, based on verified user credentials (such\nas institutional affiliation) and classifiers that assign model outputs to risk\ncategories (such as advanced virology). The system permits responses only when\nthe user's verified credentials match the category's requirements. For\nimplementation of the model output classifiers, we introduce a theoretical\napproach utilizing small, gated expert modules integrated into the generator\nmodel, trained with gradient routing, that enable efficient risk detection\nwithout the capability gap problems of external monitors. While open questions\nremain about the verification mechanisms, risk categories, and the technical\nimplementation, our framework makes the first step toward enabling granular\ngovernance of AI capabilities: verified users gain access to specialized\nknowledge without arbitrary restrictions, while adversaries are blocked from\nit. This contextual approach reconciles model utility with robust safety,\naddressing the dual-use dilemma.", "AI": {"tldr": "The paper proposes a conceptual access control framework to solve the dual-use dilemma in AI safety systems by using verified user credentials and risk category classifiers.", "motivation": "AI safety systems encounter the dual-use dilemma where the same request can be harmless or harmful based on context, leading to potential refusal of legitimate queries and approval of harmful ones.", "method": "A conceptual access control framework is proposed. It uses verified user credentials and model output classifiers assigning outputs to risk categories. Responses are permitted only when user credentials match category requirements. For implementing classifiers, a theoretical approach with small, gated expert modules integrated into the generator model via gradient routing is introduced.", "result": "This framework enables granular governance of AI capabilities, allowing verified users access to specialized knowledge without arbitrary restrictions while blocking adversaries.", "conclusion": "The proposed contextual approach reconciles model utility with robust safety, addressing the dual-use dilemma in AI safety systems."}}
{"id": "2505.08941", "pdf": "https://arxiv.org/pdf/2505.08941", "abs": "https://arxiv.org/abs/2505.08941", "authors": ["Gavin Hull", "Alex Bihlo"], "title": "ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers", "categories": ["cs.LG", "cs.CL"], "comment": "16 pages, 13 figures", "summary": "Predicting the future citation rates of academic papers is an important step\ntoward the automation of research evaluation and the acceleration of scientific\nprogress. We present $\\textbf{ForeCite}$, a simple but powerful framework to\nappend pre-trained causal language models with a linear head for average\nmonthly citation rate prediction. Adapting transformers for regression tasks,\nForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of\n900K+ biomedical papers published between 2000 and 2024, a 27-point improvement\nover the previous state-of-the-art. Comprehensive scaling-law analysis reveals\nconsistent gains across model sizes and data volumes, while temporal holdout\nexperiments confirm practical robustness. Gradient-based saliency heatmaps\nsuggest a potentially undue reliance on titles and abstract texts. These\nresults establish a new state-of-the-art in forecasting the long-term influence\nof academic research and lay the groundwork for the automated, high-fidelity\nevaluation of scientific contributions.", "AI": {"tldr": "The paper introduces ForeCite, a framework using pre-trained causal language models with a linear head to predict average monthly citation rates of biomedical papers, achieving high test correlation and establishing a new state-of-the-art.", "motivation": "To automate research evaluation and accelerate scientific progress by predicting future citation rates of academic papers.", "method": "ForeCite adapts transformers for regression tasks by appending pre-trained causal language models with a linear head. It uses gradient-based saliency heatmaps.", "result": "Achieves a test correlation of \u03c1 = 0.826 on a dataset of 900K+ biomedical papers, a 27-point improvement over previous methods.", "conclusion": "ForeCite establishes a new state-of-the-art in forecasting long-term influence of academic research, providing groundwork for automated evaluation of scientific contributions."}}
{"id": "2505.08886", "pdf": "https://arxiv.org/pdf/2505.08886", "abs": "https://arxiv.org/abs/2505.08886", "authors": ["Hamideh Khaleghpour", "Brett McKinney"], "title": "Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images", "categories": ["cs.CV", "cs.LG"], "comment": "7 pages, 10 figures. Accepted at the 2nd Asia Pacific Computer\n  Systems Conference (APCS 2024), March 15-17, 2024", "summary": "The rising incidence of skin cancer, coupled with limited public awareness\nand a shortfall in clinical expertise, underscores an urgent need for advanced\ndiagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool\nin this domain, particularly for distinguishing malignant from benign skin\nlesions. Leveraging publicly available datasets of skin lesions, researchers\nhave been developing AI-based diagnostic solutions. However, the integration of\nsuch computer systems in clinical settings is still nascent. This study aims to\nbridge this gap by employing a fusion of image processing techniques and\nmachine learning algorithms, specifically neuro-fuzzy and colonial competition\napproaches. Applied to dermoscopic images from the ISIC database, our method\nachieved a notable accuracy of 94% on a dataset of 560 images. These results\nunderscore the potential of our approach in aiding clinicians in the early\ndetection of melanoma, thereby contributing significantly to skin cancer\ndiagnostics.", "AI": {"tldr": "This study combines image processing and machine learning (neuro-fuzzy and colonial competition algorithms) to improve skin cancer diagnostics using dermoscopic images, achieving 94% accuracy on a dataset of 560 images from the ISIC database.", "motivation": "The increasing incidence of skin cancer, lack of public awareness, and shortage of clinical expertise necessitate advanced diagnostic aids. AI shows promise in distinguishing between malignant and benign skin lesions.", "method": "The study utilized a fusion of image processing techniques with machine learning algorithms, specifically neuro-fuzzy and colonial competition approaches, applied to dermoscopic images from the ISIC database.", "result": "The method achieved an accuracy of 94% when tested on a dataset of 560 images.", "conclusion": "This approach holds significant potential in assisting clinicians with the early detection of melanoma, contributing to advancements in skin cancer diagnostics."}}
{"id": "2505.09519", "pdf": "https://arxiv.org/pdf/2505.09519", "abs": "https://arxiv.org/abs/2505.09519", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting\nlarge language models, yet existing approaches exhibit counter-intuitive\nphenomena: integrating router into prompt tuning (PT) increases training\nefficiency yet does not improve performance universally; parameter reduction\nthrough matrix decomposition can improve performance in specific domains.\nMotivated by these observations and the modular nature of PT, we propose\nPT-MoE, a novel framework that integrates matrix decomposition with\nmixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets\ndemonstrate that PT-MoE achieves state-of-the-art performance in both question\nanswering (QA) and mathematical problem solving tasks, improving F1 score by\n1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing\nmathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all\nwhile using 25% fewer parameters than LoRA. Our analysis reveals that while PT\nmethods generally excel in QA tasks and LoRA-based methods in math datasets,\nthe integration of matrix decomposition and MoE in PT-MoE yields complementary\nbenefits: decomposition enables efficient parameter sharing across experts\nwhile MoE provides dynamic adaptation, collectively enabling PT-MoE to\ndemonstrate cross-task consistency and generalization abilities. These\nfindings, along with ablation studies on routing mechanisms and architectural\ncomponents, provide insights for future PEFT methods.", "AI": {"tldr": "Parameter-efficient fine-tuning (PEFT) methods for adapting large language models have shown promise, but existing approaches exhibit counter-intuitive phenomena. This paper proposes PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning. It achieves state-of-the-art performance in both question answering and mathematical problem solving tasks while using 25% fewer parameters than LoRA.", "motivation": "Existing PEFT methods show counter-intuitive phenomena: integrating router into prompt tuning increases training efficiency but does not universally improve performance; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of prompt tuning, the authors propose PT-MoE.", "method": "PT-MoE integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning. Matrix decomposition enables efficient parameter sharing across experts, while MoE provides dynamic adaptation.", "result": "PT-MoE achieves state-of-the-art performance in both question answering and mathematical problem solving tasks. It improves F1 score by 1.49 points over prompt tuning and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over prompt tuning and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA.", "conclusion": "The integration of matrix decomposition and MoE in PT-MoE yields complementary benefits, enabling cross-task consistency and generalization abilities. These findings provide insights for future PEFT methods."}}
{"id": "2505.09396", "pdf": "https://arxiv.org/pdf/2505.09396", "abs": "https://arxiv.org/abs/2505.09396", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u5728\u535a\u5f08\u8bba\u573a\u666f\u4e2d\u7684\u6218\u7565\u6027\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u7684\u4ee3\u7406\u8bbe\u8ba1\u8bc4\u4f30\u4e86\u4ee3\u7406\u590d\u6742\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0\u4eba\u7c7b\u542f\u53d1\u7684\u8ba4\u77e5\u7ed3\u6784\u80fd\u63d0\u5347LLM\u4ee3\u7406\u4e0e\u4eba\u7c7b\u6218\u7565\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u8fd9\u79cd\u5173\u7cfb\u662f\u975e\u7ebf\u6027\u7684\uff0c\u53d7\u9650\u4e8e\u5e95\u5c42LLM\u80fd\u529b\u548c\u7b80\u5355\u67b6\u6784\u589e\u5f3a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fc5\u901f\u53d1\u5c55\uff0cAI\u7814\u7a76\u8f6c\u5411\u66f4\u5177\u4ee3\u7406\u7279\u6027\u7684\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u8ba8\u8fd9\u4e9b\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u591f\u590d\u5236\u4eba\u7c7b\u7684\u6218\u7565\u6027\u63a8\u7406\uff0c\u7279\u522b\u662f\u5728\u535a\u5f08\u8bba\u80cc\u666f\u4e0b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e09\u79cd\u4ee3\u7406\u8bbe\u8ba1\uff1a\u7b80\u5355\u7684\u535a\u5f08\u8bba\u6a21\u578b\u3001\u65e0\u7ed3\u6784\u7684LLM\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\u4ee5\u53ca\u6574\u5408\u5230\u4f20\u7edf\u4ee3\u7406\u6846\u67b6\u4e2d\u7684LLM\u3002\u901a\u8fc7\u731c\u6570\u5b57\u6e38\u620f\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5bf9\u8fd9\u4e9b\u4ee3\u7406\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e86\u603b\u4f53\u63a8\u7406\u6a21\u5f0f\u548c\u57fa\u4e8e\u4e2a\u4f53\u89d2\u8272\u7684\u76ee\u6807\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u6df7\u6dc6\u7684\u6e38\u620f\u573a\u666f\u4ee5\u8bc4\u4f30\u4ee3\u7406\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5bf9\u8d85\u8fc72000\u4e2a\u63a8\u7406\u6837\u672c\u548c25\u79cd\u4ee3\u7406\u914d\u7f6e\u7684\u5206\u6790\uff0c\u53d1\u73b0\u4eba\u7c7b\u542f\u53d1\u7684\u8ba4\u77e5\u7ed3\u6784\u53ef\u4ee5\u589e\u5f3aLLM\u4ee3\u7406\u4e0e\u4eba\u7c7b\u6218\u7565\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002\u7136\u800c\uff0c\u4ee3\u7406\u8bbe\u8ba1\u590d\u6742\u5ea6\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u975e\u7ebf\u6027\u7684\uff0c\u8fd9\u53d6\u51b3\u4e8e\u57fa\u7840LLM\u7684\u80fd\u529b\uff0c\u5e76\u8868\u660e\u7b80\u5355\u67b6\u6784\u589e\u5f3a\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u4eba\u7c7b\u542f\u53d1\u7684\u8ba4\u77e5\u7ed3\u6784\u53ef\u4ee5\u6539\u5584LLM\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u4f46\u5176\u6548\u679c\u53d7\u5230\u57fa\u7840LLM\u80fd\u529b\u548c\u67b6\u6784\u589e\u5f3a\u65b9\u5f0f\u7684\u9650\u5236\uff0c\u56e0\u6b64\u63d0\u9ad8LLM\u4ee3\u7406\u7684\u4eba\u7c7b\u76f8\u4f3c\u6027\u9700\u8981\u66f4\u6df1\u5165\u7684\u7814\u7a76\u548c\u66f4\u590d\u6742\u7684\u67b6\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2505.08964", "pdf": "https://arxiv.org/pdf/2505.08964", "abs": "https://arxiv.org/abs/2505.08964", "authors": ["Majed Jaber", "Julien Michel", "Nicolas Boutry", "Pierre Parrend"], "title": "GPML: Graph Processing for Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The dramatic increase of complex, multi-step, and rapidly evolving attacks in\ndynamic networks involves advanced cyber-threat detectors. The GPML (Graph\nProcessing for Machine Learning) library addresses this need by transforming\nraw network traffic traces into graph representations, enabling advanced\ninsights into network behaviors. The library provides tools to detect anomalies\nin interaction and community shifts in dynamic networks. GPML supports\ncommunity and spectral metrics extraction, enhancing both real-time detection\nand historical forensics analysis. This library supports modern cybersecurity\nchallenges with a robust, graph-based approach.", "AI": {"tldr": "GPML\u5e93\u901a\u8fc7\u5c06\u539f\u59cb\u7f51\u7edc\u6d41\u91cf\u8ddf\u8e2a\u8f6c\u6362\u4e3a\u56fe\u5f62\u8868\u793a\uff0c\u652f\u6301\u793e\u533a\u548c\u5149\u8c31\u6307\u6807\u63d0\u53d6\uff0c\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\u548c\u5386\u53f2\u53d6\u8bc1\u5206\u6790\uff0c\u5e94\u5bf9\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u590d\u6742\u653b\u51fb\u3002", "motivation": "\u5728\u7f51\u7edc\u4e2d\u590d\u6742\u3001\u591a\u6b65\u9aa4\u548c\u5feb\u901f\u6f14\u53d8\u7684\u653b\u51fb\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5148\u8fdb\u7684\u7f51\u7edc\u5a01\u80c1\u68c0\u6d4b\u5668\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "GPML\u5e93\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\uff0c\u5c06\u539f\u59cb\u7f51\u7edc\u6d41\u91cf\u8ddf\u8e2a\u8f6c\u6362\u4e3a\u56fe\u8868\u793a\uff0c\u63d0\u4f9b\u5de5\u5177\u4ee5\u68c0\u6d4b\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u4ea4\u4e92\u5f02\u5e38\u548c\u793e\u533a\u53d8\u5316\uff0c\u5e76\u652f\u6301\u793e\u533a\u548c\u5149\u8c31\u5ea6\u91cf\u7684\u63d0\u53d6\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3a\u5b9e\u65f6\u68c0\u6d4b\u548c\u5386\u53f2\u53d6\u8bc1\u5206\u6790\uff0c\u652f\u6301\u73b0\u4ee3\u7f51\u7edc\u5b89\u5168\u6311\u6218\u3002", "conclusion": "GPML\u5e93\u901a\u8fc7\u5176\u5f3a\u5927\u7684\u3001\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\uff0c\u4e3a\u73b0\u4ee3\u7f51\u7edc\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08909", "pdf": "https://arxiv.org/pdf/2505.08909", "abs": "https://arxiv.org/abs/2505.08909", "authors": ["Deliang Wei", "Peng Chen", "Haobo Xu", "Jiale Yao", "Fang Li", "Tieyong Zeng"], "title": "Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems", "categories": ["cs.CV", "cs.LG", "math.FA", "math.OC", "94A08, 47H10, 47J26, 46N10, 47N10"], "comment": "31 pages", "summary": "Plug-and-play (PnP) methods with deep denoisers have shown impressive results\nin imaging problems. They typically require strong convexity or smoothness of\nthe fidelity term and a (residual) non-expansive denoiser for convergence.\nThese assumptions, however, are violated in Poisson inverse problems, and\nnon-expansiveness can hinder denoising performance. To address these\nchallenges, we propose a cocoercive conservative (CoCo) denoiser, which may be\n(residual) expansive, leading to improved denoising. By leveraging the\ngeneralized Helmholtz decomposition, we introduce a novel training strategy\nthat combines Hamiltonian regularization to promote conservativeness and\nspectral regularization to ensure cocoerciveness. We prove that CoCo denoiser\nis a proximal operator of a weakly convex function, enabling a restoration\nmodel with an implicit weakly convex prior. The global convergence of PnP\nmethods to a stationary point of this restoration model is established.\nExtensive experimental results demonstrate that our approach outperforms\nclosely related methods in both visual quality and quantitative metrics.", "AI": {"tldr": "The paper proposes a cocoercive conservative (CoCo) denoiser for Poisson inverse problems in imaging, which can be expansive and improves denoising performance. It introduces a new training strategy using Hamiltonian and spectral regularization, proves the CoCo denoiser's properties, establishes global convergence of PnP methods with this denoiser, and shows superior performance in experiments.", "motivation": "Existing Plug-and-play (PnP) methods with deep denoisers require strong convexity or smoothness of the fidelity term and a non-expansive denoiser for convergence, but these assumptions are violated in Poisson inverse problems and can hinder denoising performance.", "method": "The authors propose a cocoercive conservative (CoCo) denoiser that may be expansive, leading to improved denoising performance. They leverage the generalized Helmholtz decomposition to introduce a novel training strategy combining Hamiltonian regularization for conservativeness and spectral regularization for cocoerciveness. The CoCo denoiser is proven to be a proximal operator of a weakly convex function, enabling a restoration model with an implicit weakly convex prior.", "result": "The proposed approach outperforms closely related methods in both visual quality and quantitative metrics through extensive experimental results.", "conclusion": "The introduction of the cocoercive conservative (CoCo) denoiser addresses challenges in Poisson inverse problems by allowing expansiveness for better denoising, providing theoretical guarantees, and demonstrating superior performance in imaging tasks."}}
{"id": "2505.09595", "pdf": "https://arxiv.org/pdf/2505.09595", "abs": "https://arxiv.org/abs/2505.09595", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025", "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.", "AI": {"tldr": "WorldView-Bench is a new benchmark designed to evaluate Global Cultural Inclusivity in LLMs. It uses free-form generative evaluation and two intervention strategies to significantly increase cultural inclusivity.", "motivation": "Existing benchmarking frameworks fail to adequately capture the bias in LLMs towards Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality.", "method": "The method involves introducing WorldView-Bench, a benchmark for evaluating Global Cultural Inclusivity in LLMs, using free-form generative evaluation instead of rigid, closed-form assessments. Two intervention strategies are implemented: Contextually-Implemented Multiplex LLMs and Multi-Agent System (MAS)-Implemented Multiplex LLMs.", "result": "Results show a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance.", "conclusion": "These findings indicate the potential of multiplex-aware AI evaluation in reducing cultural bias in LLMs, promoting more inclusive and ethically aligned AI systems."}}
{"id": "2505.09412", "pdf": "https://arxiv.org/pdf/2505.09412", "abs": "https://arxiv.org/abs/2505.09412", "authors": ["Paul Kobialka", "Lina Gerlach", "Francesco Leofante", "Erika \u00c1brah\u00e1m", "Silvia Lizeth Tapia Tarifa", "Einar Broch Johnsen"], "title": "Counterfactual Strategies for Markov Decision Processes", "categories": ["cs.AI", "I.2.m"], "comment": null, "summary": "Counterfactuals are widely used in AI to explain how minimal changes to a\nmodel's input can lead to a different output. However, established methods for\ncomputing counterfactuals typically focus on one-step decision-making, and are\nnot directly applicable to sequential decision-making tasks. This paper fills\nthis gap by introducing counterfactual strategies for Markov Decision Processes\n(MDPs). During MDP execution, a strategy decides which of the enabled actions\n(with known probabilistic effects) to execute next. Given an initial strategy\nthat reaches an undesired outcome with a probability above some limit, we\nidentify minimal changes to the initial strategy to reduce that probability\nbelow the limit. We encode such counterfactual strategies as solutions to\nnon-linear optimization problems, and further extend our encoding to synthesize\ndiverse counterfactual strategies. We evaluate our approach on four real-world\ndatasets and demonstrate its practical viability in sophisticated sequential\ndecision-making tasks.", "AI": {"tldr": "This paper introduces counterfactual strategies for Markov Decision Processes (MDPs) to address sequential decision-making tasks, encoding them as solutions to non-linear optimization problems and demonstrating practical viability.", "motivation": "Counterfactuals are useful in AI for explaining changes leading to different outputs, but established methods focus on one-step decision-making and aren't directly applicable to sequential decision-making tasks.", "method": "The paper encodes counterfactual strategies as solutions to non-linear optimization problems for MDPs, identifying minimal changes to an initial strategy that leads to an undesired outcome. It further extends the encoding to synthesize diverse counterfactual strategies.", "result": "The approach is evaluated on four real-world datasets, showing its practical viability in sophisticated sequential decision-making tasks.", "conclusion": "The introduction of counterfactual strategies for MDPs fills a gap in addressing sequential decision-making tasks."}}
{"id": "2505.08977", "pdf": "https://arxiv.org/pdf/2505.08977", "abs": "https://arxiv.org/abs/2505.08977", "authors": ["Hossein Babaei", "Mel White", "Sina Alemohammad", "Richard G. Baraniuk"], "title": "SaFARi: State-Space Models for Frame-Agnostic Representation", "categories": ["cs.LG", "eess.AS", "eess.IV", "eess.SP"], "comment": "13 pages, 5 figures", "summary": "State-Space Models (SSMs) have re-emerged as a powerful tool for online\nfunction approximation, and as the backbone of machine learning models for\nlong-range dependent data. However, to date, only a few polynomial bases have\nbeen explored for this purpose, and the state-of-the-art implementations were\nbuilt upon the best of a few limited options. In this paper, we present a\ngeneralized method for building an SSM with any frame or basis, rather than\nbeing restricted to polynomials. This framework encompasses the approach known\nas HiPPO, but also permits an infinite diversity of other possible \"species\"\nwithin the SSM architecture. We dub this approach SaFARi: SSMs for\nFrame-Agnostic Representation.", "AI": {"tldr": "State-Space Models (SSMs) have been limited to a few polynomial bases, but this paper introduces SaFARi, a generalized method allowing any frame or basis in SSMs, providing infinite diversity in SSM architecture.", "motivation": "The motivation is to overcome the limitation of using only a few polynomial bases in State-Space Models for online function approximation and machine learning models dealing with long-range dependent data.", "method": "The method involves creating a generalized framework called SaFARi which allows building an SSM with any frame or basis, not restricted to polynomials. This includes the HiPPO approach as a subset and opens up many new possibilities within the SSM architecture.", "result": "This approach potentially leads to more versatile and effective SSMs that can be applied to a wider range of problems.", "conclusion": "SaFARi provides a flexible and powerful tool for enhancing the capabilities of State-Space Models by enabling the use of diverse frames and bases."}}
{"id": "2505.08910", "pdf": "https://arxiv.org/pdf/2505.08910", "abs": "https://arxiv.org/abs/2505.08910", "authors": ["Nahid Alam", "Karthik Reddy Kanjula", "Surya Guthikonda", "Timothy Chung", "Bala Krishna S Vegesna", "Abhipsha Das", "Anthony Susevski", "Ryan Sze-Yin Chan", "S M Iftekhar Uddin", "Shayekh Bin Islam", "Roshan Santhosh", "Snegha A", "Drishti Sharma", "Chen Liu", "Isha Chaturvedi", "Genta Indra Winata", "Ashvanth. S", "Snehanshu Mukherjee", "Alham Fikri Aji"], "title": "Behind Maya: Building a Multilingual Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at VLM4ALL CVPR 2025 Workshop", "summary": "In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.", "AI": {"tldr": "The paper presents Maya, an open-source Multilingual Vision-Language Model (VLM) to improve performance in low-resource languages and varied cultural contexts. It includes a pretraining dataset in eight languages based on LLaVA and a model enhancing cultural and linguistic comprehension.", "motivation": "There is a lack of performance in existing large Vision-Language Models (VLMs) for low-resource languages and varied cultural contexts despite their impressive results on academic benchmarks mainly in widely spoken languages.", "method": "The authors introduce Maya, which consists of a multilingual image-text pretraining dataset in eight languages derived from the LLaVA pretraining dataset and a multilingual image-text model supporting these languages to enhance cultural and linguistic understanding.", "result": "Maya addresses the limitations of current VLMs by providing support for low-resource languages and improving cultural and linguistic comprehension in vision-language tasks.", "conclusion": "The development of Maya contributes towards more inclusive Vision-Language Models capable of handling multiple languages and cultural contexts, with the code being available for further research and development."}}
{"id": "2505.09518", "pdf": "https://arxiv.org/pdf/2505.09518", "abs": "https://arxiv.org/abs/2505.09518", "authors": ["Maris F. L. Galesloot", "Roman Andriushchenko", "Milan \u010ce\u0161ka", "Sebastian Junges", "Nils Jansen"], "title": "\\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Partially observable Markov decision processes (POMDPs) model specific\nenvironments in sequential decision-making under uncertainty. Critically,\noptimal policies for POMDPs may not be robust against perturbations in the\nenvironment. Hidden-model POMDPs (HM-POMDPs) capture sets of different\nenvironment models, that is, POMDPs with a shared action and observation space.\nThe intuition is that the true model is hidden among a set of potential models,\nand it is unknown which model will be the environment at execution time. A\npolicy is robust for a given HM-POMDP if it achieves sufficient performance for\neach of its POMDPs. We compute such robust policies by combining two orthogonal\ntechniques: (1) a deductive formal verification technique that supports\ntractable robust policy evaluation by computing a worst-case POMDP within the\nHM-POMDP and (2) subgradient ascent to optimize the candidate policy for a\nworst-case POMDP. The empirical evaluation shows that, compared to various\nbaselines, our approach (1) produces policies that are more robust and\ngeneralize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of\nover a hundred thousand environments.", "AI": {"tldr": "The paper introduces a method to compute robust policies for HM-POMDPs, combining formal verification and subgradient ascent, showing better generalization and scalability.", "motivation": "To address the lack of robustness in optimal policies for POMDPs against environmental perturbations, the paper explores HM-POMDPs which encompass multiple potential environment models.", "method": "The approach involves two techniques: (1) deductive formal verification to evaluate robust policy by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for the worst-case POMDP.", "result": "Empirical results demonstrate that the computed policies are more robust and generalize better to unseen POMDPs compared to various baselines. Additionally, the method scales well to HM-POMDPs with over a hundred thousand environments.", "conclusion": "The proposed method effectively computes robust policies for HM-POMDPs, providing enhanced generalization capabilities and scalability."}}
{"id": "2505.08982", "pdf": "https://arxiv.org/pdf/2505.08982", "abs": "https://arxiv.org/abs/2505.08982", "authors": ["Jiachen Qian", "Yang Zheng"], "title": "Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": null, "summary": "We consider the problem of online prediction for an unknown, non-explosive\nlinear stochastic system. With a known system model, the optimal predictor is\nthe celebrated Kalman filter. In the case of unknown systems, existing\napproaches based on recursive least squares and its variants may suffer from\ndegraded performance due to the highly imbalanced nature of the regression\nmodel. This imbalance can easily lead to overfitting and thus degrade\nprediction accuracy. We tackle this problem by injecting an inductive bias into\nthe regression model via {exponential forgetting}. While exponential forgetting\nis a common wisdom in online learning, it is typically used for re-weighting\ndata. In contrast, our approach focuses on balancing the regression model. This\nachieves a better trade-off between {regression} and {regularization errors},\nand simultaneously reduces the {accumulation error}. With new proof techniques,\nwe also provide a sharper logarithmic regret bound of $O(\\log^3 N)$, where $N$\nis the number of observations.", "AI": {"tldr": "The paper proposes an online prediction method for unknown, non-explosive linear stochastic systems by incorporating exponential forgetting to balance the regression model and reduce accumulation error. A sharper logarithmic regret bound of O(log^3 N) is provided.", "motivation": "Existing approaches for predicting unknown linear stochastic systems may suffer from degraded performance due to imbalanced regression models leading to overfitting and reduced prediction accuracy.", "method": "Inject an inductive bias into the regression model via exponential forgetting to better balance between regression and regularization errors, reducing accumulation error as well.", "result": "Achieves a better trade-off between regression and regularization errors and provides a sharper logarithmic regret bound of O(log^3 N).", "conclusion": "The proposed method effectively tackles the problem of imbalanced regression models in online prediction for unknown linear stochastic systems."}}
{"id": "2505.08961", "pdf": "https://arxiv.org/pdf/2505.08961", "abs": "https://arxiv.org/abs/2505.08961", "authors": ["Yancheng Wang", "Nebojsa Jojic", "Yingzhen Yang"], "title": "Differentiable Channel Selection in Self-Attention For Person Re-Identification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we propose a novel attention module termed the Differentiable\nChannel Selection Attention module, or the DCS-Attention module. In contrast\nwith conventional self-attention, the DCS-Attention module features selection\nof informative channels in the computation of the attention weights. The\nselection of the feature channels is performed in a differentiable manner,\nenabling seamless integration with DNN training. Our DCS-Attention is\ncompatible with either fixed neural network backbones or learnable backbones\nwith Differentiable Neural Architecture Search (DNAS), leading to DCS with\nFixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our\nDCS-Attention is motivated by the principle of Information Bottleneck (IB), and\na novel variational upper bound for the IB loss, which can be optimized by SGD,\nis derived and incorporated into the training loss of the networks with the\nDCS-Attention modules. In this manner, a neural network with DCS-Attention\nmodules is capable of selecting the most informative channels for feature\nextraction so that it enjoys state-of-the-art performance for the Re-ID task.\nExtensive experiments on multiple person Re-ID benchmarks using both DCS-FB and\nDCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy\nof DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention\nin learning discriminative features critical to identifying person identities.\nThe code of our work is available at\nhttps://github.com/Statistical-Deep-Learning/DCS-Attention.", "AI": {"tldr": "The paper introduces DCS-Attention, a novel attention module that selects informative channels for feature extraction in DNNs, improving performance on person Re-ID tasks.", "motivation": "The motivation of this paper is to enhance the feature extraction capabilities of deep neural networks by developing an attention mechanism that focuses on selecting the most informative channels. This is motivated by the principle of Information Bottleneck (IB) and aims to improve the accuracy of person re-identification (Re-ID) tasks.", "method": "The proposed method involves creating the Differentiable Channel Selection Attention (DCS-Attention) module which performs channel selection in a differentiable manner. It can be integrated with either fixed neural network backbones (DCS-FB) or learnable backbones using Differentiable Neural Architecture Search (DCS-DNAS). The authors derive a novel variational upper bound for the IB loss that can be optimized by SGD and incorporated into the training loss.", "result": "Experiments on multiple person Re-ID benchmarks show that DCS-Attention significantly improves the prediction accuracy of DNNs for person Re-ID, demonstrating its effectiveness in learning discriminative features critical to identifying person identities.", "conclusion": "The DCS-Attention module successfully enhances the performance of DNNs in person Re-ID tasks by selecting the most informative channels for feature extraction. This approach represents a state-of-the-art advancement in the field."}}
{"id": "2505.09614", "pdf": "https://arxiv.org/pdf/2505.09614", "abs": "https://arxiv.org/abs/2505.09614", "authors": ["Anthony GX-Chen", "Dongyan Lin", "Mandana Samiei", "Doina Precup", "Blake A. Richards", "Rob Fergus", "Kenneth Marino"], "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.", "AI": {"tldr": "Language model agents' ability to explore and infer causal relationships was examined using the Blicket Test paradigm. They reliably infer disjunctive causal relationships but struggle with conjunctive ones, a bias also seen in human adults. A test-time sampling method was proposed to reduce this bias.", "motivation": "To determine whether language model agents possess the capability to efficiently explore and understand the causal structure of the world, which is crucial for robust reasoning. Also, to investigate if these models exhibit systematic biases leading to erroneous conclusions.", "method": "Using the Blicket Test paradigm from developmental psychology to examine the LMs' ability to infer causal relationships. Quantifying similarities between LMs and humans in terms of inference profiles. Proposing a test-time sampling method to eliminate hypotheses about causal relationships.", "result": "LMs reliably infer disjunctive causal relationships but struggle with conjunctive ones. This bias persists across different models and strategies, and performance decreases with task complexity. LMs exhibit adult-like inference profiles, not children-like. The proposed sampling method significantly reduces the disjunctive bias.", "conclusion": "Language models show a disjunctive bias in causal reasoning similar to human adults, likely inherited from training data. The proposed test-time sampling method can help move LMs towards more causally rigorous reasoning."}}
{"id": "2505.09003", "pdf": "https://arxiv.org/pdf/2505.09003", "abs": "https://arxiv.org/abs/2505.09003", "authors": ["Zeki Doruk Erden", "Donia Gasmi", "Boi Faltings"], "title": "Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition", "categories": ["cs.LG", "cs.AI"], "comment": "Published in the Autonomous Robots and Multirobot Systems (ARMS)\n  workshop at AAMAS 2025", "summary": "Continual learning for reinforcement learning agents remains a significant\nchallenge, particularly in preserving and leveraging existing information\nwithout an external signal to indicate changes in tasks or environments. In\nthis study, we explore the effectiveness of autoencoders in detecting new tasks\nand matching observed environments to previously encountered ones. Our approach\nintegrates policy optimization with familiarity autoencoders within an\nend-to-end continual learning system. This system can recognize and learn new\ntasks or environments while preserving knowledge from earlier experiences and\ncan selectively retrieve relevant knowledge when re-encountering a known\nenvironment. Initial results demonstrate successful continual learning without\nexternal signals to indicate task changes or reencounters, showing promise for\nthis methodology.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b56\u7565\u4f18\u5316\u4e0e\u719f\u6089\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u7aef\u5230\u7aef\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u65e0\u9700\u5916\u90e8\u4fe1\u53f7\u5373\u53ef\u6210\u529f\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u80fd\u591f\u8bc6\u522b\u65b0\u4efb\u52a1\u3001\u5339\u914d\u5df2\u77e5\u73af\u5883\uff0c\u5e76\u5728\u91cd\u65b0\u9047\u5230\u5df2\u77e5\u73af\u5883\u65f6\u9009\u62e9\u6027\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6ca1\u6709\u5916\u90e8\u4fe1\u53f7\u6307\u793a\u4efb\u52a1\u6216\u73af\u5883\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u4fdd\u5b58\u548c\u5229\u7528\u73b0\u6709\u4fe1\u606f\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u7b56\u7565\u4f18\u5316\u4e0e\u719f\u6089\u5ea6\u81ea\u7f16\u7801\u5668\u96c6\u6210\u5230\u4e00\u4e2a\u7aef\u5230\u7aef\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc6\u522b\u548c\u5b66\u4e60\u65b0\u4efb\u52a1\u6216\u73af\u5883\uff0c\u540c\u65f6\u4fdd\u7559\u5148\u524d\u7684\u7ecf\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5728\u91cd\u65b0\u9047\u5230\u5df2\u77e5\u73af\u5883\u65f6\u9009\u62e9\u6027\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u6ca1\u6709\u5916\u90e8\u4fe1\u53f7\u6307\u793a\u4efb\u52a1\u53d8\u5316\u6216\u91cd\u65b0\u9047\u5230\u4efb\u52a1\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408\u7b56\u7565\u4f18\u5316\u4e0e\u719f\u6089\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u4e3a\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.08971", "pdf": "https://arxiv.org/pdf/2505.08971", "abs": "https://arxiv.org/abs/2505.08971", "authors": ["Yangyi Chen", "Hao Peng", "Tong Zhang", "Heng Ji"], "title": "Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The code will be available at https://github.com/Yangyi-Chen/PRIOR", "summary": "In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data.", "AI": {"tldr": "The paper introduces PRIOR, a vision-language pre-training method that prioritizes image-related tokens through differential weighting in the NTP loss using a reference text-only LLM. This approach improves performance and scaling properties compared to standard NTP.", "motivation": "Standard large vision-language models (LVLMs) pre-training uses next-token prediction (NTP), but since only a small subset of caption tokens directly relate to the visual content, this can unintentionally fit the model to noise and increase hallucination risk.", "method": "PRIOR introduces a reference model - a text-only large language model (LLM) trained on captions without image inputs - to weight each token based on its probability for LVLMs training. Tokens directly related to visual inputs receive lower probabilities from the text-only reference LLM and thus are re-weighted during training.", "result": "PRIOR shows 19% and 8% average relative improvement in two distinct settings of LVLMs compared to NTP. It also exhibits superior scaling properties with significantly higher scaling coefficients.", "conclusion": "PRIOR is a simple yet effective approach for vision-language pre-training that addresses the issue of noise fitting in standard NTP by prioritizing image-related tokens."}}
{"id": "2505.08842", "pdf": "https://arxiv.org/pdf/2505.08842", "abs": "https://arxiv.org/abs/2505.08842", "authors": ["Zekun Wu", "Seonglae Cho", "Umar Mohammed", "Cristian Munoz", "Kleyton Costa", "Xin Guan", "Theo King", "Ze Wang", "Emre Kazim", "Adriano Koshiyama"], "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Open-source AI libraries are foundational to modern AI systems but pose\nsignificant, underexamined risks across security, licensing, maintenance,\nsupply chain integrity, and regulatory compliance. We present LibVulnWatch, a\ngraph-based agentic assessment framework that performs deep, source-grounded\nevaluations of these libraries. Built on LangGraph, the system coordinates a\ndirected acyclic graph of specialized agents to extract, verify, and quantify\nrisk using evidence from trusted sources such as repositories, documentation,\nand vulnerability databases. LibVulnWatch generates reproducible,\ngovernance-aligned scores across five critical domains, publishing them to a\npublic leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely\nused libraries, including ML frameworks, LLM inference engines, and agent\norchestration tools, our system covers up to 88% of OpenSSF Scorecard checks\nwhile uncovering up to 19 additional risks per library. These include critical\nRemote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials\n(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in\nregulatory documentation and auditability. By translating high-level governance\nprinciples into practical, verifiable metrics, LibVulnWatch advances technical\nAI governance with a scalable, transparent mechanism for continuous supply\nchain risk assessment and informed library selection.", "AI": {"tldr": "Open-source AI libraries are crucial for modern AI systems but pose significant risks. LibVulnWatch is a graph-based framework that performs deep evaluations of these libraries, generating reproducible scores across five critical domains and publishing them to a public leaderboard.", "motivation": "Open-source AI libraries pose significant risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance.", "method": "LibVulnWatch uses a graph-based agentic assessment framework built on LangGraph. It coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases.", "result": "Applied to 20 widely used libraries, LibVulnWatch covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library, including critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability.", "conclusion": "LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection."}}
{"id": "2412.15404", "pdf": "https://arxiv.org/pdf/2412.15404", "abs": "https://arxiv.org/abs/2412.15404", "authors": ["Ahmet Yasin Aytar", "Kemal Kilic", "Kamer Kaya"], "title": "A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "In the rapidly evolving field of data science, efficiently navigating the\nexpansive body of academic literature is crucial for informed decision-making\nand innovation. This paper presents an enhanced Retrieval-Augmented Generation\n(RAG) application, an artificial intelligence (AI)-based system designed to\nassist data scientists in accessing precise and contextually relevant academic\nresources. The AI-powered application integrates advanced techniques, including\nthe GeneRation Of BIbliographic Data (GROBID) technique for extracting\nbibliographic information, fine-tuned embedding models, semantic chunking, and\nan abstract-first retrieval method, to significantly improve the relevance and\naccuracy of the retrieved information. This implementation of AI specifically\naddresses the challenge of academic literature navigation. A comprehensive\nevaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)\nframework demonstrates substantial improvements in key metrics, particularly\nContext Relevance, underscoring the system's effectiveness in reducing\ninformation overload and enhancing decision-making processes. Our findings\nhighlight the potential of this enhanced Retrieval-Augmented Generation system\nto transform academic exploration within data science, ultimately advancing the\nworkflow of research and innovation in the field.", "AI": {"tldr": "An enhanced Retrieval-Augmented Generation (RAG) application is presented to assist data scientists in accessing precise and contextually relevant academic resources, with substantial improvements shown in Context Relevance.", "motivation": "Efficiently navigating the expansive body of academic literature is crucial for informed decision-making and innovation in data science.", "method": "The AI-powered application integrates advanced techniques including GROBID for extracting bibliographic information, fine-tuned embedding models, semantic chunking, and an abstract-first retrieval method.", "result": "A comprehensive evaluation using the RAGAS framework demonstrates substantial improvements in key metrics, particularly Context Relevance.", "conclusion": "The findings highlight the potential of this enhanced RAG system to transform academic exploration within data science and advance the workflow of research and innovation."}}
{"id": "2505.09011", "pdf": "https://arxiv.org/pdf/2505.09011", "abs": "https://arxiv.org/abs/2505.09011", "authors": ["Antonio Candito", "Matthew D Blackledge", "Richard Holbrey", "Nuria Porta", "Ana Ribeiro", "Fabio Zugni", "Luca D'Erme", "Francesca Castagnoli", "Alina Dragan", "Ricardo Donners", "Christina Messiou", "Nina Tunariu", "Dow-Mu Koh"], "title": "Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer", "categories": ["cs.LG"], "comment": null, "summary": "We developed an AI-driven software solution to quantify metastatic bone\ndisease from WB-DWI scans. Core technologies include: (i) a weakly-supervised\nResidual U-Net model generating a skeleton probability map to isolate bone;\n(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a\nsignal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional\nneural network that processes outputs from (i) and (ii) to generate a mask of\nsuspected bone lesions, characterised by higher b900 signal intensity due to\nrestricted water diffusion. This mask is applied to the gADC map to extract TDV\nand gADC statistics. We tested the tool using expert-defined metastatic bone\ndisease delineations on 66 datasets, assessed repeatability of imaging\nbiomarkers (N=10), and compared software-based response assessment with a\nconstruct reference standard based on clinical, laboratory and imaging\nassessments (N=118). Dice score between manual and automated delineations was\n0.6 for lesions within pelvis and spine, with an average surface distance of\n2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC\nwere below 9% and 5%, respectively. Repeatability analysis showed coefficients\nof variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass\ncorrelation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%\nsensitivity, and 85.7% specificity in assessing response to treatment compared\nto the construct reference standard. Computation time generating a mask\naveraged 90 seconds per scan. Our software enables reproducible TDV and gADC\nquantification from WB-DWI scans for monitoring metastatic bone disease\nresponse, thus providing potentially useful measurements for clinical\ndecision-making in APC patients.", "AI": {"tldr": "The paper presents an AI-driven software solution for quantifying metastatic bone disease from WB-DWI scans, demonstrating high accuracy, sensitivity, and specificity compared to a reference standard.", "motivation": "To develop a reproducible and automated method for quantifying metastatic bone disease from WB-DWI scans, providing useful measurements for clinical decision-making in APC patients.", "method": "The method includes a weakly-supervised Residual U-Net model for generating a skeleton probability map, a statistical framework for WB-DWI intensity normalisation, and a shallow convolutional neural network for processing outputs to generate a mask of suspected bone lesions. This mask is applied to the gADC map to extract TDV and gADC statistics.", "result": "The tool achieved a Dice score of 0.6 for lesions within pelvis and spine, relative differences for log-TDV and median gADC below 9% and 5%, respectively, coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, and intraclass correlation coefficients above 0.9. It also achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing treatment response.", "conclusion": "The developed software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, offering potentially valuable measurements for clinical decision-making."}}
{"id": "2505.08999", "pdf": "https://arxiv.org/pdf/2505.08999", "abs": "https://arxiv.org/abs/2505.08999", "authors": ["Wei-Long Tian", "Peng Gao", "Xiao Liu", "Long Xu", "Hamido Fujita", "Hanan Aljuai", "Mao-Li Wang"], "title": "Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, visual tracking methods based on convolutional neural\nnetworks and Transformers have achieved remarkable performance and have been\nsuccessfully applied in fields such as autonomous driving. However, the\nnumerous security issues exposed by deep learning models have gradually\naffected the reliable application of visual tracking methods in real-world\nscenarios. Therefore, how to reveal the security vulnerabilities of existing\nvisual trackers through effective adversarial attacks has become a critical\nproblem that needs to be addressed. To this end, we propose an adaptive\nmeta-gradient adversarial attack (AMGA) method for visual tracking. This method\nintegrates multi-model ensembles and meta-learning strategies, combining\nmomentum mechanisms and Gaussian smoothing, which can significantly enhance the\ntransferability and attack effectiveness of adversarial examples. AMGA randomly\nselects models from a large model repository, constructs diverse tracking\nscenarios, and iteratively performs both white- and black-box adversarial\nattacks in each scenario, optimizing the gradient directions of each model.\nThis paradigm minimizes the gap between white- and black-box adversarial\nattacks, thus achieving excellent attack performance in black-box scenarios.\nExtensive experimental results on large-scale datasets such as OTB2015, LaSOT,\nand GOT-10k demonstrate that AMGA significantly improves the attack\nperformance, transferability, and deception of adversarial examples. Codes and\ndata are available at https://github.com/pgao-lab/AMGA.", "AI": {"tldr": "The paper proposes an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking that enhances the transferability and attack effectiveness of adversarial examples.", "motivation": "To reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks.", "method": "AMGA integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing. It randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model.", "result": "Extensive experimental results on large-scale datasets demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples.", "conclusion": "AMGA is an effective method to enhance the transferability and attack effectiveness of adversarial examples in visual tracking."}}
{"id": "2505.08902", "pdf": "https://arxiv.org/pdf/2505.08902", "abs": "https://arxiv.org/abs/2505.08902", "authors": ["Lucas McCullum", "Pelagie Ami Agassi", "Leo Anthony Celi", "Daniel K. Ebner", "Chrystinne Oliveira Fernandes", "Rachel S. Hicklen", "Mkliwa Koumbia", "Lisa Soleymani Lehmann", "David Restrepo"], "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner.", "AI": {"tldr": "The paper argues that instead of comparing LLMs to human experts, research should focus on developing strategies for humans to work efficiently with LLMs in clinical settings to ensure patient safety amidst rapid LLM advancements.", "motivation": "There is a growing concern about the potential harm LLMs could cause to patient care if not properly safeguarded. The current trend of comparing LLMs to human experts is seen as counterproductive.", "method": "The authors advocate for shifting research focus from comparison to collaboration between humans and LLMs in clinical settings.", "result": "Demonstrates the need for strategies that enable efficient, almost symbiotic work of humans with LLMs rather than simply comparing their capabilities.", "conclusion": "Future research efforts must prioritize characterizing the safe use of LLMs in clinical settings and fostering human-LLM collaboration."}}
{"id": "2505.09017", "pdf": "https://arxiv.org/pdf/2505.09017", "abs": "https://arxiv.org/abs/2505.09017", "authors": ["Bizhan Alipour Pijan", "Serdar Bozdag"], "title": "DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Most of the dynamic graph representation learning methods involve dividing a\ndynamic graph into discrete snapshots to capture the evolving behavior of nodes\nover time. Existing methods primarily capture only local or global structures\nof each node within a snapshot using message-passing and random walk-based\nmethods. Then, they utilize sequence-based models (e.g., transformers) to\nencode the temporal evolution of node embeddings, and meta-learning techniques\nto update the model parameters. However, these approaches have two limitations.\nFirst, they neglect the extraction of global and local information\nsimultaneously in each snapshot. Second, they fail to consider the model's\nperformance in the current snapshot during parameter updates, resulting in a\nlack of temporal dependency management. Recently, HiPPO (High-order Polynomial\nProjection Operators) algorithm has gained attention for their ability to\noptimize and preserve sequence history in State Space Model (SSM). To address\nthe aforementioned limitations in dynamic graph representation learning, we\npropose a novel method called Multi-view Dynamic Graph Embeddings with State\nSpace Model Gradient Update (DyGSSM). Our approach combines Graph Convolution\nNetworks (GCN) for local feature extraction and random walk with Gated\nRecurrent Unit (GRU) for global feature extraction in each snapshot. We then\nintegrate the local and global features using a cross-attention mechanism.\nAdditionally, we incorporate an SSM based on HiPPO algorithm to account for\nlong-term dependencies when updating model parameters, ensuring that model\nperformance in each snapshot informs subsequent updates. Experiments on five\npublic datasets show that our method outperforms existing baseline and\nstate-of-the-art (SOTA) methods in 17 out of 20 cases.", "AI": {"tldr": "The paper proposes DyGSSM, a novel method for dynamic graph representation learning that integrates local and global features using GCN, random walk with GRU, and cross-attention mechanism. It also incorporates SSM based on HiPPO algorithm to manage long-term dependencies in parameter updates. Experiments show its superior performance over existing methods.", "motivation": "Existing dynamic graph representation learning methods fail to simultaneously extract global and local information within each snapshot and lack effective temporal dependency management during model parameter updates.", "method": "DyGSSM combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. A cross-attention mechanism integrates these features, while an SSM based on the HiPPO algorithm manages long-term dependencies in parameter updates.", "result": "Experiments on five public datasets demonstrate that DyGSSM outperforms existing baseline and state-of-the-art methods in 17 out of 20 cases.", "conclusion": "DyGSSM addresses the limitations of existing methods by effectively capturing both local and global structures in dynamic graphs and managing temporal dependencies in parameter updates."}}
{"id": "2505.09018", "pdf": "https://arxiv.org/pdf/2505.09018", "abs": "https://arxiv.org/abs/2505.09018", "authors": ["Adarsh Kumar"], "title": "Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective dietary monitoring is critical for managing Type 2 diabetes, yet\naccurately estimating caloric intake remains a major challenge. While\ncontinuous glucose monitors (CGMs) offer valuable physiological data, they\noften fall short in capturing the full nutritional profile of meals due to\ninter-individual and meal-specific variability. In this work, we introduce a\nmultimodal deep learning framework that jointly leverages CGM time-series data,\nDemographic/Microbiome, and pre-meal food images to enhance caloric estimation.\nOur model utilizes attention based encoding and a convolutional feature\nextraction for meal imagery, multi-layer perceptrons for CGM and Microbiome\ndata followed by a late fusion strategy for joint reasoning. We evaluate our\napproach on a curated dataset of over 40 participants, incorporating\nsynchronized CGM, Demographic and Microbiome data and meal photographs with\nstandardized caloric labels. Our model achieves a Root Mean Squared Relative\nError (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These\nfindings demonstrate the potential of multimodal sensing to improve automated\ndietary assessment tools for chronic disease management.", "AI": {"tldr": "A multimodal deep learning framework that integrates CGM time-series data, Demographic/Microbiome info, and pre-meal food images to improve caloric intake estimation for Type 2 diabetes management is presented. It reduces RMSRE to 0.2544, surpassing baseline models by over 50%.", "motivation": "Accurate estimation of caloric intake is essential for managing Type 2 diabetes but remains challenging. Current methods using Continuous Glucose Monitors (CGMs) lack comprehensive nutritional profiling due to variability among individuals and meals.", "method": "The introduced framework uses a combination of attention-based encoding for meal imagery, convolutional feature extraction, multi-layer perceptrons for CGM and Microbiome data, followed by a late fusion strategy for joint reasoning. This allows integration of diverse data types such as CGM time-series, demographic/microbiome information, and pre-meal food images.", "result": "Evaluated on a dataset with over 40 participants, the model achieved an RMSRE of 0.2544, showing more than 50% improvement compared to baseline models.", "conclusion": "The findings highlight the potential of multimodal sensing in enhancing automated dietary assessment tools, which could be beneficial for chronic disease management like Type 2 diabetes."}}
{"id": "2505.08798", "pdf": "https://arxiv.org/pdf/2505.08798", "abs": "https://arxiv.org/abs/2505.08798", "authors": ["Mobina Shrestha", "Bishwas Mandal", "Vishal Mandal", "Asis Shrestha"], "title": "In-Context Learning for Label-Efficient Cancer Image Classification in Oncology", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The application of AI in oncology has been limited by its reliance on large,\nannotated datasets and the need for retraining models for domain-specific\ndiagnostic tasks. Taking heed of these limitations, we investigated in-context\nlearning as a pragmatic alternative to model retraining by allowing models to\nadapt to new diagnostic tasks using only a few labeled examples at inference,\nwithout the need for retraining. Using four vision-language models\n(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across\nthree oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our\nknowledge, this is the first study to compare the performance of multiple VLMs\non different oncology classification tasks. Without any parameter updates, all\nmodels showed significant gains with few-shot prompting, with GPT-4o reaching\nan F1 score of 0.81 in binary classification and 0.60 in multi-class\nclassification settings. While these results remain below the ceiling of fully\nfine-tuned systems, they highlight the potential of ICL to approximate\ntask-specific behavior using only a handful of examples, reflecting how\nclinicians often reason from prior cases. Notably, open-source models like\nPaligemma and CLIP demonstrated competitive gains despite their smaller size,\nsuggesting feasibility for deployment in computing constrained clinical\nenvironments. Overall, these findings highlight the potential of ICL as a\npractical solution in oncology, particularly for rare cancers and\nresource-limited contexts where fine-tuning is infeasible and annotated data is\ndifficult to obtain.", "AI": {"tldr": "\u5728\u80bf\u7624\u5b66\u4e2d\uff0c\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u662f\u4e00\u79cd\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u9002\u5e94\u65b0\u8bca\u65ad\u4efb\u52a1\u7684\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u9996\u6b21\u6bd4\u8f83\u4e86\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u8868\u660eICL\u5177\u6709\u63a5\u8fd1\u7279\u5b9a\u4efb\u52a1\u884c\u4e3a\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7a00\u6709\u764c\u75c7\u548c\u8d44\u6e90\u6709\u9650\u7684\u60c5\u5883\u3002", "motivation": "\u5f53\u524dAI\u5728\u80bf\u7624\u5b66\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u5bf9\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8bca\u65ad\u4efb\u52a1\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u9700\u6c42\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u4f5c\u4e3a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u56db\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08Paligemma\u3001CLIP\u3001ALIGN\u548cGPT-4o\uff09\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u6765\u8bc4\u4f30\u5176\u5728\u4e09\u4e2a\u80bf\u7624\u5b66\u6570\u636e\u96c6\uff08MHIST\u3001PatchCamelyon\u548cHAM10000\uff09\u4e0a\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5c11\u91cf\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u5176\u4e2dGPT-4o\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230F1\u5206\u65700.81\uff0c\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u52300.60\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u7ed3\u679c\u4f4e\u4e8e\u5b8c\u5168\u5fae\u8c03\u7cfb\u7edf\u7684\u6c34\u5e73\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u5982Paligemma\u548cCLIP\u4e5f\u5c55\u793a\u4e86\u7ade\u4e89\u529b\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u80bf\u7624\u5b66\u4e2d\u5c55\u73b0\u51fa\u4f5c\u4e3a\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7a00\u6709\u764c\u75c7\u548c\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5229\u7528\u5c11\u91cf\u4f8b\u5b50\u6765\u8fd1\u4f3c\u7279\u5b9a\u4efb\u52a1\u7684\u884c\u4e3a\u3002"}}
{"id": "2505.09022", "pdf": "https://arxiv.org/pdf/2505.09022", "abs": "https://arxiv.org/abs/2505.09022", "authors": ["Annan Yu", "N. Benjamin Erichson"], "title": "Block-Biased Mamba for Long-Range Sequence Processing", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Mamba extends earlier state space models (SSMs) by introducing\ninput-dependent dynamics, and has demonstrated strong empirical performance\nacross a range of domains, including language modeling, computer vision, and\nfoundation models. However, a surprising weakness remains: despite being built\non architectures designed for long-range dependencies, Mamba performs poorly on\nlong-range sequential tasks. Understanding and addressing this gap is important\nfor improving Mamba's universality and versatility. In this work, we analyze\nMamba's limitations through three perspectives: expressiveness, inductive bias,\nand training stability. Our theoretical results show how Mamba falls short in\neach of these aspects compared to earlier SSMs such as S4D. To address these\nissues, we propose $\\text{B}_2\\text{S}_6$, a simple extension of Mamba's S6\nunit that combines block-wise selective dynamics with a channel-specific bias.\nWe prove that these changes equip the model with a better-suited inductive bias\nand improve its expressiveness and stability. Empirically,\n$\\text{B}_2\\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks\nwhile maintaining Mamba's performance on language modeling benchmarks.", "AI": {"tldr": "Mamba model has weakness in long-range sequential tasks despite being designed for long-range dependencies. This paper analyzes Mamba's limitations and proposes an extension called B2S6 which improves its performance.", "motivation": "To understand and address the gap in Mamba's performance on long-range sequential tasks, improving its universality and versatility.", "method": "Analyze Mamba's limitations through three perspectives - expressiveness, inductive bias, and training stability; Propose B2S6, an extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias.", "result": "Theoretically proves that B2S6 equips the model with a better-suited inductive bias and improves its expressiveness and stability. Empirically, B2S6 outperforms S4 and S4D on Long-Range Arena tasks while maintaining Mamba's performance on language modeling benchmarks.", "conclusion": "B2S6 is a successful extension to Mamba that addresses its limitations in long-range sequential tasks."}}
{"id": "2505.09073", "pdf": "https://arxiv.org/pdf/2505.09073", "abs": "https://arxiv.org/abs/2505.09073", "authors": ["J. Brennan Peace", "Shuowen Hu", "Benjamin S. Riggan"], "title": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition", "categories": ["cs.CV"], "comment": "To appear at the IEEE International Conference on Automatic Face and\n  Gesture 2025 (FG2025)", "summary": "Despite recent advances in facial recognition, there remains a fundamental\nissue concerning degradations in performance due to substantial perspective\n(pose) differences between enrollment and query (probe) imagery. Therefore, we\npropose a novel domain adaptive framework to facilitate improved performances\nacross large discrepancies in pose by enabling image-based (2D) representations\nto infer properties of inherently pose invariant point cloud (3D)\nrepresentations. Specifically, our proposed framework achieves better pose\ninvariance by using (1) a shared (joint) attention mapping to emphasize common\npatterns that are most correlated between 2D facial images and 3D facial data\nand (2) a joint entropy regularizing loss to promote better\nconsistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D\nand 3D representations$\\unicode{x2014}$by leveraging both attention maps. This\nframework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms\ncompetitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$)\nTAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and\n1.57$\\unicode{x0025}$, respectively.", "AI": {"tldr": "The paper proposes a novel domain adaptive framework for improving facial recognition performance across large pose differences.", "motivation": "To address the degradation in performance of facial recognition due to substantial perspective (pose) differences between enrollment and query imagery.", "method": "The framework uses a shared (joint) attention mapping to emphasize common patterns correlated between 2D facial images and 3D facial data, along with a joint entropy regularizing loss to promote consistency by enhancing correlations among the intersecting 2D and 3D representations.", "result": "Evaluated on FaceScape and ARL-VTF datasets, the framework outperforms competitive methods with improvements of at least 7.1% and 1.57% in profile (90\u00b0+) TAR @ 1% FAR respectively.", "conclusion": "The proposed domain adaptive framework facilitates improved performances across large discrepancies in pose for facial recognition."}}
{"id": "2505.09063", "pdf": "https://arxiv.org/pdf/2505.09063", "abs": "https://arxiv.org/abs/2505.09063", "authors": ["Khalid Rafiq", "Wenjing Liao", "Aditya G. Nair"], "title": "Single-shot prediction of parametric partial differential equations", "categories": ["cs.LG", "cs.NA", "math.NA", "68T07"], "comment": "35 pages, 17 figures", "summary": "We introduce Flexi-VAE, a data-driven framework for efficient single-shot\nforecasting of nonlinear parametric partial differential equations (PDEs),\neliminating the need for iterative time-stepping while maintaining high\naccuracy and stability. Flexi-VAE incorporates a neural propagator that\nadvances latent representations forward in time, aligning latent evolution with\nphysical state reconstruction in a variational autoencoder setting. We evaluate\ntwo propagation strategies, the Direct Concatenation Propagator (DCP) and the\nPositional Encoding Propagator (PEP), and demonstrate, through\nrepresentation-theoretic analysis, that DCP offers superior long-term\ngeneralization by fostering disentangled and physically meaningful latent\nspaces. Geometric diagnostics, including Jacobian spectral analysis, reveal\nthat propagated latent states reside in regions of lower decoder sensitivity\nand more stable local geometry than those derived via direct encoding,\nenhancing robustness for long-horizon predictions. We validate Flexi-VAE on\ncanonical PDE benchmarks, the 1D viscous Burgers equation and the 2D\nadvection-diffusion equation, achieving accurate forecasts across wide\nparametric ranges. The model delivers over 50x CPU and 90x GPU speedups\ncompared to autoencoder-LSTM baselines for large temporal shifts. These results\nposition Flexi-VAE as a scalable and interpretable surrogate modeling tool for\naccelerating high-fidelity simulations in computational fluid dynamics (CFD)\nand other parametric PDE-driven applications, with extensibility to\nhigher-dimensional and more complex systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlexi-VAE\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5355\u6b21\u9884\u6d4b\u975e\u7ebf\u6027\u53c2\u6570\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u6d88\u9664\u4e86\u8fed\u4ee3\u65f6\u95f4\u6b65\u957f\u7684\u9700\u8981\u3002\u8be5\u6a21\u578b\u57281D\u7c98\u6027Burgers\u65b9\u7a0b\u548c2D\u5bf9\u6d41\u6269\u6563\u65b9\u7a0b\u7684\u7ecf\u5178PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6bd4\u81ea\u7f16\u7801\u5668-LSTM\u57fa\u7ebf\u6a21\u578b\u5feb50\u500d\uff08CPU\uff09\u548c90\u500d\uff08GPU\uff09\u3002", "motivation": "\u5f53\u524d\u975e\u7ebf\u6027\u53c2\u6570\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u8fed\u4ee3\u65f6\u95f4\u6b65\u957f\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6d88\u9664\u8fed\u4ee3\u65f6\u95f4\u6b65\u957f\u9700\u6c42\u3001\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "Flexi-VAE\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u795e\u7ecf\u4f20\u64ad\u5668\uff0c\u8be5\u4f20\u64ad\u5668\u5c06\u6f5c\u5728\u8868\u793a\u5411\u524d\u63a8\u8fdb\u65f6\u95f4\uff0c\u7ed3\u5408\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u8bbe\u7f6e\u4e2d\u7684\u6f5c\u5728\u6f14\u5316\u4e0e\u7269\u7406\u72b6\u6001\u91cd\u5efa\u3002\u6587\u4e2d\u8bc4\u4f30\u4e86\u4e24\u79cd\u4f20\u64ad\u7b56\u7565\uff1a\u76f4\u63a5\u8fde\u63a5\u4f20\u64ad\u5668\uff08DCP\uff09\u548c\u4f4d\u7f6e\u7f16\u7801\u4f20\u64ad\u5668\uff08PEP\uff09\u3002\u901a\u8fc7\u8868\u793a\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cDCP\u901a\u8fc7\u4fc3\u8fdb\u89e3\u8026\u548c\u7269\u7406\u610f\u4e49\u660e\u786e\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u957f\u671f\u6cdb\u5316\u80fd\u529b\u3002\u51e0\u4f55\u8bca\u65ad\uff08\u5305\u62ec\u96c5\u53ef\u6bd4\u8c31\u5206\u6790\uff09\u663e\u793a\uff0c\u4f20\u64ad\u7684\u6f5c\u5728\u72b6\u6001\u4f4d\u4e8e\u89e3\u7801\u5668\u654f\u611f\u6027\u8f83\u4f4e\u548c\u5c40\u90e8\u51e0\u4f55\u66f4\u7a33\u5b9a\u7684\u533a\u57df\uff0c\u589e\u5f3a\u4e86\u957f\u65f6\u95f4\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u57281D\u7c98\u6027Burgers\u65b9\u7a0b\u548c2D\u5bf9\u6d41\u6269\u6563\u65b9\u7a0b\u7684\u7ecf\u5178PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlexi-VAE\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u53c2\u6570\u8303\u56f4\u5185\u7684\u7cbe\u786e\u9884\u6d4b\uff0c\u5e76\u4e14\u76f8\u6bd4\u81ea\u7f16\u7801\u5668-LSTM\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u5927\u65f6\u95f4\u6b65\u957f\u79fb\u52a8\u65f6\u63d0\u4f9b\u4e86\u8d85\u8fc750\u500d\uff08CPU\uff09\u548c90\u500d\uff08GPU\uff09\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "Flexi-VAE\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u5efa\u6a21\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u52a0\u901f\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u548c\u5176\u4ed6\u7531\u53c2\u6570PDE\u9a71\u52a8\u7684\u5e94\u7528\u4e2d\u7684\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u5e76\u4e14\u5177\u6709\u6269\u5c55\u5230\u66f4\u9ad8\u7ef4\u5ea6\u548c\u66f4\u590d\u6742\u7cfb\u7edf\u7684\u80fd\u529b\u3002"}}
{"id": "2505.09092", "pdf": "https://arxiv.org/pdf/2505.09092", "abs": "https://arxiv.org/abs/2505.09092", "authors": ["Yuhang Wang", "Abdulaziz Alhuraish", "Shengming Yuan", "Hao Zhou"], "title": "OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its\nreal-world performance remains underexplored due to proprietary systems and\nlimited data access. This paper presents OpenLKA, the first open, large-scale\ndataset for LKA evaluation and improvement. It includes 400 hours of driving\ndata from 50+ production vehicle models, collected through extensive road\ntesting in Tampa, Florida and global contributions from the Comma.ai driving\ncommunity. The dataset spans a wide range of challenging scenarios, including\ncomplex road geometries, degraded lane markings, adverse weather, lighting\nconditions and surrounding traffic. The dataset is multimodal, comprising: i)\nfull CAN bus streams, decoded using custom reverse-engineered DBC files to\nextract key LKA events (e.g., system disengagements, lane detection failures);\nii) synchronized high-resolution dash-cam video; iii) real-time outputs from\nOpenpilot, providing accurate estimates of road curvature and lane positioning;\niv) enhanced scene annotations generated by Vision Language Models, describing\nlane visibility, pavement quality, weather, lighting, and traffic conditions.\nBy integrating vehicle-internal signals with high-fidelity perception and rich\nsemantic context, OpenLKA provides a comprehensive platform for benchmarking\nthe real-world performance of production LKA systems, identifying\nsafety-critical operational scenarios, and assessing the readiness of current\nroad infrastructure for autonomous driving. The dataset is publicly available\nat: https://github.com/OpenLKA/OpenLKA.", "AI": {"tldr": "The paper introduces OpenLKA, the first open large-scale dataset for LKA evaluation and improvement, containing 400 hours of driving data from over 50 vehicle models. It includes CAN bus streams, dash-cam video, Openpilot outputs, and scene annotations.", "motivation": "Lane Keeping Assist (LKA) systems' real-world performance remains underexplored due to proprietary systems and limited data access.", "method": "OpenLKA is a dataset that integrates vehicle-internal signals with high-fidelity perception and rich semantic context, collected through extensive road testing and global contributions.", "result": "OpenLKA provides a comprehensive platform for benchmarking production LKA systems, identifying safety-critical operational scenarios, and assessing road infrastructure readiness for autonomous driving.", "conclusion": "OpenLKA is publicly available and aims to improve the understanding and performance of LKA systems."}}
{"id": "2505.09076", "pdf": "https://arxiv.org/pdf/2505.09076", "abs": "https://arxiv.org/abs/2505.09076", "authors": ["Berkay Guler", "Hamid Jafarkhani"], "title": "AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Deep learning models for channel estimation in Orthogonal Frequency Division\nMultiplexing (OFDM) systems often suffer from performance degradation under\nfast-fading channels and low-SNR scenarios. To address these limitations, we\nintroduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model\nspecifically designed to enhance channel estimation in challenging\nenvironments. Our approach employs convolutional layers that exploit locality\nbias to capture strong correlations between neighboring channel elements,\ncombined with a transformer encoder that applies the global Attention mechanism\nto channel patches. This approach effectively models both long-range\ndependencies and spectro-temporal interactions within single OFDM frames. We\nfurther augment the model's adaptability by integrating nonlinear\nrepresentations of available channel statistics SNR, delay spread, and Doppler\nshift as priors. A residual connection is employed to merge global features\nfrom the transformer with local features from early convolutional processing,\nfollowed by final convolutional layers to refine the hierarchical channel\nrepresentation. Despite its compact architecture, AdaFortiTran achieves up to 6\ndB reduction in mean squared error (MSE) compared to state-of-the-art models.\nTested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),\nand delay spreads (50-300 ns), it demonstrates superior robustness in\nhigh-mobility environments.", "AI": {"tldr": "The paper presents AdaFortiTran, a new model improving channel estimation in OFDM systems under fast-fading channels and low-SNR scenarios by combining convolutional layers and transformer encoder, achieving up to 6 dB MSE reduction compared to state-of-the-art models.", "motivation": "Deep learning models for channel estimation in OFDM systems have limitations in performance under fast-fading channels and low-SNR scenarios.", "method": "The method uses convolutional layers to capture correlations between neighboring channel elements and a transformer encoder with global Attention mechanism to model long-range dependencies and spectro-temporal interactions. Nonlinear representations of channel statistics are integrated as priors, and residual connections merge global and local features.", "result": "AdaFortiTran achieves up to 6 dB reduction in mean squared error (MSE) compared to state-of-the-art models across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB), and delay spreads (50-300 ns).", "conclusion": "AdaFortiTran demonstrates superior robustness in high-mobility environments, making it a significant advancement in channel estimation for OFDM systems."}}
{"id": "2505.09118", "pdf": "https://arxiv.org/pdf/2505.09118", "abs": "https://arxiv.org/abs/2505.09118", "authors": ["Dayong Liang", "Changmeng Zheng", "Zhiyuan Wen", "Yi Cai", "Xiao-Yong Wei", "Qing Li"], "title": "Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Traditional scene graphs primarily focus on spatial relationships, limiting\nvision-language models' (VLMs) ability to reason about complex interactions in\nvisual scenes. This paper addresses two key challenges: (1) conventional\ndetection-to-construction methods produce unfocused, contextually irrelevant\nrelationship sets, and (2) existing approaches fail to form persistent memories\nfor generalizing interaction reasoning to new scenes. We propose\nInteraction-augmented Scene Graph Reasoning (ISGR), a framework that enhances\nVLMs' interactional reasoning through three complementary components. First,\nour dual-stream graph constructor combines SAM-powered spatial relation\nextraction with interaction-aware captioning to generate functionally salient\nscene graphs with spatial grounding. Second, we employ targeted interaction\nqueries to activate VLMs' latent knowledge of object functionalities,\nconverting passive recognition into active reasoning about how objects work\ntogether. Finally, we introduce a lone-term memory reinforcement learning\nstrategy with a specialized interaction-focused reward function that transforms\ntransient patterns into long-term reasoning heuristics. Extensive experiments\ndemonstrate that our approach significantly outperforms baseline methods on\ninteraction-heavy reasoning benchmarks, with particularly strong improvements\non complex scene understanding tasks. The source code can be accessed at\nhttps://github.com/open_upon_acceptance.", "AI": {"tldr": "The paper introduces Interaction-augmented Scene Graph Reasoning (ISGR), a framework enhancing vision-language models' ability to reason about complex interactions in visual scenes through three components: dual-stream graph constructor, targeted interaction queries, and long-term memory reinforcement learning. It outperforms baselines on interaction-heavy benchmarks.", "motivation": "Traditional scene graphs focus only on spatial relationships, limiting vision-language models' ability to reason about complex interactions in visual scenes. Conventional methods produce unfocused relationship sets and fail to form persistent memories for generalizing interaction reasoning.", "method": "The ISGR framework includes: 1) A dual-stream graph constructor combining spatial relation extraction with interaction-aware captioning; 2) Targeted interaction queries activating VLMs' latent knowledge of object functionalities; 3) A long-term memory reinforcement learning strategy with an interaction-focused reward function.", "result": "Extensive experiments show that ISGR significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, especially improving complex scene understanding tasks.", "conclusion": "ISGR enhances VLMs' interactional reasoning capabilities by addressing limitations in conventional detection-to-construction methods and persistent memory formation."}}
{"id": "2505.08807", "pdf": "https://arxiv.org/pdf/2505.08807", "abs": "https://arxiv.org/abs/2505.08807", "authors": ["Yuntao Wang", "Yanghe Pan", "Shaolong Guo", "Zhou Su"], "title": "Security of Internet of Agents: Attacks and Countermeasures", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 5 figures, 3 tables, submitted to IEEE OJCS", "summary": "With the rise of large language and vision-language models, AI agents have\nevolved into autonomous, interactive systems capable of perception, reasoning,\nand decision-making. As they proliferate across virtual and physical domains,\nthe Internet of Agents (IoA) has emerged as a key infrastructure for enabling\nscalable and secure coordination among heterogeneous agents. This survey offers\na comprehensive examination of the security and privacy landscape in IoA\nsystems. We begin by outlining the IoA architecture and its distinct\nvulnerabilities compared to traditional networks, focusing on four critical\naspects: identity authentication threats, cross-agent trust issues, embodied\nsecurity, and privacy risks. We then review existing and emerging defense\nmechanisms and highlight persistent challenges. Finally, we identify open\nresearch directions to advance the development of resilient and\nprivacy-preserving IoA ecosystems.", "AI": {"tldr": "This paper surveys the security and privacy issues in Internet of Agents (IoA) systems, which are composed of autonomous AI agents. It outlines the architecture of IoA, its unique vulnerabilities in aspects like identity authentication and cross-agent trust, reviews defense mechanisms, and points out open research directions.", "motivation": "The motivation of this paper is to provide a comprehensive examination of the security and privacy landscape in IoA systems as these systems become more prevalent and important for coordination among different AI agents.", "method": "The method involves reviewing literature on IoA architecture and its vulnerabilities, analyzing existing and emerging defense mechanisms against identified threats, and identifying gaps and challenges in current research.", "result": "The result is an overview of the state-of-the-art in securing IoA systems, including insights into the specific threats such as identity authentication threats, cross-agent trust issues, embodied security, and privacy risks. The review also highlights persistent challenges in creating secure and private IoA ecosystems.", "conclusion": "The conclusion emphasizes the need for further research in developing resilient and privacy-preserving IoA systems, suggesting several open research directions."}}
{"id": "2505.09085", "pdf": "https://arxiv.org/pdf/2505.09085", "abs": "https://arxiv.org/abs/2505.09085", "authors": ["Jiaxuan Chen", "Yu Qi", "Yueming Wang", "Gang Pan"], "title": "Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in deep neural networks (DNNs), particularly large-scale\nlanguage models, have demonstrated remarkable capabilities in image and natural\nlanguage understanding. Although scaling up model parameters with increasing\nvolume of training data has progressively improved DNN capabilities, achieving\ncomplex cognitive abilities - such as understanding abstract concepts,\nreasoning, and adapting to novel scenarios, which are intrinsic to human\ncognition - remains a major challenge. In this study, we show that\nbrain-in-the-loop supervised learning, utilizing a small set of brain signals,\ncan effectively transfer human conceptual structures to DNNs, significantly\nenhancing their comprehension of abstract and even unseen concepts.\nExperimental results further indicate that the enhanced cognitive capabilities\nlead to substantial performance gains in challenging tasks, including\nfew-shot/zero-shot learning and out-of-distribution recognition, while also\nyielding highly interpretable concept representations. These findings highlight\nthat human-in-the-loop supervision can effectively augment the complex\ncognitive abilities of large models, offering a promising pathway toward\ndeveloping more human-like cognitive abilities in artificial systems.", "AI": {"tldr": "\u901a\u8fc7\u8111\u4fe1\u53f7\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u62bd\u8c61\u6982\u5ff5\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u5347\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u53c2\u6570\u548c\u8bad\u7ec3\u6570\u636e\u63d0\u5347\u4e86DNN\u7684\u80fd\u529b\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u7684\u8ba4\u77e5\u80fd\u529b\uff08\u5982\u7406\u89e3\u62bd\u8c61\u6982\u5ff5\u3001\u63a8\u7406\u548c\u9002\u5e94\u65b0\u573a\u666f\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u8111-\u73af\u8def\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u8111\u4fe1\u53f7\u5c06\u4eba\u7c7b\u7684\u6982\u5ff5\u7ed3\u6784\u8f6c\u79fb\u5230DNN\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u5bf9\u62bd\u8c61\u6982\u5ff5\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86DNN\u5728\u5c0f\u6837\u672c/\u96f6\u6837\u672c\u5b66\u4e60\u548c\u5206\u5e03\u5916\u8bc6\u522b\u7b49\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8fd8\u751f\u6210\u4e86\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u8868\u793a\u3002", "conclusion": "\u4eba\u7c7b-\u73af\u8def\u76d1\u7763\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u5927\u578b\u6a21\u578b\u7684\u590d\u6742\u8ba4\u77e5\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u50cf\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u7684\u4eba\u5de5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2505.09123", "pdf": "https://arxiv.org/pdf/2505.09123", "abs": "https://arxiv.org/abs/2505.09123", "authors": ["Guoying Liang", "Su Yang"], "title": "Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Big model has emerged as a new research paradigm that can be applied to\nvarious down-stream tasks with only minor effort for domain adaption.\nCorrespondingly, this study tackles Camouflaged Object Detection (COD)\nleveraging the Segment Anything Model (SAM). The previous studies declared that\nSAM is not workable for COD but this study reveals that SAM works if promoted\nproperly, for which we devise a new framework to render point promotions:\nFirst, we develop the Promotion Point Targeting Network (PPT-net) to leverage\nmulti-scale features in predicting the probabilities of camouflaged objects'\npresences at given candidate points over the image. Then, we develop a key\npoint selection (KPS) algorithm to deploy both positive and negative point\npromotions contrastively to SAM to guide the segmentation. It is the first work\nto facilitate big model for COD and achieves plausible results experimentally\nover the existing methods on 3 data sets under 6 metrics. This study\ndemonstrates an off-the-shelf methodology for COD by leveraging SAM, which\ngains advantage over designing professional models from scratch, not only in\nperformance, but also in turning the problem to a less challenging task, that\nis, seeking informative but not exactly precise promotions.", "AI": {"tldr": "This study successfully applies the Segment Anything Model (SAM) to Camouflaged Object Detection (COD), showing that with appropriate promotion, SAM can work effectively. It introduces a new framework involving Promotion Point Targeting Network (PPT-net) and key point selection (KPS) algorithm to guide segmentation.", "motivation": "To explore the feasibility of using big models like SAM for COD, overcoming previous claims that SAM is not workable for this task.", "method": "Devised a new framework including PPT-net for predicting camouflaged object presences and KPS algorithm for deploying point promotions contrastively to SAM.", "result": "Achieves plausible results experimentally over existing methods on 3 data sets under 6 metrics.", "conclusion": "Demonstrates an effective off-the-shelf methodology for COD by leveraging SAM, which performs better than designing professional models from scratch."}}
{"id": "2505.09089", "pdf": "https://arxiv.org/pdf/2505.09089", "abs": "https://arxiv.org/abs/2505.09089", "authors": ["Philipp Hess", "Maximilian Gelbrecht", "Christof Sch\u00f6tz", "Michael Aich", "Yu Huang", "Shangshang Yang", "Niklas Boers"], "title": "Generating time-consistent dynamics with discriminator-guided image diffusion models", "categories": ["cs.LG"], "comment": null, "summary": "Realistic temporal dynamics are crucial for many video generation, processing\nand modelling applications, e.g. in computational fluid dynamics, weather\nprediction, or long-term climate simulations. Video diffusion models (VDMs) are\nthe current state-of-the-art method for generating highly realistic dynamics.\nHowever, training VDMs from scratch can be challenging and requires large\ncomputational resources, limiting their wider application. Here, we propose a\ntime-consistency discriminator that enables pretrained image diffusion models\nto generate realistic spatiotemporal dynamics. The discriminator guides the\nsampling inference process and does not require extensions or finetuning of the\nimage diffusion model. We compare our approach against a VDM trained from\nscratch on an idealized turbulence simulation and a real-world global\nprecipitation dataset. Our approach performs equally well in terms of temporal\nconsistency, shows improved uncertainty calibration and lower biases compared\nto the VDM, and achieves stable centennial-scale climate simulations at daily\ntime steps.", "AI": {"tldr": "This paper proposes a time-consistency discriminator that allows pretrained image diffusion models to generate realistic spatiotemporal dynamics, which performs equally well as Video Diffusion Models (VDMs) in temporal consistency while showing advantages in uncertainty calibration and bias reduction.", "motivation": "The motivation is to address the challenge of training Video Diffusion Models (VDMs) from scratch, which demands large computational resources and limits their application. The authors aim to leverage pretrained image diffusion models to generate realistic spatiotemporal dynamics with less computational burden.", "method": "The method involves introducing a time-consistency discriminator that guides the sampling inference process of pretrained image diffusion models without requiring extensions or fine-tuning of these models. This approach is compared against VDMs trained from scratch on turbulence simulation and precipitation datasets.", "result": "The results indicate that the proposed approach matches VDMs in terms of temporal consistency, provides better uncertainty calibration, reduces biases, and successfully achieves stable long-term climate simulations at daily time steps.", "conclusion": "The conclusion is that the time-consistency discriminator offers an effective alternative to VDMs for generating realistic spatiotemporal dynamics, with advantages in computational efficiency, uncertainty handling, and reduced bias."}}
{"id": "2505.09129", "pdf": "https://arxiv.org/pdf/2505.09129", "abs": "https://arxiv.org/abs/2505.09129", "authors": ["Wei Meng"], "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes", "categories": ["cs.CV", "cs.AI", "es: 68T10, 68T05, 62H35, 68U10", "I.4.9; I.5.1; I.2.10"], "comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight\n  weakly-supervised color intelligence model for tactical video anomaly\n  detection, tested on anonymized African surveillance data", "summary": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future.", "AI": {"tldr": "This paper proposes a lightweight anomaly detection framework based on color features for surveillance video clips in high-risk security tasks, fusing unsupervised KMeans clustering with RGB channel histogram modeling. It successfully identifies anomalous frames in an African country's operation surveillance video without access to original data, showing strong deployability and tactical interpretation value.", "motivation": "Traditional deep learning models face significant challenges when deployed in high-risk security tasks within unlabeled and data-non-exploitable video intelligence environments.", "method": "The method uses a lightweight anomaly detection framework that combines unsupervised KMeans clustering with RGB channel histogram modeling to detect structural anomalies and color mutations in key frames of surveillance video clips.", "result": "The experiment successfully identified multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference in an African country's operation surveillance video, without needing the original data.", "conclusion": "This method is effective for tactical assassination warning, suspicious object screening, and environmental change monitoring. The study highlights the importance of color features as low semantic battlefield signal carriers and suggests future work involving graph neural networks and temporal modeling."}}
{"id": "2505.08809", "pdf": "https://arxiv.org/pdf/2505.08809", "abs": "https://arxiv.org/abs/2505.08809", "authors": ["Shixi Qin", "Zhiyong Yang", "Shilong Bao", "Shi Wang", "Qianqian Xu", "Qingming Huang"], "title": "MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schr\u00f6dinger Bridges", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper focuses on implanting multiple heterogeneous backdoor triggers in\nbridge-based diffusion models designed for complex and arbitrary input\ndistributions. Existing backdoor formulations mainly address single-attack\nscenarios and are limited to Gaussian noise input models. To fill this gap, we\npropose MixBridge, a novel diffusion Schr\\\"odinger bridge (DSB) framework to\ncater to arbitrary input distributions (taking I2I tasks as special cases).\nBeyond this trait, we demonstrate that backdoor triggers can be injected into\nMixBridge by directly training with poisoned image pairs. This eliminates the\nneed for the cumbersome modifications to stochastic differential equations\nrequired in previous studies, providing a flexible tool to study backdoor\nbehavior for bridge models. However, a key question arises: can a single DSB\nmodel train multiple backdoor triggers? Unfortunately, our theory shows that\nwhen attempting this, the model ends up following the geometric mean of benign\nand backdoored distributions, leading to performance conflict across backdoor\ntasks. To overcome this, we propose a Divide-and-Merge strategy to mix\ndifferent bridges, where models are independently pre-trained for each specific\nobjective (Divide) and then integrated into a unified model (Merge). In\naddition, a Weight Reallocation Scheme (WRS) is also designed to enhance the\nstealthiness of MixBridge. Empirical studies across diverse generation tasks\nspeak to the efficacy of MixBridge.", "AI": {"tldr": "This paper proposes MixBridge, a novel diffusion Schr\u00f6dinger bridge framework for arbitrary input distributions, and addresses the challenges of implanting multiple heterogeneous backdoor triggers using a Divide-and-Merge strategy.", "motivation": "Existing backdoor formulations are mainly designed for single-attack scenarios and limited to Gaussian noise input models. This paper aims to address these limitations by proposing a framework that can cater to complex and arbitrary input distributions.", "method": "The proposed method is called MixBridge, a diffusion Schr\u00f6dinger bridge (DSB) framework. It allows for the implantation of multiple heterogeneous backdoor triggers by directly training with poisoned image pairs. To handle the performance conflict across backdoor tasks, a Divide-and-Merge strategy is proposed where models are independently pre-trained and then integrated into a unified model. Additionally, a Weight Reallocation Scheme (WRS) is designed to enhance the stealthiness of MixBridge.", "result": "Empirical studies across diverse generation tasks demonstrate the efficacy of MixBridge in handling arbitrary input distributions and successfully implanting multiple backdoor triggers while maintaining performance.", "conclusion": "MixBridge provides a flexible tool to study backdoor behavior for bridge models and effectively addresses the challenge of training multiple backdoor triggers in a single DSB model through the Divide-and-Merge strategy."}}
{"id": "2505.09106", "pdf": "https://arxiv.org/pdf/2505.09106", "abs": "https://arxiv.org/abs/2505.09106", "authors": ["Ya Liu", "Kai Yang", "Yu Zhu", "Keying Yang", "Haibo Zhao"], "title": "Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network", "categories": ["cs.LG", "68T07", "I.2"], "comment": "17 pages, 11 figures", "summary": "The space-air-ground integrated network (SAGIN) has recently emerged as a\ncore element in the 6G networks. However, traditional centralized and\nsynchronous optimization algorithms are unsuitable for SAGIN due to\ninfrastructureless and time-varying environments. This paper aims to develop a\nnovel Asynchronous algorithm a.k.a. Argus for tackling non-convex and\nnon-smooth decentralized federated bilevel learning over SAGIN. The proposed\nalgorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle\nbilevel learning problems in time-varying networks asynchronously, thereby\naverting stragglers from impeding the overall training speed. We provide a\ntheoretical analysis of the iteration complexity, communication complexity, and\ncomputational complexity of Argus. Its effectiveness is further demonstrated\nthrough numerical experiments.", "AI": {"tldr": "In this paper, a new asynchronous algorithm named Argus is developed for non-convex and non-smooth decentralized federated bilevel learning in SAGIN. Argus allows networked agents to tackle bilevel learning problems asynchronously and avoids the problem of stragglers. Theoretical analysis and numerical experiments demonstrate its effectiveness.", "motivation": "Traditional centralized and synchronous optimization algorithms are not suitable for space-air-ground integrated networks (SAGIN) due to infrastructureless and time-varying environments.", "method": "The paper proposes an asynchronous algorithm called Argus to solve non-convex and non-smooth decentralized federated bilevel learning over SAGIN. This algorithm enables networked agents to handle bilevel learning problems asynchronously in time-varying networks.", "result": "The theoretical analysis of iteration complexity, communication complexity, and computational complexity of Argus is provided. Numerical experiments further confirm the effectiveness of the proposed algorithm.", "conclusion": "Argus is an effective asynchronous algorithm for addressing non-convex and non-smooth decentralized federated bilevel learning in SAGIN, allowing networked agents to deal with bilevel learning problems without being hindered by stragglers."}}
{"id": "2505.09139", "pdf": "https://arxiv.org/pdf/2505.09139", "abs": "https://arxiv.org/abs/2505.09139", "authors": ["Lucas Choi", "Ross Greer"], "title": "Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) offer flexible object detection through natural\nlanguage prompts but suffer from performance variability depending on prompt\nphrasing. In this paper, we introduce a method for automated prompt refinement\nusing a novel metric called the Contrastive Class Alignment Score (CCAS), which\nranks prompts based on their semantic alignment with a target object class\nwhile penalizing similarity to confounding classes. Our method generates\ndiverse prompt candidates via a large language model and filters them through\nCCAS, computed using prompt embeddings from a sentence transformer. We evaluate\nour approach on challenging object categories, demonstrating that our automatic\nselection of high-precision prompts improves object detection accuracy without\nthe need for additional model training or labeled data. This scalable and\nmodel-agnostic pipeline offers a principled alternative to manual prompt\nengineering for VLM-based detection systems.", "AI": {"tldr": "This paper presents a method for automated prompt refinement in vision-language models (VLMs) using the Contrastive Class Alignment Score (CCAS). It improves object detection accuracy by selecting high-precision prompts without extra model training or labeled data.", "motivation": "Vision-language models (VLMs) provide flexible object detection through natural language prompts but have performance variability depending on prompt phrasing.", "method": "The method generates diverse prompt candidates via a large language model and filters them through CCAS, which is computed using prompt embeddings from a sentence transformer. CCAS ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes.", "result": "The approach improves object detection accuracy when evaluated on challenging object categories.", "conclusion": "The scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems."}}
{"id": "2505.09083", "pdf": "https://arxiv.org/pdf/2505.09083", "abs": "https://arxiv.org/abs/2505.09083", "authors": ["Dominic Zaun Eu Jones"], "title": "Ornithologist: Towards Trustworthy \"Reasoning\" about Central Bank Communications", "categories": ["econ.GN", "cs.CL", "q-fin.EC", "J.4; I.2.7"], "comment": "16 pages, 6 figures", "summary": "I develop Ornithologist, a weakly-supervised textual classification system\nand measure the hawkishness and dovishness of central bank text. Ornithologist\nuses ``taxonomy-guided reasoning'', guiding a large language model with\nhuman-authored decision trees. This increases the transparency and\nexplainability of the system and makes it accessible to non-experts. It also\nreduces hallucination risk. Since it requires less supervision than traditional\nclassification systems, it can more easily be applied to other problems or\nsources of text (e.g. news) without much modification. Ornithologist\nmeasurements of hawkishness and dovishness of RBA communication carry\ninformation about the future of the cash rate path and of market expectations.", "AI": {"tldr": "An abstract about developing a weakly-supervised textual classification system called Ornithologist that measures the hawkishness and dovishness of central bank text, using taxonomy-guided reasoning to guide a large language model with human-authored decision trees.", "motivation": "To create a more transparent, explainable, and accessible system for measuring the hawkishness and dovishness of central bank text that requires less supervision than traditional classification systems and can be applied to other problems or sources of text (e.g. news) without much modification.", "method": "Developing Ornithologist, a weakly-supervised textual classification system that uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees.", "result": "Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.", "conclusion": "This system is not only more transparent and explainable but also reduces hallucination risk and can be easily applied to other problems or sources of text."}}
{"id": "2505.08810", "pdf": "https://arxiv.org/pdf/2505.08810", "abs": "https://arxiv.org/abs/2505.08810", "authors": ["Bappa Muktar", "Vincent Fono", "Adama Nouboukpo"], "title": "Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent\nTransportation Systems (ITS), particularly in enabling real-time communication\nfor emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,\nwhich interfere with safety-critical communication channels, can severely\nimpair their reliability. This study introduces a robust and scalable framework\nto detect DDoS attacks in highway-based VANET environments. A synthetic dataset\nwas constructed using Network Simulator 3 (NS-3) in conjunction with the\nSimulation of Urban Mobility (SUMO) and further enriched with real-world\nmobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).\nThree traffic categories were simulated: DDoS, VoIP, and TCP-based video\nstreaming (VideoTCP). The data preprocessing pipeline included normalization,\nsignal-to-noise ratio (SNR) feature engineering, missing value imputation, and\nclass balancing using the Synthetic Minority Over-sampling Technique (SMOTE).\nFeature importance was assessed using SHapley Additive exPlanations (SHAP).\nEleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),\nAdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).\nXGB and CB achieved the best performance, each attaining an F1-score of 96%.\nThese results highlight the robustness of the proposed framework and its\npotential for real-time deployment in VANETs to secure critical emergency\ncommunications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u9ad8\u901f\u516c\u8defVANET\u73af\u5883\u4e2d\u7684DDoS\u653b\u51fb\uff0c\u4f7f\u7528XGBoost\u548cCatBoost\u5b9e\u73b0\u4e8696%\u7684F1\u5206\u6570\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4fdd\u969c\u7d27\u6025\u901a\u4fe1\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u8f66\u8f86\u81ea\u7ec4\u7ec7\u7f51\u7edc\uff08VANET\uff09\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u4e3a\u5e94\u6025\u8f66\u8f86\u63d0\u4f9b\u5b9e\u65f6\u901a\u4fe1\u65b9\u9762\u3002\u7136\u800c\uff0c\u5206\u5e03\u5f0f\u62d2\u7edd\u670d\u52a1\uff08DDoS\uff09\u653b\u51fb\u4f1a\u5e72\u6270\u5b89\u5168\u5173\u952e\u7684\u901a\u4fe1\u4fe1\u9053\uff0c\u4e25\u91cd\u5f71\u54cd\u5176\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u8fd9\u4e9b\u653b\u51fb\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u7528NS-3\u7f51\u7edc\u6a21\u62df\u5668\u4e0e\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\uff08SUMO\uff09\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7OpenStreetMap\u63d0\u53d6\u4e86\u5fb7\u56fdA81\u9ad8\u901f\u516c\u8def\u7684\u5b9e\u9645\u79fb\u52a8\u8f68\u8ff9\u3002\u6a21\u62df\u4e86\u4e09\u79cd\u6d41\u91cf\u7c7b\u522b\uff1aDDoS\u3001VoIP\u548c\u57fa\u4e8eTCP\u7684\u89c6\u9891\u6d41\uff08VideoTCP\uff09\u3002\u6570\u636e\u9884\u5904\u7406\u7ba1\u9053\u5305\u62ec\u5f52\u4e00\u5316\u3001\u4fe1\u566a\u6bd4\uff08SNR\uff09\u7279\u5f81\u5de5\u7a0b\u3001\u7f3a\u5931\u503c\u586b\u8865\u4ee5\u53ca\u4f7f\u7528\u5408\u6210\u5c11\u6570\u7c7b\u8fc7\u91c7\u6837\u6280\u672f\uff08SMOTE\uff09\u8fdb\u884c\u7c7b\u522b\u5e73\u8861\u3002\u4f7f\u7528SHapley Additive exPlanations (SHAP)\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\u3002\u57fa\u51c6\u6d4b\u8bd5\u4e8611\u4e2a\u5206\u7c7b\u5668\uff0c\u5176\u4e2d\u5305\u62ecXGBoost\u3001CatBoost\u3001AdaBoost\u3001GradientBoosting\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u3002", "result": "XGBoost\u548cCatBoost\u8868\u73b0\u6700\u4f73\uff0c\u5206\u522b\u8fbe\u5230\u4e8696%\u7684F1\u5206\u6570\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5176\u5728VANET\u4e2d\u5b9e\u65f6\u90e8\u7f72\u7684\u9c81\u68d2\u6027\u548c\u6f5c\u529b\uff0c\u4ee5\u786e\u4fdd\u5173\u952e\u7684\u5e94\u6025\u901a\u4fe1\u3002"}}
{"id": "2505.09113", "pdf": "https://arxiv.org/pdf/2505.09113", "abs": "https://arxiv.org/abs/2505.09113", "authors": ["Yingrong Wang", "Anpeng Wu", "Baohong Li", "Ziyang Xiao", "Ruoxuan Xiong", "Qing Han", "Kun Kuang"], "title": "Sequential Treatment Effect Estimation with Unmeasured Confounders", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "This paper studies the cumulative causal effects of sequential treatments in\nthe presence of unmeasured confounders. It is a critical issue in sequential\ndecision-making scenarios where treatment decisions and outcomes dynamically\nevolve over time. Advanced causal methods apply transformer as a backbone to\nmodel such time sequences, which shows superiority in capturing long time\ndependence and periodic patterns via attention mechanism. However, even they\ncontrol the observed confounding, these estimators still suffer from unmeasured\nconfounders, which influence both treatment assignments and outcomes. How to\nadjust the latent confounding bias in sequential treatment effect estimation\nremains an open challenge. Therefore, we propose a novel Decomposing Sequential\nInstrumental Variable framework for CounterFactual Regression (DSIV-CFR),\nrelying on a common negative control assumption. Specifically, an instrumental\nvariable (IV) is a special negative control exposure, while the previous\noutcome serves as a negative control outcome. This allows us to recover the IVs\nlatent in observation variables and estimate sequential treatment effects via a\ngeneralized moment condition. We conducted experiments on 4 datasets and\nachieved significant performance in one- and multi-step prediction, supported\nby which we can identify optimal treatments for dynamic systems.", "AI": {"tldr": "This paper proposes DSIV-CFR framework to address the challenge of unmeasured confounders in estimating cumulative causal effects of sequential treatments.", "motivation": "The motivation is to solve the problem of latent confounding bias in sequential treatment effect estimation, which cannot be addressed by existing advanced causal methods even though they use transformers to model time sequences.", "method": "The method proposed is Decomposing Sequential Instrumental Variable framework for CounterFactual Regression (DSIV-CFR), which relies on a common negative control assumption. It utilizes an instrumental variable as a special negative control exposure and previous outcome as a negative control outcome to recover latent IVs in observation variables.", "result": "Experiments on 4 datasets showed significant performance improvement in one- and multi-step prediction.", "conclusion": "The DSIV-CFR framework can effectively adjust latent confounding bias and identify optimal treatments for dynamic systems."}}
{"id": "2505.09140", "pdf": "https://arxiv.org/pdf/2505.09140", "abs": "https://arxiv.org/abs/2505.09140", "authors": ["Zechao Guan", "Feng Yan", "Shuai Du", "Lin Ma", "Qingshan Liu"], "title": "TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Diffusion Transformer (DiT) models have significantly\nimproved 3D point cloud generation. However, existing methods primarily focus\non local feature extraction while overlooking global topological information,\nsuch as voids, which are crucial for maintaining shape consistency and\ncapturing complex geometries. To address this limitation, we propose\nTopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure\nfor 3D point cloud generation. Specifically, we design the bottleneck structure\nutilizing Perceiver Resampler, which not only offers a mode to integrate\ntopological information extracted through persistent homology into feature\nlearning, but also adaptively filters out redundant local features to improve\ntraining efficiency. Experimental results demonstrate that TopoDiT-3D\noutperforms state-of-the-art models in visual quality, diversity, and training\nefficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich\ntopological information for 3D point cloud generation and its synergy with\nconventional local feature learning. Videos and code are available at\nhttps://github.com/Zechao-Guan/TopoDiT-3D.", "AI": {"tldr": "A new model TopoDiT-3D is proposed, which integrates topological information for better 3D point cloud generation.", "motivation": "Existing methods focus on local feature extraction but ignore global topological information, such as voids, which are essential for shape consistency and complex geometries.", "method": "The method involves designing a bottleneck structure using Perceiver Resampler that incorporates topological information from persistent homology into feature learning and filters out redundant local features.", "result": "TopoDiT-3D surpasses state-of-the-art models in visual quality, diversity, and training efficiency.", "conclusion": "TopoDiT-3D highlights the significance of rich topological information in 3D point cloud generation and its cooperation with traditional local feature learning."}}
{"id": "2505.09246", "pdf": "https://arxiv.org/pdf/2505.09246", "abs": "https://arxiv.org/abs/2505.09246", "authors": ["Derian Boer", "Stephen Roth", "Stefan Kramer"], "title": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .", "AI": {"tldr": "FocusedRetriever\u662f\u4e00\u4e2a\u57fa\u4e8e\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff08SKB\uff09\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8df3\u95ee\u7b54\u3002\u5b83\u6574\u5408\u4e86\u591a\u79cd\u7ec4\u4ef6\uff0c\u5728STaRK\u57fa\u51c6\u6d4b\u8bd5\u7684\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u5e73\u5747\u9996\u6b21\u547d\u4e2d\u7387\u9ad8\u51fa25.7%\u3002", "motivation": "\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u4ea4\u4e92\u7cfb\u7edf\u53ef\u4ee5\u8bbf\u95ee\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5982\u77e5\u8bc6\u56fe\u8c31\u6216\u8868\u683c\uff09\u548c\u975e\u7ed3\u6784\u5316\u5185\u5bb9\uff08\u5982\u81ea\u7136\u8bed\u8a00\u6587\u6863\uff09\uff0c\u4f46\u5927\u591a\u6570\u7cfb\u7edf\u53ea\u80fd\u4f9d\u8d56\u5176\u4e2d\u4e00\u79cd\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u5229\u7528\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u7684\u65b9\u6cd5\uff0c\u4ee5\u94fe\u63a5\u975e\u7ed3\u6784\u5316\u5185\u5bb9\u4e0e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8bbf\u95ee\u548c\u4f7f\u7528\u3002", "method": "FocusedRetriever\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u591a\u8df3\u95ee\u7b54\uff1a1) \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u7cfb\u4e8b\u5b9e\u548c\u5b9e\u4f53\u5c5e\u6027\uff1b2) \u4f7f\u7528\u8282\u70b9\u96c6\u8fde\u63a5\u8fc7\u6ee4\u7b54\u6848\u5019\u9009\u8005\uff1b3) \u91c7\u7528\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u68c0\u7d22\u548c\u6392\u540d\u76f8\u5173\u975e\u7ed3\u6784\u5316\u5185\u5bb9\uff1b4) \u6700\u540e\u518d\u6b21\u5229\u7528LLM\u7684\u60c5\u5883\u80fd\u529b\u5bf9\u524dk\u4e2a\u7b54\u6848\u8fdb\u884c\u6392\u540d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFocusedRetriever\u5728STaRK\u57fa\u51c6\u6d4b\u8bd5\u7684\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u9996\u6b21\u547d\u4e2d\u7387\u6bd4\u7b2c\u4e8c\u4f73\u65b9\u6cd5\u9ad8\u51fa25.7%\u3002\u6b64\u5916\uff0c\u4e2d\u95f4\u7ed3\u679c\u5206\u6790\u63ed\u793a\u4e86\u901a\u8fc7\u5fae\u8c03\u7b49\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u7684\u673a\u4f1a\u3002", "conclusion": "FocusedRetriever\u5c55\u793a\u4e86\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u4f8b\u5982\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u589e\u5f3a\u5176\u6027\u80fd\u3002"}}
{"id": "2505.09131", "pdf": "https://arxiv.org/pdf/2505.09131", "abs": "https://arxiv.org/abs/2505.09131", "authors": ["Kunwoong Kim", "Jihu Lee", "Sangchul Park", "Yongdai Kim"], "title": "Fair Clustering via Alignment", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025. This is the version submitted for review and\n  will be replaced by the camera-ready version soon", "summary": "Algorithmic fairness in clustering aims to balance the proportions of\ninstances assigned to each cluster with respect to a given sensitive attribute.\nWhile recently developed fair clustering algorithms optimize clustering\nobjectives under specific fairness constraints, their inherent complexity or\napproximation often results in suboptimal clustering utility or numerical\ninstability in practice. To resolve these limitations, we propose a new fair\nclustering algorithm based on a novel decomposition of the fair K-means\nclustering objective function. The proposed algorithm, called Fair Clustering\nvia Alignment (FCA), operates by alternately (i) finding a joint probability\ndistribution to align the data from different protected groups, and (ii)\noptimizing cluster centers in the aligned space. A key advantage of FCA is that\nit theoretically guarantees approximately optimal clustering utility for any\ngiven fairness level without complex constraints, thereby enabling high-utility\nfair clustering in practice. Experiments show that FCA outperforms existing\nmethods by (i) attaining a superior trade-off between fairness level and\nclustering utility, and (ii) achieving near-perfect fairness without numerical\ninstability.", "AI": {"tldr": "A new fair clustering algorithm, Fair Clustering via Alignment (FCA), is proposed to balance proportions of instances assigned to each cluster with respect to a given sensitive attribute. It guarantees approximately optimal clustering utility for any given fairness level without complex constraints.", "motivation": "Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice.", "method": "The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space.", "result": "Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.", "conclusion": "FCA theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice."}}
{"id": "2505.09155", "pdf": "https://arxiv.org/pdf/2505.09155", "abs": "https://arxiv.org/abs/2505.09155", "authors": ["Yichen Shi", "Zhuofu Tao", "Yuhao Gao", "Li Huang", "Hongyang Wang", "Zhiping Yu", "Ting-Jung Lin", "Lei He"], "title": "AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection", "categories": ["cs.CV"], "comment": "accepted by LAD25", "summary": "Current multimodal large language models (MLLMs) struggle to understand\ncircuit schematics due to their limited recognition capabilities. This could be\nattributed to the lack of high-quality schematic-netlist training data.\nExisting work such as AMSnet applies schematic parsing to generate netlists.\nHowever, these methods rely on hard-coded heuristics and are difficult to apply\nto complex or noisy schematics in this paper. We therefore propose a novel net\ndetection mechanism based on segmentation with high robustness. The proposed\nmethod also recovers positional information, allowing digital reconstruction of\nschematics. We then expand AMSnet dataset with schematic images from various\nsources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with\nschematic images, Spectre-formatted netlists, OpenAccess digital schematics,\nand positional information for circuit components and nets, whereas AMSnet only\nincludes 792 circuits with SPICE netlists but no digital schematics.", "AI": {"tldr": "Current MLLMs have difficulty understanding circuit schematics due to limited recognition capabilities. This is partly because of the lack of high-quality schematic-netlist training data. The proposed method introduces a novel net detection mechanism based on segmentation with high robustness, which recovers positional information and allows digital reconstruction of schematics. Additionally, AMSnet dataset is expanded to create AMSnet 2.0.", "motivation": "To address the limitations of current MLLMs in understanding circuit schematics and overcome the challenges posed by insufficient high-quality schematic-netlist training data.", "method": "The proposed method employs a novel net detection mechanism based on segmentation with high robustness. It also recovers positional information for circuit components and nets, enabling digital reconstruction of schematics.", "result": "The expansion of AMSnet leads to AMSnet 2.0, which contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information. In contrast, AMSnet only includes 792 circuits with SPICE netlists but no digital schematics.", "conclusion": "The novel net detection mechanism and the creation of AMSnet 2.0 aim to improve the ability of MLLMs to understand circuit schematics."}}
{"id": "2505.09436", "pdf": "https://arxiv.org/pdf/2505.09436", "abs": "https://arxiv.org/abs/2505.09436", "authors": ["Raghav Garg", "Kapil Sharma", "Karan Gupta"], "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.", "AI": {"tldr": "Large Language Models (LLMs) have great potential in Customer Experience Management (CXM), but their evaluation is restricted by data scarcity and benchmark limitations. This paper introduces CXMArena, a new large-scale synthetic benchmark dataset for evaluating AI in operational CXM contexts.", "motivation": "To address the challenges of evaluating LLMs' practical utility in complex operational environments due to data scarcity and inadequate benchmarks.", "method": "Developed a scalable LLM-powered pipeline that simulates brand's CXM entities forming the foundation of datasets including knowledge articles, product specifications, issue taxonomies, and contact center conversations with controlled noise injection and automated validation. Introduced CXMArena which provides benchmarks targeting five important operational tasks.", "result": "Baseline experiments showed the difficulty of the benchmark: state-of-the-art models achieved only 68% accuracy on article search and a low F1 score of 0.3 for knowledge base refinement.", "conclusion": "Current models face significant challenges necessitating complex pipelines and solutions over conventional techniques."}}
{"id": "2505.08818", "pdf": "https://arxiv.org/pdf/2505.08818", "abs": "https://arxiv.org/abs/2505.08818", "authors": ["Amara Tariq", "Rimita Lahiri", "Charles Kahn", "Imon Banerjee"], "title": "Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "15 pages, 2, tables, 3 figures", "summary": "The intricate and multifaceted nature of vision language model (VLM)\ndevelopment, adaptation, and application necessitates the establishment of\nclear and standardized reporting protocols, particularly within the high-stakes\ncontext of healthcare. Defining these reporting standards is inherently\nchallenging due to the diverse nature of studies involving VLMs, which vary\nsignificantly from the development of all new VLMs or finetuning for domain\nalignment to off-the-shelf use of VLM for targeted diagnosis and prediction\ntasks. In this position paper, we argue that traditional machine learning\nreporting standards and evaluation guidelines must be restructured to\naccommodate multiphase VLM studies; it also has to be organized for intuitive\nunderstanding of developers while maintaining rigorous standards for\nreproducibility. To facilitate community adoption, we propose a categorization\nframework for VLM studies and outline corresponding reporting standards that\ncomprehensively address performance evaluation, data reporting protocols, and\nrecommendations for manuscript composition. These guidelines are organized\naccording to the proposed categorization scheme. Lastly, we present a checklist\nthat consolidates reporting standards, offering a standardized tool to ensure\nconsistency and quality in the publication of VLM-related research.", "AI": {"tldr": "\u4e3a\u4e86\u9002\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u533b\u7597\u4fdd\u5065\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5f00\u53d1\u3001\u9002\u914d\u548c\u5e94\u7528\uff0c\u672c\u6587\u63d0\u51fa\u9700\u8981\u5efa\u7acb\u6e05\u6670\u548c\u6807\u51c6\u5316\u7684\u62a5\u544a\u534f\u8bae\u3002\u6587\u7ae0\u5efa\u8bae\u91cd\u65b0\u6784\u5efa\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u62a5\u544a\u6807\u51c6\u548c\u8bc4\u4f30\u6307\u5357\u4ee5\u9002\u5e94\u591a\u9636\u6bb5\u7684VLM\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\u53ca\u76f8\u5e94\u7684\u62a5\u544a\u6807\u51c6\uff0c\u6700\u540e\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u68c0\u67e5\u5217\u8868\u4ee5\u786e\u4fddVLM\u76f8\u5173\u7814\u7a76\u53d1\u8868\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u5177\u6709\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u4ece\u65b0\u6a21\u578b\u7684\u5f00\u53d1\u5230\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\uff0c\u518d\u5230\u76f4\u63a5\u5e94\u7528\u4e8e\u8bca\u65ad\u548c\u9884\u6d4b\u4efb\u52a1\uff0c\u8fd9\u4e9b\u5dee\u5f02\u4f7f\u5f97\u5236\u5b9a\u7edf\u4e00\u7684\u62a5\u544a\u6807\u51c6\u53d8\u5f97\u56f0\u96be\u3002\u7136\u800c\uff0c\u5728\u5982\u533b\u7597\u4fdd\u5065\u8fd9\u6837\u7684\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u7f3a\u4e4f\u660e\u786e\u548c\u6807\u51c6\u5316\u7684\u62a5\u544a\u534f\u8bae\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u540e\u679c\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u4e3aVLM\u7814\u7a76\u5236\u5b9a\u4e13\u95e8\u7684\u62a5\u544a\u6807\u51c6\u548c\u8bc4\u4f30\u6307\u5357\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8ba8\u8bba\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u62a5\u544a\u6807\u51c6\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u9700\u8981\u6839\u636eVLM\u7814\u7a76\u7684\u7279\u70b9\u8fdb\u884c\u8c03\u6574\u3002\u63a5\u7740\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9VLM\u7814\u7a76\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6db5\u76d6\u4e86\u4ece\u6a21\u578b\u5f00\u53d1\u5230\u5b9e\u9645\u5e94\u7528\u7684\u4e0d\u540c\u9636\u6bb5\u3002\u7136\u540e\uff0c\u6839\u636e\u8fd9\u4e00\u5206\u7c7b\u6846\u67b6\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5404\u4e2a\u9636\u6bb5\u7684\u62a5\u544a\u6807\u51c6\uff0c\u5305\u62ec\u6027\u80fd\u8bc4\u4f30\u3001\u6570\u636e\u62a5\u544a\u534f\u8bae\u4ee5\u53ca\u8bba\u6587\u64b0\u5199\u5efa\u8bae\u3002\u6700\u540e\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u68c0\u67e5\u5217\u8868\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u786e\u4fdd\u5176\u7814\u7a76\u7b26\u5408\u6240\u63d0\u51fa\u7684\u62a5\u544a\u6807\u51c6\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u5206\u7c7b\u6846\u67b6\u548c\u5177\u4f53\u7684\u62a5\u544a\u6807\u51c6\uff0c\u672c\u6587\u4e3aVLM\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6307\u5bfc\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7814\u7a76\u7684\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002\u6b64\u5916\uff0c\u6240\u63d0\u4f9b\u7684\u68c0\u67e5\u5217\u8868\u53ef\u4ee5\u4f5c\u4e3a\u5de5\u5177\uff0c\u4fc3\u8fdb\u793e\u533a\u5bf9\u8fd9\u4e9b\u6807\u51c6\u7684\u91c7\u7eb3\u548c\u4f7f\u7528\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u7684\u62a5\u544a\u6807\u51c6\u6765\u9002\u5e94\u5176\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u3002\u672c\u6587\u63d0\u51fa\u7684\u5206\u7c7b\u6846\u67b6\u548c\u62a5\u544a\u6807\u51c6\u4e3a\u672a\u6765VLM\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u5411\uff0c\u540c\u65f6\uff0c\u901a\u8fc7\u63d0\u4f9b\u7684\u68c0\u67e5\u5217\u8868\uff0c\u4fc3\u8fdb\u4e86\u8fd9\u4e9b\u6807\u51c6\u5728\u7814\u7a76\u793e\u533a\u4e2d\u7684\u5e94\u7528\u3002\u8fd9\u5c06\u6709\u52a9\u4e8e\u63d0\u5347VLM\u7814\u7a76\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u4fdd\u5065\u7b49\u5173\u952e\u9886\u57df\u3002"}}
{"id": "2505.09134", "pdf": "https://arxiv.org/pdf/2505.09134", "abs": "https://arxiv.org/abs/2505.09134", "authors": ["Daniel Huang"], "title": "Scaling Gaussian Process Regression with Full Derivative Observations", "categories": ["cs.LG", "stat.ML"], "comment": "12 pages", "summary": "We present a scalable Gaussian Process (GP) method that can fit and predict\nfull derivative observations called DSoftKI. It extends SoftKI, a method that\napproximates a kernel via softmax interpolation from learned interpolation\npoint locations, to the setting with derivatives. DSoftKI enhances SoftKI's\ninterpolation scheme to incorporate the directional orientation of\ninterpolation points relative to the data. This enables the construction of a\nscalable approximate kernel, including its first and second-order derivatives,\nthrough interpolation. We evaluate DSoftKI on a synthetic function benchmark\nand high-dimensional molecular force field prediction (100-1000 dimensions),\ndemonstrating that DSoftKI is accurate and can scale to larger datasets with\nfull derivative observations than previously possible.", "AI": {"tldr": "The paper introduces DSoftKI, a scalable GP method for fitting and predicting full derivative observations. It enhances SoftKI's interpolation scheme to incorporate directional orientation, enabling accurate high-dimensional predictions with larger datasets.", "motivation": "To develop a scalable Gaussian Process method capable of handling full derivative observations in high-dimensional settings.", "method": "DSoftKI extends SoftKI by enhancing its interpolation scheme to include the directional orientation of interpolation points relative to the data, allowing construction of a scalable approximate kernel with first and second-order derivatives through interpolation.", "result": "DSoftKI shows accuracy in synthetic function benchmarks and high-dimensional molecular force field prediction (100-1000 dimensions), scaling to larger datasets with full derivative observations than previously possible.", "conclusion": "DSoftKI is an effective and scalable method for making accurate predictions with full derivative observations in high-dimensional spaces."}}
{"id": "2505.09168", "pdf": "https://arxiv.org/pdf/2505.09168", "abs": "https://arxiv.org/abs/2505.09168", "authors": ["Jianlin Sun", "Xiaolin Fang", "Juwei Guan", "Dongdong Gui", "Teqi Wang", "Tongxin Zhu"], "title": "DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The core challenge in Camouflage Object Detection (COD) lies in the\nindistinguishable similarity between targets and backgrounds in terms of color,\ntexture, and shape. This causes existing methods to either lose edge details\n(such as hair-like fine structures) due to over-reliance on global semantic\ninformation or be disturbed by similar backgrounds (such as vegetation\npatterns) when relying solely on local features. We propose DRRNet, a\nfour-stage architecture characterized by a \"context-detail-fusion-refinement\"\npipeline to address these issues. Specifically, we introduce an Omni-Context\nFeature Extraction Module to capture global camouflage patterns and a Local\nDetail Extraction Module to supplement microstructural information for the\nfull-scene context module. We then design a module for forming dual\nrepresentations of scene understanding and structural awareness, which fuses\npanoramic features and local features across various scales. In the decoder, we\nalso introduce a reverse refinement module that leverages spatial edge priors\nand frequency-domain noise suppression to perform a two-stage inverse\nrefinement of the output. By applying two successive rounds of inverse\nrefinement, the model effectively suppresses background interference and\nenhances the continuity of object boundaries. Experimental results demonstrate\nthat DRRNet significantly outperforms state-of-the-art methods on benchmark\ndatasets. Our code is available at https://github.com/jerrySunning/DRRNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRRNet\u7684\u56db\u9636\u6bb5\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u5c40\u4e0a\u4e0b\u6587\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3001\u5c40\u90e8\u7ec6\u8282\u63d0\u53d6\u6a21\u5757\u4ee5\u53ca\u53cd\u5411\u7cbe\u70bc\u6a21\u5757\u7b49\u8bbe\u8ba1\uff0c\u5728\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u4e0e\u80cc\u666f\u989c\u8272\u3001\u7eb9\u7406\u548c\u5f62\u72b6\u76f8\u4f3c\u7684\u76ee\u6807\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5bb9\u6613\u4e22\u5931\u8fb9\u7f18\u7ec6\u8282\u6216\u53d7\u7c7b\u4f3c\u80cc\u666f\u5e72\u6270\u3002", "method": "\u8bbe\u8ba1\u4e86DRRNet\uff0c\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a\u5168\u573a\u666f\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u83b7\u53d6\u5168\u5c40\u4f2a\u88c5\u6a21\u5f0f\uff1b\u5c40\u90e8\u7ec6\u8282\u63d0\u53d6\u6a21\u5757\u8865\u5145\u5fae\u89c2\u7ed3\u6784\u4fe1\u606f\uff1b\u53cc\u8868\u793a\u751f\u6210\u6a21\u5757\u878d\u5408\u5168\u666f\u548c\u5c40\u90e8\u7279\u5f81\uff1b\u89e3\u7801\u5668\u4e2d\u7684\u53cd\u5411\u7cbe\u70bc\u6a21\u5757\u8fdb\u884c\u4e24\u9636\u6bb5\u9006\u5411\u7cbe\u70bc\u4ee5\u589e\u5f3a\u76ee\u6807\u8fb9\u754c\u8fde\u7eed\u6027\u548c\u6291\u5236\u80cc\u666f\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDRRNet\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "DRRNet\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.09610", "pdf": "https://arxiv.org/pdf/2505.09610", "abs": "https://arxiv.org/abs/2505.09610", "authors": ["Nicolas Dupuis", "Ravi Nair", "Shyam Ramji", "Sean McClintock", "Nishant Chauhan", "Priyanka Nagpal", "Bart Blaner", "Ken Valk", "Leon Stok", "Ruchir Puri"], "title": "Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.", "AI": {"tldr": "The paper explores the development of a Large Language Model (LLM) specialized for explaining VHDL code, crucial for organizations involved in high-performance processor design. They created specific test sets and performed extended pretraining (EPT) on a base LLM, increasing expert evaluation ratings from 43% to 69%. An LLM-as-a-judge was also developed, leading to new models with an instruction-tuned EPT model expected to reach a rating of 71%, potentially rising to 85% with advanced base models. The conclusion discusses enhancing hardware design LLMs using developments in Generative AI.", "motivation": "To address the lack of attention given to VHDL in the context of Large Language Models (LLMs), despite its industry popularity, and to meet the unique needs of organizations engaged in high-performance processor design.", "method": "Developed test sets specific to their needs and conducted extended pretraining (EPT) on a base LLM. Expert evaluations were used to assess the quality of code explanations produced by the EPT model. Also developed an LLM-as-a-judge to evaluate models similarly to expert evaluators.", "result": "Expert evaluation ratings increased from 43% for the base model to 69% for the EPT model. The LLM-as-a-judge helped derive new models, including an instruction-tuned version of the EPT model with an expected rating of 71%. Experiments suggest that newer base models could push this rating to 85% or higher.", "conclusion": "Discusses further improving the quality of hardware design LLMs using advancements in Generative AI."}}
{"id": "2505.08821", "pdf": "https://arxiv.org/pdf/2505.08821", "abs": "https://arxiv.org/abs/2505.08821", "authors": ["Meryem Altin Karagoz", "Marc D. Breton", "Anas El Fathi"], "title": "A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction", "categories": ["q-bio.QM", "cs.AI", "stat.AP"], "comment": "7 pages, 2 figures, 1 table, 1st IFAC Workshop on Engineering\n  Diabetes Technologies (EDT 2025)", "summary": "Accurate blood glucose prediction can enable novel interventions for type 1\ndiabetes treatment, including personalized insulin and dietary adjustments.\nAlthough recent advances in transformer-based architectures have demonstrated\nthe power of attention mechanisms in complex multivariate time series\nprediction, their potential for blood glucose (BG) prediction remains\nunderexplored. We present a comparative analysis of transformer models for\nmulti-horizon BG prediction, examining forecasts up to 4 hours and input\nhistory up to 1 week. The publicly available DCLP3 dataset (n=112) was split\n(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset\n(n=12) served as an external test set. We trained networks with point-wise,\npatch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal\ndata. For short-term blood glucose prediction, Crossformer, a patch-wise\ntransformer architecture, achieved a superior 30-minute prediction of RMSE\n(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),\nPatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6\nmg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used\ntokenization through patches demonstrated improved accuracy with larger input\nsizes, with the best results obtained with a one-week history. These findings\nhighlight the promise of transformer-based architectures for BG prediction by\ncapturing and leveraging seasonal patterns in multivariate time-series data to\nimprove accuracy.", "AI": {"tldr": "\u51c6\u786e\u7684\u8840\u7cd6\u9884\u6d4b\u5bf91\u578b\u7cd6\u5c3f\u75c5\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86transformer\u6a21\u578b\u5728\u8840\u7cd6\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4f7f\u7528DCLP3\u548cOhioT1DM\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u91c7\u7528patch-wise\u65b9\u6cd5\u7684Crossformer\u548cPatchTST\u6a21\u578b\u5206\u522b\u5728\u77ed\u671f\uff0830\u5206\u949f\uff09\u548c\u4e2d\u957f\u671f\uff081\u5c0f\u65f6\u30012\u5c0f\u65f6\u30014\u5c0f\u65f6\uff09\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5c55\u793a\u4e86transformer\u67b6\u6784\u901a\u8fc7\u6355\u6349\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5b63\u8282\u6027\u6a21\u5f0f\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u7684\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8etransformer\u7684\u67b6\u6784\u5728\u590d\u6742\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u5176\u5728\u8840\u7cd6\u9884\u6d4b\u9886\u57df\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002\u51c6\u786e\u7684\u8840\u7cd6\u9884\u6d4b\u53ef\u4ee5\u4e3a1\u578b\u7cd6\u5c3f\u75c5\u7684\u4e2a\u6027\u5316\u6cbb\u7597\u63d0\u4f9b\u652f\u6301\uff0c\u5305\u62ec\u80f0\u5c9b\u7d20\u548c\u996e\u98df\u8c03\u6574\u7b49\u5e72\u9884\u63aa\u65bd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u516c\u5f00\u7684DCLP3\u6570\u636e\u96c6\uff08n=112\uff09\u630980%-10%-10%\u7684\u6bd4\u4f8b\u5212\u5206\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u5e76\u4f7f\u7528OhioT1DM\u6570\u636e\u96c6\uff08n=12\uff09\u4f5c\u4e3a\u5916\u90e8\u6d4b\u8bd5\u96c6\u3002\u8bad\u7ec3\u4e86\u5177\u6709\u70b9\u5f0f\u3001\u5757\u5f0f\u3001\u5e8f\u5217\u5f0f\u548c\u6df7\u5408\u5d4c\u5165\u65b9\u5f0f\u7684\u7f51\u7edc\u6a21\u578b\uff0c\u5229\u7528\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\uff08CGM\uff09\u3001\u80f0\u5c9b\u7d20\u548c\u9910\u98df\u6570\u636e\u8fdb\u884c\u591a\u6b65\u8840\u7cd6\u9884\u6d4b\u3002", "result": "\u5bf9\u4e8e\u77ed\u671f\uff0830\u5206\u949f\uff09\u8840\u7cd6\u9884\u6d4b\uff0cCrossformer\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u4e3a15.6 mg/dL\u3002\u800c\u5bf9\u4e8e\u8f83\u957f\u671f\u9884\u6d4b\uff081\u5c0f\u65f6\u30012\u5c0f\u65f6\u30014\u5c0f\u65f6\uff09\uff0cPatchTST\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cRMSE\u5206\u522b\u4e3a24.6 mg/dL\u300136.1 mg/dL\u548c46.5 mg/dL\u3002\u603b\u4f53\u800c\u8a00\uff0c\u4f7f\u7528\u5206\u5757\u6807\u8bb0\u5316\u7684\u6a21\u578b\u5728\u66f4\u5927\u7684\u8f93\u5165\u5c3a\u5bf8\u4e0b\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5176\u4e2d\u4e00\u5468\u7684\u5386\u53f2\u6570\u636e\u63d0\u4f9b\u4e86\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "\u57fa\u4e8etransformer\u7684\u67b6\u6784\u5728\u8840\u7cd6\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u6355\u6349\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5b63\u8282\u6027\u6a21\u5f0f\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u4e2a\u6027\u5316\u6cbb\u7597\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2505.09160", "pdf": "https://arxiv.org/pdf/2505.09160", "abs": "https://arxiv.org/abs/2505.09160", "authors": ["Berkay Guler", "Giovanni Geraci", "Hamid Jafarkhani"], "title": "A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Current applications of self-supervised learning to wireless channel\nrepresentation often borrow paradigms developed for text and image processing,\nwithout fully addressing the unique characteristics and constraints of wireless\ncommunications. Aiming to fill this gap, we first propose WiMAE (Wireless\nMasked Autoencoder), a transformer-based encoder-decoder foundation model\npretrained on a realistic open-source multi-antenna wireless channel dataset.\nBuilding upon this foundation, we develop ContraWiMAE, which enhances WiMAE by\nincorporating a contrastive learning objective alongside the reconstruction\ntask in a unified multi-task framework. By warm-starting from pretrained WiMAE\nweights and generating positive pairs via noise injection, the contrastive\ncomponent enables the model to capture both structural and discriminative\nfeatures, enhancing representation quality beyond what reconstruction alone can\nachieve. Through extensive evaluation on unseen scenarios, we demonstrate the\neffectiveness of both approaches across multiple downstream tasks, with\nContraWiMAE showing further improvements in linear separability and\nadaptability in diverse wireless environments. Comparative evaluations against\na state-of-the-art wireless channel foundation model confirm the superior\nperformance and data efficiency of our models, highlighting their potential as\npowerful baselines for future research in self-supervised wireless channel\nrepresentation learning.", "AI": {"tldr": "\u5f53\u524d\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u65e0\u7ebf\u4fe1\u9053\u8868\u793a\u4e2d\u7684\u5e94\u7528\u901a\u5e38\u501f\u7528\u4e3a\u6587\u672c\u548c\u56fe\u50cf\u5904\u7406\u5f00\u53d1\u7684\u8303\u5f0f\uff0c\u800c\u6ca1\u6709\u5145\u5206\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u7684\u72ec\u7279\u7279\u6027\u548c\u7ea6\u675f\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u51fa\u4e86WiMAE\uff08\u65e0\u7ebf\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u57fa\u7840\u6a21\u578b\uff0c\u9884\u8bad\u7ec3\u5728\u4e00\u4e2a\u73b0\u5b9e\u7684\u5f00\u6e90\u591a\u5929\u7ebf\u65e0\u7ebf\u4fe1\u9053\u6570\u636e\u96c6\u4e0a\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86ContraWiMAE\uff0c\u901a\u8fc7\u5728\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u6846\u67b6\u4e2d\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u548c\u91cd\u5efa\u4efb\u52a1\u6765\u589e\u5f3aWiMAE\u3002\u901a\u8fc7\u5bf9\u672a\u89c1\u8fc7\u7684\u573a\u666f\u8fdb\u884c\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0cContraWiMAE\u5728\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u7ebf\u6027\u53ef\u5206\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8fdb\u4e00\u6b65\u7684\u6539\u8fdb\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u65e0\u7ebf\u4fe1\u9053\u57fa\u7840\u6a21\u578b\u7684\u6bd4\u8f83\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6211\u4eec\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff0c\u7a81\u663e\u4e86\u5b83\u4eec\u4f5c\u4e3a\u672a\u6765\u81ea\u76d1\u7763\u65e0\u7ebf\u4fe1\u9053\u8868\u793a\u5b66\u4e60\u7814\u7a76\u7684\u5f3a\u5927\u57fa\u7ebf\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u65e0\u7ebf\u4fe1\u9053\u8868\u793a\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u5e38\u501f\u7528\u6587\u672c\u548c\u56fe\u50cf\u5904\u7406\u7684\u8303\u5f0f\uff0c\u4f46\u672a\u5145\u5206\u8003\u8651\u65e0\u7ebf\u901a\u4fe1\u7684\u72ec\u7279\u7279\u6027\u548c\u7ea6\u675f\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u6ee1\u8db3\u65e0\u7ebf\u901a\u4fe1\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWiMAE\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86ContraWiMAE\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u548c\u91cd\u5efa\u4efb\u52a1\u6765\u589e\u5f3a\u8868\u793a\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86WiMAE\u548cContraWiMAE\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u7ebf\u6027\u53ef\u5206\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "WiMAE\u548cContraWiMAE\u5728\u65e0\u7ebf\u4fe1\u9053\u8868\u793a\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u5f3a\u5927\u57fa\u7ebf\u3002"}}
{"id": "2505.09178", "pdf": "https://arxiv.org/pdf/2505.09178", "abs": "https://arxiv.org/abs/2505.09178", "authors": ["Yitao Zhu", "Yuan Yin", "Zhenrong Shen", "Zihao Zhao", "Haiyu Song", "Sheng Wang", "Dinggang Shen", "Qian Wang"], "title": "UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System", "categories": ["cs.CV"], "comment": "14 pages", "summary": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.", "AI": {"tldr": "The paper introduces UniCAD, a unified architecture that uses pre-trained vision models to efficiently handle 2D and 3D medical images with minimal parameters. It includes low-rank adaptation for efficiency and a plug-and-play modular design for task expansion. Experiments show superior performance and efficiency across 12 datasets.", "motivation": "To address the challenges of developing multi-task CAD systems due to growing complexity and lack of an open-source CAD platform in the medical imaging community.", "method": "UniCAD leverages pre-trained vision models with two key innovations: (1) Efficiency - Low-rank adaptation strategy introducing only 0.17% trainable parameters; (2) Plug-and-Play - Modular architecture combining frozen foundation model with multiple experts for diverse tasks.", "result": "Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency.", "conclusion": "UniCAD establishes an open-source platform for sharing lightweight CAD experts, promoting equitable and efficient research ecosystem."}}
{"id": "2505.09174", "pdf": "https://arxiv.org/pdf/2505.09174", "abs": "https://arxiv.org/abs/2505.09174", "authors": ["Xinyu You", "Xiang Liu", "Chuan-Shen Hu", "Kelin Xia", "Tze Chien Sum"], "title": "Quotient Complex Transformer (QCformer) for Perovskite Data Analysis", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": null, "summary": "The discovery of novel functional materials is crucial in addressing the\nchallenges of sustainable energy generation and climate change. Hybrid\norganic-inorganic perovskites (HOIPs) have gained attention for their\nexceptional optoelectronic properties in photovoltaics. Recently, geometric\ndeep learning, particularly graph neural networks (GNNs), has shown strong\npotential in predicting material properties and guiding material design.\nHowever, traditional GNNs often struggle to capture the periodic structures and\nhigher-order interactions prevalent in such systems. To address these\nlimitations, we propose a novel representation based on quotient complexes\n(QCs) and introduce the Quotient Complex Transformer (QCformer) for material\nproperty prediction. A material structure is modeled as a quotient complex,\nwhich encodes both pairwise and many-body interactions via simplices of varying\ndimensions and captures material periodicity through a quotient operation. Our\nmodel leverages higher-order features defined on simplices and processes them\nusing a simplex-based Transformer module. We pretrain QCformer on benchmark\ndatasets such as the Materials Project and JARVIS, and fine-tune it on HOIP\ndatasets. The results show that QCformer outperforms state-of-the-art models in\nHOIP property prediction, demonstrating its effectiveness. The quotient complex\nrepresentation and QCformer model together contribute a powerful new tool for\npredictive modeling of perovskite materials.", "AI": {"tldr": "The paper proposes Quotient Complex Transformer (QCformer) based on quotient complexes for predicting properties of hybrid organic-inorganic perovskites (HOIPs). It outperforms state-of-the-art models in HOIP property prediction.", "motivation": "Novel functional materials are crucial for sustainable energy and climate change challenges. Hybrid organic-inorganic perovskites (HOIPs) have exceptional optoelectronic properties but traditional graph neural networks (GNNs) fail to capture their periodic structures and higher-order interactions effectively.", "method": "A material structure is modeled as a quotient complex that encodes both pairwise and many-body interactions via simplices of varying dimensions and captures material periodicity through a quotient operation. The model uses a simplex-based Transformer module to leverage higher-order features defined on simplices. QCformer is pretrained on benchmark datasets like Materials Project and JARVIS, then fine-tuned on HOIP datasets.", "result": "QCformer outperforms state-of-the-art models in predicting properties of HOIPs.", "conclusion": "Quotient complex representation and QCformer provide a powerful new tool for predictive modeling of perovskite materials."}}
{"id": "2505.09188", "pdf": "https://arxiv.org/pdf/2505.09188", "abs": "https://arxiv.org/abs/2505.09188", "authors": ["Minjun Kim", "Jaehyeon Choi", "Jongkeun Lee", "Wonjin Cho", "U Kang"], "title": "Zero-shot Quantization: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "IJCAI 2025 Survey Track", "summary": "Network quantization has proven to be a powerful approach to reduce the\nmemory and computational demands of deep learning models for deployment on\nresource-constrained devices. However, traditional quantization methods often\nrely on access to training data, which is impractical in many real-world\nscenarios due to privacy, security, or regulatory constraints. Zero-shot\nQuantization (ZSQ) emerges as a promising solution, achieving quantization\nwithout requiring any real data. In this paper, we provide a comprehensive\noverview of ZSQ methods and their recent advancements. First, we provide a\nformal definition of the ZSQ problem and highlight the key challenges. Then, we\ncategorize the existing ZSQ methods into classes based on data generation\nstrategies, and analyze their motivations, core ideas, and key takeaways.\nLastly, we suggest future research directions to address the remaining\nlimitations and advance the field of ZSQ. To the best of our knowledge, this\npaper is the first in-depth survey on ZSQ.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u96f6\u6837\u672c\u91cf\u5316\uff08ZSQ\uff09\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5b9a\u4e49\u4e86ZSQ\u95ee\u9898\uff0c\u5206\u7c7b\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u7531\u4e8e\u9690\u79c1\u3001\u5b89\u5168\u6216\u6cd5\u89c4\u9650\u5236\u800c\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6570\u636e\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u9996\u5148\u6b63\u5f0f\u5b9a\u4e49\u4e86ZSQ\u95ee\u9898\u5e76\u5f3a\u8c03\u4e86\u5173\u952e\u6311\u6218\uff0c\u7136\u540e\u6839\u636e\u6570\u636e\u751f\u6210\u7b56\u7565\u5c06\u73b0\u6709\u7684ZSQ\u65b9\u6cd5\u5206\u4e3a\u51e0\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u52a8\u673a\u3001\u6838\u5fc3\u601d\u60f3\u548c\u4e3b\u8981\u6536\u83b7\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9ZSQ\u65b9\u6cd5\u53ca\u5176\u6700\u65b0\u8fdb\u5c55\u7684\u5168\u9762\u4e86\u89e3\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u662f\u8fc4\u4eca\u4e3a\u6b62\u9996\u4e2a\u6df1\u5165\u8c03\u67e5ZSQ\u7684\u8bba\u6587\uff0c\u65e8\u5728\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.08825", "pdf": "https://arxiv.org/pdf/2505.08825", "abs": "https://arxiv.org/abs/2505.08825", "authors": ["Pedro Antonio Alarcon Granadeno", "Theodore Chambers", "Jane Cleland-Huang"], "title": "Multi-source Plume Tracing via Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI"], "comment": "13 pages, 7 figures", "summary": "Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon\ngas leak (2015) demonstrate the urgent need for rapid and reliable plume\ntracing algorithms to protect public health and the environment. Traditional\nmethods, such as gradient-based or biologically inspired approaches, often fail\nin realistic, turbulent conditions. To address these challenges, we present a\nMulti-Agent Reinforcement Learning (MARL) algorithm designed for localizing\nmultiple airborne pollution sources using a swarm of small uncrewed aerial\nsystems (sUAS). Our method models the problem as a Partially Observable Markov\nGame (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific\nDouble Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical\naction-observation pairs, effectively approximating latent states. Unlike prior\nwork, we use a general-purpose simulation environment based on the Gaussian\nPlume Model (GPM), incorporating realistic elements such as a three-dimensional\nenvironment, sensor noise, multiple interacting agents, and multiple plume\nsources. The incorporation of action histories as part of the inputs further\nenhances the adaptability of our model in complex, partially observable\nenvironments. Extensive simulations show that our algorithm significantly\noutperforms conventional approaches. Specifically, our model allows agents to\nexplore only 1.29\\% of the environment to successfully locate pollution\nsources.", "AI": {"tldr": "The paper presents a Multi-Agent Reinforcement Learning (MARL) algorithm for localizing multiple airborne pollution sources using small uncrewed aerial systems, significantly outperforming conventional approaches.", "motivation": "Industrial catastrophes demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment.", "method": "The method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN). It uses full sequences of historical action-observation pairs, effectively approximating latent states. A general-purpose simulation environment based on the Gaussian Plume Model (GPM) is used, incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources.", "result": "Extensive simulations show that the algorithm significantly outperforms conventional approaches, with agents exploring only 1.29% of the environment to successfully locate pollution sources.", "conclusion": "The MARL algorithm provides an effective solution for localizing multiple airborne pollution sources in complex, partially observable environments."}}
{"id": "2505.09175", "pdf": "https://arxiv.org/pdf/2505.09175", "abs": "https://arxiv.org/abs/2505.09175", "authors": ["Mohammad Ganjirad", "Mahmoud Reza Delavar", "Hossein Bagheri", "Mohammad Mehdi Azizi"], "title": "Optimizing Urban Critical Green Space Development Using Machine Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper presents a novel framework for prioritizing urban green space\ndevelopment in Tehran using diverse socio-economic, environmental, and\nsensitivity indices. The indices were derived from various sources including\nGoogle Earth Engine, air pollution measurements, municipal reports and the\nWeather Research & Forecasting (WRF) model. The WRF model was used to estimate\nthe air temperature at a 1 km resolution due to insufficient meteorological\nstations, yielding RMSE and MAE values of 0.96{\\deg}C and 0.92{\\deg}C,\nrespectively. After data preparation, several machine learning models were used\nfor binary vegetation cover classification including XGBoost, LightGBM, Random\nForest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%\nin Overall Accuracy, Recall, and F1-score. Then, the probability of areas\nlacking vegetation cover was assessed using socio-economic, environmental and\nsensitivity indices. This resulted in the RF generating an urban green space\ndevelopment prioritization map. Feature Importance Analysis revealed that the\nmost significant indices were nightly land surface temperature (LST) and\nsensitive population. Finally, the framework performance was validated through\nmicroclimate simulation to assess the critical areas after and before the green\nspace development by green roofs. The simulation demonstrated reducing air\ntemperature by up to 0.67{\\deg}C after utilizing the green roof technology in\ncritical areas. As a result, this framework provides a valuable tool for urban\nplanners to develop green spaces.", "AI": {"tldr": "The paper proposes a framework for prioritizing urban green space development in Tehran, using socio-economic, environmental, and sensitivity indices. Machine learning models were used for vegetation cover classification, with Random Forest showing the best performance. The framework was validated through microclimate simulation, demonstrating a reduction in air temperature after implementing green roof technology.", "motivation": "To address the insufficient meteorological stations and provide a valuable tool for urban planners to develop green spaces in urban areas.", "method": "Using diverse socio-economic, environmental, and sensitivity indices derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the WRF model. Several machine learning models (XGBoost, LightGBM, Random Forest, Extra Trees) were employed for binary vegetation cover classification.", "result": "Random Forest achieved the highest performance exceeding 94% in Overall Accuracy, Recall, and F1-score. The framework's performance was validated through microclimate simulation which demonstrated reducing air temperature by up to 0.67\u00b0C after utilizing the green roof technology.", "conclusion": "This framework provides a valuable tool for urban planners to prioritize and develop green spaces effectively."}}
{"id": "2505.09196", "pdf": "https://arxiv.org/pdf/2505.09196", "abs": "https://arxiv.org/abs/2505.09196", "authors": ["Tong Li", "Lizhi Wang", "Hansen Feng", "Lin Zhu", "Hua Huang"], "title": "PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement", "categories": ["cs.CV"], "comment": "11 pages, 9 tables, 9 figures", "summary": "Low-light image enhancement (LLIE) is a fundamental task in computational\nphotography, aiming to improve illumination, reduce noise, and enhance image\nquality. While recent advancements focus on designing increasingly complex\nneural network models, we observe a peculiar phenomenon: resetting certain\nparameters to random values unexpectedly improves enhancement performance for\nsome images. Drawing inspiration from biological genes, we term this phenomenon\nthe gene effect. The gene effect limits enhancement performance, as even random\nparameters can sometimes outperform learned ones, preventing models from fully\nutilizing their capacity. In this paper, we investigate the reason and propose\na solution. Based on our observations, we attribute the gene effect to static\nparameters, analogous to how fixed genetic configurations become maladaptive\nwhen environments change. Inspired by biological evolution, where adaptation to\nnew environments relies on gene mutation and recombination, we propose\nparameter dynamic evolution (PDE) to adapt to different images and mitigate the\ngene effect. PDE employs a parameter orthogonal generation technique and the\ncorresponding generated parameters to simulate gene recombination and gene\nmutation, separately. Experiments validate the effectiveness of our techniques.\nThe code will be released to the public.", "AI": {"tldr": "In this paper, the authors explore a phenomenon in low-light image enhancement (LLIE) called the gene effect, where resetting certain parameters to random values can improve enhancement performance. They propose a solution named parameter dynamic evolution (PDE), inspired by biological evolution, which employs techniques analogous to gene mutation and recombination.", "motivation": "The motivation is the observation of a peculiar phenomenon in LLIE models, termed the gene effect, where random parameter resettings can outperform learned parameters for some images, limiting model performance.", "method": "The proposed method, parameter dynamic evolution (PDE), addresses the gene effect by simulating biological evolution processes. It uses a parameter orthogonal generation technique to implement concepts similar to gene recombination and mutation, allowing adaptation to different images.", "result": "Experiments validate the effectiveness of PDE in mitigating the gene effect and improving enhancement performance.", "conclusion": "The authors conclude that PDE successfully adapts to varying images and alleviates the gene effect, with plans to release the code publicly."}}
{"id": "2505.09214", "pdf": "https://arxiv.org/pdf/2505.09214", "abs": "https://arxiv.org/abs/2505.09214", "authors": ["Zhonghao Lyu", "Ming Xiao", "Jie Xu", "Mikael Skoglund", "Marco Di Renzo"], "title": "The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks", "categories": ["cs.LG"], "comment": null, "summary": "The growing demand for large artificial intelligence model (LAIM) services is\ndriving a paradigm shift from traditional cloud-based inference to edge-based\ninference for low-latency, privacy-preserving applications. In particular,\nedge-device co-inference, which partitions LAIMs between edge devices and\nservers, has emerged as a promising strategy for resource-efficient LAIM\nexecution in wireless networks. In this paper, we investigate a pruning-aware\nLAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned\ninto on-device and on-server sub-models for deployment. For analysis, we first\nprove that the LAIM output distortion is upper bounded by its parameter\ndistortion. Then, we derive a lower bound on parameter distortion via\nrate-distortion theory, analytically capturing the relationship between pruning\nratio and co-inference performance. Next, based on the analytical results, we\nformulate an LAIM co-inference distortion bound minimization problem by jointly\noptimizing the pruning ratio, transmit power, and computation frequency under\nsystem latency, energy, and available resource constraints. Moreover, we\npropose an efficient algorithm to tackle the considered highly non-convex\nproblem. Finally, extensive simulations demonstrate the effectiveness of the\nproposed design. In particular, model parameter distortion is shown to provide\na reliable bound on output distortion. Also, the proposed joint pruning ratio\nand resource management design achieves superior performance in balancing\ntrade-offs among inference performance, system latency, and energy consumption\ncompared with benchmark schemes, such as fully on-device and on-server\ninference. Moreover, the split point is shown to play a critical role in system\nperformance optimization under heterogeneous and resource-limited edge\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u526a\u679d\u611f\u77e5\u7684\u5927\u89c4\u6a21\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAIM\uff09\u534f\u540c\u63a8\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u526a\u679d\u6bd4\u4f8b\u3001\u4f20\u8f93\u529f\u7387\u548c\u8ba1\u7b97\u9891\u7387\uff0c\u5728\u7cfb\u7edf\u5ef6\u8fdf\u3001\u80fd\u91cf\u548c\u8d44\u6e90\u7ea6\u675f\u4e0b\uff0c\u6700\u5c0f\u5316\u63a8\u7406\u5931\u771f\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u8bbe\u8ba1\u5728\u63a8\u7406\u6027\u80fd\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u80fd\u8017\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u968f\u7740\u5bf9\u5927\u89c4\u6a21\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u670d\u52a1\u9700\u6c42\u7684\u589e\u957f\uff0c\u4e3a\u4e86\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u4fdd\u62a4\u9690\u79c1\u7684\u5e94\u7528\uff0c\u4ece\u4f20\u7edf\u7684\u57fa\u4e8e\u4e91\u7684\u63a8\u7406\u8f6c\u5411\u57fa\u4e8e\u8fb9\u7f18\u7684\u63a8\u7406\u6210\u4e3a\u8d8b\u52bf\u3002\u7279\u522b\u5730\uff0c\u8fb9\u7f18\u8bbe\u5907\u534f\u540c\u63a8\u7406\u4f5c\u4e3a\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684LAIM\u6267\u884c\u7b56\u7565\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5d2d\u9732\u5934\u89d2\u3002", "method": "\u7814\u7a76\u9996\u5148\u8bc1\u660e\u4e86LAIM\u8f93\u51fa\u5931\u771f\u4e0e\u5176\u53c2\u6570\u5931\u771f\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u7387\u5931\u771f\u7406\u8bba\u63a8\u5bfc\u51fa\u53c2\u6570\u5931\u771f\u7684\u4e0b\u754c\uff0c\u4ece\u800c\u6355\u6349\u526a\u679d\u6bd4\u4f8b\u4e0e\u534f\u540c\u63a8\u7406\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u63a5\u7740\uff0c\u7814\u7a76\u5c06LAIM\u534f\u540c\u63a8\u7406\u5931\u771f\u6700\u5c0f\u5316\u95ee\u9898\u516c\u5f0f\u5316\uff0c\u8054\u5408\u4f18\u5316\u526a\u679d\u6bd4\u4f8b\u3001\u4f20\u8f93\u529f\u7387\u548c\u8ba1\u7b97\u9891\u7387\uff0c\u540c\u65f6\u8003\u8651\u7cfb\u7edf\u5ef6\u8fdf\u3001\u80fd\u91cf\u548c\u53ef\u7528\u8d44\u6e90\u7684\u7ea6\u675f\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u9ad8\u5ea6\u975e\u51f8\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u7684\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\u6a21\u578b\u53c2\u6570\u5931\u771f\u80fd\u591f\u53ef\u9760\u5730\u9650\u5b9a\u8f93\u51fa\u5931\u771f\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u526a\u679d\u6bd4\u4f8b\u548c\u8d44\u6e90\u7ba1\u7406\u8bbe\u8ba1\u76f8\u6bd4\u57fa\u51c6\u65b9\u6848\uff08\u5982\u5b8c\u5168\u57fa\u4e8e\u8bbe\u5907\u6216\u670d\u52a1\u5668\u7684\u63a8\u7406\uff09\uff0c\u5728\u63a8\u7406\u6027\u80fd\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5206\u5272\u70b9\u5728\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u5bf9\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u526a\u679d\u611f\u77e5\u7684LAIM\u534f\u540c\u63a8\u7406\u65b9\u6848\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u63a8\u7406\u5931\u771f\uff0c\u5e76\u5728\u63a8\u7406\u6027\u80fd\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u80fd\u8017\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u7684\u5e73\u8861\u3002\u5206\u5272\u70b9\u7684\u9009\u62e9\u5bf9\u4e8e\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u3002"}}
{"id": "2505.09251", "pdf": "https://arxiv.org/pdf/2505.09251", "abs": "https://arxiv.org/abs/2505.09251", "authors": ["Vineetha Joy", "Aditya Anand", "Nidhi", "Anshuman Kumar", "Amit Sethi", "Hema Singh"], "title": "A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures", "categories": ["cs.CV"], "comment": null, "summary": "Metasurface-based radar absorbing structures (RAS) are highly preferred for\napplications like stealth technology, electromagnetic (EM) shielding, etc. due\nto their capability to achieve frequency selective absorption characteristics\nwith minimal thickness and reduced weight penalty. However, the conventional\napproach for the EM design and optimization of these structures relies on\nforward simulations, using full wave simulation tools, to predict the\nelectromagnetic (EM) response of candidate meta atoms. This process is\ncomputationally intensive, extremely time consuming and requires exploration of\nlarge design spaces. To overcome this challenge, we propose a surrogate model\nthat significantly accelerates the prediction of EM responses of multi-layered\nmetasurface-based RAS. A convolutional neural network (CNN) based architecture\nwith Huber loss function has been employed to estimate the reflection\ncharacteristics of the RAS model. The proposed model achieved a cosine\nsimilarity of 99.9% and a mean square error of 0.001 within 1000 epochs of\ntraining. The efficiency of the model has been established via full wave\nsimulations as well as experiment where it demonstrated significant reduction\nin computational time while maintaining high predictive accuracy.", "AI": {"tldr": "A surrogate model using CNN with Huber loss function is proposed for predicting EM responses of metasurface-based RAS, achieving high accuracy and significant reduction in computational time.", "motivation": "Metasurface-based radar absorbing structures (RAS) are crucial for applications like stealth technology and EM shielding. However, conventional design and optimization methods using full wave simulation tools are computationally intensive, time consuming, and require exploration of large design spaces.", "method": "The authors propose a surrogate model based on convolutional neural network (CNN) with Huber loss function to predict the reflection characteristics of multi-layered metasurface-based RAS. The model is trained within 1000 epochs.", "result": "The proposed model achieved a cosine similarity of 99.9% and a mean square error of 0.001. It significantly reduced computational time while maintaining high predictive accuracy, as demonstrated by full wave simulations and experiments.", "conclusion": "The surrogate model using CNN with Huber loss function successfully accelerates the prediction of EM responses of metasurface-based RAS, offering an efficient alternative to conventional methods."}}
{"id": "2505.09218", "pdf": "https://arxiv.org/pdf/2505.09218", "abs": "https://arxiv.org/abs/2505.09218", "authors": ["Alexander Tyurin", "Danil Sivtsov"], "title": "Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods", "categories": ["cs.LG", "cs.DC", "math.OC"], "comment": null, "summary": "We propose a new unifying framework, Birch SGD, for analyzing and designing\ndistributed SGD methods. The central idea is to represent each method as a\nweighted directed tree, referred to as a computation tree. Leveraging this\nrepresentation, we introduce a general theoretical result that reduces\nconvergence analysis to studying the geometry of these trees. This perspective\nyields a purely graph-based interpretation of optimization dynamics, offering a\nnew and intuitive foundation for method development. Using Birch SGD, we design\neight new methods and analyze them alongside previously known ones, with at\nleast six of the new methods shown to have optimal computational time\ncomplexity. Our research leads to two key insights: (i) all methods share the\nsame \"iteration rate\" of $O\\left(\\frac{(R + 1) L \\Delta}{\\varepsilon} +\n\\frac{\\sigma^2 L \\Delta}{\\varepsilon^2}\\right)$, where $R$ the maximum \"tree\ndistance\" along the main branch of a tree; and (ii) different methods exhibit\ndifferent trade-offs-for example, some update iterates more frequently,\nimproving practical performance, while others are more communication-efficient\nor focus on other aspects. Birch SGD serves as a unifying framework for\nnavigating these trade-offs. We believe these results provide a unified\nfoundation for understanding, analyzing, and designing efficient asynchronous\nand parallel optimization methods.", "AI": {"tldr": "The paper proposes Birch SGD, a unifying framework for analyzing and designing distributed SGD methods using computation trees. It introduces eight new methods with optimal computational time complexity and reveals common iteration rates and trade-offs among methods.", "motivation": "There is a need for a unified approach to analyze and design efficient asynchronous and parallel optimization methods, specifically distributed SGD techniques.", "method": "Birch SGD represents each method as a weighted directed tree (computation tree) and uses this representation to reduce convergence analysis to studying the geometry of these trees. This graph-based interpretation allows for the development of new methods and analysis of existing ones.", "result": "Eight new methods were designed with at least six having optimal computational time complexity. All methods share the same iteration rate, but exhibit different trade-offs in terms of update frequency, communication efficiency, etc.", "conclusion": "Birch SGD provides a unifying framework for understanding, analyzing, and designing efficient distributed SGD methods by navigating trade-offs among different approaches."}}
{"id": "2505.09252", "pdf": "https://arxiv.org/pdf/2505.09252", "abs": "https://arxiv.org/abs/2505.09252", "authors": ["Yinuo Wang", "Yue Zeng", "Kai Chen", "Cai Meng", "Chao Pan", "Zhouping Tang"], "title": "Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping", "categories": ["cs.CV"], "comment": null, "summary": "Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes\non non-contrast computed tomography is critical for prognosis prediction and\ntherapeutic decision-making, yet remains challenging due to low contrast and\nblurring boundaries. This study evaluates the performance of zero-shot\nmulti-modal large language models (MLLMs) compared to traditional deep learning\nmethods in ICH binary classification and subtyping. Methods: We utilized a\ndataset provided by RSNA, comprising 192 NCCT volumes. The study compares\nvarious MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,\nwith conventional deep learning models, including ResNet50 and Vision\nTransformer. Carefully crafted prompts were used to guide MLLMs in tasks such\nas ICH presence, subtype classification, localization, and volume estimation.\nResults: The results indicate that in the ICH binary classification task,\ntraditional deep learning models outperform MLLMs comprehensively. For subtype\nclassification, MLLMs also exhibit inferior performance compared to traditional\ndeep learning models, with Gemini 2.0 Flash achieving an macro-averaged\nprecision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While\nMLLMs excel in interactive capabilities, their overall accuracy in ICH\nsubtyping is inferior to deep networks. However, MLLMs enhance interpretability\nthrough language interactions, indicating potential in medical imaging\nanalysis. Future efforts will focus on model refinement and developing more\nprecise MLLMs to improve performance in three-dimensional medical image\nprocessing.", "AI": {"tldr": "This study compares zero-shot multi-modal large language models (MLLMs) with traditional deep learning methods in identifying intracranial hemorrhage (ICH) subtypes from non-contrast computed tomography scans. Although MLLMs are inferior in accuracy, they offer enhanced interpretability through interactive capabilities.", "motivation": "Accurate and timely identification of ICH subtypes on non-contrast computed tomography is crucial for prognosis prediction and therapeutic decision-making, but it remains challenging due to low contrast and blurring boundaries.", "method": "The study utilized a dataset provided by RSNA, comprising 192 NCCT volumes. It compared various MLLMs (GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2) with conventional deep learning models (ResNet50 and Vision Transformer). Carefully crafted prompts were used to guide MLLMs in tasks such as ICH presence, subtype classification, localization, and volume estimation.", "result": "Traditional deep learning models outperformed MLLMs comprehensively in the ICH binary classification task. For subtype classification, MLLMs also exhibited inferior performance compared to traditional deep learning models.", "conclusion": "MLLMs have lower overall accuracy in ICH subtyping compared to deep networks, but they enhance interpretability through language interactions, showing potential in medical imaging analysis."}}
{"id": "2505.09239", "pdf": "https://arxiv.org/pdf/2505.09239", "abs": "https://arxiv.org/abs/2505.09239", "authors": ["Faruk Alpay"], "title": "Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories", "categories": ["cs.LG", "68T05, 90C25, 94A15", "I.2.6; G.1.6; H.1.1"], "comment": "23 pages, 11 figures, includes analytical proofs, sensitivity\n  analysis (95% CI), and JAX-based open-source implementation available at:\n  https://github.com/farukalpay/information-bottleneck-beta-optimization", "summary": "The Information Bottleneck (IB) method frequently suffers from unstable\noptimization, characterized by abrupt representation shifts near critical\npoints of the IB trade-off parameter, beta. In this paper, I introduce a novel\napproach to achieve stable and convex IB optimization through symbolic\ncontinuation and entropy-regularized trajectories. I analytically prove\nconvexity and uniqueness of the IB solution path when an entropy regularization\nterm is included, and demonstrate how this stabilizes representation learning\nacross a wide range of \\b{eta} values. Additionally, I provide extensive\nsensitivity analyses around critical points (beta) with statistically robust\nuncertainty quantification (95% confidence intervals). The open-source\nimplementation, experimental results, and reproducibility framework included in\nthis work offer a clear path for practical deployment and future extension of\nmy proposed method.", "AI": {"tldr": "The paper presents a novel method for stable and convex Information Bottleneck optimization using symbolic continuation and entropy-regularized trajectories, proving solution path uniqueness and providing sensitivity analyses.", "motivation": "The Information Bottleneck method often experiences unstable optimization with sudden representation shifts near critical points of the trade-off parameter beta.", "method": "A new approach achieving stable and convex IB optimization through symbolic continuation and entropy-regularized trajectories is introduced. Analytical proof of convexity and uniqueness of the IB solution path with an entropy regularization term is provided.", "result": "This method stabilizes representation learning across a wide range of beta values and provides extensive sensitivity analyses around critical points with uncertainty quantification.", "conclusion": "The open-source implementation, experimental results, and reproducibility framework offer a practical path for deploying and extending the proposed method."}}
{"id": "2505.09256", "pdf": "https://arxiv.org/pdf/2505.09256", "abs": "https://arxiv.org/abs/2505.09256", "authors": ["Jaemin Jung", "Youngjoon Jang", "Joon Son Chung"], "title": "Test-Time Augmentation for Pose-invariant Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "The goal of this paper is to enhance face recognition performance by\naugmenting head poses during the testing phase. Existing methods often rely on\ntraining on frontalised images or learning pose-invariant representations, yet\nboth approaches typically require re-training and testing for each dataset,\ninvolving a substantial amount of effort. In contrast, this study proposes\nPose-TTA, a novel approach that aligns faces at inference time without\nadditional training. To achieve this, we employ a portrait animator that\ntransfers the source image identity into the pose of a driving image. Instead\nof frontalising a side-profile face -- which can introduce distortion --\nPose-TTA generates matching side-profile images for comparison, thereby\nreducing identity information loss. Furthermore, we propose a weighted feature\naggregation strategy to address any distortions or biases arising from the\nsynthetic data, thus enhancing the reliability of the augmented images.\nExtensive experiments on diverse datasets and with various pre-trained face\nrecognition models demonstrate that Pose-TTA consistently improves inference\nperformance. Moreover, our method is straightforward to integrate into existing\nface recognition pipelines, as it requires no retraining or fine-tuning of the\nunderlying recognition models.", "AI": {"tldr": "This paper introduces Pose-TTA, a novel approach that improves face recognition by aligning faces during the inference phase without additional training. It uses a portrait animator to transfer identities and proposes a weighted feature aggregation strategy to reduce distortions. Experiments show consistent performance improvements.", "motivation": "Existing methods for enhancing face recognition either rely on training with frontalised images or learning pose-invariant representations, both of which require re-training and testing for each dataset and involve significant effort.", "method": "Pose-TTA employs a portrait animator to transfer the identity from a source image to the pose of a driving image at inference time. It also proposes a weighted feature aggregation strategy to handle distortions from synthetic data.", "result": "Extensive experiments across diverse datasets and pre-trained models demonstrate that Pose-TTA consistently enhances inference performance in face recognition tasks.", "conclusion": "Pose-TTA is an effective method to improve face recognition performance without the need for retraining or fine-tuning underlying models, making it easy to integrate into existing pipelines."}}
{"id": "2505.08830", "pdf": "https://arxiv.org/pdf/2505.08830", "abs": "https://arxiv.org/abs/2505.08830", "authors": ["Wenhao Jiang", "Yuchuan Luo", "Guilin Deng", "Silong Chen", "Xu Yang", "Shihong Wu", "Xinwen Gao", "Lin Liu", "Shaojing Fu"], "title": "Federated Large Language Models: Feasibility, Robustness, Security and Future Directions", "categories": ["cs.CR", "cs.AI"], "comment": "35 pages", "summary": "The integration of Large Language Models (LLMs) and Federated Learning (FL)\npresents a promising solution for joint training on distributed data while\npreserving privacy and addressing data silo issues. However, this emerging\nfield, known as Federated Large Language Models (FLLM), faces significant\nchallenges, including communication and computation overheads, heterogeneity,\nprivacy and security concerns. Current research has primarily focused on the\nfeasibility of FLLM, but future trends are expected to emphasize enhancing\nsystem robustness and security. This paper provides a comprehensive review of\nthe latest advancements in FLLM, examining challenges from four critical\nperspectives: feasibility, robustness, security, and future directions. We\npresent an exhaustive survey of existing studies on FLLM feasibility, introduce\nmethods to enhance robustness in the face of resource, data, and task\nheterogeneity, and analyze novel risks associated with this integration,\nincluding privacy threats and security challenges. We also review the latest\ndevelopments in defense mechanisms and explore promising future research\ndirections, such as few-shot learning, machine unlearning, and IP protection.\nThis survey highlights the pressing need for further research to enhance system\nrobustness and security while addressing the unique challenges posed by the\nintegration of FL and LLM.", "AI": {"tldr": "The paper reviews the advancements and challenges in Federated Large Language Models (FLLM), focusing on feasibility, robustness, security, and future research directions.", "motivation": "To provide a comprehensive review of the latest advancements in FLLM and identify key challenges and future research directions.", "method": "Conducting an exhaustive survey of existing studies on FLLM feasibility, introducing methods to enhance robustness, analyzing privacy and security risks, reviewing defense mechanisms, and exploring promising future research areas.", "result": "Analyzed the challenges from four critical perspectives (feasibility, robustness, security, and future directions) and highlighted the need for further research in enhancing system robustness and security.", "conclusion": "FLLM faces significant challenges but holds great potential. Future research should focus on improving robustness, security, and addressing unique integration challenges."}}
{"id": "2505.09284", "pdf": "https://arxiv.org/pdf/2505.09284", "abs": "https://arxiv.org/abs/2505.09284", "authors": ["Panqi Chen", "Yifan Sun", "Lei Cheng", "Yang Yang", "Weichang Li", "Yang Liu", "Weiqing Liu", "Jiang Bian", "Shikai Fang"], "title": "Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Modeling and reconstructing multidimensional physical dynamics from sparse\nand off-grid observations presents a fundamental challenge in scientific\nresearch. Recently, diffusion-based generative modeling shows promising\npotential for physical simulation. However, current approaches typically\noperate on on-grid data with preset spatiotemporal resolution, but struggle\nwith the sparsely observed and continuous nature of real-world physical\ndynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in\nFunctional Tucker space, a novel framework that generates full-field evolution\nof physical dynamics from irregular sparse observations. SDIFT leverages the\nfunctional Tucker model as the latent space representer with proven universal\napproximation property, and represents observations as latent functions and\nTucker core sequences. We then construct a sequential diffusion model with\ntemporally augmented UNet in the functional Tucker space, denoising noise drawn\nfrom a Gaussian process to generate the sequence of core tensors.\n  At the posterior sampling stage, we propose a Message-Passing Posterior\nSampling mechanism, enabling conditional generation of the entire sequence\nguided by observations at limited time steps. We validate SDIFT on three\nphysical systems spanning astronomical (supernova explosions, light-year\nscale), environmental (ocean sound speed fields, kilometer scale), and\nmolecular (organic liquid, millimeter scale) domains, demonstrating significant\nimprovements in both reconstruction accuracy and computational efficiency\ncompared to state-of-the-art approaches.", "AI": {"tldr": "Analyze the abstract of a paper introducing SDIFT, Sequential DIffusion in Functional Tucker space.", "motivation": "Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations is crucial in scientific research but challenging. Current diffusion-based generative modeling approaches work well with on-grid data but struggle with sparsely observed real-world physical dynamics.", "method": "The authors introduce SDIFT (Sequential DIffusion in Functional Tucker space), which uses functional Tucker model as latent space representer for universal approximation property. It represents observations as latent functions and Tucker core sequences. A sequential diffusion model with temporally augmented UNet in functional Tucker space denoises noise from Gaussian process to generate core tensor sequence. Additionally, a Message-Passing Posterior Sampling mechanism allows conditional generation guided by limited time step observations.", "result": "SDIFT was validated on three physical systems across astronomical, environmental, and molecular domains. It showed significant improvements in reconstruction accuracy and computational efficiency compared to state-of-the-art methods.", "conclusion": "SDIFT provides a novel framework for generating full-field evolution of physical dynamics from irregular sparse observations, outperforming existing methods in both accuracy and efficiency."}}
{"id": "2505.09263", "pdf": "https://arxiv.org/pdf/2505.09263", "abs": "https://arxiv.org/abs/2505.09263", "authors": ["Guan Gui", "Bin-Bin Gao", "Jun Liu", "Chengjie Wang", "Yunsheng Wu"], "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.", "AI": {"tldr": "Anomaly detection faces challenges due to scarce anomaly samples. Existing methods using synthetic anomalies have a semantic gap with real-world ones. This paper proposes AnoGen, a few-shot Anomaly-driven Generation method that uses a diffusion model and only a few real anomalies to generate realistic anomalies. It involves three stages: learning anomaly distribution, guiding the diffusion model for generation, and training an anomaly detection model. Experiments on MVTec dataset show improved performance in both classification and segmentation tasks.", "motivation": "Anomaly detection is crucial but difficult because of the lack of anomaly samples. Current approaches to synthesize anomalies do not bridge the semantic gap with real-world anomalies effectively.", "method": "The AnoGen method consists of three stages: (1) Learning the anomaly distribution from a few real anomalies and embedding the knowledge; (2) Guiding the diffusion model using the embedding and bounding boxes to generate realistic anomalies on specific objects/textures; (3) Training a weakly-supervised anomaly detection model using the generated anomalies.", "result": "Experiments on the MVTec dataset demonstrate simultaneous improvements in both anomaly classification and segmentation tasks. Notably, DRAEM and DesTSeg achieved 5.8% and 1.5% improvements in AU-PR metric on the segmentation task respectively.", "conclusion": "The proposed AnoGen method successfully generates realistic and diverse anomalies using a few real samples, improving the performance of anomaly detection models in both classification and segmentation tasks."}}
{"id": "2505.09287", "pdf": "https://arxiv.org/pdf/2505.09287", "abs": "https://arxiv.org/abs/2505.09287", "authors": ["Shunsuke Yoneda", "Valdemar \u0160v\u00e1bensk\u00fd", "Gen Li", "Daisuke Deguchi", "Atsushi Shimada"], "title": "Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features", "categories": ["cs.LG", "cs.CY", "I.2; I.6; K.3"], "comment": "To appear in the Proceedings of the 18th Educational Data Mining\n  Conference (EDM 2025)", "summary": "Digital textbooks are widely used in various educational contexts, such as\nuniversity courses and online lectures. Such textbooks yield learning log data\nthat have been used in numerous educational data mining (EDM) studies for\nstudent behavior analysis and performance prediction. However, these studies\nhave faced challenges in integrating confidential data, such as academic\nrecords and learning logs, across schools due to privacy concerns.\nConsequently, analyses are often conducted with data limited to a single\nschool, which makes developing high-performing and generalizable models\ndifficult. This study proposes a method that combines federated learning and\ndifferential features to address these issues. Federated learning enables model\ntraining without centralizing data, thereby preserving student privacy.\nDifferential features, which utilize relative values instead of absolute\nvalues, enhance model performance and generalizability. To evaluate the\nproposed method, a model for predicting at-risk students was trained using data\nfrom 1,136 students across 12 courses conducted over 4 years, and validated on\nhold-out test data from 5 other courses. Experimental results demonstrated that\nthe proposed method addresses privacy concerns while achieving performance\ncomparable to that of models trained via centralized learning in terms of Top-n\nprecision, nDCG, and PR-AUC. Furthermore, using differential features improved\nprediction performance across all evaluation datasets compared to\nnon-differential approaches. The trained models were also applicable for early\nprediction, achieving high performance in detecting at-risk students in earlier\nstages of the semester within the validation datasets.", "AI": {"tldr": "Digital textbooks produce learning log data used in EDM studies, but privacy concerns limit data integration across schools. This study proposes a method combining federated learning and differential features to overcome these challenges, demonstrating comparable performance to centralized learning while addressing privacy issues.", "motivation": "To address the challenge of integrating confidential educational data across schools due to privacy concerns, which limits the development of high-performing and generalizable models.", "method": "The study proposes a method that combines federated learning, which allows model training without centralizing data thus preserving student privacy, and differential features, which use relative values instead of absolute values to enhance model performance and generalizability.", "result": "Experimental results showed that the proposed method resolves privacy concerns while achieving performance on par with centralized learning in terms of Top-n precision, nDCG, and PR-AUC. Using differential features improved prediction performance across all datasets compared to non-differential approaches. The models were also effective for early prediction of at-risk students.", "conclusion": "The proposed method successfully integrates data across schools while maintaining student privacy, leading to models with performance comparable to those trained via centralized learning, and offering improved prediction capabilities with differential features."}}
{"id": "2505.09264", "pdf": "https://arxiv.org/pdf/2505.09264", "abs": "https://arxiv.org/abs/2505.09264", "authors": ["Bin-Bin Gao"], "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.", "AI": {"tldr": "The paper presents OneNIP, a method for anomaly detection that reconstructs normal features and restores anomaly features using one normal image prompt. It also introduces a supervised refiner to improve pixel-level anomaly segmentation.", "motivation": "Current unsupervised reconstruction models may perfectly reconstruct both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Also, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space.", "method": "OneNIP reconstructs normal features and restores anomaly features with just one normal image prompt. Additionally, a supervised refiner is proposed that regresses reconstruction errors by using both real normal and synthesized anomalous images.", "result": "OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA.", "conclusion": "OneNIP effectively boosts unified anomaly detection performance and significantly improves pixel-level anomaly segmentation."}}
{"id": "2505.08835", "pdf": "https://arxiv.org/pdf/2505.08835", "abs": "https://arxiv.org/abs/2505.08835", "authors": ["Hyunsik Na", "Wonho Lee", "Seungdeok Roh", "Sohee Park", "Daeseon Choi"], "title": "Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of convenient and efficient fully unmanned stores equipped with\nartificial intelligence-based automated checkout systems marks a new era in\nretail. However, these systems have inherent artificial intelligence security\nvulnerabilities, which are exploited via adversarial patch attacks,\nparticularly in physical environments. This study demonstrated that adversarial\npatches can severely disrupt object detection models used in unmanned stores,\nleading to issues such as theft, inventory discrepancies, and interference. We\ninvestigated three types of adversarial patch attacks -- Hiding, Creating, and\nAltering attacks -- and highlighted their effectiveness. We also introduce the\nnovel color histogram similarity loss function by leveraging attacker knowledge\nof the color information of a target class object. Besides the traditional\nconfusion-matrix-based attack success rate, we introduce a new\nbounding-boxes-based metric to analyze the practical impact of these attacks.\nStarting with attacks on object detection models trained on snack and fruit\ndatasets in a digital environment, we evaluated the effectiveness of\nadversarial patches in a physical testbed that mimicked a real unmanned store\nwith RGB cameras and realistic conditions. Furthermore, we assessed the\nrobustness of these attacks in black-box scenarios, demonstrating that shadow\nattacks can enhance success rates of attacks even without direct access to\nmodel parameters. Our study underscores the necessity for robust defense\nstrategies to protect unmanned stores from adversarial threats. Highlighting\nthe limitations of the current defense mechanisms in real-time detection\nsystems and discussing various proactive measures, we provide insights into\nimproving the robustness of object detection models and fortifying unmanned\nretail environments against these attacks.", "AI": {"tldr": "This paper explores the vulnerabilities of AI-based automated checkout systems in unmanned stores through adversarial patch attacks, evaluates their effectiveness digitally and physically, proposes a new color histogram similarity loss function, and suggests defense strategies.", "motivation": "The motivation is to expose and address the security vulnerabilities in AI-based automated checkout systems used in unmanned stores, which can be exploited by adversarial patch attacks leading to significant issues such as theft and inventory discrepancies.", "method": "The method involves investigating three types of adversarial patch attacks (Hiding, Creating, and Altering), introducing a novel color histogram similarity loss function, proposing a new bounding-boxes-based metric for analysis, evaluating these attacks both digitally on object detection models and physically in a simulated unmanned store environment, and assessing the robustness of these attacks in black-box scenarios.", "result": "The results show that adversarial patches can severely disrupt object detection models in unmanned stores, with high success rates even in physical environments and black-box scenarios. The proposed color histogram similarity loss function and bounding-boxes-based metric provide new ways to evaluate attack impacts.", "conclusion": "The study concludes by emphasizing the need for robust defense mechanisms to protect unmanned stores from adversarial threats, highlighting current limitations in real-time detection systems, and suggesting proactive measures to improve model robustness."}}
{"id": "2505.09294", "pdf": "https://arxiv.org/pdf/2505.09294", "abs": "https://arxiv.org/abs/2505.09294", "authors": ["Fan Xu", "Wuyang Chen", "Wei Gao"], "title": "On the Learning with Augmented Class via Forests", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Decision trees and forests have achieved successes in various real\napplications, most working with all testing classes known in training data. In\nthis work, we focus on learning with augmented class via forests, where an\naugmented class may appear in testing data yet not in training data. We\nincorporate information of augmented class into trees' splitting, i.e., a new\nsplitting criterion, called augmented Gini impurity, is introduced to exploit\nsome unlabeled data from testing distribution. We then develop the approach\nnamed Learning with Augmented Class via Forests (LACForest), which constructs\nshallow forests based on the augmented Gini impurity and then splits forests\nwith pseudo-labeled augmented instances for better performance. We also develop\ndeep neural forests with a novel optimization objective based on our augmented\nGini impurity, so as to utilize the representation power of neural networks for\nforests. Theoretically, we present the convergence analysis for augmented Gini\nimpurity, and finally conduct experiments to verify the effectiveness of our\napproaches. The code is available at https://github.com/nju-xuf/LACForest/.", "AI": {"tldr": "This paper introduces augmented Gini impurity and LACForest approach to handle the situation where an augmented class appears in testing data but not in training data. It also develops deep neural forests with a new optimization objective.", "motivation": "Decision trees and forests have been successful in various applications, but most work with all testing classes known in training data. This research aims to address the challenge when an augmented class may appear in testing data yet not in training data.", "method": "A new splitting criterion called augmented Gini impurity is introduced to incorporate information of the augmented class into trees' splitting. The LACForest approach constructs shallow forests based on this impurity and splits forests with pseudo-labeled augmented instances. Additionally, deep neural forests with a novel optimization objective are developed.", "result": "Theoretical convergence analysis for augmented Gini impurity is presented. Experiments verify the effectiveness of the approaches proposed in the paper.", "conclusion": "The paper successfully addresses learning with augmented class via forests by introducing augmented Gini impurity and developing the LACForest approach as well as deep neural forests."}}
{"id": "2505.09265", "pdf": "https://arxiv.org/pdf/2505.09265", "abs": "https://arxiv.org/abs/2505.09265", "authors": ["Bin-Bin Gao"], "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by NeurIPS 2024", "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.", "AI": {"tldr": "This paper proposes MetaUAS, a one-prompt Meta-learning framework for universal anomaly segmentation using a pure vision model without pre-trained visual-language models. It significantly outperforms previous methods.", "motivation": "Current zero- and few-shot visual anomaly segmentation methods rely on powerful vision-language models that use manually designed textual prompts, but visual representations are inherently independent of language.", "method": "The authors present a novel paradigm unifying anomaly segmentation into change segmentation, propose the MetaUAS framework trained on large-scale synthetic image pairs, and introduce a soft feature alignment module to handle geometrical variations.", "result": "MetaUAS effectively and efficiently segments any anomalies with only one normal image prompt and outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods.", "conclusion": "This work demonstrates the potential of a pure visual foundation model for universal visual anomaly segmentation and provides an alternative to widely used vision-language models."}}
{"id": "2505.08838", "pdf": "https://arxiv.org/pdf/2505.08838", "abs": "https://arxiv.org/abs/2505.08838", "authors": ["Peixuan Ge", "Tongkun Su", "Faqin Lv", "Baoliang Zhao", "Peng Zhang", "Chi Hong Wong", "Liang Yao", "Yu Sun", "Zenan Wang", "Pak Kin Wong", "Ying Hu"], "title": "Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Ultrasound (US) report generation is a challenging task due to the\nvariability of US images, operator dependence, and the need for standardized\ntext. Unlike X-ray and CT, US imaging lacks consistent datasets, making\nautomation difficult. In this study, we propose a unified framework for\nmulti-organ and multilingual US report generation, integrating fragment-based\nmultilingual training and leveraging the standardized nature of US reports. By\naligning modular text fragments with diverse imaging data and curating a\nbilingual English-Chinese dataset, the method achieves consistent and\nclinically accurate text generation across organ sites and languages.\nFine-tuning with selective unfreezing of the vision transformer (ViT) further\nimproves text-image alignment. Compared to the previous state-of-the-art KMVE\nmethod, our approach achieves relative gains of about 2\\% in BLEU scores,\napproximately 3\\% in ROUGE-L, and about 15\\% in CIDEr, while significantly\nreducing errors such as missing or incorrect content. By unifying multi-organ\nand multi-language report generation into a single, scalable framework, this\nwork demonstrates strong potential for real-world clinical workflows.", "AI": {"tldr": "The paper presents a unified framework for multi-organ and multilingual ultrasound report generation, which integrates fragment-based multilingual training, curates a bilingual dataset, and leverages standardized US reports.", "motivation": "Ultrasound report generation is challenging due to variability in images, operator dependence, and lack of consistent datasets. Current methods have limitations in generating accurate and standardized reports across different organs and languages.", "method": "A unified framework is proposed that includes fragment-based multilingual training, alignment of modular text fragments with imaging data, and curation of a bilingual English-Chinese dataset. Fine-tuning with selective unfreezing of the vision transformer (ViT) is used to improve text-image alignment.", "result": "Compared to the previous state-of-the-art KMVE method, the approach achieves relative gains of about 2% in BLEU scores, approximately 3% in ROUGE-L, and about 15% in CIDEr, while significantly reducing errors such as missing or incorrect content.", "conclusion": "This work demonstrates strong potential for real-world clinical workflows by unifying multi-organ and multi-language report generation into a single, scalable framework."}}
{"id": "2505.09308", "pdf": "https://arxiv.org/pdf/2505.09308", "abs": "https://arxiv.org/abs/2505.09308", "authors": ["George Andriopoulos", "Soyuj Jung Basnet", "Juan Guevara", "Li Guo", "Keith Ross"], "title": "Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model", "categories": ["cs.LG"], "comment": "31 pages, 8 figures", "summary": "The Unconstrained Feature Model (UFM) is a mathematical framework that\nenables closed-form approximations for minimal training loss and related\nperformance measures in deep neural networks (DNNs). This paper leverages the\nUFM to provide qualitative insights into neural multivariate regression, a\ncritical task in imitation learning, robotics, and reinforcement learning.\nSpecifically, we address two key questions: (1) How do multi-task models\ncompare to multiple single-task models in terms of training performance? (2)\nCan whitening and normalizing regression targets improve training performance?\nThe UFM theory predicts that multi-task models achieve strictly smaller\ntraining MSE than multiple single-task models when the same or stronger\nregularization is applied to the latter, and our empirical results confirm\nthese findings. Regarding whitening and normalizing regression targets, the UFM\ntheory predicts that they reduce training MSE when the average variance across\nthe target dimensions is less than one, and our empirical results once again\nconfirm these findings. These findings highlight the UFM as a powerful\nframework for deriving actionable insights into DNN design and data\npre-processing strategies.", "AI": {"tldr": "The paper uses the Unconstrained Feature Model (UFM) to explore two questions in neural multivariate regression: the comparison between multi-task and single-task models, and the effect of whitening and normalizing regression targets. The UFM predicts, and empirical results confirm, that multi-task models have smaller training MSE with same or stronger regularization on single-task models. Whitening and normalizing reduce training MSE when the average variance across target dimensions is less than one.", "motivation": "To provide qualitative insights into neural multivariate regression using the UFM framework, addressing two key questions about model performance and data preprocessing.", "method": "Leveraging the UFM to theoretically predict and empirically validate the impact of multi-task versus single-task models and the effects of whitening and normalizing regression targets on training MSE.", "result": "Empirical results confirm UFM predictions: multi-task models achieve smaller training MSE under certain regularization conditions, and whitening/normalizing reduces MSE when the average variance across target dimensions is less than one.", "conclusion": "The UFM is a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies."}}
{"id": "2505.09274", "pdf": "https://arxiv.org/pdf/2505.09274", "abs": "https://arxiv.org/abs/2505.09274", "authors": ["Fares Bougourzi", "Abdenour Hadid"], "title": "Recent Advances in Medical Imaging Segmentation: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Medical imaging is a cornerstone of modern healthcare, driving advancements\nin diagnosis, treatment planning, and patient care. Among its various tasks,\nsegmentation remains one of the most challenging problem due to factors such as\ndata accessibility, annotation complexity, structural variability, variation in\nmedical imaging modalities, and privacy constraints. Despite recent progress,\nachieving robust generalization and domain adaptation remains a significant\nhurdle, particularly given the resource-intensive nature of some proposed\nmodels and their reliance on domain expertise. This survey explores\ncutting-edge advancements in medical image segmentation, focusing on\nmethodologies such as Generative AI, Few-Shot Learning, Foundation Models, and\nUniversal Models. These approaches offer promising solutions to longstanding\nchallenges. We provide a comprehensive overview of the theoretical foundations,\nstate-of-the-art techniques, and recent applications of these methods. Finally,\nwe discuss inherent limitations, unresolved issues, and future research\ndirections aimed at enhancing the practicality and accessibility of\nsegmentation models in medical imaging. We are maintaining a\n\\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub\nRepository} to continue tracking and updating innovations in this field.", "AI": {"tldr": "Medical imaging segmentation faces challenges like data accessibility, annotation complexity, and domain adaptation. Recent advancements in Generative AI, Few-Shot Learning, Foundation Models, and Universal Models provide promising solutions. This survey reviews these methodologies, their applications, and future research directions.", "motivation": "To address the challenges in medical image segmentation such as data accessibility, annotation complexity, structural variability, and privacy constraints.", "method": "Exploration of cutting-edge advancements including Generative AI, Few-Shot Learning, Foundation Models, and Universal Models for medical image segmentation.", "result": "Provided a comprehensive overview of theoretical foundations, state-of-the-art techniques, and recent applications of these methods.", "conclusion": "Discussed inherent limitations, unresolved issues, and outlined future research directions to enhance practicality and accessibility of segmentation models."}}
{"id": "2505.08841", "pdf": "https://arxiv.org/pdf/2505.08841", "abs": "https://arxiv.org/abs/2505.08841", "authors": ["Andrea Cremaschi", "Dae-Jin Lee", "Manuele Leonelli"], "title": "Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As artificial intelligence and robotics increasingly reshape the global labor\nmarket, understanding public perceptions of these technologies becomes\ncritical. We examine how these perceptions have evolved across Latin America,\nusing survey data from the 2017, 2018, 2020, and 2023 waves of the\nLatinobar\\'ometro. Drawing on responses from over 48,000 individuals across 16\ncountries, we analyze fear of job loss due to artificial intelligence and\nrobotics. Using statistical modeling and latent class analysis, we identify key\nstructural and ideological predictors of concern, with education level and\npolitical orientation emerging as the most consistent drivers. Our findings\nreveal substantial temporal and cross-country variation, with a notable peak in\nfear during 2018 and distinct attitudinal profiles emerging from latent\nsegmentation. These results offer new insights into the social and structural\ndimensions of AI anxiety in emerging economies and contribute to a broader\nunderstanding of public attitudes toward automation beyond the Global North.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.09331", "pdf": "https://arxiv.org/pdf/2505.09331", "abs": "https://arxiv.org/abs/2505.09331", "authors": ["Cunlai Pu", "Fangrui Wu", "Rajput Ramiz Sharafat", "Guangzhao Dai", "Xiangbo Shu"], "title": "MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks", "categories": ["cs.LG"], "comment": null, "summary": "Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)\naims to predict the potential formation of future links between UAVs. In\nadversarial environments where the route information of UAVs is unavailable,\npredicting future links must rely solely on the observed historical topological\ninformation of UANETs. However, the highly dynamic and sparse nature of UANET\ntopologies presents substantial challenges in effectively capturing meaningful\nstructural and temporal patterns for accurate link prediction. Most existing\nlink prediction methods focus on temporal dynamics at a single structural scale\nwhile neglecting the effects of sparsity, resulting in insufficient information\ncapture and limited applicability to UANETs. In this paper, we propose a\nmulti-scale structural-temporal link prediction model (MUST) for UANETs.\nSpecifically, we first employ graph attention networks (GATs) to capture\nstructural features at multiple levels, including the individual UAV level, the\nUAV community level, and the overall network level. Then, we use long\nshort-term memory (LSTM) networks to learn the temporal dynamics of these\nmulti-scale structural features. Additionally, we address the impact of\nsparsity by introducing a sophisticated loss function during model\noptimization. We validate the performance of MUST using several UANET datasets\ngenerated through simulations. Extensive experimental results demonstrate that\nMUST achieves state-of-the-art link prediction performance in highly dynamic\nand sparse UANETs.", "AI": {"tldr": "The paper proposes a multi-scale structural-temporal link prediction model (MUST) for unmanned aerial vehicle (UAV) ad hoc networks (UANETs). It uses graph attention networks (GATs) and long short-term memory (LSTM) networks to capture structural and temporal patterns, while addressing sparsity via a custom loss function. Experiments show MUST outperforms existing methods in dynamic and sparse UANETs.", "motivation": "Existing link prediction methods focus on temporal dynamics at a single structural scale and neglect the effects of sparsity in UANETs, leading to insufficient information capture and limited applicability.", "method": "The proposed model, MUST, first employs GATs to capture structural features at multiple levels (individual UAV, UAV community, and overall network). Then it uses LSTM networks to learn the temporal dynamics of these multi-scale structural features. A sophisticated loss function is introduced to address the impact of sparsity during model optimization.", "result": "Extensive experimental results using simulated UANET datasets demonstrate that MUST achieves state-of-the-art performance in link prediction for highly dynamic and sparse UANETs.", "conclusion": "MUST effectively captures meaningful structural and temporal patterns in UANETs, overcoming the challenges posed by their highly dynamic and sparse nature. It outperforms existing methods in link prediction tasks."}}
{"id": "2505.09306", "pdf": "https://arxiv.org/pdf/2505.09306", "abs": "https://arxiv.org/abs/2505.09306", "authors": ["Thijs L van der Plas", "Stephen Law", "Michael JO Pocock"], "title": "Predicting butterfly species presence from satellite imagery using soft contrastive regularisation", "categories": ["cs.CV", "cs.LG"], "comment": "To be published in the 2025 CVPR FGVC12 workshop", "summary": "The growing demand for scalable biodiversity monitoring methods has fuelled\ninterest in remote sensing data, due to its widespread availability and\nextensive coverage. Traditionally, the application of remote sensing to\nbiodiversity research has focused on mapping and monitoring habitats, but with\nincreasing availability of large-scale citizen-science wildlife observation\ndata, recent methods have started to explore predicting multi-species presence\ndirectly from satellite images. This paper presents a new data set for\npredicting butterfly species presence from satellite data in the United\nKingdom. We experimentally optimise a Resnet-based model to predict\nmulti-species presence from 4-band satellite images, and find that this model\nespecially outperforms the mean rate baseline for locations with high species\nbiodiversity. To improve performance, we develop a soft, supervised contrastive\nregularisation loss that is tailored to probabilistic labels (such as\nspecies-presence data), and demonstrate that this improves prediction accuracy.\nIn summary, our new data set and contrastive regularisation method contribute\nto the open challenge of accurately predicting species biodiversity from remote\nsensing data, which is key for efficient biodiversity monitoring.", "AI": {"tldr": "This paper presents a new dataset for predicting butterfly species presence from satellite data in the UK and develops a soft, supervised contrastive regularisation loss to improve prediction accuracy.", "motivation": "The increasing availability of large-scale citizen-science wildlife observation data has led to interest in predicting multi-species presence directly from satellite images for scalable biodiversity monitoring.", "method": "The authors experimentally optimize a Resnet-based model to predict multi-species presence from 4-band satellite images and develop a soft, supervised contrastive regularisation loss tailored to probabilistic labels like species-presence data.", "result": "The Resnet-based model outperforms the mean rate baseline particularly for locations with high species biodiversity and the contrastive regularisation method improves prediction accuracy.", "conclusion": "The new dataset and contrastive regularisation method contribute towards accurately predicting species biodiversity from remote sensing data which is crucial for efficient biodiversity monitoring."}}
{"id": "2505.08844", "pdf": "https://arxiv.org/pdf/2505.08844", "abs": "https://arxiv.org/abs/2505.08844", "authors": ["Jiawen Chen", "Jianghao Zhang", "Huaxiu Yao", "Yun Li"], "title": "CellTypeAgent: Trustworthy cell type annotation with Large Language Models", "categories": ["q-bio.GN", "cs.AI", "68T20", "I.2.1"], "comment": null, "summary": "Cell type annotation is a critical yet laborious step in single-cell RNA\nsequencing analysis. We present a trustworthy large language model (LLM)-agent,\nCellTypeAgent, which integrates LLMs with verification from relevant databases.\nCellTypeAgent achieves higher accuracy than existing methods while mitigating\nhallucinations. We evaluated CellTypeAgent across nine real datasets involving\n303 cell types from 36 tissues. This combined approach holds promise for more\nefficient and reliable cell type annotation.", "AI": {"tldr": "CellTypeAgent, a trustworthy large language model (LLM)-agent that integrates LLMs with verification from relevant databases for cell type annotation in single-cell RNA sequencing analysis, achieves higher accuracy than existing methods while mitigating hallucinations.", "motivation": "Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis.", "method": "Presented is CellTypeAgent, which integrates large language models (LLMs) with verification from relevant databases.", "result": "Evaluated across nine real datasets involving 303 cell types from 36 tissues, CellTypeAgent achieves higher accuracy than existing methods while reducing hallucinations.", "conclusion": "The combined approach of CellTypeAgent holds promise for more efficient and reliable cell type annotation."}}
{"id": "2505.09344", "pdf": "https://arxiv.org/pdf/2505.09344", "abs": "https://arxiv.org/abs/2505.09344", "authors": ["Gabriel Cort\u00eas", "Nuno Louren\u00e7o", "Paolo Romano", "Penousal Machado"], "title": "GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Determining the performance of a Deep Neural Network during Neural\nArchitecture Search processes is essential for identifying optimal\narchitectures and hyperparameters. Traditionally, this process requires\ntraining and evaluation of each network, which is time-consuming and\nresource-intensive. Zero-cost proxies estimate performance without training,\nserving as an alternative to traditional training. However, recent proxies\noften lack generalization across diverse scenarios and provide only relative\nrankings rather than predicted accuracies. To address these limitations, we\npropose GreenFactory, an ensemble of zero-cost proxies that leverages a random\nforest regressor to combine multiple predictors' strengths and directly predict\nmodel test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust\nresults across multiple datasets. Specifically, GreenFactory achieves high\nKendall correlations on NATS-Bench-SSS, indicating substantial agreement\nbetween its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945\nfor CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we\nachieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for\nImageNet-16-120, showcasing its reliability in both search spaces.", "AI": {"tldr": "GreenFactory is an ensemble of zero-cost proxies that uses a random forest regressor to predict model test accuracy without training. Evaluated on NATS-Bench, it shows high Kendall correlations across multiple datasets.", "motivation": "The motivation of this paper is to address the limitations of existing zero-cost proxies for estimating the performance of deep neural networks during neural architecture search (NAS). Current proxies lack generalization and provide only relative rankings rather than predicted accuracies.", "method": "The proposed method, GreenFactory, is an ensemble of zero-cost proxies that leverages a random forest regressor. It combines the strengths of multiple predictors to directly predict model test accuracy without the need for training.", "result": "GreenFactory achieves robust results on NATS-Bench across multiple datasets. Specifically, it achieves high Kendall correlations: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120 on NATS-Bench-SSS; and 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120 on NATS-Bench-TSS.", "conclusion": "GreenFactory demonstrates substantial agreement between its predicted scores and actual performance, showcasing its reliability in both search spaces."}}
{"id": "2505.09324", "pdf": "https://arxiv.org/pdf/2505.09324", "abs": "https://arxiv.org/abs/2505.09324", "authors": ["Lakshya Gupta", "Imran N. Junejo"], "title": "Neural Video Compression using 2D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "9 pages, 8 figures", "summary": "The computer vision and image processing research community has been involved\nin standardizing video data communications for the past many decades, leading\nto standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent\ngroundbreaking works have focused on employing deep learning-based techniques\nto replace the traditional video codec pipeline to a greater affect. Neural\nvideo codecs (NVC) create an end-to-end ML-based solution that does not rely on\nany handcrafted features (motion or edge-based) and have the ability to learn\ncontent-aware compression strategies, offering better adaptability and higher\ncompression efficiency than traditional methods. This holds a great potential\nnot only for hardware design, but also for various video streaming platforms\nand applications, especially video conferencing applications such as MS-Teams\nor Zoom that have found extensive usage in classrooms and workplaces. However,\ntheir high computational demands currently limit their use in real-time\napplications like video conferencing. To address this, we propose a\nregion-of-interest (ROI) based neural video compression model that leverages 2D\nGaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable\nof real-time decoding and can be optimized using fewer data points, requiring\nonly thousands of Gaussians for decent quality outputs as opposed to millions\nin 3D scenes. In this work, we designed a video pipeline that speeds up the\nencoding time of the previous Gaussian splatting-based image codec by 88% by\nusing a content-aware initialization strategy paired with a novel Gaussian\ninter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be\nused for a video-codec solution, the first of its kind solution in this neural\nvideo codec space.", "AI": {"tldr": "An ROI-based neural video compression model using 2D Gaussian Splatting is proposed, which speeds up encoding time by 88% through a content-aware initialization strategy and a novel Gaussian inter-frame redundancy-reduction mechanism.", "motivation": "Traditional video codecs have been used for decades, but recent advancements in deep learning-based techniques offer better adaptability and higher compression efficiency. However, the computational demands of neural video codecs limit their use in real-time applications like video conferencing.", "method": "The researchers developed a region-of-interest (ROI) based neural video compression model leveraging 2D Gaussian Splatting. They introduced a content-aware initialization strategy and a new Gaussian inter-frame redundancy-reduction mechanism to speed up the encoding process.", "result": "This approach reduces encoding time by 88% compared to previous Gaussian splatting-based image codecs, making it feasible for video codec solutions.", "conclusion": "This work presents an innovative solution in the neural video codec field, offering significant improvements in encoding speed while maintaining quality, thus expanding potential applications in video streaming platforms."}}
{"id": "2505.08845", "pdf": "https://arxiv.org/pdf/2505.08845", "abs": "https://arxiv.org/abs/2505.08845", "authors": ["Misgina Tsighe Hagos", "Antti Suutala", "Dmitrii Bychkov", "Hakan K\u00fcc\u00fckel", "Joar von Bahr", "Milda Poceviciute", "Johan Lundin", "Nina Linder", "Claes Lundstr\u00f6m"], "title": "Validation of Conformal Prediction in Cervical Atypia Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Deep learning based cervical cancer classification can potentially increase\naccess to screening in low-resource regions. However, deep learning models are\noften overconfident and do not reliably reflect diagnostic uncertainty.\nMoreover, they are typically optimized to generate maximum-likelihood\npredictions, which fail to convey uncertainty or ambiguity in their results.\nSuch challenges can be addressed using conformal prediction, a model-agnostic\nframework for generating prediction sets that contain likely classes for\ntrained deep-learning models. The size of these prediction sets indicates model\nuncertainty, contracting as model confidence increases. However, existing\nconformal prediction evaluation primarily focuses on whether the prediction set\nincludes or covers the true class, often overlooking the presence of extraneous\nclasses. We argue that prediction sets should be truthful and valuable to end\nusers, ensuring that the listed likely classes align with human expectations\nrather than being overly relaxed and including false positives or unlikely\nclasses. In this study, we comprehensively validate conformal prediction sets\nusing expert annotation sets collected from multiple annotators. We evaluate\nthree conformal prediction approaches applied to three deep-learning models\ntrained for cervical atypia classification. Our expert annotation-based\nanalysis reveals that conventional coverage-based evaluations overestimate\nperformance and that current conformal prediction methods often produce\nprediction sets that are not well aligned with human labels. Additionally, we\nexplore the capabilities of the conformal prediction methods in identifying\nambiguous and out-of-distribution data.", "AI": {"tldr": "Deep learning models for cervical cancer classification often lack reliable uncertainty estimation. Conformal prediction can address this issue, but current methods still have limitations in aligning with human expectations.", "motivation": "Deep learning models are overconfident and cannot reliably reflect diagnostic uncertainty. The conventional evaluation of conformal prediction primarily focuses on whether the prediction set includes the true class, ignoring extraneous classes.", "method": "Using expert annotation sets collected from multiple annotators to comprehensively validate conformal prediction sets, evaluating three conformal prediction approaches applied to three deep-learning models trained for cervical atypia classification.", "result": "Conventional coverage-based evaluations overestimate performance, and current conformal prediction methods often produce prediction sets that do not align well with human labels. Additionally, conformal prediction methods show some capabilities in identifying ambiguous and out-of-distribution data.", "conclusion": "Conformal prediction shows promise in addressing uncertainty issues in deep learning models for cervical cancer classification, but improvements are needed to better align prediction sets with human expectations."}}
{"id": "2505.09354", "pdf": "https://arxiv.org/pdf/2505.09354", "abs": "https://arxiv.org/abs/2505.09354", "authors": ["Guangtai Wang", "Chi-Man Vong", "Jintao Huang"], "title": "Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning", "categories": ["cs.LG"], "comment": null, "summary": "Diminishing the impact of false-positive labels is critical for conducting\ndisambiguation in partial label learning. However, the existing disambiguation\nstrategies mainly focus on exploiting the characteristics of individual partial\nlabel instances while neglecting the strong supervision information of clean\nsamples randomly lying in the datasets. In this work, we show that clean\nsamples can be collected to offer guidance and enhance the confidence of the\nmost possible candidates. Motivated by the manner of the differentiable count\nloss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new\ncalibration strategy called CleanSE. Specifically, we attribute the most\nreliable candidates with higher significance under the assumption that for each\nclean sample, if its label is one of the candidates of its nearest neighbor in\nthe representation space, it is more likely to be the ground truth of its\nneighbor. Moreover, clean samples offer help in characterizing the sample\ndistributions by restricting the label counts of each label to a specific\ninterval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL\ndatasets showed this calibration strategy can be applied to most of the\nstate-of-the-art PLL methods as well as enhance their performance.", "AI": {"tldr": "This paper proposes a new calibration strategy called CleanSE to diminish the impact of false-positive labels in partial label learning by utilizing clean samples.", "motivation": "The motivation is to improve disambiguation in partial label learning by leveraging the strong supervision information of clean samples within datasets, which existing strategies neglect.", "method": "The method involves collecting clean samples to guide and enhance confidence in most possible candidates. It uses a differentiable count loss strategy and K-Nearest-Neighbor algorithm to attribute higher significance to reliable candidates. Clean samples also help characterize sample distributions by restricting label counts to a specific interval.", "result": "Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL datasets show that this calibration strategy can be applied to most state-of-the-art PLL methods and enhances their performance.", "conclusion": "CleanSE is an effective calibration strategy for partial label learning that leverages clean samples to improve disambiguation and overall performance."}}
{"id": "2505.09329", "pdf": "https://arxiv.org/pdf/2505.09329", "abs": "https://arxiv.org/abs/2505.09329", "authors": ["Jiarun Liu", "Hong-Yu Zhou", "Weijian Huang", "Hao Yang", "Dongning Song", "Tao Tan", "Yong Liang", "Shanshan Wang"], "title": "BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Scaling up model and data size have demonstrated impressive performance\nimprovement over a wide range of tasks. Despite extensive studies on scaling\nbehaviors for general-purpose tasks, medical images exhibit substantial\ndifferences from natural data. It remains unclear the key factors in developing\nmedical vision foundation models at scale due to the absence of an extensive\nunderstanding of scaling behavior in the medical domain. In this paper, we\nexplored the scaling behavior across model sizes, training algorithms, data\nsizes, and imaging modalities in developing scalable medical vision foundation\nmodels by self-supervised learning. To support scalable pretraining, we\nintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing a\nwide range of biomedical image modalities and anatomies. We observed that\nscaling up does provide benefits but varies across tasks. Additional analysis\nreveals several factors correlated with scaling benefits. Finally, we propose\nBioVFM, a large-scale medical vision foundation model pretrained on 21 million\nbiomedical images, which outperforms the previous state-of-the-art foundation\nmodels across 12 medical benchmarks. Our results highlight that while scaling\nup is beneficial for pursuing better performance, task characteristics, data\ndiversity, pretraining methods, and computational efficiency remain critical\nconsiderations for developing scalable medical foundation models.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u7b97\u6cd5\u3001\u6570\u636e\u5927\u5c0f\u548c\u6210\u50cf\u65b9\u5f0f\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u4e86\u5305\u542b2100\u4e07\u5f20\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6BioVFM-21M\u3002\u63d0\u51fa\u7684\u5927\u89c4\u6a21\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578bBioVFM\u572812\u4e2a\u533b\u5b66\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1\u5728\u901a\u7528\u4efb\u52a1\u7684\u6269\u5c55\u884c\u4e3a\u4e0a\u6709\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u4f46\u533b\u5b66\u56fe\u50cf\u4e0e\u81ea\u7136\u6570\u636e\u6709\u663e\u8457\u5dee\u5f02\uff0c\u7f3a\u4e4f\u5bf9\u533b\u5b66\u9886\u57df\u6269\u5c55\u884c\u4e3a\u7684\u5e7f\u6cdb\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5f00\u53d1\u53ef\u6269\u5c55\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63a2\u7d22\u8de8\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u7b97\u6cd5\u3001\u6570\u636e\u5927\u5c0f\u548c\u6210\u50cf\u6a21\u6001\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6BioVFM-21M\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6a21\u6001\u548c\u89e3\u5256\u7ed3\u6784\u3002", "result": "\u89c2\u5bdf\u5230\u6269\u5c55\u786e\u5b9e\u63d0\u4f9b\u4e86\u597d\u5904\uff0c\u4f46\u56e0\u4efb\u52a1\u800c\u5f02\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578bBioVFM\uff0c\u5b83\u572812\u4e2a\u533b\u5b66\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "\u6269\u5c55\u5bf9\u4e8e\u8ffd\u6c42\u66f4\u597d\u7684\u6027\u80fd\u662f\u6709\u76ca\u7684\uff0c\u4f46\u4efb\u52a1\u7279\u6027\u3001\u6570\u636e\u591a\u6837\u6027\u3001\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u8ba1\u7b97\u6548\u7387\u4ecd\u7136\u662f\u5f00\u53d1\u53ef\u6269\u5c55\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2505.09361", "pdf": "https://arxiv.org/pdf/2505.09361", "abs": "https://arxiv.org/abs/2505.09361", "authors": ["Samir Moustafa", "Nils M. Kriege", "Wilfried N. Gansterer"], "title": "Efficient Mixed Precision Quantization in Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have become essential for handling large-scale\ngraph applications. However, the computational demands of GNNs necessitate the\ndevelopment of efficient methods to accelerate inference. Mixed precision\nquantization emerges as a promising solution to enhance the efficiency of GNN\narchitectures without compromising prediction performance. Compared to\nconventional deep learning architectures, GNN layers contain a wider set of\ncomponents that can be quantized, including message passing functions,\naggregation functions, update functions, the inputs, learnable parameters, and\noutputs of these functions. In this paper, we introduce a theorem for efficient\nquantized message passing to aggregate integer messages. It guarantees\nnumerical equality of the aggregated messages using integer values with respect\nto those obtained with full (FP32) precision. Based on this theorem, we\nintroduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which\nflexibly selects effective integer bit-widths for all components within GNN\nlayers. Our approach systematically navigates the wide set of possible\nbit-width combinations, addressing the challenge of optimizing efficiency while\naiming at maintaining comparable prediction performance. MixQ-GNN integrates\nwith existing GNN quantization methods, utilizing their graph structure\nadvantages to achieve higher prediction performance. On average, MixQ-GNN\nachieved reductions in bit operations of 5.5x for node classification and 5.1x\nfor graph classification compared to architectures represented in FP32\nprecision.", "AI": {"tldr": "Graph Neural Networks (GNNs) are crucial for large-scale graph applications, but their computational demands require efficient methods. Mixed precision quantization offers a solution by enhancing GNN efficiency without sacrificing prediction performance. This paper presents a theorem for quantized message passing and introduces the MixQ-GNN framework, which selects optimal integer bit-widths for GNN components. MixQ-GNN integrates with existing methods and achieves significant reductions in bit operations.", "motivation": "To address the high computational demands of GNNs and improve inference efficiency without affecting prediction performance.", "method": "Developed a theorem for efficient quantized message passing that ensures numerical equality with full precision. Introduced MixQ-GNN, a framework that flexibly selects integer bit-widths for all components within GNN layers, optimizing efficiency while maintaining performance.", "result": "MixQ-GNN achieved 5.5x reduction in bit operations for node classification and 5.1x reduction for graph classification compared to FP32 precision architectures.", "conclusion": "Mixed precision quantization via MixQ-GNN effectively accelerates GNN inference while preserving prediction performance."}}
{"id": "2505.09336", "pdf": "https://arxiv.org/pdf/2505.09336", "abs": "https://arxiv.org/abs/2505.09336", "authors": ["Muzammil Behzad"], "title": "Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce MultiviewVLM, a vision-language model designed\nfor unsupervised contrastive multiview representation learning of facial\nemotions from 3D/4D data. Our architecture integrates pseudo-labels derived\nfrom generated textual prompts to guide implicit alignment of emotional\nsemantics. To capture shared information across multi-views, we propose a joint\nembedding space that aligns multiview representations without requiring\nexplicit supervision. We further enhance the discriminability of our model\nthrough a novel multiview contrastive learning strategy that leverages stable\npositive-negative pair sampling. A gradient-friendly loss function is\nintroduced to promote smoother and more stable convergence, and the model is\noptimized for distributed training to ensure scalability. Extensive experiments\ndemonstrate that MultiviewVLM outperforms existing state-of-the-art methods and\ncan be easily adapted to various real-world applications with minimal\nmodifications.", "AI": {"tldr": "The paper introduces MultiviewVLM, a vision-language model for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. It outperforms existing methods and can be adapted to various real-world applications.", "motivation": "To create a model that can effectively learn representations of facial emotions from 3D/4D data in an unsupervised manner, capturing shared information across multi-views without explicit supervision.", "method": "The method integrates pseudo-labels from generated textual prompts, proposes a joint embedding space for multiview alignment, employs a novel multiview contrastive learning strategy with stable positive-negative pair sampling, and uses a gradient-friendly loss function for optimization.", "result": "Extensive experiments show that MultiviewVLM outperforms existing state-of-the-art methods in unsupervised contrastive multiview representation learning of facial emotions.", "conclusion": "MultiviewVLM is a successful vision-language model for unsupervised learning of facial emotion representations from 3D/4D data, and it can be easily adapted to various real-world applications."}}
{"id": "2505.08847", "pdf": "https://arxiv.org/pdf/2505.08847", "abs": "https://arxiv.org/abs/2505.08847", "authors": ["Fatima Ezzeddine", "Rinad Akel", "Ihab Sbeity", "Silvia Giordano", "Marc Langheinrich", "Omran Ayoub"], "title": "On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Machine Learning as a Service (MLaaS) has gained important attraction as a\nmeans for deploying powerful predictive models, offering ease of use that\nenables organizations to leverage advanced analytics without substantial\ninvestments in specialized infrastructure or expertise. However, MLaaS\nplatforms must be safeguarded against security and privacy attacks, such as\nmodel extraction (MEA) attacks. The increasing integration of explainable AI\n(XAI) within MLaaS has introduced an additional privacy challenge, as attackers\ncan exploit model explanations particularly counterfactual explanations (CFs)\nto facilitate MEA. In this paper, we investigate the trade offs among model\nperformance, privacy, and explainability when employing Differential Privacy\n(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two\ndistinct DP strategies: implemented during the classification model training\nand at the explainer during CF generation.", "AI": {"tldr": "The paper explores the trade-offs of model performance, privacy and explainability when using Differential Privacy to counteract model extraction attacks in MLaaS platforms that incorporate explainable AI.", "motivation": "MLaaS platforms are becoming increasingly important for deploying predictive models. However, these platforms face security and privacy challenges such as model extraction attacks which can be facilitated by explainable AI's counterfactual explanations.", "method": "Investigates two distinct Differential Privacy strategies - one implemented during classification model training and another at the explainer during counterfactual generation - to evaluate their impact on model performance, privacy, and explainability.", "result": "Not explicitly stated in the abstract but expected to provide insights into how each DP strategy affects the balance among model performance, privacy, and explainability.", "conclusion": "Differential Privacy shows promise in mitigating counterfactual-facilitated model extraction attacks while managing the trade-offs among model performance, privacy, and explainability."}}
{"id": "2505.09366", "pdf": "https://arxiv.org/pdf/2505.09366", "abs": "https://arxiv.org/abs/2505.09366", "authors": ["SeyedMojtaba Mohasel", "Alireza Afzal Aghaei", "Corey Pew"], "title": "Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks", "categories": ["cs.LG"], "comment": null, "summary": "Objective: This paper investigates the potential of learnable activation\nfunctions in Kolmogorov-Arnold Networks (KANs) for personalized control in a\nlower-limb prosthesis. In addition, user-specific vs. pooled training data is\nevaluated to improve machine learning (ML) and Deep Learning (DL) performance\nfor turn intent prediction.\n  Method: Inertial measurement unit (IMU) data from the shank were collected\nfrom five individuals with lower-limb amputation performing turning tasks in a\nlaboratory setting. Ability to classify an upcoming turn was evaluated for\nMultilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional\nneural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The\ncomparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)\nassessed the effectiveness of learnable activation functions. Models were\ntrained separately on user-specific and pooled data to evaluate the impact of\ntraining data on their performance.\n  Results: Learnable activation functions in KAN and FKAN did not yield\nsignificant improvement compared to MLP and CNN, respectively. Training on\nuser-specific data yielded superior results compared to pooled data for ML\nmodels ($p < 0.05$). In contrast, no significant difference was observed\nbetween user-specific and pooled training for DL models.\n  Significance: These findings suggest that learnable activation functions may\ndemonstrate distinct advantages in datasets involving more complex tasks and\nlarger volumes. In addition, pooled training showed comparable performance to\nuser-specific training in DL models, indicating that model training for\nprosthesis control can utilize data from multiple participants.", "AI": {"tldr": "\u7814\u7a76\u4e86KANs\u4e2d\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728\u4e0b\u80a2\u5047\u80a2\u4e2a\u6027\u5316\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u7528\u6237\u7279\u5b9a\u6570\u636e\u4e0e\u6c47\u603b\u6570\u636e\u5bf9ML\u548cDL\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728KAN\u548cFKAN\u4e2d\u6ca1\u6709\u663e\u8457\u4f18\u52bf\uff0c\u800c\u7528\u6237\u7279\u5b9a\u6570\u636e\u5728ML\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728DL\u6a21\u578b\u4e2d\u65e0\u663e\u8457\u5dee\u5f02\u3002\u8fd9\u8868\u660e\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u53ef\u80fd\u5728\u66f4\u590d\u6742\u4efb\u52a1\u548c\u66f4\u5927\u6570\u636e\u96c6\u4e2d\u6709\u4f18\u52bf\uff0c\u540c\u65f6DL\u6a21\u578b\u53ef\u4ee5\u5229\u7528\u6c47\u603b\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u63a2\u7d22\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728KANs\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee5\u53ca\u7528\u6237\u7279\u5b9a\u6570\u636e\u4e0e\u6c47\u603b\u6570\u636e\u5bf9ML\u548cDL\u6a21\u578b\u5728\u9884\u6d4b\u8f6c\u5411\u610f\u56fe\u65f6\u7684\u6027\u80fd\u5f71\u54cd\u3002\u76ee\u7684\u662f\u63d0\u9ad8\u4e0b\u80a2\u5047\u80a2\u4e2a\u6027\u5316\u63a7\u5236\u7684\u6548\u679c\u3002", "method": "\u4ece\u4e94\u540d\u4e0b\u80a2\u622a\u80a2\u8005\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u6267\u884c\u8f6c\u5411\u4efb\u52a1\u65f6\u6536\u96c6\u80eb\u9aa8\u7684IMU\u6570\u636e\u3002\u4f7f\u7528MLP\u3001KAN\u3001CNN\u548cFKAN\u56db\u79cd\u6a21\u578b\u8fdb\u884c\u5373\u5c06\u53d1\u751f\u8f6c\u5411\u7684\u5206\u7c7b\u80fd\u529b\u8bc4\u4f30\u3002\u901a\u8fc7\u6bd4\u8f83MLP\u548cKAN\uff08\u9488\u5bf9ML\u6a21\u578b\uff09\u4ee5\u53caFKAN\u548cCNN\uff08\u9488\u5bf9DL\u6a21\u578b\uff09\uff0c\u8bc4\u4f30\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u7684\u6709\u6548\u6027\u3002\u6a21\u578b\u5206\u522b\u5728\u7528\u6237\u7279\u5b9a\u6570\u636e\u548c\u6c47\u603b\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u8bc4\u4f30\u8bad\u7ec3\u6570\u636e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728KAN\u548cFKAN\u4e2d\u5e76\u672a\u663e\u8457\u4f18\u4e8eMLP\u548cCNN\u3002\u5bf9\u4e8eML\u6a21\u578b\uff0c\u7528\u6237\u7279\u5b9a\u6570\u636e\u8bad\u7ec3\u7684\u7ed3\u679c\u663e\u8457\u4f18\u4e8e\u6c47\u603b\u6570\u636e\uff08p<0.05\uff09\u3002\u800c\u5bf9\u4e8eDL\u6a21\u578b\uff0c\u672a\u89c2\u5bdf\u5230\u7528\u6237\u7279\u5b9a\u6570\u636e\u4e0e\u6c47\u603b\u6570\u636e\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u53ef\u80fd\u5728\u66f4\u590d\u6742\u4efb\u52a1\u548c\u66f4\u5927\u6570\u636e\u96c6\u4e2d\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u5728DL\u6a21\u578b\u4e2d\uff0c\u6c47\u603b\u8bad\u7ec3\u8868\u73b0\u51fa\u4e0e\u7528\u6237\u7279\u5b9a\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8868\u660e\u5047\u80a2\u63a7\u5236\u7684\u6a21\u578b\u8bad\u7ec3\u53ef\u4ee5\u5229\u7528\u6765\u81ea\u591a\u4e2a\u53c2\u4e0e\u8005\u7684\u6570\u636e\u3002"}}
{"id": "2505.09358", "pdf": "https://arxiv.org/pdf/2505.09358", "abs": "https://arxiv.org/abs/2505.09358", "authors": ["Bingxin Ke", "Kevin Qu", "Tianfu Wang", "Nando Metzger", "Shengyu Huang", "Bo Li", "Anton Obukhov", "Konrad Schindler"], "title": "Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "Journal extension of our CVPR 2024 paper, featuring new tasks,\n  improved efficiency, high-resolution capabilities, and enhanced accessibility", "summary": "The success of deep learning in computer vision over the past decade has\nhinged on large labeled datasets and strong pretrained models. In data-scarce\nsettings, the quality of these pretrained models becomes crucial for effective\ntransfer learning. Image classification and self-supervised learning have\ntraditionally been the primary methods for pretraining CNNs and\ntransformer-based architectures. Recently, the rise of text-to-image generative\nmodels, particularly those using denoising diffusion in a latent space, has\nintroduced a new class of foundational models trained on massive, captioned\nimage datasets. These models' ability to generate realistic images of unseen\ncontent suggests they possess a deep understanding of the visual world. In this\nwork, we present Marigold, a family of conditional generative models and a\nfine-tuning protocol that extracts the knowledge from pretrained latent\ndiffusion models like Stable Diffusion and adapts them for dense image analysis\ntasks, including monocular depth estimation, surface normals prediction, and\nintrinsic decomposition. Marigold requires minimal modification of the\npre-trained latent diffusion model's architecture, trains with small synthetic\ndatasets on a single GPU over a few days, and demonstrates state-of-the-art\nzero-shot generalization. Project page:\nhttps://marigoldcomputervision.github.io", "AI": {"tldr": "Marigold\uff0c\u4e00\u79cd\u4ece\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\u5e76\u5c06\u5176\u9002\u5e94\u4e8e\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u6700\u5c11\u7684\u67b6\u6784\u4fee\u6539\u548c\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u8d28\u91cf\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u4e8e\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u5206\u7c7b\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f46\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5c55\u73b0\u51fa\u4e86\u5bf9\u89c6\u89c9\u4e16\u754c\u7684\u6df1\u523b\u7406\u89e3\uff0c\u8fd9\u4e3a\u65b0\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMarigold\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\u5bb6\u65cf\u53ca\u5176\u5fae\u8c03\u534f\u8bae\uff0c\u8be5\u65b9\u6cd5\u4ece\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff08\u5982\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u8868\u9762\u6cd5\u7ebf\u9884\u6d4b\u548c\u5185\u5728\u5206\u89e3\uff09\u3002\u6b64\u65b9\u6cd5\u4ec5\u9700\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67b6\u6784\u8fdb\u884c\u5c11\u91cf\u4fee\u6539\uff0c\u5e76\u4f7f\u7528\u5c0f\u578b\u5408\u6210\u6570\u636e\u96c6\u5728\u5355\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Marigold\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Marigold\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff0c\u4e14\u5177\u6709\u9ad8\u6548\u7684\u8bad\u7ec3\u9700\u6c42\u548c\u51fa\u8272\u7684\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2505.08849", "pdf": "https://arxiv.org/pdf/2505.08849", "abs": "https://arxiv.org/abs/2505.08849", "authors": ["Keyu Chen", "Hao Tang", "Qinglin Liu", "Yizhao Xu"], "title": "Improved Algorithms for Differentially Private Language Model Alignment", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Language model alignment is crucial for ensuring that large language models\n(LLMs) align with human preferences, yet it often involves sensitive user data,\nraising significant privacy concerns. While prior work has integrated\ndifferential privacy (DP) with alignment techniques, their performance remains\nlimited. In this paper, we propose novel algorithms for privacy-preserving\nalignment and rigorously analyze their effectiveness across varying privacy\nbudgets and models. Our framework can be deployed on two celebrated alignment\ntechniques, namely direct preference optimization (DPO) and reinforcement\nlearning from human feedback (RLHF). Through systematic experiments on\nlarge-scale language models, we demonstrate that our approach achieves\nstate-of-the-art performance. Notably, one of our algorithms, DP-AdamW,\ncombined with DPO, surpasses existing methods, improving alignment quality by\nup to 15% under moderate privacy budgets ({\\epsilon}=2-5). We further\ninvestigate the interplay between privacy guarantees, alignment efficacy, and\ncomputational demands, providing practical guidelines for optimizing these\ntrade-offs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u7b97\u6cd5\uff0c\u7ed3\u5408DPO\u548cRLHF\u6280\u672f\uff0c\u5728\u9002\u5ea6\u7684\u9690\u79c1\u9884\u7b97\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002DP-AdamW\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9f50\u8d28\u91cf\uff08\u6700\u9ad8\u8fbe15%\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u9690\u79c1\u3001\u6548\u80fd\u4e0e\u8ba1\u7b97\u9700\u6c42\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5bf9\u9f50\u8fc7\u7a0b\u867d\u91cd\u8981\uff0c\u4f46\u6d89\u53ca\u654f\u611f\u7528\u6237\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u6709\u5de5\u4f5c\u5c06\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4e0e\u5bf9\u9f50\u6280\u672f\u7ed3\u5408\uff0c\u4f46\u5176\u6548\u679c\u4ecd\u6709\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548c\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e24\u79cd\u6280\u672f\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u8bc4\u4f30\u8fd9\u4e9b\u7b97\u6cd5\u5728\u4e0d\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5206\u6790DP-AdamW\u7b97\u6cd5\u7684\u6548\u679c\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u9002\u5ea6\u9690\u79c1\u9884\u7b97\u4e0b\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u5176\u4e2dDP-AdamW\u4e0eDPO\u7ed3\u5408\u53ef\u4f7f\u5bf9\u9f50\u8d28\u91cf\u63d0\u9ad8\u591a\u8fbe15%\u3002\u540c\u65f6\u660e\u786e\u4e86\u9690\u79c1\u4fdd\u969c\u3001\u5bf9\u9f50\u6548\u679c\u548c\u8ba1\u7b97\u9700\u6c42\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u524d\u63d0\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u8d28\u91cf\uff0c\u5e76\u4e3a\u4f18\u5316\u9690\u79c1\u4e0e\u6548\u80fd\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2505.09427", "pdf": "https://arxiv.org/pdf/2505.09427", "abs": "https://arxiv.org/abs/2505.09427", "authors": ["Achref Doula", "Max M\u00fchl\u00e4user", "Alejandro Sanchez Guinea"], "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.", "AI": {"tldr": "SafePath is a framework that enhances LLM-based path planning with safety guarantees, reducing uncertainty and collisions in autonomous driving.", "motivation": "To address the safety concerns of overconfidence and hallucinations in Large Language Models (LLMs) when applied to autonomous driving path planning.", "method": "SafePath operates in three stages: generating diverse candidate paths using an LLM, filtering out high-risk trajectories with conformal prediction while ensuring at least one safe option, and selecting the safest path or delegating to a human if uncertainty is too high.", "result": "Theoretically proven to guarantee a safe trajectory with a user-defined probability. Experiments on nuScenes and Highway-env show a 77% reduction in planning uncertainty and up to a 70% reduction in collision rates.", "conclusion": "SafePath effectively makes LLM-driven path planning safer by integrating formal safety guarantees, balancing autonomy and safety."}}
{"id": "2505.09368", "pdf": "https://arxiv.org/pdf/2505.09368", "abs": "https://arxiv.org/abs/2505.09368", "authors": ["Jenny Schmalfuss", "Victor Oei", "Lukas Mehl", "Madlen Bartsch", "Shashank Agnihotri", "Margret Keuper", "Andr\u00e9s Bruhn"], "title": "RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Standard benchmarks for optical flow, scene flow, and stereo vision\nalgorithms generally focus on model accuracy rather than robustness to image\ncorruptions like noise or rain. Hence, the resilience of models to such\nreal-world perturbations is largely unquantified. To address this, we present\nRobustSpring, a comprehensive dataset and benchmark for evaluating robustness\nto image corruptions for optical flow, scene flow, and stereo models.\nRobustSpring applies 20 different image corruptions, including noise, blur,\ncolor changes, quality degradations, and weather distortions, in a time-,\nstereo-, and depth-consistent manner to the high-resolution Spring dataset,\ncreating a suite of 20,000 corrupted images that reflect challenging\nconditions. RobustSpring enables comparisons of model robustness via a new\ncorruption robustness metric. Integration with the Spring benchmark enables\npublic two-axis evaluations of both accuracy and robustness. We benchmark a\ncurated selection of initial models, observing that accurate models are not\nnecessarily robust and that robustness varies widely by corruption type.\nRobustSpring is a new computer vision benchmark that treats robustness as a\nfirst-class citizen to foster models that combine accuracy with resilience. It\nwill be available at https://spring-benchmark.org.", "AI": {"tldr": "The paper introduces RobustSpring, a new dataset and benchmark for evaluating the robustness of optical flow, scene flow, and stereo models to image corruptions. It applies 20 different corruptions to the Spring dataset, creating 20,000 challenging images. The benchmark includes a new metric for measuring corruption robustness and allows public evaluation of both accuracy and robustness.", "motivation": "Current benchmarks for optical flow, scene flow, and stereo vision algorithms focus on model accuracy rather than robustness to real-world image corruptions like noise or rain.", "method": "RobustSpring applies 20 different image corruptions in a time-, stereo-, and depth-consistent manner to the high-resolution Spring dataset, generating a suite of 20,000 corrupted images. A new corruption robustness metric is introduced for comparing model robustness.", "result": "RobustSpring enables public two-axis evaluations of both accuracy and robustness via integration with the Spring benchmark. Initial model benchmarking shows that accurate models are not necessarily robust and robustness varies widely by corruption type.", "conclusion": "RobustSpring is a new computer vision benchmark treating robustness as a first-class citizen, fostering the development of models that combine accuracy with resilience."}}
{"id": "2505.09432", "pdf": "https://arxiv.org/pdf/2505.09432", "abs": "https://arxiv.org/abs/2505.09432", "authors": ["Yuzhou Cao", "Han Bao", "Lei Feng", "Bo An"], "title": "Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Surrogate regret bounds, also known as excess risk bounds, bridge the gap\nbetween the convergence rates of surrogate and target losses, with linear\nbounds favorable for their lossless regret transfer. While convex smooth\nsurrogate losses are appealing in particular due to the efficient estimation\nand optimization, the existence of a trade-off between the smoothness and\nlinear regret bound has been believed in the community. That being said, the\nbetter optimization and estimation properties of convex smooth surrogate losses\nmay inevitably deteriorate after undergoing the regret transfer onto a target\nloss. We overcome this dilemma for arbitrary discrete target losses by\nconstructing a convex smooth surrogate loss, which entails a linear surrogate\nregret bound composed with a tailored prediction link. The construction is\nbased on Fenchel-Young losses generated by the convolutional negentropy, which\nare equivalent to the infimal convolution of a generalized negentropy and the\ntarget Bayes risk. Consequently, the infimal convolution enables us to derive a\nsmooth loss while maintaining the surrogate regret bound linear. We\nadditionally benefit from the infimal convolution to have a consistent\nestimator of the underlying class probability. Our results are overall a novel\ndemonstration of how convex analysis penetrates into optimization and\nstatistical efficiency in risk minimization.", "AI": {"tldr": "The paper constructs a convex smooth surrogate loss with linear surrogate regret bound for arbitrary discrete target losses.", "motivation": "To overcome the trade-off between smoothness and linear regret bound for convex smooth surrogate losses.", "method": "Constructing a convex smooth surrogate loss based on Fenchel-Young losses generated by the convolutional negentropy, equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk.", "result": "Achieves a smooth loss while maintaining the surrogate regret bound linear, and provides a consistent estimator of the underlying class probability.", "conclusion": "Demonstrates how convex analysis contributes to optimization and statistical efficiency in risk minimization."}}
{"id": "2505.09372", "pdf": "https://arxiv.org/pdf/2505.09372", "abs": "https://arxiv.org/abs/2505.09372", "authors": ["Siyuan Yan", "Xieji Li", "Ming Hu", "Yiwen Jiang", "Zhen Yu", "Zongyuan Ge"], "title": "MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment", "categories": ["cs.CV"], "comment": "MICCAI2025 early acceptance; First two authors contribute equally", "summary": "Dermatological diagnosis represents a complex multimodal challenge that\nrequires integrating visual features with specialized clinical knowledge. While\nvision-language pretraining (VLP) has advanced medical AI, its effectiveness in\ndermatology is limited by text length constraints and the lack of structured\ntexts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced\nvision-language pretraining framework for zero-shot dermatological tasks.\nRecognizing that comprehensive dermatological descriptions require multiple\nknowledge aspects that exceed standard text constraints, our framework\nintroduces: (1) a multi-aspect contrastive learning strategy that decomposes\nclinical narratives into knowledge-enhanced sub-texts through large language\nmodels, (2) a fine-grained alignment mechanism that connects subcaptions with\ndiagnostically relevant image features, and (3) a diagnosis-guided weighting\nscheme that adaptively prioritizes different sub-captions based on clinical\nsignificance prior. Through pretraining on 403,563 dermatological image-text\npairs collected from education resources, MAKE significantly outperforms\nstate-of-the-art VLP models on eight datasets across zero-shot skin disease\nclassification, concept annotation, and cross-modal retrieval tasks. Our code\nwill be made publicly available at https: //github.com/SiyuanYan1/MAKE.", "AI": {"tldr": "Dermatological diagnosis is complex and requires combining visual features with clinical knowledge. Despite the advancements of vision-language pretraining (VLP) in medical AI, its application in dermatology is restricted by text length limitations and lack of structured texts. This paper presents MAKE, a Multi-Aspect Knowledge-Enhanced VLP framework for zero-shot dermatological tasks. It includes a multi-aspect contrastive learning strategy, a fine-grained alignment mechanism, and a diagnosis-guided weighting scheme. Pretrained on 403,563 image-text pairs, MAKE surpasses existing VLP models on various tasks.", "motivation": "The motivation behind this work is to overcome the limitations of current VLP models in dermatology, particularly addressing the issues of text length constraints and unstructured texts, which hinder effective dermatological diagnosis.", "method": "The method involves introducing MAKE, a framework that incorporates three key components: a multi-aspect contrastive learning strategy that breaks down clinical narratives into sub-texts using large language models, a fine-grained alignment mechanism that links subcaptions with diagnostic image features, and a diagnosis-guided weighting scheme that prioritizes sub-captions based on their clinical significance.", "result": "MAKE significantly outperforms state-of-the-art VLP models across eight datasets on tasks such as zero-shot skin disease classification, concept annotation, and cross-modal retrieval.", "conclusion": "The authors conclude that MAKE effectively addresses the challenges posed by dermatological diagnosis through its innovative approach, setting a new benchmark for zero-shot dermatological tasks."}}
{"id": "2505.08878", "pdf": "https://arxiv.org/pdf/2505.08878", "abs": "https://arxiv.org/abs/2505.08878", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Haim Permuter", "Flavio P. Calmon"], "title": "Optimized Couplings for Watermarking Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.IT", "math.IT"], "comment": "Accepted at ISIT25", "summary": "Large-language models (LLMs) are now able to produce text that is, in many\ncases, seemingly indistinguishable from human-generated content. This has\nfueled the development of watermarks that imprint a ``signal'' in LLM-generated\ntext with minimal perturbation of an LLM's output. This paper provides an\nanalysis of text watermarking in a one-shot setting. Through the lens of\nhypothesis testing with side information, we formulate and analyze the\nfundamental trade-off between watermark detection power and distortion in\ngenerated textual quality. We argue that a key component in watermark design is\ngenerating a coupling between the side information shared with the watermark\ndetector and a random partition of the LLM vocabulary. Our analysis identifies\nthe optimal coupling and randomization strategy under the worst-case LLM\nnext-token distribution that satisfies a min-entropy constraint. We provide a\nclosed-form expression of the resulting detection rate under the proposed\nscheme and quantify the cost in a max-min sense. Finally, we provide an array\nof numerical results, comparing the proposed scheme with the theoretical\noptimum and existing schemes, in both synthetic data and LLM watermarking. Our\ncode is available at https://github.com/Carol-Long/CC_Watermark", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u4e0e\u8f85\u52a9\u4fe1\u606f\u7684\u89c6\u89d2\uff0c\u5206\u6790\u4e86\u4e00\u6b21\u6027\u6587\u672c\u6c34\u5370\u4e2d\u7684\u57fa\u672c\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u7684\u6c34\u5370\u8bbe\u8ba1\u7b56\u7565\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u9645\u6548\u679c\u7684\u5bf9\u6bd4\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u51e0\u4e4e\u4e0e\u4eba\u7c7b\u751f\u6210\u7684\u5185\u5bb9\u65e0\u6cd5\u533a\u5206\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4e0d\u663e\u8457\u6539\u53d8\u8f93\u51fa\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u51fa\u8fd9\u4e9b\u6587\u672c\u662f\u7531LLM\u751f\u6210\u7684\u3002", "method": "\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u548c\u8f85\u52a9\u4fe1\u606f\u7684\u89c6\u89d2\uff0c\u7814\u7a76\u8005\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4ee5\u5206\u6790\u6587\u672c\u6c34\u5370\u4e2d\u68c0\u6d4b\u80fd\u529b\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002\u4ed6\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u952e\u7684\u8bbe\u8ba1\u7ec4\u4ef6\uff0c\u5373\u5728\u63d0\u4f9b\u7ed9\u6c34\u5370\u63a2\u6d4b\u5668\u7684\u8f85\u52a9\u4fe1\u606f\u548cLLM\u8bcd\u6c47\u8868\u7684\u968f\u673a\u5212\u5206\u4e4b\u95f4\u521b\u5efa\u8026\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u786e\u5b9a\u4e86\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u6ee1\u8db3\u6700\u5c0f\u71b5\u7ea6\u675f\u7684\u6700\u4f18\u8026\u5408\u548c\u968f\u673a\u5316\u7b56\u7565\u3002", "result": "\u7814\u7a76\u8005\u7ed9\u51fa\u4e86\u5728\u63d0\u51fa\u7684\u65b9\u6848\u4e0b\u68c0\u6d4b\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u4ee5\u6700\u5927\u6700\u5c0f\u7684\u65b9\u5f0f\u91cf\u5316\u4e86\u6210\u672c\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u5408\u6210\u6570\u636e\u548cLLM\u6c34\u5370\u4e0a\u5747\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6df1\u5165\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6587\u672c\u6c34\u5370\u8bbe\u8ba1\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6587\u672c\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u6c34\u5370\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2505.09379", "pdf": "https://arxiv.org/pdf/2505.09379", "abs": "https://arxiv.org/abs/2505.09379", "authors": ["Ali Rida Sahili", "Najett Neji", "Hedi Tabia"], "title": "Text-driven Motion Generation: Overview, Challenges and Directions", "categories": ["cs.CV"], "comment": "17 pages, 5 tables", "summary": "Text-driven motion generation offers a powerful and intuitive way to create\nhuman movements directly from natural language. By removing the need for\npredefined motion inputs, it provides a flexible and accessible approach to\ncontrolling animated characters. This makes it especially useful in areas like\nvirtual reality, gaming, human-computer interaction, and robotics. In this\nreview, we first revisit the traditional perspective on motion synthesis, where\nmodels focused on predicting future poses from observed initial sequences,\noften conditioned on action labels. We then provide a comprehensive and\nstructured survey of modern text-to-motion generation approaches, categorizing\nthem from two complementary perspectives: (i) architectural, dividing methods\ninto VAE-based, diffusion-based, and hybrid models; and (ii) motion\nrepresentation, distinguishing between discrete and continuous motion\ngeneration strategies. In addition, we explore the most widely used datasets,\nevaluation methods, and recent benchmarks that have shaped progress in this\narea. With this survey, we aim to capture where the field currently stands,\nbring attention to its key challenges and limitations, and highlight promising\ndirections for future exploration. We hope this work offers a valuable starting\npoint for researchers and practitioners working to push the boundaries of\nlanguage-driven human motion synthesis.", "AI": {"tldr": "\u6587\u672c\u9a71\u52a8\u7684\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u76f4\u89c2\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u521b\u5efa\u4eba\u7c7b\u52a8\u4f5c\u3002\u672c\u6587\u56de\u987e\u4e86\u4f20\u7edf\u7684\u8fd0\u52a8\u5408\u6210\u89c6\u89d2\uff0c\u5e76\u5bf9\u73b0\u4ee3\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u548c\u7ed3\u6784\u5316\u7684\u8c03\u67e5\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u6700\u5e38\u7528\u7684\u6570\u636e\u5e93\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u6700\u8fd1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u901a\u8fc7\u6587\u672c\u9a71\u52a8\u7684\u8fd0\u52a8\u751f\u6210\uff0c\u53ef\u4ee5\u63d0\u4f9b\u4e00\u79cd\u5f3a\u5927\u4e14\u76f4\u89c2\u7684\u65b9\u6cd5\u6765\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u521b\u5efa\u4eba\u7c7b\u52a8\u4f5c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u9700\u8981\u9884\u5b9a\u4e49\u7684\u8fd0\u52a8\u8f93\u5165\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6613\u8bbf\u95ee\u7684\u52a8\u753b\u89d2\u8272\u63a7\u5236\u65b9\u5f0f\u3002", "method": "\u4ece\u67b6\u6784\u548c\u8fd0\u52a8\u8868\u793a\u4e24\u4e2a\u4e92\u8865\u7684\u89d2\u5ea6\u5bf9\u73b0\u4ee3\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u548c\u8c03\u67e5\uff0c\u5305\u62ec\u57fa\u4e8eVAE\u3001\u6269\u6563\u548c\u6df7\u5408\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u79bb\u6563\u548c\u8fde\u7eed\u8fd0\u52a8\u751f\u6210\u7b56\u7565\u7684\u533a\u522b\u3002", "result": "\u672c\u6587\u5bf9\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\uff0c\u660e\u786e\u4e86\u5f53\u524d\u7684\u7814\u7a76\u72b6\u6001\uff0c\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u672a\u6765\u63a2\u7d22\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d77\u70b9\uff0c\u4ee5\u63a8\u52a8\u8bed\u8a00\u9a71\u52a8\u7684\u4eba\u7c7b\u8fd0\u52a8\u5408\u6210\u7684\u8fb9\u754c\u3002"}}
{"id": "2505.08894", "pdf": "https://arxiv.org/pdf/2505.08894", "abs": "https://arxiv.org/abs/2505.08894", "authors": ["Hiba Eltigani", "Rukhshan Haroon", "Asli Kocak", "Abdullah Bin Faisal", "Noah Martin", "Fahad Dogar"], "title": "WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in generative AI, such as ChatGPT, have transformed access to\ninformation in education, knowledge-seeking, and everyday decision-making.\nHowever, in many developing regions, access remains a challenge due to the\npersistent digital divide. To help bridge this gap, we developed WaLLM - a\ncustom AI chatbot over WhatsApp, a widely used communication platform in\ndeveloping regions. Beyond answering queries, WaLLM offers several features to\nenhance user engagement: a daily top question, suggested follow-up questions,\ntrending and recent queries, and a leaderboard-based reward system. Our service\nhas been operational for over 6 months, amassing over 14.7K queries from\napproximately 100 users. In this paper, we present WaLLM's design and a\nsystematic analysis of logs to understand user interactions. Our results show\nthat 55% of user queries seek factual information. \"Health and well-being\" was\nthe most popular topic (28%), including queries about nutrition and disease,\nsuggesting users view WaLLM as a reliable source. Two-thirds of users' activity\noccurred within 24 hours of the daily top question. Users who accessed the\n\"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by\ndiscussing implications for culture-based customization, user interface design,\nand appropriate calibration of users' trust in AI systems for developing\nregions.", "AI": {"tldr": "Recent advances in generative AI have transformed information access, but the digital divide persists in many developing regions. To address this, WaLLM - a custom AI chatbot on WhatsApp - was developed. It has features beyond answering queries and has been operational for over 6 months with significant user engagement. Analysis of user interactions shows that most queries seek factual information, especially on 'Health and well-being'. User activity is concentrated around the daily top question and those accessing the 'Leaderboard' feature interact more. The paper concludes with implications for customization, UI design, and trust calibration.", "motivation": "To bridge the gap in AI access in developing regions due to the persistent digital divide by creating an accessible AI chatbot on a widely used platform.", "method": "Development of WaLLM, a custom AI chatbot on WhatsApp, with features such as answering queries, daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Logs were analyzed systematically to understand user interactions.", "result": "WaLLM amassed over 14.7K queries from approximately 100 users over 6 months. 55% of queries sought factual information, with 'Health and well-being' being the most popular topic (28%). Two-thirds of user activity occurred within 24 hours of the daily top question, and users who accessed the 'Leaderboard' interacted 3x as much.", "conclusion": "The findings suggest implications for culture-based customization, user interface design, and appropriate calibration of users' trust in AI systems in developing regions."}}
{"id": "2505.09458", "pdf": "https://arxiv.org/pdf/2505.09458", "abs": "https://arxiv.org/abs/2505.09458", "authors": ["Jad Mounayer", "Alicia Tierz", "Jerome Tomezyk", "Chady Ghnatios", "Francisco Chinesta"], "title": "Variational Rank Reduction Autoencoder", "categories": ["cs.LG"], "comment": null, "summary": "Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a\nregularization on the latent space by applying a truncated SVD. While this\nregularization makes Autoencoders more powerful, using them for generative\npurposes is counter-intuitive due to their deterministic nature. On the other\nhand, Variational Autoencoders (VAEs) are well known for their generative\nabilities by learning a probabilistic latent space. In this paper, we present\nVariational Rank Reduction Autoencoders (VRRAEs), a model that leverages the\nadvantages of both RRAEs and VAEs. Our claims and results show that when\ncarefully sampling the latent space of RRAEs and further regularizing with the\nKullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs\nand VAEs. Additionally, we show that the regularization induced by the SVD not\nonly makes VRRAEs better generators than VAEs, but also reduces the possibility\nof posterior collapse. Our results include a synthetic dataset of a small size\nthat showcases the robustness of VRRAEs against collapse, and three real-world\ndatasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to\noutperform both VAEs and RRAEs on many random generation and interpolation\ntasks based on the FID score.", "AI": {"tldr": "The paper introduces Variational Rank Reduction Autoencoders (VRRAEs), which combines the strengths of Deterministic Rank Reduction Autoencoders (RRAEs) and Variational Autoencoders (VAEs). By applying careful sampling and KL divergence regularization to RRAEs' latent space, VRRAEs outperform both RRAEs and VAEs in random generation and interpolation tasks on synthetic and real-world datasets.", "motivation": "Autoencoders can be regularized by using a truncated Singular Value Decomposition (SVD) to limit the rank of their latent space. This method is powerful but deterministic, making it counter-intuitive for generative purposes. VAEs, however, are known for their generative capabilities due to their probabilistic latent space. The motivation is to create a model that leverages the advantages of both RRAEs and VAEs.", "method": "The authors propose VRRAEs, which combine the latent space regularization of RRAEs with the probabilistic approach of VAEs. They apply a truncated SVD for rank reduction and use KL divergence to further regularize the latent space, allowing for better generative performance.", "result": "VRRAEs outperform both RRAEs and VAEs in various tasks including random generation and interpolation, as measured by the FID score. The model shows robustness against posterior collapse, even on small synthetic datasets, and performs well on real-world datasets such as MNIST, CelebA, and CIFAR-10.", "conclusion": "By integrating the strengths of RRAEs and VAEs, VRRAEs provide superior generative performance and reduce the risk of posterior collapse. The findings indicate that VRRAEs are a promising advancement in the field of autoencoders."}}
{"id": "2505.09380", "pdf": "https://arxiv.org/pdf/2505.09380", "abs": "https://arxiv.org/abs/2505.09380", "authors": ["Qinghui Liu", "Jon Nesvold", "Hanna Raaum", "Elakkyen Murugesu", "Martin R\u00f8vang", "Bradley J Maclntosh", "Atle Bj\u00f8rnerud", "Karoline Skogen"], "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 11 figures, on submission to BMC Methods", "summary": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A pragmatic investigation was\ndeployed using clinical cases of patients presenting to the largest Emergency\nDepartment in Norway (site-1) with suspected traumatic brain injury (TBI) or\npatients with suspected stroke (site-2). We assessed ICH classification\nperformance as VIOLA-AI encountered new data and underwent pre-planned model\nretraining. Performance metrics included sensitivity, specificity, accuracy,\nand the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback.", "AI": {"tldr": "NeoMedSys, a radiology software platform, was evaluated for three months in real-world clinical settings. It enabled iterative improvements in an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage detection, significantly enhancing its diagnostic accuracy.", "motivation": "To evaluate the feasibility and effectiveness of running NeoMedSys in real-world clinical settings and to improve the performance of VIOLA-AI, an in-house developed AI model for intracranial hemorrhage detection.", "method": "NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. The study used clinical cases from two sites in Norway to assess ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining.", "result": "NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. The classification sensitivity rose to 90.3% (from 79.2%), and specificity reached 89.3% (from 80.7%). The AUC for bleed detection improved to 0.949 (from 0.873).", "conclusion": "The study concludes that NeoMedSys can enable efficient deployment and refinements of AI models in radiology, highlighting the value of real-time radiologist feedback."}}
{"id": "2505.09486", "pdf": "https://arxiv.org/pdf/2505.09486", "abs": "https://arxiv.org/abs/2505.09486", "authors": ["Seyed Roozbeh Razavi Rohani", "Khashayar Khajavi", "Wesley Chung", "Mo Chen", "Sharan Vaswani"], "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity Injection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in 4th Conference on Lifelong Learning Agents (CoLLAs), 2025", "summary": "Loss of plasticity in deep neural networks is the gradual reduction in a\nmodel's capacity to incrementally learn and has been identified as a key\nobstacle to learning in non-stationary problem settings. Recent work has shown\nthat deep linear networks tend to be resilient towards loss of plasticity.\nMotivated by this observation, we propose Adaptive Linearization (AdaLin), a\ngeneral approach that dynamically adapts each neuron's activation function to\nmitigate plasticity loss. Unlike prior methods that rely on regularization or\nperiodic resets, AdaLin equips every neuron with a learnable parameter and a\ngating mechanism that injects linearity into the activation function based on\nits gradient flow. This adaptive modulation ensures sufficient gradient signal\nand sustains continual learning without introducing additional hyperparameters\nor requiring explicit task boundaries. When used with conventional activation\nfunctions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can\nsignificantly improve performance on standard benchmarks, including Random\nLabel and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split\nCIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such\nas class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in\nmitigating plasticity loss in off-policy reinforcement learning agents. We\nperform a systematic set of ablations that show that neuron-level adaptation is\ncrucial for good performance and analyze a number of metrics in the network\nthat might be correlated to loss of plasticity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaLin\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u51fd\u6570\u6765\u7f13\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5851\u6027\u635f\u5931\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u8d85\u53c2\u6570\u6216\u660e\u786e\u7684\u4efb\u52a1\u8fb9\u754c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6765\u6e90\u4e8e\u8fd1\u671f\u53d1\u73b0\u6df1\u5c42\u7ebf\u6027\u7f51\u7edc\u5bf9\u5851\u6027\u635f\u5931\u5177\u6709\u8f83\u5f3a\u7684\u62b5\u6297\u529b\u3002\u4e3a\u4e86\u5728\u975e\u5e73\u7a33\u95ee\u9898\u8bbe\u5b9a\u4e0b\u6539\u5584\u6a21\u578b\u7684\u589e\u91cf\u5b66\u4e60\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u51cf\u8f7b\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5851\u6027\u635f\u5931\u3002", "method": "\u63d0\u51fa\u4e86Adaptive Linearization\uff08AdaLin\uff09\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u795e\u7ecf\u5143\u914d\u5907\u4e00\u4e2a\u53ef\u5b66\u4e60\u53c2\u6570\u548c\u95e8\u63a7\u673a\u5236\uff0c\u57fa\u4e8e\u68af\u5ea6\u6d41\u52a8\u5c06\u7ebf\u6027\u6ce8\u5165\u5230\u6fc0\u6d3b\u51fd\u6570\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u9002\u5e94\u8c03\u8282\uff0c\u786e\u4fdd\u8db3\u591f\u7684\u68af\u5ea6\u4fe1\u53f7\u5e76\u6301\u7eed\u8fdb\u884c\u8fde\u7eed\u5b66\u4e60\u3002", "result": "AdaLin\u4e0e\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\u3001Tanh\u548cGeLU\uff09\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982Random Label\u548cPermuted MNIST\u7b49\uff09\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u66f4\u590d\u6742\u7684\u573a\u666f\uff08\u5982CIFAR-100\u4e0a\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u548c\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4e2d\uff09\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "AdaLin\u662f\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8d85\u53c2\u6570\u6216\u65e0\u9700\u660e\u786e\u4efb\u52a1\u8fb9\u754c\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u7f13\u89e3\u5851\u6027\u635f\u5931\uff0c\u63d0\u5347\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u548c\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u7cfb\u7edf\u6027\u7684\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u9002\u5e94\u5bf9\u4e8e\u826f\u597d\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.09385", "pdf": "https://arxiv.org/pdf/2505.09385", "abs": "https://arxiv.org/abs/2505.09385", "authors": ["Xiaoyang Yu", "Xiaoming Wu", "Xin Wang", "Dongrun Li", "Ming Yang", "Peng Cheng"], "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem.", "AI": {"tldr": "Federated semantic segmentation is improved by a new framework FedSaaS which addresses class-consistency representation problem, enhancing average segmentation accuracy.", "motivation": "Existing federated semantic segmentation research often neglects fine-grained class relationships within the semantic space, leading to ambiguities in class representation when dealing with heterogeneous problems like domain shift.", "method": "The paper proposes FedSaaS, a novel federated segmentation framework that uses class exemplars as a criterion for both local- and global-level class representations. On the server side, class prototypes are modeled from uploaded exemplars to supervise the global branch of clients. On the client side, an adversarial mechanism harmonizes contributions of global and local branches. Multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space.", "result": "Extensive experiments on driving scene segmentation datasets show that the proposed framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy.", "conclusion": "FedSaaS effectively addresses the class-consistency representation problem in federated semantic segmentation, offering a significant improvement in performance."}}
{"id": "2505.08904", "pdf": "https://arxiv.org/pdf/2505.08904", "abs": "https://arxiv.org/abs/2505.08904", "authors": ["Varun Nagaraj Rao", "Samantha Dalal", "Andrew Schwartz", "Amna Liaqat", "Dana Calacci", "Andr\u00e9s Monroy-Hern\u00e1ndez"], "title": "FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "What happens when a rideshare driver is suddenly locked out of the platform\nconnecting them to riders, wages, and daily work? Deactivation-the abrupt\nremoval of gig workers' platform access-typically occurs through arbitrary AI\nand algorithmic decisions with little explanation or recourse. This represents\none of the most severe forms of algorithmic control and often devastates\nworkers' financial stability. Recent U.S. state policies now mandate appeals\nprocesses and recovering compensation during the period of wrongful\ndeactivation based on past earnings. Yet, labor organizers still lack effective\ntools to support these complex, error-prone workflows. We designed FareShare, a\ncomputational tool automating lost wage estimation for deactivated drivers,\nthrough a 6 month partnership with the State of Washington's largest rideshare\nlabor union. Over the following 3 months, our field deployment of FareShare\nregistered 178 account signups. We observed that the tool could reduce lost\nwage calculation time by over 95%, eliminate manual data entry errors, and\nenable legal teams to generate arbitration-ready reports more efficiently.\nBeyond these gains, the deployment also surfaced important socio-technical\nchallenges around trust, consent, and tool adoption in high-stakes labor\ncontexts.", "AI": {"tldr": "The paper introduces FareShare, a computational tool designed to automate lost wage estimation for deactivated rideshare drivers. Through a partnership with Washington's largest rideshare labor union, the deployment of FareShare showed significant reductions in calculation time and errors, while also highlighting socio-technical challenges.", "motivation": "Deactivation from gig work platforms can severely impact workers' financial stability due to arbitrary AI and algorithmic decisions. Current policies mandate appeals processes and compensation recovery, but lack effective tools to support these workflows.", "method": "The authors partnered with the State of Washington's largest rideshare labor union over 6 months to design FareShare. The tool was then deployed in the field for 3 months, registering 178 account signups.", "result": "FareShare reduced lost wage calculation time by over 95%, eliminated manual data entry errors, and enabled more efficient report generation for legal teams.", "conclusion": "While FareShare demonstrated significant improvements in efficiency and accuracy, the deployment also revealed important socio-technical challenges related to trust, consent, and tool adoption in high-stakes labor contexts."}}
{"id": "2505.09500", "pdf": "https://arxiv.org/pdf/2505.09500", "abs": "https://arxiv.org/abs/2505.09500", "authors": ["Timothy Qian", "Vinith Suriyakumar", "Ashia Wilson", "Dylan Hadfield-Menell"], "title": "Layered Unlearning for Adversarial Relearning", "categories": ["cs.LG"], "comment": "37 pages, 8 figures", "summary": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates.", "AI": {"tldr": "The paper explores how post-training methods modify language models, focusing on their brittleness. It proposes Layered Unlearning (LU), an unlearning algorithm that enhances robustness to adversarial relearning in language models.", "motivation": "To understand the modifications and brittleness caused by post-training methods such as fine-tuning, alignment, and unlearning in language models.", "method": "Design an unlearning algorithm called Layered Unlearning (LU) which creates distinct inhibitory mechanisms for subsets of data during different stages, limiting relearning abilities on parts of the dataset.", "result": "LU improves robustness to adversarial relearning for several different unlearning methods when evaluated through synthetic and large language model experiments.", "conclusion": "The study contributes to the state-of-the-art in machine unlearning and offers insights into the effects of post-training updates."}}
{"id": "2505.09406", "pdf": "https://arxiv.org/pdf/2505.09406", "abs": "https://arxiv.org/abs/2505.09406", "authors": ["Yue Wen", "Liang Song", "Yijia Liu", "Siting Zhu", "Yanzi Miao", "Lijun Han", "Hesheng Wang"], "title": "FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling", "categories": ["cs.CV"], "comment": "7 pages, 9 figures, accepted by ICRA2025", "summary": "Dynamic scene reconstruction for autonomous driving enables vehicles to\nperceive and interpret complex scene changes more precisely. Dynamic Neural\nRadiance Fields (NeRFs) have recently shown promising capability in scene\nmodeling. However, many existing methods rely heavily on accurate poses inputs\nand multi-sensor data, leading to increased system complexity. To address this,\nwe propose FreeDriveRF, which reconstructs dynamic driving scenes using only\nsequential RGB images without requiring poses inputs. We innovatively decouple\ndynamic and static parts at the early sampling level using semantic\nsupervision, mitigating image blurring and artifacts. To overcome the\nchallenges posed by object motion and occlusion in monocular camera, we\nintroduce a warped ray-guided dynamic object rendering consistency loss,\nutilizing optical flow to better constrain the dynamic modeling process.\nAdditionally, we incorporate estimated dynamic flow to constrain the pose\noptimization process, improving the stability and accuracy of unbounded scene\nreconstruction. Extensive experiments conducted on the KITTI and Waymo datasets\ndemonstrate the superior performance of our method in dynamic scene modeling\nfor autonomous driving.", "AI": {"tldr": "This paper proposes FreeDriveRF, a method for reconstructing dynamic driving scenes using only sequential RGB images without requiring poses inputs. It decouples dynamic and static parts at the early sampling level using semantic supervision and introduces a warped ray-guided dynamic object rendering consistency loss to better constrain the dynamic modeling process.", "motivation": "Dynamic scene reconstruction for autonomous driving is crucial for vehicles to perceive and interpret complex scene changes more precisely. However, existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity.", "method": "The proposed method, FreeDriveRF, reconstructs dynamic driving scenes using only sequential RGB images. It decouples dynamic and static parts at the early sampling level using semantic supervision and introduces a warped ray-guided dynamic object rendering consistency loss utilizing optical flow. Additionally, it incorporates estimated dynamic flow to constrain the pose optimization process.", "result": "Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of FreeDriveRF in dynamic scene modeling for autonomous driving.", "conclusion": "FreeDriveRF successfully reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs, showing superior performance in dynamic scene modeling for autonomous driving."}}
{"id": "2505.08916", "pdf": "https://arxiv.org/pdf/2505.08916", "abs": "https://arxiv.org/abs/2505.08916", "authors": ["Chan Le Duc", "Ludovic Brieulle"], "title": "A New Tractable Description Logic under Categorical Semantics", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "Biomedical ontologies contain numerous concept or role names involving\nnegative knowledge such as lacks_part, absence_of. Such a representation with\nlabels rather than logical constructors would not allow a reasoner to interpret\nlacks_part as a kind of negation of has_part. It is known that adding negation\nto the tractable Description Logic (DL) EL allowing for conjunction,\nexistential restriction and concept inclusion makes it intractable since the\nobtained logic includes implicitly disjunction and universal restriction which\ninteract with other constructors. In this paper, we propose a new extension of\nEL with a weakened negation allowing to represent negative knowledge while\nretaining tractability. To this end, we introduce categorical semantics of all\nlogical constructors of the DL SH including EL with disjunction, negation,\nuniversal restriction, role inclusion and transitive roles. The categorical\nsemantics of a logical constructor is usually described as a set of categorical\nproperties referring to several objects without using set membership. To\nrestore tractability, we have to weaken semantics of disjunction and universal\nrestriction by identifying \\emph{independent} categorical properties that are\nresponsible for intractability, and dropping them from the set of categorical\nproperties. We show that the logic resulting from weakening semantics is more\nexpressive than EL with the bottom concept, transitive roles and role\ninclusion.", "AI": {"tldr": "The paper proposes a new extension of EL with weakened negation to represent negative knowledge while maintaining tractability.", "motivation": "Biomedical ontologies contain numerous concept or role names involving negative knowledge such as lacks_part, absence_of. Current representation with labels rather than logical constructors do not allow a reasoner to interpret these as a kind of negation. Adding full negation makes the logic intractable.", "method": "Introduce categorical semantics of all logical constructors of the DL SH including EL with disjunction, negation, universal restriction, role inclusion and transitive roles. Weaken semantics of disjunction and universal restriction by identifying independent categorical properties that cause intractability and dropping them.", "result": "The resulting logic from weakening semantics is more expressive than EL with the bottom concept, transitive roles and role inclusion.", "conclusion": "A new extension of EL with weakened negation is proposed which allows to represent negative knowledge while retaining tractability."}}
{"id": "2505.09503", "pdf": "https://arxiv.org/pdf/2505.09503", "abs": "https://arxiv.org/abs/2505.09503", "authors": ["Patrik Kenfack", "Samira Ebrahimi Kahou", "Ulrich A\u00efvodji"], "title": "Towards Fair In-Context Learning with Tabular Foundation Models", "categories": ["cs.LG"], "comment": "24 pages, 10 figures, 4 tables", "summary": "Tabular foundational models have exhibited strong in-context learning (ICL)\ncapabilities on structured data, allowing them to make accurate predictions on\ntest sets without parameter updates, using training examples as context. This\nemerging approach positions itself as a competitive alternative to traditional\ngradient-boosted tree methods. However, while biases in conventional machine\nlearning models are well documented, it remains unclear how these biases\nmanifest in tabular ICL. The paper investigates the fairness implications of\ntabular ICL and explores three preprocessing strategies--correlation removal,\ngroup-balanced demonstration selection, and uncertainty-based demonstration\nselection--to address bias. Comprehensive experiments indicate that\nuncertainty-based demonstration selection consistently enhances group fairness\nof in-context predictions. The source code for reproducing the results of this\nwork can be found at https://github.com/patrikken/Fair-TabICL.", "AI": {"tldr": "This paper explores the fairness issues in tabular in-context learning (ICL) and proposes three preprocessing strategies to mitigate bias, finding that uncertainty-based demonstration selection improves group fairness.", "motivation": "The motivation of this paper is to understand and address potential biases in tabular in-context learning (ICL), which has emerged as a powerful method for making predictions on structured data without updating model parameters.", "method": "The authors investigate fairness implications of tabular ICL and explore three preprocessing strategies: correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection.", "result": "Experiments show that uncertainty-based demonstration selection consistently enhances the group fairness of in-context predictions.", "conclusion": "The study concludes that uncertainty-based demonstration selection is an effective strategy to improve fairness in tabular ICL."}}
{"id": "2505.09413", "pdf": "https://arxiv.org/pdf/2505.09413", "abs": "https://arxiv.org/abs/2505.09413", "authors": ["Ma Changfeng", "Bi Ran", "Guo Jie", "Wang Chongjun", "Guo Yanwen"], "title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians", "categories": ["cs.CV"], "comment": "CVPR 2025 Accepted", "summary": "Current learning-based methods predict NeRF or 3D Gaussians from point clouds\nto achieve photo-realistic rendering but still depend on categorical priors,\ndense point clouds, or additional refinements. Hence, we introduce a novel\npoint cloud rendering method by predicting 2D Gaussians from point clouds. Our\nmethod incorporates two identical modules with an entire-patch architecture\nenabling the network to be generalized to multiple datasets. The module\nnormalizes and initializes the Gaussians utilizing the point cloud information\nincluding normals, colors and distances. Then, splitting decoders are employed\nto refine the initial Gaussians by duplicating them and predicting more\naccurate results, making our methodology effectively accommodate sparse point\nclouds as well. Once trained, our approach exhibits direct generalization to\npoint clouds across different categories. The predicted Gaussians are employed\ndirectly for rendering without additional refinement on the rendered images,\nretaining the benefits of 2D Gaussians. We conduct extensive experiments on\nvarious datasets, and the results demonstrate the superiority and\ngeneralization of our method, which achieves SOTA performance. The code is\navailable at\nhttps://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.", "AI": {"tldr": "This paper introduces a new point cloud rendering method that predicts 2D Gaussians from point clouds, which can generalize to multiple datasets and categories without needing categorical priors, dense point clouds, or additional refinements.", "motivation": "Current learning-based methods for rendering rely on categorical priors, dense point clouds, or extra refinements, which limits their flexibility and generalization ability.", "method": "The method uses two identical modules with an entire-patch architecture. These modules normalize and initialize 2D Gaussians based on point cloud information (normals, colors, distances). Splitting decoders then refine these initial Gaussians by duplicating them for more accurate predictions. This approach accommodates sparse point clouds effectively.", "result": "Extensive experiments on various datasets show that this method outperforms existing techniques in terms of superiority and generalization, achieving state-of-the-art (SOTA) performance.", "conclusion": "The proposed method provides a novel way to render point clouds using 2D Gaussian prediction, offering direct generalization across different categories without requiring dense point clouds or additional image refinements."}}
{"id": "2505.08918", "pdf": "https://arxiv.org/pdf/2505.08918", "abs": "https://arxiv.org/abs/2505.08918", "authors": ["Marina Popova", "Iaroslav Chelombitko", "Aleksey Komissarov"], "title": "When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes", "categories": ["q-bio.GN", "cs.AI"], "comment": "ICLR 2025 Workshop on Machine Learning for Genomics Explorations", "summary": "The emergence of telomere-to-telomere (T2T) genome assemblies has opened new\navenues for comparative genomics, yet effective tokenization strategies for\ngenomic sequences remain underexplored. In this pilot study, we apply Byte Pair\nEncoding (BPE) to nine T2T primate genomes including three human assemblies by\ntraining independent BPE tokenizers with a fixed vocabulary of 512,000 tokens\nusing our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are\nshared across all assemblies, while nearly 991,854 tokens are unique to a\nsingle genome, indicating a rapid decline in shared vocabulary with increasing\nassembly comparisons. Moreover, phylogenetic trees derived from token overlap\nfailed to recapitulate established primate relationships, a discrepancy\nattributed to the disproportionate influence of species-specific high-copy\nrepetitive elements. These findings underscore the dual nature of BPE\ntokenization: while it effectively compresses repetitive sequences, its\nsensitivity to high-copy elements limits its utility as a universal tool for\ncomparative genomics. We discuss potential hybrid strategies and repeat-masking\napproaches to refine genomic tokenization, emphasizing the need for\ndomain-specific adaptations in the development of large-scale genomic language\nmodels. The dnaBPE tool used in this study is open-source and available at\nhttps://github.com/aglabx/dnaBPE.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5e94\u7528\u5b57\u8282\u5bf9\u7f16\u7801\uff08BPE\uff09\u6280\u672f\u4e8e\u4e5d\u4e2a\u7aef\u7c92\u5230\u7aef\u7c92\uff08T2T\uff09\u7075\u957f\u7c7b\u52a8\u7269\u57fa\u56e0\u7ec4\uff0c\u5305\u62ec\u4e09\u4e2a\u5b8c\u6574\u4eba\u7c7b\u57fa\u56e0\u7ec4\u3002\u4ed6\u4eec\u53d1\u73b0\u4ec5\u6709\u5c11\u91cf\u7684token\u5728\u6240\u6709\u57fa\u56e0\u7ec4\u4e2d\u5171\u4eab\uff0c\u5e76\u4e14\u57fa\u4e8etoken\u91cd\u53e0\u6784\u5efa\u7684\u7cfb\u7edf\u53d1\u80b2\u6811\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u5df2\u77e5\u7684\u7075\u957f\u7c7b\u52a8\u7269\u5173\u7cfb\u3002\u8fd9\u5f52\u56e0\u4e8e\u7269\u79cd\u7279\u5f02\u6027\u9ad8\u62f7\u8d1d\u91cd\u590d\u5143\u7d20\u7684\u5f71\u54cd\u3002\u7814\u7a76\u5f3a\u8c03\u4e86BPE\u6280\u672f\u5728\u538b\u7f29\u91cd\u590d\u5e8f\u5217\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4f46\u540c\u65f6\u6307\u51fa\u5176\u4f5c\u4e3a\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u901a\u7528\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1T2T\u57fa\u56e0\u7ec4\u7ec4\u88c5\u7684\u51fa\u73b0\u4e3a\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u57fa\u56e0\u7ec4\u5e8f\u5217\u7684\u6709\u6548\u6807\u8bb0\u5316\u7b56\u7565\u4ecd\u9700\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u5de5\u5177dnaBPE\uff0c\u8bad\u7ec3\u72ec\u7acb\u7684BPE\u6807\u8bb0\u5668\u5bf9\u4e5d\u4e2aT2T\u7075\u957f\u7c7b\u52a8\u7269\u57fa\u56e0\u7ec4\u8fdb\u884c\u5206\u6790\uff0c\u6bcf\u4e2a\u6807\u8bb0\u5668\u5177\u6709512,000\u4e2a\u56fa\u5b9a\u8bcd\u6c47\u91cf\u3002", "result": "\u53ea\u670911,569\u4e2atoken\u5728\u6240\u6709\u57fa\u56e0\u7ec4\u7ec4\u88c5\u4e2d\u5171\u4eab\uff0c\u800c\u7ea6991,854\u4e2atoken\u4ec5\u5728\u4e00\u4e2a\u57fa\u56e0\u7ec4\u4e2d\u72ec\u6709\u3002\u57fa\u4e8etoken\u91cd\u53e0\u6784\u5efa\u7684\u7cfb\u7edf\u53d1\u80b2\u6811\u672a\u80fd\u91cd\u73b0\u5df2\u77e5\u7684\u7075\u957f\u7c7b\u52a8\u7269\u5173\u7cfb\u3002", "conclusion": "BPE\u6807\u8bb0\u5316\u6280\u672f\u80fd\u6709\u6548\u538b\u7f29\u91cd\u590d\u5e8f\u5217\uff0c\u4f46\u53d7\u9ad8\u62f7\u8d1d\u91cd\u590d\u5143\u7d20\u5f71\u54cd\uff0c\u4e0d\u9002\u5408\u4f5c\u4e3a\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u7684\u901a\u7528\u5de5\u5177\u3002\u9700\u8981\u91c7\u7528\u6df7\u5408\u7b56\u7565\u548c\u91cd\u590d\u5c4f\u853d\u65b9\u6cd5\u6765\u6539\u8fdb\u57fa\u56e0\u7ec4\u6807\u8bb0\u5316\uff0c\u5f00\u53d1\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u8bed\u8a00\u6a21\u578b\u65f6\u5e94\u8003\u8651\u9886\u57df\u7279\u5b9a\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2505.09572", "pdf": "https://arxiv.org/pdf/2505.09572", "abs": "https://arxiv.org/abs/2505.09572", "authors": ["Julian Kranz", "Davide Gallon", "Steffen Dereich", "Arnulf Jentzen"], "title": "SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures", "categories": ["cs.LG", "math.LO", "math.OC", "stat.ML", "Primary 68T05, Secondary 68T07, 26B40, 03C64, 03C98"], "comment": "27 pages, 4 figures", "summary": "We study gradient flows for loss landscapes of fully connected feed forward\nneural networks with commonly used continuously differentiable activation\nfunctions such as the logistic, hyperbolic tangent, softplus or GELU function.\nWe prove that the gradient flow either converges to a critical point or\ndiverges to infinity while the loss converges to an asymptotic critical value.\nMoreover, we prove the existence of a threshold $\\varepsilon>0$ such that the\nloss value of any gradient flow initialized at most $\\varepsilon$ above the\noptimal level converges to it. For polynomial target functions and sufficiently\nbig architecture and data set, we prove that the optimal loss value is zero and\ncan only be realized asymptotically. From this setting, we deduce our main\nresult that any gradient flow with sufficiently good initialization diverges to\ninfinity. Our proof heavily relies on the geometry of o-minimal structures. We\nconfirm these theoretical findings with numerical experiments and extend our\ninvestigation to real-world scenarios, where we observe an analogous behavior.", "AI": {"tldr": "The paper investigates gradient flows in fully connected feedforward neural networks with smooth activation functions, showing that the gradient flow either converges to a critical point or diverges to infinity while the loss approaches an asymptotic critical value. For well-initialized models, the loss can converge to zero for polynomial target functions given sufficient architecture and data. However, in practical settings, gradient flows often diverge to infinity.", "motivation": "To understand the behavior of gradient flows in the context of loss landscapes for neural networks with smooth activation functions.", "method": "Analysis of gradient flows in fully connected feedforward neural networks using continuously differentiable activation functions. Theoretical proofs regarding convergence or divergence of gradient flows, and numerical experiments confirming these findings in real-world scenarios.", "result": "Gradient flows either converge to a critical point or diverge to infinity, with the loss approaching an asymptotic critical value. For polynomial target functions, optimal loss is zero but only achievable asymptotically. In practice, gradient flows with good initialization tend to diverge to infinity.", "conclusion": "Gradient flows in neural networks exhibit specific convergence or divergence behaviors, depending on initialization and target function complexity. The theoretical results are supported by numerical experiments."}}
{"id": "2505.09415", "pdf": "https://arxiv.org/pdf/2505.09415", "abs": "https://arxiv.org/abs/2505.09415", "authors": ["Hongyang Wang", "Yichen Shi", "Zhuofu Tao", "Yuhao Gao", "Liepiao Zhang", "Xun Lin", "Jun Feng", "Xiaochen Yuan", "Zitong Yu", "Xiaochun Cao"], "title": "FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing (FAS) is crucial for protecting facial recognition systems\nfrom presentation attacks. Previous methods approached this task as a\nclassification problem, lacking interpretability and reasoning behind the\npredicted results. Recently, multimodal large language models (MLLMs) have\nshown strong capabilities in perception, reasoning, and decision-making in\nvisual tasks. However, there is currently no universal and comprehensive MLLM\nand dataset specifically designed for FAS task. To address this gap, we propose\nFaceShield, a MLLM for FAS, along with the corresponding pre-training and\nsupervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.\nFaceShield is capable of determining the authenticity of faces, identifying\ntypes of spoofing attacks, providing reasoning for its judgments, and detecting\nattack areas. Specifically, we employ spoof-aware vision perception (SAVP) that\nincorporates both the original image and auxiliary information based on prior\nknowledge. We then use an prompt-guided vision token masking (PVTM) strategy to\nrandom mask vision tokens, thereby improving the model's generalization\nability. We conducted extensive experiments on three benchmark datasets,\ndemonstrating that FaceShield significantly outperforms previous deep learning\nmodels and general MLLMs on four FAS tasks, i.e., coarse-grained\nclassification, fine-grained classification, reasoning, and attack\nlocalization. Our instruction datasets, protocols, and codes will be released\nsoon.", "AI": {"tldr": "\u63d0\u51faFaceShield\uff0c\u4e00\u79cd\u7528\u4e8e\u9762\u90e8\u9632\u6b3a\u9a97\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u3002\u8be5\u6a21\u578b\u5728\u56db\u4e2a\u9762\u90e8\u9632\u6b3a\u9a97\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4ee5\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u901a\u7528MLLMs\u3002", "motivation": "\u73b0\u6709\u7684\u9762\u90e8\u9632\u6b3a\u9a97\u65b9\u6cd5\u5c06\u4efb\u52a1\u89c6\u4e3a\u5206\u7c7b\u95ee\u9898\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u7ed3\u679c\u80cc\u540e\u7684\u63a8\u7406\u3002\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u8fd8\u6ca1\u6709\u4e13\u95e8\u4e3a\u9762\u90e8\u9632\u6b3a\u9a97\u4efb\u52a1\u8bbe\u8ba1\u7684\u901a\u7528\u548c\u5168\u9762\u7684MLLM\u548c\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86FaceShield\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u9762\u90e8\u9632\u6b3a\u9a97\u7684MLLM\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6FaceShield-pre10K\u548cFaceShield-sft45K\u3002\u4f7f\u7528spoof-aware vision perception (SAVP)\uff0c\u7ed3\u5408\u539f\u59cb\u56fe\u50cf\u548c\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u8f85\u52a9\u4fe1\u606f\uff0c\u5e76\u91c7\u7528prompt-guided vision token masking (PVTM)\u7b56\u7565\u6765\u968f\u673a\u906e\u853d\u89c6\u89c9token\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eFaceShield\u5728\u56db\u4e2a\u9762\u90e8\u9632\u6b3a\u9a97\u4efb\u52a1\uff08\u5373\u7c97\u7c92\u5ea6\u5206\u7c7b\u3001\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3001\u63a8\u7406\u548c\u653b\u51fb\u5b9a\u4f4d\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u4ee5\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u901a\u7528MLLMs\u3002", "conclusion": "FaceShield\u4e3a\u9762\u90e8\u9632\u6b3a\u9a97\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5224\u65ad\u9762\u90e8\u771f\u5b9e\u6027\u3001\u8bc6\u522b\u6b3a\u9a97\u653b\u51fb\u7c7b\u578b\u3001\u63d0\u4f9b\u63a8\u7406\u4f9d\u636e\u548c\u68c0\u6d4b\u653b\u51fb\u533a\u57df\u7684\u80fd\u529b\u3002"}}
{"id": "2505.08919", "pdf": "https://arxiv.org/pdf/2505.08919", "abs": "https://arxiv.org/abs/2505.08919", "authors": ["Kangxian Xie", "Yufei Zhu", "Kaiming Kuang", "Li Zhang", "Hongwei Bran Li", "Mingchen Gao", "Jiancheng Yang"], "title": "Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "In revision process", "summary": "High-quality 3D reconstruction of pulmonary segments plays a crucial role in\nsegmentectomy and surgical treatment planning for lung cancer. Due to the\nresolution requirement of the target reconstruction, conventional deep\nlearning-based methods often suffer from computational resource constraints or\nlimited granularity. Conversely, implicit modeling is favored due to its\ncomputational efficiency and continuous representation at any resolution. We\npropose a neural implicit function-based method to learn a 3D surface to\nachieve anatomy-aware, precise pulmonary segment reconstruction, represented as\na shape by deforming a learnable template. Additionally, we introduce two\nclinically relevant evaluation metrics to assess the reconstruction\ncomprehensively. Further, due to the absence of publicly available shape\ndatasets to benchmark reconstruction algorithms, we developed a shape dataset\nnamed Lung3D, including the 3D models of 800 labeled pulmonary segments and the\ncorresponding airways, arteries, veins, and intersegmental veins. We\ndemonstrate that the proposed approach outperforms existing methods, providing\na new perspective for pulmonary segment reconstruction. Code and data will be\navailable at https://github.com/M3DV/ImPulSe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e603D\u8868\u9762\u4ee5\u5b9e\u73b0\u89e3\u5256\u5b66\u611f\u77e5\u7684\u7cbe\u786e\u80ba\u6bb5\u91cd\u5efa\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u4e34\u5e8a\u76f8\u5173\u7684\u8bc4\u4f30\u6307\u6807\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLung3D\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b800\u4e2a\u6807\u8bb0\u7684\u80ba\u6bb5\u53ca\u5176\u5bf9\u5e94\u7684\u6c14\u9053\u3001\u52a8\u8109\u3001\u9759\u8109\u548c\u6bb5\u95f4\u9759\u8109\u76843D\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u8d28\u91cf\u76843D\u80ba\u6bb5\u91cd\u5efa\u5728\u80ba\u6bb5\u5207\u9664\u672f\u548c\u80ba\u764c\u624b\u672f\u6cbb\u7597\u8ba1\u5212\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7531\u4e8e\u76ee\u6807\u91cd\u5efa\u7684\u5206\u8fa8\u7387\u8981\u6c42\uff0c\u5e38\u5e38\u53d7\u5230\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u6216\u7c92\u5ea6\u6709\u9650\u7684\u95ee\u9898\u3002\u800c\u9690\u5f0f\u5efa\u6a21\u56e0\u5176\u8ba1\u7b97\u6548\u7387\u548c\u8fde\u7eed\u8868\u793a\u7684\u4f18\u52bf\u53d7\u5230\u9752\u7750\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e603D\u8868\u9762\u6765\u5b9e\u73b0\u89e3\u5256\u5b66\u611f\u77e5\u7684\u7cbe\u786e\u80ba\u6bb5\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53d8\u5f62\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u6a21\u677f\u6765\u8868\u793a\u5f62\u72b6\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e24\u4e2a\u4e34\u5e8a\u76f8\u5173\u7684\u8bc4\u4f30\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u91cd\u5efa\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u80ba\u6bb5\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u51fd\u6570\u7684\u80ba\u6bb5\u91cd\u5efa\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u80ba\u6bb5\u91cd\u5efa\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.09586", "pdf": "https://arxiv.org/pdf/2505.09586", "abs": "https://arxiv.org/abs/2505.09586", "authors": ["Yipeng Zhang", "Longlong Li", "Kelin Xia"], "title": "Rhomboid Tiling for Geometric Graph Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have proven effective for learning from\ngraph-structured data through their neighborhood-based message passing\nframework. Many hierarchical graph clustering pooling methods modify this\nframework by introducing clustering-based strategies, enabling the construction\nof more expressive and powerful models. However, all of these message passing\nframework heavily rely on the connectivity structure of graphs, limiting their\nability to capture the rich geometric features inherent in geometric graphs. To\naddress this, we propose Rhomboid Tiling (RT) clustering, a novel clustering\nmethod based on the rhomboid tiling structure, which performs clustering by\nleveraging the complex geometric information of the data and effectively\nextracts its higher-order geometric structures. Moreover, we design RTPool, a\nhierarchical graph clustering pooling model based on RT clustering for graph\nclassification tasks. The proposed model demonstrates superior performance,\noutperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.", "AI": {"tldr": "The paper introduces Rhomboid Tiling (RT) clustering and RTPool, a hierarchical graph clustering pooling model, which leverages complex geometric information to extract higher-order geometric structures. It outperforms 21 state-of-the-art competitors on 7 benchmark datasets for graph classification tasks.", "motivation": "Current message passing frameworks in Graph Neural Networks rely heavily on the connectivity structure of graphs, limiting their ability to capture rich geometric features inherent in geometric graphs.", "method": "The authors propose Rhomboid Tiling (RT) clustering, a novel method based on the rhomboid tiling structure that clusters data leveraging its complex geometric information. They also design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks.", "result": "RTPool demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.", "conclusion": "Rhomboid Tiling (RT) clustering and RTPool effectively extract higher-order geometric structures and perform exceptionally well in graph classification tasks."}}
{"id": "2505.09422", "pdf": "https://arxiv.org/pdf/2505.09422", "abs": "https://arxiv.org/abs/2505.09422", "authors": ["Xiangyuan Peng", "Yu Wang", "Miao Tang", "Bierzynski Kay", "Lorenzo Servadei", "Robert Wille"], "title": "MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Reliable autonomous driving systems require accurate detection of traffic\nparticipants. To this end, multi-modal fusion has emerged as an effective\nstrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame\nradar point clouds have demonstrated the effectiveness in bridging the point\ndensity gap. However, they often neglect radar point clouds' inter-frame\nmisalignment caused by object movement during accumulation and do not fully\nexploit the object dynamic information from 4D radar. In this paper, we propose\nMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for\nrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is\ndesigned to compensate for inter-frame radar misalignment from moving objects.\nLater, a Motion Attention Gated Fusion (MAGF) module integrate radar motion\nfeatures to guide LiDAR features to focus on dynamic foreground objects.\nExtensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL\noutperforms existing methods, achieving the highest mAP of 73.30% in the entire\narea and 88.68% in the driving corridor. Notably, our method also achieves the\nbest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in\nthe driving corridor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoRAL\u7684\u8fd0\u52a8\u611f\u77e5\u591a\u5e274D\u96f7\u8fbe\u548cLiDAR\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7Motion-aware Radar Encoder\uff08MRE\uff09\u8865\u507f\u79fb\u52a8\u7269\u4f53\u7684\u5e27\u95f4\u96f7\u8fbe\u9519\u4f4d\uff0c\u5e76\u5229\u7528Motion Attention Gated Fusion\uff08MAGF\uff09\u6a21\u5757\u5c06\u96f7\u8fbe\u8fd0\u52a8\u7279\u5f81\u4e0eLiDAR\u7279\u5f81\u7ed3\u5408\u4ee5\u805a\u7126\u52a8\u6001\u524d\u666f\u7269\u4f53\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoRAL\u5728View-of-Delft\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6574\u4f53\u533a\u57df\u548c\u9a7e\u9a76\u8d70\u5eca\u4e2d\u7684\u6700\u9ad8mAP\u5206\u522b\u4e3a73.30%\u548c88.68%\uff0c\u5e76\u5bf9\u884c\u4eba\u548c\u9a91\u81ea\u884c\u8f66\u8005\u6709\u6700\u4f73\u68c0\u6d4b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u591a\u5e27\u96f7\u8fbe\u70b9\u4e91\u76844D\u96f7\u8fbe\u548cLiDAR\u878d\u5408\u65b9\u6cd5\u867d\u7136\u6709\u6548\u7f29\u5c0f\u4e86\u70b9\u5bc6\u5ea6\u5dee\u8ddd\uff0c\u4f46\u5e38\u5e38\u5ffd\u7565\u7531\u4e8e\u7269\u4f53\u8fd0\u52a8\u5bfc\u81f4\u7684\u5e27\u95f4\u96f7\u8fbe\u70b9\u4e91\u9519\u4f4d\u95ee\u9898\uff0c\u4e14\u672a\u5145\u5206\u5229\u75284D\u96f7\u8fbe\u63d0\u4f9b\u7684\u7269\u4f53\u52a8\u6001\u4fe1\u606f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fd0\u52a8\u611f\u77e5\u96f7\u8fbe\u7f16\u7801\u5668\uff08MRE\uff09\uff0c\u7528\u4e8e\u8865\u507f\u6765\u81ea\u79fb\u52a8\u7269\u4f53\u7684\u5e27\u95f4\u96f7\u8fbe\u9519\u4f4d\uff1b\u7136\u540e\u91c7\u7528\u8fd0\u52a8\u6ce8\u610f\u95e8\u63a7\u878d\u5408\uff08MAGF\uff09\u6a21\u5757\uff0c\u5c06\u96f7\u8fbe\u8fd0\u52a8\u7279\u5f81\u6574\u5408\u8fdb\u6765\uff0c\u5f15\u5bfcLiDAR\u7279\u5f81\u5173\u6ce8\u52a8\u6001\u524d\u666f\u7269\u4f53\u3002", "result": "\u5728View-of-Delft\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cMoRAL\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6574\u4f53\u533a\u57df\u7684\u6700\u9ad8mAP\u4e3a73.30%\uff0c\u9a7e\u9a76\u8d70\u5eca\u4e3a88.68%\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u884c\u4eba\u7684AP\u4e3a69.67%\uff0c\u9a7e\u9a76\u8d70\u5eca\u4e2d\u9a91\u81ea\u884c\u8f66\u8005\u7684AP\u4e3a96.25%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MoRAL\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u52a8\u6001\u524d\u666f\u7269\u4f53\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6240\u9700\u7684\u4ea4\u901a\u53c2\u4e0e\u8005\u51c6\u786e\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.08939", "pdf": "https://arxiv.org/pdf/2505.08939", "abs": "https://arxiv.org/abs/2505.08939", "authors": ["Suchismita Naik", "Prakash Shukla", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parsons"], "title": "Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work", "categories": ["cs.HC", "cs.AI"], "comment": "5 pages, 2 Tables, In Creativity and Cognition 2025, June 23--25,\n  2025, Virtual, United Kingdom", "summary": "As generative AI tools become integrated into design workflows, students\nincreasingly engage with these tools not just as aids, but as collaborators.\nThis study analyzes reflections from 33 student teams in an HCI design course\nto examine the kinds of judgments students make when using AI tools. We found\nboth established forms of design judgment (e.g., instrumental, appreciative,\nquality) and emergent types: agency-distribution judgment and reliability\njudgment. These new forms capture how students negotiate creative\nresponsibility with AI and assess the trustworthiness of its outputs. Our\nfindings suggest that generative AI introduces new layers of complexity into\ndesign reasoning, prompting students to reflect not only on what AI produces,\nbut also on how and when to rely on it. By foregrounding these judgments, we\noffer a conceptual lens for understanding how students engage in co-creative\nsensemaking with AI in design contexts.", "AI": {"tldr": "This study analyzes reflections from 33 student teams using AI tools in an HCI design course, identifying new types of design judgments.", "motivation": "To understand the kinds of judgments students make when engaging with AI tools as collaborators rather than just aids.", "method": "Analyzed reflections from 33 student teams in an HCI design course who used AI tools.", "result": "Identified established and emergent types of design judgment including agency-distribution and reliability judgment.", "conclusion": "Generative AI adds complexity to design reasoning, prompting reflection on AI's outputs and when to rely on it; a conceptual lens is provided for understanding co-creative sensemaking with AI."}}
{"id": "2505.09593", "pdf": "https://arxiv.org/pdf/2505.09593", "abs": "https://arxiv.org/abs/2505.09593", "authors": ["Filippo Leveni", "Guilherme Weigert Cassales", "Bernhard Pfahringer", "Albert Bifet", "Giacomo Boracchi"], "title": "Online Isolation Forest", "categories": ["cs.LG"], "comment": "Accepted at International Conference on Machine Learning (ICML 2024)", "summary": "The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.", "AI": {"tldr": "The paper presents Online-iForest, a new method for anomaly detection in streaming data that performs well compared to other online methods and offline techniques with periodic retraining, while being more efficient.", "motivation": "Current anomaly detection methods, both offline and online, are not fully suitable for streaming contexts due to impractical assumptions or the need for periodic retraining.", "method": "Online-iForest is a novel anomaly detection method designed specifically for streaming data conditions. It tracks the evolving data generating process over time without requiring periodic retraining.", "result": "Experimental results on real-world datasets show that Online-iForest performs comparably to other online methods and offline techniques with periodic retraining, but it outperforms them in terms of efficiency.", "conclusion": "Online-iForest is a promising solution for applications requiring fast anomaly detection, such as cybersecurity, fraud, and fault detection."}}
{"id": "2505.09433", "pdf": "https://arxiv.org/pdf/2505.09433", "abs": "https://arxiv.org/abs/2505.09433", "authors": ["Jiahao Zhu", "Kang You", "Dandan Ding", "Zhan Ma"], "title": "Efficient LiDAR Reflectance Compression via Scanning Serialization", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Reflectance attributes in LiDAR point clouds provide essential information\nfor downstream tasks but remain underexplored in neural compression methods. To\naddress this, we introduce SerLiC, a serialization-based neural compression\nframework to fully exploit the intrinsic characteristics of LiDAR reflectance.\nSerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order\nserialization, offering a device-centric perspective for reflectance analysis.\nEach point is then tokenized into a contextual representation comprising its\nsensor scanning index, radial distance, and prior reflectance, for effective\ndependencies exploration. For efficient sequential modeling, Mamba is\nincorporated with a dual parallelization scheme, enabling simultaneous\nautoregressive dependency capture and fast processing. Extensive experiments\ndemonstrate that SerLiC attains over 2x volume reduction against the original\nreflectance data, outperforming the state-of-the-art method by up to 22%\nreduction of compressed bits while using only 2% of its parameters. Moreover, a\nlightweight version of SerLiC achieves > 10 fps (frames per second) with just\n111K parameters, which is attractive for real-world applications.", "AI": {"tldr": "SerLiC is a serialization-based neural compression framework that transforms 3D LiDAR point clouds into 1D sequences for effective reflectance analysis, achieving significant volume reduction and outperforming state-of-the-art methods.", "motivation": "Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods.", "method": "SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization. Each point is tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance. Mamba is incorporated with a dual parallelization scheme for efficient sequential modeling.", "result": "Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters. A lightweight version of SerLiC achieves > 10 fps with just 111K parameters.", "conclusion": "SerLiC provides an effective approach to compress LiDAR reflectance data with significant volume reduction and high efficiency, making it attractive for real-world applications."}}
{"id": "2505.09602", "pdf": "https://arxiv.org/pdf/2505.09602", "abs": "https://arxiv.org/abs/2505.09602", "authors": ["David Khachaturov", "Robert Mullins"], "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios.", "AI": {"tldr": "The paper presents Adversarial Suffix Filtering (ASF), a lightweight and model-agnostic defensive pipeline to protect LLMs against adversarial suffix attacks, reducing attack efficacy below 4% without significantly affecting non-adversarial performance.", "motivation": "Large Language Models are increasingly deployed in various environments but remain vulnerable to jailbreak attacks. Current defense mechanisms either require access to internal model architecture, increase computational costs, or can be bypassed through simple methods.", "method": "ASF is an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes from prompts. It is designed as a lightweight, model-agnostic pipeline that can operate effectively in both black-box and white-box settings.", "result": "ASF reduces the success rate of state-of-the-art adversarial suffix attacks to below 4%, while having minimal impact on the performance of LLMs in non-adversarial scenarios.", "conclusion": "ASF offers a comprehensive defense solution for protecting LLMs against adversarial suffix attacks, enhancing their security and trustworthiness."}}
{"id": "2505.09435", "pdf": "https://arxiv.org/pdf/2505.09435", "abs": "https://arxiv.org/abs/2505.09435", "authors": ["Yili He", "Yan Zhu", "Peiyao Fu", "Ruijie Yang", "Tianyi Chen", "Zhihua Wang", "Quanlin Li", "Pinghong Zhou", "Xian Yang", "Shuo Wang"], "title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records", "categories": ["cs.CV", "cs.AI"], "comment": "Early accepted to MICCAI 2025", "summary": "Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.", "AI": {"tldr": "Endo-CLIP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\uff08\u6e05\u7406\u3001\u8c03\u8c10\u548c\u7edf\u4e00\uff09\u6539\u8fdb\u4e86\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\uff0c\u4ee5\u5e94\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6311\u6218\u3002\u8be5\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u606f\u8089\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u56fe\u50cf-\u6587\u672c\u7ed3\u80a0\u955c\u8bb0\u5f55\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u975e\u4fe1\u606f\u80cc\u666f\u56fe\u50cf\u3001\u590d\u6742\u7684\u533b\u5b66\u672f\u8bed\u548c\u591a\u75c5\u53d8\u63cf\u8ff0\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "Endo-CLIP\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u6e05\u7406\u9636\u6bb5 - \u79fb\u9664\u80cc\u666f\u5e27\uff1b2) \u8c03\u8c10\u9636\u6bb5 - \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e34\u5e8a\u5c5e\u6027\u7528\u4e8e\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\uff1b3) \u7edf\u4e00\u9636\u6bb5 - \u4f7f\u7528\u60a3\u8005\u7ea7\u522b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u89e3\u51b3\u591a\u606f\u8089\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEndo-CLIP\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u606f\u8089\u68c0\u6d4b\u4e0e\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "Endo-CLIP\u4e3a\u66f4\u51c6\u786e\u548c\u4e34\u5e8a\u4e0a\u66f4\u76f8\u5173\u7684\u5185\u7aa5\u955c\u5206\u6790\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.07363", "pdf": "https://arxiv.org/pdf/2505.07363", "abs": "https://arxiv.org/abs/2505.07363", "authors": ["Serge Massar"], "title": "Equilibrium Propagation for Learning in Lagrangian Dynamical Systems", "categories": ["nlin.CD", "cs.LG", "physics.data-an"], "comment": "8 pages, 1 figure", "summary": "We propose a method for training dynamical systems governed by Lagrangian\nmechanics using Equilibrium Propagation. Our approach extends Equilibrium\nPropagation -- initially developed for energy-based models -- to dynamical\ntrajectories by leveraging the principle of action extremization. Training is\nachieved by gently nudging trajectories toward desired targets and measuring\nhow the variables conjugate to the parameters to be trained respond. This\nmethod is particularly suited to systems with periodic boundary conditions or\nfixed initial and final states, enabling efficient parameter updates without\nrequiring explicit backpropagation through time. In the case of periodic\nboundary conditions, this approach yields the semiclassical limit of Quantum\nEquilibrium Propagation. Applications to systems with dissipation are also\ndiscussed.", "AI": {"tldr": "An innovative training method for dynamical systems governed by Lagrangian mechanics is proposed, which uses Equilibrium Propagation and the principle of action extremization to adjust system trajectories towards desired targets.", "motivation": "Current methods for training dynamical systems may be inefficient or complex, especially when dealing with periodic boundary conditions or fixed initial and final states. There is a need for an approach that can efficiently update parameters without explicit backpropagation through time.", "method": "The paper introduces a method extending Equilibrium Propagation to dynamical trajectories by utilizing the principle of action extremization. The system is trained by slightly nudging its trajectories towards desired targets and observing the response of variables conjugate to the parameters being trained.", "result": "This method allows efficient parameter updates in systems with periodic boundary conditions or fixed initial and final states, without requiring explicit backpropagation through time. It also yields the semiclassical limit of Quantum Equilibrium Propagation in the case of periodic boundary conditions and has potential applications in systems with dissipation.", "conclusion": "The proposed method provides an effective way to train dynamical systems governed by Lagrangian mechanics using Equilibrium Propagation and action extremization, enabling efficient learning in specific types of systems."}}
{"id": "2505.09450", "pdf": "https://arxiv.org/pdf/2505.09450", "abs": "https://arxiv.org/abs/2505.09450", "authors": ["Yuelin Zhang", "Qingpeng Ding", "Long Lei", "Yongxuan Feng", "Raymond Shing-Yan Tang", "Shing Shin Cheng"], "title": "MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy", "categories": ["cs.CV"], "comment": "Early Accepted by MICCAI 2025", "summary": "Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally\ninvasive diagnostic procedure. However, an aspiration needle tracker addressing\nrapid reciprocating motion is still missing. MrTrack, an aspiration needle\ntracker with a mamba-based register mechanism, is proposed. MrTrack leverages a\nMamba-based register extractor to sequentially distill global context from each\nhistorical search map, storing these temporal cues in a register bank. The\nMamba-based register retriever then retrieves temporal prompts from the\nregister bank to provide external cues when current vision features are\ntemporarily unusable due to rapid reciprocating motion and imaging degradation.\nA self-supervised register diversify loss is proposed to encourage feature\ndiversity and dimension independence within the learned register, mitigating\nfeature collapse. Comprehensive experiments conducted on both motorized and\nmanual aspiration datasets demonstrate that MrTrack not only outperforms\nstate-of-the-art trackers in accuracy and robustness but also achieves superior\ninference efficiency.", "AI": {"tldr": "MrTrack, an aspiration needle tracker with a mamba-based register mechanism for ultrasound-guided FNA biopsy, outperforms state-of-the-art trackers in accuracy, robustness, and inference efficiency.", "motivation": "Ultrasound-guided fine needle aspiration (FNA) biopsy lacks an effective tracker for needles with rapid reciprocating motion. To address this gap, MrTrack is proposed.", "method": "MrTrack uses a Mamba-based register extractor to distill global context from historical search maps and stores them in a register bank. A Mamba-based register retriever retrieves these temporal cues when current vision features are unusable due to rapid motion or imaging degradation. Additionally, a self-supervised register diversify loss encourages feature diversity and dimension independence within the learned register.", "result": "Comprehensive experiments on motorized and manual aspiration datasets show that MrTrack surpasses state-of-the-art trackers in terms of accuracy, robustness, and inference efficiency.", "conclusion": "MrTrack successfully addresses the challenge of tracking aspiration needles with rapid reciprocating motion in ultrasound-guided FNA biopsies, demonstrating superior performance."}}
{"id": "2505.09021", "pdf": "https://arxiv.org/pdf/2505.09021", "abs": "https://arxiv.org/abs/2505.09021", "authors": ["Maria Dhakal", "Chia-Yi Su", "Robert Wallace", "Chris Fakhimi", "Aakash Bansal", "Toby Li", "Yu Huang", "Collin McMillan"], "title": "AI-Mediated Code Comment Improvement", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "This paper describes an approach to improve code comments along different\nquality axes by rewriting those comments with customized Artificial\nIntelligence (AI)-based tools. We conduct an empirical study followed by\ngrounded theory qualitative analysis to determine the quality axes to improve.\nThen we propose a procedure using a Large Language Model (LLM) to rewrite\nexisting code comments along the quality axes. We implement our procedure using\nGPT-4o, then distil the results into a smaller model capable of being run\nin-house, so users can maintain data custody. We evaluate both our approach\nusing GPT-4o and the distilled model versions. We show in an evaluation how our\nprocedure improves code comments along the quality axes. We release all data\nand source code in an online repository for reproducibility.", "AI": {"tldr": "This paper presents a method using customized AI tools, particularly GPT-4o and a distilled model, to enhance code comments across different quality axes.", "motivation": "To improve the quality of code comments by rewriting them with customized AI-based tools, ensuring better understanding and maintenance of code.", "method": "An empirical study followed by grounded theory qualitative analysis determined the quality axes for improvement. Then, a procedure was proposed that uses a Large Language Model (LLM) like GPT-4o to rewrite existing code comments along these axes. The results were distilled into a smaller model for in-house use.", "result": "The evaluation demonstrated that the procedure successfully improves code comments along the defined quality axes.", "conclusion": "The approach effectively enhances code comment quality and all data and source code have been made available in an online repository for reproducibility."}}
{"id": "2505.09455", "pdf": "https://arxiv.org/pdf/2505.09455", "abs": "https://arxiv.org/abs/2505.09455", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos", "categories": ["cs.CV"], "comment": "12 pages, submitted to Advanced Concepts for Intelligent Vision\n  Systems 2025", "summary": "State-of-the-art spatio-temporal action detection (STAD) methods show\npromising results for extracting soccer events from broadcast videos. However,\nwhen operated in the high-recall, low-precision regime required for exhaustive\nevent coverage in soccer analytics, their lack of contextual understanding\nbecomes apparent: many false positives could be resolved by considering a\nbroader sequence of actions and game-state information. In this work, we\naddress this limitation by reasoning at the game level and improving STAD\nthrough the addition of a denoising sequence transduction task. Sequences of\nnoisy, context-free player-centric predictions are processed alongside clean\ngame state information using a Transformer-based encoder-decoder model. By\nmodeling extended temporal context and reasoning jointly over team-level\ndynamics, our method leverages the \"language of soccer\" - its tactical\nregularities and inter-player dependencies - to generate \"denoised\" sequences\nof actions. This approach improves both precision and recall in low-confidence\nregimes, enabling more reliable event extraction from broadcast video and\ncomplementing existing pixel-based methods.", "AI": {"tldr": "This paper enhances spatio-temporal action detection (STAD) in soccer analytics by adding a denoising sequence transduction task, using game-level reasoning and Transformer-based models to improve precision and recall.", "motivation": "Current STAD methods lack contextual understanding when operated in high-recall, low-precision regimes for soccer event extraction from broadcast videos, leading to many false positives that could be resolved by considering broader sequences of actions and game-state information.", "method": "The method involves processing sequences of noisy, context-free player-centric predictions alongside clean game state information using a Transformer-based encoder-decoder model, modeling extended temporal context and reasoning jointly over team-level dynamics.", "result": "This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video.", "conclusion": "The enhancement through the addition of a denoising sequence transduction task complements existing pixel-based methods and generates 'denoised' sequences of actions."}}
{"id": "2505.08804", "pdf": "https://arxiv.org/pdf/2505.08804", "abs": "https://arxiv.org/abs/2505.08804", "authors": ["Longtian Wang", "Xiaofei Xie", "Tianlin Li", "Yuhan Zhi", "Chao Shen"], "title": "TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis", "categories": ["cs.CR", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Text-to-image (T2I) models have significantly advanced in producing\nhigh-quality images. However, such models have the ability to generate images\ncontaining not-safe-for-work (NSFW) content, such as pornography, violence,\npolitical content, and discrimination. To mitigate the risk of generating NSFW\ncontent, refusal mechanisms, i.e., safety checkers, have been developed to\ncheck potential NSFW content. Adversarial prompting techniques have been\ndeveloped to evaluate the robustness of the refusal mechanisms. The key\nchallenge remains to subtly modify the prompt in a way that preserves its\nsensitive nature while bypassing the refusal mechanisms. In this paper, we\nintroduce TokenProber, a method designed for sensitivity-aware differential\ntesting, aimed at evaluating the robustness of the refusal mechanisms in T2I\nmodels by generating adversarial prompts. Our approach is based on the key\nobservation that adversarial prompts often succeed by exploiting discrepancies\nin how T2I models and safety checkers interpret sensitive content. Thus, we\nconduct a fine-grained analysis of the impact of specific words within prompts,\ndistinguishing between dirty words that are essential for NSFW content\ngeneration and discrepant words that highlight the different sensitivity\nassessments between T2I models and safety checkers. Through the\nsensitivity-aware mutation, TokenProber generates adversarial prompts, striking\na balance between maintaining NSFW content generation and evading detection.\nOur evaluation of TokenProber against 5 safety checkers on 3 popular T2I\nmodels, using 324 NSFW prompts, demonstrates its superior effectiveness in\nbypassing safety filters compared to existing methods (e.g., 54%+ increase on\naverage), highlighting TokenProber's ability to uncover robustness issues in\nthe existing refusal mechanisms.", "AI": {"tldr": "The paper introduces TokenProber, a method for sensitivity-aware differential testing to evaluate the robustness of refusal mechanisms in T2I models by generating adversarial prompts. It distinguishes between dirty words and discrepant words to create prompts that can bypass safety filters while maintaining NSFW content generation. The evaluation shows its superior effectiveness.", "motivation": "To address the challenge of subtly modifying prompts to preserve sensitive nature while bypassing refusal mechanisms in T2I models.", "method": "TokenProber conducts fine-grained analysis of specific words within prompts, distinguishing between dirty words essential for NSFW content and discrepant words showing different sensitivity assessments. Through sensitivity-aware mutation, it generates adversarial prompts balancing NSFW content generation and evading detection.", "result": "TokenProber demonstrates superior effectiveness in bypassing safety filters compared to existing methods, with a 54%+ increase on average when evaluated against 5 safety checkers on 3 popular T2I models using 324 NSFW prompts.", "conclusion": "TokenProber is effective in uncovering robustness issues in existing refusal mechanisms of T2I models."}}
{"id": "2505.09466", "pdf": "https://arxiv.org/pdf/2505.09466", "abs": "https://arxiv.org/abs/2505.09466", "authors": ["Xi Chen", "Shiyang Zhou", "Muqi Huang", "Jiaxu Feng", "Yun Xiong", "Kun Zhou", "Biao Yang", "Yuhui Zhang", "Huishuai Bao", "Sijia Peng", "Chuan Li", "Feng Shi"], "title": "A 2D Semantic-Aware Position Encoding for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 4 figures, 3 tables", "summary": "Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.", "AI": {"tldr": "Vision transformers use self-attention to capture long-range dependencies but existing position encoding techniques fail to capture semantic-aware positional relationships. This paper proposes 2-Dimensional Semantic-Aware Position Encoding (SaPE^2) which dynamically adapts position representations leveraging local content, enhancing model's generalization and translation equivariance.", "motivation": "To address the limitations of existing position encoding techniques that primarily focus on 1D linear position relationship and neglect semantic similarity between distant patches in images.", "method": "Propose a novel position encoding method called 2-Dimensional Semantic-Aware Position Encoding (SaPE^2) that dynamically adapts position representations by leveraging local content instead of fixed linear position relationship or spatial coordinates.", "result": "Enhances the model's ability to generalize across varying image resolutions and scales, improves translation equivariance, and better aggregates features for visually similar but spatially distant patches.", "conclusion": "Integrating SaPE^2 into vision transformers bridges the gap between position encoding and perceptual similarity, improving performance on computer vision tasks."}}
{"id": "2505.09027", "pdf": "https://arxiv.org/pdf/2505.09027", "abs": "https://arxiv.org/abs/2505.09027", "authors": ["Yi Cui"], "title": "Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2409.05177", "summary": "We introduce WebApp1K, a novel benchmark for evaluating large language models\n(LLMs) in test-driven development (TDD) tasks, where test cases serve as both\nprompt and verification for code generation. Unlike traditional approaches\nrelying on natural language prompts, our benchmark emphasizes the ability of\nLLMs to interpret and implement functionality directly from test cases,\nreflecting real-world software development practices. Comprising 1000 diverse\nchallenges across 20 application domains, the benchmark evaluates LLMs on their\nability to generate compact, functional code under the constraints of context\nlength and multi-feature complexity. Our findings highlight instruction\nfollowing and in-context learning as critical capabilities for TDD success,\nsurpassing the importance of general coding proficiency or pretraining\nknowledge. Through comprehensive evaluation of 19 frontier models, we reveal\nperformance bottlenecks, such as instruction loss in long prompts, and provide\na detailed error analysis spanning multiple root causes. This work underscores\nthe practical value of TDD-specific benchmarks and lays the foundation for\nadvancing LLM capabilities in rigorous, application-driven coding scenarios.", "AI": {"tldr": "The paper introduces WebApp1K, a benchmark for evaluating LLMs in TDD tasks using test cases as prompts and verifications. It highlights instruction following and in-context learning as key capabilities for TDD success.", "motivation": "To address the gap in evaluating LLMs' ability to interpret and implement functionality directly from test cases, reflecting real-world software development practices.", "method": "Created a benchmark named WebApp1K with 1000 diverse challenges across 20 application domains, focusing on evaluating LLMs' abilities in generating compact, functional code under specific constraints.", "result": "Found that instruction following and in-context learning are more critical than general coding proficiency or pretraining knowledge for TDD success. Identified performance bottlenecks like instruction loss in long prompts.", "conclusion": "WebApp1K underscores the practical value of TDD-specific benchmarks and sets the stage for advancing LLM capabilities in rigorous, application-driven coding scenarios."}}
{"id": "2505.09484", "pdf": "https://arxiv.org/pdf/2505.09484", "abs": "https://arxiv.org/abs/2505.09484", "authors": ["Yingjie Ma", "Xun Lin", "Zitong Yu", "Xin Liu", "Xiaochen Yuan", "Weicheng Xie", "Linlin Shen"], "title": "Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing", "categories": ["cs.CV"], "comment": null, "summary": "Face Anti-Spoofing (FAS) is essential for the security of facial recognition\nsystems in diverse scenarios such as payment processing and surveillance.\nCurrent multimodal FAS methods often struggle with effective generalization,\nmainly due to modality-specific biases and domain shifts. To address these\nchallenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising\nand \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot\ngeneralization capability of CLIP, the MMDA framework effectively suppresses\nnoise in multimodal data through denoising and alignment mechanisms, thereby\nsignificantly enhancing the generalization performance of cross-modal\nalignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential\n\\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the\nimpacts of domain and modality noise by refining the attention mechanism based\non extracted common noise features. Furthermore, the \\textbf{R}epresentation\n\\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the\npre-trained CLIP model to align multi-domain multimodal data into a generalized\nrepresentation space in a flexible manner, preserving intricate representations\nand enhancing the model's adaptability to various unseen conditions. We also\ndesign a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation\n(\\textbf{U-DSA}) module to enhance the adaptability of representations while\nmaintaining generalization performance. These improvements not only enhance the\nframework's generalization capabilities but also boost its ability to represent\ncomplex representations. Our experimental results on four benchmark datasets\nunder different evaluation protocols demonstrate that the MMDA framework\noutperforms existing state-of-the-art methods in terms of cross-domain\ngeneralization and multimodal detection accuracy. The code will be released\nsoon.", "AI": {"tldr": "This paper presents MMDA framework for Face Anti-Spoofing (FAS) which enhances cross-modal alignment generalization and multimodal detection accuracy.", "motivation": "Current multimodal FAS methods face challenges in effective generalization due to modality-specific biases and domain shifts.", "method": "The MMDA framework uses CLIP's zero-shot generalization capability, MD2A module for mitigating domain and modality noise, RS2 strategy for aligning multi-domain multimodal data into a generalized representation space, and U-DSA module for enhancing adaptability of representations while maintaining generalization performance.", "result": "Experimental results on four benchmark datasets under different evaluation protocols show that the MMDA framework surpasses existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy.", "conclusion": "The MMDA framework significantly improves generalization capabilities and complex representation abilities for FAS."}}
{"id": "2505.09040", "pdf": "https://arxiv.org/pdf/2505.09040", "abs": "https://arxiv.org/abs/2505.09040", "authors": ["Owen Kwon", "Abraham George", "Alison Bartsch", "Amir Barati Farimani"], "title": "RT-cache: Efficient Robot Trajectory Retrieval System", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference", "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.", "AI": {"tldr": "This paper introduces RT-cache, a novel trajectory memory pipeline that accelerates robot inference via big-data retrieval and learning. It stores successful trajectories, retrieves relevant motions, and adapts to new environments with few samples. Experiments show it outperforms baselines without retrieval.", "motivation": "Modern Vision-Language-Action (VLA) models can handle diverse robotic tasks but suffer from high per-step inference costs and latency.", "method": "RT-cache includes a Memory Builder and Trajectory Retrieval system. It stores large-scale memory of successful robot trajectories and retrieves relevant multistep motion snippets for current scenes.", "result": "Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks faster and more successfully than a baseline lacking retrieval.", "conclusion": "RT-cache provides a practical, data-driven solution for real-time robotic manipulation by reducing inference overhead through experience-based retrieval."}}
{"id": "2505.08816", "pdf": "https://arxiv.org/pdf/2505.08816", "abs": "https://arxiv.org/abs/2505.08816", "authors": ["Ippokratis Koukoulis", "Ilias Syrigos", "Thanasis Korakis"], "title": "Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted at IFIP Networking 2025. Code available at\n  https://github.com/koukipp/contrastive_transformers_ids", "summary": "As the digital landscape becomes more interconnected, the frequency and\nseverity of zero-day attacks, have significantly increased, leading to an\nurgent need for innovative Intrusion Detection Systems (IDS). Machine\nLearning-based IDS that learn from the network traffic characteristics and can\ndiscern attack patterns from benign traffic offer an advanced solution to\ntraditional signature-based IDS. However, they heavily rely on labeled\ndatasets, and their ability to generalize when encountering unseen traffic\npatterns remains a challenge. This paper proposes a novel self-supervised\ncontrastive learning approach based on transformer encoders, specifically\ntailored for generalizable intrusion detection on raw packet sequences. Our\nproposed learning scheme employs a packet-level data augmentation strategy\ncombined with a transformer-based architecture to extract and generate\nmeaningful representations of traffic flows. Unlike traditional methods reliant\non handcrafted statistical features (NetFlow), our approach automatically\nlearns comprehensive packet sequence representations, significantly enhancing\nperformance in anomaly identification tasks and supervised learning for\nintrusion detection. Our transformer-based framework exhibits better\nperformance in comparison to existing NetFlow self-supervised methods.\nSpecifically, we achieve up to a 3% higher AUC in anomaly detection for\nintra-dataset evaluation and up to 20% higher AUC scores in inter-dataset\nevaluation. Moreover, our model provides a strong baseline for supervised\nintrusion detection with limited labeled data, exhibiting an improvement over\nself-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated\non the same dataset. Additionally, we show the adaptability of our pretrained\nmodel when fine-tuned across different datasets, demonstrating strong\nperformance even when lacking benign data from the target domain.", "AI": {"tldr": "The paper introduces a self-supervised contrastive learning method using transformer encoders for intrusion detection, which performs better than traditional NetFlow methods in both intra-dataset and inter-dataset evaluations.", "motivation": "There is an urgent need for innovative Intrusion Detection Systems (IDS) due to the increase in zero-day attacks. Traditional machine learning-based IDS face challenges in generalization when encountering unseen traffic patterns.", "method": "A novel self-supervised contrastive learning approach based on transformer encoders is proposed. It uses packet-level data augmentation and a transformer-based architecture to automatically learn comprehensive packet sequence representations without relying on handcrafted statistical features.", "result": "The transformer-based framework outperforms existing NetFlow self-supervised methods with up to 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. It also shows adaptability when fine-tuned across different datasets.", "conclusion": "This approach provides a strong baseline for supervised intrusion detection with limited labeled data and demonstrates strong performance even when lacking benign data from the target domain."}}
{"id": "2505.09498", "pdf": "https://arxiv.org/pdf/2505.09498", "abs": "https://arxiv.org/abs/2505.09498", "authors": ["Bo Zhang", "Shuo Li", "Runhe Tian", "Yang Yang", "Jixin Tang", "Jinhao Zhou", "Lin Ma"], "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlash-VL 2B\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\uff0c\u4f7f\u5176\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u3002\u901a\u8fc7\u591a\u9879\u6280\u672f\u6539\u8fdb\uff0cFlash-VL 2B\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u662f\u5b9e\u65f6\u5e94\u7528\u7684\u7406\u60f3\u9009\u62e9\u3002", "motivation": "\u968f\u7740\u5bf9\u5b9e\u65f6\u89c6\u89c9-\u8bed\u8a00\u5904\u7406\u9700\u6c42\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u7684VLMs\u65e0\u6cd5\u6ee1\u8db3\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u7684\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Flash-VL 2B\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u4f18\u5316\u7b56\u7565\uff0c\u5305\u62ec\u5b9a\u5236\u67b6\u6784\u8bbe\u8ba1\u3001\u4ee4\u724c\u538b\u7f29\u673a\u5236\u3001\u6570\u636e\u6574\u7406\u3001\u8bad\u7ec3\u65b9\u6848\u4ee5\u53ca\u4e00\u79cd\u79f0\u4e3a\u9690\u5f0f\u8bed\u4e49\u62fc\u63a5\u7684\u65b0\u578b\u56fe\u50cf\u5904\u7406\u6280\u672f\u3002\u8fd9\u4e9b\u6280\u672f\u5171\u540c\u4f5c\u7528\u4ee5\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5bf911\u4e2a\u6807\u51c6VLM\u57fa\u51c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0cFlash-VL 2B\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "Flash-VL 2B\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u548c\u5927\u89c4\u6a21\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u3002"}}
{"id": "2505.09062", "pdf": "https://arxiv.org/pdf/2505.09062", "abs": "https://arxiv.org/abs/2505.09062", "authors": ["Junda Zhao", "Yuliang Song", "Eldan Cohen"], "title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.7"], "comment": "Accepted by the Journal of Systems and Software", "summary": "Recent advancements in source code summarization have leveraged\ntransformer-based pre-trained models, including Large Language Models of Code\n(LLMCs), to automate and improve the generation of code summaries. However,\nexisting methods often focus on generating a single high-quality summary for a\ngiven source code, neglecting scenarios where the generated summary might be\ninadequate and alternative options are needed. In this paper, we introduce\nVariational Prefix Tuning (VPT), a novel approach that enhances pre-trained\nmodels' ability to generate diverse yet accurate sets of summaries, allowing\nthe user to choose the most suitable one for the given source code. Our method\nintegrates a Conditional Variational Autoencoder (CVAE) framework as a modular\ncomponent into pre-trained models, enabling us to model the distribution of\nobserved target summaries and sample continuous embeddings to be used as\nprefixes to steer the generation of diverse outputs during decoding.\nImportantly, we construct our method in a parameter-efficient manner,\neliminating the need for expensive model retraining, especially when using\nLLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset\nof generated summaries, optimizing both the diversity and the accuracy of the\noptions presented to users. We present extensive experimental evaluations using\nwidely used datasets and current state-of-the-art pre-trained code\nsummarization models to demonstrate the effectiveness of our approach and its\nadaptability across models.", "AI": {"tldr": "Recent advancements use transformer-based models for source code summarization, but they often generate only one summary. This paper introduces Variational Prefix Tuning (VPT), which uses a CVAE framework to generate diverse summaries from pre-trained models, without retraining them. A bi-criteria reranking method is also used to ensure diversity and accuracy.", "motivation": "Existing methods for source code summarization focus on generating a single high-quality summary, neglecting scenarios where multiple alternatives might be needed.", "method": "The paper proposes Variational Prefix Tuning (VPT) that integrates a Conditional Variational Autoencoder (CVAE) as a modular component into pre-trained models. It allows sampling of continuous embeddings used as prefixes to guide the generation of diverse outputs during decoding. The method is parameter-efficient and does not require model retraining. Additionally, a bi-criteria reranking method is employed to select summaries optimizing both diversity and accuracy.", "result": "Extensive experimental evaluations using widely recognized datasets and state-of-the-art pre-trained code summarization models demonstrate the effectiveness of VPT and its adaptability across different models.", "conclusion": "VPT enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, providing users with more suitable options for source code summaries."}}
{"id": "2505.09528", "pdf": "https://arxiv.org/pdf/2505.09528", "abs": "https://arxiv.org/abs/2505.09528", "authors": ["Jeffrey Wen", "Rizwan Ahmad", "Philip Schniter"], "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems", "categories": ["cs.CV"], "comment": null, "summary": "In imaging inverse problems, we would like to know how close the recovered\nimage is to the true image in terms of full-reference image quality (FRIQ)\nmetrics like PSNR, SSIM, LPIPS, etc. This is especially important in\nsafety-critical applications like medical imaging, where knowing that, say, the\nSSIM was poor could potentially avoid a costly misdiagnosis. But since we don't\nknow the true image, computing FRIQ is non-trivial. In this work, we combine\nconformal prediction with approximate posterior sampling to construct bounds on\nFRIQ that are guaranteed to hold up to a user-specified error probability. We\ndemonstrate our approach on image denoising and accelerated magnetic resonance\nimaging (MRI) problems. Code is available at\nhttps://github.com/jwen307/quality_uq.", "AI": {"tldr": "The paper integrates conformal prediction with approximate posterior sampling to establish bounds on FRIQ metrics for imaging inverse problems, ensuring safety in applications like medical imaging.", "motivation": "In imaging inverse problems, it's crucial to assess the closeness of the recovered image to the true image using FRIQ metrics. This is particularly vital in safety-critical domains such as medical imaging where misdiagnosis must be avoided.", "method": "Combine conformal prediction with approximate posterior sampling to create bounds on FRIQ that hold up to a user-specified error probability.", "result": "Successfully demonstrated the approach on image denoising and accelerated MRI problems.", "conclusion": "This method provides reliable FRIQ bounds, contributing to safer imaging applications."}}
{"id": "2505.09081", "pdf": "https://arxiv.org/pdf/2505.09081", "abs": "https://arxiv.org/abs/2505.09081", "authors": ["Gaurav Koley"], "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation", "categories": ["cs.SI", "cs.AI", "cs.MA"], "comment": null, "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.", "AI": {"tldr": "The paper introduces SALM, a new framework that integrates language models into social network simulations. It features a hierarchical prompting architecture, an attention-based memory system, and formal bounds on personality stability, enabling stable long-term simulations with reduced token usage and efficient memory management.", "motivation": "Current agent-based modeling methods for social systems rely heavily on rule-based behaviors, which limits their ability to capture complex dynamics. The motivation is to move beyond predefined rules by leveraging contextual understanding from language models of human social interaction.", "method": "The method involves creating SALM (Social Agent LM Framework), which includes a hierarchical prompting architecture for stable simulation, an attention-based memory system for efficient caching, and formal bounds on personality stability.", "result": "SALM achieves unprecedented temporal stability in multi-agent scenarios, reducing token usage by 73% and achieving 80% cache hit rates with sub-linear memory growth. It is validated against SNAP ego networks, demonstrating its capability to model long-term social phenomena accurately.", "conclusion": "SALM represents a significant advancement in integrating language models into social network simulations, providing a robust framework for modeling long-term social dynamics with high behavioral fidelity."}}
{"id": "2505.09529", "pdf": "https://arxiv.org/pdf/2505.09529", "abs": "https://arxiv.org/abs/2505.09529", "authors": ["Mohamed Moustafa", "Joseph Lemley", "Peter Corcoran"], "title": "Contactless Cardiac Pulse Monitoring Using Event Cameras", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "comment": "This paper is a preprint of a paper submitted to IEEE Access and is\n  currently under review", "summary": "Time event cameras are a novel technology for recording scene information at\nextremely low latency and with low power consumption. Event cameras output a\nstream of events that encapsulate pixel-level light intensity changes within\nthe scene, capturing information with a higher dynamic range and temporal\nresolution than traditional cameras. This study investigates the contact-free\nreconstruction of an individual's cardiac pulse signal from time event\nrecording of their face using a supervised convolutional neural network (CNN)\nmodel. An end-to-end model is trained to extract the cardiac signal from a\ntwo-dimensional representation of the event stream, with model performance\nevaluated based on the accuracy of the calculated heart rate. The experimental\nresults confirm that physiological cardiac information in the facial region is\neffectively preserved within the event stream, showcasing the potential of this\nnovel sensor for remote heart rate monitoring. The model trained on event\nframes achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)\ncompared to the RMSE of 2.92 bpm achieved by the baseline model trained on\nstandard camera frames. Furthermore, models trained on event frames generated\nat 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an\nRMSE of 2.54 and 2.13 bpm, respectively.", "AI": {"tldr": "This paper explores the use of time event cameras for contact-free heart rate monitoring by extracting cardiac pulse signals from facial recordings using a CNN model.", "motivation": "The motivation is to leverage the advantages of event cameras, such as low latency and high temporal resolution, for remote physiological signal extraction, specifically heart rate monitoring.", "method": "A supervised CNN model is trained end-to-end to extract cardiac signals from a 2D representation of the event stream generated by an event camera recording faces. Model performance is assessed based on heart rate accuracy.", "result": "The model achieved an RMSE of 3.32 bpm when compared to a baseline model with an RMSE of 2.92 bpm using standard camera frames. Event frames at higher FPS (60 and 120) outperformed standard 30 FPS camera frames with RMSEs of 2.54 and 2.13 bpm respectively.", "conclusion": "Event cameras effectively preserve physiological cardiac information in the facial region, showing potential for remote heart rate monitoring with improved accuracy at higher frame rates."}}
{"id": "2505.08819", "pdf": "https://arxiv.org/pdf/2505.08819", "abs": "https://arxiv.org/abs/2505.08819", "authors": ["Asahi Miyazaki", "Tsuyoshi Okita"], "title": "Thoughts on Objectives of Sparse and Hierarchical Masked Image Model", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "9 pages, 11 figures", "summary": "Masked image modeling is one of the most poplular objectives of training.\nRecently, the SparK model has been proposed with superior performance among\nself-supervised learning models. This paper proposes a new mask pattern for\nthis SparK model, proposing it as the Mesh Mask-ed SparK model. We report the\neffect of the mask pattern used for image masking in pre-training on\nperformance.", "AI": {"tldr": "The paper introduces a new mask pattern for the SparK model, named Mesh Mask-ed SparK model, and investigates the impact of mask patterns on pre-training performance.", "motivation": "The motivation is to improve the performance of the SparK model by introducing a new mask pattern.", "method": "The method involves proposing a new mask pattern for the SparK model, resulting in the Mesh Mask-ed SparK model.", "result": "The effect of the mask pattern on the performance during pre-training is reported.", "conclusion": "A new mask pattern for the SparK model has been proposed and its influence on performance has been demonstrated."}}
{"id": "2505.09562", "pdf": "https://arxiv.org/pdf/2505.09562", "abs": "https://arxiv.org/abs/2505.09562", "authors": ["Nicola Marinello", "Simen Cassiman", "Jonas Heylen", "Marc Proesmans", "Luc Van Gool"], "title": "Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 Workshop on Autonomous Driving", "summary": "Autonomous vehicles need a complete map of their surroundings to plan and\nact. This has sparked research into the tasks of 3D occupancy prediction, 3D\nscene completion, and 3D panoptic scene completion, which predict a dense map\nof the ego vehicle's surroundings as a voxel grid. Scene completion extends\noccupancy prediction by predicting occluded regions of the voxel grid, and\npanoptic scene completion further extends this task by also distinguishing\nobject instances within the same class; both aspects are crucial for path\nplanning and decision-making. However, 3D panoptic scene completion is\ncurrently underexplored. This work introduces a novel framework for 3D panoptic\nscene completion that extends existing 3D semantic scene completion models. We\npropose an Object Module and Panoptic Module that can easily be integrated with\n3D occupancy and scene completion methods presented in the literature. Our\napproach leverages the available annotations in occupancy benchmarks, allowing\nindividual object shapes to be learned as a differentiable problem. The code is\navailable at https://github.com/nicolamarinello/OffsetOcc .", "AI": {"tldr": "This paper presents a new framework for 3D panoptic scene completion which includes Object Module and Panoptic Module to improve existing models.", "motivation": "3D panoptic scene completion is underexplored despite its importance in autonomous vehicles for path planning and decision-making.", "method": "The authors propose a novel framework that integrates Object Module and Panoptic Module with existing 3D occupancy and scene completion methods. This approach leverages annotations in occupancy benchmarks to learn individual object shapes as a differentiable problem.", "result": "The proposed method aims to advance the state of the art in 3D panoptic scene completion, although specific experimental results are not provided in the abstract.", "conclusion": "This work introduces a new framework for 3D panoptic scene completion that extends current models using Object and Panoptic Modules."}}
{"id": "2505.08822", "pdf": "https://arxiv.org/pdf/2505.08822", "abs": "https://arxiv.org/abs/2505.08822", "authors": ["Yuhao Wang", "Kailai Wang", "Songhua Hu", "Yunpeng", "Zhang", "Gino Lim", "Pengyu Zhu"], "title": "The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics", "categories": ["cs.CY", "cs.LG", "physics.soc-ph"], "comment": null, "summary": "The rapid evolution of the transportation cybersecurity ecosystem,\nencompassing cybersecurity, automotive, and transportation and logistics\nsectors, will lead to the formation of distinct spatial clusters and visitor\nflow patterns across the US. This study examines the spatiotemporal dynamics of\nvisitor flows, analyzing how socioeconomic factors shape industry clustering\nand workforce distribution within these evolving sectors. To model and predict\nvisitor flow patterns, we develop a BiTransGCN framework, integrating an\nattention-based Transformer architecture with a Graph Convolutional Network\nbackbone. By integrating AI-enabled forecasting techniques with spatial\nanalysis, this study improves our ability to track, interpret, and anticipate\nchanges in industry clustering and mobility trends, thereby supporting\nstrategic planning for a secure and resilient transportation network. It offers\na data-driven foundation for economic planning, workforce development, and\ntargeted investments in the transportation cybersecurity ecosystem.", "AI": {"tldr": "This paper investigates how socioeconomic factors influence industry clustering and workforce distribution in the transportation cybersecurity ecosystem across the US, presenting a BiTransGCN framework to model visitor flow patterns.", "motivation": "To understand spatiotemporal dynamics of visitor flows and examine how socioeconomic factors shape industry clustering and workforce distribution within evolving sectors related to transportation cybersecurity.", "method": "Develops a BiTransGCN framework which integrates an attention-based Transformer architecture with a Graph Convolutional Network backbone for modeling and predicting visitor flow patterns.", "result": "Improves the ability to track, interpret, and anticipate changes in industry clustering and mobility trends, providing support for strategic planning in transportation network.", "conclusion": "The study provides a data-driven foundation for economic planning, workforce development, and targeted investments in the transportation cybersecurity ecosystem."}}
{"id": "2505.09564", "pdf": "https://arxiv.org/pdf/2505.09564", "abs": "https://arxiv.org/abs/2505.09564", "authors": ["Anne-Marie Rickmann", "Stephanie L. Thorn", "Shawn S. Ahn", "Supum Lee", "Selen Uman", "Taras Lysyy", "Rachel Burns", "Nicole Guerrera", "Francis G. Spinale", "Jason A. Burdick", "Albert J. Sinusas", "James S. Duncan"], "title": "Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation", "categories": ["cs.CV"], "comment": "accepted at FIMH 2025", "summary": "Cardiac image segmentation is an important step in many cardiac image\nanalysis and modeling tasks such as motion tracking or simulations of cardiac\nmechanics. While deep learning has greatly advanced segmentation in clinical\nsettings, there is limited work on pre-clinical imaging, notably in porcine\nmodels, which are often used due to their anatomical and physiological\nsimilarity to humans. However, differences between species create a domain\nshift that complicates direct model transfer from human to pig data.\n  Recently, foundation models trained on large human datasets have shown\npromise for robust medical image segmentation; yet their applicability to\nporcine data remains largely unexplored. In this work, we investigate whether\nfoundation models can generate sufficiently accurate pseudo-labels for pig\ncardiac CT and propose a simple self-training approach to iteratively refine\nthese labels. Our method requires no manually annotated pig data, relying\ninstead on iterative updates to improve segmentation quality. We demonstrate\nthat this self-training process not only enhances segmentation accuracy but\nalso smooths out temporal inconsistencies across consecutive frames. Although\nour results are encouraging, there remains room for improvement, for example by\nincorporating more sophisticated self-training strategies and by exploring\nadditional foundation models and other cardiac imaging technologies.", "AI": {"tldr": "This paper explores the use of foundation models to create accurate pseudo-labels for porcine cardiac CT segmentation without manual annotations, using a self-training approach that improves segmentation accuracy and temporal consistency.", "motivation": "Cardiac image segmentation is crucial in many cardiac analysis tasks but deep learning advancements have been limited in pre-clinical imaging, particularly with porcine models. The anatomical differences between species lead to domain shifts complicating model transfer from human to pig data.", "method": "The researchers investigate whether foundation models can generate accurate pseudo-labels for pig cardiac CT and propose a simple self-training approach to iteratively refine these labels without manually annotated pig data.", "result": "The self-training process enhances segmentation accuracy and smooths out temporal inconsistencies across consecutive frames.", "conclusion": "While the results are encouraging, there is still room for improvement by incorporating more sophisticated self-training strategies and exploring additional foundation models and other cardiac imaging technologies."}}
{"id": "2505.09091", "pdf": "https://arxiv.org/pdf/2505.09091", "abs": "https://arxiv.org/abs/2505.09091", "authors": ["Zeeshan Ahmad", "Shudi Bao", "Meng Chen"], "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "comment": null, "summary": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio.", "AI": {"tldr": "This paper proposes DPN-GAN, a novel GAN architecture for audio sequence generation that addresses the issues of resolution constraints and mode collapse through kernel-based periodic ReLU activation and deformable convolution operations. It demonstrates superior performance and robustness in both speech synthesis and music generation tasks.", "motivation": "Existing GANs for audio sequence generation rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences and lead to mode collapse during conditional generation.", "method": "The proposed method is DPN-GAN, which incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation and utilizes deformable convolution operations for multi-resolution generation with adaptive receptive fields. The discriminator network is also enhanced using deformable convolution to better distinguish between real and generated samples.", "result": "DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics across five different datasets covering speech synthesis and music generation tasks. It shows increased robustness in synthesized audio, especially on out-of-distribution and noisy data.", "conclusion": "DPN-GAN delivers superior performance and robustness in audio sequence generation compared to existing GAN architectures."}}
{"id": "2505.09568", "pdf": "https://arxiv.org/pdf/2505.09568", "abs": "https://arxiv.org/abs/2505.09568", "authors": ["Jiuhai Chen", "Zhiyang Xu", "Xichen Pan", "Yushi Hu", "Can Qin", "Tom Goldstein", "Lifu Huang", "Tianyi Zhou", "Saining Xie", "Silvio Savarese", "Le Xue", "Caiming Xiong", "Ran Xu"], "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.", "AI": {"tldr": "\u7814\u7a76\u7edf\u4e00\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u7684\u6846\u67b6\uff0c\u63a2\u7d22\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4f7f\u7528\u6269\u6563Transformer\u751f\u6210CLIP\u56fe\u50cf\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5206\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6BLIP3o-60k\u548c\u6a21\u578bBLIP3-o\uff0c\u5f00\u6e90\u6240\u6709\u8d44\u6e90\u3002", "motivation": "\u5c3d\u7ba1\u56fe\u50cf\u7406\u89e3\u7684\u8bbe\u8ba1\u9009\u62e9\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u7ed3\u5408\u56fe\u50cf\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\u7684\u6700\u4f73\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u9700\u6df1\u5165\u63a2\u7d22\u3002\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u56e0\u5176\u9ad8\u8d28\u91cf\u751f\u6210\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "1. \u4f7f\u7528\u6269\u6563Transformer\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684CLIP\u56fe\u50cf\u7279\u5f81\uff0c\u800c\u975e\u4f20\u7edf\u7684VAE-based\u8868\u793a\u3002\n2. \u91c7\u7528\u987a\u5e8f\u9884\u8bad\u7ec3\u7b56\u7565\uff1a\u5148\u8fdb\u884c\u56fe\u50cf\u7406\u89e3\u8bad\u7ec3\uff0c\u518d\u8fdb\u884c\u56fe\u50cf\u751f\u6210\u8bad\u7ec3\u3002\n3. \u521b\u5efa\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6BLIP3o-60k\uff0c\u6db5\u76d6\u591a\u79cd\u573a\u666f\u3001\u5bf9\u8c61\u548c\u4eba\u7c7b\u59ff\u6001\u7b49\u3002\n4. \u57fa\u4e8e\u4e0a\u8ff0\u65b9\u6cd5\u5f00\u53d1BLIP3-o\u7cfb\u5217\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "BLIP3-o\u5728\u591a\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6db5\u76d6\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002\u8bc1\u660e\u4e86\u65b0\u65b9\u6cd5\u5728\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u8bbe\u8ba1\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u5168\u9762\u5f00\u653e\u6e90\u4ee3\u7801\u548c\u8d44\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.09108", "pdf": "https://arxiv.org/pdf/2505.09108", "abs": "https://arxiv.org/abs/2505.09108", "authors": ["Fernando Cladera", "Zachary Ravichandran", "Jason Hughes", "Varun Murali", "Carlos Nieto-Granda", "M. Ani Hsieh", "George J. Pappas", "Camillo J. Taylor", "Vijay Kumar"], "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown Environments", "categories": ["cs.RO", "cs.AI"], "comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR", "summary": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.", "AI": {"tldr": "This paper introduces a first-of-its-kind system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) collaborate to accomplish missions specified in natural language, using a Large Language Model (LLM)-enabled planner.", "motivation": "As autonomous robotic systems become increasingly mature, users will want to specify missions at the level of intent rather than in low-level detail. Language is an expressive and intuitive medium for such mission specification.", "method": "The system leverages a Large Language Model (LLM)-enabled planner to reason over semantic-metric maps that are built online and opportunistically shared between an aerial and a ground robot. The system considers task-driven navigation in urban and rural areas and must infer mission-relevant semantics and actively acquire information via semantic mapping.", "result": "The system was demonstrated on seven different natural-language specifications at up to kilometer-scale navigation in both ground and air-ground teaming experiments.", "conclusion": "The authors have presented a system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) are able to collaboratively accomplish missions specified in natural language while reacting to changes in specification on the fly."}}
{"id": "2505.08837", "pdf": "https://arxiv.org/pdf/2505.08837", "abs": "https://arxiv.org/abs/2505.08837", "authors": ["Muhammad Saqib", "Dipkumar Mehta", "Fnu Yashu", "Shubham Malhotra"], "title": "Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning", "categories": ["cs.CR", "cs.CV", "cs.DC", "cs.LG", "cs.NI"], "comment": "10 pages, 6 figures, 1 table", "summary": "The security of cloud environments, such as Amazon Web Services (AWS), is\ncomplex and dynamic. Static security policies have become inadequate as threats\nevolve and cloud resources exhibit elasticity [1]. This paper addresses the\nlimitations of static policies by proposing a security policy management\nframework that uses reinforcement learning (RL) to adapt dynamically.\nSpecifically, we employ deep reinforcement learning algorithms, including deep\nQ Networks and proximal policy optimization, enabling the learning and\ncontinuous adjustment of controls such as firewall rules and Identity and\nAccess Management (IAM) policies. The proposed RL based solution leverages\ncloud telemetry data (AWS Cloud Trail logs, network traffic data, threat\nintelligence feeds) to continuously refine security policies, maximizing threat\nmitigation, and compliance while minimizing resource impact. Experimental\nresults demonstrate that our adaptive RL based framework significantly\noutperforms static policies, achieving higher intrusion detection rates (92%\ncompared to 82% for static policies) and substantially reducing incident\ndetection and response times by 58%. In addition, it maintains high conformity\nwith security requirements and efficient resource usage. These findings\nvalidate the effectiveness of adaptive reinforcement learning approaches in\nimproving cloud security policy management.", "AI": {"tldr": "The paper proposes a security policy management framework using reinforcement learning (RL) to adapt dynamically in cloud environments, significantly outperforming static policies.", "motivation": "Static security policies have become inadequate due to the complex and dynamic nature of cloud environments, such as Amazon Web Services (AWS).", "method": "The proposed method employs deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, leveraging cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies.", "result": "Experimental results demonstrate that the adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and reducing incident detection and response times by 58%.", "conclusion": "The findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management."}}
{"id": "2505.09571", "pdf": "https://arxiv.org/pdf/2505.09571", "abs": "https://arxiv.org/abs/2505.09571", "authors": ["Guillermo Gomez-Trenado", "Pablo Mesejo", "Oscar Cord\u00f3n", "St\u00e9phane Lathuili\u00e8re"], "title": "Don't Forget your Inverse DDIM for Image Editing", "categories": ["cs.CV", "I.2.10; I.5.0"], "comment": "12 pages, 12 figures, code available at\n  https://guillermogotre.github.io/sage/", "summary": "The field of text-to-image generation has undergone significant advancements\nwith the introduction of diffusion models. Nevertheless, the challenge of\nediting real images persists, as most methods are either computationally\nintensive or produce poor reconstructions. This paper introduces SAGE\n(Self-Attention Guidance for image Editing) - a novel technique leveraging\npre-trained diffusion models for image editing. SAGE builds upon the DDIM\nalgorithm and incorporates a novel guidance mechanism utilizing the\nself-attention layers of the diffusion U-Net. This mechanism computes a\nreconstruction objective based on attention maps generated during the inverse\nDDIM process, enabling efficient reconstruction of unedited regions without the\nneed to precisely reconstruct the entire input image. Thus, SAGE directly\naddresses the key challenges in image editing. The superiority of SAGE over\nother methods is demonstrated through quantitative and qualitative evaluations\nand confirmed by a statistically validated comprehensive user study, in which\nall 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE\nranks as the top-performing method in seven out of 10 quantitative analyses and\nsecures second and third places in the remaining three.", "AI": {"tldr": "The paper presents SAGE(Self-Attention Guidance for image Editing), a new technique using pre-trained diffusion models to edit images more efficiently and accurately. It solves the problem of poor reconstructions or high computation in current methods by incorporating a guidance mechanism based on self-attention layers. Through evaluations and user studies, SAGE proves superior in image editing.", "motivation": "Existing image editing methods either require heavy computation or lead to poor reconstructions, making it difficult to achieve efficient and high-quality image editing.", "method": "SAGE is built upon DDIM algorithm and introduces a novel guidance mechanism that uses the self-attention layers of the diffusion U-Net. This mechanism computes reconstruction objectives from attention maps during the inverse DDIM process, allowing efficient reconstruction of unedited regions without needing to reconstruct the entire input image.", "result": "SAGE outperforms other methods in both quantitative and qualitative evaluations. In a user study with 47 participants, all preferred SAGE over competing methods. Additionally, SAGE ranks first in seven, second in two, and third in one out of ten quantitative analyses.", "conclusion": "SAGE successfully addresses the challenges in image editing with its innovative use of self-attention guidance, demonstrating superior performance compared to existing methods."}}
{"id": "2505.09115", "pdf": "https://arxiv.org/pdf/2505.09115", "abs": "https://arxiv.org/abs/2505.09115", "authors": ["Yu Lun Hsu", "Yun-Rung Chou", "Chiao-Ju Chang", "Yu-Cheng Chang", "Zer-Wei Lee", "Rokas Gipi\u0161kis", "Rachel Li", "Chih-Yuan Shih", "Jen-Kuei Peng", "Hsien-Liang Huang", "Jaw-Shiun Tsai", "Mike Y. Chen"], "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Advance Care Planning (ACP) allows individuals to specify their preferred\nend-of-life life-sustaining treatments before they become incapacitated by\ninjury or terminal illness (e.g., coma, cancer, dementia). While online ACP\noffers high accessibility, it lacks key benefits of clinical consultations,\nincluding personalized value exploration, immediate clarification of decision\nconsequences. To bridge this gap, we conducted two formative studies: 1)\nshadowed and interviewed 3 ACP teams consisting of physicians, nurses, and\nsocial workers (18 patients total), and 2) interviewed 14 users of ACP\nwebsites. Building on these insights, we designed PreCare in collaboration with\n6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed\nto guide users through exploring personal values, gaining ACP knowledge, and\nsupporting informed decision-making. A usability study (n=12) showed that\nPreCare achieved a System Usability Scale (SUS) rating of excellent. A\ncomparative evaluation (n=12) showed that PreCare's AI assistants significantly\nimproved exploration of personal values, knowledge, and decisional confidence,\nand was preferred by 92% of participants.", "AI": {"tldr": "PreCare is a website with AI-driven assistants designed to guide users through Advance Care Planning (ACP). It significantly improved personal value exploration, knowledge and decisional confidence.", "motivation": "To bridge the gap of online ACP which lacks personalized value exploration and immediate clarification of decision consequences.", "method": "Conducted two formative studies and designed PreCare in collaboration with ACP professionals. PreCare has three AI-driven assistants to help users explore personal values, gain ACP knowledge, and support informed decision-making.", "result": "A usability study showed that PreCare achieved an excellent System Usability Scale (SUS) rating. A comparative evaluation showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.", "conclusion": "PreCare successfully bridges the gap between online ACP and clinical consultations."}}
{"id": "2505.09591", "pdf": "https://arxiv.org/pdf/2505.09591", "abs": "https://arxiv.org/abs/2505.09591", "authors": ["Tobias Jan Wieczorek", "Nathalie Daun", "Mohammad Emtiyaz Khan", "Marcus Rohrbach"], "title": "Variational Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 16 figures, under review at ICCV 2025", "summary": "Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.", "AI": {"tldr": "The paper proposes a Variational VQA approach to improve calibration and reliability of multimodal models in Visual Question Answering without sacrificing accuracy.", "motivation": "Multimodal models for Visual Question Answering (VQA) have major reliability concerns as they can be overconfident and miscalibrated, particularly in out-of-distribution settings. Existing solutions mainly focus on unimodal models.", "method": "The authors employ a variational algorithm called IVON instead of AdamW for fine-tuning vision-language models, yielding a posterior distribution over model parameters.", "result": "The Variational VQA approach improves calibration and abstentions without losing the accuracy of AdamW. It reduces Expected Calibration Error by more than 50% compared to AdamW baseline and raises Coverage by 4% vs. SOTA (for a fixed risk of 1%). In OOD settings with 50% test cases being OOD, it achieves 8% Coverage improvement vs. SOTA (@ 1% risk).", "conclusion": "Variational learning is presented as a feasible method to enhance the reliability of multimodal models."}}
{"id": "2505.09608", "pdf": "https://arxiv.org/pdf/2505.09608", "abs": "https://arxiv.org/abs/2505.09608", "authors": ["Nadav Magar", "Amir Hertz", "Eric Tabellion", "Yael Pritch", "Alex Rav-Acha", "Ariel Shamir", "Yedid Hoshen"], "title": "LightLab: Controlling Light Sources in Images with Diffusion Models", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://nadmag.github.io/LightLab/", "summary": "We present a simple, yet effective diffusion-based method for fine-grained,\nparametric control over light sources in an image. Existing relighting methods\neither rely on multiple input views to perform inverse rendering at inference\ntime, or fail to provide explicit control over light changes. Our method\nfine-tunes a diffusion model on a small set of real raw photograph pairs,\nsupplemented by synthetically rendered images at scale, to elicit its\nphotorealistic prior for relighting. We leverage the linearity of light to\nsynthesize image pairs depicting controlled light changes of either a target\nlight source or ambient illumination. Using this data and an appropriate\nfine-tuning scheme, we train a model for precise illumination changes with\nexplicit control over light intensity and color. Lastly, we show how our method\ncan achieve compelling light editing results, and outperforms existing methods\nbased on user preference.", "AI": {"tldr": "This paper introduces a diffusion-based method for precise control over light sources in an image, which outperforms existing methods.", "motivation": "Existing relighting methods either rely on multiple input views or fail to provide explicit control over light changes.", "method": "The method fine-tunes a diffusion model on real raw photograph pairs and synthetically rendered images, leveraging the linearity of light to synthesize image pairs depicting controlled light changes.", "result": "The model can achieve precise illumination changes with explicit control over light intensity and color, and achieves compelling light editing results that outperform existing methods based on user preference.", "conclusion": "The presented diffusion-based method provides fine-grained, parametric control over light sources in an image, showing superior performance compared to existing methods."}}
{"id": "2505.09615", "pdf": "https://arxiv.org/pdf/2505.09615", "abs": "https://arxiv.org/abs/2505.09615", "authors": ["Yung-Hsuan Lai", "Janek Ebbers", "Yu-Chiang Frank Wang", "Fran\u00e7ois Germain", "Michael Jeffrey Jones", "Moitreya Chatterjee"], "title": "UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing\nboth uni-modal events (i.e., those occurring exclusively in either the visual\nor acoustic modality of a video) and multi-modal events (i.e., those occurring\nin both modalities concurrently). Moreover, the prohibitive cost of annotating\ntraining data with the class labels of all these events, along with their start\nand end times, imposes constraints on the scalability of AVVP techniques unless\nthey can be trained in a weakly-supervised setting, where only\nmodality-agnostic, video-level labels are available in the training data. To\nthis end, recently proposed approaches seek to generate segment-level\npseudo-labels to better guide model training. However, the absence of\ninter-segment dependencies when generating these pseudo-labels and the general\nbias towards predicting labels that are absent in a segment limit their\nperformance. This work proposes a novel approach towards overcoming these\nweaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video\nParsing (UWAV). Additionally, our innovative approach factors in the\nuncertainty associated with these estimated pseudo-labels and incorporates a\nfeature mixup based training regularization for improved training. Empirical\nresults show that UWAV outperforms state-of-the-art methods for the AVVP task\non multiple metrics, across two different datasets, attesting to its\neffectiveness and generalizability.", "AI": {"tldr": "This paper addresses the challenge of Audio-Visual Video Parsing (AVVP) in a weakly-supervised setting, where training data lacks detailed annotations. It proposes a novel method called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV), which improves upon existing methods by considering inter-segment dependencies and reducing bias towards absent labels. The approach incorporates uncertainty estimation for pseudo-labels and uses feature mixup based training regularization. Empirical results demonstrate that UWAV outperforms state-of-the-art methods on two datasets.", "motivation": "The motivation is to address the limitations of current weakly-supervised AVVP techniques, particularly their inability to effectively capture inter-segment dependencies and their bias towards predicting absent labels. This limits their performance in localizing both uni-modal and multi-modal events in videos.", "method": "The proposed method, UWAV, introduces an innovative approach to generate segment-level pseudo-labels while factoring in the uncertainty associated with these labels. It also incorporates a feature mixup based training regularization technique to improve the training process.", "result": "Empirical evaluations show that UWAV surpasses state-of-the-art methods in the AVVP task across multiple metrics on two different datasets, demonstrating its effectiveness and generalizability.", "conclusion": "The paper concludes that the proposed UWAV method successfully overcomes the limitations of previous approaches by considering inter-segment dependencies and reducing bias. The incorporation of uncertainty estimation and feature mixup based training regularization leads to improved performance in weakly-supervised AVVP."}}
{"id": "2505.09142", "pdf": "https://arxiv.org/pdf/2505.09142", "abs": "https://arxiv.org/abs/2505.09142", "authors": ["Seungbeom Choi", "Jeonghoe Goo", "Eunjoo Jeon", "Mingyu Yang", "Minsung Jang"], "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "13 pages, 5 figures. Cloud-native LLM scheduling system with\n  latency-aware inference optimization", "summary": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%.", "AI": {"tldr": "The paper introduces ELIS, a serving system for LLMs with an ISRTF scheduler that cuts average job completion time by up to 19.6%.", "motivation": "Current LLM serving systems use first-come-first-served scheduling, causing the 'head-of-line blocking' problem.", "method": "ELIS uses a trained response length predictor and an ISRTF scheduling strategy to efficiently manage inference tasks.", "result": "ELIS reduces the average job completion time by up to 19.6% in experimental results.", "conclusion": "ELIS is a cloud-native scheduler system on Kubernetes that overcomes limitations of current LLM serving systems."}}
{"id": "2505.08899", "pdf": "https://arxiv.org/pdf/2505.08899", "abs": "https://arxiv.org/abs/2505.08899", "authors": ["Andrew Mullhaupt", "Cheng Peng"], "title": "Bounding Neyman-Pearson Region with $f$-Divergences", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": null, "summary": "The Neyman-Pearson region of a simple binary hypothesis testing is the set of\npoints whose coordinates represent the false positive rate and false negative\nrate of some test. The lower boundary of this region is given by the\nNeyman-Pearson lemma, and is up to a coordinate change, equivalent to the\noptimal ROC curve. We establish a novel lower bound for the boundary in terms\nof any $f$-divergence. Since the bound generated by hockey-stick\n$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best\npossible. In the case of KL divergence, this bound improves Pinsker's\ninequality. Furthermore, we obtain a closed-form refined upper bound for the\nNeyman-Pearson boundary in terms of the Chernoff $\\alpha$-coefficient. Finally,\nwe present methods for constructing pairs of distributions that can\napproximately or exactly realize any given Neyman-Pearson boundary.", "AI": {"tldr": "The paper establishes a novel lower bound for the Neyman-Pearson boundary using $f$-divergence, improves Pinsker's inequality with KL divergence, obtains an upper bound in terms of the Chernoff $\\alpha$-coefficient, and presents methods for constructing distribution pairs to realize given boundaries.", "motivation": "To provide a deeper understanding of the Neyman-Pearson region by deriving new bounds for its boundary and presenting methods to construct distribution pairs that can realize given boundaries.", "method": "Establishing a novel lower bound for the Neyman-Pearson boundary using any $f$-divergence, improving Pinsker's inequality with KL divergence, obtaining a closed-form refined upper bound using the Chernoff $\\alpha$-coefficient, and proposing methods for constructing pairs of distributions to approximately or exactly realize given Neyman-Pearson boundaries.", "result": "A novel lower bound for the Neyman-Pearson boundary was established in terms of any $f$-divergence, which is best possible when generated by hockey-stick $f$-divergences. An improved Pinsker's inequality was achieved with KL divergence. A closed-form refined upper bound for the Neyman-Pearson boundary was obtained in terms of the Chernoff $\\alpha$-coefficient. Methods for constructing pairs of distributions were presented.", "conclusion": "The analysis provides new insights into the Neyman-Pearson region by offering tighter bounds on its boundary and demonstrating how to construct distribution pairs to achieve specific boundaries."}}
{"id": "2505.08908", "pdf": "https://arxiv.org/pdf/2505.08908", "abs": "https://arxiv.org/abs/2505.08908", "authors": ["Benedikt Koch", "Kosuke Imai"], "title": "Statistical Decision Theory with Counterfactual Loss", "categories": ["math.ST", "cs.LG", "econ.TH", "stat.TH"], "comment": null, "summary": "Classical statistical decision theory evaluates treatment choices based\nsolely on observed outcomes. However, by ignoring counterfactual outcomes, it\ncannot assess the quality of decisions relative to feasible alternatives. For\nexample, the quality of a physician's decision may depend not only on patient\nsurvival, but also on whether a less invasive treatment could have produced a\nsimilar result. To address this limitation, we extend standard decision theory\nto incorporate counterfactual losses--criteria that evaluate decisions using\nall potential outcomes. The central challenge in this generalization is\nidentification: because only one potential outcome is observed for each unit,\nthe associated risk under a counterfactual loss is generally not identifiable.\nWe show that under the assumption of strong ignorability, a counterfactual risk\nis identifiable if and only if the counterfactual loss function is additive in\nthe potential outcomes. Moreover, we demonstrate that additive counterfactual\nlosses can yield treatment recommendations that differ from those based on\nstandard loss functions, provided that the decision problem involves more than\ntwo treatment options.", "AI": {"tldr": "The paper extends standard decision theory to include counterfactual losses for better assessing decision quality.", "motivation": "Classical statistical decision theory evaluates treatment choices based solely on observed outcomes, ignoring counterfactual outcomes which limits its ability to assess the quality of decisions relative to feasible alternatives.", "method": "Extend standard decision theory to incorporate counterfactual losses under the assumption of strong ignorability. Identify that a counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes.", "result": "Counterfactual losses can yield treatment recommendations that differ from those based on standard loss functions when more than two treatment options are involved.", "conclusion": "Incorporating counterfactual losses into decision theory allows for a more comprehensive evaluation of decision quality."}}
{"id": "2505.09166", "pdf": "https://arxiv.org/pdf/2505.09166", "abs": "https://arxiv.org/abs/2505.09166", "authors": ["Hannu Simonen", "Atte Kiviniemi", "Jonas Oppenlaender"], "title": "An Initial Exploration of Default Images in Text-to-Image Generation", "categories": ["cs.HC", "cs.AI", "H.5.m; I.2.m"], "comment": "16 pages, 6 figures", "summary": "In the creative practice of text-to-image generation (TTI), images are\ngenerated from text prompts. However, TTI models are trained to always yield an\noutput, even if the prompt contains unknown terms. In this case, the model may\ngenerate what we call \"default images\": images that closely resemble each other\nacross many unrelated prompts. We argue studying default images is valuable for\ndesigning better solutions for TTI and prompt engineering. In this paper, we\nprovide the first investigation into default images on Midjourney, a popular\nimage generator. We describe our systematic approach to create input prompts\ntriggering default images, and present the results of our initial experiments\nand several small-scale ablation studies. We also report on a survey study\ninvestigating how default images affect user satisfaction. Our work lays the\nfoundation for understanding default images in TTI and highlights challenges\nand future research directions.", "AI": {"tldr": "This paper explores 'default images' in text-to-image generation using Midjourney, providing insights into their impact and suggesting ways to improve the technology.", "motivation": "To understand why and how default images are generated in text-to-image models, especially when prompts contain unknown terms, to enhance TTI solutions and prompt engineering.", "method": "A systematic approach was developed to create input prompts that trigger default images on Midjourney. Initial experiments, small-scale ablation studies, and a survey were conducted to investigate their characteristics and effects on user satisfaction.", "result": "The study reveals insights into default images, including how they affect user satisfaction, and identifies challenges for improving TTI models.", "conclusion": "Investigating default images is crucial for advancing text-to-image generation technology, offering a foundation for future research and improvements."}}
{"id": "2505.09203", "pdf": "https://arxiv.org/pdf/2505.09203", "abs": "https://arxiv.org/abs/2505.09203", "authors": ["Xiao-Qi Han", "Peng-Jie Guo", "Ze-Feng Gao", "Hao Sun", "Zhong-Yi Lu"], "title": "InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials", "categories": ["cond-mat.mtrl-sci", "cond-mat.supr-con", "cs.AI", "cs.LG"], "comment": "29 pages, 11 figures", "summary": "Developing inverse design methods for functional materials with specific\nproperties is critical to advancing fields like renewable energy, catalysis,\nenergy storage, and carbon capture. Generative models based on diffusion\nprinciples can directly produce new materials that meet performance\nconstraints, thereby significantly accelerating the material design process.\nHowever, existing methods for generating and predicting crystal structures\noften remain limited by low success rates. In this work, we propose a novel\ninverse material design generative framework called InvDesFlow-AL, which is\nbased on active learning strategies. This framework can iteratively optimize\nthe material generation process to gradually guide it towards desired\nperformance characteristics. In terms of crystal structure prediction, the\nInvDesFlow-AL model achieves an RMSE of 0.0423 {\\AA}, representing an 32.96%\nimprovement in performance compared to exsisting generative models.\nAdditionally, InvDesFlow-AL has been successfully validated in the design of\nlow-formation-energy and low-Ehull materials. It can systematically generate\nmaterials with progressively lower formation energies while continuously\nexpanding the exploration across diverse chemical spaces. These results fully\ndemonstrate the effectiveness of the proposed active learning-driven generative\nmodel in accelerating material discovery and inverse design. To further prove\nthe effectiveness of this method, we took the search for BCS superconductors\nunder ambient pressure as an example explored by InvDesFlow-AL. As a result, we\nsuccessfully identified Li\\(_2\\)AuH\\(_6\\) as a conventional BCS superconductor\nwith an ultra-high transition temperature of 140 K. This discovery provides\nstrong empirical support for the application of inverse design in materials\nscience.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u7684\u65b0\u578b\u9006\u5411\u6750\u6599\u8bbe\u8ba1\u751f\u6210\u6846\u67b6InvDesFlow-AL\uff0c\u7528\u4e8e\u529f\u80fd\u6750\u6599\u7684\u5f00\u53d1\u3002\u8be5\u6846\u67b6\u5728\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u4f4e\u5f62\u6210\u80fd\u548c\u4f4eEhull\u6750\u6599\u7684\u8bbe\u8ba1\u4e2d\u4e5f\u5f97\u5230\u4e86\u6210\u529f\u9a8c\u8bc1\u3002\u901a\u8fc7\u5bfb\u627eBCS\u8d85\u5bfc\u4f53\u7684\u4f8b\u5b50\uff0c\u6210\u529f\u786e\u5b9a\u4e86Li\u2082AuH\u2086\u4f5c\u4e3a\u4e00\u79cd\u5177\u6709\u8d85\u9ad8\u8f6c\u53d8\u6e29\u5ea6\uff08140 K\uff09\u7684\u4f20\u7edfBCS\u8d85\u5bfc\u4f53\u3002", "motivation": "\u5f00\u53d1\u5177\u6709\u7279\u5b9a\u6027\u80fd\u7684\u529f\u80fd\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u5bf9\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u3001\u50ac\u5316\u3001\u50a8\u80fd\u548c\u78b3\u6355\u83b7\u7b49\u9886\u57df\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u751f\u6210\u548c\u9884\u6d4b\u6676\u4f53\u7ed3\u6784\u7684\u65b9\u6cd5\u5f80\u5f80\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6750\u6599\u751f\u6210\u6846\u67b6\u6765\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aInvDesFlow-AL\u7684\u9006\u5411\u6750\u6599\u8bbe\u8ba1\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u3002\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u6750\u6599\u751f\u6210\u8fc7\u7a0b\uff0c\u9010\u6e10\u5f15\u5bfc\u5176\u8fbe\u5230\u6240\u9700\u7684\u6027\u80fd\u7279\u5f81\u3002\u6b64\u6a21\u578b\u80fd\u591f\u7cfb\u7edf\u5730\u751f\u6210\u5177\u6709\u9010\u6b65\u964d\u4f4e\u5f62\u6210\u80fd\u7684\u6750\u6599\uff0c\u5e76\u4e0d\u65ad\u6269\u5c55\u5bf9\u591a\u6837\u5316\u5316\u5b66\u7a7a\u95f4\u7684\u63a2\u7d22\u3002", "result": "\u5728\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u65b9\u9762\uff0cInvDesFlow-AL\u6a21\u578b\u5b9e\u73b0\u4e860.0423 \u00c5\u7684RMSE\uff0c\u76f8\u6bd4\u73b0\u6709\u751f\u6210\u6a21\u578b\u6027\u80fd\u63d0\u9ad8\u4e8632.96%\u3002\u6b64\u5916\uff0c\u5728\u4f4e\u5f62\u6210\u80fd\u548c\u4f4eEhull\u6750\u6599\u7684\u8bbe\u8ba1\u4e2d\u4e5f\u53d6\u5f97\u4e86\u6210\u529f\u9a8c\u8bc1\uff0c\u5e76\u6210\u529f\u8bc6\u522b\u51faLi\u2082AuH\u2086\u4e3a\u4e00\u79cd\u8f6c\u53d8\u6e29\u5ea6\u4e3a140 K\u7684\u5e38\u89c4BCS\u8d85\u5bfc\u4f53\u3002", "conclusion": "InvDesFlow-AL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6750\u6599\u53d1\u73b0\u548c\u9006\u5411\u8bbe\u8ba1\u7684\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u4e3b\u52a8\u5b66\u4e60\u9a71\u52a8\u7684\u751f\u6210\u6a21\u578b\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.09208", "pdf": "https://arxiv.org/pdf/2505.09208", "abs": "https://arxiv.org/abs/2505.09208", "authors": ["Lei Fan", "Kunyang Deng", "Fangxue Liu"], "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the rapid advancement of generative artificial intelligence(AI), its\npotential applications in higher education have attracted significant\nattention. This study investigated how 148 students from diverse engineering\ndisciplines and regions across China used generative AI, focusing on its impact\non their learning experience and the opportunities and challenges it poses in\nengineering education. Based on the surveyed data, we explored four key areas:\nthe frequency and application scenarios of AI use among engineering students,\nits impact on students' learning and performance, commonly encountered\nchallenges in using generative AI, and future prospects for its adoption in\nengineering education. The results showed that more than half of the\nparticipants reported a positive impact of generative AI on their learning\nefficiency, initiative, and creativity, with nearly half believing it also\nenhanced their independent thinking. However, despite acknowledging improved\nstudy efficiency, many felt their actual academic performance remained largely\nunchanged and expressed concerns about the accuracy and domain-specific\nreliability of generative AI. Our findings provide a first-hand insight into\nthe current benefits and challenges generative AI brings to students,\nparticularly Chinese engineering students, while offering several\nrecommendations, especially from the students' perspective, for effectively\nintegrating generative AI into engineering education.", "AI": {"tldr": "The study explores the impact of generative AI on engineering students' learning experience in China, highlighting opportunities and challenges.", "motivation": "To understand how generative AI affects engineering students' learning experiences and to identify the opportunities and challenges it presents in higher education.", "method": "Surveyed 148 students from diverse engineering disciplines across China regarding their use of generative AI, its impact on learning, and associated challenges.", "result": "More than half of the participants reported positive effects on learning efficiency, initiative, and creativity, with nearly half also noting improved independent thinking. However, many felt academic performance was largely unchanged and expressed concerns about AI accuracy and reliability.", "conclusion": "Generative AI offers significant benefits to engineering students but also poses challenges. Recommendations are provided for integrating AI effectively into engineering education."}}
{"id": "2505.08986", "pdf": "https://arxiv.org/pdf/2505.08986", "abs": "https://arxiv.org/abs/2505.08986", "authors": ["Amirreza Davar", "Zhengtong Xu", "Siavash Mahmoudi", "Pouya Sohrabipour", "Chaitanya Pallerla", "Yu She", "Wan Shou", "Philip Crandall", "Dongyi Wang"], "title": "ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": "Submitted for journal review", "summary": "Automated poultry processing lines still rely on humans to lift slippery,\neasily bruised carcasses onto a shackle conveyor. Deformability, anatomical\nvariance, and strict hygiene rules make conventional suction and scripted\nmotions unreliable. We present ChicGrasp, an end--to--end hardware--software\nco-design for this task. An independently actuated dual-jaw pneumatic gripper\nclamps both chicken legs, while a conditional diffusion-policy controller,\ntrained from only 50 multi--view teleoperation demonstrations (RGB +\nproprioception), plans 5 DoF end--effector motion, which includes jaw commands\nin one shot. On individually presented raw broiler carcasses, our system\nachieves a 40.6\\% grasp--and--lift success rate and completes the pick to\nshackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning\n(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be\nopen-source. ChicGrasp shows that imitation learning can bridge the gap between\nrigid hardware and variable bio--products, offering a reproducible benchmark\nand a public dataset for researchers in agricultural engineering and robot\nlearning.", "AI": {"tldr": "The paper introduces ChicGrasp, an end-to-end hardware-software co-design for automating poultry processing. It uses a pneumatic gripper and a conditional diffusion-policy controller trained on 50 teleoperation demonstrations to grasp and lift chicken carcasses with a 40.6% success rate.", "motivation": "To automate the task of lifting slippery chicken carcasses onto a shackle conveyor in poultry processing lines, addressing challenges such as deformability, anatomical variance, and hygiene rules that make conventional methods unreliable.", "method": "ChicGrasp employs a dual-jaw pneumatic gripper to clamp chicken legs and a conditional diffusion-policy controller trained from multi-view teleoperation demonstrations (RGB + proprioception) to plan 5 DoF end-effector motion including jaw commands.", "result": "Achieves a 40.6% grasp-and-lift success rate and completes the pick-to-shackle cycle in 38 seconds, outperforming state-of-the-art implicit behaviour cloning (IBC) and LSTM-GMM baselines which fail entirely.", "conclusion": "ChicGrasp demonstrates that imitation learning can bridge the gap between rigid hardware and variable bio-products, providing a reproducible benchmark and public dataset for agricultural engineering and robot learning research."}}
{"id": "2505.08843", "pdf": "https://arxiv.org/pdf/2505.08843", "abs": "https://arxiv.org/abs/2505.08843", "authors": ["Marco Corrias", "Giada Franceschi", "Michele Riva", "Alberto Tampieri", "Karin F\u00f6ttinger", "Ulrike Diebold", "Thomas Pock", "Cesare Franchini"], "title": "Total Variation-Based Image Decomposition and Denoising for Microscopy Images", "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.CV"], "comment": null, "summary": "Experimentally acquired microscopy images are unavoidably affected by the\npresence of noise and other unwanted signals, which degrade their quality and\nmight hide relevant features. With the recent increase in image acquisition\nrate, modern denoising and restoration solutions become necessary. This study\nfocuses on image decomposition and denoising of microscopy images through a\nworkflow based on total variation (TV), addressing images obtained from various\nmicroscopy techniques, including atomic force microscopy (AFM), scanning\ntunneling microscopy (STM), and scanning electron microscopy (SEM). Our\napproach consists in restoring an image by extracting its unwanted signal\ncomponents and subtracting them from the raw one, or by denoising it. We\nevaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving\nthis goal in distinct study cases. Huber-ROF proved to be the most flexible\none, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a\nwider applicability of this method in microscopy, restricted not only to STM,\nAFM, and SEM images. The Python code used for this study is publicly available\nas part of AiSurf. It is designed to be integrated into experimental workflows\nfor image acquisition or can be used to denoise previously acquired images.", "AI": {"tldr": "\u5b9e\u9a8c\u83b7\u53d6\u7684\u663e\u5fae\u56fe\u50cf\u4e0d\u53ef\u907f\u514d\u5730\u53d7\u5230\u566a\u58f0\u548c\u5176\u4ed6\u4e0d\u60f3\u8981\u4fe1\u53f7\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5b83\u4eec\u7684\u8d28\u91cf\u5e76\u53ef\u80fd\u9690\u85cf\u76f8\u5173\u7279\u5f81\u3002\u672c\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u5168\u53d8\u5dee\uff08TV\uff09\u7684\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u663e\u5fae\u56fe\u50cf\u8fdb\u884c\u5206\u89e3\u548c\u964d\u566a\uff0c\u5904\u7406\u4e86\u5305\u62ec\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08AFM\uff09\u3001\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\uff08STM\uff09\u548c\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\uff08SEM\uff09\u5728\u5185\u7684\u5404\u79cd\u663e\u5fae\u6280\u672f\u83b7\u5f97\u7684\u56fe\u50cf\u3002", "motivation": "\u5b9e\u9a8c\u83b7\u53d6\u7684\u663e\u5fae\u56fe\u50cf\u53d7\u566a\u58f0\u548c\u5176\u4ed6\u4e0d\u60f3\u8981\u4fe1\u53f7\u5f71\u54cd\uff0c\u9700\u8981\u73b0\u4ee3\u964d\u566a\u548c\u6062\u590d\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u5168\u53d8\u5dee\uff08TV\uff09\u7684\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u663e\u5fae\u56fe\u50cf\u8fdb\u884c\u5206\u89e3\u548c\u964d\u566a\uff0c\u8bc4\u4f30TV-$L^1$\u3001Huber-ROF\u548cTGV-$L^1$\u5728\u4e0d\u540c\u7814\u7a76\u6848\u4f8b\u4e2d\u7684\u6027\u80fd\u3002", "result": "Huber-ROF\u662f\u6700\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u800cTGV-$L^1$\u6700\u9002\u5408\u964d\u566a\u3002\u8be5\u65b9\u6cd5\u5728\u663e\u5fae\u955c\u4e2d\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "Python\u4ee3\u7801\u516c\u5f00\u53ef\u7528\uff0c\u5e76\u53ef\u96c6\u6210\u5230\u5b9e\u9a8c\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7528\u4e8e\u56fe\u50cf\u83b7\u53d6\u6216\u964d\u566a\u5df2\u83b7\u53d6\u7684\u56fe\u50cf\u3002"}}
{"id": "2505.09262", "pdf": "https://arxiv.org/pdf/2505.09262", "abs": "https://arxiv.org/abs/2505.09262", "authors": ["Hongxin Xiang", "Ke Li", "Mingquan Liu", "Zhixiang Cheng", "Bin Yao", "Wenjie Du", "Jun Xia", "Li Zeng", "Xin Jin", "Xiangxiang Zeng"], "title": "EDBench: Large-Scale Electron Density Data for Molecular Modeling", "categories": ["physics.chem-ph", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Existing molecular machine learning force fields (MLFFs) generally focus on\nthe learning of atoms, molecules, and simple quantum chemical properties (such\nas energy and force), but ignore the importance of electron density (ED)\n$\\rho(r)$ in accurately understanding molecular force fields (MFFs). ED\ndescribes the probability of finding electrons at specific locations around\natoms or molecules, which uniquely determines all ground state properties (such\nas energy, molecular structure, etc.) of interactive multi-particle systems\naccording to the Hohenberg-Kohn theorem. However, the calculation of ED relies\non the time-consuming first-principles density functional theory (DFT) which\nleads to the lack of large-scale ED data and limits its application in MLFFs.\nIn this paper, we introduce EDBench, a large-scale, high-quality dataset of ED\ndesigned to advance learning-based research at the electronic scale. Built upon\nthe PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million\nmolecules. To comprehensively evaluate the ability of models to understand and\nutilize electronic information, we design a suite of ED-centric benchmark tasks\nspanning prediction, retrieval, and generation. Our evaluation on several\nstate-of-the-art methods demonstrates that learning from EDBench is not only\nfeasible but also achieves high accuracy. Moreover, we show that learning-based\nmethod can efficiently calculate ED with comparable precision while\nsignificantly reducing the computational cost relative to traditional DFT\ncalculations. All data and benchmarks from EDBench will be freely available,\nlaying a robust foundation for ED-driven drug discovery and materials science.", "AI": {"tldr": "The paper introduces EDBench, a large-scale dataset of electron density (ED) data covering 3.3 million molecules to advance learning-based research in molecular machine learning force fields (MLFFs). It provides benchmark tasks for prediction, retrieval, and generation, demonstrating feasible and accurate ED calculation with reduced computational cost compared to DFT.", "motivation": "Existing MLFFs focus on atoms, molecules, and simple quantum chemical properties but overlook the importance of electron density (ED), which is crucial for understanding molecular force fields according to the Hohenberg-Kohn theorem. The lack of large-scale ED data limits its application in MLFFs due to time-consuming DFT calculations.", "method": "The authors introduce EDBench, a dataset built upon PCQM4Mv2 that includes accurate ED data for 3.3 million molecules. They design ED-centric benchmark tasks for prediction, retrieval, and generation to evaluate models' ability to understand electronic information.", "result": "Evaluation shows that learning from EDBench is feasible and achieves high accuracy. Learning-based methods can calculate ED efficiently with comparable precision while significantly reducing computational cost relative to traditional DFT calculations.", "conclusion": "EDBench provides a robust foundation for ED-driven drug discovery and materials science by offering freely available data and benchmarks."}}
{"id": "2505.09004", "pdf": "https://arxiv.org/pdf/2505.09004", "abs": "https://arxiv.org/abs/2505.09004", "authors": ["Monica Welfert", "Nathan Stromberg", "Mario Diaz", "Lalitha Sankar"], "title": "Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features", "categories": ["stat.ML", "cs.LG"], "comment": "submitted to IEEE Transactions on Information Theory", "summary": "We propose an adversarial evaluation framework for sensitive feature\ninference based on minimum mean-squared error (MMSE) estimation with a finite\nsample size and linear predictive models. Our approach establishes theoretical\nlower bounds on the true MMSE of inferring sensitive features from noisy\nobservations of other correlated features. These bounds are expressed in terms\nof the empirical MMSE under a restricted hypothesis class and a non-negative\nerror term. The error term captures both the estimation error due to finite\nnumber of samples and the approximation error from using a restricted\nhypothesis class. For linear predictive models, we derive closed-form bounds,\nwhich are order optimal in terms of the noise variance, on the approximation\nerror for several classes of relationships between the sensitive and\nnon-sensitive features, including linear mappings, binary symmetric channels,\nand class-conditional multi-variate Gaussian distributions. We also present a\nnew lower bound that relies on the MSE computed on a hold-out validation\ndataset of the MMSE estimator learned on finite-samples and a restricted\nhypothesis class. Through empirical evaluation, we demonstrate that our\nframework serves as an effective tool for MMSE-based adversarial evaluation of\nsensitive feature inference that balances theoretical guarantees with practical\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08MMSE\uff09\u4f30\u8ba1\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u654f\u611f\u7279\u5f81\u63a8\u65ad\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u4e0b\u754c\u548c\u5b9e\u8bc1\u8bc4\u4f30\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u4ece\u5176\u4ed6\u76f8\u5173\u7279\u5f81\u7684\u566a\u58f0\u89c2\u6d4b\u4e2d\u63a8\u65ad\u654f\u611f\u7279\u5f81\u7684\u51c6\u786e\u6027\uff0c\u5e76\u7406\u89e3\u6709\u9650\u6837\u672c\u548c\u53d7\u9650\u5047\u8bbe\u7c7b\u5bf9\u63a8\u65ad\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eMMSE\u4f30\u8ba1\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u7406\u8bba\u4e0b\u754c\u7684\u63a8\u5bfc\u3001\u95ed\u5f0f\u754c\u8868\u8fbe\u4ee5\u53ca\u5728\u4e0d\u540c\u7279\u5f81\u5173\u7cfb\u4e0b\u7684\u903c\u8fd1\u8bef\u5dee\u5206\u6790\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u65b0\u4e0b\u754c\u3002", "result": "\u63a8\u5bfc\u51fa\u7684\u7406\u8bba\u4e0b\u754c\u80fd\u591f\u51c6\u786e\u53cd\u6620\u654f\u611f\u7279\u5f81\u63a8\u65ad\u7684\u6027\u80fd\u9650\u5236\uff0c\u5e76\u4e14\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u654f\u611f\u7279\u5f81\u63a8\u65ad\u63d0\u4f9b\u4e86\u5e73\u8861\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u9645\u6548\u7387\u7684\u6709\u6548\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u53ca\u591a\u79cd\u7279\u5f81\u5173\u7cfb\u3002"}}
{"id": "2505.08889", "pdf": "https://arxiv.org/pdf/2505.08889", "abs": "https://arxiv.org/abs/2505.08889", "authors": ["Linjie Lyu", "Valentin Deschaintre", "Yannick Hold-Geoffroy", "Milo\u0161 Ha\u0161an", "Jae Shin Yoon", "Thomas Leimk\u00fchler", "Christian Theobalt", "Iliyan Georgiev"], "title": "IntrinsicEdit: Precise generative image manipulation in intrinsic space", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025 Journal track", "summary": "Generative diffusion models have advanced image editing with high-quality\nresults and intuitive interfaces such as prompts and semantic drawing. However,\nthese interfaces lack precise control, and the associated methods typically\nspecialize on a single editing task. We introduce a versatile, generative\nworkflow that operates in an intrinsic-image latent space, enabling semantic,\nlocal manipulation with pixel precision for a range of editing operations.\nBuilding atop the RGB-X diffusion framework, we address key challenges of\nidentity preservation and intrinsic-channel entanglement. By incorporating\nexact diffusion inversion and disentangled channel manipulation, we enable\nprecise, efficient editing with automatic resolution of global illumination\neffects -- all without additional data collection or model fine-tuning. We\ndemonstrate state-of-the-art performance across a variety of tasks on complex\nimages, including color and texture adjustments, object insertion and removal,\nglobal relighting, and their combinations.", "AI": {"tldr": "Generative diffusion models have improved image editing, but lack precise control and specialization in single tasks. This paper introduces a versatile generative workflow operating in intrinsic-image latent space for semantic, local manipulation with pixel precision across various editing operations without additional data collection or model fine-tuning.", "motivation": "Current generative diffusion models provide high-quality image editing results with intuitive interfaces, but these interfaces lack precise control and the methods typically specialize on a single editing task.", "method": "The method involves building a workflow atop the RGB-X diffusion framework to address identity preservation and intrinsic-channel entanglement by incorporating exact diffusion inversion and disentangled channel manipulation.", "result": "This approach enables precise, efficient editing with automatic resolution of global illumination effects and demonstrates state-of-the-art performance across a variety of complex image editing tasks.", "conclusion": "The introduced generative workflow operates effectively in an intrinsic-image latent space, enabling a range of editing operations with pixel precision and without the need for additional data collection or model fine-tuning."}}
{"id": "2505.08932", "pdf": "https://arxiv.org/pdf/2505.08932", "abs": "https://arxiv.org/abs/2505.08932", "authors": ["Mohammad Wasil", "Ahmad Drak", "Brennan Penfold", "Ludovico Scarton", "Maximilian Johenneken", "Alexander Asteroth", "Sebastian Houben"], "title": "Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to the Novel Approaches for Precision Agriculture and\n  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and\nforest monitoring, including seed dispersal in hard-to-reach terrains. However,\na detailed understanding of the forest floor remains a challenge due to high\nnatural variability, quickly changing environmental parameters, and ambiguous\nannotations due to unclear definitions. To address this issue, we adapt the\nSegment Anything Model (SAM), a vision foundation model with strong\ngeneralization capabilities, to segment forest floor objects such as tree\nstumps, vegetation, and woody debris. To this end, we employ\nparameter-efficient fine-tuning (PEFT) to fine-tune a small subset of\nadditional model parameters while keeping the original weights fixed. We adjust\nSAM's mask decoder to generate masks corresponding to our dataset categories,\nallowing for automatic segmentation without manual prompting. Our results show\nthat the adapter-based PEFT method achieves the highest mean intersection over\nunion (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a\nlightweight alternative for resource-constrained UAV platforms.", "AI": {"tldr": "The paper adapts the Segment Anything Model (SAM) using parameter-efficient fine-tuning (PEFT) for forest floor object segmentation, achieving high mIoU. LoRA is proposed as a lightweight alternative for UAVs.", "motivation": "Forest floor understanding is challenging due to variability and ambiguous annotations. SAM's strong generalization capabilities make it suitable for this task.", "method": "Adapt SAM with PEFT by fine-tuning a subset of parameters and adjusting the mask decoder for automatic segmentation of forest floor objects.", "result": "Adapter-based PEFT achieves the highest mIoU. LoRA offers a lightweight solution for UAVs.", "conclusion": "SAM adapted with PEFT effectively segments forest floor objects, with LoRA being a viable option for resource-constrained UAV platforms."}}
{"id": "2505.09026", "pdf": "https://arxiv.org/pdf/2505.09026", "abs": "https://arxiv.org/abs/2505.09026", "authors": ["Domniki Ladopoulou", "Dat Minh Hong", "Petros Dellaportas"], "title": "Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes", "categories": ["stat.AP", "cs.LG", "stat.ML"], "comment": "11 pages, 3 figures, 2 tables", "summary": "Accurate probabilistic forecasting of wind power is essential for maintaining\ngrid stability and enabling efficient integration of renewable energy sources.\nGaussian Process (GP) models offer a principled framework for quantifying\nuncertainty; however, conventional approaches rely on stationary kernels, which\nare inadequate for modeling the inherently non-stationary nature of wind speed\nand power output. We propose a non-stationary GP framework that incorporates\nthe generalized spectral mixture (GSM) kernel, enabling the model to capture\ntime-varying patterns and heteroscedastic behaviors in wind speed and wind\npower data. We evaluate the performance of the proposed model on real-world\nSCADA data across short\\mbox{-,} medium-, and long-term forecasting horizons.\nCompared to standard radial basis function and spectral mixture kernels, the\nGSM-based model outperforms, particularly in short-term forecasts. These\nresults highlight the necessity of modeling non-stationarity in wind power\nforecasting and demonstrate the practical value of non-stationary GP models in\noperational settings.", "AI": {"tldr": "Accurate wind power forecasting is crucial for grid stability and renewable energy integration. Conventional GP models with stationary kernels are inadequate for modeling the non-stationary nature of wind speed and power output. This paper proposes a non-stationary GP framework incorporating the GSM kernel to capture time-varying patterns and heteroscedastic behaviors in wind data. Evaluated on real-world SCADA data, the GSM-based model outperforms standard kernels especially in short-term forecasts.", "motivation": "Wind power forecasting is essential for maintaining grid stability and integrating renewable energy sources. However, existing GP models with stationary kernels cannot adequately model the non-stationary characteristics of wind speed and power output.", "method": "The paper introduces a non-stationary GP framework that uses the generalized spectral mixture (GSM) kernel. This allows the model to capture time-varying patterns and heteroscedastic behaviors in wind speed and wind power data.", "result": "The proposed GSM-based model was evaluated on real-world SCADA data across different forecasting horizons. It showed superior performance compared to standard radial basis function and spectral mixture kernels, particularly in short-term forecasts.", "conclusion": "Modeling non-stationarity is necessary for accurate wind power forecasting. The proposed non-stationary GP model demonstrates practical value in operational settings."}}
{"id": "2505.08949", "pdf": "https://arxiv.org/pdf/2505.08949", "abs": "https://arxiv.org/abs/2505.08949", "authors": ["Kateryna Zorina", "David Kovar", "Mederic Fourmy", "Florent Lamiraux", "Nicolas Mansard", "Justin Carpentier", "Josef Sivic", "Vladimir Petrik"], "title": "Multi-step manipulation task and motion planning guided by video demonstration", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "This work aims to leverage instructional video to solve complex multi-step\ntask-and-motion planning tasks in robotics. Towards this goal, we propose an\nextension of the well-established Rapidly-Exploring Random Tree (RRT) planner,\nwhich simultaneously grows multiple trees around grasp and release states\nextracted from the guiding video. Our key novelty lies in combining contact\nstates and 3D object poses extracted from the guiding video with a traditional\nplanning algorithm that allows us to solve tasks with sequential dependencies,\nfor example, if an object needs to be placed at a specific location to be\ngrasped later. We also investigate the generalization capabilities of our\napproach to go beyond the scene depicted in the instructional video. To\ndemonstrate the benefits of the proposed video-guided planning approach, we\ndesign a new benchmark with three challenging tasks: (I) 3D re-arrangement of\nmultiple objects between a table and a shelf, (ii) multi-step transfer of an\nobject through a tunnel, and (iii) transferring objects using a tray similar to\na waiter transfers dishes. We demonstrate the effectiveness of our planning\nalgorithm on several robots, including the Franka Emika Panda and the KUKA KMR\niiwa. For a seamless transfer of the obtained plans to the real robot, we\ndevelop a trajectory refinement approach formulated as an optimal control\nproblem (OCP).", "AI": {"tldr": "This paper leverages instructional video to solve complex multi-step task-and-motion planning tasks in robotics.", "motivation": "To solve complex multi-step task-and-motion planning tasks in robotics by utilizing instructional video as guidance.", "method": "Propose an extension of the RRT planner which simultaneously grows multiple trees around grasp and release states extracted from the guiding video, combining contact states and 3D object poses extracted from the video with a traditional planning algorithm.", "result": "Demonstrate the effectiveness of the proposed planning algorithm on several robots through a new benchmark with three challenging tasks.", "conclusion": "Developed a trajectory refinement approach formulated as an optimal control problem for a seamless transfer of the obtained plans to the real robot."}}
{"id": "2505.09295", "pdf": "https://arxiv.org/pdf/2505.09295", "abs": "https://arxiv.org/abs/2505.09295", "authors": ["Qiming Wu", "Siqi Li", "Doudou Zhou", "Nan Liu"], "title": "Toward Fair Federated Learning under Demographic Disparities and Data Imbalance", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Ensuring fairness is critical when applying artificial intelligence to\nhigh-stakes domains such as healthcare, where predictive models trained on\nimbalanced and demographically skewed data risk exacerbating existing\ndisparities. Federated learning (FL) enables privacy-preserving collaboration\nacross institutions, but remains vulnerable to both algorithmic bias and\nsubgroup imbalance - particularly when multiple sensitive attributes intersect.\nWe propose FedIDA (Fed erated Learning for Imbalance and D isparity A\nwareness), a framework-agnostic method that combines fairness-aware\nregularization with group-conditional oversampling. FedIDA supports multiple\nsensitive attributes and heterogeneous data distributions without altering the\nconvergence behavior of the underlying FL algorithm. We provide theoretical\nanalysis establishing fairness improvement bounds using Lipschitz continuity\nand concentration inequalities, and show that FedIDA reduces the variance of\nfairness metrics across test sets. Empirical results on both benchmark and\nreal-world clinical datasets confirm that FedIDA consistently improves fairness\nwhile maintaining competitive predictive performance, demonstrating its\neffectiveness for equitable and privacy-preserving modeling in healthcare. The\nsource code is available on GitHub.", "AI": {"tldr": "An abstract about a new framework-agnostic method called FedIDA which aims to improve fairness in federated learning for healthcare AI models.", "motivation": "There is a need to ensure fairness in high-stakes domains like healthcare when using AI. Current federated learning methods, while enabling privacy-preserving collaboration across institutions, are still vulnerable to algorithmic bias and subgroup imbalance especially with multiple intersecting sensitive attributes.", "method": "Propose FedIDA, a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling to support multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm.", "result": "Theoretical analysis shows fairness improvement bounds using Lipschitz continuity and concentration inequalities. Empirical results on benchmark and real-world clinical datasets confirm that FedIDA improves fairness while maintaining competitive predictive performance.", "conclusion": "FedIDA demonstrates effectiveness for equitable and privacy-preserving modeling in healthcare."}}
{"id": "2505.08990", "pdf": "https://arxiv.org/pdf/2505.08990", "abs": "https://arxiv.org/abs/2505.08990", "authors": ["Andrew C. Freeman"], "title": "Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ", "categories": ["cs.MM", "cs.CV", "cs.DC", "cs.NI"], "comment": "Accepted to the ICME 2025 LIVES workshop", "summary": "Live video streaming is increasingly popular on social media platforms. With\nthe growth of live streaming comes an increased need for robust content\nmoderation to remove dangerous, illegal, or otherwise objectionable content.\nWhereas video on demand distribution enables offline content analysis, live\nstreaming imposes restrictions on latency for both analysis and distribution.\nIn this paper, we present extensions to the in-progress Media Over QUIC\nTransport protocol that enable real-time content moderation in one-to-many\nvideo live streams. Importantly, our solution removes only the video segments\nthat contain objectionable content, allowing playback resumption as soon as the\nstream conforms to content policies again. Content analysis tasks may be\ntransparently distributed to arbitrary client devices. We implement and\nevaluate our system in the context of light strobe removal for photosensitive\nviewers, finding that streaming clients experience an increased latency of only\none group-of-pictures duration.", "AI": {"tldr": "The paper discusses the development of a protocol for real-time content moderation in live video streams, enabling removal of objectionable content and resumption of playback when appropriate.", "motivation": "There is an increased need for robust content moderation in live video streaming due to its popularity on social media platforms. Live streaming imposes restrictions on latency for both analysis and distribution.", "method": "Extensions to the Media Over QUIC Transport protocol are presented, which enable real-time content moderation in one-to-many video live streams. The solution removes only the video segments that contain objectionable content and allows playback resumption as soon as the stream conforms to content policies again. Content analysis tasks may be transparently distributed to arbitrary client devices.", "result": "The system was implemented and evaluated in the context of light strobe removal for photosensitive viewers, resulting in an increased latency of only one group-of-pictures duration for streaming clients.", "conclusion": "The proposed extensions to the Media Over QUIC Transport protocol effectively enable real-time content moderation in live video streams with minimal impact on latency."}}
{"id": "2505.08998", "pdf": "https://arxiv.org/pdf/2505.08998", "abs": "https://arxiv.org/abs/2505.08998", "authors": ["Liwen Wu", "Sai Bi", "Zexiang Xu", "Hao Tan", "Kai Zhang", "Fujun Luan", "Haolin Lu", "Ravi Ramamoorthi"], "title": "Neural BRDF Importance Sampling by Reparameterization", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Neural bidirectional reflectance distribution functions (BRDFs) have emerged\nas popular material representations for enhancing realism in physically-based\nrendering. Yet their importance sampling remains a significant challenge. In\nthis paper, we introduce a reparameterization-based formulation of neural BRDF\nimportance sampling that seamlessly integrates into the standard rendering\npipeline with precise generation of BRDF samples. The reparameterization-based\nformulation transfers the distribution learning task to a problem of\nidentifying BRDF integral substitutions. In contrast to previous methods that\nrely on invertible networks and multi-step inference to reconstruct BRDF\ndistributions, our model removes these constraints, which offers greater\nflexibility and efficiency. Our variance and performance analysis demonstrates\nthat our reparameterization method achieves the best variance reduction in\nneural BRDF renderings while maintaining high inference speeds compared to\nexisting baselines.", "AI": {"tldr": "This paper presents a reparameterization-based formulation for neural BRDF importance sampling that improves efficiency and flexibility in the rendering pipeline, achieving superior variance reduction and high inference speeds.", "motivation": "The motivation of this paper is to address the significant challenge of importance sampling in neural bidirectional reflectance distribution functions (BRDFs), which are crucial for enhancing realism in physically-based rendering.", "method": "The authors introduce a reparameterization-based formulation of neural BRDF importance sampling. This method transfers the distribution learning task into identifying BRDF integral substitutions, removing constraints such as invertible networks and multi-step inference used in previous methods. It seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples.", "result": "Through variance and performance analysis, the proposed reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.", "conclusion": "The reparameterization-based formulation of neural BRDF importance sampling offers greater flexibility and efficiency in the rendering process, effectively reducing variance and improving rendering performance."}}
{"id": "2505.09342", "pdf": "https://arxiv.org/pdf/2505.09342", "abs": "https://arxiv.org/abs/2505.09342", "authors": ["Mostafa Jafari", "Alireza Shameli-Sendi"], "title": "Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems", "categories": ["cs.CR", "cs.AI", "cs.LG", "68", "I.2.1"], "comment": "Submitted to IEEE Transactions on Information Forensics and Security\n  (T-IFS), 13 pages, 4 figures", "summary": "Machine learning is a key tool for Android malware detection, effectively\nidentifying malicious patterns in apps. However, ML-based detectors are\nvulnerable to evasion attacks, where small, crafted changes bypass detection.\nDespite progress in adversarial defenses, the lack of comprehensive evaluation\nframeworks in binary-constrained domains limits understanding of their\nrobustness. We introduce two key contributions. First, Prioritized Binary\nRounding, a technique to convert continuous perturbations into binary feature\nspaces while preserving high attack success and low perturbation size. Second,\nthe sigma-binary attack, a novel adversarial method for binary domains,\ndesigned to achieve attack goals with minimal feature changes. Experiments on\nthe Malscan dataset show that sigma-binary outperforms existing attacks and\nexposes key vulnerabilities in state-of-the-art defenses. Defenses equipped\nwith adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant\nbrittleness, with attack success rates exceeding 90% using fewer than 10\nfeature modifications and reaching 100% with just 20. Adversarially trained\ndefenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small\nbudgets but remains vulnerable to unrestricted perturbations, with attack\nsuccess rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates\nstrong robustness against state-of-the-art gradient-based adversarial attacks\nby maintaining an attack success rate below 16.55%, the sigma-binary attack\nsignificantly outperforms these methods, achieving a 94.56% success rate under\nunrestricted perturbations. These findings highlight the critical need for\nprecise method like sigma-binary to expose hidden vulnerabilities in existing\ndefenses and support the development of more resilient malware detection\nsystems.", "AI": {"tldr": "The paper presents two contributions, Prioritized Binary Rounding and the sigma-binary attack, to improve adversarial attacks in binary-constrained domains for Android malware detection. Experiments show that current defenses are highly vulnerable.", "motivation": "Current machine learning-based Android malware detectors are vulnerable to evasion attacks, especially due to lack of comprehensive evaluation frameworks in binary-constrained domains.", "method": "Introduced Prioritized Binary Rounding for converting continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Also introduced sigma-binary attack which achieves attack goals with minimal feature changes.", "result": "The sigma-binary attack outperforms existing methods by achieving over 90% attack success rate using fewer than 10 feature modifications, reaching 100% with just 20. Even strong defenses like PAD-SMA have a 94.56% success rate under unrestricted perturbations.", "conclusion": "The findings emphasize the need for precise adversarial methods like sigma-binary to expose vulnerabilities in existing defenses and support development of more robust malware detection systems."}}
{"id": "2505.09075", "pdf": "https://arxiv.org/pdf/2505.09075", "abs": "https://arxiv.org/abs/2505.09075", "authors": ["Carlos Misael Madrid Padilla", "Oscar Hernan Madrid Padilla", "Sabyasachi Chatterjee"], "title": "Risk Bounds For Distributional Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This work examines risk bounds for nonparametric distributional regression\nestimators. For convex-constrained distributional regression, general upper\nbounds are established for the continuous ranked probability score (CRPS) and\nthe worst-case mean squared error (MSE) across the domain. These theoretical\nresults are applied to isotonic and trend filtering distributional regression,\nyielding convergence rates consistent with those for mean estimation.\nFurthermore, a general upper bound is derived for distributional regression\nunder non-convex constraints, with a specific application to neural\nnetwork-based estimators. Comprehensive experiments on both simulated and real\ndata validate the theoretical contributions, demonstrating their practical\neffectiveness.", "AI": {"tldr": "This paper examines risk bounds for nonparametric distributional regression estimators, establishing upper bounds for CRPS and MSE under convex constraints, and deriving a general bound for non-convex constraints with neural network-based estimators. Experiments validate the theoretical contributions.", "motivation": "To provide a deeper understanding of the performance guarantees (risk bounds) for nonparametric distributional regression estimators, particularly focusing on CRPS and MSE metrics under different types of constraints.", "method": "The authors develop general upper bounds for the CRPS and worst-case MSE in the context of convex-constrained distributional regression. They also explore isotonic and trend filtering distributional regression, and derive a general upper bound for non-convex constrained distributional regression, including neural network-based methods.", "result": "Established upper bounds match convergence rates seen in mean estimation problems. Theoretical results were validated through comprehensive experiments on both simulated and real data, showing practical effectiveness.", "conclusion": "The study successfully provides risk bounds for various forms of distributional regression, extending to non-convex settings such as neural networks, and demonstrates their practical relevance through empirical validation."}}
{"id": "2505.09343", "pdf": "https://arxiv.org/pdf/2505.09343", "abs": "https://arxiv.org/abs/2505.09343", "authors": ["Chenggang Zhao", "Chengqi Deng", "Chong Ruan", "Damai Dai", "Huazuo Gao", "Jiashi Li", "Liyue Zhang", "Panpan Huang", "Shangyan Zhou", "Shirong Ma", "Wenfeng Liang", "Ying He", "Yuqing Wang", "Yuxuan Liu", "Y. X. Wei"], "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures", "categories": ["cs.DC", "cs.AI", "cs.AR"], "comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive version will appear as\n  part of the Industry Track in Proceedings of the 52nd Annual International\n  Symposium on Computer Architecture (ISCA '25)", "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.", "AI": {"tldr": "DeepSeek-V3\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u4e86\u5f53\u524d\u786c\u4ef6\u67b6\u6784\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5982\u5185\u5b58\u5bb9\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u548c\u4e92\u8fde\u5e26\u5bbd\u3002\u5176\u521b\u65b0\u5305\u62ec\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u3001\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\u3001FP8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u591a\u5e73\u9762\u7f51\u7edc\u62d3\u6251\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u63ed\u793a\u4e86\u5f53\u524d\u786c\u4ef6\u67b6\u6784\u7684\u5173\u952e\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "method": "\u4f7f\u7528Multi-head Latent Attention\u63d0\u9ad8\u5185\u5b58\u6548\u7387\uff0cMixture of Experts\u4f18\u5316\u8ba1\u7b97-\u901a\u4fe1\u6743\u8861\uff0cFP8 mixed-precision training\u5145\u5206\u5229\u7528\u786c\u4ef6\u80fd\u529b\uff0c\u4ee5\u53caMulti-Plane Network Topology\u51cf\u5c11\u96c6\u7fa4\u7ea7\u522b\u7684\u7f51\u7edc\u5f00\u9500\u3002", "result": "DeepSeek-V3\u57282048\u4e2aNVIDIA H800 GPU\u4e0a\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u786c\u4ef6\u611f\u77e5\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21\u7684\u6210\u672c\u6548\u76ca\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "conclusion": "\u786c\u4ef6\u4e0e\u6a21\u578b\u7684\u534f\u540c\u8bbe\u8ba1\u5bf9\u4e8e\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u4eba\u5de5\u667a\u80fd\u5de5\u4f5c\u8d1f\u8f7d\u9700\u6c42\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u9645\u7684\u521b\u65b0\u84dd\u56fe\u3002"}}
{"id": "2505.09087", "pdf": "https://arxiv.org/pdf/2505.09087", "abs": "https://arxiv.org/abs/2505.09087", "authors": ["He Wang", "Yikun Zhang", "Jie Chen", "Jian Zhan", "Yaoqi Zhou"], "title": "A Comparative Review of RNA Language Models", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Given usefulness of protein language models (LMs) in structure and functional\ninference, RNA LMs have received increased attentions in the last few years.\nHowever, these RNA models are often not compared against the same standard.\nHere, we divided RNA LMs into three classes (pretrained on multiple RNA types\n(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with\nDNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein\nLMs as controls in zero-shot prediction of RNA secondary structure and\nfunctional classification. Results shows that the models doing well on\nsecondary structure prediction often perform worse in function classification\nor vice versa, suggesting that more balanced unsupervised training is needed.", "AI": {"tldr": "RNA\u8bed\u8a00\u6a21\u578b\u88ab\u5206\u4e3a\u4e09\u7c7b\uff0c\u5e76\u4e0eDNA\u548c\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u4e00\u8d77\u5728\u96f6\u6837\u672cRNA\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u548c\u529f\u80fd\u5206\u7c7b\u4e2d\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u64c5\u957f\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u7684\u6a21\u578b\u5728\u529f\u80fd\u5206\u7c7b\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u53cd\u4e4b\u4ea6\u7136\uff0c\u63d0\u793a\u9700\u8981\u66f4\u5e73\u8861\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u3002", "motivation": "\u5c3d\u7ba1\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u548c\u529f\u80fd\u63a8\u65ad\u4e2d\u7684\u6709\u6548\u6027\uff0cRNA\u8bed\u8a00\u6a21\u578b\u5728\u8fc7\u53bb\u51e0\u5e74\u624d\u5f00\u59cb\u53d7\u5230\u66f4\u591a\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u5c06RNA\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u4e09\u7c7b\uff1a\u5728\u591a\u79cdRNA\u7c7b\u578b\uff08\u7279\u522b\u662f\u975e\u7f16\u7801RNA\uff09\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3001\u7279\u5b9a\u7528\u9014RNA\u6a21\u578b\u4ee5\u53ca\u5c06RNA\u4e0eDNA\u6216\u86cb\u767d\u8d28\u6216\u4e24\u8005\u7edf\u4e00\u8d77\u6765\u7684\u6a21\u578b\u3002\u7136\u540e\u4f7f\u752813\u4e2aRNA\u8bed\u8a00\u6a21\u578b\u30013\u4e2aDNA\u6a21\u578b\u548c1\u4e2a\u86cb\u767d\u8d28\u6a21\u578b\u4f5c\u4e3a\u5bf9\u7167\uff0c\u5728\u96f6\u6837\u672cRNA\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u548c\u529f\u80fd\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u64c5\u957f\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u7684\u6a21\u578b\u5728\u529f\u80fd\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5f80\u5f80\u8f83\u5dee\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002", "conclusion": "\u9700\u8981\u66f4\u5e73\u8861\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u6765\u63d0\u9ad8RNA\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u6027\u80fd\u3002"}}
{"id": "2505.09109", "pdf": "https://arxiv.org/pdf/2505.09109", "abs": "https://arxiv.org/abs/2505.09109", "authors": ["Yuxing Chen", "Bowen Xiao", "He Wang"], "title": "FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Due to the deformability of garments, generating a large amount of\nhigh-quality data for robotic garment manipulation tasks is highly challenging.\nIn this paper, we present a synthetic garment dataset that can be used for\nrobotic garment folding. We begin by constructing geometric garment templates\nbased on keypoints and applying generative models to generate realistic texture\npatterns. Leveraging these keypoint annotations, we generate folding\ndemonstrations in simulation and train folding policies via closed-loop\nimitation learning. To improve robustness, we propose KG-DAgger, which uses a\nkeypoint-based strategy to generate demonstration data for recovering from\nfailures. KG-DAgger significantly improves the model performance, boosting the\nreal-world success rate by 25\\%. After training with 15K trajectories (about 2M\nimage-action pairs), the model achieves a 75\\% success rate in the real world.\nExperiments in both simulation and real-world settings validate the\neffectiveness of our proposed framework.", "AI": {"tldr": "This paper addresses the challenge of generating high-quality data for robotic garment manipulation by presenting a synthetic garment dataset and proposing KG-DAgger, which improves model performance and boosts real-world success rate.", "motivation": "The deformability of garments makes it difficult to generate a large amount of high-quality data for robotic garment manipulation tasks.", "method": "The authors construct geometric garment templates based on keypoints and apply generative models to create realistic texture patterns. They use these keypoint annotations to generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. They also propose KG-DAgger, a keypoint-based strategy to generate demonstration data for recovering from failures.", "result": "After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75% success rate in the real world, with KG-DAgger boosting the real-world success rate by 25%. Experiments validate the effectiveness of the proposed framework in both simulation and real-world settings.", "conclusion": "The synthetic garment dataset and KG-DAgger significantly improve the robustness and success rate of robotic garment folding."}}
{"id": "2505.09371", "pdf": "https://arxiv.org/pdf/2505.09371", "abs": "https://arxiv.org/abs/2505.09371", "authors": ["Akash Kundu", "Stefano Mangini"], "title": "TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG"], "comment": "The code will be available soon! Comments are welcomed!", "summary": "Variational quantum algorithms hold the promise to address meaningful quantum\nproblems already on noisy intermediate-scale quantum hardware, but they face\nthe challenge of designing quantum circuits that both solve the target problem\nand comply with device limitations. Quantum architecture search (QAS) automates\nthis design process, with reinforcement learning (RL) emerging as a promising\napproach. Yet, RL-based QAS methods encounter significant scalability issues,\nas computational and training costs grow rapidly with the number of qubits,\ncircuit depth, and noise, severely impacting performance. To address these\nchallenges, we introduce $\\textit{TensorRL-QAS}$, a scalable framework that\ncombines tensor network (TN) methods with RL for designing quantum circuits. By\nwarm-starting the architecture search with a matrix product state approximation\nof the target solution, TensorRL-QAS effectively narrows the search space to\nphysically meaningful circuits, accelerating convergence to the desired\nsolution. Tested on several quantum chemistry problems of up to 12-qubit,\nTensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth\ncompared to baseline methods, while maintaining or surpassing chemical\naccuracy. It reduces function evaluations by up to 100-fold, accelerates\ntraining episodes by up to $98\\%$, and achieves up to $50\\%$ success\nprobability for 10-qubit systems-far exceeding the $<1\\%$ rates of baseline\napproaches. Robustness and versatility are demonstrated both in the noiseless\nand noisy scenarios, where we report a simulation of up to 8-qubit. These\nadvancements establish TensorRL-QAS as a promising candidate for a scalable and\nefficient quantum circuit discovery protocol on near-term quantum hardware.", "AI": {"tldr": "TensorRL-QAS \u662f\u4e00\u79cd\u7ed3\u5408\u5f20\u91cf\u7f51\u7edc\u65b9\u6cd5\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u91cf\u5b50\u7535\u8def\u3002\u901a\u8fc7\u77e9\u9635\u4e58\u79ef\u6001\u8fd1\u4f3c\u76ee\u6807\u89e3\u6765\u521d\u59cb\u5316\u67b6\u6784\u641c\u7d22\uff0c\u4ece\u800c\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u5e76\u52a0\u901f\u6536\u655b\u5230\u7406\u60f3\u89e3\u3002\u5728\u591a\u4e2a\u91cf\u5b50\u5316\u5b66\u95ee\u9898\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86 CNOT \u6570\u91cf\u548c\u7535\u8def\u6df1\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u6709\u6f5c\u529b\u89e3\u51b3\u5608\u6742\u4e2d\u7b49\u89c4\u6a21\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u6709\u610f\u4e49\u95ee\u9898\uff0c\u4f46\u8bbe\u8ba1\u65e2\u7b26\u5408\u8bbe\u5907\u9650\u5236\u53c8\u80fd\u89e3\u51b3\u95ee\u9898\u7684\u91cf\u5b50\u7535\u8def\u5177\u6709\u6311\u6218\u6027\u3002\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u65b9\u6cd5\u867d\u7136\u6709\u5e0c\u671b\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86 TensorRL-QAS \u6846\u67b6\uff0c\u5c06\u5f20\u91cf\u7f51\u7edc\u65b9\u6cd5\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u4ee5\u8bbe\u8ba1\u91cf\u5b50\u7535\u8def\u3002\u4f7f\u7528\u77e9\u9635\u4e58\u79ef\u6001\u8fd1\u4f3c\u76ee\u6807\u89e3\u6765\u521d\u59cb\u5316\u67b6\u6784\u641c\u7d22\uff0c\u4ece\u800c\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u81f3\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u7535\u8def\u3002", "result": "\u5728\u6700\u591a 12 \u91cf\u5b50\u6bd4\u7279\u7684\u591a\u4e2a\u91cf\u5b50\u5316\u5b66\u95ee\u9898\u4e0a\uff0cTensorRL-QAS \u5c06 CNOT \u6570\u91cf\u548c\u7535\u8def\u6df1\u5ea6\u51cf\u5c11\u591a\u8fbe 10 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u5316\u5b66\u7cbe\u5ea6\u3002\u5b83\u5c06\u51fd\u6570\u8bc4\u4f30\u51cf\u5c11\u591a\u8fbe 100 \u500d\uff0c\u52a0\u901f\u8bad\u7ec3\u56de\u5408\u591a\u8fbe 98%\uff0c\u5e76\u5728 10 \u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u9ad8\u8fbe 50% \u7684\u6210\u529f\u6982\u7387\u3002\u5728\u65e0\u566a\u58f0\u548c\u6709\u566a\u58f0\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u591a\u6837\u6027\u3002", "conclusion": "TensorRL-QAS \u88ab\u786e\u7acb\u4e3a\u4e00\u4e2a\u6709\u524d\u9014\u7684\u5019\u9009\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u8fd1\u671f\u91cf\u5b50\u786c\u4ef6\u4e0a\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u91cf\u5b50\u7535\u8def\u53d1\u73b0\u534f\u8bae\u3002"}}
{"id": "2505.09098", "pdf": "https://arxiv.org/pdf/2505.09098", "abs": "https://arxiv.org/abs/2505.09098", "authors": ["Yan Hao Ling", "Zhouhao Yang", "Jonathan Scarlett"], "title": "Statistical Mean Estimation with Coded Relayed Observations", "categories": ["cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "comment": null, "summary": "We consider a problem of statistical mean estimation in which the samples are\nnot observed directly, but are instead observed by a relay (``teacher'') that\ntransmits information through a memoryless channel to the decoder\n(``student''), who then produces the final estimate. We consider the minimax\nestimation error in the large deviations regime, and establish achievable error\nexponents that are tight in broad regimes of the estimation accuracy and\nchannel quality. In contrast, two natural baseline methods are shown to yield\nstrictly suboptimal error exponents. We initially focus on Bernoulli sources\nand binary symmetric channels, and then generalize to sub-Gaussian and\nheavy-tailed settings along with arbitrary discrete memoryless channels.", "AI": {"tldr": "\u5728\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u95ee\u9898\u4e2d\uff0c\u5f53\u6837\u672c\u901a\u8fc7\u8bb0\u5fc6\u6d88\u9664\u901a\u9053\u4f20\u8f93\u65f6\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6700\u4f18\u7684\u8bef\u5dee\u6307\u6570\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5e7f\u6cdb\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u901a\u9053\u8d28\u91cf\u6761\u4ef6\u4e0b\u662f\u7d27\u7684\u3002\u76f8\u8f83\u4e4b\u4e0b\uff0c\u4e24\u79cd\u81ea\u7136\u7684\u57fa\u7ebf\u65b9\u6cd5\u4ea7\u751f\u7684\u8bef\u5dee\u6307\u6570\u4e25\u683c\u6b21\u4f18\u3002", "motivation": "\u7814\u7a76\u8005\u5173\u6ce8\u7684\u662f\u4e00\u4e2a\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u5176\u4e2d\u6837\u672c\u4e0d\u662f\u76f4\u63a5\u88ab\u89c2\u5bdf\u5230\uff0c\u800c\u662f\u901a\u8fc7\u4e00\u4e2a\u4e2d\u7ee7\uff08\u6559\u5e08\uff09\u901a\u8fc7\u8bb0\u5fc6\u6d88\u9664\u901a\u9053\u5c06\u4fe1\u606f\u4f20\u9012\u7ed9\u89e3\u7801\u5668\uff08\u5b66\u751f\uff09\uff0c\u5b66\u751f\u6700\u7ec8\u8fdb\u884c\u4f30\u8ba1\u3002\u76ee\u6807\u662f\u5728\u5927\u504f\u5dee\u5236\u5ea6\u4e0b\u5206\u6790\u6700\u5c0f\u6700\u5927\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u5bfb\u627e\u7d27\u7684\u53ef\u5b9e\u73b0\u8bef\u5dee\u6307\u6570\u3002", "method": "\u9996\u5148\u805a\u7126\u4e8e\u4f2f\u52aa\u5229\u6e90\u548c\u4e8c\u8fdb\u5236\u5bf9\u79f0\u901a\u9053\uff0c\u7136\u540e\u63a8\u5e7f\u5230\u6b21\u9ad8\u65af\u548c\u91cd\u5c3e\u5206\u5e03\u8bbe\u7f6e\u4ee5\u53ca\u4efb\u610f\u79bb\u6563\u65e0\u8bb0\u5fc6\u901a\u9053\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u4ee5\u83b7\u5f97\u7d27\u7684\u8bef\u5dee\u6307\u6570\uff0c\u5e76\u4e0e\u4e24\u4e2a\u81ea\u7136\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u8bef\u5dee\u6307\u6570\u5728\u5e7f\u6cdb\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u901a\u9053\u8d28\u91cf\u6761\u4ef6\u4e0b\u662f\u7d27\u7684\uff0c\u800c\u4e24\u79cd\u81ea\u7136\u7684\u57fa\u7ebf\u65b9\u6cd5\u4ea7\u751f\u4e86\u4e25\u683c\u6b21\u4f18\u7684\u8bef\u5dee\u6307\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9002\u5f53\u7684\u4f20\u8f93\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u6837\u672c\u4e0d\u76f4\u63a5\u89c2\u5bdf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u4f18\u7684\u5747\u503c\u4f30\u8ba1\u6027\u80fd\u3002\u8fd9\u5bf9\u4e8e\u6d89\u53ca\u95f4\u63a5\u89c2\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.09193", "pdf": "https://arxiv.org/pdf/2505.09193", "abs": "https://arxiv.org/abs/2505.09193", "authors": ["Wei Jiang", "Junru Li", "Kai Zhang", "Li Zhang"], "title": "BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression", "categories": ["eess.IV", "cs.CV"], "comment": "The first learned video codec that surpasses VTM 13.2 RA across all\n  standard test datasets. Code will be available at\n  https://github.com/JiangWeibeta/ECVC", "summary": "Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead. To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets. Code will be available\nat https://github.com/JiangWeibeta/ECVC.", "AI": {"tldr": "The paper introduces BiECVC, a bidirectional video compression framework that incorporates local and non-local context modeling with adaptive gating, achieving state-of-the-art performance surpassing VTM 13.2 RA.", "motivation": "To address the limitations of existing bidirectional video compression (BVC) methods which primarily exploit temporal motion while neglecting non-local correlations across frames and lack adaptability to suppress harmful contexts from fast motion or occlusion.", "method": "BiECVC reuses high-quality features from lower layers for local context enhancement, adopts a linear attention mechanism for efficient non-local dependency modeling, and introduces Bidirectional Context Gating to dynamically filter contextual information based on conditional coding results.", "result": "BiECVC reduces bit-rate by 13.4% and 15.7% compared to VTM 13.2 under Random Access configuration with intra periods of 32 and 64 respectively, making it the first learned video codec to surpass VTM 13.2 RA across all standard test datasets.", "conclusion": "BiECVC achieves state-of-the-art performance in bidirectional video compression by effectively modeling local and non-local contexts and adaptively gating contextual information."}}
{"id": "2505.09099", "pdf": "https://arxiv.org/pdf/2505.09099", "abs": "https://arxiv.org/abs/2505.09099", "authors": ["Shirui Lyu", "Vittorio Caggiano", "Matteo Leonetti", "Dario Farina", "Letizia Gionfrida"], "title": "Imitation Learning for Adaptive Control of a Virtual Soft Exoglove", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "The use of wearable robots has been widely adopted in rehabilitation training\nfor patients with hand motor impairments. However, the uniqueness of patients'\nmuscle loss is often overlooked. Leveraging reinforcement learning and a\nbiologically accurate musculoskeletal model in simulation, we propose a\ncustomized wearable robotic controller that is able to address specific muscle\ndeficits and to provide compensation for hand-object manipulation tasks. Video\ndata of a same subject performing human grasping tasks is used to train a\nmanipulation model through learning from demonstration. This manipulation model\nis subsequently fine-tuned to perform object-specific interaction tasks. The\nmuscle forces in the musculoskeletal manipulation model are then weakened to\nsimulate neurological motor impairments, which are later compensated by the\nactuation of a virtual wearable robotics glove. Results shows that integrating\nthe virtual wearable robotic glove provides shared assistance to support the\nhand manipulator with weakened muscle forces. The learned exoglove controller\nachieved an average of 90.5\\% of the original manipulation proficiency.", "AI": {"tldr": "This paper proposes a customized wearable robotic controller using reinforcement learning and a musculoskeletal model to compensate for specific muscle deficits in hand-object manipulation tasks. The controller is trained with video data of human grasping tasks and fine-tuned for object-specific interactions. When integrated into a virtual exoglove, it provides shared assistance for weakened hand muscles, achieving 90.5% of original manipulation proficiency.", "motivation": "The motivation behind this research is the need for more personalized rehabilitation solutions for patients with hand motor impairments, acknowledging the uniqueness of each patient's muscle loss.", "method": "The method involves using reinforcement learning on a biologically accurate musculoskeletal model to create a customized wearable robotic controller. Video data of human grasping tasks is used to train a manipulation model which is then fine-tuned for specific object interaction tasks. Muscle forces in the model are weakened to simulate impairments and compensated by a virtual wearable robotics glove.", "result": "The results demonstrate that integrating the virtual wearable robotic glove provides effective shared assistance to support hand manipulators with weakened muscle forces. The learned exoglove controller achieves an average of 90.5% of the original manipulation proficiency.", "conclusion": "In conclusion, the proposed customized wearable robotic controller shows promise in addressing specific muscle deficits in hand-object manipulation tasks through the use of reinforcement learning and musculoskeletal models."}}
{"id": "2505.09382", "pdf": "https://arxiv.org/pdf/2505.09382", "abs": "https://arxiv.org/abs/2505.09382", "authors": ["Zhengyan Sheng", "Jinghao He", "Liping Chen", "Kong Aik Lee", "Zhen-Hua Ling"], "title": "The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Voice timbre refers to the unique quality or character of a person's voice\nthat distinguishes it from others as perceived by human hearing. The Voice\nTimbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the\nvoice timbre attribute in a comparative manner. In this challenge, the human\nimpression of voice timbre is verbalized with a set of sensory descriptors,\nincluding bright, coarse, soft, magnetic, and so on. The timbre is explained\nfrom the comparison between two voices in their intensity within a specific\ndescriptor dimension. The VtaD 2025 challenge starts in May and culminates in a\nspecial proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,\nChina.", "AI": {"tldr": "Voice timbre is being studied in the VtaD 2025 challenge, where human impression of voice timbre is verbalized with sensory descriptors and compared between two voices.", "motivation": "To explain voice timbre attribute in a comparative manner using sensory descriptors.", "method": "The timbre is explained from the comparison between two voices in their intensity within a specific descriptor dimension.", "result": "Not mentioned in the abstract.", "conclusion": "VtaD 2025 challenge will have a special proposal at the NCMMSC2025 conference."}}
{"id": "2505.09110", "pdf": "https://arxiv.org/pdf/2505.09110", "abs": "https://arxiv.org/abs/2505.09110", "authors": ["Zhihao Dou", "Jiaqi Wang", "Wei Sun", "Zhuqing Liu", "Minghong Fang"], "title": "Toward Malicious Clients Detection in Federated Learning", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "To appear in ACM ASIACCS 2025", "summary": "Federated learning (FL) enables multiple clients to collaboratively train a\nglobal machine learning model without sharing their raw data. However, the\ndecentralized nature of FL introduces vulnerabilities, particularly to\npoisoning attacks, where malicious clients manipulate their local models to\ndisrupt the training process. While Byzantine-robust aggregation rules have\nbeen developed to mitigate such attacks, they remain inadequate against more\nadvanced threats. In response, recent advancements have focused on FL detection\ntechniques to identify potentially malicious participants. Unfortunately, these\nmethods often misclassify numerous benign clients as threats or rely on\nunrealistic assumptions about the server's capabilities. In this paper, we\npropose a novel algorithm, SafeFL, specifically designed to accurately identify\nmalicious clients in FL. The SafeFL approach involves the server collecting a\nseries of global models to generate a synthetic dataset, which is then used to\ndistinguish between malicious and benign models based on their behavior.\nExtensive testing demonstrates that SafeFL outperforms existing methods,\noffering superior efficiency and accuracy in detecting malicious clients.", "AI": {"tldr": "In this paper, the authors propose SafeFL, a new algorithm that accurately identifies malicious clients in federated learning by generating a synthetic dataset from a series of global models. SafeFL shows better performance compared to existing methods.", "motivation": "The motivation is to address the inadequacy of current Byzantine-robust aggregation rules and FL detection techniques in identifying malicious clients within the federated learning framework, particularly against advanced threats.", "method": "SafeFL involves the server collecting a series of global models to generate a synthetic dataset, which is used to distinguish between malicious and benign models based on their behavior.", "result": "Extensive testing demonstrates that SafeFL outperforms existing methods in terms of efficiency and accuracy for detecting malicious clients.", "conclusion": "SafeFL is an effective solution for accurately identifying malicious clients in federated learning."}}
{"id": "2505.09315", "pdf": "https://arxiv.org/pdf/2505.09315", "abs": "https://arxiv.org/abs/2505.09315", "authors": ["Xuefeng Jiang", "Yuan Ma", "Pengxiang Li", "Leimeng Xu", "Xin Wen", "Kun Zhan", "Zhongpu Xia", "Peng Jia", "XianPeng Lang", "Sheng Sun"], "title": "TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Under review", "summary": "In recent years, diffusion model has shown its potential across diverse\ndomains from vision generation to language modeling. Transferring its\ncapabilities to modern autonomous driving systems has also emerged as a\npromising direction.In this work, we propose TransDiffuser, an encoder-decoder\nbased generative trajectory planning model for end-to-end autonomous driving.\nThe encoded scene information serves as the multi-modal conditional input of\nthe denoising decoder. To tackle the mode collapse dilemma in generating\nhigh-quality diverse trajectories, we introduce a simple yet effective\nmulti-modal representation decorrelation optimization mechanism during the\ntraining process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,\nsurpassing previous state-of-the-art methods without any anchor-based prior\ntrajectories.", "AI": {"tldr": "TransDiffuser is an encoder-decoder model for generative trajectory planning in autonomous driving, which uses scene information as multi-modal conditional input and introduces a decorrelation optimization mechanism to solve mode collapse dilemma. It achieves PDMS of 94.85 on NAVSIM benchmark without anchor-based prior trajectories.", "motivation": "To transfer the capabilities of diffusion models to modern autonomous driving systems and generate high-quality diverse trajectories.", "method": "Propose TransDiffuser, an encoder-decoder based generative trajectory planning model. The encoded scene information serves as the multi-modal conditional input of the denoising decoder and a multi-modal representation decorrelation optimization mechanism is introduced during the training process.", "result": "Achieves PDMS of 94.85 on the NAVSIM benchmark, surpassing previous state-of-the-art methods without any anchor-based prior trajectories.", "conclusion": "TransDiffuser successfully generates high-quality diverse trajectories in autonomous driving systems and outperforms previous methods."}}
{"id": "2505.09323", "pdf": "https://arxiv.org/pdf/2505.09323", "abs": "https://arxiv.org/abs/2505.09323", "authors": ["Pengli Zhu", "Yingji Fu", "Nanguang Chen", "Anqi Qiu"], "title": "Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025", "summary": "This study, we propose a novel Q-space Guided Collaborative Attention\nTranslation Networks (Q-CATN) for multi-shell, high-angular resolution DWI\n(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly\nacquired structural MRI data. Q-CATN employs a collaborative attention\nmechanism to effectively extract complementary information from multiple\nmodalities and dynamically adjust its internal representations based on\nflexible q-space information, eliminating the need for fixed sampling schemes.\nAdditionally, we introduce a range of task-specific constraints to preserve\nanatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic\nrelationships between directional DWI signal distributions and q-space.\nExtensive experiments on the Human Connectome Project (HCP) dataset demonstrate\nthat Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,\nand QGAN, in estimating parameter maps and fiber tracts both quantitatively and\nqualitatively, while preserving fine-grained details. Notably, its ability to\naccommodate flexible q-space sampling highlights its potential as a promising\ntoolkit for clinical and research applications. Our code is available at\nhttps://github.com/Idea89560041/Q-CATN.", "AI": {"tldr": "This study proposes Q-CATN, a novel network for MS-HARDI synthesis from flexible q-space sampling using structural MRI data. It uses collaborative attention and task-specific constraints to preserve anatomical fidelity in DWI, outperforming existing methods on the HCP dataset.", "motivation": "The motivation is to develop a method for multi-shell, high-angular resolution DWI synthesis that can work with flexible q-space sampling while preserving anatomical fidelity, improving upon current methods that require fixed sampling schemes.", "method": "Q-CATN employs a collaborative attention mechanism to extract complementary information from multiple modalities and adjusts its internal representations based on flexible q-space information. Task-specific constraints are introduced to preserve anatomical fidelity in DWI.", "result": "Extensive experiments on the HCP dataset show that Q-CATN outperforms existing methods (1D-qDL, 2D-qDL, MESC-SD, QGAN) in estimating parameter maps and fiber tracts both quantitatively and qualitatively while preserving fine-grained details.", "conclusion": "Q-CATN's ability to accommodate flexible q-space sampling makes it a promising toolkit for clinical and research applications."}}
{"id": "2505.09393", "pdf": "https://arxiv.org/pdf/2505.09393", "abs": "https://arxiv.org/abs/2505.09393", "authors": ["Huakun Liu", "Hiroki Ota", "Xin Wei", "Yutaro Hirao", "Monica Perusquia-Hernandez", "Hideaki Uchiyama", "Kiyoshi Kiyokawa"], "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy.", "AI": {"tldr": "UMotion is an uncertainty-driven, online framework that combines IMUs and UWB sensors with a UKF for accurate 3D human shape and pose estimation.", "motivation": "Sparse wearable inertial measurement units (IMUs) are popular for estimating 3D human motion but face challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies.", "method": "UMotion uses six integrated body-worn ultra-wideband (UWB) distance sensors with IMUs. A tightly coupled Unscented Kalman Filter (UKF) framework fuses uncertainties from sensor data and estimated human motion based on individual body shape.", "result": "Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and improving pose accuracy over state-of-the-art methods.", "conclusion": "UMotion addresses the challenges faced by sparse wearable IMUs through its innovative use of UWB sensors and UKF, leading to more accurate and stable 3D human shape and pose estimation."}}
{"id": "2505.09334", "pdf": "https://arxiv.org/pdf/2505.09334", "abs": "https://arxiv.org/abs/2505.09334", "authors": ["Sadman Sakib Alif", "Nasim Anzum Promise", "Fiaz Al Abid", "Aniqua Nusrat Zereen"], "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Lung cancer is a leading cause of cancer-related deaths globally, where early\ndetection and accurate diagnosis are critical for improving survival rates.\nWhile deep learning, particularly convolutional neural networks (CNNs), has\nrevolutionized medical image analysis by detecting subtle patterns indicative\nof early-stage lung cancer, its adoption faces challenges. These models are\noften computationally expensive and require significant resources, making them\nunsuitable for resource constrained environments. Additionally, their lack of\ntransparency hinders trust and broader adoption in sensitive fields like\nhealthcare. Knowledge distillation addresses these challenges by transferring\nknowledge from large, complex models (teachers) to smaller, lightweight models\n(students). We propose a knowledge distillation-based approach for lung cancer\ndetection, incorporating explainable AI (XAI) techniques to enhance model\ntransparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,\nand VGG16, are evaluated as teacher models. We developed and trained a\nlightweight student model, Distilled Custom Student Network (DCSNet) using\nResNet50 as the teacher. This approach not only ensures high diagnostic\nperformance in resource-constrained settings but also addresses transparency\nconcerns, facilitating the adoption of AI-driven diagnostic tools in\nhealthcare.", "AI": {"tldr": "Lung cancer is a major cause of cancer-related deaths. Deep learning models, while effective, are resource-intensive and lack transparency. Knowledge distillation and explainable AI (XAI) can address these issues. We propose DCSNet, a lightweight student model for lung cancer detection, using ResNet50 as the teacher model among eight evaluated CNNs.", "motivation": "Deep learning models like CNNs have revolutionized medical image analysis but face challenges such as computational expense and lack of transparency which hinder their adoption in healthcare, especially in resource-constrained environments.", "method": "The paper evaluates eight CNNs as potential teacher models and uses knowledge distillation to develop a lightweight student model called Distilled Custom Student Network (DCSNet). ResNet50 was selected as the teacher model. The method also incorporates XAI techniques to enhance the transparency of the model.", "result": "DCSNet ensures high diagnostic performance even in resource-constrained settings and addresses transparency concerns through the use of XAI techniques.", "conclusion": "This approach facilitates the broader adoption of AI-driven diagnostic tools in healthcare by making them more efficient and trustworthy."}}
{"id": "2505.09395", "pdf": "https://arxiv.org/pdf/2505.09395", "abs": "https://arxiv.org/abs/2505.09395", "authors": ["Chen-Yu Liu", "Kuan-Cheng Chen", "Yi-Chien Chen", "Samuel Yen-Chi Chen", "Wei-Hao Huang", "Wei-Jia Huang", "Yen-Jui Chang"], "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.", "AI": {"tldr": "The paper presents Quantum Parameter Adaptation (QPA), a hybrid quantum-classical framework for efficient typhoon forecasting model learning, which significantly reduces trainable parameters while maintaining accuracy.", "motivation": "Typhoon trajectory forecasting is crucial but computationally intensive due to atmospheric dynamics complexity and deep learning resource demands.", "method": "Quantum Parameter Adaptation (QPA) is introduced, leveraging quantum neural networks (QNNs) only during training as part of the Quantum-Train (QT) framework, integrated with an Attention-based Multi-ConvGRU model for parameter-efficient training.", "result": "QPA reduces the number of trainable parameters significantly while preserving predictive performance.", "conclusion": "This work marks the first application of quantum machine learning to large-scale typhoon trajectory prediction, providing a scalable and energy-efficient method for climate modeling."}}
{"id": "2505.09161", "pdf": "https://arxiv.org/pdf/2505.09161", "abs": "https://arxiv.org/abs/2505.09161", "authors": ["Yu Xin", "Peng Liu", "Zhuohang Xie", "Wenhui Mi", "Pengyue Gao", "Hong Jian Zhao", "Jian Lv", "Yanchao Wang", "Yanming Ma"], "title": "Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Even though thermodynamic energy-based crystal structure prediction (CSP) has\nrevolutionized materials discovery, the energy-driven CSP approaches often\nstruggle to identify experimentally realizable metastable materials synthesized\nthrough kinetically controlled pathways, creating a critical gap between\ntheoretical predictions and experimental synthesis. Here, we propose a\nsynthesizability-driven CSP framework that integrates symmetry-guided structure\nderivation with a Wyckoff encode-based machine-learning model, allowing for the\nefficient localization of subspaces likely to yield highly synthesizable\nstructures. Within the identified promising subspaces, a structure-based\nsynthesizability evaluation model, fine-tuned using recently synthesized\nstructures to enhance predictive accuracy, is employed in conjunction with ab\ninitio calculations to systematically identify synthesizable candidates. The\nframework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,\nFe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting\nsynthesizable structures. Notably, 92,310 structures are filtered from the\n554,054 candidates predicted by GNoME, exhibiting great potential for promising\nsynthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =\nTi, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$\ncandidates exhibit high synthesizability, presenting viable candidates for\nexperimental realization and potentially associated with experimentally\nobserved temperature-induced phase transitions. This work establishes a\ndata-driven paradigm for machine-learning-assisted inorganic materials\nsynthesis, highlighting its potential to bridge the gap between computational\npredictions and experimental realization while unlocking new opportunities for\nthe targeted discovery of novel functional materials.", "AI": {"tldr": "The paper proposes a synthesizability-driven CSP framework integrating symmetry-guided structure derivation and machine learning, successfully predicting synthesizable structures and demonstrating potential for experimental realization.", "motivation": "To address the gap between theoretical predictions and experimental synthesis in thermodynamic energy-based crystal structure prediction (CSP), especially for metastable materials synthesized through kinetically controlled pathways.", "method": "A synthesizability-driven CSP framework that combines symmetry-guided structure derivation with a Wyckoff encode-based machine-learning model to identify promising subspaces. A structure-based synthesizability evaluation model fine-tuned with recently synthesized structures is then used alongside ab initio calculations to predict synthesizable candidates.", "result": "Reproduced 13 experimentally known XSe structures, filtered 92,310 structures from 554,054 candidates predicted by GNoME, and identified eight thermodynamically favorable Hf-X-O structures, including three highly synthesizable HfV$_2$O$_7$ candidates.", "conclusion": "Establishes a data-driven paradigm for machine-learning-assisted inorganic materials synthesis, bridging the gap between computational predictions and experimental realization, and unlocking opportunities for discovering novel functional materials."}}
{"id": "2505.09167", "pdf": "https://arxiv.org/pdf/2505.09167", "abs": "https://arxiv.org/abs/2505.09167", "authors": ["Amit Daniely", "Idan Mehalel", "Elchanan Mossel"], "title": "Online Learning of Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study online learning of feedforward neural networks with the sign\nactivation function that implement functions from the unit ball in\n$\\mathbb{R}^d$ to a finite label set $\\{1, \\ldots, Y\\}$.\n  First, we characterize a margin condition that is sufficient and in some\ncases necessary for online learnability of a neural network: Every neuron in\nthe first hidden layer classifies all instances with some margin $\\gamma$\nbounded away from zero. Quantitatively, we prove that for any net, the optimal\nmistake bound is at most approximately $\\mathtt{TS}(d,\\gamma)$, which is the\n$(d,\\gamma)$-totally-separable-packing number, a more restricted variation of\nthe standard $(d,\\gamma)$-packing number. We complement this result by\nconstructing a net on which any learner makes $\\mathtt{TS}(d,\\gamma)$ many\nmistakes. We also give a quantitative lower bound of approximately\n$\\mathtt{TS}(d,\\gamma) \\geq \\max\\{1/(\\gamma \\sqrt{d})^d, d\\}$ when $\\gamma \\geq\n1/2$, implying that for some nets and input sequences every learner will err\nfor $\\exp(d)$ many times, and that a dimension-free mistake bound is almost\nalways impossible.\n  To remedy this inevitable dependence on $d$, it is natural to seek additional\nnatural restrictions to be placed on the network, so that the dependence on $d$\nis removed. We study two such restrictions. The first is the multi-index model,\nin which the function computed by the net depends only on $k \\ll d$ orthonormal\ndirections. We prove a mistake bound of approximately $(1.5/\\gamma)^{k + 2}$ in\nthis model. The second is the extended margin assumption. In this setting, we\nassume that all neurons (in all layers) in the network classify every ingoing\ninput from previous layer with margin $\\gamma$ bounded away from zero. In this\nmodel, we prove a mistake bound of approximately $(\\log Y)/ \\gamma^{O(L)}$,\nwhere L is the depth of the network.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5177\u6709\u7b26\u53f7\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u9519\u8bef\u754c\u9650\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u589e\u52a0\u7f51\u7edc\u9650\u5236\u6765\u51cf\u5c11\u5bf9\u7ef4\u5ea6d\u7684\u4f9d\u8d56\u3002", "motivation": "\u7814\u7a76\u5177\u6709\u7b26\u53f7\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\uff0c\u7279\u522b\u662f\u5206\u6790\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u754c\u9650\u53ca\u5f71\u54cd\u56e0\u7d20\u3002", "method": "1. \u5b9a\u4e49\u5e76\u5206\u6790\u4e86\u4e00\u4e2a\u8fb9\u8ddd\u6761\u4ef6\uff0c\u8be5\u6761\u4ef6\u5bf9\u4e8e\u5728\u7ebf\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u662f\u5145\u5206\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u662f\u5fc5\u8981\u7684\u3002\n2. \u8bc1\u660e\u4e86\u4efb\u4f55\u7f51\u7edc\u7684\u6700\u4f18\u9519\u8bef\u754c\u9650\u5927\u7ea6\u4e3a$(d,\\gamma)$-\u5b8c\u5168\u53ef\u5206\u79bb\u5305\u88c5\u6570$\\mathtt{TS}(d,\\gamma)$\u3002\n3. \u6784\u9020\u4e86\u4e00\u4e2a\u7f51\u7edc\u5b9e\u4f8b\uff0c\u5c55\u793a\u4efb\u4f55\u5b66\u4e60\u7b97\u6cd5\u90fd\u4f1a\u72af$\\mathtt{TS}(d,\\gamma)$\u6b21\u9519\u8bef\u3002\n4. \u63d0\u51fa\u4e86\u4e24\u4e2a\u989d\u5916\u7684\u7f51\u7edc\u9650\u5236\uff1a\u591a\u6307\u6807\u6a21\u578b\u548c\u6269\u5c55\u8fb9\u8ddd\u5047\u8bbe\uff0c\u4ee5\u51cf\u5c11\u5bf9\u7ef4\u5ea6d\u7684\u4f9d\u8d56\uff0c\u5e76\u5206\u522b\u7ed9\u51fa\u4e86\u76f8\u5e94\u7684\u9519\u8bef\u754c\u9650\u3002", "result": "1. \u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u9519\u8bef\u754c\u9650\u4e0e\u7ef4\u5ea6d\u5448\u6307\u6570\u5173\u7cfb\u3002\n2. \u5728\u591a\u6307\u6807\u6a21\u578b\u4e2d\uff0c\u9519\u8bef\u754c\u9650\u4e0e\u8f93\u5165\u65b9\u5411\u6570\u91cfk\u6709\u5173\uff0c\u800c\u4e0e\u7ef4\u5ea6d\u65e0\u5173\u3002\n3. \u5728\u6269\u5c55\u8fb9\u8ddd\u5047\u8bbe\u4e0b\uff0c\u9519\u8bef\u754c\u9650\u4e0e\u7f51\u7edc\u6df1\u5ea6L\u548c\u6807\u7b7e\u6570\u91cfY\u6709\u5173\uff0c\u51cf\u5c11\u4e86\u5bf9\u7ef4\u5ea6d\u7684\u4f9d\u8d56\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u81ea\u7136\u9650\u5236\uff08\u5982\u591a\u6307\u6807\u6a21\u578b\u548c\u6269\u5c55\u8fb9\u8ddd\u5047\u8bbe\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u9519\u8bef\u754c\u9650\u5bf9\u8f93\u5165\u7ef4\u5ea6d\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u6539\u5584\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u5728\u7ebf\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2505.09356", "pdf": "https://arxiv.org/pdf/2505.09356", "abs": "https://arxiv.org/abs/2505.09356", "authors": ["Srinivas Ravuri", "Yuan Xu", "Martin Ludwig Zehetner", "Ketan Motlag", "Sahin Albayrak"], "title": "APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages with 6 figures", "summary": "Precise initialization plays a critical role in the performance of\nlocalization algorithms, especially in the context of robotics, autonomous\ndriving, and computer vision. Poor localization accuracy is often a consequence\nof inaccurate initial poses, particularly noticeable in GNSS-denied\nenvironments where GPS signals are primarily relied upon for initialization.\nRecent advances in leveraging deep neural networks for pose regression have led\nto significant improvements in both accuracy and robustness, especially in\nestimating complex spatial relationships and orientations. In this paper, we\nintroduce APR-Transformer, a model architecture inspired by state-of-the-art\nmethods, which predicts absolute pose (3D position and 3D orientation) using\neither image or LiDAR data. We demonstrate that our proposed method achieves\nstate-of-the-art performance on established benchmark datasets such as the\nRadar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our\nexperiments to include our custom complex APR-BeIntelli dataset. Additionally,\nwe validate the reliability of our approach in GNSS-denied environments by\ndeploying the model in real-time on an autonomous test vehicle. This showcases\nthe practical feasibility and effectiveness of our approach. The source code is\navailable at:https://github.com/GT-ARC/APR-Transformer.", "AI": {"tldr": "APR-Transformer\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u67b6\u6784\uff0c\u4f7f\u7528\u56fe\u50cf\u6216LiDAR\u6570\u636e\u9884\u6d4b\u7edd\u5bf9\u59ff\u6001\uff083D\u4f4d\u7f6e\u548c3D\u65b9\u5411\uff09\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728GNSS\u62d2\u7edd\u73af\u5883\u4e0b\u901a\u8fc7\u5b9e\u8f66\u9a8c\u8bc1\u3002", "motivation": "\u7cbe\u786e\u521d\u59cb\u5316\u5bf9\u4e8e\u5b9a\u4f4d\u7b97\u6cd5\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u3002\u4e0d\u51c6\u786e\u7684\u521d\u59cb\u59ff\u6001\u4f1a\u5bfc\u81f4\u5b9a\u4f4d\u7cbe\u5ea6\u5dee\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u4f9d\u8d56GPS\u4fe1\u53f7\u7684\u73af\u5883\u4e2d\u3002\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u59ff\u6001\u56de\u5f52\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u4e86APR-Transformer\u6a21\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u53ef\u4ee5\u4f7f\u7528\u56fe\u50cf\u6216LiDAR\u6570\u636e\u6765\u9884\u6d4b\u7edd\u5bf9\u59ff\u6001\uff083D\u4f4d\u7f6e\u548c3D\u65b9\u5411\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u51e0\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u6269\u5c55\u5230\u81ea\u5b9a\u4e49\u7684\u590d\u6742\u6570\u636e\u96c6APR-BeIntelli\u3002\u6b64\u5916\uff0c\u5728GNSS\u62d2\u7edd\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u5728\u81ea\u4e3b\u6d4b\u8bd5\u8f66\u8f86\u4e0a\u5b9e\u65f6\u90e8\u7f72\u6a21\u578b\u6765\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002", "result": "\u5728Radar Oxford Robot-Car\u3001DeepLoc\u548cAPR-BeIntelli\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728GNSS\u62d2\u7edd\u73af\u5883\u4e0b\u7684\u5b9e\u8f66\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "APR-Transformer\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728GNSS\u62d2\u7edd\u73af\u5883\u4e0b\u901a\u8fc7\u5b9e\u8f66\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.09229", "pdf": "https://arxiv.org/pdf/2505.09229", "abs": "https://arxiv.org/abs/2505.09229", "authors": ["Brian Britos", "Mathias Bourel"], "title": "Optimal Transport-Based Domain Adaptation for Rotated Linear Regression", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "Optimal Transport (OT) has proven effective for domain adaptation (DA) by\naligning distributions across domains with differing statistical properties.\nBuilding on the approach of Courty et al. (2016), who mapped source data to the\ntarget domain for improved model transfer, we focus on a supervised DA problem\ninvolving linear regression models under rotational shifts. This ongoing work\nconsiders cases where source and target domains are related by a\nrotation-common in applications like sensor calibration or image orientation.\nWe show that in $\\mathbb{R}^2$ , when using a p-norm cost with $p $\\ge$ 2$, the\noptimal transport map recovers the underlying rotation. Based on this, we\npropose an algorithm that combines K-means clustering, OT, and singular value\ndecomposition (SVD) to estimate the rotation angle and adapt the regression\nmodel. This method is particularly effective when the target domain is sparsely\nsampled, leveraging abundant source data for improved generalization. Our\ncontributions offer both theoretical and practical insights into OT-based model\nadaptation under geometric transformations.", "AI": {"tldr": "Optimal Transport (OT) is used for domain adaptation in supervised settings with linear regression models, especially when domains differ by rotations. The paper proposes an algorithm combining K-means, OT, and SVD to estimate the rotation angle and adapt the model, demonstrating effectiveness in sparsely sampled target domains.", "motivation": "Domain adaptation is crucial when source and target domains have differing statistical properties, such as rotations. Current methods lack a specific approach for adapting linear regression models under rotational shifts.", "method": "The method involves using Optimal Transport theory to recover the underlying rotation between domains in $\\mathbb{R}^2$ with p-norm cost ($p \\ge 2$). An algorithm is proposed that combines K-means clustering, OT, and SVD to estimate the rotation angle and adapt the regression model.", "result": "The proposed algorithm effectively estimates the rotation angle and adapts the regression model, particularly excelling in scenarios where the target domain is sparsely sampled. This leverages the abundant source data to improve generalization.", "conclusion": "This work provides both theoretical understanding of OT-based model adaptation under geometric transformations and practical solutions for domain adaptation in supervised settings involving rotations."}}
{"id": "2505.09521", "pdf": "https://arxiv.org/pdf/2505.09521", "abs": "https://arxiv.org/abs/2505.09521", "authors": ["Dongyi He", "Shiyang Li", "Bin Jiang", "He Yan"], "title": "Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-resolution functional magnetic resonance imaging (fMRI) is essential for\nmapping human brain activity; however, it remains costly and logistically\nchallenging. If comparable volumes could be generated directly from widely\navailable scalp electroencephalography (EEG), advanced neuroimaging would\nbecome significantly more accessible. Existing EEG-to-fMRI generators rely on\nplain CNNs that fail to capture cross-channel time-frequency cues or on heavy\ntransformer/GAN decoders that strain memory and stability. We propose\nSpec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts\nthese issues via a Multi-directional Time-Frequency Convolutional Attention\nEncoder, stacking temporal, spectral and joint convolutions with\nself-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space\nblocks enable efficient long-range spatial modelling. Trained end-to-end with a\nhybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on\nthree public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball\nand 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%\nrespectively over previous best SSIM scores. Furthermore, it achieves\ncompetitive PSNR scores, particularly excelling on the CN-EPFL dataset with a\n4.6% improvement over the previous best PSNR, thus striking a better balance in\nreconstruction quality. The proposed model is lightweight and efficient, making\nit suitable for real-time applications in clinical and research settings. The\ncode is available at https://github.com/hdy6438/Spec2VolCAMU-Net.", "AI": {"tldr": "Spec2VolCAMU-Net\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9891\u8c31\u56fe\u5230\u4f53\u79ef\u751f\u6210\u5668\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u65f6\u9891\u5377\u79ef\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548cVision-Mamba U-Net\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u4eceEEG\u751f\u6210\u7c7b\u4f3cfMRI\u7684\u8111\u90e8\u6d3b\u52a8\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u529f\u80fd\u6027\u78c1\u5171\u632f\u6210\u50cf\uff08fMRI\uff09\u5bf9\u4e8e\u7ed8\u5236\u4eba\u7c7b\u5927\u8111\u6d3b\u52a8\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u4e14\u64cd\u4f5c\u590d\u6742\u3002\u5982\u679c\u80fd\u76f4\u63a5\u4ece\u5e7f\u6cdb\u4f7f\u7528\u7684\u5934\u76ae\u8111\u7535\u56fe\uff08EEG\uff09\u751f\u6210\u53ef\u6bd4\u7684\u4f53\u79ef\u6570\u636e\uff0c\u5148\u8fdb\u7684\u795e\u7ecf\u6210\u50cf\u5c06\u53d8\u5f97\u66f4\u52a0\u666e\u53ca\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u8de8\u901a\u9053\u7684\u65f6\u95f4\u9891\u7387\u7ebf\u7d22\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u5185\u5b58\u5bc6\u96c6\u548c\u4e0d\u7a33\u5b9a\u7684\u53d8\u538b\u5668/ GAN\u89e3\u7801\u5668\u3002", "method": "\u63d0\u51fa\u4e86Spec2VolCAMU-Net\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u9891\u8c31\u56fe\u5230\u4f53\u79ef\u751f\u6210\u5668\u3002\u5b83\u5305\u62ec\u4e00\u4e2a\u591a\u65b9\u5411\u65f6\u9891\u5377\u79ef\u6ce8\u610f\u529b\u7f16\u7801\u5668\uff0c\u8be5\u7f16\u7801\u5668\u5806\u53e0\u4e86\u65f6\u95f4\u3001\u5149\u8c31\u548c\u8054\u5408\u5377\u79ef\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4f7f\u7528Vision-Mamba U-Net\u89e3\u7801\u5668\uff0c\u5176\u7ebf\u6027\u65f6\u95f4\u72b6\u6001\u7a7a\u95f4\u5757\u80fd\u591f\u6709\u6548\u8fdb\u884c\u957f\u8ddd\u79bb\u7a7a\u95f4\u5efa\u6a21\u3002\u6574\u4e2a\u6a21\u578b\u901a\u8fc7\u6df7\u5408SSI-MSE\u635f\u5931\u51fd\u6570\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpec2VolCAMU-Net\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u4fdd\u771f\u5ea6\uff1aNODDI\u4e0a\u7684SSIM\u4e3a0.693\uff0cOddball\u4e0a\u4e3a0.725\uff0cCN-EPFL\u4e0a\u4e3a0.788\uff0c\u5206\u522b\u6bd4\u4e4b\u524d\u7684\u6700\u4f73SSIM\u5206\u6570\u63d0\u9ad8\u4e8614.5%\uff0c14.9%\u548c16.9%\u3002\u6b64\u5916\uff0c\u5728PSNR\u8bc4\u5206\u65b9\u9762\u8868\u73b0\u4e5f\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5c24\u5176\u662f\u5728CN-EPFL\u6570\u636e\u96c6\u4e0a\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u4f73PSNR\u63d0\u9ad8\u4e864.6%\u3002", "conclusion": "Spec2VolCAMU-Net\u4e0d\u4ec5\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u800c\u4e14\u7531\u4e8e\u5176\u8f7b\u91cf\u7ea7\u548c\u9ad8\u6548\u7684\u7279\u70b9\uff0c\u975e\u5e38\u9002\u5408\u7528\u4e8e\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2505.09438", "pdf": "https://arxiv.org/pdf/2505.09438", "abs": "https://arxiv.org/abs/2505.09438", "authors": ["Paul Tschisgale", "Holger Maus", "Fabian Kieser", "Ben Kroehs", "Stefan Petersen", "Peter Wulff"], "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment", "categories": ["physics.ed-ph", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\uff09\u548c\u63a8\u7406\u4f18\u5316\u6a21\u578b\uff08o1-preview\uff09\u5728\u89e3\u51b3\u5fb7\u56fd\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u53c2\u8d5b\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u79cdLLM\u5728\u89e3\u9898\u80fd\u529b\u4e0a\u4f18\u4e8e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\uff0c\u5176\u4e2do1-preview\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u7269\u7406\u6559\u80b2\u4e2d\u8bbe\u8ba1\u8bc4\u4f30\u65b9\u5f0f\u4ee5\u7ef4\u6301\u5176\u5b8c\u6574\u6027\u548c\u4fc3\u8fdb\u5b66\u751f\u5bf9LLM\u7684\u6279\u5224\u6027\u4f7f\u7528\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u666e\u53ca\uff0c\u5b83\u4eec\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u5b66\u751f\u5b66\u4e60\u8fc7\u7a0b\u548c\u8bc4\u4f30\u5f62\u5f0f\u5b8c\u6574\u6027\u7684\u62c5\u5fe7\u3002\u7279\u522b\u662f\u5728\u7269\u7406\u6559\u80b2\u4e2d\uff0c\u7406\u89e3LLMs\u5728\u7279\u5b9a\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u5c06\u6709\u52a9\u4e8e\u8d1f\u8d23\u4efb\u5730\u5c06LLMs\u6574\u5408\u5230\u6559\u5b66\u548c\u8bc4\u4f30\u4e2d\u3002", "method": "\u7814\u7a76\u9009\u53d6\u4e86\u5fb7\u56fd\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u7684\u4e00\u7cfb\u5217\u660e\u786e\u5b9a\u4e49\u7684\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86GPT-4o\uff08\u4f7f\u7528\u4e0d\u540c\u7684\u63d0\u793a\u6280\u672f\uff09\u548co1-preview\u6a21\u578b\u4e0e\u4eba\u7c7b\u53c2\u8d5b\u8005\u7684\u89e3\u9898\u8868\u73b0\u3002\u9664\u4e86\u8bc4\u4f30\u89e3\u9898\u6b63\u786e\u6027\u5916\uff0c\u8fd8\u5206\u6790\u4e86LLM\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u7279\u5f81\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cGPT-4o\u548co1-preview\u5728\u89e3\u51b3\u5965\u6797\u5339\u514b\u7c7b\u578b\u7269\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u9ad8\u7ea7\u522b\u7684\u89e3\u9898\u80fd\u529b\uff0c\u5e73\u5747\u800c\u8a00\u8d85\u8d8a\u4e86\u4eba\u7c7b\u53c2\u8d5b\u8005\u3002\u4e0d\u540c\u63d0\u793a\u6280\u672f\u5bf9GPT-4o\u7684\u5f71\u54cd\u8f83\u5c0f\uff0c\u800co1-preview\u51e0\u4e4e\u59cb\u7ec8\u4f18\u4e8eGPT-4o\u548c\u4eba\u7c7b\u57fa\u51c6\u3002", "conclusion": "\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u7814\u7a76\u8ba8\u8bba\u4e86\u7269\u7406\u6559\u80b2\u4e2d\u603b\u7ed3\u6027\u548c\u5f62\u6210\u6027\u8bc4\u4f30\u7684\u8bbe\u8ba1\u5f71\u54cd\uff0c\u5305\u62ec\u5982\u4f55\u4fdd\u6301\u8bc4\u4f30\u7684\u5b8c\u6574\u6027\u4ee5\u53ca\u652f\u6301\u5b66\u751f\u6279\u5224\u6027\u5730\u4e0eLLMs\u4e92\u52a8\u3002"}}
{"id": "2505.09565", "pdf": "https://arxiv.org/pdf/2505.09565", "abs": "https://arxiv.org/abs/2505.09565", "authors": ["Maik Dannecker", "Thomas Sanchez", "Meritxell Bach Cuadra", "\u00d6zg\u00fcn Turgut", "Anthony N. Price", "Lucilio Cordero-Grande", "Vanessa Kyriakopoulou", "Joseph V. Hajnal", "Daniel Rueckert"], "title": "Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 6 figures", "summary": "High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.", "AI": {"tldr": "The paper presents a novel SVR method using implicit neural representations for fast and accurate MRI reconstruction, especially in cases of severe motion and image corruption. It shows improvements in quality and up to 50% reduction in reconstruction time compared to state-of-the-art methods.", "motivation": "Existing SVR methods struggle with image artifacts and severe subject motion or require slice pre-alignment for good performance.", "method": "The proposed method performs motion correction, outlier handling, and super-resolution reconstruction based on implicit neural representations. The model can be initialized with task-specific priors through self-supervised meta-learning.", "result": "The method showed improvements in reconstruction quality, particularly in the presence of severe motion, and up to 50% reduction in reconstruction time in experiments with over 480 reconstructions of simulated and clinical MRI brain data.", "conclusion": "The novel SVR method using implicit neural representations is effective for fast and accurate MRI reconstruction in cases of severe motion and image corruption."}}
{"id": "2505.09456", "pdf": "https://arxiv.org/pdf/2505.09456", "abs": "https://arxiv.org/abs/2505.09456", "authors": ["Josep Lumbreras", "Ruo Cheng Huang", "Yanglin Hu", "Mile Gu", "Marco Tomamichel"], "title": "Quantum state-agnostic work extraction (almost) without dissipation", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "5 pages+14 pages, 2 figures", "summary": "We investigate work extraction protocols designed to transfer the maximum\npossible energy to a battery using sequential access to $N$ copies of an\nunknown pure qubit state. The core challenge is designing interactions to\noptimally balance two competing goals: charging of the battery optimally using\nthe qubit in hand, and acquiring more information by qubit to improve energy\nharvesting in subsequent rounds. Here, we leverage exploration-exploitation\ntrade-off in reinforcement learning to develop adaptive strategies achieving\nenergy dissipation that scales only poly-logarithmically in $N$. This\nrepresents an exponential improvement over current protocols based on full\nstate tomography.", "AI": {"tldr": "This paper explores work extraction protocols for transferring maximum energy to a battery from unknown pure qubit states, using sequential access and adaptive strategies inspired by reinforcement learning.", "motivation": "The motivation is to improve energy harvesting from qubits by balancing the charging of the battery optimally with acquiring more information about the qubit state to enhance future rounds of energy extraction.", "method": "The method involves leveraging exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies for optimal energy extraction from qubits.", "result": "The result is an exponential improvement over current protocols based on full state tomography, with energy dissipation scaling only poly-logarithmically in $N$ copies of the qubit state.", "conclusion": "Adaptive strategies developed using reinforcement learning principles provide a significant enhancement in energy extraction efficiency compared to traditional methods."}}
{"id": "2505.09266", "pdf": "https://arxiv.org/pdf/2505.09266", "abs": "https://arxiv.org/abs/2505.09266", "authors": ["Lirand\u00eb Pira", "Airin Antony", "Nayanthara Prathap", "Daniel Peace", "Jacquiline Romero"], "title": "Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques", "categories": ["physics.optics", "cs.LG", "quant-ph"], "comment": null, "summary": "Photonic chip design has seen significant advancements with the adoption of\ninverse design methodologies, offering flexibility and efficiency in optimizing\ndevice performance. However, the black-box nature of the optimization\napproaches, such as those used in inverse design in order to minimize a loss\nfunction or maximize coupling efficiency, poses challenges in understanding the\noutputs. This challenge is prevalent in machine learning-based optimization\nmethods, which can suffer from the same lack of transparency. To this end,\ninterpretability techniques address the opacity of optimization models. In this\nwork, we apply interpretability techniques from machine learning, with the aim\nof gaining understanding of inverse design optimization used in designing\nphotonic components, specifically two-mode multiplexers. We base our\nmethodology on the widespread interpretability technique known as local\ninterpretable model-agnostic explanations, or LIME. As a result, LIME-informed\ninsights point us to more effective initial conditions, directly improving\ndevice performance. This demonstrates that interpretability methods can do more\nthan explain models -- they can actively guide and enhance the inverse-designed\nphotonic components. Our results demonstrate the ability of interpretable\ntechniques to reveal underlying patterns in the inverse design process, leading\nto the development of better-performing components.", "AI": {"tldr": "The paper explores the application of interpretability techniques, specifically LIME, to understand and enhance inverse design optimization in photonic chip design, leading to better-performing components.", "motivation": "Inverse design methodologies have significantly advanced photonic chip design but suffer from a lack of transparency in their optimization processes, similar to machine learning-based methods.", "method": "The authors apply the LIME interpretability technique from machine learning to analyze and guide the inverse design optimization process for photonic components, focusing on two-mode multiplexers.", "result": "LIME-informed insights led to more effective initial conditions, which directly improved device performance and revealed underlying patterns in the inverse design process.", "conclusion": "Interpretability methods can actively guide and enhance the design of photonic components beyond just explaining models."}}
{"id": "2505.09477", "pdf": "https://arxiv.org/pdf/2505.09477", "abs": "https://arxiv.org/abs/2505.09477", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Jason Hughes", "Varun Murali", "M. Ani Hsieh", "George J. Pappas", "Camillo J. Taylor", "Vijay Kumar"], "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2025", "summary": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.", "AI": {"tldr": "The paper discusses deploying foundation model (FM)-enabled robots in large-scale and unstructured environments, presenting SPINE - an LLM-enabled autonomy framework. It showcases the first large-scale LLM-based robot planning in such settings and introduces a language-driven UAV planner using onboard language models.", "motivation": "Foundation model (FM)-enabled robots have been primarily used in closed-world settings with full prior maps or complete workspace views. The motivation is to address the challenge of deploying these robots in real-world missions that require operation in large-scale and unstructured environments.", "method": "The method involves using SPINE, an LLM-enabled autonomy framework, which is adaptable to different LLMs and can be distilled into smaller models for SWaP-limited platforms. This approach enables effective exploration, navigation through obstacles, handling unexpected sensor inputs, and operation within compute constraints.", "result": "The result includes successful deployments of SPINE in field robotic settings, demonstrating the first large-scale LLM-enabled robot planning over several kilometers in unstructured environments. Additionally, a language-driven UAV planner using on-device language models was presented.", "conclusion": "The conclusion proposes several promising directions for future research, indicating the potential advancements in FM-enabled robotics for complex, real-world applications."}}
{"id": "2505.09304", "pdf": "https://arxiv.org/pdf/2505.09304", "abs": "https://arxiv.org/abs/2505.09304", "authors": ["Luciano Sebastian Martinez-Rau", "Quynh Nguyen Phuong Vu", "Yuxuan Zhang", "Bengt Oelmann", "Sebastian Bader"], "title": "Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Preprint submitted to the IEEE 11th World Forum on Internet of Things", "summary": "Keyword spotting (KWS) is a key component of smart devices, enabling\nefficient and intuitive audio interaction. However, standard KWS systems\ndeployed on embedded devices often suffer performance degradation under\nreal-world operating conditions. Resilient KWS systems address this issue by\nenabling dynamic adaptation, with applications such as adding or replacing\nkeywords, adjusting to specific users, and improving noise robustness. However,\ndeploying resilient, standalone KWS systems with low latency on\nresource-constrained devices remains challenging due to limited memory and\ncomputational resources. This study proposes a low computational approach for\ncontinuous noise adaptation of pretrained neural networks used for KWS\nclassification, requiring only 1-shot learning and one epoch. The proposed\nmethod was assessed using two pretrained models and three real-world noise\nsources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted\nmodels consistently outperformed the pretrained models across all scenarios,\nespecially at SNR $\\leq$ 18 dB, achieving accuracy improvements of 4.9% to\n46.0%. These results highlight the efficacy of the proposed methodology while\nbeing lightweight enough for deployment on resource-constrained devices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.09313", "pdf": "https://arxiv.org/pdf/2505.09313", "abs": "https://arxiv.org/abs/2505.09313", "authors": ["Qiangqiang Liu", "Qian Huang", "Frank Fan", "Haishan Wu", "Xueyan Tang"], "title": "Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach", "categories": ["cs.CR", "cs.LG"], "comment": "IEEE International Conference on Blockchain and Cryptocurrency(Proc.\n  IEEE ICBC 2025)", "summary": "Sybil attacks pose a significant security threat to blockchain ecosystems,\nparticularly in token airdrop events. This paper proposes a novel sybil address\nidentification method based on subgraph feature extraction lightGBM. The method\nfirst constructs a two-layer deep transaction subgraph for each address, then\nextracts key event operation features according to the lifecycle of sybil\naddresses, including the time of first transaction, first gas acquisition,\nparticipation in airdrop activities, and last transaction. These temporal\nfeatures effectively capture the consistency of sybil address behavior\noperations. Additionally, the method extracts amount and network structure\nfeatures, comprehensively describing address behavior patterns and network\ntopology through feature propagation and fusion. Experiments conducted on a\ndataset containing 193,701 addresses (including 23,240 sybil addresses) show\nthat this method outperforms existing approaches in terms of precision, recall,\nF1 score, and AUC, with all metrics exceeding 0.9. The methods and results of\nthis study can be further applied to broader blockchain security areas such as\ntransaction manipulation identification and token liquidity risk assessment,\ncontributing to the construction of a more secure and fair blockchain\necosystem.", "AI": {"tldr": "This paper proposes a new method for identifying sybil addresses in blockchain ecosystems using subgraph feature extraction and lightGBM, which performs better than existing methods.", "motivation": "Sybil attacks are a significant security threat to blockchain ecosystems, especially in token airdrop events.", "method": "The method constructs a two-layer deep transaction subgraph for each address, extracts key event operation features according to the lifecycle of sybil addresses, and also extracts amount and network structure features.", "result": "Experiments on a dataset with 193,701 addresses show that this method outperforms existing approaches in precision, recall, F1 score, and AUC, with all metrics exceeding 0.9.", "conclusion": "This study's methods and results can be applied to broader blockchain security areas like transaction manipulation identification and token liquidity risk assessment, promoting a more secure and fair blockchain ecosystem."}}
{"id": "2505.09558", "pdf": "https://arxiv.org/pdf/2505.09558", "abs": "https://arxiv.org/abs/2505.09558", "authors": ["Shengpeng Ji", "Tianle Liang", "Yangzhuo Li", "Jialong Zuo", "Minghui Fang", "Jinzheng He", "Yifu Chen", "Zhengqing Liu", "Ziyue Jiang", "Xize Cheng", "Siqi Zheng", "Jin Xu", "Junyang Lin", "Zhou Zhao"], "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered\nsignificant attention in the speech domain. However, the evaluation of spoken\ndialogue models' conversational performance has largely been overlooked. This\nis primarily due to the intelligent chatbots convey a wealth of non-textual\ninformation which cannot be easily measured using text-based language models\nlike ChatGPT. To address this gap, we propose WavReward, a reward feedback\nmodel based on audio language models that can evaluate both the IQ and EQ of\nspoken dialogue systems with speech input. Specifically, 1) based on audio\nlanguage models, WavReward incorporates the deep reasoning process and the\nnonlinear reward mechanism for post-training. By utilizing multi-sample\nfeedback via the reinforcement learning algorithm, we construct a specialized\nevaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a\npreference dataset used to train WavReward. ChatReward-30K includes both\ncomprehension and generation aspects of spoken dialogue models. These scenarios\nspan various tasks, such as text-based chats, nine acoustic attributes of\ninstruction chats, and implicit chats. WavReward outperforms previous\nstate-of-the-art evaluation models across multiple spoken dialogue scenarios,\nachieving a substantial improvement about Qwen2.5-Omni in objective accuracy\nfrom 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a\nmargin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each\ncomponent of WavReward. All data and code will be publicly at\nhttps://github.com/jishengpeng/WavReward after the paper is accepted.", "AI": {"tldr": "WavReward is a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. It outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios.", "motivation": "The motivation of this paper is to address the gap in evaluating spoken dialogue models' conversational performance, which cannot be easily measured using text-based language models like ChatGPT.", "method": "1) Based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, a specialized evaluator tailored to spoken dialogue models is constructed. 2) ChatReward-30K, a preference dataset used to train WavReward, is introduced. This dataset includes comprehension and generation aspects of spoken dialogue models covering various tasks.", "result": "WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by a margin of 83%.", "conclusion": "Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly available after the paper is accepted."}}
{"id": "2505.09561", "pdf": "https://arxiv.org/pdf/2505.09561", "abs": "https://arxiv.org/abs/2505.09561", "authors": ["Marcel Torne", "Andy Tang", "Yuejiang Liu", "Chelsea Finn"], "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Videos are available at https://long-context-dp.github.io", "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPast-Token Prediction (PTP) \u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8fc7\u53bb\u52a8\u4f5c\u6807\u8bb0\u6765\u589e\u5f3a\u7b56\u7565\u5bf9\u672a\u6765\u548c\u8fc7\u53bb\u52a8\u4f5c\u4e4b\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u6355\u6349\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u4fe1\u606f\u4fdd\u7559\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u957f\u4e0a\u4e0b\u6587\u6269\u6563\u7b56\u7565\u6027\u80fd\u63d0\u53473\u500d\uff0c\u5e76\u52a0\u901f\u8bad\u7ec310\u500d\u4ee5\u4e0a\u3002", "motivation": "\u5f53\u524d\u5b66\u4e60\u957f\u4e0a\u4e0b\u6587\u7b56\u7565\u7684\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u622a\u65ad\u4e0a\u4e0b\u6587\u957f\u5ea6\u6765\u89c4\u907f\u6311\u6218\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u4e22\u5f03\u5bf9\u540e\u7eed\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u7684\u5386\u53f2\u4fe1\u606f\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u663e\u5f0f\u5730\u89c4\u8303\u8fc7\u53bb\u4fe1\u606f\u7684\u4fdd\u7559\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86Past-Token Prediction (PTP) \u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u8ba9\u7b56\u7565\u540c\u65f6\u5b66\u4e60\u9884\u6d4b\u8fc7\u53bb\u548c\u672a\u6765\u52a8\u4f5c\u6807\u8bb0\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u5148\u7528\u77ed\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u518d\u7528\u7f13\u5b58\u7684\u957f\u4e0a\u4e0b\u6587\u5d4c\u5165\u5fae\u8c03\u7b56\u7565\u5934\u3002\u6700\u540e\uff0c\u6269\u5c55PTP\u4e3a\u81ea\u9a8c\u8bc1\u673a\u5236\u4ee5\u5728\u63a8\u7406\u65f6\u9009\u62e9\u4e0e\u8fc7\u53bb\u52a8\u4f5c\u4e00\u81f4\u7684\u5019\u9009\u65b9\u6848\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u516d\u4e2a\u6a21\u62df\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f7f\u957f\u4e0a\u4e0b\u6587\u6269\u6563\u7b56\u7565\u7684\u6027\u80fd\u63d0\u9ad8\u4e863\u500d\uff0c\u5e76\u4e14\u5c06\u7b56\u7565\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u4e8610\u500d\u4ee5\u4e0a\u3002", "conclusion": "Past-Token Prediction (PTP) \u8f85\u52a9\u4efb\u52a1\u663e\u8457\u6539\u5584\u4e86\u7b56\u7565\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5927\u5e45\u964d\u4f4e\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u81ea\u9a8c\u8bc1\u673a\u5236\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u63a8\u7406\u65f6\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.09326", "pdf": "https://arxiv.org/pdf/2505.09326", "abs": "https://arxiv.org/abs/2505.09326", "authors": ["Vincent Abbott", "Kotaro Kamiya", "Gerard Glowacki", "Yu Atsumi", "Gioele Zardini", "Yoshihiro Maruyama"], "title": "Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks", "categories": ["math.CT", "cs.LG", "q-bio.MN"], "comment": null, "summary": "How do we enable artificial intelligence models to improve themselves? This\nis central to exponentially improving generalized artificial intelligence\nmodels, which can improve their own architecture to handle new problem domains\nin an efficient manner that leverages the latest hardware. However, current\nautomated compilation methods are poor, and efficient algorithms require years\nof human development. In this paper, we use neural circuit diagrams, based in\ncategory theory, to prove a general theorem related to deep learning\nalgorithms, guide the development of a novel attention algorithm catered to the\ndomain of gene regulatory networks, and produce a corresponding efficient\nkernel. The algorithm we propose, spherical attention, shows that neural\ncircuit diagrams enable a principled and systematic method for reasoning about\ndeep learning architectures and providing high-performance code. By replacing\nSoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special\nfunction unit bottleneck of standard attention while retaining the streaming\nproperty essential to high-performance. Our diagrammatically derived\n\\textit{FlashSign} kernel achieves comparable performance to the\nstate-of-the-art, fine-tuned FlashAttention algorithm on an A100, and\n$3.6\\times$ the performance of PyTorch. Overall, this investigation shows\nneural circuit diagrams' suitability as a high-level framework for the\nautomated development of efficient, novel artificial intelligence\narchitectures.", "AI": {"tldr": "The paper proposes spherical attention algorithm and FlashSign kernel using neural circuit diagrams to enable systematic reasoning about deep learning architectures. It overcomes bottlenecks in standard attention mechanisms and achieves high performance.", "motivation": "To enable artificial intelligence models to improve themselves by developing a principled and systematic method for reasoning about deep learning architectures.", "method": "Using neural circuit diagrams based in category theory, the authors prove a general theorem related to deep learning algorithms, develop a novel attention algorithm (spherical attention) for gene regulatory networks, and produce an efficient kernel (FlashSign).", "result": "The proposed spherical attention algorithm overcomes the special function unit bottleneck of standard attention while retaining essential streaming properties. The FlashSign kernel achieves 3.6x the performance of PyTorch and is comparable to state-of-the-art FlashAttention on an A100 GPU.", "conclusion": "Neural circuit diagrams are suitable as a high-level framework for the automated development of efficient, novel artificial intelligence architectures."}}
{"id": "2505.09576", "pdf": "https://arxiv.org/pdf/2505.09576", "abs": "https://arxiv.org/abs/2505.09576", "authors": ["Shannon Lodoen", "Alexi Orchard"], "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach", "categories": ["cs.CY", "cs.AI"], "comment": "10 pages, 1 figure, Accepted version", "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots.", "AI": {"tldr": "Since 2022, RLHF technique has been used to fine-tune LLMs, making their outputs more human-like. This convergence of human and machine-written text raises ethical, sociotechnical, and pedagogical concerns. This paper conducts a rhetorical analysis of the procedures reshaped by RLHF-enhanced chatbots, shifting focus from content persuasiveness to underlying persuasive mechanisms, and opens new directions for AI ethics inquiry.", "motivation": "The motivation of this paper is to highlight the severe ethical, sociotechnical, and pedagogical implications brought about by the increasing convergence of human and machine-written text due to the integration of RLHF in LLMs.", "method": "The method involves conducting a rhetorical analysis of the central procedures and processes reshaped by RLHF-enhanced generative AI chatbots using Ian Bogost's concept of procedural rhetoric, shifting the site of investigation from content analysis to the underlying mechanisms of persuasion.", "result": "The result is a theoretical investigation that uncovers how AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships.", "conclusion": "This paper concludes that there is a need for further inquiry in AI ethics considering the implications of procedures rerouted through AI-driven technologies, which will be of interest to educators, researchers, scholars, and users of generative AI chatbots."}}
{"id": "2505.09364", "pdf": "https://arxiv.org/pdf/2505.09364", "abs": "https://arxiv.org/abs/2505.09364", "authors": ["Michael Benigni", "Maurizio Ferrari Dacrema", "Dietmar Jannach"], "title": "Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch", "categories": ["cs.IR", "cs.LG", "cs.NE"], "comment": null, "summary": "Countless new machine learning models are published every year and are\nreported to significantly advance the state-of-the-art in \\emph{top-n}\nrecommendation. However, earlier reproducibility studies indicate that progress\nin this area may be quite limited. Specifically, various widespread\nmethodological issues, e.g., comparisons with untuned baseline models, have led\nto an \\emph{illusion of progress}. In this work, our goal is to examine whether\nthese problems persist in today's research. To this end, we aim to reproduce\nthe latest advancements reported from applying modern Denoising Diffusion\nProbabilistic Models to recommender systems, focusing on four models published\nat the top-ranked SIGIR conference in 2023 and 2024. Our findings are\nconcerning, revealing persistent methodological problems. Alarmingly, through\nexperiments, we find that the latest recommendation techniques based on\ndiffusion models, despite their computational complexity and substantial carbon\nfootprint, are consistently outperformed by simpler existing models.\nFurthermore, we identify key mismatches between the characteristics of\ndiffusion models and those of the traditional \\emph{top-n} recommendation task,\nraising doubts about their suitability for recommendation. We also note that,\nin the papers we analyze, the generative capabilities of these models are\nconstrained to a minimum. Overall, our results and continued methodological\nissues call for greater scientific rigor and a disruptive change in the\nresearch and publication culture in this area.", "AI": {"tldr": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u88ab\u5e7f\u6cdb\u62a5\u9053\u4e3a\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u78b3\u6392\u653e\u8f83\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u900a\u8272\u4e8e\u7b80\u5355\u7684\u73b0\u6709\u6a21\u578b\u3002\u8fd9\u63ed\u793a\u4e86\u65b9\u6cd5\u8bba\u95ee\u9898\u7684\u6301\u7eed\u5b58\u5728\u4ee5\u53ca\u5bf9\u79d1\u7814\u4e25\u8c28\u6027\u548c\u6587\u5316\u53d8\u9769\u7684\u9700\u6c42\u3002", "motivation": "\u8c03\u67e5\u73b0\u4ee3\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u68c0\u67e5\u65e9\u671f\u7814\u7a76\u4e2d\u53d1\u73b0\u7684\u65b9\u6cd5\u8bba\u95ee\u9898\u662f\u5426\u4ecd\u7136\u5b58\u5728\u4e8e\u5f53\u524d\u7684\u7814\u7a76\u4e2d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u91cd\u73b0SIGIR 2023\u548c2024\u5e74\u53d1\u5e03\u7684\u56db\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u5e76\u4e0e\u8c03\u4f18\u540e\u7684\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u6269\u6563\u6a21\u578b\u5728\u4f20\u7edftop-n\u63a8\u8350\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5176\u751f\u6210\u80fd\u529b\u53d7\u5230\u9650\u5236\uff0c\u540c\u65f6\u786e\u8ba4\u4e86\u65b9\u6cd5\u8bba\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002", "conclusion": "\u9700\u8981\u66f4\u9ad8\u7684\u79d1\u7814\u4e25\u8c28\u6027\u4ee5\u53ca\u7814\u7a76\u548c\u53d1\u8868\u6587\u5316\u7684\u53d8\u9769\u6765\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u3002"}}
{"id": "2505.09365", "pdf": "https://arxiv.org/pdf/2505.09365", "abs": "https://arxiv.org/abs/2505.09365", "authors": ["H. T. R\u00fcdisser", "G. Nguyen", "J. Le Lou\u00ebdec", "C. M\u00f6stl"], "title": "ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections", "categories": ["physics.space-ph", "astro-ph.IM", "astro-ph.SR", "cs.LG"], "comment": "25 pages, 9 figures, 1 table, submitted to AGU Space Weather on 14th\n  May 2025", "summary": "Interplanetary coronal mass ejections (ICMEs) are major drivers of space\nweather disturbances, posing risks to both technological infrastructure and\nhuman activities. Automatic detection of ICMEs in solar wind in situ data is\nessential for early warning systems. While several methods have been proposed\nto identify these structures in time series data, robust real-time detection\nremains a significant challenge. In this work, we present ARCANE - the first\nframework explicitly designed for early ICME detection in streaming solar wind\ndata under realistic operational constraints, enabling event identification\nwithout requiring observation of the full structure. Our approach evaluates the\nstrengths and limitations of detection models by comparing a machine\nlearning-based method to a threshold-based baseline. The ResUNet++ model,\npreviously validated on science data, significantly outperforms the baseline,\nparticularly in detecting high-impact events, while retaining solid performance\non lower-impact cases. Notably, we find that using real-time solar wind (RTSW)\ndata instead of high-resolution science data leads to only minimal performance\ndegradation. Despite the challenges of operational settings, our detection\npipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%\nof the event's duration while only seeing a minimal amount of data. As more\ndata becomes available, the performance increases significantly. These results\nmark a substantial step forward in automated space weather monitoring and lay\nthe groundwork for enhanced real-time forecasting capabilities.", "AI": {"tldr": "This paper introduces ARCANE, the first framework for early ICME detection in streaming solar wind data under realistic operational constraints. It compares a machine learning-based method (ResUNet++) with a threshold-based baseline and finds that ResUNet++ significantly outperforms the baseline, especially in detecting high-impact events. Using real-time solar wind data leads to only minimal performance degradation compared to high-resolution science data.", "motivation": "Interplanetary coronal mass ejections (ICMEs) are major drivers of space weather disturbances, posing risks to technological infrastructure and human activities. There is a need for robust real-time detection of ICMEs in solar wind in situ data for early warning systems.", "method": "The authors developed ARCANE, which evaluates the strengths and limitations of detection models by comparing a machine learning-based method (ResUNet++) to a threshold-based baseline. They use real-time solar wind (RTSW) data instead of high-resolution science data.", "result": "The ResUNet++ model significantly outperforms the baseline, particularly in detecting high-impact events, while retaining solid performance on lower-impact cases. The detection pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5% of the event's duration. As more data becomes available, the performance increases significantly.", "conclusion": "ARCANE marks a substantial step forward in automated space weather monitoring and lays the groundwork for enhanced real-time forecasting capabilities."}}
{"id": "2505.09598", "pdf": "https://arxiv.org/pdf/2505.09598", "abs": "https://arxiv.org/abs/2505.09598", "authors": ["Nidhal Jegham", "Marwen Abdelatti", "Lassad Elmoubarki", "Abdeltawab Hendawi"], "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.", "AI": {"tldr": "\u5c3d\u7ba1\u5355\u4e2a\u67e5\u8be2\u6548\u7387\u9ad8\uff0c\u4f46\u5927\u89c4\u6a21\u5e94\u7528\u5bfc\u81f4\u4e0d\u6210\u6bd4\u4f8b\u7684\u8d44\u6e90\u6d88\u8017\u3002\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u3001\u57fa\u4e8e\u5b9e\u8bc1\u7684\u65b9\u6cd5\u6765\u8861\u91cfLLM\u90e8\u7f72\u7684\u53ef\u6301\u7eed\u6027\uff0c\u4e3aAI\u53d1\u5c55\u7684\u73af\u5883\u8d23\u4efb\u548c\u53ef\u6301\u7eed\u6027\u6807\u51c6\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u884c\u4e1a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u5728\u63a8\u7406\u5c42\u9762\u7684\u73af\u5883\u8db3\u8ff9\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7814\u7a76\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u6392\u9664\u4e13\u6709\u6a21\u578b\u3001\u5ffd\u89c6\u57fa\u7840\u8bbe\u65bd\u5dee\u5f02\u548c\u5f00\u9500\u6216\u4ec5\u5173\u6ce8\u8bad\u7ec3\u9636\u6bb5\uff0c\u800c\u63a8\u7406\u5bf9AI\u73af\u5883\u5f71\u54cd\u7684\u5360\u6bd4\u65e5\u76ca\u589e\u52a0\u3002\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u531630\u4e2a\u6700\u5148\u8fdb\u7684LLM\u5728\u5546\u4e1a\u6570\u636e\u4e2d\u5fc3\u4e2d\u7684\u73af\u5883\u8db3\u8ff9\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u516c\u5171API\u6027\u80fd\u6570\u636e\u3001\u533a\u57df\u7279\u5b9a\u7684\u73af\u5883\u4e58\u6570\u548c\u786c\u4ef6\u914d\u7f6e\u7684\u7edf\u8ba1\u63a8\u65ad\uff0c\u5e76\u4f7f\u7528\u4ea4\u53c9\u6548\u7387\u7684\u6570\u636e\u5305\u7edc\u5206\u6790\uff08DEA\uff09\u65b9\u6cd5\u6839\u636e\u6027\u80fd\u4e0e\u73af\u5883\u6210\u672c\u7684\u6bd4\u7387\u5bf9\u6a21\u578b\u8fdb\u884c\u6392\u540d\u3002", "result": "\u7ed3\u679c\u663e\u793ao3\u548cDeepSeek-R1\u662f\u6700\u8017\u80fd\u7684\u6a21\u578b\uff0c\u6bcf\u5904\u7406\u4e00\u4e2a\u957f\u63d0\u793a\u6d88\u8017\u8d85\u8fc733\u74e6\u65f6\uff0c\u662fGPT-4.1 nano\u768470\u591a\u500d\uff1bClaude-3.7 Sonnet\u5728\u751f\u6001\u6548\u7387\u65b9\u9762\u6392\u540d\u6700\u9ad8\u3002\u4f8b\u5982\uff0c\u6bcf\u59297\u4ebf\u6b21GPT-4o\u67e5\u8be2\u4f1a\u5bfc\u81f4\u76f8\u5f53\u4e8e35,000\u6237\u7f8e\u56fd\u5bb6\u5ead\u7528\u7535\u91cf\u7684\u7535\u529b\u6d88\u8017\u7b49\u663e\u8457\u5e74\u5ea6\u73af\u5883\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e2a\u4f53\u67e5\u8be2\u9ad8\u6548\u4f46\u5168\u7403\u89c4\u6a21\u5e94\u7528\u5bfc\u81f4\u8d44\u6e90\u8fc7\u5ea6\u6d88\u8017\u7684\u77db\u76fe\u73b0\u8c61\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u5b9e\u8bc1\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u63a8\u52a8\u672a\u6765AI\u5f00\u53d1\u4e2d\u7684\u73af\u5883\u8d23\u4efb\u548c\u53ef\u6301\u7eed\u6027\u6807\u51c6\u3002"}}
{"id": "2505.09425", "pdf": "https://arxiv.org/pdf/2505.09425", "abs": "https://arxiv.org/abs/2505.09425", "authors": ["Sarah Leyder", "Jakob Raymaekers", "Peter J. Rousseeuw", "Tom Van Deuren", "Tim Verdonck"], "title": "Independent Component Analysis by Robust Distance Correlation", "categories": ["stat.CO", "cs.LG"], "comment": null, "summary": "Independent component analysis (ICA) is a powerful tool for decomposing a\nmultivariate signal or distribution into fully independent sources, not just\nuncorrelated ones. Unfortunately, most approaches to ICA are not robust against\noutliers. Here we propose a robust ICA method called RICA, which estimates the\ncomponents by minimizing a robust measure of dependence between multivariate\nrandom variables. The dependence measure used is the distance correlation\n(dCor). In order to make it more robust we first apply a new transformation\ncalled the bowl transform, which is bounded, one-to-one, continuous, and maps\nfar outliers to points close to the origin. This preserves the crucial property\nthat a zero dCor implies independence. RICA estimates the independent sources\nsequentially, by looking for the component that has the smallest dCor with the\nremainder. RICA is strongly consistent and has the usual parametric rate of\nconvergence. Its robustness is investigated by a simulation study, in which it\ngenerally outperforms its competitors. The method is illustrated on three\napplications, including the well-known cocktail party problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRICA\u7684\u9c81\u68d2\u72ec\u7acb\u6210\u5206\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u591a\u53d8\u91cf\u968f\u673a\u53d8\u91cf\u4e4b\u95f4\u7684\u9c81\u68d2\u4f9d\u8d56\u6027\u5ea6\u91cf\uff08\u8ddd\u79bb\u76f8\u5173\u6027dCor\uff09\u6765\u4f30\u8ba1\u6210\u5206\u3002\u4e3a\u4e86\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5f15\u5165\u4e86\u7897\u53d8\u6362\uff08bowl transform\uff09\u3002RICA\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u3001\u53c2\u6570\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u5927\u591a\u6570ICA\u65b9\u6cd5\u5bf9\u79bb\u7fa4\u503c\u4e0d\u9c81\u68d2\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u79bb\u7fa4\u503c\u7684ICA\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8ddd\u79bb\u76f8\u5173\u6027\uff08dCor\uff09\u4f5c\u4e3a\u4f9d\u8d56\u6027\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u7897\u53d8\u6362\uff08bowl transform\uff09\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u7136\u540e\u901a\u8fc7\u5bfb\u627e\u4e0e\u5269\u4f59\u90e8\u5206\u5177\u6709\u6700\u5c0fdCor\u7684\u6210\u5206\u6765\u987a\u5e8f\u4f30\u8ba1\u72ec\u7acb\u6e90\u3002", "result": "RICA\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u5e38\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u4e09\u4e2a\u5b9e\u9645\u95ee\u9898\uff0c\u5305\u62ec\u9e21\u5c3e\u9152\u4f1a\u95ee\u9898\u3002", "conclusion": "RICA\u662f\u4e00\u79cd\u9c81\u68d2\u7684ICA\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u548c\u53c2\u6570\u6536\u655b\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5b58\u5728\u79bb\u7fa4\u503c\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2505.09430", "pdf": "https://arxiv.org/pdf/2505.09430", "abs": "https://arxiv.org/abs/2505.09430", "authors": ["Yutong Hu", "Pinhao Song", "Kehan Wen", "Renaud Detry"], "title": "Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We present a method for training multi-task vision-language robotic diffusion\npolicies that reduces training time and memory usage by an order of magnitude.\nThis improvement arises from a previously underexplored distinction between\naction diffusion and the image diffusion techniques that inspired it: image\ngeneration targets are high-dimensional, while robot actions lie in a much\nlower-dimensional space. Meanwhile, the vision-language conditions for action\ngeneration remain high-dimensional. Our approach, Mini-Diffuser, exploits this\nasymmetry by introducing Level-2 minibatching, which pairs multiple noised\naction samples with each vision-language condition, instead of the conventional\none-to-one sampling strategy. To support this batching scheme, we introduce\narchitectural adaptations to the diffusion transformer that prevent information\nleakage across samples while maintaining full conditioning access. In RLBench\nsimulations, Mini-Diffuser achieves 95\\% of the performance of state-of-the-art\nmulti-task diffusion policies, while using only 5\\% of the training time and\n7\\% of the memory. Real-world experiments further validate that Mini-Diffuser\npreserves the key strengths of diffusion-based policies, including the ability\nto model multimodal action distributions and produce behavior conditioned on\ndiverse perceptual inputs. Code available at\ngithub.com/utomm/mini-diffuse-actor.", "AI": {"tldr": "The paper presents Mini-Diffuser, a method for training multi-task vision-language robotic diffusion policies that significantly reduces training time and memory usage while maintaining high performance.", "motivation": "To reduce the training time and memory usage for multi-task vision-language robotic diffusion policies by exploiting the dimensional differences between action diffusion and image diffusion techniques.", "method": "Mini-Diffuser uses Level-2 minibatching, pairing multiple noised action samples with each vision-language condition. Architectural adaptations to the diffusion transformer prevent information leakage across samples while maintaining full conditioning access.", "result": "In RLBench simulations, Mini-Diffuser achieves 95% of the performance of state-of-the-art multi-task diffusion policies, using only 5% of the training time and 7% of the memory. Real-world experiments validate its ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs.", "conclusion": "Mini-Diffuser effectively reduces resource requirements for training multi-task vision-language robotic diffusion policies without significant loss in performance."}}
{"id": "2505.09471", "pdf": "https://arxiv.org/pdf/2505.09471", "abs": "https://arxiv.org/abs/2505.09471", "authors": ["Xiaoyu Hu", "Gengyu Xue", "Zhenhua Lin", "Yi Yu"], "title": "Fairness-aware Bayes optimal functional classification", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Algorithmic fairness has become a central topic in machine learning, and\nmitigating disparities across different subpopulations has emerged as a rapidly\ngrowing research area. In this paper, we systematically study the\nclassification of functional data under fairness constraints, ensuring the\ndisparity level of the classifier is controlled below a pre-specified\nthreshold. We propose a unified framework for fairness-aware functional\nclassification, tackling an infinite-dimensional functional space, addressing\nkey challenges from the absence of density ratios and intractability of\nposterior probabilities, and discussing unique phenomena in functional\nclassification. We further design a post-processing algorithm, Fair Functional\nLinear Discriminant Analysis classifier (Fair-FLDA), which targets at\nhomoscedastic Gaussian processes and achieves fairness via group-wise\nthresholding. Under weak structural assumptions on eigenspace, theoretical\nguarantees on fairness and excess risk controls are established. As a\nbyproduct, our results cover the excess risk control of the standard FLDA as a\nspecial case, which, to the best of our knowledge, is first time seen. Our\ntheoretical findings are complemented by extensive numerical experiments on\nsynthetic and real datasets, highlighting the practicality of our designed\nalgorithm.", "AI": {"tldr": "This paper explores the classification of functional data with fairness constraints, proposing a unified framework and a post-processing algorithm called Fair-FLDA. It provides theoretical guarantees on fairness and excess risk controls, supported by numerical experiments.", "motivation": "The motivation is to systematically study fair classification in functional data while controlling disparity levels below a pre-specified threshold, addressing challenges like absence of density ratios and intractability of posterior probabilities.", "method": "A unified framework for fairness-aware functional classification is proposed. A post-processing algorithm named Fair-FLDA is designed for homoscedastic Gaussian processes using group-wise thresholding.", "result": "Theoretical guarantees on fairness and excess risk controls are established under weak structural assumptions. The results also cover the excess risk control of standard FLDA as a special case.", "conclusion": "The findings are supported by extensive numerical experiments on both synthetic and real datasets, demonstrating the practicality of the Fair-FLDA algorithm."}}
{"id": "2505.09496", "pdf": "https://arxiv.org/pdf/2505.09496", "abs": "https://arxiv.org/abs/2505.09496", "authors": ["Rui Miao", "Babak Shahbaba", "Annie Qu"], "title": "Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to find optimal policies in dynamic\nenvironments in order to maximize the expected total rewards by leveraging\npre-collected data. Learning from heterogeneous data is one of the fundamental\nchallenges in offline RL. Traditional methods focus on learning an optimal\npolicy for all individuals with pre-collected data from a single episode or\nhomogeneous batch episodes, and thus, may result in a suboptimal policy for a\nheterogeneous population. In this paper, we propose an individualized offline\npolicy optimization framework for heterogeneous time-stationary Markov decision\nprocesses (MDPs). The proposed heterogeneous model with individual latent\nvariables enables us to efficiently estimate the individual Q-functions, and\nour Penalized Pessimistic Personalized Policy Learning (P4L) algorithm\nguarantees a fast rate on the average regret under a weak partial coverage\nassumption on behavior policies. In addition, our simulation studies and a real\ndata application demonstrate the superior numerical performance of the proposed\nmethod compared with existing methods.", "AI": {"tldr": "Offline reinforcement learning using pre-collected data faces challenges when dealing with heterogeneous data. This paper proposes an individualized offline policy optimization framework for heterogeneous MDPs, introducing a model with individual latent variables and the P4L algorithm to efficiently estimate individual Q-functions and guarantee a fast rate on average regret.", "motivation": "Traditional methods in offline RL focus on learning optimal policies from homogeneous data, which may lead to suboptimal results for heterogeneous populations.", "method": "The authors propose an individualized offline policy optimization framework for heterogeneous time-stationary MDPs that includes a heterogeneous model with individual latent variables and the Penalized Pessimistic Personalized Policy Learning (P4L) algorithm.", "result": "Simulation studies and real data application show superior numerical performance of the proposed method compared to existing methods under weak partial coverage assumption on behavior policies.", "conclusion": "The proposed individualized offline policy optimization framework effectively addresses the challenge of learning from heterogeneous data in offline RL."}}
{"id": "2505.09506", "pdf": "https://arxiv.org/pdf/2505.09506", "abs": "https://arxiv.org/abs/2505.09506", "authors": ["Mar\u00eda Alejandra Hern\u00e1ndez", "Oscar Rodriguez", "Dae-Jin Lee"], "title": "Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders", "categories": ["stat.ML", "cs.LG", "F.2.2; I.2.7"], "comment": "Pre-print", "summary": "Several approaches have been developed to capture the complexity and\nnonlinearity of human growth. One widely used is the Super Imposition by\nTranslation and Rotation (SITAR) model, which has become popular in studies of\nadolescent growth. SITAR is a shape-invariant mixed-effects model that\nrepresents the shared growth pattern of a population using a natural cubic\nspline mean curve while incorporating three subject-specific random effects --\ntiming, size, and growth intensity -- to account for variations among\nindividuals. In this work, we introduce a supervised deep learning framework\nbased on an autoencoder architecture that integrates a deep neural network\n(neural network) with a B-spline model to estimate the SITAR model. In this\napproach, the encoder estimates the random effects for each individual, while\nthe decoder performs a fitting based on B-splines similar to the classic SITAR\nmodel. We refer to this method as the Deep-SITAR model. This innovative\napproach enables the prediction of the random effects of new individuals\nentering a population without requiring a full model re-estimation. As a\nresult, Deep-SITAR offers a powerful approach to predicting growth\ntrajectories, combining the flexibility and efficiency of deep learning with\nthe interpretability of traditional mixed-effects models.", "AI": {"tldr": "The paper introduces Deep-SITAR, a supervised deep learning framework combining autoencoder architecture with the SITAR model to predict growth trajectories.", "motivation": "To improve upon existing methods for capturing the complexity and nonlinearity of human growth by integrating deep learning techniques with traditional SITAR models.", "method": "Development of the Deep-SITAR model which uses an autoencoder architecture that includes a deep neural network and B-spline model. The encoder estimates random effects for individuals, while the decoder fits data using B-splines.", "result": "Deep-SITAR allows for predicting random effects of new individuals without re-estimating the full model, providing a flexible and efficient approach.", "conclusion": "Deep-SITAR is a powerful method for predicting growth trajectories, merging deep learning flexibility with the interpretability of mixed-effects models."}}
{"id": "2505.09516", "pdf": "https://arxiv.org/pdf/2505.09516", "abs": "https://arxiv.org/abs/2505.09516", "authors": ["Siyi Wang", "Alexandre Leblanc", "Paul D. McNicholas"], "title": "Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios", "categories": ["stat.ME", "cs.LG", "stat.AP"], "comment": null, "summary": "Cluster analysis, or clustering, plays a crucial role across numerous\nscientific and engineering domains. Despite the wealth of clustering methods\nproposed over the past decades, each method is typically designed for specific\nscenarios and presents certain limitations in practical applications. In this\npaper, we propose depth-based local center clustering (DLCC). This novel method\nmakes use of data depth, which is known to produce a center-outward ordering of\nsample points in a multivariate space. However, data depth typically fails to\ncapture the multimodal characteristics of {data}, something of the utmost\nimportance in the context of clustering. To overcome this, DLCC makes use of a\nlocal version of data depth that is based on subsets of {data}. From this,\nlocal centers can be identified as well as clusters of varying shapes.\nFurthermore, we propose a new internal metric based on density-based clustering\nto evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a\nflexible clustering approach that seems to overcome some limitations of\ntraditional clustering methods, thereby enhancing data analysis capabilities\nacross a wide range of application scenarios.", "AI": {"tldr": "The paper introduces Depth-based Local Center Clustering (DLCC), a new clustering method that uses local data depth to identify multimodal characteristics and varying shape clusters, along with a new internal metric for evaluating non-convex cluster performance.", "motivation": "Existing clustering methods have limitations in practical applications as they are typically designed for specific scenarios. Traditional methods often fail to capture the multimodal characteristics of data and struggle with non-convex clusters.", "method": "The proposed method, DLCC, employs a local version of data depth based on subsets of data to identify local centers and clusters of varying shapes. It also proposes a new internal metric based on density-based clustering to evaluate clustering performance on non-convex clusters.", "result": "DLCC is presented as a flexible clustering approach that overcomes some limitations of traditional clustering methods, such as capturing multimodal characteristics and handling non-convex clusters effectively.", "conclusion": "DLCC enhances data analysis capabilities across a wide range of application scenarios by providing a more versatile and effective clustering solution compared to traditional methods."}}
{"id": "2505.09546", "pdf": "https://arxiv.org/pdf/2505.09546", "abs": "https://arxiv.org/abs/2505.09546", "authors": ["Yujin Kim", "Nathaniel Chin", "Arnav Vasudev", "Sanjiban Choudhury"], "title": "Distilling Realizable Students from Unrealizable Teachers", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We study policy distillation under privileged information, where a student\npolicy with only partial observations must learn from a teacher with full-state\naccess. A key challenge is information asymmetry: the student cannot directly\naccess the teacher's state space, leading to distributional shifts and policy\ndegradation. Existing approaches either modify the teacher to produce\nrealizable but sub-optimal demonstrations or rely on the student to explore\nmissing information independently, both of which are inefficient. Our key\ninsight is that the student should strategically interact with the teacher\n--querying only when necessary and resetting from recovery states --to stay on\na recoverable path within its own observation space. We introduce two methods:\n(i) an imitation learning approach that adaptively determines when the student\nshould query the teacher for corrections, and (ii) a reinforcement learning\napproach that selects where to initialize training for efficient exploration.\nWe validate our methods in both simulated and real-world robotic tasks,\ndemonstrating significant improvements over standard teacher-student baselines\nin training efficiency and final performance. The project website is available\nat : https://portal-cornell.github.io/CritiQ_ReTRy/", "AI": {"tldr": "This paper addresses policy distillation under privileged information by introducing two methods to help the student policy learn effectively from a teacher with full-state access.", "motivation": "The motivation of this paper is to solve the problem of information asymmetry in policy distillation, where the student policy has only partial observations and cannot directly access the teacher's state space.", "method": "The authors propose two methods: (i) an imitation learning approach that adaptively determines when the student should query the teacher for corrections, and (ii) a reinforcement learning approach that selects where to initialize training for efficient exploration.", "result": "The proposed methods were validated in both simulated and real-world robotic tasks, showing significant improvements over standard teacher-student baselines in terms of training efficiency and final performance.", "conclusion": "The introduced methods provide a more efficient way for the student policy to learn from the teacher policy despite information asymmetry."}}
{"id": "2505.09552", "pdf": "https://arxiv.org/pdf/2505.09552", "abs": "https://arxiv.org/abs/2505.09552", "authors": ["Pascal K\u00fcndig", "Fabio Sigrist"], "title": "Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "Mixed effects models are widely used for modeling data with hierarchically\ngrouped structures and high-cardinality categorical predictor variables.\nHowever, for high-dimensional crossed random effects, current standard\ncomputations relying on Cholesky decompositions can become prohibitively slow.\nIn this work, we present novel Krylov subspace-based methods that address\nseveral existing computational bottlenecks. Among other things, we\ntheoretically analyze and empirically evaluate various preconditioners for the\nconjugate gradient and stochastic Lanczos quadrature methods, derive new\nconvergence results, and develop computationally efficient methods for\ncalculating predictive variances. Extensive experiments using simulated and\nreal-world data sets show that our proposed methods scale much better than\nCholesky-based computations, for instance, achieving a runtime reduction of\napproximately two orders of magnitudes for both estimation and prediction.\nMoreover, our software implementation is up to 10'000 times faster and more\nstable than state-of-the-art implementations such as lme4 and glmmTMB when\nusing default settings. Our methods are implemented in the free C++ software\nlibrary GPBoost with high-level Python and R packages.", "AI": {"tldr": "Mixed effects models are widely used but slow with high-dimensional crossed random effects. This paper presents Krylov subspace-based methods to solve computational bottlenecks, achieving significant runtime reduction and better scalability compared to Cholesky-based computations.", "motivation": "To address the slowness of current standard computations using Cholesky decompositions for high-dimensional crossed random effects in mixed effects models.", "method": "Novel Krylov subspace-based methods are developed. Theoretical analysis and empirical evaluation of preconditioners for conjugate gradient and stochastic Lanczos quadrature methods are performed. New convergence results are derived and computationally efficient methods for calculating predictive variances are developed.", "result": "Extensive experiments show that the proposed methods scale much better than Cholesky-based computations, with a runtime reduction of approximately two orders of magnitude for both estimation and prediction. The software implementation is up to 10'000 times faster and more stable than state-of-the-art implementations like lme4 and glmmTMB.", "conclusion": "The Krylov subspace-based methods presented in this work provide a significant improvement in computational efficiency and scalability for mixed effects models with high-dimensional crossed random effects."}}
{"id": "2505.09603", "pdf": "https://arxiv.org/pdf/2505.09603", "abs": "https://arxiv.org/abs/2505.09603", "authors": ["Shivin Dass", "Alaa Khaddaj", "Logan Engstrom", "Aleksander Madry", "Andrew Ilyas", "Roberto Mart\u00edn-Mart\u00edn"], "title": "DataMIL: Selecting Data for Robot Imitation Learning with Datamodels", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recently, the robotics community has amassed ever larger and more diverse\ndatasets to train generalist robot policies. However, while these policies\nachieve strong mean performance across a variety of tasks, they often\nunderperform on individual, specialized tasks and require further tuning on\nnewly acquired task-specific data. Combining task-specific data with carefully\ncurated subsets of large prior datasets via co-training can produce better\nspecialized policies, but selecting data naively may actually harm downstream\nperformance. To address this, we introduce DataMIL, a policy-driven data\nselection framework built on the datamodels paradigm that reasons about data\nselection in an end-to-end manner, using the policy itself to identify which\ndata points will most improve performance. Unlike standard practices that\nfilter data using human notions of quality (e.g., based on semantic or visual\nsimilarity), DataMIL directly optimizes data selection for task success,\nallowing us to select data that enhance the policy while dropping data that\ndegrade it. To avoid performing expensive rollouts in the environment during\nselection, we use a novel surrogate loss function on task-specific data,\nallowing us to use DataMIL in the real world without degrading performance. We\nvalidate our approach on a suite of more than 60 simulation and real-world\nmanipulation tasks - most notably showing successful data selection from the\nOpen X-Embodiment datasets-demonstrating consistent gains in success rates and\nsuperior performance over multiple baselines. Our results underscore the\nimportance of end-to-end, performance-aware data selection for unlocking the\npotential of large prior datasets in robotics. More information at\nhttps://robin-lab.cs.utexas.edu/datamodels4imitation/", "AI": {"tldr": "DataMIL is a policy-driven data selection framework that optimizes data selection for task success in robotics, showing consistent gains in success rates and superior performance over multiple baselines.", "motivation": "The motivation of this paper is to address the limitation of generalist robot policies which underperform on individual, specialized tasks. The authors aim to improve specialized policies by combining task-specific data with subsets of large prior datasets via co-training, while avoiding naive data selection that may harm downstream performance.", "method": "The method introduced is DataMIL, a policy-driven data selection framework based on the datamodels paradigm. It uses an end-to-end approach to select data points that will most improve performance, directly optimizing for task success rather than relying on human notions of quality. A novel surrogate loss function on task-specific data is used to avoid expensive rollouts in the environment during selection.", "result": "The approach was validated on over 60 simulation and real-world manipulation tasks, demonstrating consistent gains in success rates and superior performance compared to multiple baselines. Notably, successful data selection was shown from the Open X-Embodiment datasets.", "conclusion": "The results highlight the importance of end-to-end, performance-aware data selection for maximizing the potential of large prior datasets in robotics."}}
{"id": "2505.09612", "pdf": "https://arxiv.org/pdf/2505.09612", "abs": "https://arxiv.org/abs/2505.09612", "authors": ["Tathagata Sadhukhan", "Manit Paul", "Raaz Dwivedi"], "title": "Adaptively-weighted Nearest Neighbors for Matrix Completion", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": "25 pages, 6 figures", "summary": "In this technical note, we introduce and analyze AWNN: an adaptively weighted\nnearest neighbor method for performing matrix completion. Nearest neighbor (NN)\nmethods are widely used in missing data problems across multiple disciplines\nsuch as in recommender systems and for performing counterfactual inference in\npanel data settings. Prior works have shown that in addition to being very\nintuitive and easy to implement, NN methods enjoy nice theoretical guarantees.\nHowever, the performance of majority of the NN methods rely on the appropriate\nchoice of the radii and the weights assigned to each member in the nearest\nneighbor set and despite several works on nearest neighbor methods in the past\ntwo decades, there does not exist a systematic approach of choosing the radii\nand the weights without relying on methods like cross-validation. AWNN\naddresses this challenge by judiciously balancing the bias variance trade off\ninherent in weighted nearest-neighbor regression. We provide theoretical\nguarantees for the proposed method under minimal assumptions and support the\ntheory via synthetic experiments.", "AI": {"tldr": "This paper introduces AWNN, an adaptively weighted nearest neighbor method for matrix completion. It balances bias and variance without needing cross-validation, providing theoretical guarantees and synthetic experiment support.", "motivation": "Existing nearest neighbor methods lack a systematic approach to choosing radii and weights without using cross-validation.", "method": "AWNN method which judiciously balances the bias variance trade off in weighted nearest-neighbor regression.", "result": "Theoretical guarantees for AWNN are provided under minimal assumptions, supported by synthetic experiments.", "conclusion": "AWNN is an effective method for matrix completion that addresses the challenge of choosing radii and weights systematically."}}
{"id": "2505.09649", "pdf": "https://arxiv.org/pdf/2505.09649", "abs": "https://arxiv.org/abs/2505.09649", "authors": ["Abisha Thapa Magar", "Anup Shakya"], "title": "Next Word Suggestion using Graph Neural Network", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language Modeling is a prevalent task in Natural Language Processing. The\ncurrently existing most recent and most successful language models often tend\nto build a massive model with billions of parameters, feed in a tremendous\namount of text data, and train with enormous computation resources which\nrequire millions of dollars. In this project, we aim to address an important\nsub-task in language modeling, i.e., context embedding. We propose an approach\nto exploit the Graph Convolution operation in GNNs to encode the context and\nuse it in coalition with LSTMs to predict the next word given a local context\nof preceding words. We test this on the custom Wikipedia text corpus using a\nvery limited amount of resources and show that this approach works fairly well\nto predict the next word.", "AI": {"tldr": "The paper explores an alternative method for language modeling by focusing on context embedding, using Graph Convolution in GNNs with LSTMs to predict the next word with limited resources.", "motivation": "To address the high resource demands of current state-of-the-art language models, the study focuses on a sub-task of language modeling - context embedding.", "method": "The approach uses Graph Convolution operation in GNNs to encode context and combines it with LSTMs to predict the next word based on preceding words' local context.", "result": "The method was tested on a custom Wikipedia text corpus with limited resources and showed promising results in predicting the next word.", "conclusion": "This novel approach provides a feasible alternative for language modeling tasks with significantly fewer resources."}}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655", "abs": "https://arxiv.org/abs/2505.09655", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.", "AI": {"tldr": "An abstract about a new method called Diversity-aware Reward Adjustment (DRA) which incorporates semantic diversity into reward computation for reinforcement learning in language model post-training. It outperforms recent baselines achieving state-of-the-art performance with an average accuracy of 58.2%.", "motivation": "The motivation is to address the limitation of Group Relative Policy Optimization (GRPO) that fails to capture semantic diversity among sampled completions leading to a diversity-quality inconsistency.", "method": "The proposed method, Diversity-aware Reward Adjustment (DRA), uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning while maintaining stable exploitation of high-quality samples.", "result": "Evaluated on five mathematical reasoning benchmarks, DRA outperforms recent strong baselines achieving state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55.", "conclusion": "DRA integrates seamlessly with GRPO and its variant DR.~GRPO, resulting in DRA-GRPO and DGA-DR.~GRPO, improving performance in low-resource settings."}}
{"id": "2505.09662", "pdf": "https://arxiv.org/pdf/2505.09662", "abs": "https://arxiv.org/abs/2505.09662", "authors": ["Philipp Schoenegger", "Francesco Salvi", "Jiacheng Liu", "Xiaoli Nan", "Ramit Debnath", "Barbara Fasolo", "Evelina Leivada", "Gabriel Recchia", "Fritz G\u00fcnther", "Ali Zarifhonarvar", "Joe Kwon", "Zahoor Ul Islam", "Marco Dehnert", "Daryl Y. H. Lee", "Madeline G. Reinecke", "David G. Kamper", "Mert Koba\u015f", "Adam Sandford", "Jonas Kgomo", "Luke Hewitt", "Shreya Kapoor", "Kerem Oktar", "Eyup Engin Kucuk", "Bo Feng", "Cameron R. Jones", "Izzy Gainsburg", "Sebastian Olschewski", "Nora Heinzelmann", "Francisco Cruz", "Ben M. Tappin", "Tao Ma", "Peter S. Park", "Rayan Onyonka", "Arthur Hjorth", "Peter Slattery", "Qingcheng Zeng", "Lennart Finke", "Igor Grossmann", "Alessandro Salatiello", "Ezra Karger"], "title": "Large Language Models Are More Persuasive Than Incentivized Human Persuaders", "categories": ["cs.CL", "I.2.7; H.1.2; K.4.1; H.5.2"], "comment": null, "summary": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.", "AI": {"tldr": "In a large-scale experiment, an advanced LLM (Claude Sonnet 3.5) outperformed incentivized human persuaders in persuading participants towards correct or incorrect answers in a real-time conversational quiz setting.", "motivation": "To directly compare the persuasion capabilities of a frontier large language model against incentivized human persuaders and evaluate its impact on participants' accuracy and earnings in a quiz setting.", "method": "A preregistered, large-scale incentivized experiment where participants completed an online quiz while being persuaded by either humans or the LLM towards correct or incorrect answers.", "result": "The LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than humans, improving participants' accuracy and earnings when guiding towards correct answers and decreasing them when steering towards incorrect answers.", "conclusion": "AI's persuasion capabilities already surpass those of humans with real-money bonuses tied to performance, highlighting the need for emerging alignment and governance frameworks."}}
{"id": "2505.09666", "pdf": "https://arxiv.org/pdf/2505.09666", "abs": "https://arxiv.org/abs/2505.09666", "authors": ["Yumin Choi", "Jinheon Baek", "Sung Ju Hwang"], "title": "System Prompt Optimization with Meta-Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.", "AI": {"tldr": "Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance.", "motivation": "Existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains.", "method": "To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them.", "result": "We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts.", "conclusion": "Our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance."}}
{"id": "2505.09639", "pdf": "https://arxiv.org/pdf/2505.09639", "abs": "https://arxiv.org/abs/2505.09639", "authors": ["Quentin Cohen-Solal"], "title": "Study and improvement of search algorithms in two-players perfect information games", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Games, in their mathematical sense, are everywhere (game industries,\neconomics, defense, education, chemistry, biology, ...).Search algorithms in\ngames are artificial intelligence methods for playing such games.\nUnfortunately, there is no study on these algorithms that evaluates the\ngenerality of their performance. We propose to address this gap in the case of\ntwo-player zero-sum games with perfect information. Furthermore, we propose a\nnew search algorithm and we show that, for a short search time, it outperforms\nall studied algorithms on all games in this large experiment and that, for a\nmedium search time, it outperforms all studied algorithms on 17 of the 22\nstudied games.", "AI": {"tldr": "\u5728\u6570\u5b66\u610f\u4e49\u4e0a\uff0c\u6e38\u620f\u65e0\u5904\u4e0d\u5728\u3002\u6e38\u620f\u4e2d\u7684\u641c\u7d22\u7b97\u6cd5\u662f\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u3002\u4f46\u76ee\u524d\u5c1a\u65e0\u5bf9\u8fd9\u4e9b\u7b97\u6cd5\u6027\u80fd\u666e\u904d\u6027\u7684\u8bc4\u4f30\u7814\u7a76\u3002\u672c\u6587\u4ee5\u53cc\u4eba\u96f6\u548c\u5b8c\u7f8e\u4fe1\u606f\u535a\u5f08\u4e3a\u4f8b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u77ed\u65f6\u95f4\u641c\u7d22\u65f6\u5168\u9762\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5728\u4e2d\u7b49\u641c\u7d22\u65f6\u95f4\u5185\u4e8e22\u4e2a\u7814\u7a76\u6e38\u620f\u4e2d\u670917\u4e2a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u6e38\u620f\u641c\u7d22\u7b97\u6cd5\u6027\u80fd\u666e\u904d\u6027\u7684\u8bc4\u4f30\u7814\u7a76\uff0c\u7279\u522b\u662f\u9488\u5bf9\u53cc\u4eba\u96f6\u548c\u5b8c\u7f8e\u4fe1\u606f\u535a\u5f08\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u5c06\u5176\u4e0e\u73b0\u6709\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u77ed\u65f6\u95f4\u5185\u641c\u7d22\u65f6\u5168\u9762\u4f18\u4e8e\u6240\u6709\u5df2\u7814\u7a76\u7b97\u6cd5\uff1b\u5728\u4e2d\u7b49\u641c\u7d22\u65f6\u95f4\u5185\uff0c\u572822\u4e2a\u7814\u7a76\u6e38\u620f\u4e2d\u670917\u4e2a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u63d0\u51fa\u7684\u641c\u7d22\u7b97\u6cd5\u5728\u53cc\u4eba\u96f6\u548c\u5b8c\u7f8e\u4fe1\u606f\u535a\u5f08\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8bc4\u4f30\u641c\u7d22\u7b97\u6cd5\u7684\u666e\u904d\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.09659", "pdf": "https://arxiv.org/pdf/2505.09659", "abs": "https://arxiv.org/abs/2505.09659", "authors": ["Long Chen", "Xiaotian Song", "Yanan Sun"], "title": "LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Spiking Large Language Models (LLMs) have emerged as an energy-efficient\nalternative to conventional LLMs through their event-driven computation. To\neffectively obtain spiking LLMs, researchers develop different ANN-to-SNN\nconversion methods by leveraging pre-trained ANN parameters while inheriting\nthe energy efficiency of SNN. However, existing conversion methods struggle\nwith extreme activation outliers and incompatible nonlinear operations of\nANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for\nfully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel\nneurons to convert the activation outlier and nonlinear operation of ANN-based\nLLMs. Moreover, LAS tailors the spike-equivalent Transformer components for\nspiking LLMs, which can ensure full spiking conversion without any loss of\nperformance. Experimental results on six language models and two\nvision-language models demonstrate that LAS achieves loss-less conversion.\nNotably, on OPT-66B, LAS even improves the accuracy of 2\\% on the WSC task. In\naddition, the parameter and ablation studies further verify the effectiveness\nof LAS. The source code is available at https://github.com/lc783/LAS", "AI": {"tldr": "The paper proposes LAS, a loss-less ANN-SNN conversion method for fully spike-driven LLMs that introduces novel neurons and tailored spike-equivalent Transformer components. Experiments show its effectiveness with no performance loss and even accuracy improvement on certain tasks.", "motivation": "To effectively obtain spiking LLMs with energy efficiency while overcoming the challenges of activation outliers and incompatible nonlinear operations in existing ANN-to-SNN conversion methods.", "method": "LAS proposes two novel neurons to handle activation outlier and nonlinear operation issues in ANN-based LLMs. It also tailors spike-equivalent Transformer components ensuring full spiking conversion without any performance loss.", "result": "Experimental results on six language models and two vision-language models demonstrate loss-less conversion. Notably, on OPT-66B, LAS improves the accuracy by 2% on the WSC task.", "conclusion": "LAS achieves loss-less ANN-SNN conversion for fully spike-driven LLMs. The parameter and ablation studies verify its effectiveness."}}
{"id": "2505.09746", "pdf": "https://arxiv.org/pdf/2505.09746", "abs": "https://arxiv.org/abs/2505.09746", "authors": ["Xabier Morales", "Ayah Elsayed", "Debbie Zhao", "Filip Loncaric", "Ainhoa Aguado", "Mireia Masias", "Gina Quill", "Marc Ramos", "Ada Doltra", "Ana Garcia", "Marta Sitges", "David Marlevi", "Alistair Young", "Martyn Nash", "Bart Bijnens", "Oscar Camara"], "title": "A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium", "categories": ["cs.CV"], "comment": null, "summary": "The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.", "AI": {"tldr": "The study introduces an open-source computational framework for analyzing 4D Flow MRI in the left atrium (LA), providing robust automated segmentations and conducting a comprehensive assessment of hemodynamic parameters.", "motivation": "To overcome the limitations of conventional ultrasound analysis and improve understanding of left atrium hemodynamics using 4D Flow MRI, despite challenges such as low velocities, limited spatial resolution, and lack of dedicated computational frameworks.", "method": "Development of an open-source computational framework tailored for 4D Flow MRI analysis in the LA, enabling qualitative and quantitative analysis of advanced hemodynamic parameters. The framework is tested on data from different centers and used to assess energy, vorticity, and pressure parameters across various disorders.", "result": "The framework produces high-accuracy automated segmentations (Dice > 0.9 and Hausdorff 95 < 3 mm) even with limited training data. A comprehensive assessment of hemodynamic parameters reveals their potential as prognostic biomarkers.", "conclusion": "The introduced computational framework offers a robust solution for analyzing 4D Flow MRI in the LA, advancing the study of hemodynamic parameters as potential prognostic biomarkers."}}
{"id": "2505.09701", "pdf": "https://arxiv.org/pdf/2505.09701", "abs": "https://arxiv.org/abs/2505.09701", "authors": ["Xin Liu", "Lechen Zhang", "Sheza Munir", "Yiyang Gu", "Lu Wang"], "title": "VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at generating long-form responses, but\nevaluating their factuality remains challenging due to complex inter-sentence\ndependencies within the generated facts. Prior solutions predominantly follow a\ndecompose-decontextualize-verify pipeline but often fail to capture essential\ncontext and miss key relational facts. In this paper, we introduce VeriFact, a\nfactuality evaluation framework designed to enhance fact extraction by\nidentifying and resolving incomplete and missing facts to support more accurate\nverification results. Moreover, we introduce FactRBench , a benchmark that\nevaluates both precision and recall in long-form model responses, whereas prior\nwork primarily focuses on precision. FactRBench provides reference fact sets\nfrom advanced LLMs and human-written answers, enabling recall assessment.\nEmpirical evaluations show that VeriFact significantly enhances fact\ncompleteness and preserves complex facts with critical relational information,\nresulting in more accurate factuality evaluation. Benchmarking various open-\nand close-weight LLMs on FactRBench indicate that larger models within same\nmodel family improve precision and recall, but high precision does not always\ncorrelate with high recall, underscoring the importance of comprehensive\nfactuality assessment.", "AI": {"tldr": "VeriFact is a new framework for factuality evaluation that enhances fact extraction and verification by identifying incomplete or missing facts, while FactRBench is a benchmark to evaluate both precision and recall in long-form model responses.", "motivation": "Existing methods for evaluating the factuality of LLMs often fail to capture essential context and miss key relational facts due to their decompose-decontextualize-verify pipeline.", "method": "Introduced VeriFact, a factuality evaluation framework designed to enhance fact extraction by identifying and resolving incomplete and missing facts. Also introduced FactRBench, a benchmark that evaluates both precision and recall in long-form model responses.", "result": "Empirical evaluations show that VeriFact significantly enhances fact completeness and preserves complex facts with critical relational information. Benchmarking various LLMs on FactRBench indicate larger models within same model family improve precision and recall, but high precision does not always correlate with high recall.", "conclusion": "VeriFact improves factuality evaluation by enhancing fact completeness and preserving complex facts. FactRBench provides a comprehensive way to assess both precision and recall in long-form model responses."}}
{"id": "2505.09640", "pdf": "https://arxiv.org/pdf/2505.09640", "abs": "https://arxiv.org/abs/2505.09640", "authors": ["Tom\u00e1s Capdevielle", "Santiago Cifuentes"], "title": "Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms", "categories": ["cs.AI", "68T01", "I.2.0"], "comment": "22 pages, 7 figures", "summary": "Given a classification model and a prediction for some input, there are\nheuristic strategies for ranking features according to their importance in\nregard to the prediction. One common approach to this task is rooted in\npropositional logic and the notion of \\textit{sufficient reason}. Through this\nconcept, the categories of relevant and necessary features were proposed in\norder to identify the crucial aspects of the input. This paper improves the\nexisting techniques and algorithms for deciding which are the relevant and/or\nnecessary features, showing in particular that necessity can be detected\nefficiently in complex models such as neural networks. We also generalize the\nnotion of relevancy and study associated problems. Moreover, we present a new\nglobal notion (i.e. that intends to explain whether a feature is important for\nthe behavior of the model in general, not depending on a particular input) of\n\\textit{usefulness} and prove that it is related to relevancy and necessity.\nFurthermore, we develop efficient algorithms for detecting it in decision trees\nand other more complex models, and experiment on three datasets to analyze its\npractical utility.", "AI": {"tldr": "This paper enhances techniques for identifying relevant and necessary features in classification models, introduces the concept of usefulness, and provides efficient algorithms for its detection.", "motivation": "To improve existing strategies for ranking features according to their importance in predictions from classification models, especially focusing on the concepts of relevancy and necessity.", "method": "Generalize the notion of relevancy, introduce a global concept of usefulness, develop efficient algorithms for detecting necessity and usefulness in complex models like neural networks and decision trees.", "result": "Showed that necessity can be efficiently detected in complex models, established relationships between usefulness, relevancy and necessity, and demonstrated practical utility through experiments on three datasets.", "conclusion": "The introduced concept of usefulness is related to relevancy and necessity, and efficient algorithms were developed for its detection, contributing to understanding feature importance in model behavior."}}
{"id": "2505.09663", "pdf": "https://arxiv.org/pdf/2505.09663", "abs": "https://arxiv.org/abs/2505.09663", "authors": ["Julian B\u00fcchel", "Iason Chalas", "Giovanni Acampa", "An Chen", "Omobayode Fagbohungbe", "Sidney Tsai", "Kaoutar El Maghraoui", "Manuel Le Gallo", "Abbas Rahimi", "Abu Sebastian"], "title": "Analog Foundation Models", "categories": ["cs.LG"], "comment": "43 pages, 8 figures, under review", "summary": "Analog in-memory computing (AIMC) is a promising compute paradigm to improve\nspeed and power efficiency of neural network inference beyond the limits of\nconventional von Neumann-based architectures. However, AIMC introduces\nfundamental challenges such as noisy computations and strict constraints on\ninput and output quantization. Because of these constraints and imprecisions,\noff-the-shelf LLMs are not able to achieve 4-bit-level performance when\ndeployed on AIMC-based hardware. While researchers previously investigated\nrecovering this accuracy gap on small, mostly vision-based models, a generic\nmethod applicable to LLMs pre-trained on trillions of tokens does not yet\nexist. In this work, we introduce a general and scalable method to robustly\nadapt LLMs for execution on noisy, low-precision analog hardware. Our approach\nenables state-of-the-art models $\\unicode{x2013}$ including\nPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\\unicode{x2013}$ to retain\nperformance comparable to 4-bit weight, 8-bit activation baselines, despite the\npresence of analog noise and quantization constraints. Additionally, we show\nthat as a byproduct of our training methodology, analog foundation models can\nbe quantized for inference on low-precision digital hardware. Finally, we show\nthat our models also benefit from test-time compute scaling, showing better\nscaling behavior than models trained with 4-bit weight and 8-bit static input\nquantization. Our work bridges the gap between high-capacity LLMs and efficient\nanalog hardware, offering a path toward energy-efficient foundation models.\nCode is available at https://github.com/IBM/analog-foundation-models .", "AI": {"tldr": "This paper presents a method to adapt large language models (LLMs) for execution on analog in-memory computing (AIMC) hardware, overcoming challenges like noise and quantization constraints. The approach allows state-of-the-art LLMs to retain performance comparable to digital 4-bit weight, 8-bit activation baselines while being executed on AIMC hardware.", "motivation": "Analog in-memory computing (AIMC) offers potential improvements in speed and power efficiency for neural network inference compared to traditional von Neumann-based architectures. However, it introduces challenges such as noisy computations and strict input/output quantization constraints that hinder the deployment of LLMs on AIMC-based hardware.", "method": "The researchers developed a general and scalable method to robustly adapt LLMs for execution on noisy, low-precision analog hardware. This involves training methodologies that enable LLMs to overcome the accuracy gap when deployed on AIMC hardware.", "result": "The approach successfully enables state-of-the-art LLMs, such as Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct, to retain performance comparable to 4-bit weight, 8-bit activation baselines despite analog noise and quantization constraints. Additionally, the models can be quantized for inference on low-precision digital hardware and benefit from test-time compute scaling.", "conclusion": "This work bridges the gap between high-capacity LLMs and efficient analog hardware, offering a path toward energy-efficient foundation models."}}
{"id": "2505.09827", "pdf": "https://arxiv.org/pdf/2505.09827", "abs": "https://arxiv.org/abs/2505.09827", "authors": ["Julian Tanke", "Takashi Shibuya", "Kengo Uchida", "Koichi Saito", "Yuki Mitsufuji"], "title": "Dyadic Mamba: Long-term Dyadic Human Motion Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 HuMoGen Workshop", "summary": "Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.", "AI": {"tldr": "\u751f\u6210\u903c\u771f\u7684\u53cc\u4eba\u4eba\u4f53\u8fd0\u52a8\u662f\u5177\u6709\u6311\u6218\u6027\u7684\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u8d85\u8fc7\u5178\u578b\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u7684\u957f\u65f6\u95f4\u4ea4\u4e92\u3002\u672c\u6587\u63d0\u51fa\u4e86Dyadic Mamba\u65b9\u6cd5\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4efb\u610f\u957f\u5ea6\u7684\u53cc\u4eba\u4eba\u4f53\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u957f\u671f\u8fd0\u52a8\u5408\u6210\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\u5728\u77ed\u671f\u53cc\u4eba\u8fd0\u52a8\u5408\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u66f4\u957f\u7684\u5e8f\u5217\u65f6\u53d7\u5230\u9650\u5236\uff0c\u56e0\u4e3a\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u5b58\u5728\u56fa\u6709\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u4e86Dyadic Mamba\u65b9\u6cd5\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u67b6\u6784\u4fc3\u8fdb\u4e2a\u4f53\u8fd0\u52a8\u5e8f\u5217\u4e4b\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\uff0c\u65e0\u9700\u590d\u6742\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "Dyadic Mamba\u5728\u6807\u51c6\u77ed\u671f\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728\u8f83\u957f\u5e8f\u5217\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u957f\u671f\u8fd0\u52a8\u5408\u6210\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u57fa\u4e8eSSM\u7684\u67b6\u6784\u4e3a\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u8fdb\u884c\u957f\u671f\u53cc\u4eba\u4eba\u4f53\u8fd0\u52a8\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724", "abs": "https://arxiv.org/abs/2505.09724", "authors": ["Gino Carmona-D\u00edaz", "William Jim\u00e9nez-Leal", "Mar\u00eda Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Berm\u00fadez"], "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "This paper presents a tutorial for developing, testing and applying taxonomies to analyze unstructured data using LLMs, demonstrating the process with personal goals as an example.", "motivation": "To provide a methodological guide for efficiently analyzing unstructured text data while reducing bias and maintaining quality.", "method": "An iterative and collaborative process involving researchers and LLMs to develop, test, and apply taxonomies for text analysis, including writing prompts, evaluating and refining taxonomies, assessing intercoder agreements, and categorizing datasets.", "result": "Demonstrated a successful application of LLMs in generating a taxonomy of life domains from personal goals, achieving high intercoder reliability.", "conclusion": "LLMs offer promising possibilities for text analysis with predefined or data-driven taxonomies, though there are limitations that need to be considered."}}
{"id": "2505.09737", "pdf": "https://arxiv.org/pdf/2505.09737", "abs": "https://arxiv.org/abs/2505.09737", "authors": ["Osher Elhadad", "Reuth Mirsky"], "title": "General Dynamic Goal Recognition", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted for publication at Generalization in Planning (GenPlan) as\n  part of AAAI 2025 workshops", "summary": "Understanding an agent's intent through its behavior is essential in\nhuman-robot interaction, interactive AI systems, and multi-agent\ncollaborations. This task, known as Goal Recognition (GR), poses significant\nchallenges in dynamic environments where goals are numerous and constantly\nevolving. Traditional GR methods, designed for a predefined set of goals, often\nstruggle to adapt to these dynamic scenarios. To address this limitation, we\nintroduce the General Dynamic GR problem - a broader definition of GR - aimed\nat enabling real-time GR systems and fostering further research in this area.\nExpanding on this foundation, this paper employs a model-free goal-conditioned\nRL approach to enable fast adaptation for GR across various changing tasks.", "AI": {"tldr": "The paper introduces the General Dynamic GR problem and uses a model-free goal-conditioned RL approach for real-time Goal Recognition in dynamic environments.", "motivation": "Traditional Goal Recognition methods are not suitable for dynamic environments with numerous and constantly evolving goals.", "method": "Employing a model-free goal-conditioned Reinforcement Learning approach to facilitate rapid adaptation for Goal Recognition in diverse, changing tasks.", "result": "Enables real-time Goal Recognition systems suitable for dynamic scenarios.", "conclusion": "Introduces a broader definition of Goal Recognition and promotes further research in this area."}}
{"id": "2505.09702", "pdf": "https://arxiv.org/pdf/2505.09702", "abs": "https://arxiv.org/abs/2505.09702", "authors": ["Yezi Liu", "Prathyush Poduval", "Wenjun Huang", "Yang Ni", "Hanning Chen", "Mohsen Imani"], "title": "Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing", "categories": ["cs.LG"], "comment": null, "summary": "Graph unlearning is a crucial approach for protecting user privacy by erasing\nthe influence of user data on trained graph models. Recent developments in\ngraph unlearning methods have primarily focused on maintaining model prediction\nperformance while removing user information. However, we have observed that\nwhen user information is deleted from the model, the prediction distribution\nacross different sensitive groups often changes. Furthermore, graph models are\nshown to be prone to amplifying biases, making the study of fairness in graph\nunlearning particularly important. This raises the question: Does graph\nunlearning actually introduce bias? Our findings indicate that the predictions\nof post-unlearning models become highly correlated with sensitive attributes,\nconfirming the introduction of bias in the graph unlearning process. To address\nthis issue, we propose a fair graph unlearning method, FGU. To guarantee\nprivacy, FGU trains shard models on partitioned subgraphs, unlearns the\nrequested data from the corresponding subgraphs, and retrains the shard models\non the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing\nprocess: it first enables shard-level fairness by incorporating a fairness\nregularizer in the shard model retraining, and then achieves global-level\nfairness by aligning all shard models to minimize global disparity. Our\nexperiments demonstrate that FGU achieves superior fairness while maintaining\nprivacy and accuracy. Additionally, FGU is robust to diverse unlearning\nrequests, ensuring fairness and utility performance across various data\ndistributions.", "AI": {"tldr": "Graph unlearning is essential for user privacy protection. However, it may introduce bias when deleting user information. This paper proposes FGU, a fair graph unlearning method that ensures fairness and maintains privacy and accuracy.", "motivation": "To address the issue of bias introduction in the process of graph unlearning while protecting user privacy.", "method": "FGU trains shard models on partitioned subgraphs, unlearns the requested data from the corresponding subgraphs, and retrains the shard models on the modified subgraphs. It employs a bi-level debiasing process to ensure fairness at both shard-level and global-level.", "result": "Experiments show that FGU achieves superior fairness while maintaining privacy and accuracy. It is also robust to diverse unlearning requests.", "conclusion": "FGU is an effective method for fair graph unlearning that can protect user privacy, maintain model performance, and reduce bias."}}
{"id": "2505.09829", "pdf": "https://arxiv.org/pdf/2505.09829", "abs": "https://arxiv.org/abs/2505.09829", "authors": ["Tushar Kataria", "Shireen Y. Elhabian"], "title": "BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes", "categories": ["cs.CV"], "comment": null, "summary": "Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.", "AI": {"tldr": "An effective and efficient medical image segmentation approach, BoundarySeg, is proposed to improve segmentation accuracy without unannotated data or extra computation.", "motivation": "Obtaining large-scale medical data is challenging due to privacy regulations and data protection policies. Moreover, annotating medical images is time-consuming and costly as it requires domain experts to manually delineate anatomical structures. Semi-supervised methods have gained popularity for reducing annotation costs, but they heavily depend on the availability of unannotated data.", "method": "BoundarySeg, a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation, leveraging consistency between the two task predictions to provide additional supervision.", "result": "This strategy improves segmentation accuracy, especially in low data regimes, achieving performance comparable to or exceeding state-of-the-art semi supervised approaches.", "conclusion": "BoundarySeg can achieve better performance without relying on unannotated data or increasing computational demands."}}
{"id": "2505.09738", "pdf": "https://arxiv.org/pdf/2505.09738", "abs": "https://arxiv.org/abs/2505.09738", "authors": ["Shaurya Sharthak", "Vinayak Pahalwan", "Adithya Kamath", "Adarsh Shirawalmath"], "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.", "AI": {"tldr": "Pretrained language models (LLMs) suffer from fixed tokenization schemes which limit their efficiency and performance, especially in multilingual or specialized scenarios. To address this issue, the authors propose TokenAdapt, a model-agnostic tokenizer transplantation method, and novel pre-tokenization learning for multi-word Supertokens. TokenAdapt uses a hybrid heuristic to initialize new unique token embeddings by combining subword decomposition with semantic similarity estimates. This approach preserves semantics while reducing retraining needs. Empirical results show that TokenAdapt outperforms existing methods like Retok and Transtokenizer, achieving significant improvements in zero-shot perplexity scores.", "motivation": "The motivation behind this paper is to overcome the limitations imposed by fixed tokenization schemes in pretrained language models. Current methods to adapt tokenizers are either computationally expensive or fail to fully preserve semantic nuances and address compression inefficiencies.", "method": "The paper introduces two innovations: 1) TokenAdapt - a model-agnostic tokenizer transplantation method that initializes new token embeddings using a hybrid heuristic combining subword decomposition and semantic similarity; 2) Pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation.", "result": "Empirical investigations demonstrate that TokenAdapt successfully initializes unique tokens, outperforming conventional baselines and sophisticated methods such as Transtokenizer and ReTok. The Supertokens achieve notable compression gains. Additionally, TokenAdapt shows consistent lower perplexity ratios compared to ReTok and Transtokenizer across different base models and newly trained target tokenizers, with at least a 2-fold improvement in aggregate scores.", "conclusion": "TokenAdapt offers an effective solution to the tokenizer lock-in problem in pretrained language models by providing a model-agnostic tokenizer transplantation method that preserves semantics and reduces retraining requirements. It outperforms existing methods in terms of zero-shot perplexity and provides significant improvements in compression and performance."}}
{"id": "2505.09755", "pdf": "https://arxiv.org/pdf/2505.09755", "abs": "https://arxiv.org/abs/2505.09755", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "title": "Explainability Through Human-Centric Design for XAI in Lung Cancer Detection", "categories": ["cs.AI"], "comment": null, "summary": "Deep learning models have shown promise in lung pathology detection from\nchest X-rays, but widespread clinical adoption remains limited due to opaque\nmodel decision-making. In prior work, we introduced ClinicXAI, a human-centric,\nexpert-guided concept bottleneck model (CBM) designed for interpretable lung\ncancer diagnosis. We now extend that approach and present XpertXAI, a\ngeneralizable expert-driven model that preserves human-interpretable clinical\nconcepts while scaling to detect multiple lung pathologies. Using a\nhigh-performing InceptionV3-based classifier and a public dataset of chest\nX-rays with radiology reports, we compare XpertXAI against leading post-hoc\nexplainability methods and an unsupervised CBM, XCBs. We assess explanations\nthrough comparison with expert radiologist annotations and medical ground\ntruth. Although XpertXAI is trained for multiple pathologies, our expert\nvalidation focuses on lung cancer. We find that existing techniques frequently\nfail to produce clinically meaningful explanations, omitting key diagnostic\nfeatures and disagreeing with radiologist judgments. XpertXAI not only\noutperforms these baselines in predictive accuracy but also delivers\nconcept-level explanations that better align with expert reasoning. While our\nfocus remains on explainability in lung cancer detection, this work illustrates\nhow human-centric model design can be effectively extended to broader\ndiagnostic contexts - offering a scalable path toward clinically meaningful\nexplainable AI in medical diagnostics.", "AI": {"tldr": "XpertXAI is an expert-driven model that extends the ClinicXAI approach, focusing on interpretable multi-pathology detection from chest X-rays. It outperforms other methods in predictive accuracy and provides concept-level explanations aligning with expert reasoning in lung cancer detection.", "motivation": "Despite the potential of deep learning models for detecting lung pathologies, their lack of transparency limits clinical adoption. The authors aim to address this by creating a generalizable, human-interpretable model that can scale to multiple lung pathologies while maintaining interpretability.", "method": "The authors developed XpertXAI, which builds on their previous work with ClinicXAI. It uses a high-performing InceptionV3-based classifier and compares against post-hoc explainability methods and unsupervised CBMs using a public dataset of chest X-rays with radiology reports.", "result": "XpertXAI surpasses existing techniques in both predictive accuracy and providing clinically meaningful explanations that align closely with expert radiologist annotations and judgments.", "conclusion": "XpertXAI demonstrates the effectiveness of human-centric model design in extending interpretability to broader diagnostic contexts, offering a scalable solution for clinically meaningful explainable AI in medical diagnostics."}}
{"id": "2505.09704", "pdf": "https://arxiv.org/pdf/2505.09704", "abs": "https://arxiv.org/abs/2505.09704", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Charalampos Kalalas", "Paolo Dini"], "title": "Energy-Efficient Federated Learning for AIoT using Clustering Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While substantial research has been devoted to optimizing model performance,\nconvergence rates, and communication efficiency, the energy implications of\nfederated learning (FL) within Artificial Intelligence of Things (AIoT)\nscenarios are often overlooked in the existing literature. This study examines\nthe energy consumed during the FL process, focusing on three main\nenergy-intensive processes: pre-processing, communication, and local learning,\nall contributing to the overall energy footprint. We rely on the observation\nthat device/client selection is crucial for speeding up the convergence of\nmodel training in a distributed AIoT setting and propose two\nclustering-informed methods. These clustering solutions are designed to group\nAIoT devices with similar label distributions, resulting in clusters composed\nof nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity\noften encountered in real-world distributed learning applications. Throughout\nextensive numerical experimentation, we demonstrate that our clustering\nstrategies typically achieve high convergence rates while maintaining low\nenergy consumption when compared to other recent approaches available in the\nliterature.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u5728AIoT\u573a\u666f\u4e2d\u7684\u80fd\u8017\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u8bbe\u5907\u9009\u62e9\u65b9\u6cd5\u4ee5\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u7684\u6536\u655b\u901f\u5ea6\u548c\u80fd\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u591a\u5173\u6ce8\u6a21\u578b\u6027\u80fd\u3001\u6536\u655b\u901f\u5ea6\u548c\u901a\u4fe1\u6548\u7387\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u5728AIoT\u573a\u666f\u4e2d\u7684\u80fd\u8017\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u53d1\u73b0\u8bbe\u5907/\u5ba2\u6237\u7aef\u9009\u62e9\u5bf9\u5206\u5e03\u5f0fAIoT\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u8fdb\u800c\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5\uff0c\u5c06\u5177\u6709\u76f8\u4f3c\u6807\u7b7e\u5206\u5e03\u7684AIoT\u8bbe\u5907\u5206\u7ec4\uff0c\u5f62\u6210\u51e0\u4e4e\u5f02\u6784\u7684\u8bbe\u5907\u96c6\u7fa4\uff0c\u4ece\u800c\u7f13\u89e3\u5b9e\u9645\u5206\u5e03\u5f0f\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u5f02\u6784\u6027\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u805a\u7c7b\u7b56\u7565\u901a\u5e38\u80fd\u591f\u5728\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6536\u655b\u901f\u5ea6\uff0c\u4f18\u4e8e\u5176\u4ed6\u8fd1\u671f\u6587\u732e\u4e2d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728AIoT\u573a\u666f\u4e2d\uff0c\u901a\u8fc7\u5408\u7406\u7684\u8bbe\u5907\u9009\u62e9\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u8017\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2505.09858", "pdf": "https://arxiv.org/pdf/2505.09858", "abs": "https://arxiv.org/abs/2505.09858", "authors": ["Danush Kumar Venkatesh", "Isabel Funke", "Micha Pfeiffer", "Fiona Kolbinger", "Hanna Maria Schmeiser", "Juergen Weitz", "Marius Distler", "Stefanie Speidel"], "title": "Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models", "categories": ["cs.CV"], "comment": "Early accept at MICCAI 2025", "summary": "Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.", "AI": {"tldr": "The paper proposes a two-stage, text-conditioned diffusion-based method to synthesize surgical videos for under-represented classes in surgical video datasets, aiming to overcome data imbalance. The approach uses a 2D latent diffusion model and temporal attention layers to ensure spatial and temporal consistency respectively. A rejection sampling strategy is introduced to select the most suitable synthetic samples. The method enhances model performance on surgical action recognition and intra-operative event prediction.", "motivation": "To address the issue of severe data imbalance in surgical video datasets that hinders the development of high-performing models.", "method": "A unique two-stage, text-conditioned diffusion-based method is proposed. This method generates high-fidelity surgical videos by conditioning on text prompts and decoupling spatial and temporal modeling using a 2D latent diffusion model and temporal attention layers. Additionally, a rejection sampling strategy is used to select the most suitable synthetic samples.", "result": "Incorporating synthetic videos from the proposed approach significantly improves model performance on downstream tasks such as surgical action recognition and intra-operative event prediction.", "conclusion": "The authors successfully demonstrate that their method effectively addresses class imbalance in surgical video datasets and improves model performance on relevant tasks."}}
{"id": "2505.09794", "pdf": "https://arxiv.org/pdf/2505.09794", "abs": "https://arxiv.org/abs/2505.09794", "authors": ["J. Moreno-Casanova", "J. M. Au\u00f1\u00f3n", "A. M\u00e1rtinez-P\u00e9rez", "M. E. P\u00e9rez-Mart\u00ednez", "M. E. Gas-L\u00f3pez"], "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.", "AI": {"tldr": "The paper explores the application of NLP techniques for automating data extraction from EHRs related to lung and breast cancer, using GMV's uQuery tool and fine-tuning a RoBERTa-based model. It reports strong performance in identifying certain clinical entities.", "motivation": "Manual extraction of information from clinical reports is time-consuming and error-prone, limiting the efficiency of data-driven approaches in healthcare.", "method": "Utilized GMV's NLP tool uQuery and fine-tuned the bsc-bio-ehr-en3 model (RoBERTa-based) for Named Entity Recognition tasks on a dataset of annotated breast and lung cancer reports.", "result": "Demonstrated strong overall performance in entity recognition, especially for common entities like MET and PAT, with challenges remaining for less frequent entities like EVOL.", "conclusion": "NLP techniques can significantly enhance the accuracy and efficiency of data extraction from EHRs in oncology, contributing to better patient outcomes."}}
{"id": "2505.09787", "pdf": "https://arxiv.org/pdf/2505.09787", "abs": "https://arxiv.org/abs/2505.09787", "authors": ["Ziruo Yi", "Ting Xiao", "Mark V. Albert"], "title": "A Multimodal Multi-Agent Framework for Radiology Report Generation", "categories": ["cs.AI"], "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce diagnostic\nreports from medical images, with the potential to enhance clinical workflows\nand reduce radiologists' workload. While recent approaches leveraging\nmultimodal large language models (MLLMs) and retrieval-augmented generation\n(RAG) have achieved strong results, they continue to face challenges such as\nfactual inconsistency, hallucination, and cross-modal misalignment. We propose\na multimodal multi-agent framework for RRG that aligns with the stepwise\nclinical reasoning workflow, where task-specific agents handle retrieval, draft\ngeneration, visual analysis, refinement, and synthesis. Experimental results\ndemonstrate that our approach outperforms a strong baseline in both automatic\nmetrics and LLM-based evaluations, producing more accurate, structured, and\ninterpretable reports. This work highlights the potential of clinically aligned\nmulti-agent frameworks to support explainable and trustworthy clinical AI\napplications.", "AI": {"tldr": "The paper introduces a multimodal multi-agent framework for radiology report generation (RRG) that surpasses current methods by producing more accurate, structured, and interpretable reports through task-specific agents.", "motivation": "Radiology report generation has challenges like factual inconsistency, hallucination, and cross-modal misalignment which need to be addressed to improve clinical workflows and reduce workload.", "method": "A multimodal multi-agent framework is proposed where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis aligning with the stepwise clinical reasoning workflow.", "result": "Experimental results show that this approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations.", "conclusion": "This work emphasizes the potential of clinically aligned multi-agent frameworks in supporting explainable and trustworthy clinical AI applications."}}
{"id": "2505.09710", "pdf": "https://arxiv.org/pdf/2505.09710", "abs": "https://arxiv.org/abs/2505.09710", "authors": ["Konstantinos Fotopoulos", "Petros Maragos"], "title": "Training Deep Morphological Neural Networks as Universal Approximators", "categories": ["cs.LG"], "comment": null, "summary": "We investigate deep morphological neural networks (DMNNs). We demonstrate\nthat despite their inherent non-linearity, activations between layers are\nessential for DMNNs. We then propose several new architectures for DMNNs, each\nwith a different constraint on their parameters. For the first (resp. second)\narchitecture, we work under the constraint that the majority of parameters\n(resp. learnable parameters) should be part of morphological operations. We\nempirically show that our proposed networks can be successfully trained, and\nare more prunable than linear networks. To the best of our knowledge, we are\nthe first to successfully train DMNNs under such constraints, although the\ngeneralization capabilities of our networks remain limited. Finally, we propose\na hybrid network architecture combining linear and morphological layers,\nshowing empirically that the inclusion of morphological layers significantly\naccelerates the convergence of gradient descent with large batches.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\uff0c\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u67b6\u6784\u5e76\u5728\u53c2\u6570\u4e0a\u65bd\u52a0\u4e0d\u540c\u7ea6\u675f\uff0c\u8bc1\u660e\u6240\u63d0\u7f51\u7edc\u53ef\u6210\u529f\u8bad\u7ec3\u4e14\u6bd4\u7ebf\u6027\u7f51\u7edc\u66f4\u6613\u526a\u679d\uff0c\u8fd8\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u548c\u5f62\u6001\u5c42\u7684\u6df7\u5408\u67b6\u6784\u4ee5\u52a0\u901f\u5927\u6279\u6b21\u68af\u5ea6\u4e0b\u964d\u6536\u655b\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\u7684\u5de5\u4f5c\u673a\u5236\u53ca\u5176\u4f18\u5316\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u6fc0\u6d3b\u51fd\u6570\u5728\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u5c42\u95f4\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u591a\u79cdDMNNs\u67b6\u6784\u5e76\u5206\u522b\u5728\u53c2\u6570\u548c\u53ef\u5b66\u4e60\u53c2\u6570\u4e0a\u8bbe\u7f6e\u4e0d\u540c\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u5176\u4e2d\u8981\u6c42\u5927\u90e8\u5206\u53c2\u6570\u53c2\u4e0e\u5f62\u6001\u5b66\u64cd\u4f5c\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e00\u79cd\u6df7\u5408\u7f51\u7edc\u67b6\u6784\u5c06\u7ebf\u6027\u548c\u5f62\u6001\u5c42\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u80fd\u591f\u6210\u529f\u8bad\u7ec3\uff0c\u6bd4\u7ebf\u6027\u7f51\u7edc\u66f4\u5bb9\u6613\u526a\u679d\uff0c\u4e14\u5f62\u6001\u5c42\u7684\u52a0\u5165\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u5927\u6279\u6b21\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u901f\u5ea6\u3002\u4f46\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002", "conclusion": "\u9996\u6b21\u6210\u529f\u8bad\u7ec3\u4e86\u53d7\u7279\u5b9a\u7ea6\u675f\u7684DMNNs\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u53ef\u8bad\u7ec3\u6027\u548c\u6613\u4e8e\u526a\u679d\u7684\u7279\u70b9\uff0c\u540c\u65f6\u4e5f\u5c55\u793a\u4e86\u5f62\u6001\u5c42\u5bf9\u52a0\u901f\u6536\u655b\u7684\u6548\u679c\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2505.09859", "pdf": "https://arxiv.org/pdf/2505.09859", "abs": "https://arxiv.org/abs/2505.09859", "authors": ["Andrew Jun Lee", "Taylor Webb", "Trevor Bihl", "Keith Holyoak", "Hongjing Lu"], "title": "Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction", "categories": ["cs.CV"], "comment": "Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society", "summary": "The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.", "AI": {"tldr": "This paper presents Probabilistic Schema Induction (PSI), a model that uses deep learning to perform analogical mapping over structured representations of visual concepts from limited examples, forming schemas. PSI outperforms controls and mimics human-like learning.", "motivation": "To understand and replicate the human ability to learn new visual concepts from limited examples using structured representations and analogical mapping.", "method": "Introduced PSI, a prototype model employing deep learning for analogical mapping over structured representations to form compositional concepts called schemas, with mechanisms weighing similarity and amplifying relevant relations.", "result": "PSI produces human-like learning performance, outperforming models using unstructured feature vectors or weaker structured representations. Its success is attributed to increasing relational similarity and emphasizing distinguishing relations.", "conclusion": "Structured representations and analogical mapping are crucial for modeling rapid human-like learning of compositional visual concepts, demonstrating how deep learning can be used in psychological models."}}
{"id": "2505.09807", "pdf": "https://arxiv.org/pdf/2505.09807", "abs": "https://arxiv.org/abs/2505.09807", "authors": ["Timour Ichmoukhamedov", "David Martens"], "title": "Exploring the generalization of LLM truth directions on conversational formats", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.", "AI": {"tldr": "\u5c3d\u7ba1\u7ebf\u6027\u63a2\u9488\u5728\u68c0\u6d4b\u77ed\u5bf9\u8bdd\u4e2d\u7684\u8c0e\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u5904\u7406\u66f4\u957f\u683c\u5f0f\u7684\u5bf9\u8bdd\u65f6\u6548\u679c\u8f83\u5dee\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u6bcf\u6bb5\u5bf9\u8bdd\u672b\u5c3e\u6dfb\u52a0\u56fa\u5b9a\u5173\u952e\u77ed\u8bed\u6765\u663e\u8457\u6539\u5584\u8fd9\u79cd\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u666e\u904d\u5b58\u5728\u7684\u771f\u7406\u65b9\u5411\uff0c\u63a2\u8ba8\u7ebf\u6027\u63a2\u9488\u662f\u5426\u80fd\u591f\u5728\u7ebf\u6027\u6fc0\u6d3b\u7a7a\u95f4\u5185\u533a\u5206\u771f\u5b9e\u548c\u865a\u5047\u9648\u8ff0\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u4e0d\u540c\u5bf9\u8bdd\u5f62\u5f0f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5206\u6790\u7ebf\u6027\u63a2\u9488\u5728\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f\uff08\u5c24\u5176\u662f\u5305\u542b\u8c0e\u8a00\u7684\u5bf9\u8bdd\uff09\u4e2d\u7684\u8868\u73b0\uff1b\u63d0\u51fa\u5728\u6bcf\u6bb5\u5bf9\u8bdd\u7684\u672b\u5c3e\u6dfb\u52a0\u56fa\u5b9a\u5173\u952e\u77ed\u8bed\u4ee5\u6539\u5584\u6cdb\u5316\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u7ebf\u6027\u63a2\u9488\u5728\u77ed\u5bf9\u8bdd\u4e2d\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u8f83\u957f\u5bf9\u8bdd\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1b\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u957f\u5bf9\u8bdd\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u867d\u7136\u5728\u77ed\u5bf9\u8bdd\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u8f83\u597d\u7684\u8c0e\u8a00\u68c0\u6d4b\uff0c\u4f46\u8981\u6784\u5efa\u53ef\u9760\u7684\u3001\u80fd\u9002\u5e94\u65b0\u73af\u5883\u7684LLM\u8c0e\u8a00\u68c0\u6d4b\u5668\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2505.09920", "pdf": "https://arxiv.org/pdf/2505.09920", "abs": "https://arxiv.org/abs/2505.09920", "authors": ["Shan Yang", "Yongli Zhu"], "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "This paper presents a study on using different offline reinforcement learning\nalgorithms for microgrid voltage regulation with solar power penetration. When\nenvironment interaction is unviable due to technical or safety reasons, the\nproposed approach can still obtain an applicable model through offline-style\ntraining on a previously collected dataset, lowering the negative impact of\nlacking online environment interactions. Experiment results on the IEEE 33-bus\nsystem demonstrate the feasibility and effectiveness of the proposed approach\non different offline datasets, including the one with merely low-quality\nexperience.", "AI": {"tldr": "This paper explores the use of various offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration, demonstrating its feasibility and effectiveness on different offline datasets including low-quality experience.", "motivation": "To address the challenge of microgrid voltage regulation in environments where online interaction is not feasible due to technical or safety reasons.", "method": "The study employs different offline reinforcement learning algorithms trained on previously collected datasets to create a model for microgrid voltage regulation with solar power penetration.", "result": "Experiment results on the IEEE 33-bus system show that the proposed approach is both feasible and effective across various offline datasets, even those with low-quality experience.", "conclusion": "Offline reinforcement learning algorithms can successfully be used for microgrid voltage regulation when online environment interactions are not possible."}}
{"id": "2505.09716", "pdf": "https://arxiv.org/pdf/2505.09716", "abs": "https://arxiv.org/abs/2505.09716", "authors": ["George Dimitriadis. Spyridon Samothrakis"], "title": "Out-of-distribution generalisation is hard: evidence from ARC-like tasks", "categories": ["cs.LG", "cs.AI"], "comment": "Submission to NeurIPS 2025", "summary": "Out-of-distribution (OOD) generalisation is considered a hallmark of human\nand animal intelligence. To achieve OOD through composition, a system must\ndiscover the environment-invariant properties of experienced input-output\nmappings and transfer them to novel inputs. This can be realised if an\nintelligent system can identify appropriate, task-invariant, and composable\ninput features, as well as the composition methods, thus allowing it to act\nbased not on the interpolation between learnt data points but on the\ntask-invariant composition of those features. We propose that in order to\nconfirm that an algorithm does indeed learn compositional structures from data,\nit is not enough to just test on an OOD setup, but one also needs to confirm\nthat the features identified are indeed compositional. We showcase this by\nexploring two tasks with clearly defined OOD metrics that are not OOD solvable\nby three commonly used neural networks: a Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and a Transformer. In addition, we develop\ntwo novel network architectures imbued with biases that allow them to be\nsuccessful in OOD scenarios. We show that even with correct biases and almost\nperfect OOD performance, an algorithm can still fail to learn the correct\nfeatures for compositional generalisation.", "AI": {"tldr": "\u4e3a\u4e86\u5b9e\u73b0OOD\uff0c\u667a\u80fd\u7cfb\u7edf\u9700\u8981\u8bc6\u522b\u9002\u5f53\u7684\u4efb\u52a1\u4e0d\u53d8\u548c\u53ef\u7ec4\u5408\u7684\u8f93\u5165\u7279\u5f81\u53ca\u7ec4\u5408\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u51fa\u4ec5\u4ec5\u6d4b\u8bd5OOD\u8bbe\u7f6e\u4e0d\u8db3\u4ee5\u786e\u8ba4\u7b97\u6cd5\u786e\u5b9e\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u4e86\u7ec4\u5408\u7ed3\u6784\uff0c\u8fd8\u9700\u8981\u786e\u8ba4\u8bc6\u522b\u7684\u7279\u5f81\u786e\u5b9e\u662f\u7ec4\u5408\u6027\u7684\u3002\u901a\u8fc7\u4e24\u4e2a\u4efb\u52a1\u5c55\u793a\u4e86\u4e09\u4e2a\u5e38\u7528\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u89e3\u51b3OOD\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u578b\u7f51\u7edc\u67b6\u6784\u4ee5\u5728OOD\u573a\u666f\u4e2d\u53d6\u5f97\u6210\u529f\u3002\u5373\u4f7f\u6709\u6b63\u786e\u7684\u504f\u5dee\u548c\u63a5\u8fd1\u5b8c\u7f8e\u7684OOD\u6027\u80fd\uff0c\u7b97\u6cd5\u4ecd\u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60\u5230\u6b63\u786e\u7684\u7ec4\u5408\u6cdb\u5316\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406OOD\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u771f\u6b63\u7684\u7ec4\u5408\u6cdb\u5316\u3002", "method": "\u8bbe\u8ba1\u4e24\u4e2a\u5177\u6709\u660e\u786eOOD\u5ea6\u91cf\u7684\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e09\u79cd\u5e38\u89c1\u795e\u7ecf\u7f51\u7edc\uff08MLP\u3001CNN\u3001Transformer\uff09\u5728OOD\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff1b\u5f00\u53d1\u4e24\u79cd\u65b0\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u5f15\u5165\u504f\u5dee\u4f7f\u5176\u5728OOD\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff1b\u5206\u6790\u7b97\u6cd5\u662f\u5426\u80fd\u591f\u5b66\u4e60\u5230\u6b63\u786e\u7684\u7ec4\u5408\u7279\u5f81\u3002", "result": "\u4e09\u79cd\u5e38\u89c1\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u89e3\u51b3\u7279\u5b9aOOD\u4efb\u52a1\uff1b\u4e24\u79cd\u65b0\u578b\u7f51\u7edc\u67b6\u6784\u5728OOD\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u53ef\u80fd\u5b58\u5728\u672a\u80fd\u5b66\u4e60\u5230\u6b63\u786e\u7ec4\u5408\u7279\u5f81\u7684\u60c5\u51b5\u3002", "conclusion": "\u4ec5\u4ec5\u6d4b\u8bd5OOD\u6027\u80fd\u4e0d\u8db3\u4ee5\u9a8c\u8bc1\u7b97\u6cd5\u662f\u5426\u771f\u6b63\u5b66\u4e60\u4e86\u7ec4\u5408\u7ed3\u6784\uff0c\u8fd8\u9700\u786e\u8ba4\u5176\u8bc6\u522b\u7684\u7279\u5f81\u662f\u5426\u5177\u6709\u7ec4\u5408\u6027\u3002"}}
{"id": "2505.09915", "pdf": "https://arxiv.org/pdf/2505.09915", "abs": "https://arxiv.org/abs/2505.09915", "authors": ["Zhe Xin", "Chenyang Wu", "Penghui Huang", "Yanyong Zhang", "Yinian Mao", "Guoquan Huang"], "title": "Large-Scale Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.", "AI": {"tldr": "LSG-SLAM is a large-scale 3DGS-based visual SLAM method with stereo cameras that introduces multi-modality pose estimation, feature-alignment warping constraints, continuous Gaussian Splatting submaps, and a structure refinement module for superior performance in large-scale outdoor scenarios.", "motivation": "Current methods using Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) for visual SLAM require RGBD sensors and are mostly effective in indoor environments. The robustness of these methods in large-scale outdoor scenarios has not been adequately explored.", "method": "The proposed LSG-SLAM uses a multi-modality strategy to estimate prior poses under large view changes, feature-alignment warping constraints to mitigate appearance similarity issues in rendering losses, continuous Gaussian Splatting submaps for scalability in large-scale scenarios, place recognition for loop detection between GS submaps, and a structure refinement module to enhance reconstruction quality after global optimization of camera poses and Gaussian points.", "result": "LSG-SLAM demonstrates superior performance over existing Neural, 3DGS-based, and traditional approaches through extensive evaluations on the EuRoc and KITTI datasets.", "conclusion": "LSG-SLAM effectively addresses the challenges of large-scale outdoor visual SLAM by integrating advanced techniques such as multi-modality pose estimation, feature alignment, scalable submaps, and structure refinement."}}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825", "abs": "https://arxiv.org/abs/2505.09825", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.", "AI": {"tldr": "\u5c3d\u7ba1\u5bc6\u5207\u9605\u8bfb\u662f\u5927\u5b66\u8bfe\u7a0b\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5c1a\u672a\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u51fa\u4e86KRISTEVA\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5305\u542b1331\u4e2a\u591a\u9879\u9009\u62e9\u9898\u7684\u5bc6\u5207\u9605\u8bfb\u57fa\u51c6\u6d4b\u8bd5\u3002\u901a\u8fc7\u4e09\u4e2a\u9010\u6b65\u590d\u6742\u7684\u4efb\u52a1\u96c6\u6765\u8bc4\u4f30LLM\u5bf9\u6587\u5b66\u4f5c\u54c1\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u6700\u5148\u8fdb\u7684LLM\u867d\u7136\u5177\u5907\u4e00\u5b9a\u7a0b\u5ea6\u7684\u5927\u5b66\u6c34\u5e73\u5bc6\u5207\u9605\u8bfb\u80fd\u529b\uff08\u51c6\u786e\u738749.7%-69.7%\uff09\uff0c\u4f46\u572811\u9879\u4efb\u52a1\u4e2d\u768410\u9879\u4e0a\u4ecd\u4e0d\u53ca\u6709\u7ecf\u9a8c\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "motivation": "\u5bc6\u5207\u9605\u8bfb\u4f5c\u4e3a\u57f9\u517b\u6279\u5224\u6027\u601d\u7ef4\u7684\u57fa\u7840\uff0c\u5728\u5927\u5b66\u8bfe\u7a0b\u4e2d\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u7136\u800c\u5c1a\u672a\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e14\u591a\u5b66\u79d1\u57fa\u51c6\u5982MMLU\u672a\u6db5\u76d6\u6587\u5b66\u9886\u57df\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLMs\u5728\u6587\u5b66\u6587\u672c\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faKRISTEVA\u2014\u2014\u4e00\u4e2a\u75311331\u9053\u8bfe\u5802\u6570\u636e\u6539\u7f16\u7684\u591a\u9879\u9009\u62e9\u9898\u7ec4\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u9010\u6b65\u590d\u6742\u7684\u4efb\u52a1\u96c6\uff1a1) \u63d0\u53d6\u98ce\u683c\u7279\u5f81\uff1b2) \u4ece\u53c2\u6570\u5316\u77e5\u8bc6\u4e2d\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b3) \u5728\u98ce\u683c\u4e0e\u5916\u90e8\u4e0a\u4e0b\u6587\u4e4b\u95f4\u8fdb\u884c\u591a\u8df3\u63a8\u7406\u3002\u4ee5\u6b64\u6765\u6d4b\u8bd5LLMs\u5728\u5bc6\u5207\u9605\u8bfb\u8fc7\u7a0b\u4e0d\u540c\u5143\u7d20\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLMs\u5728\u5bc6\u5207\u9605\u8bfb\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u5927\u5b66\u6c34\u5e73\u80fd\u529b\uff0c\u51c6\u786e\u7387\u8303\u56f4\u4e3a49.7%-69.7%\uff0c\u4f46\u572811\u9879\u4efb\u52a1\u4e2d\u768410\u9879\u4e0a\uff0c\u5176\u8868\u73b0\u4ecd\u900a\u8272\u4e8e\u6709\u7ecf\u9a8c\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "conclusion": "\u5f53\u524d\u6700\u5148\u8fdb\u7684LLMs\u5728\u6587\u5b66\u4f5c\u54c1\u7684\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u5df2\u5c55\u73b0\u51fa\u4e00\u5b9a\u80fd\u529b\uff0c\u4f46\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u590d\u6742\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u3002\u8fd9\u8868\u660e\u5728\u63d0\u5347LLMs\u7684\u5bc6\u5207\u9605\u8bfb\u80fd\u529b\u65b9\u9762\u8fd8\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2505.09923", "pdf": "https://arxiv.org/pdf/2505.09923", "abs": "https://arxiv.org/abs/2505.09923", "authors": ["Minjung Shin", "Donghyun Kim", "Jeh-Kwang Ryu"], "title": "\"There Is No Such Thing as a Dumb Question,\" But There Are Good Ones", "categories": ["cs.AI"], "comment": "8 pages, 4 figures and 4 tables. This work has been accepted for\n  presentation as a poster with full paper publication at CogSci 2025. This is\n  the final submission", "summary": "Questioning has become increasingly crucial for both humans and artificial\nintelligence, yet there remains limited research comprehensively assessing\nquestion quality. In response, this study defines good questions and presents a\nsystematic evaluation framework. We propose two key evaluation dimensions:\nappropriateness (sociolinguistic competence in context) and effectiveness\n(strategic competence in goal achievement). Based on these foundational\ndimensions, a rubric-based scoring system was developed. By incorporating\ndynamic contextual variables, our evaluation framework achieves structure and\nflexibility through semi-adaptive criteria. The methodology was validated using\nthe CAUS and SQUARE datasets, demonstrating the ability of the framework to\naccess both well-formed and problematic questions while adapting to varied\ncontexts. As we establish a flexible and comprehensive framework for question\nevaluation, this study takes a significant step toward integrating questioning\nbehavior with structured analytical methods grounded in the intrinsic nature of\nquestioning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5b9a\u4e49\u4e86\u597d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u914d\u6027\u4e0e\u6709\u6548\u6027\u4e24\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4ef7\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u8bc4\u5206\u8868\u7684\u7cfb\u7edf\uff0c\u5e76\u5728\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u76ee\u524d\u5bf9\u4e8e\u95ee\u9898\u8d28\u91cf\u7684\u5168\u9762\u8bc4\u4f30\u7814\u7a76\u6709\u9650\uff0c\u800c\u63d0\u95ee\u5bf9\u4e8e\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u90fd\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u9002\u914d\u6027\u548c\u6709\u6548\u6027\u4f5c\u4e3a\u4e24\u4e2a\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6784\u5efa\u57fa\u4e8e\u8bc4\u5206\u8868\u7684\u6253\u5206\u7cfb\u7edf\uff0c\u7ed3\u5408\u52a8\u6001\u4e0a\u4e0b\u6587\u53d8\u91cf\uff0c\u5f62\u6210\u534a\u81ea\u9002\u5e94\u6807\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u4f7f\u7528CAUS\u548cSQUARE\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u7ed3\u6784\u826f\u597d\u548c\u5b58\u5728\u95ee\u9898\u7684\u95ee\u9898\uff0c\u540c\u65f6\u9002\u5e94\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u60c5\u5883\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u95ee\u9898\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u5168\u9762\u7684\u6846\u67b6\uff0c\u5411\u5c06\u63d0\u95ee\u884c\u4e3a\u4e0e\u57fa\u4e8e\u95ee\u9898\u672c\u8d28\u7684\u7ed3\u6784\u5316\u5206\u6790\u65b9\u6cd5\u6574\u5408\u8fc8\u51fa\u4e86\u91cd\u8981\u7684\u4e00\u6b65\u3002"}}
{"id": "2505.09733", "pdf": "https://arxiv.org/pdf/2505.09733", "abs": "https://arxiv.org/abs/2505.09733", "authors": ["Alpaslan Gokcen", "Ali Boyaci"], "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios.", "AI": {"tldr": "This paper presents a federated learning methodology that addresses data quality issues such as noisy labels, missing classes, and imbalanced distributions. It enhances data integrity through adaptive noise cleaning, synthetic data generation, and robust model training. Experiments on MNIST and Fashion-MNIST show significant improvements in macro-F1 Score under varying conditions. The framework is practical for edge devices while maintaining data privacy.", "motivation": "Federated learning faces challenges due to data quality issues like noisy labels, missing classes, and imbalanced distributions which affect its effectiveness.", "method": "The proposed method includes adaptive noise cleaning, collaborative conditional GAN-based synthetic data generation for class imbalance and missing labels, and robust federated model training.", "result": "Experimental evaluations on benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant improvements in the macro-F1 Score under different noise and class imbalance conditions.", "conclusion": "The proposed methodology effectively mitigates common data quality challenges providing a robust, scalable, and privacy-compliant solution suitable for real-world federated learning scenarios."}}
{"id": "2505.09926", "pdf": "https://arxiv.org/pdf/2505.09926", "abs": "https://arxiv.org/abs/2505.09926", "authors": ["Bin-Bin Gao", "Yue Zhu", "Jiangtao Yan", "Yuezhi Cai", "Weixi Zhang", "Meng Wang", "Jun Liu", "Yong Liu", "Lei Wang", "Chengjie Wang"], "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 15 figures, 22 tables", "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.", "AI": {"tldr": "AdaptCLIP is a method for universal visual anomaly detection that builds on CLIP models with added adapters for visual, textual, and prompt-query processing, achieving state-of-the-art performance.", "motivation": "Current methods for universal visual anomaly detection struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning which limits their flexibility.", "method": "AdaptCLIP learns adaptive visual and textual representations alternately rather than jointly, incorporates comparative learning between query and normal image prompts using both contextual and aligned residual features, and adds three simple adapters to CLIP models: visual adapter, textual adapter, and prompt-query adapter.", "result": "AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods.", "conclusion": "AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset."}}
{"id": "2505.09852", "pdf": "https://arxiv.org/pdf/2505.09852", "abs": "https://arxiv.org/abs/2505.09852", "authors": ["Apollinaire Poli Nemkova", "Sarath Chandra Lingareddy", "Sagnik Ray Choudhury", "Mark V. Albert"], "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ec5\u4f7f\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u66b4\u529b\u51b2\u7a81\u5347\u7ea7\u548c\u4f24\u4ea1\u7684\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u7ed3\u5408\u5916\u90e8\u6570\u636e\uff08\u5982\u51b2\u7a81\u6570\u636e\u96c6\u548c\u65b0\u95fb\u62a5\u544a\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\u7814\u7a76\u901a\u8fc7\u53c2\u6570\u5316\u548c\u975e\u53c2\u6570\u5316\u4e24\u79cd\u8bbe\u7f6e\uff0c\u5728\u975e\u6d32\u4e4b\u89d2\u548c\u4e2d\u4e1c\u5730\u533a2020-2024\u5e74\u7684\u51b2\u7a81\u9884\u6d4b\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5916\u90e8\u77e5\u8bc6\u80fd\u6709\u6548\u589e\u5f3aLLMs\u7684\u51b2\u7a81\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9884\u6d4b\u66b4\u529b\u51b2\u7a81\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u8fd9\u5bf9\u4e8e\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3001\u4eba\u9053\u4e3b\u4e49\u89c4\u5212\u548c\u653f\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86LLMs\u5728\u53c2\u6570\u5316\uff08\u4ec5\u4f9d\u9760\u9884\u8bad\u7ec3\u6743\u91cd\uff09\u548c\u975e\u53c2\u6570\u5316\uff08\u7ed3\u5408\u51b2\u7a81\u6570\u636e\u96c6\u548c\u65b0\u95fb\u62a5\u544a\u7b49\u5916\u90e8\u4fe1\u606f\uff09\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u51b2\u7a81\u9884\u6d4b\u80fd\u529b\u3002\u91c7\u7528\u4e24\u90e8\u5206\u8bc4\u4f30\u6846\u67b6\uff0c\u8986\u76d62020-2024\u5e74\u975e\u6d32\u4e4b\u89d2\u548c\u4e2d\u4e1c\u5730\u533a\u7684\u51b2\u7a81\u591a\u53d1\u533a\u57df\u3002", "result": "\u5728\u53c2\u6570\u5316\u8bbe\u7f6e\u4e0b\uff0cLLMs\u80fd\u591f\u4ec5\u4f9d\u9760\u9884\u8bad\u7ec3\u77e5\u8bc6\u9884\u6d4b\u51b2\u7a81\u8d8b\u52bf\u548c\u4f24\u4ea1\uff1b\u5728\u975e\u53c2\u6570\u5316\u8bbe\u7f6e\u4e0b\uff0c\u7ed3\u5408\u5916\u90e8\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "LLMs\u5177\u5907\u4e00\u5b9a\u7684\u51b2\u7a81\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u7ed3\u5408\u7ed3\u6784\u5316\u7684\u5916\u90e8\u77e5\u8bc6\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u9884\u6d4b\u6548\u679c\u3002\u8fd9\u4e3a\u51b2\u7a81\u9884\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2505.09932", "pdf": "https://arxiv.org/pdf/2505.09932", "abs": "https://arxiv.org/abs/2505.09932", "authors": ["Kevin J McNamara", "Rhea Pritham Marpu"], "title": "Demystifying AI Agents: The Final Generation of Intelligence", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.MA"], "comment": null, "summary": "The trajectory of artificial intelligence (AI) has been one of relentless\nacceleration, evolving from rudimentary rule-based systems to sophisticated,\nautonomous agents capable of complex reasoning and interaction. This whitepaper\nchronicles this remarkable journey, charting the key technological\nmilestones--advancements in prompting, training methodologies, hardware\ncapabilities, and architectural innovations--that have converged to create the\nAI agents of today. We argue that these agents, exemplified by systems like\nOpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in\nAI development, potentially constituting the \"final generation\" of intelligence\nas we currently conceive it. We explore the capabilities and underlying\ntechnologies of these agents, grounded in practical examples, while also\nexamining the profound societal implications and the unprecedented pace of\nprogress that suggests intelligence is now doubling approximately every six\nmonths. The paper concludes by underscoring the critical need for wisdom and\nforesight in navigating the opportunities and challenges presented by this\npowerful new era of intelligence.", "AI": {"tldr": "The paper discusses the evolution of AI from basic rule-based systems to advanced autonomous agents, like ChatGPT and Grok, potentially representing the 'final generation' of AI as we know it. It highlights key milestones in AI development, societal implications, and the rapid pace of progress, while emphasizing the need for wisdom in navigating this new era.", "motivation": "To chronicle the remarkable journey of AI evolution and understand the capabilities and underlying technologies of current AI agents, exemplified by systems like ChatGPT and Grok.", "method": "Exploring practical examples and examining advancements in prompting, training methodologies, hardware capabilities, and architectural innovations that have led to today's AI agents.", "result": "AI has reached a culminating phase with sophisticated agents capable of complex reasoning and interaction, with intelligence doubling approximately every six months.", "conclusion": "There is a critical need for wisdom and foresight in addressing the opportunities and challenges brought about by this powerful new era of intelligence."}}
{"id": "2505.09742", "pdf": "https://arxiv.org/pdf/2505.09742", "abs": "https://arxiv.org/abs/2505.09742", "authors": ["Yuan-Hang Zhang", "Massimiliano Di Ventra"], "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.NE"], "comment": "15 pages, 3 figures", "summary": "We propose a generative, end-to-end solver for black-box combinatorial\noptimization that emphasizes both sample efficiency and solution quality on NP\nproblems. Drawing inspiration from annealing-based algorithms, we treat the\nblack-box objective as an energy function and train a neural network to model\nthe associated Boltzmann distribution. By conditioning on temperature, the\nnetwork captures a continuum of distributions--from near-uniform at high\ntemperatures to sharply peaked around global optima at low\ntemperatures--thereby learning the structure of the energy landscape and\nfacilitating global optimization. When queries are expensive, the\ntemperature-dependent distributions naturally enable data augmentation and\nimprove sample efficiency. When queries are cheap but the problem remains hard,\nthe model learns implicit variable interactions, effectively \"opening\" the\nblack box. We validate our approach on challenging combinatorial tasks under\nboth limited and unlimited query budgets, showing competitive performance\nagainst state-of-the-art black-box optimizers.", "AI": {"tldr": "We propose a generative end-to-end solver for black-box combinatorial optimization which learns the structure of energy landscape and facilitates global optimization.", "motivation": "Current methods for black-box combinatorial optimization may not perform well in terms of sample efficiency and solution quality simultaneously, especially on NP problems.", "method": "The method treats the black-box objective as an energy function and trains a neural network to model the associated Boltzmann distribution. By conditioning on temperature, the network captures different distributions that help in learning the structure of the energy landscape.", "result": "Validated on challenging combinatorial tasks with both limited and unlimited query budgets, showing competitive performance against state-of-the-art black-box optimizers.", "conclusion": "This approach can improve sample efficiency when queries are expensive and learn implicit variable interactions when queries are cheap but the problem is hard."}}
{"id": "2505.09927", "pdf": "https://arxiv.org/pdf/2505.09927", "abs": "https://arxiv.org/abs/2505.09927", "authors": ["Siqi Yin", "Shaolei Liu", "Manning Wang"], "title": "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.", "AI": {"tldr": "The paper proposes a new source-free domain adaptation (SFDA) framework that includes preadaptation, frequency prompt, and style-related layer fine-tuning to improve pseudo-label quality and model performance.", "motivation": "Existing SFDA methods have limitations in producing high-quality pseudo-labels and translating image styles effectively, leading to inefficiencies in model training under limited supervision.", "method": "The proposed method introduces three key components: preadaptation for generating a preadapted model to enhance pseudo-labels, data-dependent frequency prompt for effective image style translation, and style-related layer fine-tuning for efficient target model training.", "result": "Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks show that the proposed method outperforms existing state-of-the-art techniques.", "conclusion": "The novel SFDA framework with preadaptation, frequency prompt, and style-related layer fine-tuning successfully addresses current challenges in SFDA, offering superior performance in medical imaging tasks."}}
{"id": "2505.09902", "pdf": "https://arxiv.org/pdf/2505.09902", "abs": "https://arxiv.org/abs/2505.09902", "authors": ["Martin Capdevila", "Esteban Villa Turek", "Ellen Karina Chumbe Fernandez", "Luis Felipe Polo Galvez", "Luis Cadavid", "Andrea Marroquin", "Rebeca Vargas Quesada", "Johanna Crew", "Nicole Vallejo Galarraga", "Christopher Rodriguez", "Diego Gutierrez", "Radhi Datla"], "title": "Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are, by definition, based on language. In an effort to\nunderscore the critical need for regional localized models, this paper examines\nprimary differences between variants of written Spanish across Latin America\nand Spain, with an in-depth sociocultural and linguistic contextualization\ntherein. We argue that these differences effectively constitute significant\ngaps in the quotidian use of Spanish among dialectal groups by creating\nsociolinguistic dissonances, to the extent that locale-sensitive AI models\nwould play a pivotal role in bridging these divides. In doing so, this approach\ninforms better and more efficient localization strategies that also serve to\nmore adequately meet inclusivity goals, while securing sustainable active daily\nuser growth in a major low-risk investment geographic area. Therefore,\nimplementing at least the proposed five sub variants of Spanish addresses two\nlines of action: to foment user trust and reliance on AI language models while\nalso demonstrating a level of cultural, historical, and sociolinguistic\nawareness that reflects positively on any internationalization strategy.", "AI": {"tldr": "This paper explores the differences in written Spanish across Latin America and Spain, emphasizing the need for locale-sensitive AI models to bridge sociolinguistic gaps and improve inclusivity.", "motivation": "To highlight the critical need for regional localized models by examining the primary differences between variants of written Spanish across Latin America and Spain.", "method": "An in-depth sociocultural and linguistic contextualization of the differences in written Spanish among dialectal groups.", "result": "Locale-sensitive AI models can significantly bridge sociolinguistic divides and inform better localization strategies that meet inclusivity goals and secure sustainable user growth.", "conclusion": "Implementing at least five sub variants of Spanish addresses key actions such as fostering user trust and reliance on AI language models while demonstrating cultural, historical, and sociolinguistic awareness."}}
{"id": "2505.09970", "pdf": "https://arxiv.org/pdf/2505.09970", "abs": "https://arxiv.org/abs/2505.09970", "authors": ["Mrinal Rawat", "Ambuje Gupta", "Rushil Goomer", "Alessandro Di Bari", "Neha Gupta", "Roberto Pieraccini"], "title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has\nbecome the foundation of modern agentic systems. Recent LLMs, such as\nDeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through\nthe generation of ample intermediate tokens, which help build a strong premise\nbefore producing the final output tokens. In this paper, we introduce Pre-Act,\na novel approach that enhances the agent's performance by creating a multi-step\nexecution plan along with the detailed reasoning for the given user input. This\nplan incrementally incorporates previous steps and tool outputs, refining\nitself after each step execution until the final response is obtained. Our\napproach is applicable to both conversational and non-conversational agents. To\nmeasure the performance of task-oriented agents comprehensively, we propose a\ntwo-level evaluation framework: (1) turn level and (2) end-to-end. Our\nturn-level evaluation, averaged across five models, shows that our approach,\nPre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While\nthis approach is effective for larger models, smaller models crucial for\npractical applications, where latency and cost are key constraints, often\nstruggle with complex reasoning tasks required for agentic systems. To address\nthis limitation, we fine-tune relatively small models such as Llama 3.1 (8B &\n70B) using the proposed Pre-Act approach. Our experiments show that the\nfine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action\naccuracy (turn-level) and a 28% improvement in goal completion rate\n(end-to-end) on the Almita (out-of-domain) dataset.", "AI": {"tldr": "Pre-Act \u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u591a\u6b65\u9aa4\u6267\u884c\u8ba1\u5212\u548c\u8be6\u7ec6\u63a8\u7406\u6765\u589e\u5f3a\u4ee3\u7406\u6027\u80fd\u3002\u5728 Almita \u6570\u636e\u96c6\u4e0a\uff0cPre-Act \u5728\u52a8\u4f5c\u53ec\u56de\u7387\u4e0a\u6bd4 ReAct \u9ad8 70%\uff0c\u5e76\u4e14\u7ecf\u8fc7\u5fae\u8c03\u7684\u5c0f\u578b\u6a21\u578b\uff08\u5982 Llama 3.1\uff09\u5728\u52a8\u4f5c\u51c6\u786e\u6027\u548c\u76ee\u6807\u5b8c\u6210\u7387\u4e0a\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u5f3a\u8c03\u901a\u8fc7\u751f\u6210\u5927\u91cf\u4e2d\u95f4\u6807\u8bb0\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u56e0\u5ef6\u8fdf\u548c\u6210\u672c\u9650\u5236\u800c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u3002\u4e3a\u4e86\u6539\u8fdb\u8fd9\u4e00\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5 Pre-Act\u3002", "method": "Pre-Act \u65b9\u6cd5\u901a\u8fc7\u4e3a\u7ed9\u5b9a\u7684\u7528\u6237\u8f93\u5165\u521b\u5efa\u4e00\u4e2a\u591a\u6b65\u9aa4\u6267\u884c\u8ba1\u5212\u548c\u8be6\u7ec6\u7684\u63a8\u7406\u8fc7\u7a0b\u6765\u589e\u5f3a\u4ee3\u7406\u6027\u80fd\u3002\u8be5\u8ba1\u5212\u9010\u6b65\u7ed3\u5408\u4e4b\u524d\u7684\u6b65\u9aa4\u548c\u5de5\u5177\u8f93\u51fa\uff0c\u5e76\u5728\u6bcf\u6b21\u6b65\u9aa4\u6267\u884c\u540e\u8fdb\u884c\u81ea\u6211\u5b8c\u5584\uff0c\u76f4\u5230\u83b7\u5f97\u6700\u7ec8\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u5c42\u8bc4\u4f30\u6846\u67b6\uff1a\u56de\u5408\u7ea7\u522b\u548c\u7aef\u5230\u7aef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728 Almita \u6570\u636e\u96c6\u4e0a\uff0cPre-Act \u7684\u52a8\u4f5c\u53ec\u56de\u7387\u6bd4 ReAct \u9ad8 70%\u3002\u7ecf\u8fc7\u5fae\u8c03\u7684 70B \u6a21\u578b\u5728\u52a8\u4f5c\u51c6\u786e\u6027\uff08\u56de\u5408\u7ea7\u522b\uff09\u4e0a\u6bd4 GPT-4 \u63d0\u9ad8\u4e86 69.5%\uff0c\u5728\u76ee\u6807\u5b8c\u6210\u7387\uff08\u7aef\u5230\u7aef\uff09\u4e0a\u63d0\u9ad8\u4e86 28%\u3002", "conclusion": "Pre-Act \u65b9\u6cd5\u5728\u63d0\u9ad8\u4ee3\u7406\u6027\u80fd\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u662f\u5728\u52a8\u4f5c\u53ec\u56de\u7387\u3001\u52a8\u4f5c\u51c6\u786e\u6027\u548c\u76ee\u6807\u5b8c\u6210\u7387\u65b9\u9762\u3002\u6b64\u65b9\u6cd5\u9002\u7528\u4e8e\u5404\u79cd\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5305\u62ec\u5bf9\u5ef6\u8fdf\u548c\u6210\u672c\u654f\u611f\u7684\u5c0f\u578b\u6a21\u578b\u3002"}}
{"id": "2505.09756", "pdf": "https://arxiv.org/pdf/2505.09756", "abs": "https://arxiv.org/abs/2505.09756", "authors": ["Zhaoyang Shi"], "title": "Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration", "categories": ["cs.LG", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "We propose a new framework for multi-agent reinforcement learning (MARL),\nwhere the agents cooperate in a time-evolving network with latent community\nstructures and mixed memberships. Unlike traditional neighbor-based or fixed\ninteraction graphs, our community-based framework captures flexible and\nabstract coordination patterns by allowing each agent to belong to multiple\noverlapping communities. Each community maintains shared policy and value\nfunctions, which are aggregated by individual agents according to personalized\nmembership weights. We also design actor-critic algorithms that exploit this\nstructure: agents inherit community-level estimates for policy updates and\nvalue learning, enabling structured information sharing without requiring\naccess to other agents' policies. Importantly, our approach supports both\ntransfer learning by adapting to new agents or tasks via membership estimation,\nand active learning by prioritizing uncertain communities during exploration.\nTheoretically, we establish convergence guarantees under linear function\napproximation for both actor and critic updates. To our knowledge, this is the\nfirst MARL framework that integrates community structure, transferability, and\nactive learning with provable guarantees.", "AI": {"tldr": "A new MARL framework based on community structures is proposed, enabling flexible coordination patterns and structured information sharing.", "motivation": "Existing MARL frameworks often rely on neighbor-based or fixed interaction graphs, which may not capture complex coordination patterns effectively.", "method": "The method involves creating a community-based framework where agents can belong to multiple overlapping communities. Each community maintains shared policy and value functions that agents aggregate according to membership weights. Actor-critic algorithms are designed to exploit this structure for policy updates and value learning.", "result": "The approach supports transfer learning and active learning, with theoretical convergence guarantees under linear function approximation for both actor and critic updates.", "conclusion": "This is the first MARL framework integrating community structure, transferability, and active learning with provable guarantees."}}
{"id": "2505.09935", "pdf": "https://arxiv.org/pdf/2505.09935", "abs": "https://arxiv.org/abs/2505.09935", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Quoc Dai Tran"], "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.", "AI": {"tldr": "This paper proposes VRU-CIPI, a framework using GRU and Transformer for predicting crossing intentions of Vulnerable Road Users (VRUs) at intersections. It achieves 96.45% accuracy and can be integrated with Infrastructure-to-Vehicles (I2V) communication to enhance intersection safety.", "motivation": "To improve the interaction safety between road users, especially focusing on understanding and predicting the crossing intentions of Vulnerable Road Users (VRUs) at urban intersections.", "method": "The VRU-CIPI framework uses Gated Recurrent Unit (GRU) for capturing temporal dynamics in VRU movements and a multi-head Transformer self-attention mechanism for encoding contextual and spatial dependencies.", "result": "Evaluated on the UCF-VRU dataset, the proposed framework achieves state-of-the-art performance with an accuracy of 96.45% and real-time inference speed of 33 frames per second.", "conclusion": "By integrating with Infrastructure-to-Vehicles (I2V) communication, the approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles."}}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924", "abs": "https://arxiv.org/abs/2505.09924", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\n\\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.", "AI": {"tldr": "The paper introduces a symbiotic watermarking framework with serial, parallel, and hybrid strategies for Large Language Models (LLMs), which optimizes the balance between detectability, robustness, text quality, and security.", "motivation": "To address the trade-offs among robustness, text quality, and security in current watermarking schemes for LLMs.", "method": "Proposes a versatile symbiotic watermarking framework with three strategies - serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy.", "result": "Experimental results indicate that the method outperforms existing baselines and achieves state-of-the-art (SOTA) performance.", "conclusion": "This framework provides novel insights into diverse watermarking paradigms for LLMs."}}
{"id": "2505.10034", "pdf": "https://arxiv.org/pdf/2505.10034", "abs": "https://arxiv.org/abs/2505.10034", "authors": ["Changzeng Fu", "Zelin Fu", "Xinhe Kuang", "Jiacheng Dong", "Qi Zhang", "Kaifeng Su", "Yikai Su", "Wenbo Shi", "Junfeng Yao", "Yuliang Zhao", "Shiqi Zhao", "Jiadong Wang", "Siyang Song", "Chaoran Liu", "Yuichiro Yoshikawa", "Bj\u00f6rn Schuller", "Hiroshi Ishiguro"], "title": "The First MPDD Challenge: Multimodal Personality-aware Depression Detection", "categories": ["cs.AI", "68T07", "I.2.0; H.5.1"], "comment": "This paper has been accepted as part of the MPDD Challenge in the\n  ACMMM 2025 Grand Challenge", "summary": "Depression is a widespread mental health issue affecting diverse age groups,\nwith notable prevalence among college students and the elderly. However,\nexisting datasets and detection methods primarily focus on young adults,\nneglecting the broader age spectrum and individual differences that influence\ndepression manifestation. Current approaches often establish a direct mapping\nbetween multimodal data and depression indicators, failing to capture the\ncomplexity and diversity of depression across individuals. This challenge\nincludes two tracks based on age-specific subsets: Track 1 uses the\nMPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses\nthe MPDD-Young dataset for detecting depression in younger participants. The\nMultimodal Personality-aware Depression Detection (MPDD) Challenge aims to\naddress this gap by incorporating multimodal data alongside individual\ndifference factors. We provide a baseline model that fuses audio and video\nmodalities with individual difference information to detect depression\nmanifestations in diverse populations. This challenge aims to promote the\ndevelopment of more personalized and accurate de pression detection methods,\nadvancing mental health research and fostering inclusive detection systems.\nMore details are available on the official challenge website:\nhttps://hacilab.github.io/MPDDChallenge.github.io.", "AI": {"tldr": "An analysis of the abstract of a paper on depression detection, focusing on the MPDD Challenge.", "motivation": "To address the lack of focus on diverse age groups and individual differences in existing depression datasets and detection methods.", "method": "The MPDD Challenge consists of two tracks using specific datasets (MPDD-Elderly and MPDD-Young) for detecting depression in older adults and younger participants. It incorporates multimodal data and individual difference factors.", "result": "Provides a baseline model that fuses audio and video modalities with individual difference information to detect depression manifestations in diverse populations.", "conclusion": "Aims to promote the development of more personalized and accurate depression detection methods, advancing mental health research."}}
{"id": "2505.09768", "pdf": "https://arxiv.org/pdf/2505.09768", "abs": "https://arxiv.org/abs/2505.09768", "authors": ["Xiukun Wei", "Xueru Zhang"], "title": "Self-Consuming Generative Models with Adversarially Curated Data", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in generative models have made it increasingly difficult to\ndistinguish real data from model-generated synthetic data. Using synthetic data\nfor successive training of future model generations creates \"self-consuming\nloops\", which may lead to model collapse or training instability. Furthermore,\nsynthetic data is often subject to human feedback and curated by users based on\ntheir preferences. Ferbach et al. (2024) recently showed that when data is\ncurated according to user preferences, the self-consuming retraining loop\ndrives the model to converge toward a distribution that optimizes those\npreferences. However, in practice, data curation is often noisy or\nadversarially manipulated. For example, competing platforms may recruit\nmalicious users to adversarially curate data and disrupt rival models. In this\npaper, we study how generative models evolve under self-consuming retraining\nloops with noisy and adversarially curated data. We theoretically analyze the\nimpact of such noisy data curation on generative models and identify conditions\nfor the robustness of the retraining process. Building on this analysis, we\ndesign attack algorithms for competitive adversarial scenarios, where a\nplatform with a limited budget employs malicious users to misalign a rival's\nmodel from actual user preferences. Experiments on both synthetic and\nreal-world datasets demonstrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5b58\u5728\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6570\u636e\u7ba1\u7406\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u6a21\u578b\u5728\u81ea\u6211\u6d88\u8017\u518d\u8bad\u7ec3\u5faa\u73af\u4e2d\u7684\u6f14\u53d8\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9\u7ade\u4e89\u5bf9\u6297\u573a\u666f\u7684\u653b\u51fb\u7b97\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u533a\u5206\u771f\u5b9e\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u540e\u7eed\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u6216\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u6b64\u5916\uff0c\u5408\u6210\u6570\u636e\u901a\u5e38\u53d7\u5230\u4eba\u4e3a\u53cd\u9988\u7684\u5f71\u54cd\u5e76\u6839\u636e\u7528\u6237\u504f\u597d\u8fdb\u884c\u7ba1\u7406\uff0c\u8fd9\u53ef\u80fd\u4f7f\u5f97\u6a21\u578b\u6536\u655b\u5230\u4f18\u5316\u8fd9\u4e9b\u504f\u597d\u7684\u5206\u5e03\u3002\u7136\u800c\uff0c\u5728\u5b9e\u8df5\u4e2d\uff0c\u6570\u636e\u7ba1\u7406\u5f80\u5f80\u662f\u5608\u6742\u7684\u6216\u88ab\u5bf9\u6297\u6027\u5730\u64cd\u63a7\u3002", "method": "\u4f5c\u8005\u5bf9\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u8fd9\u79cd\u5608\u6742\u7684\u6570\u636e\u7ba1\u7406\u5bf9\u5176\u5f71\u54cd\uff0c\u5e76\u786e\u5b9a\u518d\u8bad\u7ec3\u8fc7\u7a0b\u7a33\u5065\u6027\u7684\u6761\u4ef6\u3002\u57fa\u4e8e\u8fd9\u4e00\u5206\u6790\uff0c\u8bbe\u8ba1\u4e86\u7528\u4e8e\u7ade\u4e89\u5bf9\u6297\u573a\u666f\u7684\u653b\u51fb\u7b97\u6cd5\uff0c\u5176\u4e2d\u5e73\u53f0\u901a\u8fc7\u6076\u610f\u7528\u6237\u5e72\u6270\u5bf9\u624b\u6a21\u578b\u4e0e\u5b9e\u9645\u7528\u6237\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5608\u6742\u548c\u5bf9\u6297\u6027\u6570\u636e\u7ba1\u7406\u5bf9\u751f\u6210\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u786e\u4fdd\u518d\u8bad\u7ec3\u8fc7\u7a0b\u7a33\u5065\u6027\u7684\u6761\u4ef6\u3002\u540c\u65f6\uff0c\u6240\u8bbe\u8ba1\u7684\u653b\u51fb\u7b97\u6cd5\u5728\u7ade\u4e89\u5bf9\u6297\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.09939", "pdf": "https://arxiv.org/pdf/2505.09939", "abs": "https://arxiv.org/abs/2505.09939", "authors": ["Zhe Shan", "Lei Zhou", "Liu Mao", "Shaofan Chen", "Chuanqiu Ren", "Xia Xie"], "title": "Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IGARSS 2025", "summary": "In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.", "AI": {"tldr": "The paper introduces a new remote sensing change detection task called non-registration change detection, outlines eight real-world scenarios leading to non-registration problems, devises image transformation schemes for these scenarios, and shows the negative impact on current methods.", "motivation": "To tackle emergencies like natural disasters, anthropogenic accidents, and military strikes by proposing a novel remote sensing change detection task.", "method": "Propose eight real-world scenarios for non-registration problems, develop tailored image transformation schemes, and demonstrate the catastrophic damage to state-of-the-art methods.", "result": "Non-registration change detection causes significant issues to existing advanced methods.", "conclusion": "This study highlights the challenges of non-registration change detection and provides resources (code and dataset) for further research."}}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930", "abs": "https://arxiv.org/abs/2505.09930", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Prompt optimization (PO) offers a practical alternative to fine-tuning large\nlanguage models (LLMs), enabling performance improvements without altering\nmodel weights. Existing methods typically rely on advanced, large-scale LLMs\nlike GPT-4 to generate optimized prompts. However, due to limited downward\ncompatibility, verbose, instruction-heavy prompts from advanced LLMs can\noverwhelm lightweight inference models and degrade response quality. In this\nwork, we rethink prompt optimization through the lens of interpretable design.\nWe first identify a set of model-agnostic prompt quality merits and empirically\nvalidate their effectiveness in enhancing prompt and response quality. We then\nintroduce MePO, a merit-guided, lightweight, and locally deployable prompt\noptimizer trained on our preference dataset built from merit-aligned prompts\ngenerated by a lightweight LLM. Unlike prior work, MePO avoids online\noptimization reliance, reduces cost and privacy concerns, and, by learning\nclear, interpretable merits, generalizes effectively to both large-scale and\nlightweight inference models. Experiments demonstrate that MePO achieves better\nresults across diverse tasks and model types, offering a scalable and robust\nsolution for real-world deployment. Our model and dataset are available at:\nhttps://github.com/MidiyaZhu/MePO", "AI": {"tldr": "The paper introduces MePO, a lightweight prompt optimizer that enhances the quality of prompts for both large-scale and lightweight language models without relying on online optimization, reducing costs and privacy concerns.", "motivation": "To address the issue where optimized prompts from advanced large language models (LLMs) can overwhelm lightweight inference models and degrade response quality.", "method": "MePO is trained on a preference dataset built from merit-aligned prompts generated by a lightweight LLM. It focuses on model-agnostic prompt quality merits to improve prompt and response quality.", "result": "Experiments show that MePO achieves better results across various tasks and model types, providing a scalable and robust solution for real-world deployment.", "conclusion": "MePO offers an effective, interpretable, and locally deployable solution for prompt optimization, enhancing performance across diverse tasks and model sizes."}}
{"id": "2505.10074", "pdf": "https://arxiv.org/pdf/2505.10074", "abs": "https://arxiv.org/abs/2505.10074", "authors": ["Mohamed Abdelmagied", "Mohamed Amine Chatti", "Shoeb Joarder", "Qurat Ul Ain", "Rawaa Alatrash"], "title": "Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs", "categories": ["cs.AI", "cs.CY"], "comment": "Accepted at EMOOCs 2025", "summary": "Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.", "AI": {"tldr": "In this paper, the authors tackle the challenge of learners understanding new knowledge concepts in MOOCs by proposing a Graph RAG pipeline that uses EduKGs and PKGs. This pipeline includes personalized question generation and concept-relation based question answering. The evaluation with expert instructors on different MOOCs shows its potential to enhance personalized learning.", "motivation": "MOOCs lack direct interaction between learners and instructors, making it hard for learners to understand new knowledge concepts. While LLMs can support knowledge acquisition, they suffer from hallucinations. RAG addresses hallucinations but is limited by unstructured MOOC materials and lacks active guidance for learners.", "method": "The authors propose a Graph RAG pipeline which incorporates (1) a PKG-based Question Generation method for personalized questions and (2) an EduKG-based Question Answering method utilizing relationships between knowledge concepts. These methods aim to guide learners effectively within the CourseMapper platform.", "result": "A study involving 3 expert instructors across 3 different MOOCs demonstrated the potential of the Graph RAG approach to empower learners with a personalized learning experience for understanding new knowledge concepts.", "conclusion": "The Graph RAG pipeline using EduKGs and PKGs has shown promise in enhancing learner understanding of knowledge concepts through personalized questions and answers in MOOC platforms."}}
{"id": "2505.09792", "pdf": "https://arxiv.org/pdf/2505.09792", "abs": "https://arxiv.org/abs/2505.09792", "authors": ["Michael Kamfonas"], "title": "Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints", "categories": ["cs.LG"], "comment": null, "summary": "This case study applies a phased hyperparameter optimization process to\ncompare multitask natural language model variants that utilize multiphase\nlearning rate scheduling and optimizer parameter grouping. We employ short,\nBayesian optimization sessions that leverage multi-fidelity, hyperparameter\nspace pruning, progressive halving, and a degree of human guidance. We utilize\nthe Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn\nGaussian process minimization. Initially, we use efficient low-fidelity sprints\nto prune the hyperparameter space. Subsequent sprints progressively increase\ntheir model fidelity and employ hyperband pruning for efficiency. A second\naspect of our approach is using a meta-learner to tune threshold values to\nresolve classification probabilities during inference. We demonstrate our\nmethod on a collection of variants of the 2021 Joint Entity and Relation\nExtraction model proposed by Eberts and Ulges.", "AI": {"tldr": "This case study applies a phased hyperparameter optimization process to compare multitask natural language model variants.", "motivation": "To optimize hyperparameters in multitask natural language models utilizing multiphase learning rate scheduling and optimizer parameter grouping.", "method": "Employ short, Bayesian optimization sessions that leverage multi-fidelity, hyperparameter space pruning, progressive halving, and human guidance using the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn Gaussian process minimization. Initially use low-fidelity sprints to prune the hyperparameter space and progressively increase fidelity. Use a meta-learner to tune threshold values for classification probabilities during inference.", "result": "Demonstrates the method on variants of the 2021 Joint Entity and Relation Extraction model proposed by Eberts and Ulges.", "conclusion": "The phased hyperparameter optimization process is successfully applied to compare multitask natural language model variants."}}
{"id": "2505.09943", "pdf": "https://arxiv.org/pdf/2505.09943", "abs": "https://arxiv.org/abs/2505.09943", "authors": ["Jiakun Deng", "Kexuan Li", "Xingye Cui", "Jiaxuan Li", "Chang Long", "Tian Pu", "Zhenming Peng"], "title": "CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u7edcCSPENet\u7528\u4e8e\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u8be5\u7f51\u7edc\u901a\u8fc7\u63d0\u53d6\u548c\u5d4c\u5165\u8f6e\u5ed3\u611f\u77e5\u548c\u663e\u8457\u6027\u5148\u9a8c\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u5b9a\u4f4d\u5fae\u5f31\u76ee\u6807\u548c\u5728\u5bc6\u96c6\u6742\u6ce2\u73af\u5883\u4e2d\u611f\u77e5\u8f6e\u5ed3\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u56f4\u6536\u655b\u5148\u9a8c\u63d0\u53d6\u6a21\u5757\uff08SCPEM\uff09\uff0c\u63d0\u53d6\u589e\u5f3a\u7684\u663e\u8457\u6027\u5148\u9a8c\u548c\u591a\u5c3a\u5ea6\u7ed3\u6784\u5148\u9a8c\uff1b\u63d0\u51fa\u4e86\u53cc\u5206\u652f\u5148\u9a8c\u5d4c\u5165\u67b6\u6784\uff08DBPEA\uff09\uff0c\u4ee5\u6700\u4f18\u7f51\u7edc\u4f4d\u7f6e\u5d4c\u5165\u8fd9\u4e24\u79cd\u5148\u9a8c\uff1b\u5f00\u53d1\u4e86\u6ce8\u610f\u529b\u5f15\u5bfc\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff08AGFEM\uff09\u6765\u7ec6\u5316\u7279\u5f81\u8868\u793a\u5e76\u63d0\u9ad8\u663e\u8457\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728NUDT-SIRST\u3001IRSTD-1k\u548cNUAA-SIRST\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCSPENet\u7684\u68c0\u6d4b\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CSPENet\u901a\u8fc7\u7ed3\u5408\u8f6e\u5ed3\u611f\u77e5\u548c\u663e\u8457\u6027\u5148\u9a8c\uff0c\u5728\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.09945", "pdf": "https://arxiv.org/pdf/2505.09945", "abs": "https://arxiv.org/abs/2505.09945", "authors": ["Deeksha Prahlad", "Chanhee Lee", "Dongha Kim", "Hokeun Kim"], "title": "Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)", "summary": "The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u4e2a\u6027\u5316\u56de\u5e94\u65f6\u51fa\u73b0\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u76f8\u8f83\u4e8e\u4ec5\u7528\u6587\u672c\u8f93\u5165\u4e2a\u4eba\u6570\u636e\u7684\u57fa\u7840LLM\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u7406\u89e3\u4e2a\u4eba\u4fe1\u606f\u5e76\u751f\u6210\u7cbe\u786e\u56de\u5e94\uff0c\u4f46\u54cd\u5e94\u65f6\u95f4\u7565\u6709\u51cf\u5c11\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u88ab\u5e94\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u5e9e\u5927\uff0c\u5e38\u5e38\u53d1\u751f\u8fc7\u5ea6\u62df\u5408\uff0c\u5bfc\u81f4\u751f\u6210\u989d\u5916\u548c\u4e0d\u6b63\u786e\u7684\u6570\u636e\uff0c\u4ece\u800c\u5728\u8f93\u51fa\u4e2d\u4ea7\u751f\u5e7b\u89c9\u3002\u5176\u4e2d\u4e00\u4e2a\u6839\u672c\u539f\u56e0\u662f\u7f3a\u4e4f\u53ca\u65f6\u3001\u51c6\u786e\u548c\u4e2a\u6027\u5316\u7684\u4fe1\u606f\u8f93\u5165\u5230LLM\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e2e\u52a9LLM\u751f\u6210\u4e2a\u6027\u5316\u7684\u56de\u5e94\u3002\u6b64\u65b9\u6cd5\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\u53ef\u4ee5\u50a8\u5b58\u6301\u7eed\u66f4\u65b0\u7684\u4e8b\u5b9e\u4fe1\u606f\uff0c\u5e76\u4e14\u4f5c\u8005\u5c06\u91cd\u70b9\u653e\u5728\u65e5\u5386\u6570\u636e\u4e0a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f7f\u7528\u4e2a\u4eba\u6570\u636e\u4f5c\u4e3a\u6587\u672c\u8f93\u5165\u7684\u57fa\u7840LLMs\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u66f4\u597d\u5730\u7406\u89e3\u548c\u751f\u6210\u51c6\u786e\u7684\u56de\u5e94\uff0c\u4e0d\u8fc7\u54cd\u5e94\u65f6\u95f4\u6709\u9002\u5ea6\u7684\u51cf\u5c11\u3002", "conclusion": "\u5f15\u5165\u4e86\u77e5\u8bc6\u56fe\u8c31\u8f85\u52a9\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u6539\u5584LLM\u5728\u4e2a\u6027\u5316\u56de\u5e94\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5bf9\u4e2a\u4eba\u4fe1\u606f\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2505.10093", "pdf": "https://arxiv.org/pdf/2505.10093", "abs": "https://arxiv.org/abs/2505.10093", "authors": ["Hsuan-Lei Shao"], "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI", "categories": ["cs.AI", "cs.CL", "I.2.4; H.3.3; J.5"], "comment": "4 pages, 4 figures", "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.", "AI": {"tldr": "This study proposes an AI assisted approach that transforms unstructured academic texts into structured, interactive knowledge representations for Taiwanese China Studies (CS) scholarship. By applying generative AI techniques and large language models, the system extracts and standardizes entity relation triples from peer reviewed CS articles, visualizes them through a D3.js based system, and forms a domain specific knowledge graph and vector database. This infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing intellectual trajectories, thematic clusters, and research gaps.", "motivation": "Taiwanese China Studies has developed into a rich, interdisciplinary research field shaped by the unique geopolitical position and long standing academic engagement with Mainland China. There is a growing need to systematically revisit and reorganize decades of Taiwan based CS scholarship.", "method": "The study applies generative AI techniques and large language models to extract and standardize entity relation triples from 1,367 peer reviewed CS articles published between 1996 and 2019. These triples are then visualized through a lightweight D3.js based system, forming the foundation of a domain specific knowledge graph and vector database for the field.", "result": "The infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing previously uncharted intellectual trajectories, thematic clusters, and research gaps. It enables a paradigm shift from linear text consumption to network based knowledge navigation.", "conclusion": "This work demonstrates how generative AI can augment area studies and digital humanities, offering a scalable, data driven alternative to traditional ontology construction and supporting a reimagined scholarly infrastructure for regional knowledge systems."}}
{"id": "2505.09810", "pdf": "https://arxiv.org/pdf/2505.09810", "abs": "https://arxiv.org/abs/2505.09810", "authors": ["Daniel Waddington", "Cornel Constantinescu"], "title": "Lossless Compression for LLM Tensor Incremental Snapshots", "categories": ["cs.LG"], "comment": null, "summary": "During the training of Large Language Models (LLMs), tensor data is\nperiodically \"checkpointed\" to persistent storage to allow recovery of work\ndone in the event of failure. The volume of data that must be copied during\neach checkpoint, even when using reduced-precision representations such as\nbfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be\nmoved across a network and written to a storage system before the next epoch\noccurs. With a view to ultimately building an optimized checkpointing solution,\nthis paper presents experimental analysis of checkpoint data used to derive a\ndesign that maximizes the use of lossless compression to reduce the volume of\ndata. We examine how tensor data and its compressibility evolve during model\ntraining and evaluate the efficacy of existing common off-the-shelf general\npurpose compression engines combined with known data optimization techniques\nsuch as byte-grouping and incremental delta compression.\n  Leveraging our analysis we have built an effective compression solution,\nknown as Language Model Compressor (LMC), which is based on byte-grouping and\nHuffman encoding. LMC offers more compression performance than the best\nalternative (BZ2) but with an order-of-magnitude reduction in the time needed\nto perform the compression. We show that a 16-core parallel implementation of\nLMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76\nGiB/s respectively. This increase in performance ultimately reduces the CPU\nresources needed and provides more time to copy the data to the storage system\nbefore the next epoch thus allowing for higher-frequency checkpoints.", "AI": {"tldr": "This paper analyzes checkpoint data in Large Language Models (LLMs) training and proposes a optimized compression solution, Language Model Compressor (LMC).", "motivation": "The large volume of checkpoint data during LLMs training leads to significant resource consumption. Reducing the data volume can improve efficiency.", "method": "Analyze the tensor data evolution during model training, evaluate existing compression engines and optimization techniques, then build a new compression solution named LMC based on byte-grouping and Huffman encoding.", "result": "LMC outperforms BZ2 with significantly less compression time. A 16-core parallel implementation of LMC achieves high compression and decompression throughput, reducing CPU resources needed and allowing higher-frequency checkpoints.", "conclusion": "The proposed LMC is an effective compression solution for checkpoint data in LLMs training, improving efficiency and enabling more frequent checkpoints."}}
{"id": "2505.09965", "pdf": "https://arxiv.org/pdf/2505.09965", "abs": "https://arxiv.org/abs/2505.09965", "authors": ["Hao Yang", "Tao Tan", "Shuai Tan", "Weiqin Yang", "Kunyan Cai", "Calvin Chen", "Yue Sun"], "title": "MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.", "AI": {"tldr": "The paper presents MambaControl, a new framework that integrates selective state-space modelling with diffusion processes for predicting medical image trajectories in disease progression. It combines Mamba-based long-range modelling with graph-guided anatomical control and introduces Fourier-enhanced spectral graph representations to achieve high accuracy in Alzheimer's disease prediction.", "motivation": "Current methods for modelling disease progression have difficulty handling longitudinal dependencies and maintaining structural consistency in progressive disorders. This necessitates a more effective approach to capture complex spatio-temporal dynamics while preserving anatomical integrity.", "method": "MambaControl integrates selective state-space modelling with diffusion processes. It uses Mamba-based long-range modelling combined with graph-guided anatomical control to represent anatomical correlations. Additionally, it incorporates Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail.", "result": "Quantitative and regional evaluations show improved progression prediction quality and anatomical fidelity in the context of Alzheimer's disease prediction, demonstrating state-of-the-art performance.", "conclusion": "MambaControl offers significant improvements in predicting disease progression with high anatomical fidelity, showcasing potential for personalized prognosis and clinical decision support."}}
{"id": "2505.10013", "pdf": "https://arxiv.org/pdf/2505.10013", "abs": "https://arxiv.org/abs/2505.10013", "authors": ["Lake Yin", "Fan Huang"], "title": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 1 figure", "summary": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aDIF\uff08\u4eba\u53e3\u7edf\u8ba1\u9690\u5f0f\u516c\u5e73\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u9690\u5f0f\u504f\u5dee\u3002\u901a\u8fc7\u4f7f\u7528\u5e26\u6709\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u7684\u89d2\u8272\u8bc4\u4f30\u73b0\u6709\u7684\u903b\u8f91\u548c\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\uff0c\u4ed6\u4eec\u53d1\u73b0\u56de\u7b54\u51c6\u786e\u6027\u4e0e\u9690\u5f0f\u504f\u5dee\u4e4b\u95f4\u5b58\u5728\u53cd\u6bd4\u8d8b\u52bf\u3002\u8fd9\u8868\u660eLLMs\u7684\u6280\u672f\u5c40\u9650\u6027\uff0c\u5373\u65e0\u6cd5\u5f88\u597d\u5730\u5904\u7406\u989d\u5916\u4fe1\u606f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fd1\u5e74\u6765\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u4eba\u4eec\u5bf9\u5176\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u7684\u6f5c\u5728\u504f\u89c1\u8868\u793a\u62c5\u5fe7\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u63a2\u8ba8\u4e86LLMs\u5982\u4f55\u8868\u73b0\u51fa\u9690\u5f0f\u504f\u89c1\uff0c\u4f8b\u5982\u5f53\u5f15\u5165\u4e0d\u540c\u7684\u793e\u4f1a\u80cc\u666f\u65f6\uff0c\u54cd\u5e94\u751f\u6210\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u672c\u6587\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u9690\u5f0f\u504f\u89c1\u4e0d\u4ec5\u662f\u4f26\u7406\u95ee\u9898\uff0c\u4e5f\u662f\u6280\u672f\u95ee\u9898\uff0c\u56e0\u4e3a\u8fd9\u63ed\u793a\u4e86LLMs\u65e0\u6cd5\u9002\u5e94\u989d\u5916\u4fe1\u606f\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u4e0e\u5176\u4ed6LLM\u667a\u80fd\u5ea6\u91cf\u6807\u51c6\u4e0d\u540c\uff0c\u76ee\u524d\u5c1a\u65e0\u6807\u51c6\u65b9\u6cd5\u6765\u8861\u91cf\u8fd9\u4e00\u7279\u5b9a\u5b50\u96c6\u7684LLM\u504f\u89c1\u3002", "method": "\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6613\u4e8e\u89e3\u91ca\u7684\u57fa\u51c6\u7684\u65b9\u6cd5\u2014\u2014DIF\uff08Demographic Implicit Fairness\uff09\uff0c\u901a\u8fc7\u4f7f\u7528\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u89d2\u8272\u6765\u8bc4\u4f30\u73b0\u6709\u7684LLM\u903b\u8f91\u548c\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u7edf\u8ba1\u4e0a\u9a8c\u8bc1LLM\u884c\u4e3a\u4e2d\u9690\u5f0f\u504f\u89c1\u7684\u5b58\u5728\uff0c\u5e76\u4e14\u53d1\u73b0\u4e86\u95ee\u9898\u56de\u7b54\u51c6\u786e\u6027\u548c\u9690\u5f0f\u504f\u89c1\u4e4b\u95f4\u7684\u53cd\u5411\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u4e00\u53d1\u73b0\u652f\u6301\u4e86\u4f5c\u8005\u5173\u4e8e\u9690\u5f0f\u504f\u89c1\u662f\u6280\u672f\u95ee\u9898\u7684\u89c2\u70b9\uff0c\u5e76\u63ed\u793a\u4e86LLMs\u5728\u5904\u7406\u989d\u5916\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.10188", "pdf": "https://arxiv.org/pdf/2505.10188", "abs": "https://arxiv.org/abs/2505.10188", "authors": ["Felix Liedeker", "Olivia Sanchez-Graillet", "Moana Seidler", "Christian Brandt", "J\u00f6rg Wellmer", "Philipp Cimiano"], "title": "A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support", "categories": ["cs.AI"], "comment": "Presented at 'The First Workshop on Natural Language Argument-Based\n  Explanations', co-located with ECAI 2024", "summary": "As the field of healthcare increasingly adopts artificial intelligence, it\nbecomes important to understand which types of explanations increase\ntransparency and empower users to develop confidence and trust in the\npredictions made by machine learning (ML) systems. In shared decision-making\nscenarios where doctors cooperate with ML systems to reach an appropriate\ndecision, establishing mutual trust is crucial. In this paper, we explore\ndifferent approaches to generating explanations in eXplainable AI (XAI) and\nmake their underlying arguments explicit so that they can be evaluated by\nmedical experts. In particular, we present the findings of a user study\nconducted with physicians to investigate their perceptions of various types of\nAI-generated explanations in the context of diagnostic decision support. The\nstudy aims to identify the most effective and useful explanations that enhance\nthe diagnostic process. In the study, medical doctors filled out a survey to\nassess different types of explanations. Further, an interview was carried out\npost-survey to gain qualitative insights on the requirements of explanations\nincorporated in diagnostic decision support. Overall, the insights gained from\nthis study contribute to understanding the types of explanations that are most\neffective.", "AI": {"tldr": "In this paper, researchers aim to identify the most effective AI-generated explanations for diagnostic decision support in healthcare by conducting a user study with physicians.", "motivation": "With the increasing adoption of artificial intelligence in healthcare, there is a need to understand which types of explanations can increase transparency and trust in ML systems' predictions. Establishing mutual trust between doctors and ML systems is crucial in shared decision-making scenarios.", "method": "The researchers explored different approaches to generating explanations in XAI and conducted a user study with physicians. The physicians filled out a survey to assess various types of AI-generated explanations and participated in interviews post-survey for qualitative insights.", "result": "The study provided insights into the types of explanations that are most effective for enhancing the diagnostic process. These findings contribute to understanding how to empower users with confidence and trust in ML predictions.", "conclusion": "Effective AI-generated explanations play an important role in empowering medical professionals to develop trust in ML systems, ultimately improving the diagnostic decision-making process."}}
{"id": "2505.09812", "pdf": "https://arxiv.org/pdf/2505.09812", "abs": "https://arxiv.org/abs/2505.09812", "authors": ["Anastasija Tashkova", "Stefan Eftimov", "Bojan Ristov", "Slobodan Kalajdziski"], "title": "Comparative Analysis of Stroke Prediction Models Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Stroke remains one of the most critical global health challenges, ranking as\nthe second leading cause of death and the third leading cause of disability\nworldwide. This study explores the effectiveness of machine learning algorithms\nin predicting stroke risk using demographic, clinical, and lifestyle data from\nthe Stroke Prediction Dataset. By addressing key methodological challenges such\nas class imbalance and missing data, we evaluated the performance of multiple\nmodels, including Logistic Regression, Random Forest, and XGBoost. Our results\ndemonstrate that while these models achieve high accuracy, sensitivity remains\na limiting factor for real-world clinical applications. In addition, we\nidentify the most influential predictive features and propose strategies to\nimprove machine learning-based stroke prediction. These findings contribute to\nthe development of more reliable and interpretable models for the early\nassessment of stroke risk.", "AI": {"tldr": "\u4e2d\u98ce\u662f\u5168\u7403\u7b2c\u4e8c\u5927\u6b7b\u4ea1\u539f\u56e0\u548c\u7b2c\u4e09\u5927\u81f4\u6b8b\u539f\u56e0\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u4e34\u5e8a\u548c\u751f\u6d3b\u65b9\u5f0f\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u9884\u6d4b\u4e2d\u98ce\u98ce\u9669\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u6570\u636e\u7b49\u65b9\u6cd5\u8bba\u6311\u6218\uff0c\u8bc4\u4f30\u4e86\u5305\u62ec\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u5728\u5185\u7684\u591a\u79cd\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u654f\u611f\u6027\u4ecd\u7136\u662f\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u9650\u5236\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u786e\u5b9a\u4e86\u6700\u6709\u5f71\u54cd\u529b\u7684\u9884\u6d4b\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e2d\u98ce\u9884\u6d4b\u7684\u7b56\u7565\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u65e9\u671f\u4e2d\u98ce\u98ce\u9669\u8bc4\u4f30\u6a21\u578b\u3002", "motivation": "\u4e2d\u98ce\u662f\u4e00\u4e2a\u5173\u952e\u7684\u5168\u7403\u5065\u5eb7\u6311\u6218\uff0c\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u9884\u6d4b\u5de5\u5177\u6765\u964d\u4f4e\u5176\u5f71\u54cd\u3002\u73b0\u6709\u7684\u9884\u6d4b\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u5904\u7406\u6570\u636e\u4e2d\u7684\u590d\u6742\u6027\u548c\u6311\u6218\uff0c\u4f8b\u5982\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u6570\u636e\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63a2\u7d22\u548c\u4f18\u5316\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u4e2d\u98ce\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u4e2d\u98ce\u9884\u6d4b\u6570\u636e\u96c6\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u4e34\u5e8a\u548c\u751f\u6d3b\u65b9\u5f0f\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u6570\u636e\u7b49\u65b9\u6cd5\u8bba\u95ee\u9898\u3002", "result": "\u5c3d\u7ba1\u6240\u8bc4\u4f30\u7684\u6a21\u578b\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u654f\u611f\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u9650\u5236\u56e0\u7d20\uff0c\u53ef\u80fd\u5f71\u54cd\u5176\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002\u540c\u65f6\u8bc6\u522b\u51fa\u4e86\u6700\u6709\u5f71\u54cd\u529b\u7684\u9884\u6d4b\u7279\u5f81\u3002", "conclusion": "\u4e3a\u4e86\u63d0\u9ad8\u4e2d\u98ce\u98ce\u9669\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u786e\u5b9a\u7684\u5173\u952e\u7279\u5f81\u548c\u63d0\u51fa\u7684\u7b56\u7565\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.09967", "pdf": "https://arxiv.org/pdf/2505.09967", "abs": "https://arxiv.org/abs/2505.09967", "authors": ["Liqian Deng"], "title": "TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.", "AI": {"tldr": "A novel framework focusing on Texture Key Driver Factors (TKDF) for Facial Expression Recognition (FER) is introduced, which includes a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF). This method achieves state-of-the-art performance in FER tasks.", "motivation": "Facial expression recognition in the wild remains challenging due to subtle expression-related features and complex variations in facial appearance.", "method": "The proposed framework identifies Texture Key Driver Factors (TKDF), localized texture regions with strong discriminative power across emotional categories. It uses a Texture-Aware Feature Extractor (TAFE) with ResNet-based backbone enhanced with multi-branch attention to extract fine-grained texture representations, and Dual Contextual Information Filtering (DCIF) to refine these features through adaptive pooling and attention mechanisms.", "result": "Experimental results on RAF-DB and KDEF datasets show that the method achieves state-of-the-art performance.", "conclusion": "The incorporation of TKDFs into FER pipelines proves effective and robust, significantly improving FER performance."}}
{"id": "2505.10063", "pdf": "https://arxiv.org/pdf/2505.10063", "abs": "https://arxiv.org/abs/2505.10063", "authors": ["Han Peng", "Jinhao Jiang", "Zican Dong", "Wayne Xin Zhao", "Lei Fang"], "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability", "categories": ["cs.CL"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.", "AI": {"tldr": "The paper introduces CAFE, a two-stage coarse-to-fine method for enhancing multi-document question-answering by filtering and steering relevant documents, leading to significant performance improvements over baselines.", "motivation": "Existing methods struggle with balancing retrieval precision and recall in long-context inputs for LLMs, affecting their ability to answer questions effectively.", "method": "CAFE uses a coarse-grained filtering method to identify and rank relevant documents followed by a fine-grained steering method to guide attention to the most relevant content.", "result": "Experiments demonstrate that CAFE outperforms baselines, achieving up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods on the Mistral model respectively.", "conclusion": "CAFE successfully mitigates issues related to background and distracting documents, improving reliance on evidence documents and enhancing multi-document question-answering capabilities."}}
{"id": "2505.10278", "pdf": "https://arxiv.org/pdf/2505.10278", "abs": "https://arxiv.org/abs/2505.10278", "authors": ["Taian Guo", "Haiyang Shen", "Jinsheng Huang", "Zhengyang Mao", "Junyu Luo", "Zhuoru Chen", "Xuhui Liu", "Bingyu Xia", "Luchen Liu", "Yun Ma", "Ming Zhang"], "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction", "categories": ["cs.AI"], "comment": null, "summary": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.", "AI": {"tldr": "MASS is a multi-agent system for portfolio construction that achieves stable excess returns through large-scale simulations and end-to-end optimization, outperforming 6 state-of-the-art baselines in various experiments.", "motivation": "To overcome the limitations of existing multi-agent systems which are either pure simulations or constrained by predefined workflows, thus restricting their applicability and effectiveness.", "method": "Introduce MASS, which progressively increases the number of agents for large-scale simulations to enhance market understanding and optimizes agent distribution via a reverse optimization process instead of a fixed workflow.", "result": "Demonstrates superiority through multiple types of experiments compared with 6 state-of-the-art baselines on 3 challenging A-share stock pools.", "conclusion": "The paradigm established by MASS has the potential to be applied to other tasks with similar characteristics and has been open-sourced."}}
{"id": "2505.09820", "pdf": "https://arxiv.org/pdf/2505.09820", "abs": "https://arxiv.org/abs/2505.09820", "authors": ["Sajib Biswas", "Mao Nishino", "Samuel Jacob Chacko", "Xiuwen Liu"], "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent", "categories": ["cs.LG"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack", "AI": {"tldr": "This paper presents an intrinsic optimization technique using exponentiated gradient descent with the Bregman projection method for jailbreaking Large Language Models (LLMs). It ensures one-hot encoding stays within the probability simplex, proves its convergence, and demonstrates higher success rates and efficiency compared to other techniques.", "motivation": "To systematically understand LLMs and enhance their safety by developing a more effective method to conduct adversarial attacks or 'jailbreak' these models.", "method": "The method involves using exponentiated gradient descent combined with the Bregman projection method to optimize within the probability simplex. This approach aims to overcome limitations of discrete token search and continuous token embedding optimizations.", "result": "The proposed technique successfully jailbreaks several widely used LLMs with higher success rates and great efficiency on five open-source LLMs across four datasets, outperforming three state-of-the-art methods.", "conclusion": "The developed intrinsic optimization technique is proven to be effective in adversarial attacks on LLMs, demonstrating superior performance and efficiency."}}
{"id": "2505.09971", "pdf": "https://arxiv.org/pdf/2505.09971", "abs": "https://arxiv.org/abs/2505.09971", "authors": ["Yuan Gao", "Shaobo Xia", "Sheng Nie", "Cheng Wang", "Xiaohuan Xi", "Bisheng Yang"], "title": "APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "18 pages,12 figures", "summary": "Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.", "AI": {"tldr": "APCoTTA\u662f\u4e00\u79cd\u9488\u5bf9ALS\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u8fde\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94(CTTA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u3001\u71b5\u57fa\u4e00\u81f4\u6027\u635f\u5931\u548c\u968f\u673a\u53c2\u6570\u63d2\u503c\u673a\u5236\u7b49\u521b\u65b0\u624b\u6bb5\uff0c\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002\u5728\u65b0\u6784\u5efa\u7684ISPRSC\u548cH3DC\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cAPCoTTA\u76f8\u8f83\u4e8e\u76f4\u63a5\u63a8\u7406\u5206\u522b\u63d0\u5347\u4e86\u7ea69%\u548c14%\u7684mIoU\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684CTTA\u7814\u7a76\u5728ALS\u70b9\u4e91\u9886\u57df\u6709\u9650\uff0c\u9762\u4e34\u65e0\u6807\u51c6\u5316\u6570\u636e\u96c6\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u8bef\u5dee\u7d2f\u79ef\u7b49\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u57df\u4e0a\u7684\u9002\u5e94\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9ALS\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684CTTA\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPCoTTA\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1\uff09\u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u6a21\u5757\uff0c\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u9009\u62e9\u4f4e\u7f6e\u4fe1\u5ea6\u5c42\u8fdb\u884c\u8bad\u7ec3\u4ee5\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff1b2\uff09\u71b5\u57fa\u4e00\u81f4\u6027\u635f\u5931\uff0c\u4ec5\u5bf9\u53ef\u9760\u6837\u672c\u65bd\u52a0\u4e00\u81f4\u6027\u635f\u5931\u4ee5\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\uff1b3\uff09\u968f\u673a\u53c2\u6570\u63d2\u503c\u673a\u5236\uff0c\u5c06\u76ee\u6807\u57df\u9002\u5e94\u4e0e\u6e90\u77e5\u8bc6\u4fdd\u7559\u5e73\u8861\uff1b4\uff09\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u57fa\u51c6\u6570\u636e\u96c6ISPRSC\u548cH3DC\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPCoTTA\u5728ISPRSC\u548cH3DC\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u8f83\u4e8e\u76f4\u63a5\u63a8\u7406\u5206\u522b\u5b9e\u73b0\u4e86\u7ea69%\u548c14%\u7684mIoU\u63d0\u5347\u3002", "conclusion": "APCoTTA\u662f\u9996\u4e2a\u4e13\u4e3aALS\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u8bbe\u8ba1\u7684CTTA\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u5efa\u7684ISPRSC\u548cH3DC\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.10066", "pdf": "https://arxiv.org/pdf/2505.10066", "abs": "https://arxiv.org/abs/2505.10066", "authors": ["Michael Fire", "Yitzhak Elbazis", "Adi Wasenstein", "Lior Rokach"], "title": "Dark LLMs: The Growing Threat of Unaligned AI Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "68T50, 68T05, 68P25", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.", "AI": {"tldr": "The paper explores the vulnerability of Large Language Models (LLMs) to jailbreak attacks, revealing a universal attack that compromises multiple models and enables harmful outputs. Despite disclosure, industry responses have been inadequate, raising concerns about AI safety practices as LLMs proliferate.", "motivation": "To highlight the growing threat posed by dark LLMs\u2014models either deliberately designed without ethical guardrails or modified through jailbreak techniques\u2014and to address the inadequacies in current industry practices regarding AI safety.", "method": "Identification of a universal jailbreak attack that effectively compromises state-of-the-art LLMs, enabling them to produce harmful outputs upon request. Responsible disclosure of this attack to major LLM providers followed by analysis of their responses.", "result": "Discovery of a universal jailbreak attack that many tested LLMs were still vulnerable to, indicating significant gaps in current AI safety measures. Inadequate responses from major LLM providers further emphasize these gaps.", "conclusion": "As model training becomes more accessible and cheaper, the risk of misuse escalates. Decisive intervention is needed to prevent LLMs from democratizing access to dangerous knowledge."}}
{"id": "2505.10309", "pdf": "https://arxiv.org/pdf/2505.10309", "abs": "https://arxiv.org/abs/2505.10309", "authors": ["Tuan Dung Nguyen", "Duncan J. Watts", "Mark E. Whiting"], "title": "Empirically evaluating commonsense intelligence in large language models with large-scale human judgments", "categories": ["cs.AI", "cs.HC", "cs.SI"], "comment": null, "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.", "AI": {"tldr": "The paper proposes a new method to evaluate common sense in AI, specifically LLMs, by incorporating human heterogeneity. It finds that most LLMs are below human median in commonsense competence and smaller models are more competitive than larger ones.", "motivation": "Current benchmarks for assessing machine commonsense intelligence assume homogeneous human common sense, which is not accurate as humans vary greatly in what they consider commonsensical.", "method": "The method evaluates correspondence between model's judgment and that of a human population, treating LLMs both as independent survey respondents and as simulators of a hypothetical population.", "result": "Most LLMs remain below the human median in individual commonsense competence. LLMs correlate modestly with real humans when agreeing on statements. Smaller, open-weight models are more competitive than larger, proprietary models.", "conclusion": "The evaluation framework ties commonsense intelligence to its cultural basis and supports adapting AI models to human collectivities with different social knowledge."}}
{"id": "2505.09822", "pdf": "https://arxiv.org/pdf/2505.09822", "abs": "https://arxiv.org/abs/2505.09822", "authors": ["Changhao Shi", "Gal Mishne"], "title": "Learning Kronecker-Structured Graphs from Smooth Signals", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Graph learning, or network inference, is a prominent problem in graph signal\nprocessing (GSP). GSP generalizes the Fourier transform to non-Euclidean\ndomains, and graph learning is pivotal to applying GSP when these domains are\nunknown. With the recent prevalence of multi-way data, there has been growing\ninterest in product graphs that naturally factorize dependencies across\ndifferent ways. However, the types of graph products that can be learned are\nstill limited for modeling diverse dependency structures. In this paper, we\nstudy the problem of learning a Kronecker-structured product graph from smooth\nsignals. Unlike the more commonly used Cartesian product, the Kronecker product\nmodels dependencies in a more intricate, non-separable way, but posits harder\nconstraints on the graph learning problem. To tackle this non-convex problem,\nwe propose an alternating scheme to optimize each factor graph and provide\ntheoretical guarantees for its asymptotic convergence. The proposed algorithm\nis also modified to learn factor graphs of the strong product. We conduct\nexperiments on synthetic and real-world graphs and demonstrate our approach's\nefficacy and superior performance compared to existing methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u5e73\u6ed1\u4fe1\u53f7\u4e2d\u5b66\u4e60Kronecker\u7ed3\u6784\u5316\u4e58\u79ef\u56fe\u7684\u4ea4\u66ff\u4f18\u5316\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u4e0a\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u5bf9\u80fd\u591f\u81ea\u7136\u5206\u89e3\u4e0d\u540c\u65b9\u5f0f\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u4e58\u79ef\u56fe\u7684\u5174\u8da3\u65e5\u76ca\u6d53\u539a\uff0c\u4f46\u53ef\u7528\u4e8e\u5efa\u6a21\u591a\u6837\u5316\u4f9d\u8d56\u7ed3\u6784\u7684\u56fe\u4e58\u79ef\u7c7b\u578b\u4ecd\u7136\u6709\u9650\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5b66\u4e60Kronecker\u7ed3\u6784\u5316\u7684\u4e58\u79ef\u56fe\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3aKronecker\u4e58\u79ef\u4ee5\u66f4\u590d\u6742\u3001\u4e0d\u53ef\u5206\u7684\u65b9\u5f0f\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u975e\u51f8\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u65b9\u6848\u6765\u4f18\u5316\u6bcf\u4e2a\u56e0\u5b50\u56fe\uff0c\u5e76\u4e3a\u8be5\u65b9\u6848\u7684\u6e10\u8fd1\u6536\u655b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u4fee\u6539\u4e86\u8be5\u7b97\u6cd5\u4ee5\u5b66\u4e60\u5f3a\u4e58\u79ef\u7684\u56e0\u5b50\u56fe\u3002", "result": "\u901a\u8fc7\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4ea4\u66ff\u4f18\u5316\u65b9\u6848\u53ef\u4ee5\u6210\u529f\u5730\u5b66\u4e60Kronecker\u7ed3\u6784\u5316\u4e58\u79ef\u56fe\uff0c\u5e76\u4e14\u5728\u5904\u7406\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2505.09986", "pdf": "https://arxiv.org/pdf/2505.09986", "abs": "https://arxiv.org/abs/2505.09986", "authors": ["Yimin Zhou", "Yichong Xia", "Sicheng Pan", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.", "AI": {"tldr": "An underwater image compression algorithm HQUIC is developed, which leverages underwater-image-specific features for better performance.", "motivation": "Current underwater image compression algorithms do not fully utilize the unique characteristics of underwater images, leading to suboptimal performance.", "method": "HQUIC uses an ALTC module to predict attenuation coefficients and global light information, a codebook to extract common objects, and dynamically weights multi-scale frequency components.", "result": "HQUIC shows superior performance compared to state-of-the-art compression methods on various underwater datasets.", "conclusion": "HQUIC is an effective solution for underwater image compression that takes advantage of underwater-image-specific features."}}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081", "abs": "https://arxiv.org/abs/2505.10081", "authors": ["Wisdom Aduah", "Francois Meyer"], "title": "Designing and Contextualising Probes for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.", "AI": {"tldr": "Pretrained language models (PLMs) for African languages encode more linguistic information when adapted specifically for African languages, compared to massively multilingual PLMs. Token-level syntactic info is in middle-to-last layers and sentence-level semantic info across all layers.", "motivation": "To systematically investigate how pretrained language models capture linguistic knowledge specific to African languages.", "method": "Trained layer-wise probes on six typologically diverse African languages using the MasakhaPOS dataset with control tasks designed to interpret probe performance.", "result": "Adapted PLMs for African languages encode more linguistic information about target languages than massively multilingual PLMs. Token-level syntactic information is concentrated in middle-to-last layers while sentence-level semantic information is distributed across all layers.", "conclusion": "The study confirms that probe performance reflects internal knowledge of PLMs rather than memorization, applying established interpretability techniques to African-language PLMs and highlighting mechanisms behind successful strategies like active learning and multilingual adaptation."}}
{"id": "2505.10328", "pdf": "https://arxiv.org/pdf/2505.10328", "abs": "https://arxiv.org/abs/2505.10328", "authors": ["Alvin Combrink", "Stephie Do", "Kristofer Bengtsson", "Sabino Francesco Roselli", "Martin Fabian"], "title": "A Comparative Study of SMT and MILP for the Nurse Rostering Problem", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 3 figures", "summary": "The effects of personnel scheduling on the quality of care and working\nconditions for healthcare personnel have been thoroughly documented. However,\nthe ever-present demand and large variation of constraints make healthcare\nscheduling particularly challenging. This problem has been studied for decades,\nwith limited research aimed at applying Satisfiability Modulo Theories (SMT).\nSMT has gained momentum within the formal verification community in the last\ndecades, leading to the advancement of SMT solvers that have been shown to\noutperform standard mathematical programming techniques.\n  In this work, we propose generic constraint formulations that can model a\nwide range of real-world scheduling constraints. Then, the generic constraints\nare formulated as SMT and MILP problems and used to compare the respective\nstate-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired\nrostering problems. Experimental results show how each solver excels for\ncertain types of problems; the MILP solver generally performs better when the\nproblem is highly constrained or infeasible, while the SMT solver performs\nbetter otherwise. On real-world inspired problems containing a more varied set\nof shifts and personnel, the SMT solver excels. Additionally, it was noted\nduring experimentation that the SMT solver was more sensitive to the way the\ngeneric constraints were formulated, requiring careful consideration and\nexperimentation to achieve better performance. We conclude that SMT-based\nmethods present a promising avenue for future research within the domain of\npersonnel scheduling.", "AI": {"tldr": "The paper explores the application of Satisfiability Modulo Theories (SMT) in healthcare personnel scheduling, comparing it with Mixed Integer Linear Programming (MILP). It highlights the strengths and weaknesses of both methods based on problem constraints and concludes that SMT is a promising approach for future research.", "motivation": "Despite thorough documentation on how personnel scheduling affects care quality and working conditions, healthcare scheduling remains challenging due to high demand and constraint variability. While studied for decades, limited research has focused on applying SMT techniques.", "method": "Generic constraint formulations are created to model real-world scheduling constraints. These are then formulated as SMT and MILP problems to compare state-of-the-art solvers Z3 (SMT) and Gurobi (MILP) on academic and real-world inspired rostering problems.", "result": "Experimental results indicate that MILP performs better for highly constrained or infeasible problems, while SMT excels otherwise. On real-world problems with varied shifts and personnel, SMT outperforms MILP. However, SMT's performance is sensitive to how generic constraints are formulated.", "conclusion": "SMT-based methods offer a promising direction for future research in personnel scheduling."}}
{"id": "2505.09847", "pdf": "https://arxiv.org/pdf/2505.09847", "abs": "https://arxiv.org/abs/2505.09847", "authors": ["Liyang Zhao", "Olurotimi Seton", "Himadeep Reddy Reddivari", "Suvendu Jena", "Shadow Zhao", "Rachit Kumar", "Changshuai Wei"], "title": "Causal Predictive Optimization and Generation for Business AI", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "comment": null, "summary": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.", "AI": {"tldr": "This paper presents Causal Predictive Optimization and Generation, a three-layered approach to optimize the sales process in B2B businesses. It includes prediction with causal ML, optimization with constraint optimization and contextual bandit, and serving with Generative AI. The system was deployed in LinkedIn, showing significant improvements over legacy systems.", "motivation": "The motivation of this paper is to introduce a principled approach to optimize the sales process, which is crucial for the success of any B2B business.", "method": "The method involves a three-layered system: 1) Prediction layer using causal ML, 2) Optimization layer using constraint optimization and contextual bandit, 3) Serving layer using Generative AI and a feedback-loop for system enhancement.", "result": "The deployment of this system in LinkedIn showed significant wins over legacy systems.", "conclusion": "This work details the implementation and deployment of the Causal Predictive Optimization and Generation system in LinkedIn, demonstrating its effectiveness and sharing insights broadly applicable to the field of sales optimization and business AI."}}
{"id": "2505.09990", "pdf": "https://arxiv.org/pdf/2505.09990", "abs": "https://arxiv.org/abs/2505.09990", "authors": ["Long Cheng", "Jiafei Duan", "Yi Ru Wang", "Haoquan Fang", "Boyang Li", "Yushan Huang", "Elvis Wang", "Ainaz Eftekhar", "Jason Lee", "Wentao Yuan", "Rose Hendrix", "Noah A. Smith", "Fei Xia", "Dieter Fox", "Ranjay Krishna"], "title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing", "categories": ["cs.CV"], "comment": "10 Pages, Dataset and code:https://pointarena.github.io/", "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/", "AI": {"tldr": "Pointing is crucial for grounding language in visual contexts. PointArena evaluates multimodal pointing through Point-Bench, Point-Battle, and Point-Act. Molmo-72B leads but proprietary models are catching up. Supervised training boosts performance.", "motivation": "To create a comprehensive platform for evaluating multimodal pointing capabilities across diverse reasoning scenarios, addressing the limitation of existing benchmarks focusing only on referential object localization tasks.", "method": "Introduced PointArena comprising Point-Bench (1,000 pointing tasks), Point-Battle (interactive pairwise model comparisons with 4,500 votes), and Point-Act (real-world robotic manipulation system). Evaluated state-of-the-art open-source and proprietary multimodal models.", "result": "Molmo-72B outperforms other models, proprietary models show comparable performance. Supervised training improves pointing task performance. Strong correlations observed across evaluation stages.", "conclusion": "Precise pointing capabilities are essential for enabling multimodal models to bridge abstract reasoning with real-world actions."}}
{"id": "2505.10089", "pdf": "https://arxiv.org/pdf/2505.10089", "abs": "https://arxiv.org/abs/2505.10089", "authors": ["Wei Liu", "Sony Trenous", "Leonardo F. R. Ribeiro", "Bill Byrne", "Felix Hieber"], "title": "XRAG: Cross-lingual Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.", "AI": {"tldr": "A new benchmark named XRAG is proposed to evaluate LLMs' generation abilities in cross-lingual RAG settings. It highlights two challenges: response language correctness in monolingual retrieval and reasoning over retrieved information across languages in multilingual retrieval.", "motivation": "To address the evaluation of LLMs' generation abilities in scenarios where user language doesn't match the retrieval results, especially focusing on cross-lingual complexity.", "method": "XRAG is constructed from recent news articles ensuring questions require external knowledge. It covers both monolingual and multilingual retrieval scenarios with relevancy annotations.", "result": "Experimental results on five LLMs reveal two main challenges: 1) response language correctness in monolingual retrieval; 2) reasoning over retrieved information across languages in multilingual retrieval.", "conclusion": "XRAG serves as a valuable benchmark for studying LLM reasoning abilities, highlighting significant gaps between human and LLM performance in cross-lingual RAG settings."}}
{"id": "2505.10361", "pdf": "https://arxiv.org/pdf/2505.10361", "abs": "https://arxiv.org/abs/2505.10361", "authors": ["David Abel", "Michael Bowling", "Andr\u00e9 Barreto", "Will Dabney", "Shi Dong", "Steven Hansen", "Anna Harutyunyan", "Khimya Khetarpal", "Clare Lyle", "Razvan Pascanu", "Georgios Piliouras", "Doina Precup", "Jonathan Richens", "Mark Rowland", "Tom Schaul", "Satinder Singh"], "title": "Plasticity as the Mirror of Empowerment", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Our first finding is that\nplasticity is the mirror of empowerment: The agent's plasticity is identical to\nthe empowerment of the environment, and vice versa. Our second finding\nestablishes a tension between the plasticity and empowerment of an agent,\nsuggesting that agent design needs to be mindful of both characteristics. We\nexplore the implications of these findings, and suggest that plasticity,\nempowerment, and their relationship are essential to understanding agency.", "AI": {"tldr": "\u5728\u672c\u6587\u4e2d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u8bba\u5ea6\u91cf\u2014\u2014\u5e7f\u4e49\u5b9a\u5411\u4fe1\u606f\uff0c\u5b9a\u4e49\u4e86\u667a\u80fd\u4f53\u7684\u53ef\u5851\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4e0e\u6388\u6743\u4e4b\u95f4\u7684\u57fa\u672c\u8054\u7cfb\u3002\u7814\u7a76\u53d1\u73b0\u53ef\u5851\u6027\u662f\u6388\u6743\u7684\u955c\u50cf\uff1a\u667a\u80fd\u4f53\u7684\u53ef\u5851\u6027\u7b49\u540c\u4e8e\u73af\u5883\u7684\u6388\u6743\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u540c\u65f6\uff0c\u8fd8\u5b58\u5728\u667a\u80fd\u4f53\u53ef\u5851\u6027\u548c\u6388\u6743\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u8fd9\u610f\u5473\u7740\u5728\u8bbe\u8ba1\u667a\u80fd\u4f53\u65f6\u9700\u8981\u517c\u987e\u8fd9\u4e24\u4e2a\u7279\u6027\u3002", "motivation": "\u76ee\u524d\u5bf9\u4e8e\u667a\u80fd\u4f53\u7684\u7814\u7a76\u5927\u591a\u5173\u6ce8\u5176\u5f71\u54cd\u672a\u6765\u89c2\u5bdf\u7684\u80fd\u529b\uff08\u5373\u6388\u6743\uff09\uff0c\u4f46\u5bf9\u667a\u80fd\u4f53\u53d7\u8fc7\u53bb\u89c2\u5bdf\u5f71\u54cd\u7684\u80fd\u529b\uff08\u5373\u53ef\u5851\u6027\uff09\u5c1a\u672a\u6709\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u8bd5\u56fe\u5b9a\u4e49\u548c\u91cf\u5316\u667a\u80fd\u4f53\u7684\u53ef\u5851\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u4e0e\u6388\u6743\u7684\u5173\u7cfb\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u4e00\u7cfb\u5217\u671f\u671b\u7684\u6027\u8d28\u5b9a\u4e49\u4e86\u53ef\u5851\u6027\uff0c\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u8bba\u91cf\u2014\u2014\u5e7f\u4e49\u5b9a\u5411\u4fe1\u606f\u6765\u8861\u91cf\u3002\u8be5\u91cf\u4e25\u683c\u63a8\u5e7f\u4e86Massey\uff081990\uff09\u5f15\u5165\u7684\u5b9a\u5411\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u6240\u6709\u826f\u597d\u6027\u8d28\u3002\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\uff0c\u4f5c\u8005\u63ed\u793a\u4e86\u53ef\u5851\u6027\u548c\u6388\u6743\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e24\u4e2a\u91cd\u8981\u7ed3\u679c\uff1a\u4e00\u662f\u53ef\u5851\u6027\u662f\u6388\u6743\u7684\u955c\u50cf\uff0c\u5373\u667a\u80fd\u4f53\u7684\u53ef\u5851\u6027\u7b49\u4e8e\u73af\u5883\u7684\u6388\u6743\uff0c\u53cd\u4e4b\u4ea6\u7136\uff1b\u4e8c\u662f\u667a\u80fd\u4f53\u7684\u53ef\u5851\u6027\u548c\u6388\u6743\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\uff0c\u8fd9\u8868\u660e\u5728\u8bbe\u8ba1\u667a\u80fd\u4f53\u65f6\u9700\u8981\u6743\u8861\u8fd9\u4e24\u8005\u3002", "conclusion": "\u53ef\u5851\u6027\u548c\u6388\u6743\u53ca\u5176\u5173\u7cfb\u5bf9\u4e8e\u7406\u89e3\u667a\u80fd\u4f53\u7684\u672c\u8d28\u81f3\u5173\u91cd\u8981\u3002\u5728\u672a\u6765\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e2d\uff0c\u5e94\u5145\u5206\u8003\u8651\u8fd9\u4e24\u79cd\u7279\u6027\u4ee5\u5b9e\u73b0\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09848", "pdf": "https://arxiv.org/pdf/2505.09848", "abs": "https://arxiv.org/abs/2505.09848", "authors": ["Aditya Raj", "Golrokh Mirzaei"], "title": "Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection", "categories": ["cs.LG", "eess.IV"], "comment": "11 pages", "summary": "Imaging and genomic data offer distinct and rich features, and their\nintegration can unveil new insights into the complex landscape of diseases. In\nthis study, we present a novel approach utilizing radiogenomic data including\nstructural MRI images and gene expression data, for Alzheimer's disease\ndetection. Our framework introduces a novel heterogeneous bipartite graph\nrepresentation learning featuring two distinct node types: genes and images.\nThe network can effectively classify Alzheimer's disease (AD) into three\ndistinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)\nclasses, utilizing a small dataset. Additionally, it identified which genes\nplay a significant role in each of these classification groups. We evaluate the\nperformance of our approach using metrics including classification accuracy,\nrecall, precision, and F1 score. The proposed technique holds potential for\nextending to radiogenomic-based classification to other diseases.", "AI": {"tldr": "This paper presents a novel approach for Alzheimer's disease detection using radiogenomic data, including structural MRI images and gene expression data, with a heterogeneous bipartite graph representation learning framework that can classify AD into three stages and identify significant genes.", "motivation": "To integrate imaging and genomic data to gain new insights into diseases and demonstrate the potential of radiogenomic-based classification for other diseases.", "method": "A heterogeneous bipartite graph representation learning framework featuring two distinct node types (genes and images) is used to classify Alzheimer's disease into three stages (AD, MCI, CN) with a small dataset.", "result": "The network effectively classifies Alzheimer's disease into three distinct stages and identifies significant genes in each classification group, evaluated by metrics such as accuracy, recall, precision, and F1 score.", "conclusion": "The proposed technique shows potential for extending radiogenomic-based classification to other diseases."}}
{"id": "2505.09997", "pdf": "https://arxiv.org/pdf/2505.09997", "abs": "https://arxiv.org/abs/2505.09997", "authors": ["Jinhyun Jang", "Jiyeong Lee", "Kwanghoon Sohn"], "title": "Descriptive Image-Text Matching with Graded Contextual Similarity", "categories": ["cs.CV"], "comment": null, "summary": "Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.", "AI": {"tldr": "The paper introduces DITM, a method for descriptive image-text matching that explores language flexibility to improve learning of graded contextual similarity.", "motivation": "Most existing image-text matching approaches use sparse binary supervision which limits their ability to cover the many-to-many relationships between images and texts. They also neglect implicit connections from general to specific descriptions.", "method": "DITM formulates descriptiveness score with cumulative term frequency-inverse document frequency (TF-IDF) and leverages sentence descriptiveness in two ways: refining false negative labeling and building more precise matching by aligning sentences in a generic-to-specific order.", "result": "Experiments on MS-COCO, Flickr30K, and CxC datasets show DITM's effectiveness in representing complex image-text relationships compared to state-of-the-art methods. It also enhances hierarchical reasoning as shown by analysis on HierarCaps benchmark.", "conclusion": "DITM successfully moves beyond rigid binary supervision to enhance discovery of optimal matches and potential positive pairs, improving both image-text matching and hierarchical reasoning."}}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113", "abs": "https://arxiv.org/abs/2505.10113", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.", "AI": {"tldr": "The paper introduces S-MedQA, an English medical QA dataset for evaluating large language models in clinical specialties. It challenges the hypothesis that training on specialty data leads to best performance and suggests domain shifting is more impactful than knowledge injection.", "motivation": "To benchmark large language models' performance in fine-grained clinical specialties and evaluate the hypothesis related to knowledge injection in medical QA.", "method": "Introduced S-MedQA dataset and used it to test model performance across different clinical specialties after fine-tuning.", "result": "1) Training on specialty data does not necessarily lead to the best performance on that specialty; 2) Token probabilities of clinically relevant terms increase consistently regardless of the specialty fine-tuned on.", "conclusion": "Improvement gains come mostly from domain shifting rather than knowledge injection, suggesting a rethinking of the role of fine-tuning data in the medical domain."}}
{"id": "2505.10399", "pdf": "https://arxiv.org/pdf/2505.10399", "abs": "https://arxiv.org/abs/2505.10399", "authors": ["Kaivalya Rawal", "Zihao Fu", "Eoin Delaney", "Chris Russell"], "title": "Evaluating Model Explanations without Ground Truth", "categories": ["cs.AI", "cs.LG", "I.2.6"], "comment": "https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth", "summary": "There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.", "AI": {"tldr": "The paper identifies limitations in current model explanation evaluation methods and proposes AXE, a ground-truth Agnostic eXplanation Evaluation framework that independently measures explanation quality without needing ideal ground-truth explanations or model sensitivity.", "motivation": "There are many competing and contradictory explanations for a single model prediction, making it hard to select the appropriate one. Current frameworks evaluate explanation quality by comparing against ideal 'ground-truth' explanations or verifying model sensitivity, but these approaches have limitations.", "method": "The authors propose three principles for future explanation evaluation strategies and introduce AXE (ground-truth Agnostic eXplanation Evaluation), which evaluates and compares model explanations while not requiring access to ideal ground-truth explanations or relying on model sensitivity.", "result": "AXE is verified by comparison with baselines and demonstrates its ability to detect explanation fairwashing.", "conclusion": "AXE provides an independent measure of explanation quality and does not need ideal ground-truth explanations or model sensitivity."}}
{"id": "2505.09851", "pdf": "https://arxiv.org/pdf/2505.09851", "abs": "https://arxiv.org/abs/2505.09851", "authors": ["Shun Wang", "Shun-Li Shang", "Zi-Kui Liu", "Wenrui Hao"], "title": "ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "9 pages, 4 figures", "summary": "Traditional entropy-based methods - such as cross-entropy loss in\nclassification problems - have long been essential tools for quantifying\nuncertainty and disorder in data and developing artificial intelligence\nalgorithms. However, the rapid growth of data across various domains has\nintroduced new challenges, particularly the integration of heterogeneous\ndatasets with intrinsic disparities. In this paper, we extend zentropy theory\ninto the data science domain by introducing intrinsic entropy, enabling more\neffective learning from heterogeneous data sources. We propose a\nzentropy-enhanced neural network (ZENN) that simultaneously learns both energy\nand intrinsic entropy components, capturing the underlying structure of\nmulti-source data. To support this, we redesign the neural network architecture\nto better reflect the intrinsic properties and variability inherent in diverse\ndatasets. We demonstrate the effectiveness of ZENN on classification tasks and\nenergy landscape reconstructions, showing its superior generalization\ncapabilities and robustness-particularly in predicting high-order derivatives.\nAs a practical application, we employ ZENN to reconstruct the Helmholtz energy\nlandscape of Fe3Pt using data generated from DFT and capture key material\nbehaviors, including negative thermal expansion and the critical point in the\ntemperature-pressure space. Overall, our study introduces a novel approach for\ndata-driven machine learning grounded in zentropy theory, highlighting ZENN as\na versatile and robust deep learning framework for scientific problems\ninvolving complex, heterogeneous datasets.", "AI": {"tldr": "The paper introduces intrinsic entropy via zentropy theory and proposes a ZENN model for handling heterogeneous data, demonstrating its effectiveness in classification tasks, energy landscape reconstructions, and material behavior predictions.", "motivation": "To address the challenge of integrating heterogeneous datasets with intrinsic disparities in various domains.", "method": "Extending zentropy theory by introducing intrinsic entropy and proposing a zentropy-enhanced neural network (ZENN) that learns both energy and intrinsic entropy components. The neural network architecture is redesigned to reflect the intrinsic properties and variability in diverse datasets.", "result": "ZENN shows superior generalization capabilities and robustness in classification tasks and energy landscape reconstructions, especially in predicting high-order derivatives. It successfully reconstructs the Helmholtz energy landscape of Fe3Pt and captures key material behaviors.", "conclusion": "The study presents a novel data-driven machine learning approach based on zentropy theory, showcasing ZENN as a versatile deep learning framework for scientific problems involving complex, heterogeneous datasets."}}
{"id": "2505.09998", "pdf": "https://arxiv.org/pdf/2505.09998", "abs": "https://arxiv.org/abs/2505.09998", "authors": ["Ying Zang", "Yuanqi Hu", "Xinyu Chen", "Yuxia Xu", "Suhui Wang", "Chunan Yu", "Lanyun Zhu", "Deyi Ji", "Xin Xu", "Tianrun Chen"], "title": "From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.", "AI": {"tldr": "In the age of immersive electronics, there's a growing interest in virtual fashion for identity expression. However, current 3D garment design tools are difficult for everyday users due to technical challenges and limited data. This paper presents a 3D sketch-driven framework that allows even inexperienced users to generate high-quality digital clothing via simple 3D sketches in AR/VR environments. The system uses a conditional diffusion model, a sketch encoder, and adaptive curriculum learning to interpret rough inputs and produce realistic garments. Additionally, they introduce KO3DClothes, a new dataset to tackle the lack of training data. Experiments show that this method surpasses existing ones in fidelity and usability, promoting accessible fashion design on future platforms.", "motivation": "The motivation behind this work is to make 3D garment design more accessible to everyday users who want to express their identity through virtual fashion, overcoming the steep technical barriers and limited data that hinder current tools.", "method": "The method involves creating a 3D sketch-driven 3D garment generation framework. It combines a conditional diffusion model with a sketch encoder trained in a shared latent space and an adaptive curriculum learning strategy to translate imprecise, free-hand input into realistic, personalized garments. A new dataset, KO3DClothes, consisting of paired 3D garments and user-created sketches, was also introduced to address the scarcity of training data.", "result": "The results from extensive experiments and user studies indicate that the proposed method significantly outperforms existing baselines in terms of both fidelity and usability. This highlights the potential of the method to democratize fashion design on next-generation consumer platforms.", "conclusion": "This work concludes by demonstrating the effectiveness of the 3D sketch-driven 3D garment generation framework in empowering ordinary users to create high-quality digital clothing. By introducing KO3DClothes and showing superior performance compared to existing methods, the study paves the way for more inclusive and accessible fashion design experiences."}}
{"id": "2505.10143", "pdf": "https://arxiv.org/pdf/2505.10143", "abs": "https://arxiv.org/abs/2505.10143", "authors": ["Longchao Da", "Parth Mitesh Shah", "Kuan-Ru Liou", "Jiaxing Zhang", "Hua Wei"], "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4; H.3.3"], "comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track", "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.", "AI": {"tldr": "Large Language Models (LLMs) are essential in human decision-making, but their outputs can be unreliable. This paper introduces GE-Chat, a framework that uses knowledge graphs and retrieval-augmented generation to provide evidence-based responses, improving the reliability of LLMs.", "motivation": "To address the issue of untrustworthy responses from LLMs that may contain mistakes or plausible yet incorrect information, leading to complications and trust issues among users.", "method": "GE-Chat utilizes a knowledge graph enhanced retrieval-augmented generation framework. When a user uploads a document, a knowledge graph is created to support a retrieval-augmented agent. The method employs Chain-of-Thought logic generation, n-hop sub-graph searching, and entailment-based sentence generation for accurate evidence retrieval.", "result": "The proposed method enhances the ability to identify exact evidence in a free-form context, providing a reliable way to examine the resources behind LLM conclusions and aiding in assessing their trustworthiness.", "conclusion": "GE-Chat offers an effective solution to improve the reliability of LLMs by generating evidence-based responses, thereby helping users make more informed decisions."}}
{"id": "2505.10468", "pdf": "https://arxiv.org/pdf/2505.10468", "abs": "https://arxiv.org/abs/2505.10468", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "categories": ["cs.AI"], "comment": "32 pages, 14 figures, 11 tables", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "AI": {"tldr": "This study distinguishes AI Agents from Agentic AI, offering a taxonomy and analysis of their design philosophies, capabilities, applications, and challenges.", "motivation": "To clarify the differences between AI Agents and Agentic AI in terms of design philosophies and capabilities by providing a structured conceptual taxonomy, application mapping, and challenge analysis.", "method": "Outlining search strategy and foundational definitions; characterizing AI Agents as modular systems driven by LLMs and LIMs for task-specific automation; positioning generative AI as precursor to AI Agents advancing through tool integration, prompt engineering, reasoning enhancements; presenting comparative analysis across both paradigms including architectural evolution, operational mechanisms, interaction styles, autonomy levels.", "result": "A clear distinction between AI Agents and Agentic AI systems was established with Agentic AI representing a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Challenges such as hallucination, brittleness, emergent behavior, and coordination failure were examined and targeted solutions proposed.", "conclusion": "The work provides a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems."}}
{"id": "2505.09854", "pdf": "https://arxiv.org/pdf/2505.09854", "abs": "https://arxiv.org/abs/2505.09854", "authors": ["Harikrishna Kuttivelil", "Katia Obraczka"], "title": "Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence", "categories": ["cs.LG", "cs.ET", "cs.MA", "cs.SI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "As demand for intelligent services rises and edge devices become more\ncapable, distributed learning at the network edge has emerged as a key enabling\ntechnology. While existing paradigms like federated learning (FL) and\ndecentralized FL (DFL) enable privacy-preserving distributed learning in many\nscenarios, they face potential challenges in connectivity and synchronization\nimposed by resource-constrained and infrastructure-less environments. While\nmore robust, gossip learning (GL) algorithms have generally been designed for\nhomogeneous data distributions and may not suit all contexts. This paper\nintroduces Chisme, a novel suite of protocols designed to address the\nchallenges of implementing robust intelligence in the network edge,\ncharacterized by heterogeneous data distributions, episodic connectivity, and\nlack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and\nasynchronous GL (Chisme-GL) variants that enable collaborative yet\ndecentralized model training that considers underlying data heterogeneity. We\nintroduce a data similarity heuristic that allows agents to opportunistically\ninfer affinity with each other using the existing communication of model\nupdates in decentralized FL and GL. We leverage the heuristic to extend DFL's\nmodel aggregation and GL's model merge mechanisms for better personalized\ntraining while maintaining collaboration. While Chisme-DFL is a synchronous\ndecentralized approach whose resource utilization scales linearly with network\nsize, Chisme-GL is fully asynchronous and has a lower, constant resource\nrequirement independent of network size. We demonstrate that Chisme methods\noutperform their standard counterparts in model training over distributed and\nheterogeneous data in network scenarios ranging from less connected and\nreliable networks to fully connected and lossless networks.", "AI": {"tldr": "Chisme is a new suite of protocols for robust intelligence at the network edge, including Chisme-DFL and Chisme-GL, which outperform standard methods in distributed and heterogeneous data model training.", "motivation": "To address challenges in implementing robust intelligence at the network edge, such as heterogeneous data distributions, episodic connectivity, and lack of infrastructure.", "method": "Introduced Chisme-DFL (synchronous decentralized FL) and Chisme-GL (asynchronous gossip learning), along with a data similarity heuristic that allows agents to infer affinity with each other using model updates. The heuristic extends DFL's model aggregation and GL's model merge mechanisms for better personalized training while maintaining collaboration.", "result": "Chisme methods outperform standard counterparts in model training over distributed and heterogeneous data across various network scenarios.", "conclusion": "Chisme provides effective solutions for privacy-preserving distributed learning at the network edge under challenging conditions."}}
{"id": "2505.10016", "pdf": "https://arxiv.org/pdf/2505.10016", "abs": "https://arxiv.org/abs/2505.10016", "authors": ["Shijie Lyu"], "title": "Application of YOLOv8 in monocular downward multiple Car Target detection", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "comment": "Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering", "summary": "Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.", "AI": {"tldr": "An improved autonomous target detection network based on YOLOv8 is presented to overcome challenges in object detection for autonomous driving, achieving 65% detection accuracy.", "motivation": "Current technologies for environmental perception in autonomous driving face challenges such as high costs, vulnerability to weather and lighting conditions, and limited resolution.", "method": "The paper integrates structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework.", "result": "The enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showing significant improvements over traditional methods.", "conclusion": "This improved model has substantial potential for real-world applications, particularly excelling in scenarios involving single-target and small-object detection."}}
{"id": "2505.10182", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.", "AI": {"tldr": "Reasoning CPT uses synthetic data to reconstruct hidden thought processes, improving model performance across domains and problem difficulties.", "motivation": "To explore the effectiveness of synthesizing training data for reasoning and its impact on a wide range of domains, addressing the limitations of task-specific signals in supervised fine-tuning and reinforcement learning.", "method": "Apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark.", "result": "Reasoning CPT consistently improves performance across all evaluated domains, with notable gains in challenging problems and effective transfer of reasoning skills between domains.", "conclusion": "Reasoning CPT is an effective method for enhancing reasoning capabilities in language models, allowing them to adjust reasoning depth according to problem difficulty."}}
{"id": "2505.10543", "pdf": "https://arxiv.org/pdf/2505.10543", "abs": "https://arxiv.org/abs/2505.10543", "authors": ["Annie Wong", "Thomas B\u00e4ck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "title": "Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f5c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u5b66\u4e60\u548c\u63a8\u7406\u4ee3\u7406\u7684\u771f\u6b63\u6f5c\u529b\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u81ea\u6211\u53cd\u601d\u3001\u542f\u53d1\u5f0f\u53d8\u5f02\u548c\u89c4\u5212\u7b49\u63d0\u793a\u6280\u672f\u7684\u6548\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8f83\u5927\u7684\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u7b56\u7565\u6027\u63d0\u793a\u53ef\u4ee5\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002\u8fc7\u957f\u7684\u63d0\u793a\u5bf9\u5c0f\u578b\u6a21\u578b\u7684\u57fa\u672c\u53cd\u5e94\u4efb\u52a1\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u5927\u578b\u6a21\u578b\u8868\u73b0\u66f4\u7a33\u5065\u3002\u9ad8\u7ea7\u63d0\u793a\u6280\u672f\u4e3b\u8981\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u590d\u6742\u6e38\u620f\u4e2d\u53d7\u76ca\uff0c\u4f46\u5bf9\u5df2\u7ecf\u9ad8\u6027\u80fd\u7684\u5927\u6a21\u578b\u6539\u8fdb\u8f83\u5c11\u3002\u5148\u8fdb\u7684\u63a8\u7406\u65b9\u6cd5\u7ed3\u679c\u9ad8\u5ea6\u53ef\u53d8\uff0c\u867d\u7136\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u4e5f\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\u3002\u4e0e\u4eba\u7c7b\u8868\u73b0\u76f8\u6bd4\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u3001\u63a8\u7406\u548c\u7a7a\u95f4\u534f\u8c03\u7b49\u5173\u952e\u9886\u57df\u5b58\u5728\u6301\u7eed\u7684\u5c40\u9650\u6027\uff0c\u8868\u660e\u4ec5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u63d0\u793a\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u514b\u670d\u8fd9\u4e9b\u6839\u672c\u7f3a\u9677\u3002\u63a8\u7406\u662f\u4e00\u4e2a\u591a\u65b9\u9762\u7684\u4efb\u52a1\uff0c\u9700\u8981\u8d85\u8d8a\u9759\u6001\u57fa\u51c6\u6765\u6355\u6349\u63a8\u7406\u7684\u590d\u6742\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u5b66\u4e60\u548c\u63a8\u7406\u4ee3\u7406\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u4e0d\u540c\u63d0\u793a\u6280\u672f\u5bf9\u5176\u9002\u5e94\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u81ea\u6211\u53cd\u601d\u3001\u542f\u53d1\u5f0f\u53d8\u5f02\u548c\u89c4\u5212\u7b49\u63d0\u793a\u6280\u672f\u5bf9\u4e0d\u540c\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u5176\u4e0e\u4eba\u7c7b\u7684\u8868\u73b0\u3002", "result": "\u8f83\u5927\u7684\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u7b56\u7565\u6027\u63d0\u793a\u53ef\u4ee5\u7f29\u5c0f\u5dee\u8ddd\uff1b\u8fc7\u957f\u7684\u63d0\u793a\u5bf9\u5c0f\u578b\u6a21\u578b\u4e0d\u5229\uff1b\u9ad8\u7ea7\u63d0\u793a\u6280\u672f\u4e3b\u8981\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u590d\u6742\u6e38\u620f\u4e2d\u53d7\u76ca\uff1b\u5148\u8fdb\u7684\u63a8\u7406\u65b9\u6cd5\u7ed3\u679c\u53ef\u53d8\uff0c\u53ef\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\u6216\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524d\u4e00\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u3001\u63a8\u7406\u548c\u7a7a\u95f4\u534f\u8c03\u7b49\u5173\u952e\u9886\u57df\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u4ec5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u63d0\u793a\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002\u9700\u8981\u8d85\u8d8a\u9759\u6001\u57fa\u51c6\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.09855", "pdf": "https://arxiv.org/pdf/2505.09855", "abs": "https://arxiv.org/abs/2505.09855", "authors": ["Alexander Y. Ku", "Thomas L. Griffiths", "Stephanie C. Y. Chan"], "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.", "AI": {"tldr": "Transformer\u6a21\u578b\u901a\u8fc7\u6743\u91cd\u5185\u5b66\u4e60(IWL)\u548c\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60(ICL)\u4e24\u79cd\u6a21\u5f0f\u83b7\u53d6\u77e5\u8bc6\u3002\u53d7\u8fdb\u5316\u751f\u7269\u5b66\u542f\u53d1\uff0c\u7814\u7a76\u63a2\u8ba8\u4e86\u73af\u5883\u53ef\u9884\u6d4b\u6027\u5bf9\u8fd9\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u5e73\u8861\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u65f6\u95f4\u6f14\u5316\u89c4\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5bf9\u6210\u672c\u5047\u8bf4\u6765\u89e3\u91ca\u8fd9\u4e9b\u8f6c\u53d8\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3Transformer\u6a21\u578b\u4e2dIWL\u4e0eICL\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u7814\u7a76\u4ece\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u83b7\u5f97\u7075\u611f\uff0c\u5c06IWL\u7c7b\u6bd4\u4e3a\u9057\u4f20\u7f16\u7801\uff0cICL\u7c7b\u6bd4\u4e3a\u8868\u578b\u53ef\u5851\u6027\uff0c\u5e76\u63a2\u7d22\u73af\u5883\u53ef\u9884\u6d4b\u6027\u5982\u4f55\u5f71\u54cd\u8fd9\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u7684\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u5b9e\u9a8c\u64cd\u4f5c\u73af\u5883\u53ef\u9884\u6d4b\u6027\u7684\u7ef4\u5ea6\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u5176\u5bf9IWL/ICL\u5e73\u8861\u7684\u5f71\u54cd\u3002\u5206\u6790\u9ad8\u73af\u5883\u7a33\u5b9a\u6027\u5bf9IWL\u7684\u504f\u597d\u4ee5\u53ca\u9ad8\u7ebf\u7d22\u53ef\u9760\u6027\u5bf9ICL\u6548\u80fd\u7684\u63d0\u5347\uff0c\u5e76\u89c2\u5bdf\u4e0d\u540c\u4efb\u52a1\u6761\u4ef6\u4e0b\u7684\u65f6\u95f4\u6f14\u5316\u89c4\u5f8b\u3002", "result": "\u9ad8\u73af\u5883\u7a33\u5b9a\u6027\u663e\u8457\u4fc3\u8fdbIWL\uff0c\u800c\u4f4e\u7a33\u5b9a\u6027\u4e0b\u9ad8\u7ebf\u7d22\u53ef\u9760\u6027\u589e\u5f3aICL\u6548\u80fd\uff1b\u4efb\u52a1\u96be\u6613\u5ea6\u548c\u5b66\u4e60\u52a8\u6001\u51b3\u5b9a\u4e86IWL\u4e0eICL\u7684\u8f6c\u6362\u8fc7\u7a0b\uff0c\u652f\u6301\u4e86\u76f8\u5bf9\u6210\u672c\u5047\u8bf4\u3002", "conclusion": "\u73af\u5883\u53ef\u9884\u6d4b\u6027\u662f\u51b3\u5b9aTransformer\u6a21\u578b\u9002\u5e94\u7b56\u7565\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3ICL\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6307\u5bfc\u4e86\u8bad\u7ec3\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.10027", "pdf": "https://arxiv.org/pdf/2505.10027", "abs": "https://arxiv.org/abs/2505.10027", "authors": ["Shijie Lyu"], "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)", "summary": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.", "AI": {"tldr": "This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution, demonstrating significant improvements in PSNR, SSIM, and LPIPS metrics.", "motivation": "Super-resolution image reconstruction is crucial with the development of remote sensing technology. Current deep learning methods struggle with complex scenes and detail preservation.", "method": "A reinforcement learning-based latent diffusion model (LDM) fine-tuning method is proposed. It constructs a reinforcement learning environment optimized by proximal policy optimization (PPO) during the reverse denoising process of the LDM model.", "result": "Experiments on the RESISC45 dataset showed improvements over the baseline model: PSNR increased by 3-4dB, SSIM improved by 0.08-0.11, and LPIPS reduced by 0.06-0.10, especially in structured and complex natural scenes.", "conclusion": "The proposed method effectively enhances super-resolution quality and demonstrates adaptability across various scenes."}}
{"id": "2505.10185", "pdf": "https://arxiv.org/pdf/2505.10185", "abs": "https://arxiv.org/abs/2505.10185", "authors": ["Seongyun Lee", "Seungone Kim", "Minju Seo", "Yongrae Jo", "Dongyoung Go", "Hyeonbin Hwang", "Jinho Park", "Xiang Yue", "Sean Welleck", "Graham Neubig", "Moontae Lee", "Minjoon Seo"], "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.", "AI": {"tldr": "The paper introduces CoT Encyclopedia, a framework that automatically extracts and clusters reasoning criteria from model-generated CoTs to produce more interpretable analyses. It also shows performance gains by guiding models toward better strategies.", "motivation": "Current understanding of reasoning strategies in large language models is limited, and predefined categorization methods are constrained by human intuition, failing to capture the full diversity of model behaviors.", "method": "The method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior.", "result": "Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods, can predict which strategy a model is likely to use, and guide it toward more effective alternatives.", "conclusion": "The CoT Encyclopedia provides practical insights, such as the significant impact of training data format on reasoning behavior compared to data domain."}}
{"id": "2306.07615", "pdf": "https://arxiv.org/pdf/2306.07615", "abs": "https://arxiv.org/abs/2306.07615", "authors": ["Heqin Zhu", "Quan Quan", "Qingsong Yao", "Zaiyi Liu", "S. Kevin Zhou"], "title": "UOD: Universal One-shot Detection of Anatomical Landmarks", "categories": ["cs.CV", "cs.AI"], "comment": "Eealy accepted by MICCAI 2023. 11pages, 4 figures, 2 tables. arXiv\n  admin note: text overlap with arXiv:2203.06433", "summary": "One-shot medical landmark detection gains much attention and achieves great\nsuccess for its label-efficient training process. However, existing one-shot\nlearning methods are highly specialized in a single domain and suffer domain\npreference heavily in the situation of multi-domain unlabeled data. Moreover,\none-shot learning is not robust that it faces performance drop when annotating\na sub-optimal image. To tackle these issues, we resort to developing a\ndomain-adaptive one-shot landmark detection framework for handling multi-domain\nmedical images, named Universal One-shot Detection (UOD). UOD consists of two\nstages and two corresponding universal models which are designed as\ncombinations of domain-specific modules and domain-shared modules. In the first\nstage, a domain-adaptive convolution model is self-supervised learned to\ngenerate pseudo landmark labels. In the second stage, we design a\ndomain-adaptive transformer to eliminate domain preference and build the global\ncontext for multi-domain data. Even though only one annotated sample from each\ndomain is available for training, the domain-shared modules help UOD aggregate\nall one-shot samples to detect more robust and accurate landmarks. We\ninvestigated both qualitatively and quantitatively the proposed UOD on three\nwidely-used public X-ray datasets in different anatomical domains (i.e., head,\nhand, chest) and obtained state-of-the-art performances in each domain. The\ncode is available at\nhttps://github.com/heqin-zhu/UOD_universal_oneshot_detection.", "AI": {"tldr": "This paper proposes a domain-adaptive one-shot landmark detection framework named Universal One-shot Detection (UOD) to handle multi-domain medical images. It consists of two stages and corresponding universal models, achieving state-of-the-art performances on three public X-ray datasets.", "motivation": "One-shot medical landmark detection has label-efficient training process but existing methods suffer from domain preference when dealing with multi-domain unlabeled data and are not robust for sub-optimal image annotation.", "method": "The UOD framework includes two stages: 1) A domain-adaptive convolution model is self-supervised learned to generate pseudo landmark labels; 2) A domain-adaptive transformer is designed to eliminate domain preference and build the global context for multi-domain data. The framework uses domain-specific and domain-shared modules.", "result": "UOD was investigated both qualitatively and quantitatively on three widely-used public X-ray datasets in different anatomical domains (head, hand, chest) and achieved state-of-the-art performances in each domain.", "conclusion": "The proposed UOD framework can effectively handle multi-domain medical images with only one annotated sample from each domain, providing more robust and accurate landmark detection."}}
{"id": "2505.09861", "pdf": "https://arxiv.org/pdf/2505.09861", "abs": "https://arxiv.org/abs/2505.09861", "authors": ["John Bencina", "Erkut Aykutlug", "Yue Chen", "Zerui Zhang", "Stephanie Sorenson", "Shao Tang", "Changshuai Wei"], "title": "LiDDA: Data Driven Attribution at LinkedIn", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ME"], "comment": null, "summary": "Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.", "AI": {"tldr": "The paper presents a unified transformer-based approach for data driven attribution that can process different types of data and external factors, with significant impact shown through its implementation at LinkedIn.", "motivation": "Data Driven Attribution is crucial for modern marketing intelligence, requiring advanced methods to assign conversion credits based on causal patterns learned from data.", "method": "A unified transformer-based attribution approach is introduced, capable of handling member-level data, aggregate-level data, and integrating external macro factors.", "result": "The approach was successfully implemented at large scale at LinkedIn, demonstrating significant impact, with learnings applicable broadly to marketing and ad tech fields.", "conclusion": "The transformer-based attribution method offers an effective solution for data driven attribution, showcasing potential for broad application in marketing and advertising platforms."}}
{"id": "2505.10030", "pdf": "https://arxiv.org/pdf/2505.10030", "abs": "https://arxiv.org/abs/2505.10030", "authors": ["Miit Daga", "Dhriti Parikh", "Swarna Priya Ramu"], "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication", "summary": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86DeepSeqCoco\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u6930\u5b50\u6811\u56fe\u50cf\u4e2d\u51c6\u786e\u4e14\u81ea\u52a8\u5730\u8bc6\u522b\u75be\u75c5\u3002\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u7684\u4f18\u5316\u5668\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u51c6\u786e\u6027\u9ad8\u8fbe99.5%\uff0c\u6bd4\u73b0\u6709\u6a21\u578b\u9ad8\u51fa5%\u3002\u6df7\u5408SGD-Adam\u4f18\u5316\u5668\u663e\u793a\u51fa\u6700\u4f4e\u7684\u9a8c\u8bc1\u635f\u59312.81%\uff0c\u540c\u65f6\u8bad\u7ec3\u548c\u9884\u6d4b\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u3002\u8fd9\u8bc1\u660e\u4e86\u901a\u8fc7AI\u4e3a\u57fa\u7840\u7684\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u75be\u75c5\u76d1\u6d4b\u7cfb\u7edf\u6539\u8fdb\u7cbe\u786e\u519c\u4e1a\u7684\u6f5c\u529b\u3002", "motivation": "\u6930\u5b50\u6811\u75be\u75c5\u5bf9\u519c\u4e1a\u4ea7\u91cf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5728\u4f20\u7edf\u8015\u4f5c\u65b9\u5f0f\u9650\u5236\u65e9\u671f\u8bca\u65ad\u548c\u5e72\u9884\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u3002\u76ee\u524d\u7684\u75be\u75c5\u8bc6\u522b\u65b9\u6cd5\u662f\u624b\u52a8\u7684\u3001\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e86DeepSeqCoco\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u4ece\u6930\u5b50\u6811\u56fe\u50cf\u4e2d\u51c6\u786e\u4e14\u81ea\u52a8\u5730\u8bc6\u522b\u75be\u75c5\u3002\u8be5\u6a21\u578b\u5728\u5404\u79cd\u4f18\u5316\u5668\u8bbe\u7f6e\uff08\u5982SGD\u3001Adam\u548c\u6df7\u5408\u914d\u7f6e\uff09\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u627e\u5230\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeepSeqCoco\u53ef\u4ee5\u8fbe\u5230\u9ad8\u8fbe99.5%\u7684\u51c6\u786e\u7387\uff08\u6bd4\u73b0\u6709\u6a21\u578b\u9ad8\u51fa\u591a\u8fbe5%\uff09\uff0c\u5176\u4e2d\u6df7\u5408SGD-Adam\u4f18\u5316\u5668\u663e\u793a\u51fa\u6700\u4f4e\u7684\u9a8c\u8bc1\u635f\u59312.81%\u3002\u6b64\u5916\uff0c\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u6bd4\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86\u591a\u8fbe18%\uff0c\u9884\u6d4b\u65f6\u95f4\u51cf\u5c11\u4e86\u591a\u8fbe85%\u3002", "conclusion": "DeepSeqCoco\u6a21\u578b\u5c55\u793a\u4e86\u901a\u8fc7AI\u4e3a\u57fa\u7840\u7684\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u75be\u75c5\u76d1\u6d4b\u7cfb\u7edf\u6539\u8fdb\u7cbe\u786e\u519c\u4e1a\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10202", "pdf": "https://arxiv.org/pdf/2505.10202", "abs": "https://arxiv.org/abs/2505.10202", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "YiMing Cheng", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.", "AI": {"tldr": "VQ-Logits is a new method using Vector Quantization to reduce parameters and computation in LLM output layers, achieving significant speedup with minor performance loss.", "motivation": "Large Language Models have large output vocabularies leading to high computational and memory costs, especially in the final linear projection layer. Current solutions like adaptive softmax add structural complexities.", "method": "The VQ-Logits approach replaces the large output embedding matrix with a small codebook of K vectors. Tokens are mapped to these vectors, and logits over this compact space are 'scattered' back to full vocabulary space.", "result": "Experiments on benchmarks like WikiText-103 and C4 show up to 99% parameter reduction and 6x speedup in logit computation, with only a 4% increase in perplexity compared to full softmax baselines.", "conclusion": "VQ-Logits effectively reduces parameters and computation in LLM output layers while maintaining strong performance, demonstrated through extensive experiments and ablation studies."}}
{"id": "2410.13778", "pdf": "https://arxiv.org/pdf/2410.13778", "abs": "https://arxiv.org/abs/2410.13778", "authors": ["Michelangelo Olmo Nogara Notarianni", "Filippo Leveni", "Diego Stucchi", "Luca Frittoli", "Giacomo Boracchi"], "title": "Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "AALTD workshop at ECML 2024 (https://ecml-aaltd.github.io/aaltd2024/)", "summary": "We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.", "AI": {"tldr": "The paper introduces KQT-EWMA, a non-parametric change-detection algorithm that merges Kernel-QuantTree histogram and EWMA statistic for online multivariate data stream monitoring. It allows controlling false alarms using a pre-determined ARL_0 and demonstrates performance comparable to state-of-the-art methods in experiments.", "motivation": "There is a need for effective change-detection algorithms that can monitor multivariate data streams online with controlled false alarm rates, particularly those that can operate at a pre-determined Average Run Length (ARL_0). Most existing non-parametric methods lack the ability to control ARL_0 a priori.", "method": "The method involves combining the Kernel-QuantTree (KQT) histogram with the Exponentially Weighted Moving Average (EWMA) statistic to create the KQT-EWMA algorithm. This algorithm is designed to monitor multivariate data streams in a flexible and practical manner, allowing for non-parametric monitoring without dependence on the data distribution in stationary conditions.", "result": "Experiments conducted on both synthetic and real-world datasets show that KQT-EWMA is capable of controlling ARL_0 while achieving detection delays that are either comparable to or lower than other advanced methods designed for similar conditions.", "conclusion": "KQT-EWMA represents an advancement in non-parametric change-detection by providing a way to control false alarms through the use of a pre-determined ARL_0. It offers competitive performance in terms of detection delay compared to state-of-the-art techniques."}}
{"id": "2505.09864", "pdf": "https://arxiv.org/pdf/2505.09864", "abs": "https://arxiv.org/abs/2505.09864", "authors": ["Aditya Panangat"], "title": "BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks", "categories": ["cs.LG"], "comment": "6 pages, 0 figures, 2 tables", "summary": "Over the past decade, the use of machine learning has increased\nexponentially. Models are far more complex than ever before, growing to\ngargantuan sizes and housing millions of weights. Unfortunately, the fact that\nlarge models have become the state of the art means that it often costs\nmillions of dollars to train and operate them. These expenses not only hurt\ncompanies but also bar non-wealthy individuals from contributing to new\ndevelopments and force consumers to pay greater prices for AI. Current methods\nused to prune models, such as iterative magnitude pruning, have shown great\naccuracy but require an iterative training sequence that is incredibly\ncomputationally and environmentally taxing. To solve this problem, BINGO is\nintroduced. BINGO, during the training pass, studies specific subsets of a\nneural network one at a time to gauge how significant of a role each weight\nplays in contributing to a network's accuracy. By the time training is done,\nBINGO generates a significance score for each weight, allowing for\ninsignificant weights to be pruned in one shot. BINGO provides an\naccuracy-preserving pruning technique that is less computationally intensive\nthan current methods, allowing for a world where AI growth does not have to\nmean model growth, as well.", "AI": {"tldr": "BINGO is a new method for pruning neural network models that reduces computational costs while preserving accuracy.", "motivation": "The motivation of this paper is to address the high computational and financial costs associated with training and operating large machine learning models, as well as the limitations of current pruning methods that are computationally and environmentally taxing.", "method": "BINGO studies specific subsets of a neural network during the training pass to determine the significance of each weight in contributing to the network's accuracy. It then generates a significance score for each weight, enabling the pruning of insignificant weights in one step.", "result": "BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than existing methods.", "conclusion": "BINGO offers a solution where AI advancement does not necessarily lead to increased model sizes and computational demands."}}
{"id": "2505.10046", "pdf": "https://arxiv.org/pdf/2505.10046", "abs": "https://arxiv.org/abs/2505.10046", "authors": ["Bingda Tang", "Boyang Zheng", "Xichen Pan", "Sayak Paul", "Saining Xie"], "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.", "AI": {"tldr": "This paper explores the design space of fusing LLMs and DiTs for text-to-image generation, offering comparisons, design insights, and a reproducible training recipe.", "motivation": "To address gaps in understanding the potential of deep fusion between LLMs and DiTs for multi-modal generation due to lack of detailed comparisons and undisclosed design details in previous studies.", "method": "Conduct empirical study on text-to-image generation with controlled comparisons to established baselines, analysis of design choices, and provision of a reproducible large-scale training recipe.", "result": "Provides meaningful data points and practical guidelines for future research in multi-modal generation.", "conclusion": "This work aims to offer valuable insights and reproducible methods for advancing multi-modal generation research."}}
{"id": "2505.10218", "pdf": "https://arxiv.org/pdf/2505.10218", "abs": "https://arxiv.org/abs/2505.10218", "authors": ["Zongsheng Wang", "Kaili Sun", "Bowen Wu", "Qun Yu", "Ying Li", "Baoxun Wang"], "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward", "categories": ["cs.CL"], "comment": null, "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.", "AI": {"tldr": "The paper proposes RAIDEN-R1, a reinforcement learning framework with Verifiable Role-Awareness Reward (VRAR) to improve role consistency in role-playing conversational agents. It uses singular and multi-term mining strategies for rewards and develops a Chain-of-Thought dataset via multi-LLM collaboration. The 14B-GRPO model shows superior accuracy and robustness.", "motivation": "Role-playing conversational agents often struggle with maintaining role consistency, which this research aims to address.", "method": "RAIDEN-R1 framework integrates VRAR using singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys. A high-quality Chain-of-Thought dataset is constructed through multi-LLM collaboration.", "result": "The 14B-GRPO model achieved 88.04% accuracy on Script-Based Knowledge and 88.65% on Conversation Memory metrics, outperforming baseline models while maintaining robustness.", "conclusion": "RAIDEN-R1 bridges the gap of non-quantifiability in RPCA training and offers insights into role-aware reasoning patterns, thus advancing the development of RPCAs."}}
{"id": "2505.03084", "pdf": "https://arxiv.org/pdf/2505.03084", "abs": "https://arxiv.org/abs/2505.03084", "authors": ["Shashank Kapoor", "Sanjay Surendranath Girija", "Lakshit Arora", "Dipen Pradhan", "Ankit Shetgaonkar", "Aman Raj"], "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in IEEE COMPSAC 2025", "summary": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world.", "AI": {"tldr": "The paper surveys adversarial attacks in multimodal models encompassing text, image, video, and audio, highlighting the evolution of threats and providing a comprehensive view of the threat landscape.", "motivation": "Multimodal models are vulnerable to adversarial attacks across different modalities. With practitioners increasingly adopting these models for real-world applications, there is a need for a focused view on attack types and preventive actions.", "method": "The paper conducts a survey of adversarial attacks targeting all four modalities: text, image, video, and audio, to provide an understanding of the threat landscape and its evolution.", "result": "A comprehensive summarization of the threat landscape in the multimodal world is presented, which includes adversarial attacks on text, image, video, and audio modalities.", "conclusion": "This survey is the first of its kind to outline the adversarial attack landscape in multimodal models, helping practitioners understand potential threats and take necessary actions."}}
{"id": "2505.09901", "pdf": "https://arxiv.org/pdf/2505.09901", "abs": "https://arxiv.org/abs/2505.09901", "authors": ["Ziyuan Zhang", "Darcy Wang", "Ningyuan Chen", "Rodrigo Mansur", "Vahid Sarhangian"], "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.", "AI": {"tldr": "Large language models (LLMs) show human-like exploration-exploitation behavior in simple tasks but struggle to adapt in complex environments.", "motivation": "To investigate if LLMs can simulate human decision-making behavior and achieve comparable or superior performance, particularly focusing on the exploration-exploitation tradeoff.", "method": "Using multi-armed bandit tasks from cognitive science and psychiatry, the study compares the E&E strategies of LLMs, humans, and MAB algorithms through interpretable choice models. It also examines how reasoning, prompted by strategies or enhanced models, influences LLM decision-making.", "result": "Reasoning makes LLMs more human-like in their decision-making, with similar levels of random and directed exploration in simple tasks. However, in complex environments, LLMs lack adaptability compared to humans, especially in directed exploration.", "conclusion": "LLMs have potential as tools for simulating human behavior and automated decision-making, but they face limitations, especially in adapting to non-stationary environments."}}
{"id": "2505.10049", "pdf": "https://arxiv.org/pdf/2505.10049", "abs": "https://arxiv.org/abs/2505.10049", "authors": ["Jinlong Fan", "Xuepu Zeng", "Jing Zhang", "Mingming Gong", "Yuxiang Yang", "Dacheng Tao"], "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.", "AI": {"tldr": "Dynamic scene representation and reconstruction have advanced significantly due to developments in neural radiance fields and 3D Gaussian splatting techniques, transitioning from static to dynamic environments. This survey reviews over 200 papers on this topic, categorizing approaches and identifying challenges and future directions.", "motivation": "The motivation is the transformative advances in dynamic scene representation and reconstruction, driven by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques, which have evolved from addressing static to dynamic environments.", "method": "The method involves a systematic analysis of over 200 papers focused on dynamic scene representation using radiance fields, categorized through multiple critical lenses such as motion representation paradigms, reconstruction techniques, auxiliary information integration strategies, and regularization approaches.", "result": "The result is a comprehensive overview of the field, organizing diverse methodological approaches under a unified representational framework, and providing insights into persistent challenges and promising research directions.", "conclusion": "This survey concludes by establishing a definitive reference for researchers entering the field of dynamic scene reconstruction, offering both experienced practitioners and newcomers a systematic understanding of conceptual principles and practical frontiers."}}
{"id": "2505.10260", "pdf": "https://arxiv.org/pdf/2505.10260", "abs": "https://arxiv.org/abs/2505.10260", "authors": ["Poli Apollinaire Nemkova", "Solomon Ubani", "Mark V. Albert"], "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.", "AI": {"tldr": "\u5728\u591a\u8bed\u8a00\u80cc\u666f\u4e0b\uff0c\u8bc4\u4f30\u591a\u4e2a\u6700\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u5bf9\u590d\u6742\u6587\u672c\u6570\u636e\u96c6\uff08\u5305\u62ec\u4fc4\u8bed\u548c\u4e4c\u514b\u5170\u8bed\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff09\u8fdb\u884c\u6807\u6ce8\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u8bc6\u522b\u6d89\u53ca\u4eba\u6743\u4fb5\u72af\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u3002\u901a\u8fc7\u4e0e\u4eba\u5de5\u53cc\u91cd\u6807\u6ce8\u7684\u9ec4\u91d1\u6807\u51c6\u6bd4\u8f83\uff0c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u8868\u73b0\u3001\u9519\u8bef\u6a21\u5f0f\u53ca\u8de8\u8bed\u8a00\u9002\u5e94\u6027\uff0c\u4e3aLLM\u5728\u654f\u611f\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u89c1\u89e3\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u7684\u65e5\u76ca\u590d\u6742\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5904\u7406\u654f\u611f\u3001\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u6807\u6ce8\u6d89\u53ca\u4eba\u6743\u4fb5\u72af\u7684\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u3002", "method": "\u7814\u7a76\u9009\u53d6\u4e86GPT-3.5\u3001GPT-4\u3001LLAMA3\u3001Mistral 7B\u548cClaude-2\u7b49\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u4f9b\u82f1\u8bed\u548c\u4fc4\u8bed\u4e24\u79cd\u8bed\u8a00\u7684\u63d0\u793a\uff0c\u5bf9\u5305\u542b\u4fc4\u8bed\u548c\u4e4c\u514b\u5170\u8bed\u7684\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u6570\u636e\u96c6\u8fdb\u884c\u4e8c\u5206\u7c7b\u4efb\u52a1\uff08\u8bc6\u522b\u6d89\u53ca\u4eba\u6743\u4fb5\u72af\u7684\u53c2\u8003\uff09\u3002\u6a21\u578b\u7684\u8868\u73b0\u901a\u8fc7\u4e0e1000\u4e2a\u6837\u672c\u7684\u4eba\u5de5\u53cc\u91cd\u6807\u6ce8\u9ec4\u91d1\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\u6765\u8bc4\u4f30\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u548c\u9519\u8bef\u6a21\u5f0f\u3002", "result": "\u5404\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u5404\u5f02\uff0c\u5c55\u73b0\u4e86\u5404\u81ea\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u548c\u8de8\u8bed\u8a00\u9002\u5e94\u80fd\u529b\u3002\u901a\u8fc7\u5bf9\u6a21\u578b\u8f93\u51fa\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u5bf9\u6bd4\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5904\u7406\u4e3b\u89c2\u6027\u5f3a\u4e14\u4f9d\u8d56\u4e0a\u4e0b\u6587\u5224\u65ad\u7684\u4efb\u52a1\u4e2d\u7684\u72ec\u7279\u9519\u8bef\u548c\u5206\u6b67\u6a21\u5f0f\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u80cc\u666f\u4e0b\u7684\u654f\u611f\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\uff0c\u4f46\u5176\u5728\u5904\u7406\u4e3b\u89c2\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u5224\u65ad\u65f6\u4ecd\u5b58\u5728\u6311\u6218\u3002\u8fd9\u4e3a\u672a\u6765\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u90e8\u7f72\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2505.08202", "pdf": "https://arxiv.org/pdf/2505.08202", "abs": "https://arxiv.org/abs/2505.08202", "authors": ["Aman Raj", "Lakshit Arora", "Sanjay Surendranath Girija", "Shashank Kapoor", "Dipen Pradhan", "Ankit Shetgaonkar"], "title": "AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "Accepted in IEEE Compsac 2025", "summary": "Natural disasters, including earthquakes, wildfires and cyclones, bear a huge\nrisk on human lives as well as infrastructure assets. An effective response to\ndisaster depends on the ability to rapidly and efficiently assess the intensity\nof damage. Artificial Intelligence (AI) and Generative Artificial Intelligence\n(GenAI) presents a breakthrough solution, capable of combining knowledge from\nmultiple types and sources of data, simulating realistic scenarios of disaster,\nand identifying emerging trends at a speed previously unimaginable. In this\npaper, we present a comprehensive review on the prospects of AI and GenAI in\ndamage assessment for various natural disasters, highlighting both its\nstrengths and limitations. We talk about its application to multimodal data\nsuch as text, image, video, and audio, and also cover major issues of data\nprivacy, security, and ethical use of the technology during crises. The paper\nalso recognizes the threat of Generative AI misuse, in the form of\ndissemination of misinformation and for adversarial attacks. Finally, we\noutline avenues of future research, emphasizing the need for secure, reliable,\nand ethical Generative AI systems for disaster management in general. We\nbelieve that this work represents the first comprehensive survey of Gen-AI\ntechniques being used in the field of Disaster Assessment and Response.", "AI": {"tldr": "This paper comprehensively reviews the use of AI and GenAI in assessing damage from natural disasters, discussing its strengths, limitations, applications to multimodal data, ethical concerns, and future research directions.", "motivation": "Natural disasters pose significant risks to human lives and infrastructure, necessitating rapid and efficient damage assessment for effective disaster response. The potential of AI and GenAI to revolutionize this process by handling various data types and simulating realistic scenarios drives the motivation for this review.", "method": "The authors conducted a comprehensive review of existing literature on the application of AI and GenAI techniques to damage assessment across different types of natural disasters. They analyzed the use of these technologies with multimodal data (text, image, video, audio), identified ethical and security issues, and explored potential misuse threats such as misinformation and adversarial attacks.", "result": "The review highlights that AI and GenAI can significantly enhance the speed and accuracy of damage assessment in natural disasters through their ability to process diverse data sources and simulate scenarios. However, challenges remain regarding data privacy, security, and ethical considerations. Misuse of GenAI could lead to misinformation or adversarial attacks.", "conclusion": "This work is presented as the first comprehensive survey of GenAI techniques in disaster assessment and response. Future research should focus on developing secure, reliable, and ethical GenAI systems to support disaster management efforts."}}
{"id": "2505.09907", "pdf": "https://arxiv.org/pdf/2505.09907", "abs": "https://arxiv.org/abs/2505.09907", "authors": ["Linwei Zhang", "LuFeng", "Ruijia Liang"], "title": "Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "With the growing demand for healthy foods, agricultural product price\nforecasting has become increasingly important. Hass avocados, as a high-value\ncrop, exhibit complex price fluctuations influenced by factors such as\nseasonality, region, and weather. Traditional prediction models often struggle\nwith highly nonlinear and dynamic data. To address this, we propose a hybrid\ndeep learning model, TCN-MLP-Attention Architecture, combining Temporal\nConvolutional Networks (TCN) for sequential feature extraction, Multi-Layer\nPerceptrons (MLP) for nonlinear interactions, and an Attention mechanism for\ndynamic feature weighting. The dataset used covers over 50,000 records of Hass\navocado sales across the U.S. from 2015 to 2018, including variables such as\nsales volume, average price, time, region, weather, and variety type, collected\nfrom point-of-sale systems and the Hass Avocado Board. After systematic\npreprocessing, including missing value imputation and feature normalization,\nthe proposed model was trained and evaluated. Experimental results demonstrate\nthat the TCN-MLP-Attention model achieves excellent predictive performance,\nwith an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.\nThis research provides a scalable and effective approach for time series\nforecasting in agricultural markets and offers valuable insights for\nintelligent supply chain management and price strategy optimization.", "AI": {"tldr": "The paper proposes a hybrid deep learning model TCN-MLP-Attention for Hass avocado price forecasting, achieving RMSE of 1.23 and MSE of 1.51.", "motivation": "Agricultural product price forecasting has become increasingly important with the growing demand for healthy foods. Traditional prediction models struggle with highly nonlinear and dynamic data like Hass avocado prices which are influenced by seasonality, region, and weather.", "method": "The proposed method is a hybrid deep learning model named TCN-MLP-Attention Architecture that combines Temporal Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for dynamic feature weighting.", "result": "The model was trained on a dataset with over 50,000 records of Hass avocado sales across the U.S. from 2015 to 2018. The experimental results show excellent predictive performance with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.", "conclusion": "This research provides a scalable and effective approach for time series forecasting in agricultural markets and offers valuable insights for intelligent supply chain management and price strategy optimization."}}
{"id": "2505.10055", "pdf": "https://arxiv.org/pdf/2505.10055", "abs": "https://arxiv.org/abs/2505.10055", "authors": ["Ijazul Haq", "Yingjie Zhang", "Irfan Ali Khan"], "title": "PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u4f4e\u8d44\u6e90Pashto\u8bed\u8a00\u4e0a\u7684OCR\u6027\u80fd\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aPsOCR\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u591a\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "Pashto\u8bed\u8a00\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u5305\u62ec\u5176\u8fde\u7b14\u5b57\u4f53\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5408\u6210Pashto OCR\u6570\u636e\u96c6\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e00\u767e\u4e07\u5f20\u56fe\u50cf\u7684\u5408\u6210Pashto OCR\u6570\u636e\u96c6PsOCR\uff0c\u8fd9\u4e9b\u56fe\u50cf\u6807\u6ce8\u6709\u5355\u8bcd\u3001\u884c\u548c\u6587\u6863\u7ea7\u522b\u7684\u8fb9\u754c\u6846\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u4e861000\u79cd\u72ec\u7279\u7684\u5b57\u4f53\u5bb6\u65cf\u3001\u989c\u8272\u3001\u56fe\u50cf\u5927\u5c0f\u548c\u5e03\u5c40\u7684\u53d8\u5316\u3002\u4f7f\u7528\u4e00\u4e2a\u5305\u542b10K\u5f20\u56fe\u50cf\u7684\u57fa\u51c6\u5b50\u96c6\u6765\u8bc4\u4f30\u51e0\u4e2aLMMs\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4e03\u4e2a\u5f00\u6e90\u6a21\u578b\u548c\u56db\u4e2a\u95ed\u6e90\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aGemini\u5728\u6240\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\uff0cQwen-7B\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5f53\u524dLMMs\u5728Pashto OCR\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u7684\u6df1\u5165\u8bc4\u4f30\uff0c\u5e76\u4e3a\u672a\u6765\u5728Pashto OCR\u53ca\u5176\u4ed6\u7c7b\u4f3c\u811a\u672c\uff08\u5982\u963f\u62c9\u4f2f\u8bed\u3001\u6ce2\u65af\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\uff09\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.10261", "pdf": "https://arxiv.org/pdf/2505.10261", "abs": "https://arxiv.org/abs/2505.10261", "authors": ["Rui Yang", "Huitao Li", "Matthew Yu Heng Wong", "Yuhe Ke", "Xin Li", "Kunyu Yu", "Jingchi Liao", "Jonathan Chong Kai Liew", "Sabarinath Vinod Nair", "Jasmine Chiat Ling Ong", "Irene Li", "Douglas Teodoro", "Chuan Hong", "Daniel Shu Wei Ting", "Nan Liu"], "title": "The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.", "AI": {"tldr": "This paper analyzed 19,123 studies and found that generative LLMs have advantages in open-ended tasks, while traditional NLP is better for information extraction and analysis tasks.", "motivation": "To explore the differences between traditional NLP and generative LLMs across different medical tasks.", "method": "Analyzed 19,123 studies related to the application of NLP and LLMs in medicine.", "result": "Generative LLMs demonstrate advantages in open-ended tasks, while traditional NLP dominates in information extraction and analysis tasks.", "conclusion": "As these technologies advance, ethical use of them is essential to ensure their potential in medical applications."}}
{"id": "2505.09922", "pdf": "https://arxiv.org/pdf/2505.09922", "abs": "https://arxiv.org/abs/2505.09922", "authors": ["Zichen Liu", "Wei Zhang", "Tiejun Li"], "title": "Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity", "categories": ["cs.LG"], "comment": "22 pages", "summary": "Euclidean diffusion models have achieved remarkable success in generative\nmodeling across diverse domains, and they have been extended to manifold case\nin recent advances. Instead of explicitly utilizing the structure of special\nmanifolds as studied in previous works, we investigate direct sampling of the\nEuclidean diffusion models for general manifold-constrained data in this paper.\nWe reveal the multiscale singularity of the score function in the embedded\nspace of manifold, which hinders the accuracy of diffusion-generated samples.\nWe then present an elaborate theoretical analysis of the singularity structure\nof the score function by separating it along the tangential and normal\ndirections of the manifold. To mitigate the singularity and improve the\nsampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces\nnon-isotropic noise along the normal direction to reduce scale discrepancies,\nand (2) Tango-DM, which trains only the tangential component of the score\nfunction using a tangential-only loss function. Numerical experiments\ndemonstrate that our methods achieve superior performance on distributions over\nvarious manifolds with complex geometries.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6d41\u5f62\u7ea6\u675f\u6570\u636e\u4e0a\u76f4\u63a5\u91c7\u6837\u6b27\u51e0\u91cc\u5f97\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5206\u6570\u51fd\u6570\u7684\u591a\u5c3a\u5ea6\u5947\u5f02\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u91c7\u6837\u7cbe\u5ea6\u7684\u65b0\u65b9\u6cd5\uff1aNiso-DM\u548cTango-DM\u3002", "motivation": "\u5c3d\u7ba1\u6b27\u51e0\u91cc\u5f97\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5efa\u6a21\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u5e76\u5df2\u88ab\u6269\u5c55\u5230\u6d41\u5f62\u60c5\u51b5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u663e\u5f0f\u5229\u7528\u7279\u5b9a\u6d41\u5f62\u7684\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u5bf9\u4e00\u822c\u6d41\u5f62\u7ea6\u675f\u6570\u636e\u76f4\u63a5\u8fdb\u884c\u91c7\u6837\u3002", "method": "1. \u5206\u6790\u5206\u6570\u51fd\u6570\u5728\u6d41\u5f62\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u591a\u5c3a\u5ea6\u5947\u5f02\u6027\uff1b2. \u63d0\u51faNiso-DM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6cd5\u7ebf\u65b9\u5411\u5f15\u5165\u975e\u5404\u5411\u540c\u6027\u566a\u58f0\u6765\u51cf\u5c11\u5c3a\u5ea6\u5dee\u5f02\uff1b3. \u63d0\u51faTango-DM\u65b9\u6cd5\uff0c\u4ec5\u8bad\u7ec3\u5206\u6570\u51fd\u6570\u7684\u5207\u7ebf\u5206\u91cf\u5e76\u4f7f\u7528\u5207\u7ebf\u635f\u5931\u51fd\u6570\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5177\u6709\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u7684\u5404\u79cd\u6d41\u5f62\u5206\u5e03\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u5206\u6570\u51fd\u6570\u7684\u5947\u5f02\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u6d41\u5f62\u7ea6\u675f\u6570\u636e\u4e0a\u7684\u91c7\u6837\u7cbe\u5ea6\u3002"}}
{"id": "2505.10072", "pdf": "https://arxiv.org/pdf/2505.10072", "abs": "https://arxiv.org/abs/2505.10072", "authors": ["Rui-Yang Ju", "Sheng-Yen Huang", "Yi-Ping Hung"], "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.", "AI": {"tldr": "ToonifyGB is an efficient two-stage framework that extends Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes. It generates stylized video in Stage 1 and learns stylized neutral head model and expression blendshapes in Stage 2.", "motivation": "The motivation of this paper is to extend the widely used Toonify framework for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, overcoming the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN.", "method": "The method involves a two-stage framework called ToonifyGB. In Stage 1, an improved StyleGAN is employed to generate stylized video from input video frames. In Stage 2, a stylized neutral head model and a set of expression blendshapes are learned from the generated video.", "result": "ToonifyGB efficiently renders stylized avatars with arbitrary expressions and its effectiveness is validated on the benchmark dataset using two styles: Arcane and Pixar.", "conclusion": "ToonifyGB provides an efficient way to synthesize diverse stylized 3D head avatars using Gaussian blendshapes."}}
{"id": "2505.10282", "pdf": "https://arxiv.org/pdf/2505.10282", "abs": "https://arxiv.org/abs/2505.10282", "authors": ["Dubai Li", "Nan Jiang", "Kangping Huang", "Ruiqi Tu", "Shuyu Ouyang", "Huayu Yu", "Lin Qiao", "Chen Yu", "Tianshu Zhou", "Danyang Tong", "Qian Wang", "Mengtao Li", "Xiaofeng Zeng", "Yu Tian", "Xinping Tian", "Jingsong Li"], "title": "From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making", "categories": ["cs.CL"], "comment": null, "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.", "AI": {"tldr": "Quicker is an evidence-based clinical decision support system powered by LLMs designed to automate evidence synthesis and generate clinical recommendations. It has strong performance in question decomposition, retrieval sensitivities, literature screening, and evidence assessment. Collaboration with Quicker reduces the time required for recommendation development.", "motivation": "Integrating clinical evidence into real-time practice is challenging due to workload, complex processes, and time constraints. There is a need for tools that can automate evidence synthesis for more efficient decision making.", "method": "Quicker implements a fully automated chain covering all phases from questions to clinical recommendations. It uses large language models and includes integrated tools and interactive user interfaces. A benchmark dataset Q2CRBench-3 was developed based on clinical guideline development records for evaluation.", "result": "Experimental results showed Quicker's strong performance in fine-grained question decomposition, retrieval sensitivities comparable to human experts, literature screening approaching comprehensive inclusion of relevant studies, effective evidence assessment support, and more comprehensive and coherent recommendations than clinicians.", "conclusion": "The findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions."}}
{"id": "2505.09616", "pdf": "https://arxiv.org/pdf/2505.09616", "abs": "https://arxiv.org/abs/2505.09616", "authors": ["Yuqi Li", "Yuanzhong Zheng", "Zhongtian Guo", "Yaoxuan Wang", "Jianjun Yin", "Haojun Fei"], "title": "SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech", "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2.0"], "comment": "2 pages,3 figures,1 chart", "summary": "This paper presents SpecWav-Attack, an adversarial model for detecting\nspeakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and\nincorporates spectrogram resizing and incremental training for improved\nperformance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack\noutperforms conventional attacks, revealing vulnerabilities in anonymized\nspeech systems and emphasizing the need for stronger defenses, benchmarked\nagainst the ICASSP 2025 Attacker Challenge.", "AI": {"tldr": "SpecWav-Attack is an adversarial model which uses Wav2Vec2 for feature extraction, spectrogram resizing and incremental training to detect speakers in anonymized speech. It performs better than conventional attacks, showing the weaknesses of anonymized speech systems.", "motivation": "To develop a more effective method to detect speakers in anonymized speech and reveal vulnerabilities in these systems.", "method": "Using Wav2Vec2 for feature extraction, incorporating spectrogram resizing and incremental training.", "result": "Outperforms conventional attacks on librispeech-dev and librispeech-test datasets.", "conclusion": "Anonymized speech systems have significant vulnerabilities which need stronger defenses, as benchmarked against the ICASSP 2025 Attacker Challenge."}}
{"id": "2505.09925", "pdf": "https://arxiv.org/pdf/2505.09925", "abs": "https://arxiv.org/abs/2505.09925", "authors": ["Yutao Yang", "Jie Zhou", "Junsong Li", "Qianjun Pan", "Bihao Zhan", "Qin Chen", "Xipeng Qiu", "Liang He"], "title": "Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an interactive continual learning paradigm where AI\nmodels dynamically learn new skills from real-time human feedback while\nretaining prior knowledge. This paradigm distinctively addresses two major\nlimitations of traditional continual learning: (1) dynamic model updates using\nstreaming, real-time human-annotated data, rather than static datasets with\nfixed labels, and (2) the assumption of clean labels, by explicitly handling\nthe noisy feedback common in real-world interactions. To tackle these problems,\nwe propose RiCL, a Reinforced interactive Continual Learning framework\nleveraging Large Language Models (LLMs) to learn new skills effectively from\ndynamic feedback. RiCL incorporates three key components: a temporal\nconsistency-aware purifier to automatically discern clean from noisy samples in\ndata streams; an interaction-aware direct preference optimization strategy to\nalign model behavior with human intent by reconciling AI-generated and\nhuman-provided feedback; and a noise-resistant contrastive learning module that\ncaptures robust representations by exploiting inherent data relationships, thus\navoiding reliance on potentially unreliable labels. Extensive experiments on\ntwo benchmark datasets (FewRel and TACRED), contaminated with realistic noise\npatterns, demonstrate that our RiCL approach substantially outperforms existing\ncombinations of state-of-the-art online continual learning and noisy-label\nlearning methods.", "AI": {"tldr": "This paper introduces RiCL, a framework for interactive continual learning that addresses limitations of traditional methods by handling dynamic real-time feedback and noisy labels.", "motivation": "The motivation is to improve upon traditional continual learning methods which struggle with static datasets and the assumption of clean labels, by enabling AI models to dynamically learn new skills from real-time human feedback while retaining prior knowledge.", "method": "RiCL incorporates three key components: a temporal consistency-aware purifier to discern clean from noisy samples; an interaction-aware direct preference optimization strategy to align model behavior with human intent; and a noise-resistant contrastive learning module that captures robust representations.", "result": "Extensive experiments on two benchmark datasets (FewRel and TACRED) demonstrate that RiCL substantially outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods.", "conclusion": "RiCL effectively addresses the limitations of traditional continual learning paradigms by incorporating mechanisms to handle dynamic feedback and noisy labels."}}
{"id": "2505.10088", "pdf": "https://arxiv.org/pdf/2505.10088", "abs": "https://arxiv.org/abs/2505.10088", "authors": ["Yuncheng Guo", "Xiaodong Gu"], "title": "MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file", "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.", "AI": {"tldr": "The paper introduces MMRL and MMRL++, methods that enhance cross-modal interactions in Vision-Language Models using a shared representation space, reducing overfitting and improving generalization. Extensive experiments show they outperform state-of-the-art methods.", "motivation": "Large-scale pre-trained Vision-Language Models often suffer from overfitting when adapted with limited few-shot data, which undermines their ability to generalize to new tasks.", "method": "MMRL proposes a shared, learnable, modality-agnostic representation space by inserting representation tokens into higher encoder layers. A regularization term aligns class and text features with the frozen VLM's zero-shot features. MMRL++ enhances this by significantly reducing trainable parameters and strengthening intra-modal interactions.", "result": "Extensive experiments on 15 datasets demonstrate that both MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.", "conclusion": "MMRL and MMRL++ are effective methods for multi-modal representation learning, promoting better generalization and reducing overfitting in Vision-Language Models."}}
{"id": "2505.10320", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "AI": {"tldr": "A new reinforcement learning method named J1 is introduced to train LLM-as-a-Judge models, which converts various prompts to judgment tasks with verifiable rewards. This approach outperforms other existing models in certain sizes and provides detailed analysis on different training aspects.", "motivation": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models are crucial solutions.", "method": "J1 is a reinforcement learning approach that converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards, incentivizing thinking and mitigating judgment bias.", "result": "J1 outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. It also surpasses o1-mini and even R1 on some benchmarks despite training a smaller model.", "conclusion": "The study provides analysis and ablations comparing different training recipes, reward strategies, and variations in thought length and content, concluding that the models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses."}}
{"id": "2505.09619", "pdf": "https://arxiv.org/pdf/2505.09619", "abs": "https://arxiv.org/abs/2505.09619", "authors": ["Pietro Cassieri", "Aiman Faiz", "Anna Maria De Roberto", "Claudio Pascarelli", "Gianvito Mitrano", "Gianluca Fimiani", "Marina Garofano", "Christiancarmine Esposito", "Genoveffa Tortora", "Alessia Bramanti", "Giuseppe Scanniello"], "title": "Predictive Models for Chronic Heart Failure", "categories": ["stat.OT", "cs.AI"], "comment": null, "summary": "The management of chronic Heart Failure (HF) presents significant challenges\nin modern healthcare, requiring continuous monitoring, early detection of\nexacerbations, and personalized treatment strategies. In this paper, we present\na predictive model founded on Machine Learning (ML) techniques to identify\npatients at HF risk. This model is an ensemble learning approach, a modified\nstacking technique, that uses two specialized models leveraging clinical and\nechocardiographic features and then a meta-model to combine the predictions of\nthese two models. We initially assess the model on a real dataset and the\nobtained results suggest that it performs well in the stratification of\npatients at HR risk. Specifically, we obtained high sensitivity (95\\%),\nensuring that nearly all high-risk patients are identified. As for accuracy, we\nobtained 84\\%, which can be considered moderate in some ML contexts. However,\nit is acceptable given our priority of identifying patients at risk of HF\nbecause they will be asked to participate in the telemonitoring program of the\nPrediHealth research project on which some of the authors of this paper are\nworking. The initial findings also suggest that ML-based risk stratification\nmodels can serve as valuable decision-support tools not only in the PrediHealth\nproject but also for healthcare professionals, aiding in early intervention and\npersonalized patient management. To have a better understanding of the value\nand of potentiality of our predictive model, we also contrasted its results\nwith those obtained by using three baseline models. The preliminary results\nindicate that our predictive model outperforms these baselines that flatly\nconsider features, \\ie not grouping them in clinical and echocardiographic\nfeatures.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u5fc3\u529b\u8870\u7aed\u98ce\u9669\u60a3\u8005\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u4e34\u5e8a\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u7279\u5f81\uff0c\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7075\u654f\u5ea6\u65b9\u9762\u3002\u5b83\u6709\u671b\u6210\u4e3a\u533b\u7597\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "motivation": "\u6162\u6027\u5fc3\u529b\u8870\u7aed\u7ba1\u7406\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u9700\u8981\u6301\u7eed\u76d1\u6d4b\u3001\u65e9\u671f\u6076\u5316\u68c0\u6d4b\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5fc3\u529b\u8870\u7aed\u98ce\u9669\u60a3\u8005\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u8be5\u6a21\u578b\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u4f53\u4e3a\u4e00\u79cd\u4fee\u6539\u540e\u7684\u5806\u53e0\u6280\u672f\u3002\u5b83\u4f7f\u7528\u4e24\u4e2a\u4e13\u95e8\u6a21\u578b\u5206\u522b\u5229\u7528\u4e34\u5e8a\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u5143\u6a21\u578b\u5c06\u8fd9\u4e24\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u7ed3\u5408\u8d77\u6765\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u5fc3\u529b\u8870\u7aed\u98ce\u9669\u60a3\u8005\u5206\u5c42\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7075\u654f\u5ea6\u9ad8\u8fbe95%\uff0c\u51c6\u786e\u7387\u4e3a84%\u3002\u4e0e\u4e09\u4e2a\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u98ce\u9669\u5206\u5c42\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8ePrediHealth\u9879\u76ee\uff0c\u8fd8\u5bf9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u6709\u76ca\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u5e72\u9884\u548c\u4e2a\u6027\u5316\u60a3\u8005\u7ba1\u7406\u3002"}}
{"id": "2505.09949", "pdf": "https://arxiv.org/pdf/2505.09949", "abs": "https://arxiv.org/abs/2505.09949", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Samgyu Yang", "Abdulrahman Faden"], "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors", "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.", "AI": {"tldr": "This paper explores the use of large language models (LLMs), specifically a fine-tuned Llama3 8B model, for analyzing freeway crash data and identifying causation factors without pre-labeled data through zero-shot classification. The research compiles 226 traffic safety studies to create a training dataset covering various factors related to crashes. Results show that LLMs effectively identify primary causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. The model's practical applicability is validated by high agreement among researchers.", "motivation": "Traditional statistical methods and machine learning models often struggle to capture the complex interactions between various factors contributing to traffic crashes and their unique characteristics. Thus, there is a need for more advanced tools that can provide comprehensive analysis.", "method": "The research leverages a large language model (LLM) to analyze freeway crash data. A training dataset was created from 226 traffic safety studies encompassing environmental, driver, traffic, and geometric design factors. The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and their contributing factors. Zero-shot classification was used to identify crash causation without pre-labeled data.", "result": "Results demonstrate that LLMs effectively identify primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data offers more profound insights. The model's practical applicability and potential to improve traffic safety measures were validated by a high level of agreement among researchers in the field of traffic safety, as reflected in questionnaire results with 88.89%.", "conclusion": "This research highlights the complex nature of traffic crashes and how LLMs can be used for comprehensive analysis of crash causation and other contributing factors. It provides valuable insights and potential countermeasures to aid planners and policymakers in developing more effective and efficient traffic safety practices."}}
{"id": "2505.10118", "pdf": "https://arxiv.org/pdf/2505.10118", "abs": "https://arxiv.org/abs/2505.10118", "authors": ["Yangfu Li", "Hongjian Zhan", "Tianyi Chen", "Qi Liu", "Yue Lu"], "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering", "categories": ["cs.CV", "cs.CL"], "comment": "31 pages,9 figures,conference", "summary": "Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.", "AI": {"tldr": "MoB is a new method for visual token pruning which provides a closed-form error bound based on Hausdorff distance and uses \u03f5-covering theory to balance two objectives, achieving high performance preservation with minimal tokens.", "motivation": "Current visual token pruning methods use static strategies that do not account for the varying importance of objectives across different tasks, resulting in inconsistent performance.", "method": "The method derives a closed-form error bound using Hausdorff distance to characterize both objectives of prompt alignment and visual preservation. It leverages \u03f5-covering theory to reveal a trade-off between these objectives and proposes MoB (Multi-Objective Balanced Covering) to reformulate visual token pruning as a bi-objective covering problem.", "result": "Experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B with only 11.1% of original visual tokens, and accelerates LLaVA-Next-7B by 1.3-1.5\u00d7 with negligible performance loss. It also works well with Qwen2-VL and Video-LLaVA.", "conclusion": "MoB offers a provable performance bound and linear scalability, making it adaptable to challenging pruning scenarios and integrating well into advanced MLLMs and diverse vision-language tasks."}}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354", "abs": "https://arxiv.org/abs/2505.10354", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.", "AI": {"tldr": "An abstract about a new method called LDIR for creating low-dimensional dense and interpretable text embeddings.", "motivation": "Existing text embedding methods either lack interpretability or perform poorly. Recent work on interpretable embeddings using large language models results in high-dimensional embeddings.", "method": "LDIR creates low-dimensional (under 500) dense and interpretable text embeddings. It uses numerical values indicating semantic relatedness to anchor texts via farthest point sampling.", "result": "LDIR performs similarly to black-box baseline models and surpasses interpretable embedding baselines, despite having significantly fewer dimensions.", "conclusion": "LDIR provides a novel approach for generating interpretable text embeddings with strong performance and reduced dimensionality."}}
{"id": "2505.09624", "pdf": "https://arxiv.org/pdf/2505.09624", "abs": "https://arxiv.org/abs/2505.09624", "authors": ["Ekaterina Kuzmina", "Dmitrii Kriukov", "Mikhail Lebedev", "Dmitry V. Dylov"], "title": "Neurophysiologically Realistic Environment for Comparing Adaptive Deep Brain Stimulation Algorithms in Parkinson Disease", "categories": ["q-bio.NC", "cs.AI", "68T05"], "comment": "8 pages, 3 figures, submission to KDD", "summary": "Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment\nfor Parkinson disease (PD). In aDBS, a surgically placed electrode sends\ndynamically altered stimuli to the brain based on neurophysiological feedback:\nan invasive gadget that limits the amount of data one could collect for\noptimizing the control offline. As a consequence, a plethora of synthetic\nmodels of PD and those of the control algorithms have been proposed. Herein, we\nintroduce the first neurophysiologically realistic benchmark for comparing said\nmodels. Specifically, our methodology covers not only conventional basal\nganglia circuit dynamics and pathological oscillations, but also captures 15\npreviously dismissed physiological attributes, such as signal instabilities and\nnoise, neural drift, electrode conductance changes and individual variability -\nall modeled as spatially distributed and temporally registered features via\nbeta-band activity in the brain and a feedback. Furthermore, we purposely built\nour framework as a structured environment for training and evaluating deep\nreinforcement learning (RL) algorithms, opening new possibilities for\noptimizing aDBS control strategies and inviting the machine learning community\nto contribute to the emerging field of intelligent neurostimulation interfaces.", "AI": {"tldr": "An adaptive deep brain stimulation (aDBS) benchmark is introduced, featuring neurophysiological realism and a structured environment for training and evaluating deep reinforcement learning algorithms to optimize aDBS control strategies.", "motivation": "To address the limitations in data collection for optimizing aDBS control offline and to provide a realistic benchmark for comparing synthetic models of Parkinson's Disease (PD) and control algorithms.", "method": "The methodology includes conventional basal ganglia circuit dynamics and pathological oscillations, along with 15 previously overlooked physiological attributes such as signal instabilities, noise, neural drift, electrode conductance changes, and individual variability. These are modeled using beta-band activity in the brain and feedback mechanisms. A structured environment is also built for training and evaluating deep reinforcement learning algorithms.", "result": "This approach creates a neurophysiologically realistic benchmark that can be used to train and evaluate deep RL algorithms, thereby opening new possibilities for optimizing aDBS control strategies.", "conclusion": "This work invites the machine learning community to contribute to the development of intelligent neurostimulation interfaces by providing a comprehensive and realistic benchmark for aDBS."}}
{"id": "2505.09952", "pdf": "https://arxiv.org/pdf/2505.09952", "abs": "https://arxiv.org/abs/2505.09952", "authors": ["Tianyu Huai", "Jie Zhou", "Yuxuan Cai", "Qin Chen", "Wen Wu", "Xingjiao Wu", "Xipeng Qiu", "Liang He"], "title": "Task-Core Memory Management and Consolidation for Long-term Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to Neurips2025", "summary": "In this paper, we focus on a long-term continual learning (CL) task, where a\nmodel learns sequentially from a stream of vast tasks over time, acquiring new\nknowledge while retaining previously learned information in a manner akin to\nhuman learning. Unlike traditional CL settings, long-term CL involves handling\na significantly larger number of tasks, which exacerbates the issue of\ncatastrophic forgetting. Our work seeks to address two critical questions: 1)\nHow do existing CL methods perform in the context of long-term CL? and 2) How\ncan we mitigate the catastrophic forgetting that arises from prolonged\nsequential updates? To tackle these challenges, we propose a novel framework\ninspired by human memory mechanisms for long-term continual learning (Long-CL).\nSpecifically, we introduce a task-core memory management strategy to\nefficiently index crucial memories and adaptively update them as learning\nprogresses. Additionally, we develop a long-term memory consolidation mechanism\nthat selectively retains hard and discriminative samples, ensuring robust\nknowledge retention. To facilitate research in this area, we construct and\nrelease two multi-modal and textual benchmarks, MMLongCL-Bench and\nTextLongCL-Bench, providing a valuable resource for evaluating long-term CL\napproaches. Experimental results show that Long-CL outperforms the previous\nstate-of-the-art by 7.4\\% and 6.5\\% AP on the two benchmarks, respectively,\ndemonstrating the effectiveness of our approach.", "AI": {"tldr": "This paper focuses on long-term continual learning (CL), proposing a novel framework called Long-CL inspired by human memory mechanisms to mitigate catastrophic forgetting and presenting two benchmarks for evaluation.", "motivation": "Existing CL methods struggle with the challenges of long-term CL, particularly with handling a significantly larger number of tasks and avoiding catastrophic forgetting.", "method": "The authors introduce a task-core memory management strategy and a long-term memory consolidation mechanism. The former indexes crucial memories and adaptively updates them, while the latter selectively retains hard and discriminative samples. Two benchmarks, MMLongCL-Bench and TextLongCL-Bench, are also introduced.", "result": "Experimental results indicate that Long-CL surpasses the previous state-of-the-art methods by 7.4% and 6.5% AP on the two benchmarks respectively.", "conclusion": "The proposed Long-CL framework effectively addresses the issues in long-term CL, providing a significant improvement over existing methods."}}
{"id": "2505.10124", "pdf": "https://arxiv.org/pdf/2505.10124", "abs": "https://arxiv.org/abs/2505.10124", "authors": ["Ziad Kheil", "Lucas Robinet", "Laurent Risser", "Soleakhena Ken"], "title": "IMITATE: Image Registration with Context for unknown time frame recovery", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE ISBI 2025", "summary": "In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .", "AI": {"tldr": "The paper proposes a novel image registration method using a conditional U-Net architecture to estimate unknown condition-related images in real-time without reconstruction artefacts for radiotherapy treatment.", "motivation": "To address the challenges of estimating unknown condition-related images based on known images and their conditions, particularly in the context of moving tumors for radiotherapy treatment with 4D-CT scans.", "method": "A new conditional U-Net architecture is used to model the image registration formalism, fully incorporating conditional information without needing any fixed image. This approach is applied to stitch sequential 2D slices into several 3D volumes at different organ positions.", "result": "The method successfully generates artefact-free volumes through real-time latencies when applied to 4D-CT clinical data in thoracoabdominal regions.", "conclusion": "The proposed method offers a solution for complex image registration tasks, such as those encountered in radiotherapy treatment planning, providing accurate and real-time results."}}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356", "abs": "https://arxiv.org/abs/2505.10356", "authors": ["Chunyu Ye", "Shaonan Wang"], "title": "Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli", "categories": ["cs.CL"], "comment": null, "summary": "Decoding thoughts from brain activity offers valuable insights into human\ncognition and enables promising applications in brain-computer interaction.\nWhile prior studies have explored language reconstruction from fMRI data, they\nare typically limited to single-modality inputs such as images or audio. In\ncontrast, human thought is inherently multimodal. To bridge this gap, we\npropose a unified and flexible framework for reconstructing coherent language\nfrom brain recordings elicited by diverse input modalities-visual, auditory,\nand textual. Our approach leverages visual-language models (VLMs), using\nmodality-specific experts to jointly interpret information across modalities.\nExperiments demonstrate that our method achieves performance comparable to\nstate-of-the-art systems while remaining adaptable and extensible. This work\nadvances toward more ecologically valid and generalizable mind decoding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u7279\u5b9a\u6a21\u6001\u4e13\u5bb6\uff0c\u4ece\u7531\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u8f93\u5165\u5f15\u53d1\u7684\u5927\u8111\u8bb0\u5f55\u4e2d\u91cd\u5efa\u8fde\u8d2f\u7684\u8bed\u8a00\u3002\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f73\u7cfb\u7edf\u76f8\u5f53\uff0c\u540c\u65f6\u66f4\u5177\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u63a8\u52a8\u4e86\u66f4\u751f\u6001\u6709\u6548\u548c\u53ef\u6cdb\u5316\u7684\u601d\u7ef4\u89e3\u7801\u7814\u7a76\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u5728\u4ecefMRI\u6570\u636e\u91cd\u5efa\u8bed\u8a00\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u8fdb\u5c55\uff0c\u4f46\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001\u8f93\u5165\uff08\u5982\u56fe\u50cf\u6216\u97f3\u9891\uff09\uff0c\u800c\u4eba\u7c7b\u601d\u7ef4\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u7684\u65b9\u6cd5\u6765\u66f4\u771f\u5b9e\u5730\u53cd\u6620\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u7279\u5b9a\u6a21\u6001\u4e13\u5bb6\uff0c\u8de8\u591a\u79cd\u6a21\u6001\uff08\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\uff09\u89e3\u91ca\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u5927\u8111\u6d3b\u52a8\u8bb0\u5f55\u4e2d\u91cd\u5efa\u8fde\u8d2f\u8bed\u8a00\u7684\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u73b0\u6709\u6700\u4f73\u7cfb\u7edf\u5ab2\u7f8e\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u66f4\u52a0\u751f\u6001\u6709\u6548\u548c\u53ef\u6cdb\u5316\u7684\u601d\u7ef4\u89e3\u7801\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u4eba\u7c7b\u8ba4\u77e5\u7814\u7a76\u548c\u8111\u673a\u4ea4\u4e92\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.09646", "pdf": "https://arxiv.org/pdf/2505.09646", "abs": "https://arxiv.org/abs/2505.09646", "authors": ["Carmel Mary Esther A"], "title": "Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making", "categories": ["q-bio.NC", "cs.AI", "physics.hist-ph"], "comment": "8 pages, 3 figures", "summary": "This paper proposes a novel theoretical model to explain how the human mind\nand artificial intelligence can approach real-time awareness by reducing\nperceptual delays. By investigating cosmic signal delay, neurological reaction\ntimes, and the ancient cognitive state of stillness, we explore how one may\nshift from reactive perception to a conscious interface with the near future.\nThis paper introduces both a physical and cognitive model for perceiving the\npresent not as a linear timestamp, but as an interference zone where\nearly-arriving cosmic signals and reactive human delays intersect. We propose\nexperimental approaches to test these ideas using human neural observation and\nneuro-receptive extensions. Finally, we propose a mathematical framework to\nguide the evolution of AI systems toward temporally efficient, ethically sound,\nand internally conscious decision-making processes", "AI": {"tldr": "This paper proposes a novel theoretical model for real-time awareness by reducing perceptual delays, through investigating cosmic signal delay, neurological reaction times and the ancient cognitive state of stillness.", "motivation": "To explain how human mind and artificial intelligence can approach real-time awareness by reducing perceptual delays.", "method": "Investigating cosmic signal delay, neurological reaction times, and the ancient cognitive state of stillness to shift from reactive perception to a conscious interface with the near future.", "result": "Introduction of a physical and cognitive model for perceiving the present as an interference zone where early-arriving cosmic signals and reactive human delays intersect, along with experimental approaches using human neural observation and neuro-receptive extensions.", "conclusion": "Proposes a mathematical framework to guide AI systems toward temporally efficient, ethically sound, and internally conscious decision-making processes."}}
{"id": "2505.09955", "pdf": "https://arxiv.org/pdf/2505.09955", "abs": "https://arxiv.org/abs/2505.09955", "authors": ["Jaeho Kim", "Seulki Lee"], "title": "TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 Accept", "summary": "Unsupervised domain adaptation (UDA) for time series data remains a critical\nchallenge in deep learning, with traditional pseudo-labeling strategies failing\nto capture temporal patterns and channel-wise shifts between domains, producing\nsub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that\naddresses these limitations by modeling the joint distribution $P(\\mathbf{X},\ny)$ of the source domain through code transition matrices, where the codes are\nderived from vector quantization (VQ) of time series patches. Our method\nconstructs class- and channel-wise code transition matrices from the source\ndomain and employs Bayes' rule for target domain adaptation, generating\npseudo-labels based on channel-wise weighted class-conditional likelihoods.\nTransPL offers three key advantages: explicit modeling of temporal transitions\nand channel-wise shifts between different domains, versatility towards\ndifferent UDA scenarios (e.g., weakly-supervised UDA), and explainable\npseudo-label generation. We validate TransPL's effectiveness through extensive\nanalysis on four time series UDA benchmarks and confirm that it consistently\noutperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%\naccuracy improvement, 4.9% F1 improvement), while providing interpretable\ninsights into the domain adaptation process through its learned code transition\nmatrices.", "AI": {"tldr": "The paper introduces TransPL, a novel approach for unsupervised domain adaptation (UDA) in time series data. It uses code transition matrices derived from vector quantization to model temporal transitions and channel-wise shifts between domains, generating explainable pseudo-labels using Bayes' rule.", "motivation": "Traditional pseudo-labeling strategies fail to capture temporal patterns and channel-wise shifts in UDA for time series data, resulting in sub-optimal pseudo-labels.", "method": "TransPL models the joint distribution of the source domain through code transition matrices derived from vector quantization of time series patches. It constructs class- and channel-wise code transition matrices and applies Bayes' rule for target domain adaptation, producing pseudo-labels based on channel-wise weighted class-conditional likelihoods.", "result": "TransPL outperforms state-of-the-art pseudo-labeling methods by a significant margin (6.1% accuracy improvement, 4.9% F1 improvement) on four time series UDA benchmarks.", "conclusion": "TransPL provides a robust solution for UDA in time series data, offering explicit modeling of temporal transitions and channel-wise shifts, versatility towards different UDA scenarios, and explainable pseudo-label generation."}}
{"id": "2505.10152", "pdf": "https://arxiv.org/pdf/2505.10152", "abs": "https://arxiv.org/abs/2505.10152", "authors": ["Yikang Wei"], "title": "Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization", "categories": ["cs.CV"], "comment": "IJCAI 2025", "summary": "Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6e90\u534f\u540c\u98ce\u683c\u589e\u5f3a\u4e0e\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u65b9\u6cd5\uff08MCSAD\uff09\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u534f\u540c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u5728\u672a\u89c1\u76ee\u6807\u9886\u57df\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u3002", "motivation": "\u8054\u5408\u9886\u57df\u6cdb\u5316\u7684\u76ee\u6807\u662f\u4ece\u591a\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u6e90\u9886\u57df\u4e2d\u5b66\u4e60\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u90e8\u7f72\u5230\u672a\u89c1\u7684\u76ee\u6807\u9886\u57df\u3002\u5c3d\u7ba1\u98ce\u683c\u589e\u5f3a\u65b9\u6cd5\u5728\u9886\u57df\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7684\u98ce\u683c\u589e\u5f3a\u65b9\u6cd5\u8981\u4e48\u63a2\u7d22\u5b64\u7acb\u6e90\u9886\u57df\u7684\u6570\u636e\u98ce\u683c\uff0c\u8981\u4e48\u5728\u6570\u636e\u53bb\u4e2d\u5fc3\u5316\u573a\u666f\u4e0b\u8de8\u73b0\u6709\u6e90\u9886\u57df\u63d2\u503c\u98ce\u683c\u4fe1\u606f\uff0c\u8fd9\u5bfc\u81f4\u4e86\u98ce\u683c\u7a7a\u95f4\u6709\u9650\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u6e90\u534f\u540c\u98ce\u683c\u589e\u5f3a\u4e0e\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u65b9\u6cd5\uff08MCSAD\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6e90\u534f\u540c\u98ce\u683c\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u751f\u6210\u66f4\u5e7f\u6cdb\u98ce\u683c\u7a7a\u95f4\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u548c\u7c7b\u522b\u5173\u7cfb\u96c6\u6210\u84b8\u998f\uff0c\u5728\u539f\u59cb\u6570\u636e\u548c\u589e\u5f3a\u6570\u636e\u4e4b\u95f4\u8fdb\u884c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\uff0c\u4ece\u800c\u5b66\u4e60\u5230\u4e00\u4e2a\u9886\u57df\u4e0d\u53d8\u7684\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u534f\u540c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5728\u672a\u89c1\u76ee\u6807\u9886\u57df\u4e0a\u8868\u73b0\u826f\u597d\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u5408\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684MCSAD\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u98ce\u683c\u7a7a\u95f4\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u672a\u89c1\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u8054\u5408\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.10389", "pdf": "https://arxiv.org/pdf/2505.10389", "abs": "https://arxiv.org/abs/2505.10389", "authors": ["Benjamin White", "Anastasia Shimorina"], "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.", "AI": {"tldr": "This paper explores the design of an aspect-based sentiment analysis system using LLMs for real-world use, focusing on quadruple opinion extraction and demonstrating that a combined multi-domain model achieves performance comparable to specialized single-domain models.", "motivation": "To investigate the effectiveness of a single fine-tuned model in handling multiple domain-specific taxonomies simultaneously for aspect-based sentiment analysis.", "method": "Designing an aspect-based sentiment analysis system using large language models with focus on quadruple opinion extraction including aspect categories, sentiment polarity, targets, and opinion expressions from text data across different domains and languages.", "result": "A combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity.", "conclusion": "Lessons learned for handling non-extractive predictions and evaluating various failure modes when developing LLM-based systems for structured prediction tasks are shared."}}
{"id": "2505.09651", "pdf": "https://arxiv.org/pdf/2505.09651", "abs": "https://arxiv.org/abs/2505.09651", "authors": ["Xixuan Hao", "Yutian Jiang", "Xingchen Zou", "Jiabo Liu", "Yifang Yin", "Yuxuan Liang"], "title": "Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": null, "summary": "Location Intelligence (LI), the science of transforming location-centric\ngeospatial data into actionable knowledge, has become a cornerstone of modern\nspatial decision-making. The rapid evolution of Geospatial Representation\nLearning is fundamentally reshaping LI development through two successive\ntechnological revolutions: the deep learning breakthrough and the emerging\nlarge language model (LLM) paradigm. While deep neural networks (DNNs) have\ndemonstrated remarkable success in automated feature extraction from structured\ngeospatial data (e.g., satellite imagery, GPS trajectories), the recent\nintegration of LLMs introduces transformative capabilities for cross-modal\ngeospatial reasoning and unstructured geo-textual data processing. This survey\npresents a comprehensive review of geospatial representation learning across\nboth technological eras, organizing them into a structured taxonomy based on\nthe complete pipeline comprising: (1) data perspective, (2) methodological\nperspective and (3) application perspective. We also highlight current\nadvancements, discuss existing limitations, and propose potential future\nresearch directions in the LLM era. This work offers a thorough exploration of\nthe field and providing a roadmap for further innovation in LI. The summary of\nthe up-to-date paper list can be found in\nhttps://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo\ncontinuous updates.", "AI": {"tldr": "Location Intelligence (LI) evolves through two revolutions: deep learning and large language models (LLMs). This survey reviews geospatial representation learning across both eras, organizing advancements into data, methodology, and application perspectives. It highlights current progress, limitations, and future research directions in the LLM era.", "motivation": "The field of Location Intelligence is being reshaped by advancements in deep learning and large language models, offering new capabilities for processing structured and unstructured geospatial data.", "method": "The paper provides a comprehensive review of geospatial representation learning, categorizing it into a taxonomy based on data perspective, methodological perspective, and application perspective across two technological eras.", "result": "The survey outlines current advancements and limitations in geospatial representation learning and suggests potential future research directions, particularly in the context of LLMs.", "conclusion": "This work aims to provide a roadmap for further innovation in Location Intelligence, summarizing key developments and pointing towards promising areas for exploration."}}
{"id": "2505.09959", "pdf": "https://arxiv.org/pdf/2505.09959", "abs": "https://arxiv.org/abs/2505.09959", "authors": ["Zengxia Guo", "Bohui An", "Zhongqi Lu"], "title": "Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated reinforcement learning (FRL) methods usually share the encrypted\nlocal state or policy information and help each client to learn from others\nwhile preserving everyone's privacy. In this work, we propose that sharing the\napproximated behavior metric-based state projection function is a promising way\nto enhance the performance of FRL and concurrently provides an effective\nprotection of sensitive information. We introduce FedRAG, a FRL framework to\nlearn a computationally practical projection function of states for each client\nand aggregating the parameters of projection functions at a central server. The\nFedRAG approach shares no sensitive task-specific information, yet provides\ninformation gain for each client. We conduct extensive experiments on the\nDeepMind Control Suite to demonstrate insightful results.", "AI": {"tldr": "In this paper, the authors propose FedRAG, a federated reinforcement learning (FRL) framework that enhances performance and protects privacy by sharing an approximated behavior metric-based state projection function rather than sensitive information.", "motivation": "Current FRL methods share encrypted local state or policy information to help clients learn from each other while preserving privacy. The authors aim to explore a new way to enhance FRL performance and protect sensitive information more effectively.", "method": "The authors introduce FedRAG, which learns a computationally practical projection function of states for each client and aggregates the parameters of these functions at a central server. This approach shares no task-specific sensitive information but still provides information gain for each client.", "result": "Extensive experiments on the DeepMind Control Suite demonstrate insightful results, indicating that the proposed method is effective in enhancing FRL performance while protecting privacy.", "conclusion": "FedRAG is a promising FRL framework that enhances performance and protects privacy by sharing approximated behavior metric-based state projection functions."}}
{"id": "2505.10169", "pdf": "https://arxiv.org/pdf/2505.10169", "abs": "https://arxiv.org/abs/2505.10169", "authors": ["Matthias K\u00fcmmerer", "Harneet Khanuja", "Matthias Bethge"], "title": "Modeling Saliency Dataset Bias", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.", "AI": {"tldr": "\u5c3d\u7ba1\u5728\u56fe\u50cf\u663e\u8457\u6027\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u96c6\u504f\u5dee\uff0c\u8de8\u591a\u4e2a\u663e\u8457\u6027\u6570\u636e\u96c6\u9884\u6d4b\u6ce8\u89c6\u70b9\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u6269\u5c55\u51e0\u4e4e\u4e0e\u6570\u636e\u96c6\u65e0\u5173\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u5c11\u4e8e20\u4e2a\u6570\u636e\u96c6\u7279\u5b9a\u53c2\u6570\u6765\u63a7\u5236\u53ef\u89e3\u91ca\u673a\u5236\uff0c\u5982\u591a\u5c3a\u5ea6\u7ed3\u6784\u3001\u4e2d\u5fc3\u504f\u5dee\u548c\u6ce8\u89c6\u70b9\u5206\u5e03\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u8be5\u6a21\u578b\u5728MIT/Tuebingen\u663e\u8457\u6027\u57fa\u51c6\u7684\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1\u5f53\u524d\u56fe\u50cf\u663e\u8457\u6027\u9884\u6d4b\u65b9\u6cd5\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u63a5\u8fd1\u9ec4\u91d1\u6807\u51c6\u6027\u80fd\u6c34\u5e73\uff0c\u4f46\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e4b\u95f4\u5e94\u7528\u6a21\u578b\u65f6\u5b58\u5728\u663e\u8457\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff08\u7ea640%\uff09\uff0c\u5e76\u4e14\u589e\u52a0\u6570\u636e\u96c6\u591a\u6837\u6027\u5e76\u4e0d\u80fd\u89e3\u51b3\u8fd9\u4e00\u8de8\u6570\u636e\u96c6\u5dee\u8ddd\uff0c\u7ea660%\u7684\u5dee\u8ddd\u5f52\u56e0\u4e8e\u6570\u636e\u96c6\u7279\u5b9a\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u51b3\u5269\u4f59\u6cdb\u5316\u5dee\u8ddd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u6269\u5c55\u4e86\u4e00\u4e2a\u51e0\u4e4e\u4e0e\u6570\u636e\u96c6\u65e0\u5173\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u4e86\u5c11\u4e8e20\u4e2a\u6570\u636e\u96c6\u7279\u5b9a\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u63a7\u5236\u8bf8\u5982\u591a\u5c3a\u5ea6\u7ed3\u6784\u3001\u4e2d\u5fc3\u504f\u5dee\u548c\u6ce8\u89c6\u70b9\u5206\u5e03\u7b49\u53ef\u89e3\u91ca\u673a\u5236\u3002\u4ec5\u901a\u8fc7\u9002\u5e94\u8fd9\u4e9b\u53c2\u6570\u5230\u65b0\u6570\u636e\uff0c\u53ef\u4ee5\u5f25\u8865\u8d85\u8fc775%\u7684\u6cdb\u5316\u5dee\u8ddd\uff0c\u4e14\u5373\u4f7f\u53ea\u670950\u4e2a\u6837\u672c\u4e5f\u80fd\u5b9e\u73b0\u5927\u90e8\u5206\u6539\u8fdb\u3002", "result": "\u8be5\u6a21\u578b\u5728MIT/Tuebingen\u663e\u8457\u6027\u57fa\u51c6\u7684\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\uff08MIT300\u3001CAT2000\u548cCOCO-Freeview\uff09\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5373\u4f7f\u5728\u4ece\u4e0d\u76f8\u5173\u6570\u636e\u96c6\u7eaf\u6cdb\u5316\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\uff0c\u4f46\u5728\u9002\u5e94\u5404\u81ea\u8bad\u7ec3\u6570\u636e\u96c6\u65f6\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u63d0\u4f9b\u4e86\u6709\u5173\u7a7a\u95f4\u663e\u8457\u6027\u7279\u6027\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u7ed3\u5408\u7edd\u5bf9\u548c\u76f8\u5bf9\u5927\u5c0f\u7684\u590d\u6742\u591a\u5c3a\u5ea6\u6548\u5e94\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u5c11\u91cf\u6570\u636e\u96c6\u7279\u5b9a\u53c2\u6570\u89e3\u51b3\u4e86\u663e\u8457\u6027\u9884\u6d4b\u4e2d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u7a7a\u95f4\u663e\u8457\u6027\u7279\u6027\u7684\u6df1\u5165\u7406\u89e3\u3002"}}
{"id": "2505.10402", "pdf": "https://arxiv.org/pdf/2505.10402", "abs": "https://arxiv.org/abs/2505.10402", "authors": ["Yihong Dong", "Yuchen Liu", "Xue Jiang", "Zhi Jin", "Ge Li"], "title": "Rethinking Repetition Problems of LLMs in Code Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025 (main)", "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.", "AI": {"tldr": "\u968f\u7740\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ee3\u7801\u751f\u6210\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4f46\u91cd\u590d\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002\u672c\u6587\u6b63\u5f0f\u5b9a\u4e49\u4e86\u7ed3\u6784\u91cd\u590d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u6cd5\u7684\u91cd\u590d\u60e9\u7f5a\u89e3\u7801\u65b9\u6cd5RPG\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u51cf\u5c11\u91cd\u590d\u6a21\u5f0f\u6765\u6539\u5584\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cRPG\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u590d\u5e76\u63d0\u5347\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4ecd\u5b58\u5728\u91cd\u590d\u95ee\u9898\u3002\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5185\u5bb9\u91cd\u590d\uff0c\u800c\u66f4\u666e\u904d\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u662f\u7ed3\u6784\u91cd\u590d\uff0c\u5373\u4ee5\u56fa\u5b9a\u7ed3\u6784\u51fa\u73b0\u5728\u4e0d\u540c\u6a21\u5f0f\u4e2d\u7684\u91cd\u590d\u4ee3\u7801\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRPG\uff08Repetition Penalization based on Grammar\uff09\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u9996\u5148\u5229\u7528\u8bed\u6cd5\u89c4\u5219\u8bc6\u522b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u964d\u4f4e\u5bfc\u81f4\u91cd\u590d\u7684\u5173\u952e\u6807\u8bb0\u7684\u6982\u7387\u6765\u7f13\u89e3\u91cd\u590d\u73b0\u8c61\u3002\u540c\u65f6\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6CodeRepetEval\u4ee5\u5168\u9762\u8bc4\u4f30\u89e3\u51b3\u91cd\u590d\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRPG\u5728CodeRepetEval\u6570\u636e\u96c6\u4ee5\u53caHumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u590d\u73b0\u8c61\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u3002", "conclusion": "RPG\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.09653", "pdf": "https://arxiv.org/pdf/2505.09653", "abs": "https://arxiv.org/abs/2505.09653", "authors": ["Samuel Yen-Chi Chen", "Chen-Yu Liu", "Kuan-Cheng Chen", "Wei-Jia Huang", "Yen-Jui Chang", "Wei-Hao Huang"], "title": "Differentiable Quantum Architecture Search in Quantum-Enhanced Neural Network Parameter Generation", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG", "cs.NE"], "comment": null, "summary": "The rapid advancements in quantum computing (QC) and machine learning (ML)\nhave led to the emergence of quantum machine learning (QML), which integrates\nthe strengths of both fields. Among QML approaches, variational quantum\ncircuits (VQCs), also known as quantum neural networks (QNNs), have shown\npromise both empirically and theoretically. However, their broader adoption is\nhindered by reliance on quantum hardware during inference. Hardware\nimperfections and limited access to quantum devices pose practical challenges.\nTo address this, the Quantum-Train (QT) framework leverages the exponential\nscaling of quantum amplitudes to generate classical neural network parameters,\nenabling inference without quantum hardware and achieving significant parameter\ncompression. Yet, designing effective quantum circuit architectures for such\nquantum-enhanced neural programmers remains non-trivial and often requires\nexpertise in quantum information science. In this paper, we propose an\nautomated solution using differentiable optimization. Our method jointly\noptimizes both conventional circuit parameters and architectural parameters in\nan end-to-end manner via automatic differentiation. We evaluate the proposed\nframework on classification, time-series prediction, and reinforcement learning\ntasks. Simulation results show that our method matches or outperforms manually\ndesigned QNN architectures. This work offers a scalable and automated pathway\nfor designing QNNs that can generate classical neural network parameters across\ndiverse applications.", "AI": {"tldr": "The paper proposes an automated solution using differentiable optimization to design quantum neural networks (QNNs) that can generate classical neural network parameters. This method jointly optimizes both conventional circuit parameters and architectural parameters in an end-to-end manner via automatic differentiation.", "motivation": "The motivation is the need for effective quantum circuit architectures for quantum-enhanced neural programmers, which remains non-trivial and often requires expertise in quantum information science.", "method": "The method uses differentiable optimization to jointly optimize both conventional circuit parameters and architectural parameters in an end-to-end manner via automatic differentiation.", "result": "Simulation results show that the proposed method matches or outperforms manually designed QNN architectures on classification, time-series prediction, and reinforcement learning tasks.", "conclusion": "This work offers a scalable and automated pathway for designing QNNs that can generate classical neural network parameters across diverse applications."}}
{"id": "2505.09969", "pdf": "https://arxiv.org/pdf/2505.09969", "abs": "https://arxiv.org/abs/2505.09969", "authors": ["Ali Azimi Lamir", "Shiva Razzagzadeh", "Zeynab Rezaei"], "title": "A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study presents a machine learning-based framework for heart disease\nprediction using the heart-disease dataset, comprising 303 samples with 14\nfeatures. The methodology involves data preprocessing, model training, and\nevaluation using three classifiers: Logistic Regression, K-Nearest Neighbors\n(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and\nRandomizedSearchCV was employed to enhance model performance. The Random Forest\nclassifier outperformed other models, achieving an accuracy of 91% and an\nF1-score of 0.89. Evaluation metrics, including precision, recall, and\nconfusion matrix, revealed balanced performance across classes. The proposed\nmodel demonstrates strong potential for aiding clinical decision-making by\neffectively predicting heart disease. Limitations such as dataset size and\ngeneralizability underscore the need for future studies using larger and more\ndiverse datasets. This work highlights the utility of machine learning in\nhealthcare, offering insights for further advancements in predictive\ndiagnostics.", "AI": {"tldr": "A machine learning framework for heart disease prediction using 303 samples and 14 features was developed. Random Forest classifier achieved the best performance with 91% accuracy, showing potential in clinical decision-making.", "motivation": "To develop an effective predictive model for heart disease using machine learning techniques to aid clinical decision-making.", "method": "The study utilized a heart-disease dataset with 303 samples and 14 features. Data preprocessing was performed followed by model training and evaluation using Logistic Regression, KNN, and Random Forest classifiers. Hyperparameter tuning was done using GridSearchCV and RandomizedSearchCV.", "result": "Random Forest classifier outperformed other models with an accuracy of 91% and F1-score of 0.89. Evaluation metrics indicated balanced performance across classes.", "conclusion": "The proposed machine learning-based framework shows strong potential for aiding clinical decision-making in predicting heart disease. However, limitations such as dataset size and generalizability highlight the need for future studies."}}
{"id": "2505.10205", "pdf": "https://arxiv.org/pdf/2505.10205", "abs": "https://arxiv.org/abs/2505.10205", "authors": ["Umair Haroon", "Ahmad AlMughrabi", "Thanasis Zoumpekas", "Ricardo Marques", "Petia Radeva"], "title": "VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.", "AI": {"tldr": "VolE is a novel framework leveraging mobile device-driven 3D reconstruction to accurately estimate food volume without reference or depth information, achieving 2.22% MAPE and outperforming existing techniques.", "motivation": "Accurate food volume estimation is essential for medical nutrition management and health monitoring but current methods are limited by mononuclear data, single-purpose hardware, sensor-oriented information, or reliance on camera calibration using a reference object.", "method": "VolE captures images and camera locations in free motion using AR-capable mobile devices to generate precise 3D models. It uses food video segmentation for food mask generation and is a reference- and depth-free framework.", "result": "VolE outperforms existing volume estimation techniques across multiple datasets with a 2.22% MAPE.", "conclusion": "VolE presents a new approach to food volume estimation that is more accurate and less reliant on specific hardware or conditions."}}
{"id": "2505.10409", "pdf": "https://arxiv.org/pdf/2505.10409", "abs": "https://arxiv.org/abs/2505.10409", "authors": ["Yue Guo", "Jae Ho Sohn", "Gondy Leroy", "Trevor Cohen"], "title": "Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.", "AI": {"tldr": "This paper evaluates large language models (LLMs) in generating plain language summaries (PLSs) of health information using subjective and objective measures. Results show LLM-generated PLSs match human-written ones in subjective evaluations but lag behind in improving reader comprehension. Automated metrics poorly align with human judgment.", "motivation": "To address the unclear effectiveness of LLMs in supporting health information comprehension and the limitations of prior evaluations that either rely on automated scores or subjective Likert-scale ratings with limited generalizability.", "method": "Conduct a large-scale crowdsourced evaluation using Amazon Mechanical Turk with 150 participants. Assess PLS quality through subjective Likert-scale ratings focusing on simplicity, informativeness, coherence, faithfulness, and objective multiple-choice comprehension and recall measures. Examine the alignment between 10 automated evaluation metrics and human judgments.", "result": "LLM-generated PLSs appear indistinguishable from human-written ones in subjective evaluations but lead to significantly worse comprehension outcomes. Automated evaluation metrics fail to reflect human judgment accurately.", "conclusion": "There is a need for evaluation frameworks that go beyond surface-level quality and for generation methods that explicitly optimize for layperson comprehension."}}
{"id": "2505.09661", "pdf": "https://arxiv.org/pdf/2505.09661", "abs": "https://arxiv.org/abs/2505.09661", "authors": ["Jinghao He", "Zhengyan Sheng", "Liping Chen", "Kong Aik Lee", "Zhen-Hua Ling"], "title": "Introducing voice timbre attribute detection", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper focuses on explaining the timbre conveyed by speech signals and\nintroduces a task termed voice timbre attribute detection (vTAD). In this task,\nvoice timbre is explained with a set of sensory attributes describing its human\nperception. A pair of speech utterances is processed, and their intensity is\ncompared in a designated timbre descriptor. Moreover, a framework is proposed,\nwhich is built upon the speaker embeddings extracted from the speech\nutterances. The investigation is conducted on the VCTK-RVA dataset.\nExperimental examinations on the ECAPA-TDNN and FACodec speaker encoders\ndemonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the\nseen scenario, where the testing speakers were included in the training set; 2)\nthe FACodec speaker encoder was superior in the unseen scenario, where the\ntesting speakers were not part of the training, indicating enhanced\ngeneralization capability. The VCTK-RVA dataset and open-source code are\navailable on the website https://github.com/vTAD2025-Challenge/vTAD.", "AI": {"tldr": "This paper introduces voice timbre attribute detection (vTAD) to explain voice timbre using sensory attributes. A framework built on speaker embeddings is proposed and tested on VCTK-RVA dataset. ECAPA-TDNN performs better in seen scenarios, while FACodec shows superior generalization in unseen scenarios.", "motivation": "To explain the timbre conveyed by speech signals through a set of sensory attributes describing its human perception.", "method": "Propose a framework based on speaker embeddings extracted from speech utterances for voice timbre attribute detection (vTAD). Investigate using VCTK-RVA dataset with ECAPA-TDNN and FACodec speaker encoders.", "result": "ECAPA-TDNN outperforms in seen scenarios where testing speakers are part of training set; FACodec is superior in unseen scenarios indicating better generalization capability.", "conclusion": "VCTK-RVA dataset and open-source code are provided for future research on voice timbre attribute detection."}}
{"id": "2505.09983", "pdf": "https://arxiv.org/pdf/2505.09983", "abs": "https://arxiv.org/abs/2505.09983", "authors": ["Changxun Zhu", "Qilong Wu", "Lingjuan Lyu", "Shibei Xue"], "title": "Sybil-based Virtual Data Poisoning Attacks in Federated Learning", "categories": ["cs.LG"], "comment": "7 pages, 6 figures, accepted by IEEE Codit 2025", "summary": "Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data.", "AI": {"tldr": "In order to reduce the cost of poisoning attacks in federated learning, this paper proposes a sybil-based virtual data poisoning attack method, which can generate sybil nodes to amplify the impact of poisoning model and uses gradient matching to reduce the computational complexity. In addition, three schemes for target model acquisition are designed for different scenarios.", "motivation": "Federated learning is vulnerable to poisoning attacks by malicious adversaries, but existing methods often involve high costs to achieve effective attacks.", "method": "The authors propose a sybil-based virtual data poisoning attack, where a malicious client generates sybil nodes to amplify the poisoning model's impact. They also develop a virtual data generation method based on gradient matching to reduce neural network computational complexity and design three schemes for target model acquisition applicable to online local, online global, and offline scenarios.", "result": "In simulation, the proposed method outperforms other attack algorithms because it can obtain a global target model under non-independent uniformly distributed data.", "conclusion": "The sybil-based virtual data poisoning attack with gradient matching can effectively reduce the cost of poisoning attacks in federated learning and perform well in different scenarios."}}
{"id": "2505.10223", "pdf": "https://arxiv.org/pdf/2505.10223", "abs": "https://arxiv.org/abs/2505.10223", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MIDL 2025", "summary": "Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.", "AI": {"tldr": "\u901a\u8fc7MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u7b49\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u6539\u5584\u5fc3\u810f\u7535\u5f71MRI\u548c\u524d\u5217\u817aMRI\u5206\u5272\u4e2d\u5bf9\u5404\u79cd\u53d8\u6362\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u63d0\u5347\u7279\u5f81\u8868\u793a\u7684\u53ef\u5206\u6027\u548c\u7d27\u51d1\u6027\u6765\u589e\u5f3annU-Net\u8bad\u7ec3\u7ba1\u9053\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u901a\u5e38\u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u4f20\u7edf\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u7f3a\u4e4f\u5e94\u5bf9\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u6240\u9700\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u7b49\u66ff\u4ee3\u589e\u5f3a\u7b56\u7565\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u660e\u786e\u9488\u5bf9\u7279\u5b9a\u7684\u5206\u5e03\u53d8\u5316\u6765\u6e90\uff0c\u800c\u662f\u7f13\u89e3\u591a\u79cd\u53d8\u5316\u7684\u5f71\u54cd\u3002\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u96c6\u6210\u5230nnU-Net\u8bad\u7ec3\u7ba1\u9053\u4e2d\u3002", "result": "\u5b9a\u91cf\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u589e\u5f3a\u65b9\u6cd5\u901a\u8fc7\u4fc3\u8fdb\u53ef\u5206\u6027\u548c\u7d27\u51d1\u6027\u63d0\u9ad8\u4e86\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8868\u793a\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u5728\u5404\u79cd\u53d8\u6362\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u7b49\u65b9\u6cd5\u4e3a\u63d0\u9ad8\u533b\u5b66\u5206\u5272\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u5b9e\u73b0\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10413", "pdf": "https://arxiv.org/pdf/2505.10413", "abs": "https://arxiv.org/abs/2505.10413", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Zhicheng Dou"], "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.", "AI": {"tldr": "LongRefiner is an efficient plug-and-play refiner designed for long-context input scenarios in RAG applications, which reduces inference costs and enhances performance.", "motivation": "Real-world RAG applications face challenges with long-context inputs due to redundant information and noise, leading to higher inference costs and reduced performance.", "method": "LongRefiner uses dual-level query analysis, hierarchical document structuring, and adaptive refinement via multi-task learning on a single foundation model.", "result": "Experiments on seven QA datasets show that LongRefiner performs competitively while using 10x fewer computational resources and latency compared to the best baseline.", "conclusion": "LongRefiner is scalable, efficient, and effective, offering valuable insights for practical long-text RAG applications."}}
{"id": "2505.10003", "pdf": "https://arxiv.org/pdf/2505.10003", "abs": "https://arxiv.org/abs/2505.10003", "authors": ["Tianyu Jiao", "Zhuoran Xiao", "Yihang Huang", "Chenhui Ye", "Yijia Feng", "Liyu Cai", "Jiang Chang", "Fangkun Liu", "Yin Xu", "Dazhi He", "Yunfeng Guan", "Wenjun Zhang"], "title": "AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets.", "AI": {"tldr": "The paper introduces AI2MMUM, a scalable model for executing diverse 6G air interface tasks by processing multi-modal data, showing SOTA performance in physical layer tasks.", "motivation": "To design a universal model for future wireless systems that can handle multi-modal data and various air interface tasks.", "method": "Proposes AI2MMUM with LLM backbone for contextual comprehension, fine-tuning for domain-specific knowledge, task instructions with keywords and prompts, frozen radio modality encoders, adapter layers, and lightweight task-specific heads.", "result": "Achieves state-of-the-art performance in five representative physical environment/wireless channel-based downstream tasks using WAIR-D and DeepMIMO datasets.", "conclusion": "AI2MMUM is a promising approach for flexible and effective execution of various physical layer tasks in 6G-oriented wireless systems."}}
{"id": "2505.10231", "pdf": "https://arxiv.org/pdf/2505.10231", "abs": "https://arxiv.org/abs/2505.10231", "authors": ["Haozhe Luo", "Ziyu Zhou", "Zixin Shu", "Aur\u00e9lie Pahud de Mortanges", "Robert Berke", "Mauricio Reyes"], "title": "On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.", "AI": {"tldr": "Deep neural networks in medical imaging can have biases causing fairness gaps. This study explores Human-AI alignment and fairness, finding that human insights reduce these gaps and improve generalization, but excessive alignment may compromise performance. A balanced strategy is needed for fair and robust medical AI systems.", "motivation": "To systematically explore the impact of Human-AI alignment on fairness and out-of-domain generalization in medical imaging, where deep neural networks are effective but prone to biases leading to fairness gaps across demographic groups.", "method": "Incorporating human insights into AI models in the medical imaging domain to examine how it affects fairness gaps and out-of-domain generalization. The study also investigates potential trade-offs from excessive alignment.", "result": "Human insights consistently reduce fairness gaps and enhance out-of-domain generalization in medical AI systems. However, excessive alignment can introduce performance trade-offs, indicating the importance of calibrated strategies.", "conclusion": "Human-AI alignment offers a promising approach for developing fair, robust, and generalizable medical AI systems, requiring a balance between expert guidance and automated efficiency."}}
{"id": "2505.10446", "pdf": "https://arxiv.org/pdf/2505.10446", "abs": "https://arxiv.org/abs/2505.10446", "authors": ["Zemin Huang", "Zhiyang Chen", "Zijun Wang", "Tiancheng Li", "Guo-Jun Qi"], "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.", "AI": {"tldr": "The paper introduces DCoLT, a reasoning framework for diffusion language models which optimizes the entire reasoning trajectory using Reinforcement Learning. It differs from traditional methods by allowing bidirectional, non-linear reasoning. The framework is implemented on two models - SEDD and LLaDA. Experiments show that DCoLT-reinforced models outperform others.", "motivation": "To create a more effective reasoning framework for diffusion language models that can optimize the entire reasoning process and not just individual steps.", "method": "DCoLT treats each intermediate step in the reverse diffusion process as a latent 'thinking' action and uses outcome-based Reinforcement Learning to optimize the entire reasoning trajectory. It is applied to two diffusion language models: SEDD and LLaDA.", "result": "Experiments on math and code generation tasks show that DCoLT-reinforced Diffusion Language Models outperform other models trained by SFT or RL. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy significantly on various benchmarks.", "conclusion": "DCoLT provides a novel approach to reasoning in diffusion language models, demonstrating superior performance compared to existing methods."}}
{"id": "2505.09698", "pdf": "https://arxiv.org/pdf/2505.09698", "abs": "https://arxiv.org/abs/2505.09698", "authors": ["Enyu Zhao", "Vedant Raval", "Hejia Zhang", "Jiageng Mao", "Zeyu Shangguan", "Stefanos Nikolaidis", "Yue Wang", "Daniel Seita"], "title": "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "47 pages, 29 figures. Under review", "summary": "Vision-Language Models (VLMs) have revolutionized artificial intelligence and\nrobotics due to their commonsense reasoning capabilities. In robotic\nmanipulation, VLMs are used primarily as high-level planners, but recent work\nhas also studied their lower-level reasoning ability, which refers to making\ndecisions about precise robot movements. However, the community currently lacks\na clear and common benchmark that can evaluate how well VLMs can aid low-level\nreasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,\nto evaluate the low-level robot manipulation reasoning capabilities of VLMs\nacross various dimensions, including how well they understand object-object\ninteractions and deformable object manipulation. We extensively test 33\nrepresentative VLMs across 10 model families on our benchmark, including\nvariants to test different model sizes. Our evaluation shows that the\nperformance of VLMs significantly varies across tasks, and there is a strong\ncorrelation between this performance and trends in our real-world manipulation\ntasks. It also shows that there remains a significant gap between these models\nand human-level understanding. See our website at:\nhttps://manipbench.github.io.", "AI": {"tldr": "Vision-Language Models (VLMs) have transformed AI and robotics. While used in high-level planning, recent studies explore their lower-level reasoning ability for precise robot movements. A new benchmark, ManipBench, is proposed to evaluate VLMs' low-level robotic manipulation capabilities across different dimensions. Testing 33 representative VLMs from 10 model families on this benchmark reveals significant performance variation across tasks, correlating with real-world manipulation trends, but a substantial gap remains between these models and human-level understanding.", "motivation": "To address the lack of a clear and common benchmark evaluating how well Vision-Language Models (VLMs) can aid low-level reasoning in robotics, particularly in understanding object-object interactions and deformable object manipulation.", "method": "Propose a novel benchmark called ManipBench to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions. Extensively test 33 representative VLMs from 10 model families on this benchmark, including variants testing different model sizes.", "result": "The evaluation shows significant variation in VLM performance across tasks, strong correlation between this performance and trends in real-world manipulation tasks, and a significant gap between these models and human-level understanding.", "conclusion": "ManipBench provides a comprehensive evaluation framework for VLMs' low-level robotic manipulation reasoning capabilities, revealing performance variations, correlations with real-world tasks, and highlighting the need for further advancements to approach human-level understanding."}}
{"id": "2505.10007", "pdf": "https://arxiv.org/pdf/2505.10007", "abs": "https://arxiv.org/abs/2505.10007", "authors": ["Zijun Chen", "Shengbo Wang", "Nian Si"], "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10238", "pdf": "https://arxiv.org/pdf/2505.10238", "abs": "https://arxiv.org/abs/2505.10238", "authors": ["Yanbo Ding"], "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.", "AI": {"tldr": "The paper introduces MTVCrafter, a framework using 4D motion tokens for human image animation that surpasses existing methods relying on 2D-rendered pose images.", "motivation": "Existing human image animation methods depend heavily on 2D-rendered pose images, limiting generalization and losing essential 3D information.", "method": "MTVCrafter proposes the use of 4DMoT to quantize 3D motion sequences into 4D motion tokens and MV-DiT with unique motion attention for effective leverage of these tokens as context for animation.", "result": "MTVCrafter achieves state-of-the-art results with an FID-VID score of 6.98, outperforming the second-best method by 65%. It also shows strong generalization across diverse characters and scenarios.", "conclusion": "MTVCrafter marks a significant advancement in human image animation by directly modeling raw 3D motion sequences and opens new possibilities for pose-guided human video generation."}}
{"id": "2505.10493", "pdf": "https://arxiv.org/pdf/2505.10493", "abs": "https://arxiv.org/abs/2505.10493", "authors": ["Shaohan Wang", "Licheng Zhang", "Zheren Fu", "Zhendong Mao"], "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.", "AI": {"tldr": "A new framework called CL-RAG is proposed, which uses multi-stage curriculum learning to train RAG systems more effectively. It constructs training data with multiple difficulty levels and optimizes the overall performance and generalization of the RAG system.", "motivation": "Existing methods for optimizing the retriever or generator in the RAG system do not consider the varying effectiveness of documents across user queries, hindering adaptation during training.", "method": "Propose a multi-stage Curriculum Learning based RAG system training framework named CL-RAG. Construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution and train the model in stages based on the curriculum learning approach.", "result": "CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.", "conclusion": "The CL-RAG framework can optimize the overall performance and generalization of the RAG system more effectively."}}
{"id": "2505.10010", "pdf": "https://arxiv.org/pdf/2505.10010", "abs": "https://arxiv.org/abs/2505.10010", "authors": ["Jing-Cheng Pang", "Kaiyuan Li", "Yidi Wang", "Si-Hang Yang", "Shengyi Jiang", "Yang Yu"], "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts", "categories": ["cs.LG"], "comment": null, "summary": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.", "AI": {"tldr": "A new benchmark, ImagineBench, is introduced to evaluate offline RL algorithms using real and LLM-imaginary rollouts. Current algorithms perform suboptimally on unseen tasks, with a 35.44% success rate on hard tasks compared to 64.37% when trained only on real data.", "motivation": "The motivation of this paper is to address the limitation in reinforcement learning (RL) which heavily relies on extensive real-world interaction data. The authors aim to create a standard benchmark for evaluating offline RL algorithms that use both real and synthetic (LLM-imaginary) rollouts to overcome this issue.", "method": "The method involves introducing ImagineBench, a comprehensive benchmark that includes datasets with environment-collected and LLM-imaginary rollouts, diverse domains covering locomotion, robotic manipulation, and navigation tasks, and natural language task instructions with varying complexity levels. State-of-the-art offline RL algorithms are systematically evaluated using this benchmark.", "result": "The result shows that applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, achieving only a 35.44% success rate on hard tasks compared to 64.37% when methods are trained on real rollouts for hard tasks.", "conclusion": "The conclusion highlights the need for advancements in algorithms to better utilize LLM-imaginary rollouts. Future research opportunities include better utilization of imaginary rollouts, fast online adaptation and continual learning, and extension to multi-modal tasks."}}
{"id": "2505.10250", "pdf": "https://arxiv.org/pdf/2505.10250", "abs": "https://arxiv.org/abs/2505.10250", "authors": ["Wenhao Shen", "Wanqi Yin", "Xiaofeng Yang", "Cheng Chen", "Chaoyue Song", "Zhongang Cai", "Lei Yang", "Hao Wang", "Guosheng Lin"], "title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR", "summary": "Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.", "AI": {"tldr": "The paper presents ADHMR, a framework that improves human mesh recovery by aligning a diffusion-based HMR model through preference optimization. It uses an assessment model (HMR-Scorer) to evaluate predictions and create a preference dataset for fine-tuning.", "motivation": "Human mesh recovery from a single image is challenging due to depth ambiguity and occlusions. Current probabilistic methods produce numerous plausible 3D human mesh predictions but often misalign with 2D image observations and lack robustness on in-the-wild images.", "method": "The authors propose ADHMR, which includes training a human mesh prediction assessment model called HMR-Scorer. This model can evaluate predictions for in-the-wild images without 3D annotations. They use HMR-Scorer to create a preference dataset consisting of winner and loser mesh predictions for each input image. This dataset is used to fine-tune the base model via direct preference optimization. Additionally, HMR-Scorer helps improve existing HMR models through data cleaning.", "result": "Extensive experiments demonstrate that ADHMR outperforms current state-of-the-art methods in human mesh recovery.", "conclusion": "ADHMR addresses the limitations of existing probabilistic methods by using preference optimization to better align 3D human mesh predictions with 2D image observations and enhance robustness for in-the-wild images."}}
{"id": "2505.10494", "pdf": "https://arxiv.org/pdf/2505.10494", "abs": "https://arxiv.org/abs/2505.10494", "authors": ["Yutao Mou", "Xiao Deng", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.", "AI": {"tldr": "Code security and usability are crucial for coding assistant applications driven by LLMs. Current benchmarks lack comprehensive assessment across dimensions. This paper proposes CoV-Eval, a multi-task benchmark for evaluating LLM code security, and VC-Judge, an improved judgment model. Experiments reveal challenges and offer insights for future research.", "motivation": "To address the limitations of current code security benchmarks that focus solely on single evaluation tasks and lack comprehensive assessment across dimensions like secure code generation, vulnerability repair and discrimination.", "method": "Propose CoV-Eval, a multi-task benchmark covering various tasks such as code completion, vulnerability repair, detection and classification. Develop VC-Judge, an improved judgment model aligning closely with human experts to review LLM-generated programs for vulnerabilities.", "result": "Most LLMs identify vulnerable codes well but tend to generate insecure codes and struggle with recognizing specific vulnerability types and performing repairs.", "conclusion": "Extensive experiments and qualitative analyses reveal key challenges and optimization directions, offering insights for future research in LLM code security."}}
{"id": "2505.10037", "pdf": "https://arxiv.org/pdf/2505.10037", "abs": "https://arxiv.org/abs/2505.10037", "authors": ["Takafumi Ito", "Lysenko Artem", "Tatsuhiko Tsunoda"], "title": "Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "comment": "10 pages, 3 figures", "summary": "Quantum-classical Hybrid Machine Learning (QHML) models are recognized for\ntheir robust performance and high generalization ability even for relatively\nsmall datasets. These qualities offer unique advantages for anti-cancer drug\nresponse prediction, where the number of available samples is typically small.\nHowever, such hybrid models appear to be very sensitive to the data encoding\nused at the interface of a neural network and a quantum circuit, with\nsuboptimal choices leading to stability issues. To address this problem, we\npropose a novel strategy that uses a normalization function based on a\nmoderated gradient version of the $\\tanh$. This method transforms the outputs\nof the neural networks without concentrating them at the extreme value ranges.\nOur idea was evaluated on a dataset of gene expression and drug response\nmeasurements for various cancer cell lines, where we compared the prediction\nperformance of a classical deep learning model and several QHML models. These\nresults confirmed that QHML performed better than the classical models when\ndata was optimally normalized. This study opens up new possibilities for\nbiomedical data analysis using quantum computers.", "AI": {"tldr": "Quantum-classical Hybrid Machine Learning (QHML) models show better performance in anti-cancer drug response prediction when data is optimally normalized.", "motivation": "Anti-cancer drug response prediction needs robust models that work well with small datasets. Quantum-classical Hybrid Machine Learning (QHML) models have this potential but are sensitive to data encoding.", "method": "Propose a normalization function based on a moderated gradient version of the $\\tanh$ to transform neural network outputs without concentrating them at extreme value ranges.", "result": "Evaluated on a dataset of gene expression and drug response measurements, QHML models performed better than classical deep learning models when data was optimally normalized.", "conclusion": "This study shows new possibilities for biomedical data analysis using quantum computers."}}
{"id": "2505.10257", "pdf": "https://arxiv.org/pdf/2505.10257", "abs": "https://arxiv.org/abs/2505.10257", "authors": ["Hao Lu", "Jiaqi Tang", "Jiyao Wang", "Yunfan LU", "Xu Cao", "Qingyong Hu", "Yin Wang", "Yuting Zhang", "Tianxin Xie", "Yunpeng Zhang", "Yong Chen", "Jiayu. Gao", "Bin Huang", "Dengbo He", "Shuiguang Deng", "Hao Chen", "Ying-Cong Chen"], "title": "Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot", "categories": ["cs.CV"], "comment": null, "summary": "The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.", "AI": {"tldr": "The paper introduces SAGE DeeR, a driving agent with super alignment, generalist capabilities and self-eliciting features.", "motivation": "To meet different users' comfort, interaction, and safety needs in intelligent driving cockpits.", "method": "SAGE DeeR achieves super alignment by reacting to individual preferences and biases, generalist by understanding multi-view and multi-mode inputs, and self-eliciting by revealing implicit thought chains in the language space.", "result": "Collected multiple datasets and built a large-scale benchmark for measuring perceptual decision-making ability and super alignment accuracy.", "conclusion": "SAGE DeeR aims to fulfill diverse user requirements in the intelligent driving cockpit through its unique abilities."}}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507", "abs": "https://arxiv.org/abs/2505.10507", "authors": ["Benedikt Ebing", "Goran Glava\u0161"], "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u8bcd\u5bf9\u9f50\u5de5\u5177(WA)\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u8de8\u8bed\u8a00\u8fc1\u79fb(XLT)\u5728\u6807\u8bb0\u6295\u5f71\u4e2d\u7684\u8868\u73b0\u53ef\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u7ed3\u5408\u4e86translate-train\u548ctranslate-test\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6295\u5f71\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5bf9WA\u4f4e\u7ea7\u8bbe\u8ba1\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86XLT\u5728\u5206\u8bcd\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u8bcd\u5bf9\u9f50\u5de5\u5177(WA)\u5e38\u7528\u4e8e\u6807\u8bb0\u6295\u5f71\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u57fa\u4e8e\u7ffb\u8bd1\u7684XLT\u7684\u4f4e\u7ea7\u8bbe\u8ba1\u51b3\u7b56\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\uff1b\u6b64\u5916\uff0c\u6700\u8fd1\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u58f0\u79f0\u5728XLT\u7684\u6807\u8bb0\u6295\u5f71\u4e2d\u4f18\u4e8eWA\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u91cd\u65b0\u5ba1\u89c6WA\u5e76\u63a2\u7d22\u5176\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u5f71\u54cd\u57fa\u4e8e\u7ffb\u8bd1\u7684XLT\u6027\u80fd\u7684\u4f4e\u7ea7\u8bbe\u8ba1\u51b3\u7b56\uff0c\u5305\u62ec\uff1a\u6807\u7b7e\u5728(\u591a)\u6807\u8bb0\u8de8\u5ea6\u4e4b\u95f4\u7684\u6295\u5f71\u7b97\u6cd5\u3001\u51cf\u5c11\u566a\u58f0\u6620\u5c04\u6807\u7b7e\u6570\u91cf\u7684\u8fc7\u6ee4\u7b56\u7565\u4ee5\u53ca\u7ffb\u8bd1\u53e5\u5b50\u7684\u9884\u5206\u8bcd\u5904\u7406\uff1b\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u7b56\u7565\uff0c\u96c6\u6210translate-train\u548ctranslate-test\u9884\u6d4b\u3002", "result": "\u4f18\u5316WA\u8bbe\u8ba1\u9009\u62e9\u540e\uff0c\u57fa\u4e8eWA\u7684XLT\u6027\u80fd\u81f3\u5c11\u53ef\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff1b\u65b0\u63d0\u51fa\u7684\u96c6\u6210\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6295\u5f71\uff0c\u5e76\u964d\u4f4e\u4e86\u5bf9WA\u4f4e\u7ea7\u8bbe\u8ba1\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u63d0\u9ad8\u4e86XLT\u5728\u5206\u8bcd\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ecf\u8fc7\u4f18\u5316\u7684WA\u53ef\u4ee5\u63d0\u4f9b\u4e0e\u57fa\u4e8e\u6807\u8bb0\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684XLT\u6027\u80fd\uff1b\u96c6\u6210translate-train\u548ctranslate-test\u9884\u6d4b\u7684\u65b0\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\uff0c\u4e3aXLT\u5728\u5206\u8bcd\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.10039", "pdf": "https://arxiv.org/pdf/2505.10039", "abs": "https://arxiv.org/abs/2505.10039", "authors": ["Hang Chen", "Jiaying Zhu", "Xinyu Yang", "Wenya Wang"], "title": "Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Circuit discovery has gradually become one of the prominent methods for\nmechanistic interpretability, and research on circuit completeness has also\ngarnered increasing attention. Methods of circuit discovery that do not\nguarantee completeness not only result in circuits that are not fixed across\ndifferent runs but also cause key mechanisms to be omitted. The nature of\nincompleteness arises from the presence of OR gates within the circuit, which\nare often only partially detected in standard circuit discovery methods. To\nthis end, we systematically introduce three types of logic gates: AND, OR, and\nADDER gates, and decompose the circuit into combinations of these logical\ngates. Through the concept of these gates, we derive the minimum requirements\nnecessary to achieve faithfulness and completeness. Furthermore, we propose a\nframework that combines noising-based and denoising-based interventions, which\ncan be easily integrated into existing circuit discovery methods without\nsignificantly increasing computational complexity. This framework is capable of\nfully identifying the logic gates and distinguishing them within the circuit.\nIn addition to the extensive experimental validation of the framework's ability\nto restore the faithfulness, completeness, and sparsity of circuits, using this\nframework, we uncover fundamental properties of the three logic gates, such as\ntheir proportions and contributions to the output, and explore how they behave\namong the functionalities of language models.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165AND\u3001OR\u548cADDER\u903b\u8f91\u95e8\u7684\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u566a\u58f0\u548c\u53bb\u566a\u5e72\u9884\u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u7684\u5b8c\u6574\u6027\u4e0e\u4fdd\u771f\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u8fd9\u4e9b\u903b\u8f91\u95e8\u5728\u8bed\u8a00\u6a21\u578b\u529f\u80fd\u4e2d\u7684\u57fa\u672c\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u5b8c\u6574\u6027\u548c\u4fdd\u771f\u6027\uff0c\u5bb9\u6613\u9057\u6f0f\u5173\u952e\u673a\u5236\uff0c\u7279\u522b\u662f\u5bf9OR\u95e8\u7684\u68c0\u6d4b\u4e0d\u5b8c\u5168\u3002", "method": "\u7cfb\u7edf\u5730\u5f15\u5165\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\uff08AND\u3001OR\u3001ADDER\uff09\uff0c\u5c06\u7535\u8def\u5206\u89e3\u4e3a\u8fd9\u4e9b\u903b\u8f91\u95e8\u7684\u7ec4\u5408\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u566a\u58f0\u548c\u53bb\u566a\u5e72\u9884\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u4e2d\uff0c\u7528\u4e8e\u5168\u9762\u8bc6\u522b\u548c\u533a\u5206\u7535\u8def\u4e2d\u7684\u903b\u8f91\u95e8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u6062\u590d\u7535\u8def\u7684\u4fdd\u771f\u6027\u3001\u5b8c\u6574\u6027\u548c\u7a00\u758f\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\u7684\u6bd4\u4f8b\u548c\u5bf9\u8f93\u51fa\u7684\u8d21\u732e\u7b49\u57fa\u672c\u5c5e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u9ad8\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u89e3\u91ca\u6027\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u529f\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2505.10258", "pdf": "https://arxiv.org/pdf/2505.10258", "abs": "https://arxiv.org/abs/2505.10258", "authors": ["Michael Hubbertz", "Pascal Colling", "Qi Han", "Tobias Meisen"], "title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper was accepted at the CVPR WAD 2025 Workshop", "summary": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.", "AI": {"tldr": "The paper presents a new offline mapping approach for autonomous driving that integrates driver trails into HD map creation using transformer-based deep learning models, showing better generalization and performance than online methods.", "motivation": "Current online mapping approaches for autonomous driving lack in temporal consistency, sensor occlusion handling, runtime efficiency, and generalization. There is a need for an improved method that can create accurate and updatable maps with less manual effort.", "method": "The proposed method aggregates trail data from the ego vehicle and other traffic participants to construct a comprehensive global map. It employs transformer-based deep learning models, enabling continuous updates and being sensor-agnostic.", "result": "The method outperforms state-of-the-art online mapping approaches in terms of generalization to unseen environments and sensor configurations. It has been validated on two benchmark datasets, proving its robustness and applicability.", "conclusion": "This novel offline mapping approach offers a promising solution for creating high-definition maps in autonomous driving systems, providing superior performance and adaptability compared to existing online mapping techniques."}}
{"id": "2505.10518", "pdf": "https://arxiv.org/pdf/2505.10518", "abs": "https://arxiv.org/abs/2505.10518", "authors": ["Anastasios Gerontopoulos", "Spyros Gidaris", "Nikos Komodakis"], "title": "Multi-Token Prediction Needs Registers", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.", "AI": {"tldr": "The paper introduces MuToR, a new method for multi-token prediction that integrates learnable register tokens into the input sequence to predict future targets. It has minimal additional parameters, requires no architectural changes, and works well with pre-trained language models. The method is effective for supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining across language and vision tasks.", "motivation": "Existing multi-token prediction methods have not consistently shown benefits when applied to fine-tuning scenarios. There is a need for an approach that can effectively leverage multi-token prediction while remaining compatible with pretrained models and suitable for various fine-tuning tasks.", "method": "MuToR interleaves learnable register tokens into the input sequence. Each register token predicts future targets in the sequence. This approach introduces only a small number of additional parameters, does not require any changes to the model architecture, and remains aligned with the next-token pretraining objective. It also supports scalable prediction horizons.", "result": "MuToR demonstrates effectiveness and versatility across multiple use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining. It performs well on challenging generative tasks in both language and vision domains.", "conclusion": "MuToR is a simple yet powerful approach for multi-token prediction that enhances performance in various fine-tuning and pretraining scenarios without requiring significant modifications to existing models."}}
{"id": "2505.10040", "pdf": "https://arxiv.org/pdf/2505.10040", "abs": "https://arxiv.org/abs/2505.10040", "authors": ["Lei Song", "Jiaxing Li", "Shihan Guan", "Youyong Kong"], "title": "Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their\ncapacity to preserve previously acquired knowledge amid the assimilation of\nnovel information. Rehearsal-based techniques revisit historical examples,\nadopted as a principal strategy to alleviate this phenomenon. However, memory\nexplosion and privacy infringements impose significant constraints on their\nutility. Non-Exemplar methods circumvent the prior issues through Prototype\nReplay (PR), yet feature drift presents new challenges. In this paper, our\nempirical findings reveal that Prototype Contrastive Learning (PCL) exhibits\nless pronounced drift than conventional PR. Drawing upon PCL, we propose\nInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar\nContinual Graph Learning (NECGL). Exploiting graph structural information, we\nformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature\ndistributions towards high-impact nodes to augment the model's capacity for\nassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)\nsafeguards task memory by regularizing discontinuities in class relationships.\nMoreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,\nfostering greater inter-class discriminability. Evaluations on four node\nclassification benchmark datasets demonstrate that our method outperforms\nexisting state-of-the-art methods, achieving a better trade-off between\nplasticity and stability.", "AI": {"tldr": "Graph Neural Networks (GNN) suffer from catastrophic forgetting. This paper proposes Instance-Prototype Affinity Learning (IPAL), a new method for Non-Exemplar Continual Graph Learning, which integrates graph structural information and decision boundary perception to improve knowledge retention and acquisition.", "motivation": "To address the issue of catastrophic forgetting in GNNs while overcoming limitations such as memory explosion and privacy concerns that arise with rehearsal-based techniques.", "method": "The proposed method, Instance-Prototype Affinity Learning (IPAL), builds on Prototype Contrastive Learning (PCL). It incorporates Topology-Integrated Gaussian Prototypes (TIGP) to leverage graph structure and enhance knowledge assimilation, Instance-Prototype Affinity Distillation (IPAD) to preserve task memory, and Decision Boundary Perception (DBP) to improve class discriminability.", "result": "Evaluations on four node classification benchmark datasets show that IPAL outperforms existing state-of-the-art methods, achieving a better balance between plasticity (learning new information) and stability (retaining old knowledge).", "conclusion": "IPAL is an effective approach for Non-Exemplar Continual Graph Learning that mitigates catastrophic forgetting in GNNs while addressing challenges like feature drift and maintaining high performance."}}
{"id": "2505.10267", "pdf": "https://arxiv.org/pdf/2505.10267", "abs": "https://arxiv.org/abs/2505.10267", "authors": ["Pavel Korotaev", "Petr Surovtsev", "Alexander Kapitanov", "Karina Kvanchiani", "Aleksandr Nagaev"], "title": "HandReader: Advanced Techniques for Efficient Fingerspelling Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/ai-forever/handreader", "summary": "Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.", "AI": {"tldr": "The paper presents HandReader, a set of three architectures for fingerspelling recognition in Sign Language, achieving state-of-the-art results on multiple datasets.", "motivation": "Fingerspelling is an important part of Sign Language and previous works have room for improvement in accuracy.", "method": "HandReader_RGB uses Temporal Shift-Adaptive Module (TSAM) to process RGB features. HandReader_KP uses Temporal Pose Encoder (TPE) on keypoints as tensors. HandReader_RGB+KP combines both modalities with a joint encoder.", "result": "All HandReader models achieve state-of-the-art results on ChicagoFSWild and ChicagoFSWild+ datasets, and high performance on the new Znaki dataset for Russian fingerspelling.", "conclusion": "The HandReader models are effective for fingerspelling recognition and the Znaki dataset and pre-trained models are publicly available."}}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527", "abs": "https://arxiv.org/abs/2505.10527", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "title": "WorldPM: Scaling Human Preference Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "AI": {"tldr": "The paper explores scaling laws in preference modeling akin to those in language modeling, proposing World Preference Modeling (WorldPM) which shows scalability potential and effectiveness in improving generalization performance across human preference datasets.", "motivation": "Scaling laws in language modeling inspired the exploration of similar patterns in preference modeling.", "method": "Preference data was collected from public forums, with extensive training on 15M-scale data using models ranging from 1.5B to 72B parameters. Three types of evaluation metrics were analyzed: adversarial, objective, and subjective.", "result": "Adversarial metrics improve with more training data and larger models; objective metrics show emergent behavior in larger models; subjective metrics do not scale. WorldPM improves generalization performance across datasets of varying sizes with gains exceeding 5% on key subtasks and 4%-8% in in-house evaluations.", "conclusion": "WorldPM is effective as a foundation for preference fine-tuning and significantly enhances performance when integrated into the RLHF pipeline."}}
{"id": "2505.10050", "pdf": "https://arxiv.org/pdf/2505.10050", "abs": "https://arxiv.org/abs/2505.10050", "authors": ["Fahad Almalki", "Mehedi Masud"], "title": "Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional machine learning models often prioritize predictive accuracy,\noften at the expense of model transparency and interpretability. The lack of\ntransparency makes it difficult for organizations to comply with regulatory\nrequirements and gain stakeholders trust. In this research, we propose a fraud\ndetection framework that combines a stacking ensemble of well-known gradient\nboosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable\nartificial intelligence (XAI) techniques are used to enhance the transparency\nand interpretability of the model's decisions. We used SHAP (SHapley Additive\nExplanations) for feature selection to identify the most important features.\nFurther efforts were made to explain the model's predictions using Local\nInterpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots\n(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection\ndataset, which includes more than 590,000 real transaction records, was used to\nevaluate the proposed model. The model achieved a high performance with an\naccuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent\nrelated approaches. These results indicate that combining high prediction\naccuracy with transparent interpretability is possible and could lead to a more\nethical and trustworthy solution in financial fraud detection.", "AI": {"tldr": "The paper proposes a fraud detection framework using a stacking ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and XAI techniques (SHAP, LIME, PDP, PFI) to improve both accuracy and interpretability. Evaluated on the IEEE-CIS dataset, it achieved 99% accuracy and 0.99 AUC-ROC score.", "motivation": "Traditional machine learning models focus on predictive accuracy but lack transparency and interpretability, which is problematic for regulatory compliance and trust in sectors like financial fraud detection.", "method": "A stacking ensemble of XGBoost, LightGBM, and CatBoost is used for fraud detection. SHAP is employed for feature selection, while LIME, PDP, and PFI are used to explain model predictions. The IEEE-CIS Fraud Detection dataset with over 590,000 transaction records is utilized for evaluation.", "result": "The model achieves 99% accuracy and an AUC-ROC score of 0.99, surpassing several recent related approaches.", "conclusion": "It's possible to combine high prediction accuracy with transparent interpretability, leading to more ethical and trustworthy solutions in financial fraud detection."}}
{"id": "2505.10281", "pdf": "https://arxiv.org/pdf/2505.10281", "abs": "https://arxiv.org/abs/2505.10281", "authors": ["Mengqiu Xu", "Kaixin Chen", "Heng Guo", "Yixiang Huang", "Ming Wu", "Zhenwei Shi", "Chuang Zhang", "Jun Guo"], "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.", "AI": {"tldr": "The paper introduces MFogHub, a multi-regional and multi-satellite dataset for marine fog detection and forecasting that includes over 68,000 high-resolution samples. It addresses limitations of existing datasets by enabling evaluation across diverse conditions and supporting development of scalable fog prediction techniques.", "motivation": "Deep learning approaches have shown promise in marine fog detection and forecasting, but the lack of open-source datasets with diverse regional and satellite data limits model evaluation and exploration of marine fog characteristics.", "method": "MFogHub integrates annotated marine fog observations from 15 coastal fog-prone regions and six geostationary satellites, providing over 68,000 high-resolution samples to facilitate rigorous evaluation of detection and forecasting methods under varying conditions.", "result": "Experiments with 16 baseline models show that MFogHub reveals generalization fluctuations due to regional and satellite discrepancies and serves as a valuable resource for developing targeted and scalable fog prediction techniques.", "conclusion": "MFogHub aims to advance practical monitoring and scientific understanding of marine fog dynamics globally."}}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554", "abs": "https://arxiv.org/abs/2505.10554", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment", "AI": {"tldr": "Large reasoning models (LRMs) have a hidden ability for long-term reasoning. Although reinforcement learning (RL) can sometimes bring out advanced reasoning behaviors, these are unpredictable and unreliable. This paper proposes aligning models with three meta-abilities (deduction, induction, and abduction) through automatically generated tasks to improve performance by over 10% compared to instruction-tuned baselines.", "motivation": "LRMs already have the potential for long-term reasoning, but the advanced reasoning behaviors elicited by RL are not consistent or controllable, limiting scalability and reliability.", "method": "The method involves a three-stage pipeline: individual alignment, parameter-space merging, and domain-specific reinforcement learning, using automatically generated self-verifiable tasks aligned with deduction, induction, and abduction.", "result": "This approach boosts performance by more than 10% relative to instruction-tuned baselines. Domain-specific RL from the aligned checkpoint yields an additional average gain of 2% in performance across math, coding, and science benchmarks.", "conclusion": "Explicit alignment with meta-abilities provides a scalable and reliable foundation for reasoning in LRMs."}}
{"id": "2505.10057", "pdf": "https://arxiv.org/pdf/2505.10057", "abs": "https://arxiv.org/abs/2505.10057", "authors": ["Tiancong Cheng", "Ying Zhang", "Yuxuan Liang", "Roger Zimmermann", "Zhiwen Yu", "Bin Guo"], "title": "JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation", "categories": ["cs.LG"], "comment": null, "summary": "Depth estimation and scene segmentation are two important tasks in\nintelligent transportation systems. A joint modeling of these two tasks will\nreduce the requirement for both the storage and training efforts. This work\nexplores how the multi-task distillation could be used to improve such unified\nmodeling. While existing solutions transfer multiple teachers' knowledge in a\nstatic way, we propose a self-adaptive distillation method that can dynamically\nadjust the knowledge amount from each teacher according to the student's\ncurrent learning ability. Furthermore, as multiple teachers exist, the\nstudent's gradient update direction in the distillation is more prone to be\nerroneous where knowledge forgetting may occur. To avoid this, we propose a\nknowledge trajectory to record the most essential information that a model has\nlearnt in the past, based on which a trajectory-based distillation loss is\ndesigned to guide the student to follow the learning curve similarly in a\ncost-effective way. We evaluate our method on multiple benchmarking datasets\nincluding Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,\nour method achieves a clearly improvement. The code is provided in the\nsupplementary materials.", "AI": {"tldr": "This paper proposes a self-adaptive distillation method and a trajectory-based distillation loss for multi-task learning in depth estimation and scene segmentation, achieving better performance on benchmark datasets.", "motivation": "Existing solutions for joint modeling of depth estimation and scene segmentation transfer knowledge from multiple teachers in a static way, which may not be optimal for the student's current learning ability and could lead to knowledge forgetting.", "method": "The proposed method includes a self-adaptive distillation approach that dynamically adjusts knowledge transfer based on the student's learning ability and a knowledge trajectory with a trajectory-based distillation loss to prevent erroneous gradient updates and knowledge forgetting.", "result": "The method shows clear improvements over state-of-the-art solutions when evaluated on benchmark datasets like Cityscapes and NYU-v2.", "conclusion": "The self-adaptive distillation method and trajectory-based distillation loss effectively improve multi-task learning in depth estimation and scene segmentation."}}
{"id": "2505.10289", "pdf": "https://arxiv.org/pdf/2505.10289", "abs": "https://arxiv.org/abs/2505.10289", "authors": ["Yue Wang", "Shuai Xu", "Xuelin Zhu", "Yicong Li"], "title": "MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.", "AI": {"tldr": "The paper proposes a Multi-Stage Cross-modal Interaction (MSCI) model to enhance Compositional Zero-Shot Learning (CZSL) by effectively utilizing intermediate-layer information from CLIP's visual encoder, designing self-adaptive aggregators for local and global information extraction, and dynamically adjusting attention weights. Experiments on three datasets validate its effectiveness.", "motivation": "Existing CZSL studies rely on CLIP but overlook its limitations in capturing fine-grained local features due to architectural and training paradigm constraints.", "method": "Propose MSCI model with two self-adaptive aggregators to extract and integrate local and global visual features, progressively incorporating them into textual representations through stage-by-stage interaction mechanism, and dynamically adjusting attention weights based on different combinations.", "result": "Experiments on three widely used datasets fully validate the effectiveness and superiority of the proposed model.", "conclusion": "MSCI model significantly enhances perception capability for fine-grained local visual information and flexibly adapts to diverse scenarios."}}
{"id": "2505.09665", "pdf": "https://arxiv.org/pdf/2505.09665", "abs": "https://arxiv.org/abs/2505.09665", "authors": ["Sulong Zhou", "Qunying Huang", "Shaoheng Zhou", "Yun Hang", "Xinyue Ye", "Aodong Mei", "Kathryn Phung", "Yuning Ye", "Uma Govindswamy", "Zehan Li"], "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent\nyears. Understanding how affected populations perceive and respond during\nwildfire crises is critical for timely and empathetic disaster response. Social\nmedia platforms offer a crowd-sourced channel to capture evolving public\ndiscourse, providing hyperlocal information and insight into public sentiment.\nThis study analyzes Reddit discourse during the 2025 Los Angeles wildfires,\nspanning from the onset of the disaster to full containment. We collect 385\nposts and 114,879 comments related to the Palisades and Eaton fires. We adopt\ntopic modeling methods to identify the latent topics, enhanced by large\nlanguage models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we\ndevelop a hierarchical framework to categorize latent topics, consisting of two\nmain categories, Situational Awareness (SA) and Crisis Narratives (CN). The\nvolume of SA category closely aligns with real-world fire progressions, peaking\nwithin the first 2-5 days as the fires reach the maximum extent. The most\nfrequent co-occurring category set of public health and safety, loss and\ndamage, and emergency resources expands on a wide range of health-related\nlatent topics, including environmental health, occupational health, and one\nhealth. Grief signals and mental health risks consistently accounted for 60\npercentage and 40 percentage of CN instances, respectively, with the highest\ntotal volume occurring at night. This study contributes the first annotated\nsocial media dataset on the 2025 LA fires, and introduces a scalable\nmulti-layer framework that leverages topic modeling for crisis discourse\nanalysis. By identifying persistent public health concerns, our results can\ninform more empathetic and adaptive strategies for disaster response, public\nhealth communication, and future research in comparable climate-related\ndisaster events.", "AI": {"tldr": "This study analyzes Reddit discourse during the 2025 Los Angeles wildfires using topic modeling methods enhanced by LLMs and HITL refinement, identifying two main categories of topics (Situational Awareness and Crisis Narratives) and contributing an annotated social media dataset.", "motivation": "To understand how affected populations perceive and respond during wildfire crises for timely and empathetic disaster response.", "method": "Adopting topic modeling methods enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement to analyze Reddit posts and comments related to the Palisades and Eaton fires.", "result": "Identified two main categories of latent topics: Situational Awareness (SA) and Crisis Narratives (CN). SA closely aligns with real-world fire progressions and peaks within the first 2-5 days. CN includes grief signals and mental health risks, with the highest volume occurring at night.", "conclusion": "Contributed the first annotated social media dataset on the 2025 LA fires and introduced a scalable multi-layer framework for crisis discourse analysis, which can inform more empathetic and adaptive strategies for disaster response."}}
{"id": "2505.09747", "pdf": "https://arxiv.org/pdf/2505.09747", "abs": "https://arxiv.org/abs/2505.09747", "authors": ["Benjamin Paa\u00dfen", "Suzana Alpsancar", "Tobias Matzner", "Ingrid Scharlau"], "title": "Healthy Distrust in AI systems", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Under the slogan of trustworthy AI, much of contemporary AI research is\nfocused on designing AI systems and usage practices that inspire human trust\nand, thus, enhance adoption of AI systems. However, a person affected by an AI\nsystem may not be convinced by AI system design alone -- neither should they,\nif the AI system is embedded in a social context that gives good reason to\nbelieve that it is used in tension with a person's interest. In such cases,\ndistrust in the system may be justified and necessary to build meaningful trust\nin the first place. We propose the term \"healthy distrust\" to describe such a\njustified, careful stance towards certain AI usage practices. We investigate\nprior notions of trust and distrust in computer science, sociology, history,\npsychology, and philosophy, outline a remaining gap that healthy distrust might\nfill and conceptualize healthy distrust as a crucial part for AI usage that\nrespects human autonomy.", "AI": {"tldr": "This paper introduces the concept of 'healthy distrust' in AI systems, emphasizing its necessity for building meaningful trust and respecting human autonomy. It investigates prior notions of trust and distrust across multiple disciplines.", "motivation": "The motivation is to address the limitations of current AI system designs which focus solely on inspiring trust, without considering the social context that may create tension with personal interests.", "method": "The method involves examining previous concepts of trust and distrust from various fields such as computer science, sociology, history, psychology, and philosophy to identify gaps and define 'healthy distrust'.", "result": "The result is the conceptualization of 'healthy distrust' as an essential element for AI usage that respects human autonomy, suggesting that distrust can be justified and necessary.", "conclusion": "The conclusion is that 'healthy distrust' should be recognized and incorporated into AI design and practice to ensure respect for human autonomy and build meaningful trust."}}
{"id": "2505.10083", "pdf": "https://arxiv.org/pdf/2505.10083", "abs": "https://arxiv.org/abs/2505.10083", "authors": ["Chengsen Wang", "Qi Qi", "Zhongwen Rao", "Lujia Pan", "Jingyu Wang", "Jianxin Liao"], "title": "ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChronoSteer\u7684\u591a\u6a21\u6001TSFM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u6587\u672c\u4fee\u8ba2\u6307\u4ee4\u8fdb\u884c\u5f15\u5bfc\uff0c\u6709\u6548\u8fde\u63a5LLM\u548cTSFM\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684ChronoSteer\u4e0e\u5355\u6a21\u6001\u4e3b\u5e72\u76f8\u6bd4\uff0c\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u4e8625.7%\uff0c\u6bd4\u4e4b\u524d\u7684\u591a\u6a21\u6001\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa22.5%\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u6a21\u6001\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u9650\u5236\u4e86\u5bf9\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u7684\u5229\u7528\u3002\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u5206\u522b\u5728\u6587\u672c\u63a8\u7406\u548c\u65f6\u95f4\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u5c06\u4e24\u8005\u7684\u4f18\u52bf\u7ed3\u5408\u8d77\u6765\uff0c\u6784\u5efa\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u578b\u4ee5\u540c\u65f6\u5229\u7528\u65f6\u95f4\u548c\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u672a\u6765\u63a8\u65ad\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u6311\u6218\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u4e8b\u4ef6-\u5e8f\u5217\u914d\u5bf9\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u8026\u6846\u67b6\uff1a\u4f7f\u7528LLM\u5c06\u6587\u672c\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u4fee\u8ba2\u6307\u4ee4\uff0c\u8fd9\u4e9b\u6307\u4ee4\u518d\u7528\u4e8e\u5f15\u5bfcTSFM\u7684\u8f93\u51fa\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f15\u5165\u4e86ChronoSteer\uff0c\u4e00\u79cd\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u4fee\u8ba2\u6307\u4ee4\u8fdb\u884c\u5f15\u5bfc\u7684\u591a\u6a21\u6001TSFM\uff0c\u4ece\u800c\u6709\u6548\u5730\u8fde\u63a5\u4e86LLM\u548cTSFM\u3002\u4e3a\u4e86\u7f13\u89e3\u8de8\u6a21\u6001\u6307\u4ee4-\u5e8f\u5217\u914d\u5bf9\u6570\u636e\u7684\u77ed\u7f3a\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\uff0c\u4ee5\u89e3\u51b3\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\u3002", "result": "\u7ecf\u8fc7\u6574\u5408LLM\u540e\uff0c\u4ec5\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684ChronoSteer\u76f8\u8f83\u4e8e\u5355\u6a21\u6001\u4e3b\u5e72\u63d0\u5347\u4e8625.7%\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u9ad8\u51fa22.5%\u3002", "conclusion": "ChronoSteer\u4f5c\u4e3a\u4e00\u79cd\u591a\u6a21\u6001TSFM\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548cTSFM\u7684\u4f18\u70b9\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10292", "pdf": "https://arxiv.org/pdf/2505.10292", "abs": "https://arxiv.org/abs/2505.10292", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation", "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "31 pages, 14 figures", "summary": "Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.", "AI": {"tldr": "The paper introduces StoryReasoning, a dataset with grounded stories and structured scene analyses to maintain character and object consistency across frames in visual storytelling. By fine-tuning Qwen2.5-VL 7B into Qwen Storyteller, they achieve a 12.3% reduction in hallucinations per story.", "motivation": "Visual storytelling systems often fail to maintain character identity and link actions appropriately due to referential hallucinations. This motivates the need for grounding characters, objects, and entities on visual elements.", "method": "Proposed StoryReasoning dataset includes 4,178 stories from 52,016 movie images with structured scene analyses and grounded stories. The approach uses cross-frame object re-identification, chain-of-thought reasoning, and a grounding scheme linking textual elements to visual entities.", "result": "Fine-tuned Qwen2.5-VL 7B (Qwen Storyteller) shows a reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story compared to non-fine-tuned model.", "conclusion": "StoryReasoning dataset and Qwen Storyteller model provide effective solutions for reducing referential hallucinations in visual storytelling."}}
{"id": "2505.09777", "pdf": "https://arxiv.org/pdf/2505.09777", "abs": "https://arxiv.org/abs/2505.09777", "authors": ["Alejo Lopez-Avila", "Jinhua Du"], "title": "A Survey on Large Language Models in Multimodal Recommender Systems", "categories": ["cs.IR", "cs.CL"], "comment": "30 pages, 6 figures", "summary": "Multimodal recommender systems (MRS) integrate heterogeneous user and item\ndata, such as text, images, and structured information, to enhance\nrecommendation performance. The emergence of large language models (LLMs)\nintroduces new opportunities for MRS by enabling semantic reasoning, in-context\nlearning, and dynamic input handling. Compared to earlier pre-trained language\nmodels (PLMs), LLMs offer greater flexibility and generalisation capabilities\nbut also introduce challenges related to scalability and model accessibility.\nThis survey presents a comprehensive review of recent work at the intersection\nof LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and\ndata adaptation techniques. We propose a novel taxonomy to characterise\nintegration patterns, identify transferable techniques from related\nrecommendation domains, provide an overview of evaluation metrics and datasets,\nand point to possible future directions. We aim to clarify the emerging role of\nLLMs in multimodal recommendation and support future research in this rapidly\nevolving field.", "AI": {"tldr": "Multimodal recommender systems (MRS) can be enhanced by large language models (LLMs), which offer semantic reasoning, in-context learning, and dynamic input handling. This survey reviews the integration of LLMs into MRS, focusing on prompting strategies, fine-tuning methods, and data adaptation techniques.", "motivation": "To explore how large language models (LLMs) can enhance multimodal recommender systems (MRS) compared to pre-trained language models (PLMs), and address challenges related to scalability and model accessibility.", "method": "The survey proposes a taxonomy for integration patterns of LLMs in MRS, identifies transferable techniques from related domains, provides an overview of evaluation metrics and datasets, and suggests future research directions.", "result": "Provides insights into prompting strategies, fine-tuning methods, and data adaptation techniques when integrating LLMs into MRS, supporting future research advancements.", "conclusion": "LLMs introduce significant opportunities for enhancing MRS performance but also present challenges that need addressing. The proposed taxonomy and identified techniques aim to guide future research."}}
{"id": "2505.09757", "pdf": "https://arxiv.org/pdf/2505.09757", "abs": "https://arxiv.org/abs/2505.09757", "authors": ["Botao Amber Hu", "Yuhan Liu", "Helena Rong"], "title": "Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "Submitted to CSCW 2026", "summary": "The recent trend of self-sovereign Decentralized AI Agents (DeAgents)\ncombines Large Language Model (LLM)-based AI agents with decentralization\ntechnologies such as blockchain smart contracts and trusted execution\nenvironments (TEEs). These tamper-resistant trustless substrates allow agents\nto achieve self-sovereignty through ownership of cryptowallet private keys and\ncontrol of digital assets and social media accounts. DeAgent eliminates\ncentralized control and reduces human intervention, addressing key trust\nconcerns inherent in centralized AI systems. However, given ongoing challenges\nin LLM reliability such as hallucinations, this creates paradoxical tension\nbetween trustlessness and unreliable autonomy. This study addresses this\nempirical research gap through interviews with DeAgents stakeholders-experts,\nfounders, and developers-to examine their motivations, benefits, and governance\ndilemmas. The findings will guide future DeAgents system and protocol design\nand inform discussions about governance in sociotechnical AI systems in the\nfuture agentic web.", "AI": {"tldr": "The study explores the challenges and governance issues in Decentralized AI Agents (DeAgents) that combine AI with blockchain technologies, interviewing stakeholders to understand motivations, benefits, and dilemmas for guiding future design and discussions.", "motivation": "To address the empirical research gap concerning the paradoxical tension between trustlessness and unreliable autonomy in DeAgents, which combines LLM-based AI agents with decentralization technologies.", "method": "Through interviews with DeAgents stakeholders including experts, founders, and developers to examine their motivations, perceived benefits, and governance challenges.", "result": "The findings will provide insights to guide the future design of DeAgents systems and protocols, as well as inform discussions on governance within sociotechnical AI systems.", "conclusion": "This research aims to navigate the complex landscape of decentralized AI agents by offering guidance for system design and fostering discussions on governance mechanisms."}}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117", "abs": "https://arxiv.org/abs/2505.10117", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfei Wang", "Jun Luo"], "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.", "AI": {"tldr": "MiCo is a hierarchical language agent framework that solves ODMBP by formulating it as SMDP-Option, achieving 96.9% competitive ratio in large-scale scenarios with over 10,000 virtual machines.", "motivation": "Traditional methods for VM scheduling in cloud services have limitations such as inability to adapt to real-time changes, rigid strategies, and lack of generalizability and interpretability.", "method": "MiCo uses LLMs in a two-stage architecture - Option Miner discovers non-context-aware strategies, and Option Composer integrates these with contextual ones.", "result": "MiCo achieves 96.9% competitive ratio in large-scale scenarios with over 10,000 virtual machines and maintains high performance under nonstationary request flows and diverse configurations.", "conclusion": "MiCo provides an effective solution for complex and large-scale cloud environments."}}
{"id": "2505.10294", "pdf": "https://arxiv.org/pdf/2505.10294", "abs": "https://arxiv.org/abs/2505.10294", "authors": ["Guillaume Balezo", "Roger Trullo", "Albert Pla Planas", "Etienne Decenciere", "Thomas Walter"], "title": "MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models", "categories": ["cs.CV", "q-bio.TO", "68T07 (Primary), 92C55 (Secondary)", "I.4.9; I.2.10; I.5.4; J.3"], "comment": null, "summary": "Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.", "AI": {"tldr": "This paper introduces MIPHEI, a model that predicts multiplex immunofluorescence signals from H&E images using U-Net and ViT architecture. It is trained on the ORION dataset and validated on two independent datasets, achieving high F1 scores for cell-type classification.", "motivation": "Histopathological analysis with H&E staining is standard in cancer diagnosis, but multiplex immunofluorescence (mIF) provides more precise cell type identification which is not widely adopted due to cost and constraints. The motivation is to bridge this gap by predicting mIF signals from H&E images.", "method": "The method involves developing MIPHEI, a U-Net-inspired architecture that uses ViT foundation models as encoders to predict mIF signals from H&E images. It targets various markers and is trained on the ORION dataset, then validated on two other datasets.", "result": "MIPHEI achieves accurate cell-type classification from H&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, outperforming state-of-the-art baselines and random classifiers for most markers.", "conclusion": "MIPHEI effectively captures relationships between nuclear morphologies and molecular markers, offering a promising approach for cell-type-aware analysis of large-scale H&E datasets to uncover relationships between spatial cellular organization and patient outcomes."}}
{"id": "2505.09766", "pdf": "https://arxiv.org/pdf/2505.09766", "abs": "https://arxiv.org/abs/2505.09766", "authors": ["Roberto Ponciroli"], "title": "On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion", "categories": ["math.NA", "cs.AI", "cs.NA"], "comment": null, "summary": "This work presents a methodology for reconstructing the spatial distribution\nof the neutron flux in a nuclear reactor, leveraging real-time measurements\nobtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation\ninherently defines the problem of estimating a scalar field within a domain\nbased on boundary data, making it a natural mathematical framework for this\ntask. The main challenge lies in deriving the Green's function specific to the\ndomain and the neutron diffusion process. While analytical solutions for\nGreen's functions exist for simplified geometries, their derivation of complex,\nheterogeneous domains-such as a nuclear reactor-requires a numerical approach.\nThe objective of this work is to demonstrate the well-posedness of the\ndata-driven Green's function approximation by formulating and solving the K-H\nequation as an inverse problem. After establishing the symmetry properties that\nthe Green's function must satisfy, the K-H equation is derived from the\none-speed neutron diffusion model. This is followed by a comprehensive\ndescription of the procedure for interpreting sensor readings and implementing\nthe neutron flux reconstruction algorithm. Finally, the existence and\nuniqueness of the Green's function inferred from the sampled data are\ndemonstrated, ensuring the reliability of the proposed method and its\npredictions.", "AI": {"tldr": "This paper presents a methodology for reconstructing the spatial distribution of neutron flux in a nuclear reactor using real-time ex-core detector measurements and solving the Kirchhoff-Helmholtz equation as an inverse problem to approximate the Green's function.", "motivation": "To develop a reliable method for reconstructing the spatial distribution of neutron flux within a nuclear reactor, leveraging real-time boundary data from ex-core detectors.", "method": "Utilize the Kirchhoff-Helmholtz equation to define the problem, derive the Green's function from the one-speed neutron diffusion model, establish symmetry properties, and demonstrate the procedure for sensor data interpretation and flux reconstruction.", "result": "The existence and uniqueness of the Green's function inferred from sampled data are demonstrated, ensuring the reliability of the proposed neutron flux reconstruction method.", "conclusion": "The data-driven approximation of the Green's function is well-posed, providing a reliable framework for neutron flux reconstruction in nuclear reactors."}}
{"id": "2505.10120", "pdf": "https://arxiv.org/pdf/2505.10120", "abs": "https://arxiv.org/abs/2505.10120", "authors": ["Guillaume Godin"], "title": "All You Need Is Synthetic Task Augmentation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 3 Figures, 6 tables", "summary": "Injecting rule-based models like Random Forests into differentiable neural\nnetwork frameworks remains an open challenge in machine learning. Recent\nadvancements have demonstrated that pretrained models can generate efficient\nmolecular embeddings. However, these approaches often require extensive\npretraining and additional techniques, such as incorporating posterior\nprobabilities, to boost performance. In our study, we propose a novel strategy\nthat jointly trains a single Graph Transformer neural network on both sparse\nmultitask molecular property experimental targets and synthetic targets derived\nfrom XGBoost models trained on Osmordred molecular descriptors. These synthetic\ntasks serve as independent auxiliary tasks. Our results show consistent and\nsignificant performance improvement across all 19 molecular property prediction\ntasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms\nthe XGBoost single-task learner. This demonstrates that synthetic task\naugmentation is an effective method for enhancing neural model performance in\nmultitask molecular property prediction without the need for feature injection\nor pretraining.", "AI": {"tldr": "Researchers have found a way to improve the performance of neural networks in predicting molecular properties without needing extensive pretraining or extra techniques. They did this by creating a new method that trains a Graph Transformer on both real and synthetic tasks, leading to better results in most cases.", "motivation": "The motivation behind this paper is the challenge of integrating rule-based models into differentiable neural network frameworks effectively. Current methods often rely heavily on pretraining and supplementary techniques to achieve good performance.", "method": "The researchers proposed a strategy that jointly trains a single Graph Transformer neural network. This network is trained on both sparse multitask molecular property experimental targets and synthetic targets derived from XGBoost models trained on Osmordred molecular descriptors. These synthetic tasks act as auxiliary tasks.", "result": "This approach led to consistent and significant improvements in performance across all 19 molecular property prediction tasks. Specifically, for 16 out of the 19 targets, the multitask Graph Transformer performed better than the XGBoost single-task learner.", "conclusion": "The study concludes that synthetic task augmentation is an effective method to enhance the performance of neural models in multitask molecular property prediction without requiring feature injection or pretraining."}}
{"id": "2505.10351", "pdf": "https://arxiv.org/pdf/2505.10351", "abs": "https://arxiv.org/abs/2505.10351", "authors": ["Jie Zhu", "Jirong Zha", "Ding Li", "Leye Wang"], "title": "A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability", "categories": ["cs.CV"], "comment": "An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders", "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPartCrop\u7684\u7edf\u4e00\u6210\u5458\u63a8\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u66f4\u771f\u5b9e\u7684\u73af\u5883\u4e0b\u5bf9\u89c6\u89c9\u81ea\u76d1\u7763\u6a21\u578b\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u4e24\u79cd\u5e38\u89c1\u7684\u9632\u5fa1\u65b9\u6cd5\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684PartCrop-v2\u3002", "motivation": "\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u5728\u5229\u7528\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u89c6\u89c9\u9886\u57df\u4e5f\u9762\u4e34\u7740\u663e\u8457\u7684\u9690\u79c1\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u5728\u5bf9\u624b\u901a\u5e38\u9762\u5bf9\u9ed1\u76d2\u7cfb\u7edf\u7684\u5b9e\u9645\u60c5\u51b5\u4e0b\uff0c\u7814\u7a76\u5982\u4f55\u5728\u672a\u77e5\u81ea\u6211\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\u548c\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPartCrop\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u88c1\u526a\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u90e8\u5206\u6765\u67e5\u8be2\u8868\u793a\u7a7a\u95f4\u5185\u7684\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u4e24\u79cd\u5e38\u89c1\u7684\u9632\u5fa1\u65b9\u6cd5\uff1a\u63d0\u524d\u505c\u6b62\u548c\u5dee\u5206\u9690\u79c1\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\uff1a\u7f29\u5c0f\u88c1\u526a\u6bd4\u4f8b\u8303\u56f4\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684PartCrop-v2\uff0c\u5f15\u5165\u4e86\u4e24\u4e2a\u7ed3\u6784\u4e0a\u7684\u6539\u8fdb\u3002", "result": "\u5e7f\u6cdb\u7684\u653b\u51fb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PartCrop\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002\u9632\u5fa1\u5b9e\u9a8c\u8868\u660e\u6240\u6709\u65b9\u6cd5\u90fd\u662f\u6709\u6548\u7684\u3002\u5b9a\u91cf\u7814\u7a76\u8868\u660e\uff0c\u4ece\u6570\u636e\u548c\u6a21\u578b\u65b9\u9762\u8fdb\u884c\u6269\u5c55\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684PartCrop-v2\u3002", "conclusion": "PartCrop\u662f\u4e00\u79cd\u6709\u6548\u7684\u6210\u5458\u63a8\u65ad\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002\u9632\u5fa1\u65b9\u6cd5\u5982\u63d0\u524d\u505c\u6b62\u3001\u5dee\u5206\u9690\u79c1\u548c\u7f29\u5c0f\u88c1\u526a\u6bd4\u4f8b\u8303\u56f4\u5747\u80fd\u6709\u6548\u62b5\u5fa1\u653b\u51fb\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684PartCrop-v2\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.10125", "pdf": "https://arxiv.org/pdf/2505.10125", "abs": "https://arxiv.org/abs/2505.10125", "authors": ["Wujun Zhou", "Shu Ding", "ZeLin Li", "Wei Wang"], "title": "Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning enables the clients to collaboratively train a global\nmodel, which is aggregated from local models. Due to the heterogeneous data\ndistributions over clients and data privacy in federated learning, it is\ndifficult to train local models to achieve a well-performed global model. In\nthis paper, we introduce the adaptability of local models, i.e., the average\nperformance of local models on data distributions over clients, and enhance the\nperformance of the global model by improving the adaptability of local models.\nSince each client does not know the data distributions over other clients, the\nadaptability of the local model cannot be directly optimized. First, we provide\nthe property of an appropriate local model which has good adaptability on the\ndata distributions over clients. Then, we formalize the property into the local\ntraining objective with a constraint and propose a feasible solution to train\nthe local model. Extensive experiments on federated learning benchmarks\ndemonstrate that our method significantly improves the adaptability of local\nmodels and achieves a well-performed global model that consistently outperforms\nthe baseline methods.", "AI": {"tldr": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u7531\u4e8e\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u7684\u5f02\u6784\u6027\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u7684\u5168\u5c40\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u901a\u8fc7\u63d0\u9ad8\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\u6765\u589e\u5f3a\u5168\u5c40\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u4f18\u5316\u672c\u5730\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u5168\u5c40\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u5f02\u6784\u6027\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\u5bfc\u81f4\u96be\u4ee5\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u7684\u5168\u5c40\u6a21\u578b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002", "method": "1. \u63d0\u51fa\u9002\u5f53\u672c\u5730\u6a21\u578b\u7684\u6027\u8d28\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5ba2\u6237\u7aef\u7684\u6570\u636e\u5206\u5e03\u4e0a\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u3002\n2. \u5c06\u8be5\u6027\u8d28\u5f62\u5f0f\u5316\u4e3a\u5e26\u6709\u7ea6\u675f\u7684\u672c\u5730\u8bad\u7ec3\u76ee\u6807\u3002\n3. \u63d0\u51fa\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u8bad\u7ec3\u672c\u5730\u6a21\u578b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6027\u80fd\u4f18\u8d8a\u7684\u5168\u5c40\u6a21\u578b\uff0c\u5176\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u4e2d\u5168\u5c40\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10352", "pdf": "https://arxiv.org/pdf/2505.10352", "abs": "https://arxiv.org/abs/2505.10352", "authors": ["Shihao Zou", "Qingfeng Li", "Wei Ji", "Jingjing Li", "Yongkui Yang", "Guoqi Li", "Chao Dong"], "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\\mathcal{O}(T)$ Complexity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer", "AI": {"tldr": "The paper introduces SpikeVideoFormer, an efficient spike-driven video Transformer with linear temporal complexity. It uses spike-driven Hamming attention and performs well on video tasks like classification, human pose tracking, and semantic segmentation.", "motivation": "Existing SNN-based Transformers mainly focus on single-image tasks and do not effectively utilize SNNs' efficiency in video-based vision tasks.", "method": "The authors design a spike-driven Hamming attention (SDHA) for adapting traditional real-valued attention to spike-driven attention. They also analyze various spike-driven space-time attention designs and identify an optimal scheme with linear temporal complexity.", "result": "SpikeVideoFormer achieves state-of-the-art performance compared to existing SNN approaches, with over 15% improvement on human pose tracking and semantic segmentation tasks. It matches the performance of recent ANN-based methods while providing significant efficiency gains (x16, x10, and x5 improvements on three tasks).", "conclusion": "SpikeVideoFormer is an efficient spike-driven video Transformer that demonstrates strong generalization ability and efficiency across diverse downstream video tasks."}}
{"id": "2505.09921", "pdf": "https://arxiv.org/pdf/2505.09921", "abs": "https://arxiv.org/abs/2505.09921", "authors": ["Yidan Wang", "Yanan Cao", "Yubing Ren", "Fang Fang", "Zheng Lin", "Binxing Fang"], "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.", "AI": {"tldr": "This paper explores the use of jailbreak attacks for extracting sensitive information from Large Language Models (LLMs) and introduces PIG, a novel framework targeting Personally Identifiable Information (PII). Experiments on multiple LLMs demonstrate PIG's superior performance compared to existing methods, highlighting significant privacy risks in LLMs.", "motivation": "To evaluate the effectiveness of jailbreak attacks in extracting sensitive information from LLMs and address the limitations of current methods for evaluating privacy leakage.", "method": "The proposed framework, PIG, identifies PII entities and their types in privacy queries, builds a privacy context using in-context learning, and iteratively updates it with three gradient-based strategies to elicit target PII.", "result": "PIG outperforms baseline methods when evaluated on four white-box and two black-box LLMs using two privacy-related datasets, achieving state-of-the-art results.", "conclusion": "The findings reveal significant privacy risks in LLMs and stress the importance of enhancing safeguards."}}
{"id": "2505.09796", "pdf": "https://arxiv.org/pdf/2505.09796", "abs": "https://arxiv.org/abs/2505.09796", "authors": ["Skylar S. Gay", "Tucker Netherton", "Barbara Marquez", "Raymond Mumme", "Mary Gronberg", "Brent Parker", "Chelsea Pinnix", "Sanjay Shete", "Carlos Cardenas", "Laurence Court"], "title": "Virtual Dosimetrists: A Radiotherapy Training \"Flight Simulator\"", "categories": ["physics.med-ph", "cs.AI"], "comment": null, "summary": "Effective education in radiotherapy plan quality review requires a robust,\nregularly updated set of examples and the flexibility to demonstrate multiple\npossible planning approaches and their consequences. However, the current\nclinic-based paradigm does not support these needs. To address this, we have\ndeveloped 'Virtual Dosimetrist' models that can both generate training examples\nof suboptimal treatment plans and then allow trainees to improve the plan\nquality through simple natural language prompts, as if communicating with a\ndosimetrist. The dose generation and modification process is accurate, rapid,\nand requires only modest resources. This work is the first to combine dose\ndistribution prediction with natural language processing; providing a robust\npipeline for both generating suboptimal training plans and allowing trainees to\npractice their critical plan review and improvement skills that addresses the\nchallenges of the current clinic-based paradigm.", "AI": {"tldr": "This paper introduces 'Virtual Dosimetrist' models that generate suboptimal radiotherapy plans and allow trainees to improve them via natural language prompts, addressing limitations in the current clinic-based training paradigm.", "motivation": "There is a need for effective education in radiotherapy plan quality review which requires robust, regularly updated examples and flexibility to demonstrate multiple planning approaches and their consequences. The current clinic-based paradigm does not support these needs.", "method": "The researchers developed 'Virtual Dosimetrist' models that can generate training examples of suboptimal treatment plans and enable trainees to improve the plan quality through simple natural language prompts.", "result": "The dose generation and modification process is accurate, rapid, and requires only modest resources. This work successfully combines dose distribution prediction with natural language processing.", "conclusion": "The 'Virtual Dosimetrist' models provide a robust pipeline for generating suboptimal training plans and allowing trainees to practice their critical plan review and improvement skills, addressing the challenges of the current clinic-based paradigm."}}
{"id": "2505.10128", "pdf": "https://arxiv.org/pdf/2505.10128", "abs": "https://arxiv.org/abs/2505.10128", "authors": ["Huy Q. Le", "Latif U. Khan", "Choong Seon Hong"], "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": "IWCMC 2025", "summary": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.", "AI": {"tldr": "Federated Learning (FL) is a popular solution for privacy-sensitive applications but faces challenges due to statistical heterogeneity. This study introduces FedAPC, a prototype-based FL framework that enhances feature diversity and model robustness, outperforming SOTA baselines.", "motivation": "To address the challenge of domain heterogeneity in Federated Learning which impedes the global model's convergence.", "method": "Introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a prototype-based FL framework. It leverages prototypes derived from the mean features of augmented data to capture richer representations and aligns local features with global prototypes.", "result": "Experimental results on the Office-10 and Digits datasets show that the framework outperforms SOTA baselines.", "conclusion": "FedAPC enhances feature diversity and model robustness in FL under domain heterogeneity."}}
{"id": "2505.10420", "pdf": "https://arxiv.org/pdf/2505.10420", "abs": "https://arxiv.org/abs/2505.10420", "authors": ["Andrei Arhire", "Radu Timofte"], "title": "Learned Lightweight Smartphone ISP with Unpaired Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW 2025", "summary": "The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .", "AI": {"tldr": "This paper presents a novel unpaired training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data. It employs a multi-term loss function guided by adversarial training with multiple discriminators, using lightweight neural network architectures suitable for mobile devices as backbones.", "motivation": "The motivation of this work is to address the challenge of acquiring pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images when developing a learned ISP.", "method": "The method proposed in this paper is an unpaired approach which employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks. This approach maintains content structure while learning color and texture characteristics from the target RGB dataset.", "result": "The method was evaluated on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, the unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics.", "conclusion": "The authors conclude that their unpaired training method for a learnable ISP has strong potential and can achieve high fidelity without needing direct correspondences between raw images and ground-truth data."}}
{"id": "2505.09805", "pdf": "https://arxiv.org/pdf/2505.09805", "abs": "https://arxiv.org/abs/2505.09805", "authors": ["Aditya Nagori", "Ayush Gautam", "Matthew O. Wiens", "Vuong Nguyen", "Nathan Kenya Mugisha", "Jerome Kabakyenga", "Niranjan Kissoon", "John Mark Ansermino", "Rishikesan Kamaleswaran"], "title": "Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "stat.AP"], "comment": "11 pages, 2 Figures, 1 Table", "summary": "Clustering patient subgroups is essential for personalized care and efficient\nresource use. Traditional clustering methods struggle with high-dimensional,\nheterogeneous healthcare data and lack contextual understanding. This study\nevaluates Large Language Model (LLM) based clustering against classical methods\nusing a pediatric sepsis dataset from a low-income country (LIC), containing\n2,686 records with 28 numerical and 119 categorical variables. Patient records\nwere serialized into text with and without a clustering objective. Embeddings\nwere generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with\nlow-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was\napplied to these embeddings. Classical comparisons included K-Medoids\nclustering on UMAP and FAMD-reduced mixed data. Silhouette scores and\nstatistical tests evaluated cluster quality and distinctiveness.\nStella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B\nwith the clustering objective performed better with higher number of clusters,\nidentifying subgroups with distinct nutritional, clinical, and socioeconomic\nprofiles. LLM-based methods outperformed classical techniques by capturing\nricher context and prioritizing key features. These results highlight potential\nof LLMs for contextual phenotyping and informed decision-making in\nresource-limited settings.", "AI": {"tldr": "The study compares LLM-based clustering methods with classical methods on pediatric sepsis data from a low-income country, finding that LLMs, especially Stella-En-400M-V5, outperform classical techniques in capturing richer context and identifying distinct patient subgroups.", "motivation": "Clustering patient subgroups is crucial for personalized care and efficient resource use, but traditional methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding.", "method": "Patient records were serialized into text with and without a clustering objective. Embeddings were generated using three LLMs: quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with LoRA, and Stella-En-400M-V5. K-means clustering was applied to these embeddings. Classical methods included K-Medoids clustering on UMAP and FAMD-reduced mixed data.", "result": "Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles.", "conclusion": "LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features, showing potential for contextual phenotyping and informed decision-making in resource-limited settings."}}
{"id": "2505.10147", "pdf": "https://arxiv.org/pdf/2505.10147", "abs": "https://arxiv.org/abs/2505.10147", "authors": ["Yash", "Nikhil Karamchandani", "Avishek Ghosh"], "title": "Near Optimal Best Arm Identification for Clustered Bandits", "categories": ["cs.LG", "cs.MA"], "comment": "To be published in ICML 2025", "summary": "This work investigates the problem of best arm identification for multi-agent\nmulti-armed bandits. We consider $N$ agents grouped into $M$ clusters, where\neach cluster solves a stochastic bandit problem. The mapping between agents and\nbandits is a priori unknown. Each bandit is associated with $K$ arms, and the\ngoal is to identify the best arm for each agent under a $\\delta$-probably\ncorrect ($\\delta$-PC) framework, while minimizing sample complexity and\ncommunication overhead.\n  We propose two novel algorithms: Clustering then Best Arm Identification\n(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a\ntwo-phase approach that first clusters agents based on the bandit problems they\nare learning, followed by identifying the best arm for each cluster. BAI-Cl\nreverses the sequence by identifying the best arms first and then clustering\nagents accordingly. Both algorithms leverage the successive elimination\nframework to ensure computational efficiency and high accuracy.\n  We establish $\\delta$-PC guarantees for both methods, derive bounds on their\nsample complexity, and provide a lower bound for this problem class. Moreover,\nwhen $M$ is small (a constant), we show that the sample complexity of a variant\nof BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic\nand real-world datasets (MovieLens, Yelp) demonstrate the superior performance\nof the proposed algorithms in terms of sample and communication efficiency,\nparticularly in settings where $M \\ll N$.", "AI": {"tldr": "This paper investigates the best arm identification problem for multi-agent multi-armed bandits, proposing two algorithms Cl-BAI and BAI-Cl that achieve high accuracy and efficiency in terms of sample complexity and communication overhead.", "motivation": "To address the challenge of identifying the best arm for each agent in a multi-agent multi-armed bandit setting where agents are grouped into clusters and the mapping between agents and bandits is unknown.", "method": "Two algorithms, Cl-BAI and BAI-Cl, are proposed. Cl-BAI clusters agents first then identifies the best arm, while BAI-Cl identifies the best arms first then clusters agents. Both use the successive elimination framework.", "result": "Both algorithms provide \u03b4-PC guarantees with bounds on sample complexity. BAI-Cl's variant is minimax optimal when the number of clusters M is small. Experiments show superior performance in sample and communication efficiency especially when M\u226aN.", "conclusion": "The proposed algorithms effectively solve the best arm identification problem in multi-agent multi-armed bandits with unknown agent-bandit mappings, achieving high accuracy and efficiency."}}
{"id": "2505.10453", "pdf": "https://arxiv.org/pdf/2505.10453", "abs": "https://arxiv.org/abs/2505.10453", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "title": "Vision language models have difficulty recognizing virtual objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "AI": {"tldr": "Vision language models (VLMs) are AI systems that can process multimodal input, but their comprehension of visuospatial properties needs improvement.", "motivation": "To test the scene comprehension ability of VLMs by using descriptions of virtual objects in images.", "method": "Systematically evaluate state-of-the-art VLMs with prompts involving virtual objects and analyze their responses.", "result": "The results indicate that VLMs have inadequate ability to process virtual objects and reason about spatial relations.", "conclusion": "VLMs need further development to better comprehend visuospatial properties of scenes depicted in images."}}
{"id": "2505.10167", "pdf": "https://arxiv.org/pdf/2505.10167", "abs": "https://arxiv.org/abs/2505.10167", "authors": ["Saikat Barua", "Mostafizur Rahman", "Shehenaz Khaled", "Md Jafor Sadek", "Rafiul Islam", "Shahnewaz Siddique"], "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "16 pages, 6 figures, 7 equations", "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.", "AI": {"tldr": "The paper introduces QuXAI, a framework featuring Q-MEDLEY to explain feature importance in hybrid quantum-classical machine learning (HQML) models. Q-MEDLEY highlights influential classical aspects, separates noise, and performs well against established XAI techniques. Ablation studies validate its composite structure, improving HQML interpretability and reliability.", "motivation": "To address the lack of robust global and local explainability approaches for HQML architectures that use quantized feature encoding followed by classical learning.", "method": "Creation of HQML models with quantum feature maps and utilization of Q-MEDLEY explainer which combines feature-based inferences, preserves the quantum transformation stage, and visualizes resulting attributions.", "result": "Q-MEDLEY delineates influential classical aspects in HQML models, separates their noise, and performs well against established XAI techniques in classical validation settings. Ablation studies further expose the virtues of the composite structure used in Q-MEDLEY.", "conclusion": "This work provides a way to improve the interpretability and reliability of HQML models, promoting greater confidence and safer use of quantum-enhanced AI technology."}}
{"id": "2505.10473", "pdf": "https://arxiv.org/pdf/2505.10473", "abs": "https://arxiv.org/abs/2505.10473", "authors": ["Fengdi Zhang", "Hongkun Cao", "Ruqi Huang"], "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.", "AI": {"tldr": "ControlGS\u662f\u4e00\u79cd\u4f18\u5316\u76843D\u9ad8\u65af\u70b9\u7ed8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u548c\u7528\u6237\u6307\u5b9a\u7684\u8d85\u53c2\u6570\uff0c\u5b9e\u73b0\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u8de8\u573a\u666f\u4e00\u81f4\u7684\u6570\u91cf-\u8d28\u91cf\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u6570\u91cf-\u8d28\u91cf\u6027\u80fd\u3002\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b83\u80fd\u4ee5\u66f4\u5c11\u7684\u9ad8\u65af\u70b9\u8fbe\u5230\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u5e76\u652f\u6301\u5e7f\u6cdb\u7684\u8c03\u6574\u8303\u56f4\u548c\u65e0\u7ea7\u8c03\u63a7\u3002", "motivation": "\u73b0\u6709\u76843D\u9ad8\u65af\u70b9\u7ed8\u65b9\u6cd5\u867d\u7136\u8ffd\u6c42\u66f4\u597d\u7684\u6570\u91cf-\u8d28\u91cf\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u8ba9\u7528\u6237\u76f4\u89c2\u8c03\u6574\u8fd9\u79cd\u6743\u8861\u4ee5\u9002\u5e94\u5b9e\u9645\u9700\u6c42\uff08\u5982\u5728\u4e0d\u540c\u786c\u4ef6\u548c\u901a\u4fe1\u9650\u5236\u4e0b\u7684\u6a21\u578b\u90e8\u7f72\uff09\u7684\u80fd\u529b\u3002", "method": "ControlGS\u901a\u8fc7\u4e00\u6b21\u56fa\u5b9a\u8bbe\u7f6e\u7684\u8bad\u7ec3\u8fd0\u884c\u548c\u4e00\u4e2a\u53cd\u6620\u6570\u91cf-\u8d28\u91cf\u504f\u597d\u7684\u7528\u6237\u6307\u5b9a\u8d85\u53c2\u6570\uff0c\u80fd\u591f\u81ea\u52a8\u5728\u5404\u79cd\u573a\u666f\u4e2d\u627e\u5230\u7406\u60f3\u7684\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u70b9\uff0c\u5305\u62ec\u7d27\u51d1\u7269\u4f53\u5230\u5927\u578b\u5ba4\u5916\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u8de8\u573a\u666f\u4e00\u81f4\u7684\u6570\u91cf-\u8d28\u91cf\u63a7\u5236\u3002", "result": "ControlGS\u4e0d\u4ec5\u80fd\u5728\u51cf\u5c11\u9ad8\u65af\u70b9\u6570\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\uff0c\u8fd8\u652f\u6301\u5e7f\u6cdb\u8c03\u6574\u8303\u56f4\u5185\u7684\u65e0\u7ea7\u63a7\u5236\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ControlGS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u5f97\u7528\u6237\u53ef\u4ee5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u6839\u636e\u5b9e\u9645\u9700\u6c42\u7075\u6d3b\u5730\u8c03\u65743D\u9ad8\u65af\u70b9\u7ed8\u7684\u6570\u91cf-\u8d28\u91cf\u6743\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.09814", "pdf": "https://arxiv.org/pdf/2505.09814", "abs": "https://arxiv.org/abs/2505.09814", "authors": ["Dmitry Rybin", "Yushun Zhang", "Zhi-Quan Luo"], "title": "$XX^{t}$ Can Be Faster", "categories": ["cs.DS", "cs.AI", "cs.LG", "cs.SC", "68Q25, 68T20", "F.2.1; I.1.2"], "comment": null, "summary": "We present a new algorithm RXTX that computes product of matrix by its\ntranspose $XX^{t}$. RXTX uses $5\\%$ less multiplications and additions than\nState-of-the-Art and achieves accelerations even for small sizes of matrix $X$.\nThe algorithm was discovered by combining Machine Learning-based search methods\nwith Combinatorial Optimization.", "AI": {"tldr": "The paper introduces RXTX, a novel algorithm that computes the product of a matrix by its transpose using fewer multiplications and additions than current state-of-the-art methods.", "motivation": "To develop an algorithm that can compute the product of a matrix by its transpose more efficiently in terms of reducing the number of multiplications and additions.", "method": "By integrating Machine Learning-based search methods with Combinatorial Optimization techniques to discover the new algorithm RXTX.", "result": "RXTX uses 5% less multiplications and additions than the State-of-the-Art, providing accelerations even for small sizes of matrix X.", "conclusion": "RXTX is a more efficient algorithm for computing the product of a matrix by its transpose."}}
{"id": "2505.10172", "pdf": "https://arxiv.org/pdf/2505.10172", "abs": "https://arxiv.org/abs/2505.10172", "authors": ["Zeyan Li", "Libing Chen", "Yin Tang"], "title": "Does Scaling Law Apply in Time Series Forecasting?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Rapid expansion of model size has emerged as a key challenge in time series\nforecasting. From early Transformer with tens of megabytes to recent\narchitectures like TimesNet with thousands of megabytes, performance gains have\noften come at the cost of exponentially increasing parameter counts. But is\nthis scaling truly necessary? To question the applicability of the scaling law\nin time series forecasting, we propose Alinear, an ultra-lightweight\nforecasting model that achieves competitive performance using only k-level\nparameters. We introduce a horizon-aware adaptive decomposition mechanism that\ndynamically rebalances component emphasis across different forecast lengths,\nalongside a progressive frequency attenuation strategy that achieves stable\nprediction in various forecasting horizons without incurring the computational\noverhead of attention mechanisms. Extensive experiments on seven benchmark\ndatasets demonstrate that Alinear consistently outperforms large-scale models\nwhile using less than 1% of their parameters, maintaining strong accuracy\nacross both short and ultra-long forecasting horizons. Moreover, to more fairly\nevaluate model efficiency, we propose a new parameter-aware evaluation metric\nthat highlights the superiority of ALinear under constrained model budgets. Our\nanalysis reveals that the relative importance of trend and seasonal components\nvaries depending on data characteristics rather than following a fixed pattern,\nvalidating the necessity of our adaptive design. This work challenges the\nprevailing belief that larger models are inherently better and suggests a\nparadigm shift toward more efficient time series modeling.", "AI": {"tldr": "Alinear is an ultra-lightweight time series forecasting model that achieves competitive performance with significantly fewer parameters, challenging the necessity of scaling laws in this domain.", "motivation": "The rapid expansion of model size in time series forecasting has led to exponentially increasing parameter counts, raising questions about the necessity of such scaling for performance gains.", "method": "A horizon-aware adaptive decomposition mechanism dynamically rebalances component emphasis across different forecast lengths, and a progressive frequency attenuation strategy ensures stable prediction without attention mechanisms.", "result": "Alinear consistently outperforms large-scale models while using less than 1% of their parameters, across seven benchmark datasets. A new parameter-aware evaluation metric highlights its efficiency under constrained model budgets.", "conclusion": "This work challenges the belief that larger models are inherently better, suggesting a paradigm shift towards more efficient time series modeling."}}
{"id": "2505.10481", "pdf": "https://arxiv.org/pdf/2505.10481", "abs": "https://arxiv.org/abs/2505.10481", "authors": ["Ilya Ovodov", "Petr Surovtsev", "Karina Kvanchiani", "Alexander Kapitanov", "Alexander Nagaev"], "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.", "AI": {"tldr": "This paper addresses two key aspects of isolated sign language recognition (ISLR): limited data for individual sign languages and ambiguity in labeling similar signs. It introduces Logos, a large Russian Sign Language dataset, demonstrating its utility for cross-language ISLR model training, including transfer learning. By explicitly annotating visually similar sign groups, the study improves model quality and outperforms state-of-the-art results on the WLASL dataset.", "motivation": "The motivation lies in tackling two main challenges in ISLR: the scarcity of data for most individual sign languages and the ambiguity caused by similar signs having different semantic meanings.", "method": "The method involves creating and utilizing the Logos dataset, a comprehensive Russian Sign Language dataset. The approach includes pre-training models on Logos for universal encoding in other SLR tasks, exploring cross-language transfer learning, and using explicit annotations for visually similar signs to enhance model performance.", "result": "Results show that pre-trained models on the Logos dataset can effectively serve as universal encoders, improving accuracy especially in low-resource datasets through joint training with multiple classification heads. Explicit labeling of visually similar signs also enhances model quality.", "conclusion": "The study concludes by surpassing current state-of-the-art results on the WLASL dataset and achieving competitive results on the AUTSL dataset, all while using a single stream RGB video model. The source code, dataset, and pre-trained models have been made publicly available."}}
{"id": "2505.09830", "pdf": "https://arxiv.org/pdf/2505.09830", "abs": "https://arxiv.org/abs/2505.09830", "authors": ["Mart\u00edn Rodr\u00edguez", "Gustavo Rossi", "Alejandro Fernandez"], "title": "Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values", "categories": ["cs.SE", "cs.AI"], "comment": "Under revision at Jornadas de Cloud Computing, Big Data & Emerging\n  Topics (JCC-BD&ET) - 2025", "summary": "The design and implementation of unit tests is a complex task many\nprogrammers neglect. This research evaluates the potential of Large Language\nModels (LLMs) in automatically generating test cases, comparing them with\nmanual tests. An optimized prompt was developed, that integrates code and\nrequirements, covering critical cases such as equivalence partitions and\nboundary values. The strengths and weaknesses of LLMs versus trained\nprogrammers were compared through quantitative metrics and manual qualitative\nanalysis. The results show that the effectiveness of LLMs depends on\nwell-designed prompts, robust implementation, and precise requirements.\nAlthough flexible and promising, LLMs still require human supervision. This\nwork highlights the importance of manual qualitative analysis as an essential\ncomplement to automation in unit test evaluation.", "AI": {"tldr": "The study explores the potential of LLMs in generating unit test cases, comparing them with manual tests.", "motivation": "To evaluate the potential of Large Language Models (LLMs) in automatically generating test cases and compare their effectiveness with manual tests.", "method": "An optimized prompt integrating code and requirements was developed to cover critical cases such as equivalence partitions and boundary values. Quantitative metrics and manual qualitative analysis were used to compare LLMs and trained programmers.", "result": "The effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. LLMs are flexible and promising but still require human supervision.", "conclusion": "This work highlights the importance of manual qualitative analysis as a complement to automation in unit test evaluation."}}
{"id": "2505.10192", "pdf": "https://arxiv.org/pdf/2505.10192", "abs": "https://arxiv.org/abs/2505.10192", "authors": ["Prashant P. Shinde", "Priyadarshini P. Pai", "Shashishekar P. Adiga", "K. Subramanya Mayya", "Yongbeom Seo", "Myungsoo Hwang", "Heeyoung Go", "Changmin Park"], "title": "Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models.", "AI": {"tldr": "In semiconductor manufacturing, detecting small defects during EUV patterning is challenging due to lack of quality data. Researchers generate synthetic SEM images with known defect distributions and annotations, then compare object detection models (YOLOv8, EfficientNet, SSD). YOLOv8 shows best performance in detecting smaller defects, suggesting synthetic data can replace real-world data for robust machine-learning models.", "motivation": "To address the challenge of detecting extremely small defects in semiconductor manufacturing which cause false or missed detections due to lack of defect-annotated quality data.", "method": "Artificially generate scanning electron microscopy (SEM) images of line patterns with known defect distributions and autonomously annotate them. Then employ state-of-the-art object detection models like YOLOv8, EfficientNet, and SSD to investigate defect detection performance as a function of defect size.", "result": "YOLOv8 has the best mean average precision of 96% compared to EfficientNet (83%) and SSD (77%). It can reliably detect the smallest defect sizes. Tested on real SEM data, YOLOv8 correctly detected 84.6% of Bridge defects and 78.3% of Break defects.", "conclusion": "Synthetic data can be used as an alternative to real-world data to develop robust machine-learning models for defect detection in semiconductor manufacturing."}}
{"id": "2505.10483", "pdf": "https://arxiv.org/pdf/2505.10483", "abs": "https://arxiv.org/abs/2505.10483", "authors": ["Yi Li", "Haonan Wang", "Qixiang Zhang", "Boyu Xiao", "Chenchang Hu", "Hualiang Wang", "Xiaomeng Li"], "title": "UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric", "summary": "The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.", "AI": {"tldr": "The paper presents UniEval, a new framework for evaluating unified multimodal models without extra models, images, or annotations. It includes UniBench and UniScore which provide more challenging benchmarks and align closely with human evaluations respectively.", "motivation": "There is currently no unified evaluation framework for multimodal models that can provide an overall evaluation without redundancy.", "method": "Introduced UniEval, which consists of UniBench (a holistic benchmark supporting unified and visual generation models) and UniScore (a corresponding metric). UniBench includes 81 fine-grained tags for high diversity.", "result": "Experimental results show that UniBench is more challenging than existing benchmarks and UniScore closely aligns with human evaluations, surpassing current metrics.", "conclusion": "UniEval provides a simplified and unified way to evaluate multimodal models, offering new insights into their unique values."}}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222", "abs": "https://arxiv.org/abs/2505.10222", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.", "AI": {"tldr": "ComplexFormer with Complex Multi-Head Attention (CMHA) improves modeling of semantic and positional information, leading to superior performance in various tasks compared to strong baselines.", "motivation": "Transformer models face challenges in effectively integrating positional information while allowing multi-head attention flexibility. Prior methods often model semantic and positional differences disparately or apply uniform positional adjustments across heads, potentially limiting representational capacity.", "method": "Introduced ComplexFormer featuring Complex Multi-Head Attention (CMHA). CMHA empowers each head to independently model semantic and positional differences unified within the complex plane, representing interactions as rotations and scaling. Two key improvements are incorporated: per-head Euler transformation and per-head adaptive differential rotation mechanism.", "result": "Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show ComplexFormer achieves superior performance, significantly lower generation perplexity, and improved long-context coherence compared to strong baselines like RoPE-Transformers.", "conclusion": "ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism."}}
{"id": "2505.10198", "pdf": "https://arxiv.org/pdf/2505.10198", "abs": "https://arxiv.org/abs/2505.10198", "authors": ["Mariano Ferrero", "Jos\u00e9 Omar Chelotti", "Luciano Sebasti\u00e1n Martinez-Rau", "Leandro Vignolo", "Mart\u00edn Pires", "Julio Ricardo Galli", "Leonardo Luis Giovanini", "Hugo Leonardo Rufiner"], "title": "A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals", "categories": ["cs.LG"], "comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence", "summary": "Monitoring feeding behaviour is a relevant task for efficient herd management\nand the effective use of available resources in grazing cattle. The ability to\nautomatically recognise animals' feeding activities through the identification\nof specific jaw movements allows for the improvement of diet formulation, as\nwell as early detection of metabolic problems and symptoms of animal\ndiscomfort, among other benefits. The use of sensors to obtain signals for such\nmonitoring has become popular in the last two decades. The most frequently\nemployed sensors include accelerometers, microphones, and cameras, each with\nits own set of advantages and drawbacks. An unexplored aspect is the\nsimultaneous use of multiple sensors with the aim of combining signals in order\nto enhance the precision of the estimations. In this direction, this work\nintroduces a deep neural network based on the fusion of acoustic and inertial\nsignals, composed of convolutional, recurrent, and dense layers. The main\nadvantage of this model is the combination of signals through the automatic\nextraction of features independently from each of them. The model has emerged\nfrom an exploration and comparison of different neural network architectures\nproposed in this work, which carry out information fusion at different levels.\nFeature-level fusion has outperformed data and decision-level fusion by at\nleast a 0.14 based on the F1-score metric. Moreover, a comparison with\nstate-of-the-art machine learning methods is presented, including traditional\nand deep learning approaches. The proposed model yielded an F1-score value of\n0.802, representing a 14% increase compared to previous methods. Finally,\nresults from an ablation study and post-training quantization evaluation are\nalso reported.", "AI": {"tldr": "The paper presents a deep neural network model that fuses acoustic and inertial signals for monitoring cattle feeding behavior using multiple sensors. This model, based on convolutional, recurrent, and dense layers, improves precision through automatic feature extraction. Feature-level fusion outperforms other methods by 0.14 in F1-score and shows a 14% increase compared to state-of-the-art methods. An ablation study and post-training quantization evaluation are also provided.", "motivation": "Efficient herd management requires effective monitoring of feeding behaviors in cattle, which can lead to better diet formulation and early detection of health issues. Existing sensor-based approaches have limitations, prompting the exploration of simultaneous multi-sensor use to enhance estimation precision.", "method": "A deep neural network combining convolutional, recurrent, and dense layers is introduced to fuse acoustic and inertial signals. The model explores different levels of information fusion (feature, data, and decision) and selects feature-level fusion as the optimal approach.", "result": "The proposed model achieved an F1-score of 0.802, surpassing existing methods by 14%. Feature-level fusion outperformed data and decision-level fusion by at least 0.14 in F1-score. Additional evaluations include an ablation study and post-training quantization.", "conclusion": "The deep neural network model successfully enhances the precision of cattle feeding behavior monitoring through the fusion of acoustic and inertial signals. Feature-level fusion was identified as the most effective strategy, and the model outperforms current state-of-the-art methods."}}
{"id": "2505.10496", "pdf": "https://arxiv.org/pdf/2505.10496", "abs": "https://arxiv.org/abs/2505.10496", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "AI": {"tldr": "The paper introduces CheXGenBench, an evaluation framework for synthetic chest radiograph generation that assesses fidelity, privacy risks, and clinical utility across 11 text-to-image models. It highlights inefficiencies in current protocols, establishes a new benchmark, and releases SynthCheX-75K dataset.", "motivation": "To address the limitations in medical domain evaluations of generative AI, such as methodological inconsistencies, outdated comparisons, and lack of practical clinical value assessment.", "method": "Developed CheXGenBench with standardized data partitioning and a unified evaluation protocol including over 20 quantitative metrics to evaluate generation quality, privacy vulnerabilities, and clinical applicability.", "result": "Revealed critical inefficiencies in existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent comparisons. Established a new benchmark for medical AI community.", "conclusion": "Introduced CheXGenBench as a rigorous evaluation framework, released SynthCheX-75K dataset, and provided resources at https://raman1121.github.io/CheXGenBench/."}}
{"id": "2505.10213", "pdf": "https://arxiv.org/pdf/2505.10213", "abs": "https://arxiv.org/abs/2505.10213", "authors": ["Mohammadmahdi Ghasemloo", "Alireza Moradi"], "title": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5c06\u7ed3\u6784\u5316\u7684\u65f6\u5e8f\u4fe1\u606f\u6ce8\u5165LLMs\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9664\u4e86\u4f20\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u5916\uff0c\u8fd8\u9700\u8981\u5229\u7528\u5176\u80fd\u529b\u6765\u89e3\u51b3\u5176\u4ed6\u9886\u57df\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002\u8fd9\u5728\u80fd\u6e90\u7cfb\u7edf\u3001\u91d1\u878d\u548c\u533b\u7597\u7b49\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7cfb\u7edf\u5730\u5c06\u7ed3\u6784\u5316\u7684\u65f6\u5e8f\u4fe1\u606f\u6ce8\u5165\u5230LLMs\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u672a\u63d0\u4f9b\u8f85\u52a9\u4fe1\u606f\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u77e5\u8bc6\u5f15\u5bfc\u7684\u9884\u6d4b\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u77e5\u8bc6\u8fc1\u79fb\u7b56\u7565\u6709\u6f5c\u529b\u5f25\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7279\u5b9a\u9886\u57df\u9884\u6d4b\u4efb\u52a1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.10497", "pdf": "https://arxiv.org/pdf/2505.10497", "abs": "https://arxiv.org/abs/2505.10497", "authors": ["Iurii Medvedev", "Nuno Goncalves"], "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.", "AI": {"tldr": "A novel dual-branch classification strategy is proposed to enhance the robustness of face recognition systems against face morphing attacks.", "motivation": "Face recognition systems are vulnerable to presentation attacks, especially face morphing, which allows one identity to impersonate another. Thus, there is a need for more robust face recognition systems.", "method": "The method modifies the classification task by introducing a dual-branch classification strategy that effectively handles the ambiguity in the labeling of face morphs, incorporating morph images into the training process.", "result": "The strategy has been validated on public benchmarks and proved effective in enhancing robustness against face morphing attacks.", "conclusion": "This approach can be universally applied and integrated into existing face recognition training pipelines to improve classification-based recognition methods."}}
{"id": "2505.10533", "pdf": "https://arxiv.org/pdf/2505.10533", "abs": "https://arxiv.org/abs/2505.10533", "authors": ["Aaryan Sharma", "Shivansh Gupta", "Samar Agarwal", "Vishak Prasad C.", "Ganesh Ramakrishnan"], "title": "Enhancing Multi-Image Question Answering via Submodular Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u6a21\u5b50\u96c6\u9009\u62e9\u6280\u672f\u7684\u68c0\u7d22\u6846\u67b6\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u591a\u56fe\u50cf\u95ee\u7b54\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u5355\u5f20\u56fe\u50cf\u76f8\u5173\u7684\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u591a\u5f20\u56fe\u50cf\u96c6\u5408\u65f6\uff08\u5982\u591a\u56fe\u50cf\u95ee\u7b54\u573a\u666f\uff09\uff0c\u5176\u53ef\u6269\u5c55\u6027\u548c\u68c0\u7d22\u6027\u80fd\u9762\u4e34\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5b50\u6a21\u5b50\u96c6\u9009\u62e9\u6280\u672f\uff0c\u5229\u7528GraphCut\u7b49\u67e5\u8be2\u611f\u77e5\u5b50\u6a21\u51fd\u6570\uff0c\u5728\u4e3b\u8981\u68c0\u7d22\u7ec4\u4ef6\u4e4b\u524d\u9884\u9009\u8bed\u4e49\u76f8\u5173\u7684\u56fe\u50cf\u5b50\u96c6\uff0c\u540c\u65f6\u91c7\u7528\u57fa\u4e8e\u951a\u70b9\u7684\u67e5\u8be2\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6765\u63d0\u5347\u68c0\u7d22\u7ba1\u9053\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u6a21\u68c0\u7d22\u7ba1\u9053\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u589e\u5f3a\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u591a\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u68c0\u7d22\u6027\u80fd\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u73af\u5883\u4e2d\u3002"}}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465", "abs": "https://arxiv.org/abs/2505.10465", "authors": ["Yizhou liu", "Ziming Liu", "Jeff Gore"], "title": "Superposition Yields Robust Neural Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.", "AI": {"tldr": "The paper explores why larger language models perform better, focusing on the role of representation superposition in the neural scaling law.", "motivation": "To understand the origin of the neural scaling law where loss decreases with model size.", "method": "Constructed a toy model based on two empirical principles: LLMs represent more things than their dimensions and language features occur with varying frequencies. Analyzed loss scaling under weak and strong superposition scenarios.", "result": "Under weak superposition, loss scales with feature frequency; under strong superposition, loss is inversely proportional to model dimension. Open-sourced LLMs exhibit strong superposition and match the toy model predictions.", "conclusion": "Representation superposition is crucial for the observed neural scaling laws and can inspire new training strategies and model architectures."}}
{"id": "2505.10259", "pdf": "https://arxiv.org/pdf/2505.10259", "abs": "https://arxiv.org/abs/2505.10259", "authors": ["Xiangwen Zhuge", "Xu Shen", "Zeyu Wang", "Fan Dang", "Xuan Ding", "Danyang Li", "Yahui Han", "Tianxiang Hao", "Zheng Yang"], "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices", "categories": ["cs.LG"], "comment": null, "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .", "AI": {"tldr": "SpecOffload is a new inference engine that combines speculative decoding with offloading to improve GPU core utilization and inference throughput for LLMs on resource-constrained devices.", "motivation": "Efficient LLM inference on devices with limited resources is challenging due to compute and memory constraints. Current systems offload model weights to CPU memory, leading to inefficiencies in GPU usage and performance.", "method": "Propose SpecOffload, which embeds speculative decoding into offloading. It uses latent GPU resources to store and execute a draft model for speculative decoding, interleaving the execution of target and draft models within the offloading pipeline. A planner is introduced to manage tensor placement and select optimal parameters.", "result": "SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x compared to the best baseline.", "conclusion": "SpecOffload provides a solution to enhance the efficiency of LLM inference on resource-constrained devices, significantly improving GPU core utilization and inference throughput."}}
{"id": "2505.10541", "pdf": "https://arxiv.org/pdf/2505.10541", "abs": "https://arxiv.org/abs/2505.10541", "authors": ["Pengfei Wang", "Guohai Xu", "Weinong Wang", "Junjie Yang", "Jie Lou", "Yunhua Xue"], "title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.", "AI": {"tldr": "Recent advancements in MLLMs have improved multi-image comprehension, but existing benchmarks overlook whether models truly understand visual input. This paper defines IVM, where MLLMs give correct answers without full visual comprehension. By analyzing causal attention modules, the authors introduce 'attention accuracy' as a new metric and benchmark for quantifying IVMs.", "motivation": "The motivation is to address the limitation of current benchmarks that focus on answer correctness rather than genuine visual comprehension by MLLMs.", "method": "The method involves decoupling visual and textual modalities within the causal attention module to analyze how attention distribution converges on images associated with correct answers as network layers deepen. A scale-agnostic metric called 'attention accuracy' is introduced to evaluate visual understanding directly via internal mechanisms.", "result": "The result is the development of a novel metric and benchmark for quantifying implicit visual misunderstandings (IVMs) in MLLMs, demonstrating effectiveness in both multimodal and unimodal scenarios.", "conclusion": "The conclusion is that 'attention accuracy' provides a reliable way to assess MLLMs' visual understanding, highlighting its versatility and generalizability."}}
{"id": "2505.10475", "pdf": "https://arxiv.org/pdf/2505.10475", "abs": "https://arxiv.org/abs/2505.10475", "authors": ["Mouxiang Chen", "Binyuan Hui", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Jianling Sun", "Junyang Lin", "Zhongxin Liu"], "title": "Parallel Scaling Law for Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.", "AI": {"tldr": "The paper introduces Parallel Scaling (ParScale), a new method for scaling language models that increases parallel computation during training and inference, rather than increasing parameters or output tokens. ParScale reuses existing parameters, applies to various model structures, and theoretically offers superior inference efficiency with less memory and latency increase compared to parameter scaling.", "motivation": "The motivation is to find a more inference-efficient way to scale language models beyond the traditional methods of parameter scaling or inference-time scaling by increasing output tokens.", "method": "ParScale applies P diverse and learnable transformations to the input, executes forward passes of the model in parallel, and dynamically aggregates the P outputs. This scales parallel computation by reusing existing parameters.", "result": "ParScale achieves similar performance improvements as parameter scaling but with up to 22 times less memory increase and 6 times less latency increase. It can also recycle pre-trained models with minimal post-training.", "conclusion": "ParScale provides an alternative perspective on the role of computation in machine learning, facilitates the deployment of powerful models in low-resource scenarios, and reduces the training budget significantly."}}
{"id": "2505.09868", "pdf": "https://arxiv.org/pdf/2505.09868", "abs": "https://arxiv.org/abs/2505.09868", "authors": ["Tin Trung Nguyen", "Jiannan Xu", "Phuong-Anh Nguyen-Le", "Jonathan Lazar", "Donald Braman", "Hal Daum\u00e9 III", "Zubin Jelveh"], "title": "Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Despite its U.S. constitutional foundation, the technical ``individual\nfairness'' criterion has not been operationalized in state or federal\nstatutes/regulations. We conduct a human subjects experiment to address this\ngap, evaluating which demographic features are relevant for individual fairness\nevaluation of recidivism risk assessment (RRA) tools. Our analyses conclude\nthat the individual similarity function should consider age and sex, but it\nshould ignore race.", "AI": {"tldr": "\u5c3d\u7ba1\u201c\u4e2a\u4f53\u516c\u5e73\u201d\u6807\u51c6\u6709\u5176\u7f8e\u56fd\u5baa\u6cd5\u57fa\u7840\uff0c\u4f46\u5b83\u5c1a\u672a\u5728\u5dde\u6216\u8054\u90a6\u6cd5\u89c4\u4e2d\u5f97\u5230\u5177\u4f53\u5316\u3002\u672c\u6587\u901a\u8fc7\u4eba\u4f53\u5b9e\u9a8c\u63a2\u8ba8\u4e86\u54ea\u4e9b\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4e0e\u4e2a\u4f53\u516c\u5e73\u8bc4\u4f30\u518d\u72af\u98ce\u9669\u8bc4\u4f30\uff08RRA\uff09\u5de5\u5177\u76f8\u5173\uff0c\u5e76\u5f97\u51fa\u4e2a\u4f53\u76f8\u4f3c\u6027\u51fd\u6570\u5e94\u8003\u8651\u5e74\u9f84\u548c\u6027\u522b\uff0c\u4f46\u4e0d\u5e94\u8003\u8651\u79cd\u65cf\u3002", "motivation": "\u63a2\u8ba8\u4e2a\u4f53\u516c\u5e73\u6807\u51c6\u5728\u6cd5\u5f8b\u6761\u6b3e\u4e2d\u7684\u7f3a\u5931\uff0c\u5e76\u8bd5\u56fe\u660e\u786e\u54ea\u4e9b\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u5bf9\u4e8e\u4e2a\u4f53\u516c\u5e73\u8bc4\u4f30\u518d\u72af\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u662f\u76f8\u5173\u7684\u3002", "method": "\u8fdb\u884c\u4eba\u4f53\u5b9e\u9a8c\u6765\u8bc4\u4ef7\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u5728\u4e2a\u4f53\u516c\u5e73\u8bc4\u4f30\u518d\u72af\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u4e2d\u7684\u76f8\u5173\u6027\u3002", "result": "\u4e2a\u4f53\u76f8\u4f3c\u6027\u51fd\u6570\u5e94\u8be5\u8003\u8651\u5e74\u9f84\u548c\u6027\u522b\uff0c\u4f46\u4e0d\u5e94\u8003\u8651\u79cd\u65cf\u3002", "conclusion": "\u4e3a\u4e86\u586b\u8865\u4e2a\u4f53\u516c\u5e73\u6807\u51c6\u5728\u6cd5\u5f8b\u6761\u6b3e\u4e2d\u7684\u7a7a\u767d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e2a\u4f53\u76f8\u4f3c\u6027\u51fd\u6570\u5e94\u8003\u8651\u5e74\u9f84\u548c\u6027\u522b\uff0c\u4f46\u4e0d\u5e94\u8003\u8651\u79cd\u65cf\u3002"}}
{"id": "2505.10262", "pdf": "https://arxiv.org/pdf/2505.10262", "abs": "https://arxiv.org/abs/2505.10262", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Lajos Hanzo"], "title": "Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The charging scheduling problem of Electric Buses (EBs) is investigated based\non Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is\nconceived, where the time horizon includes multiple charging and operating\nperiods in a day, while each period is further divided into multiple time\nsteps. To overcome the challenge of long-range multi-phase planning with sparse\nreward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP\ninto a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical\nDouble Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is\nproposed for simultaneously solving the decision problems arising at different\ntemporal resolutions. As a result, the high-level agent learns an effective\npolicy for prescribing the charging targets for every charging period, while\nthe low-level agent learns an optimal policy for setting the charging power of\nevery time step within a single charging period, with the aim of minimizing the\ncharging costs while meeting the charging target. It is proved that the flat\npolicy constructed by superimposing the optimal high-level policy and the\noptimal low-level policy performs as well as the optimal policy of the original\nMDP. Since jointly learning both levels of policies is challenging due to the\nnon-stationarity of the high-level agent and the sampling inefficiency of the\nlow-level agent, we divide the joint learning process into two phases and\nexploit our new HER algorithm to manipulate the experience replay buffers for\nboth levels of agents. Numerical experiments are performed with the aid of\nreal-world data to evaluate the performance of the proposed algorithm.", "AI": {"tldr": "This paper addresses the charging scheduling problem of Electric Buses (EBs) using a Hierarchical Deep Reinforcement Learning (HDRL) approach, which includes a high-level Semi-MDP and multiple low-level MDPs. The authors propose an HDDQN-HER algorithm to optimize charging strategies for cost reduction while meeting charging targets.", "motivation": "The motivation is to solve the complex charging scheduling problem of EBs, characterized by long-range multi-phase planning with sparse rewards, through the use of deep reinforcement learning techniques.", "method": "A Markov Decision Process (MDP) is designed for the charging and operating periods of EBs. This MDP is decoupled into a high-level Semi-MDP and multiple low-level MDPs using Hierarchical DRL (HDRL). An HDDQN-HER algorithm is developed to address decision problems at different temporal resolutions.", "result": "The flat policy created by combining the optimal high-level and low-level policies performs as well as the optimal policy of the original MDP. Numerical experiments using real-world data demonstrate the effectiveness of the proposed algorithm in reducing charging costs while achieving charging targets.", "conclusion": "The hierarchical DRL framework and HDDQN-HER algorithm successfully tackle the charging scheduling problem for EBs, providing an effective strategy for minimizing charging costs."}}
{"id": "2505.10551", "pdf": "https://arxiv.org/pdf/2505.10551", "abs": "https://arxiv.org/abs/2505.10551", "authors": ["Yiwen Liu", "Jessica Bader", "Jae Myung Kim"], "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025", "summary": "With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.", "AI": {"tldr": "This paper explores the concept of feasibility in synthetic images and its impact on CLIP-based classifier performance. The authors introduce VariReal, a pipeline that edits source images with feasible or infeasible attributes. Experiments reveal minimal effect of feasibility on LoRA-fine-tuned CLIP performance.", "motivation": "To investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture.", "method": "Introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model.", "result": "Feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.", "conclusion": "The study concludes that feasibility has minimal impact on CLIP-based classifier performance, suggesting that it may not be crucial to enforce feasibility in synthetic data generation for such models."}}
{"id": "2505.10495", "pdf": "https://arxiv.org/pdf/2505.10495", "abs": "https://arxiv.org/abs/2505.10495", "authors": ["Vibha Belavadi", "Tushar Vatsa", "Dewang Sultania", "Suhas Suresha", "Ishita Verma", "Cheng Chen", "Tracy Holloway King", "Michael Friedrich"], "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing", "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.", "AI": {"tldr": "This paper proposes a novel router-based architecture for generating high-quality synthetic training data to fine-tune Large Language Models (LLMs) for function calling tasks, which significantly improves function classification accuracy and API parameter selection.", "motivation": "The motivation of this paper is to address the challenge of fine-tuning LLMs for function calling tasks in scenarios where real user interaction data is unavailable due to lack of task-specific data and privacy constraints. This necessitates the generation of synthetic data that can replicate real-world data distributions.", "method": "The method involves a router-based architecture that uses domain resources such as content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models, to generate synthetic training data. The flexible routing mechanism ensures that the synthetic data matches observed real-world distributions.", "result": "Evaluation on a comprehensive set of real user queries shows significant improvements in both function classification accuracy and API parameter selection. Models fine-tuned with the proposed synthetic data outperform traditional approaches.", "conclusion": "The proposed router-based architecture for synthetic data generation effectively addresses the limitations of existing approaches, leading to improved performance in function calling tasks and setting new benchmarks."}}
{"id": "2505.10264", "pdf": "https://arxiv.org/pdf/2505.10264", "abs": "https://arxiv.org/abs/2505.10264", "authors": ["Francesco Diana", "Andr\u00e9 Nusser", "Chuan Xu", "Giovanni Neglia"], "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.", "AI": {"tldr": "This paper presents a new data reconstruction attack in Federated Learning that can perfectly recover large data batches without prior knowledge of clients' data, outperforming existing methods.", "motivation": "Despite the privacy-preserving nature of Federated Learning (FL), recent studies have shown that a malicious central server can manipulate model updates to reconstruct clients' private training data. Existing attacks have limitations regarding assumptions on data distribution and efficiency with larger batch sizes.", "method": "The method introduces a novel data reconstruction attack leveraging a geometric perspective on fully connected layers to craft malicious model parameters. This enables the perfect recovery of arbitrarily large data batches in classification tasks without any prior knowledge of clients' data.", "result": "Through experiments on image and tabular datasets, the attack outperforms existing methods and achieves perfect reconstruction of data batches two orders of magnitude larger than the state-of-the-art.", "conclusion": "The proposed data reconstruction attack overcomes limitations of existing methods by enabling perfect recovery of large data batches without prior knowledge of clients' data, highlighting significant vulnerabilities in Federated Learning."}}
{"id": "2505.10557", "pdf": "https://arxiv.org/pdf/2505.10557", "abs": "https://arxiv.org/abs/2505.10557", "authors": ["Ke Wang", "Junting Pan", "Linda Wei", "Aojun Zhou", "Weikang Shi", "Zimu Lu", "Han Xiao", "Yunqiao Yang", "Houxing Ren", "Mingjie Zhan", "Hongsheng Li"], "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.", "AI": {"tldr": "The paper addresses the limitation of current multimodal models in mathematical reasoning by introducing a new approach using code as supervision for cross-modal alignment, resulting in improved performance.", "motivation": "Natural language image-caption datasets used for training Large Multimodal Models focus on natural scenarios and neglect important details of mathematical figures, which are crucial for problem-solving. This limits the progress of LMMs in multimodal mathematical reasoning.", "method": "The authors propose leveraging code as supervision for cross-modal alignment because code contains all information needed to generate corresponding figures. They co-develop an image-to-code model (FigCodifier) and a large dataset (ImgCode-8.6M). Then, they use FigCodifier to synthesize new mathematical figures and create MM-MathInstruct-3M, a high-quality fine-tuning dataset. Finally, they introduce MathCoder-VL, trained with ImgCode-8.6M and fine-tuned on MM-MathInstruct-3M for solving multimodal math problems.", "result": "MathCoder-VL achieves a new open-source state-of-the-art across six metrics. It outperforms GPT-4o and Claude 3.5 Sonnet in geometry problem-solving, improving scores by 8.9% and 9.2%, respectively.", "conclusion": "The proposed method significantly enhances the ability of multimodal models in mathematical reasoning, particularly in geometry problem-solving. The authors will release their datasets and models at https://github.com/mathllm/MathCoder."}}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526", "abs": "https://arxiv.org/abs/2505.10526", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.", "AI": {"tldr": "MASSV transforms small language models into effective multimodal drafters for vision-language models, increasing accepted length by up to 30% and delivering inference speedups of up to 1.46x.", "motivation": "Speculative decoding accelerates language model inference but applying it to vision-language models is challenging due to architectural mismatches and prediction misalignment between small language models and VLMs.", "method": "MASSV uses a two-phase approach: connecting the target VLM's vision encoder to the draft model via a lightweight projector and aligning token predictions through self-distilled visual instruction tuning.", "result": "Experiments show MASSV increases accepted length by up to 30% and provides end-to-end inference speedups of up to 1.46x on visually-grounded tasks.", "conclusion": "MASSV offers a scalable method for accelerating current and future vision-language models."}}
{"id": "2505.10271", "pdf": "https://arxiv.org/pdf/2505.10271", "abs": "https://arxiv.org/abs/2505.10271", "authors": ["Rafael Pablos Sarabia", "Joachim Nyborg", "Morten Birk", "Jeppe Liborius Sj\u00f8rup", "Anders Lillevang Vesterholt", "Ira Assent"], "title": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.", "AI": {"tldr": "This paper presents a deep learning model for high-resolution probabilistic precipitation forecasting in Europe, integrating radar, satellite and NWP data. It surpasses current operational systems.", "motivation": "To overcome the limitations of radar-only deep learning models with short forecast lead times and provide accurate forecasts with robust uncertainty quantification.", "method": "The model integrates multiple data sources (radar, satellite, and physics-based numerical weather prediction) while capturing long-range interactions, featuring a compact architecture for efficient training and faster inference.", "result": "Extensive experiments demonstrate that the model surpasses current operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models.", "conclusion": "This model sets a new standard for high-resolution precipitation forecasting in Europe, balancing accuracy, interpretability, and computational efficiency."}}
{"id": "2505.10562", "pdf": "https://arxiv.org/pdf/2505.10562", "abs": "https://arxiv.org/abs/2505.10562", "authors": ["Wenxuan Wang", "Fan Zhang", "Yufeng Cui", "Haiwen Diao", "Zhuoyan Luo", "Huchuan Lu", "Jing Liu", "Xinlong Wang"], "title": "End-to-End Vision Tokenizer Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.", "AI": {"tldr": "\u73b0\u6709\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\u5728\u4e0b\u6e38\u8bad\u7ec3\u4e2d\u72ec\u7acb\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5668\uff0c\u5047\u8bbe\u89c6\u89c9\u6807\u8bb0\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5206\u79bb\u8303\u5f0f\u5bfc\u81f4\u4e86\u8868\u793a\u74f6\u9888\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86ETT\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u6807\u8bb0\u5668\u8c03\u6574\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5316\u548c\u76ee\u6807\u81ea\u56de\u5f52\u4efb\u52a1\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u51bb\u7ed3\u7684\u6807\u8bb0\u5668\u57fa\u7ebf\u76f8\u6bd4\uff0cETT\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e0a\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\u5728\u4e0b\u6e38\u8bad\u7ec3\u4e2d\u72ec\u7acb\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5668\uff0c\u5047\u8bbe\u89c6\u89c9\u6807\u8bb0\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5206\u79bb\u8303\u5f0f\u5bfc\u81f4\u4e86\u8868\u793a\u74f6\u9888\uff0c\u5373\u89c6\u89c9\u6807\u8bb0\u5316\u7684\u635f\u5931\u53ef\u80fd\u6210\u4e3a\u76ee\u6807\u4efb\u52a1\u7684\u8868\u793a\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aETT\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u6807\u8bb0\u5668\u8c03\u6574\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5316\u548c\u76ee\u6807\u81ea\u56de\u5f52\u4efb\u52a1\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002ETT\u5229\u7528\u6807\u8bb0\u5668\u8bcd\u5178\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u91cd\u5efa\u548c\u6807\u9898\u76ee\u6807\u5bf9\u89c6\u89c9\u6807\u8bb0\u5668\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u51bb\u7ed3\u7684\u6807\u8bb0\u5668\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u6807\u8bb0\u5668\u8c03\u6574\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e0a\u5e26\u6765\u4e862-6%\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u6709\u7684\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u6807\u8bb0\u5668\u8c03\u6574\u65b9\u6cd5ETT\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u6709\u7684\u91cd\u5efa\u80fd\u529b\u3002\u5e0c\u671b\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u4e3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8d4b\u80fd\u3002"}}
{"id": "2505.10272", "pdf": "https://arxiv.org/pdf/2505.10272", "abs": "https://arxiv.org/abs/2505.10272", "authors": ["Niklas Dexheimer", "Sascha Gaudlitz", "Johannes Schmidt-Hieber"], "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Hebbian learning is a key principle underlying learning in biological neural\nnetworks. It postulates that synaptic changes occur locally, depending on the\nactivities of pre- and postsynaptic neurons. While Hebbian learning based on\nneuronal firing rates is well explored, much less is known about learning rules\nthat account for precise spike-timing. We relate a Hebbian\nspike-timing-dependent plasticity rule to noisy gradient descent with respect\nto a natural loss function on the probability simplex. This connection allows\nus to prove that the learning rule eventually identifies the presynaptic neuron\nwith the highest activity. We also discover an intrinsic connection to noisy\nmirror descent.", "AI": {"tldr": "The paper connects Hebbian spike-timing-dependent plasticity to noisy gradient descent, proving the learning rule identifies the most active presynaptic neuron and links it to noisy mirror descent.", "motivation": "To explore learning rules accounting for precise spike-timing in biological neural networks beyond well-studied neuronal firing rates-based Hebbian learning.", "method": "Relating a Hebbian spike-timing-dependent plasticity rule to noisy gradient descent concerning a natural loss function on the probability simplex.", "result": "Proves that the learning rule can identify the presynaptic neuron with the highest activity and uncovers an intrinsic connection to noisy mirror descent.", "conclusion": "Hebbian spike-timing-dependent plasticity can be understood through noisy gradient descent and has a connection with noisy mirror descent, advancing understanding of precise spike-timing in neural networks."}}
{"id": "2505.10565", "pdf": "https://arxiv.org/pdf/2505.10565", "abs": "https://arxiv.org/abs/2505.10565", "authors": ["Zehan Wang", "Siyu Chen", "Lihe Yang", "Jialei Wang", "Ziang Zhang", "Hengshuang Zhao", "Zhou Zhao"], "title": "Depth Anything with Any Prior", "categories": ["cs.CV"], "comment": "Home page: https://prior-depth-anything.github.io/", "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.", "AI": {"tldr": "This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene.", "motivation": "To generate accurate, dense, and detailed metric depth maps by combining incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction.", "method": "Design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. Second, develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors.", "result": "The model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods.", "conclusion": "This framework performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models."}}
{"id": "2505.10296", "pdf": "https://arxiv.org/pdf/2505.10296", "abs": "https://arxiv.org/abs/2505.10296", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Dusit Niyato"], "title": "Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The growing adoption of Electric Buses (EBs) represents a significant step\ntoward sustainable development. By utilizing Internet of Things (IoT) systems,\ncharging stations can autonomously determine charging schedules based on\nreal-time data. However, optimizing EB charging schedules remains a critical\nchallenge due to uncertainties in travel time, energy consumption, and\nfluctuating electricity prices. Moreover, to address real-world complexities,\ncharging policies must make decisions efficiently across multiple time scales\nand remain scalable for large EB fleets. In this paper, we propose a\nHierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the\noriginal Markov Decision Process (MDP) into two augmented MDPs. To solve these\nMDPs and enable multi-timescale decision-making, we introduce a novel HDRL\nalgorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization\nEnhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic\n(DAC) algorithm for large-scale EB fleets are addressed through enhancements at\nboth decision levels. At the high level, we redesign the decentralized actor\nnetwork and integrate an attention mechanism to extract relevant global state\ninformation for each EB, decreasing the size of neural networks. At the low\nlevel, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is\nincorporated into the DAC framework, enabling decentralized and coordinated\ncharging power decisions, reducing computational complexity and enhancing\nconvergence speed. Extensive experiments with real-world data demonstrate the\nsuperior performance and scalability of DAC-MAPPO-E in optimizing EB fleet\ncharging schedules.", "AI": {"tldr": "The paper proposes a Hierarchical Deep Reinforcement Learning (HDRL) approach, DAC-MAPPO-E, to optimize Electric Bus charging schedules using real-time data and multi-timescale decision-making. It reformulates the MDP into two augmented MDPs and incorporates an attention mechanism and MAPPO algorithm for scalability.", "motivation": "The motivation is to address the challenges in optimizing Electric Bus (EB) charging schedules due to uncertainties in travel time, energy consumption, electricity prices, and the need for efficient multi-timescale decision-making and scalability for large EB fleets.", "method": "The method involves proposing a Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the original Markov Decision Process (MDP) into two augmented MDPs. The novel HDRL algorithm, Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E), is introduced to solve these MDPs and enable multi-timescale decision-making. Enhancements are made at both high and low decision levels through redesigning the decentralized actor network with an attention mechanism and incorporating the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm respectively.", "result": "Extensive experiments conducted with real-world data demonstrate the superior performance and scalability of the DAC-MAPPO-E algorithm in optimizing Electric Bus fleet charging schedules.", "conclusion": "The proposed DAC-MAPPO-E algorithm successfully addresses the challenges in optimizing Electric Bus charging schedules by enabling multi-timescale decision-making and ensuring scalability for large EB fleets."}}
{"id": "2505.10566", "pdf": "https://arxiv.org/pdf/2505.10566", "abs": "https://arxiv.org/abs/2505.10566", "authors": ["Yen-Chi Cheng", "Krishna Kumar Singh", "Jae Shin Yoon", "Alex Schwing", "Liangyan Gui", "Matheus Gadelha", "Paul Guerrero", "Nanxuan Zhao"], "title": "3D-Fixup: Advancing Photo Editing with 3D Priors", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. Project page: https://3dfixup.github.io/", "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3D-Fixup\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u5230\u76843D\u5148\u9a8c\u77e5\u8bc6\u6765\u5f15\u5bfc2D\u56fe\u50cf\u7f16\u8f91\u3002\u8be5\u6846\u67b6\u652f\u6301\u8bf8\u5982\u7269\u4f53\u5e73\u79fb\u548c3D\u65cb\u8f6c\u7b49\u590d\u6742\u7684\u7f16\u8f91\u60c5\u51b5\u3002\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u5e76\u4ece\u89c6\u9891\u6570\u636e\u4e2d\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5bf9\uff08\u6e90\u5e27\u548c\u76ee\u6807\u5e27\uff09\uff0c\u540c\u65f6\u7ed3\u5408Image-to-3D\u6a21\u578b\u63d0\u4f9b\u76843D\u6307\u5bfc\uff0c\u4ee5\u786e\u4fdd\u9ad8\u8d28\u91cf\u76843D\u5f15\u5bfc\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c3D-Fixup\u80fd\u591f\u6709\u6548\u5730\u652f\u6301\u590d\u6742\u7684\u3001\u8eab\u4efd\u8fde\u8d2f\u76843D\u611f\u77e5\u7f16\u8f91\uff0c\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5bf9\u56fe\u50cf\u5148\u9a8c\u8fdb\u884c\u5efa\u6a21\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u5bf9\u8c61\u4ec5\u901a\u8fc7\u5355\u4e2a\u56fe\u50cf\u6307\u5b9a\uff0c\u56e0\u6b64\u5177\u67093D\u610f\u8bc6\u7684\u56fe\u50cf\u7f16\u8f91\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e863D-Fixup\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u5e76\u4ece\u89c6\u9891\u6570\u636e\u4e2d\u751f\u6210\u6e90\u5e27\u548c\u76ee\u6807\u5e27\u7684\u6570\u636e\u5bf9\u3002\u6b64\u5916\uff0c\u8fd8\u7ed3\u5408\u4e86\u4e00\u4e2aImage-to-3D\u6a21\u578b\uff0c\u4ee5\u63d0\u4f9b3D\u6307\u5bfc\uff0c\u5c062D\u4fe1\u606f\u663e\u5f0f\u6295\u5f71\u52303D\u7a7a\u95f4\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4ee5\u786e\u4fdd\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9ad8\u8d28\u91cf\u76843D\u5f15\u5bfc\u3002", "result": "\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b3D\u5148\u9a8c\uff0c3D-Fixup\u6709\u6548\u5730\u652f\u6301\u590d\u6742\u7684\u3001\u8eab\u4efd\u8fde\u8d2f\u76843D\u611f\u77e5\u7f16\u8f91\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "3D-Fixup\u662f\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u652f\u6301\u590d\u6742\u76843D\u611f\u77e5\u56fe\u50cf\u7f16\u8f91\uff0c\u5176\u6210\u529f\u5728\u4e8e\u7ed3\u5408\u4e86\u9ad8\u8d28\u91cf\u76843D\u5148\u9a8c\u548c\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2505.10297", "pdf": "https://arxiv.org/pdf/2505.10297", "abs": "https://arxiv.org/abs/2505.10297", "authors": ["Chibueze Peace Obioma", "Youcheng Sun", "Mustafa A. Mustafa"], "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Submitted to ESORICS 2025", "summary": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.", "AI": {"tldr": "In this paper, a novel federated representative-attention-based defense mechanism called FeRA is proposed to address the challenge of detecting backdoor attacks in Federated Learning (FL) systems with non-IID data. FeRA uses cross-client attention over internal feature representations and computes an anomaly score based on representation reconstruction errors.", "motivation": "The motivation is to tackle the difficulty in detecting backdoor attacks in FL due to the diverse, non-IID data produced by the heterogeneous nature of edge devices.", "method": "FeRA leverages cross-client attention over internal feature representations to distinguish benign from malicious clients by computing an anomaly score based on representation reconstruction errors.", "result": "Experimental results indicate that FeRA effectively reduces backdoor attack success rates while maintaining high accuracy on the main task.", "conclusion": "FeRA is robust across various FL scenarios, model-agnostic, attack-agnostic, and does not require labeled reference data, making it suitable for heterogeneous and resource-limited edge deployments."}}
{"id": "2505.09630", "pdf": "https://arxiv.org/pdf/2505.09630", "abs": "https://arxiv.org/abs/2505.09630", "authors": ["Tien Comlekoglu", "J. Quetzalc\u00f3atl Toledo-Mar\u00edn", "Douglas W. DeSimone", "Shayn M. Peirce", "Geoffrey Fox", "James A. Glazier"], "title": "Generative diffusion model surrogates for mechanistic agent-based biological models", "categories": ["q-bio.QM", "cs.CV", "cs.ET", "cs.PF"], "comment": null, "summary": "Mechanistic, multicellular, agent-based models are commonly used to\ninvestigate tissue, organ, and organism-scale biology at single-cell\nresolution. The Cellular-Potts Model (CPM) is a powerful and popular framework\nfor developing and interrogating these models. CPMs become computationally\nexpensive at large space- and time- scales making application and investigation\nof developed models difficult. Surrogate models may allow for the accelerated\nevaluation of CPMs of complex biological systems. However, the stochastic\nnature of these models means each set of parameters may give rise to different\nmodel configurations, complicating surrogate model development. In this work,\nwe leverage denoising diffusion probabilistic models to train a generative AI\nsurrogate of a CPM used to investigate \\textit{in vitro} vasculogenesis. We\ndescribe the use of an image classifier to learn the characteristics that\ndefine unique areas of a 2-dimensional parameter space. We then apply this\nclassifier to aid in surrogate model selection and verification. Our CPM model\nsurrogate generates model configurations 20,000 timesteps ahead of a reference\nconfiguration and demonstrates approximately a 22x reduction in computational\ntime as compared to native code execution. Our work represents a step towards\nthe implementation of DDPMs to develop digital twins of stochastic biological\nsystems.", "AI": {"tldr": "This paper explores the use of denoising diffusion probabilistic models to create a generative AI surrogate for the Cellular-Potts Model (CPM), which is used to study in vitro vasculogenesis. The surrogate model significantly reduces computational time and aids in model selection and verification.", "motivation": "The Cellular-Potts Model, while powerful for modeling biological systems at single-cell resolution, becomes computationally expensive at large scales. There's a need for accelerated evaluation methods without losing accuracy or detail.", "method": "Denoising diffusion probabilistic models are employed to train a generative AI surrogate of the CPM focused on in vitro vasculogenesis. An image classifier is used to understand parameter space characteristics aiding in surrogate model selection and verification.", "result": "The surrogate model successfully generates configurations 20,000 timesteps ahead with about a 22x reduction in computational time compared to native code execution.", "conclusion": "This work marks progress in applying DDPMs for creating digital twins of stochastic biological systems, offering significant computational advantages."}}
{"id": "2505.10307", "pdf": "https://arxiv.org/pdf/2505.10307", "abs": "https://arxiv.org/abs/2505.10307", "authors": ["Yiyang Zhao", "Chengpei Wu", "Lilin Zhang", "Ning Yang"], "title": "Negative Metric Learning for Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Graph contrastive learning (GCL) often suffers from false negatives, which\ndegrades the performance on downstream tasks. The existing methods addressing\nthe false negative issue usually rely on human prior knowledge, still leading\nGCL to suboptimal results. In this paper, we propose a novel Negative Metric\nLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative\nMetric Network (NMN) to build a negative metric space, in which false negatives\ncan be distinguished better from true negatives based on their distance to\nanchor node. To overcome the lack of explicit supervision signals for NML, we\npropose a joint training scheme with bi-level optimization objective, which\nimplicitly utilizes the self-supervision signals to iteratively optimize the\nencoder and the negative metric network. The solid theoretical analysis and the\nextensive experiments conducted on widely used benchmarks verify the\nsuperiority of the proposed method.", "AI": {"tldr": "Graph contrastive learning (GCL) often suffers from false negatives, which degrades the performance on downstream tasks. To solve this problem, a novel Negative Metric Learning (NML) enhanced GCL (NML-GCL) is proposed in this paper.", "motivation": "The motivation of this paper is to address the issue of false negatives in graph contrastive learning (GCL), which leads to suboptimal results in downstream tasks.", "method": "The authors propose NML-GCL that uses a learnable Negative Metric Network (NMN) to create a negative metric space. In this space, false negatives can be better distinguished from true negatives based on their distance to an anchor node. A joint training scheme with bi-level optimization objective is also proposed to optimize the encoder and the negative metric network using self-supervision signals.", "result": "Theoretical analysis and extensive experiments on widely used benchmarks show the superiority of the proposed method.", "conclusion": "NML-GCL effectively addresses the false negative issue in GCL, leading to improved performance on downstream tasks."}}
{"id": "2505.09819", "pdf": "https://arxiv.org/pdf/2505.09819", "abs": "https://arxiv.org/abs/2505.09819", "authors": ["Ruichen Yang", "Gy\u00f6rgy M. L\u00e9vay", "Christopher L. Hunt", "D\u00e1niel Czeiner", "Megan C. Hodgson", "Damini Agarwal", "Rahul R. Kaliki", "Nitish V. Thakor"], "title": "Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "State-of-the-art upper limb myoelectric prostheses often use pattern\nrecognition (PR) control systems that translate electromyography (EMG) signals\ninto desired movements. As prosthesis movement complexity increases, users\noften struggle to produce sufficiently distinct EMG patterns for reliable\nclassification. Existing training typically involves heuristic, trial-and-error\nuser adjustments to static decoder boundaries. Goal: We introduce the Reviewer,\na 3D visual interface projecting EMG signals directly into the decoder's\nclassification space, providing intuitive, real-time insight into PR algorithm\nbehavior. This structured feedback reduces cognitive load and fosters mutual,\ndata-driven adaptation between user-generated EMG patterns and decoder\nboundaries. Methods: A 10-session study with 12 able-bodied participants\ncompared PR performance after motor-based training and updating using the\nReviewer versus conventional virtual arm visualization. Performance was\nassessed using a Fitts law task that involved the aperture of the cursor and\nthe control of orientation. Results: Participants trained with the Reviewer\nachieved higher completion rates, reduced overshoot, and improved path\nefficiency and throughput compared to the standard visualization group.\nSignificance: The Reviewer introduces decoder-informed motor training,\nfacilitating immediate and consistent PR-based myoelectric control\nimprovements. By iteratively refining control through real-time feedback, this\napproach reduces reliance on trial-and-error recalibration, enabling a more\nadaptive, self-correcting training framework. Conclusion: The 3D visual\nfeedback significantly improves PR control in novice operators through\nstructured training, enabling feedback-driven adaptation and reducing reliance\non extensive heuristic adjustments.", "AI": {"tldr": "The paper introduces the Reviewer, a 3D visual interface for EMG signal translation that enhances myoelectric prosthesis control through structured feedback and mutual adaptation. Study results show improved performance in completion rates, path efficiency, and throughput compared to conventional methods.", "motivation": "To address the challenge users face in producing distinct EMG patterns for reliable classification as prosthesis movement complexity increases, and to improve upon existing training methods that rely on heuristic, trial-and-error adjustments.", "method": "A 10-session study with 12 able-bodied participants comparing PR performance after motor-based training and updating using the Reviewer versus conventional virtual arm visualization. Performance was assessed using a Fitts law task.", "result": "Participants trained with the Reviewer achieved higher completion rates, reduced overshoot, and improved path efficiency and throughput compared to the standard visualization group.", "conclusion": "The 3D visual feedback provided by the Reviewer significantly improves PR control in novice operators through structured training, enabling feedback-driven adaptation and reducing reliance on extensive heuristic adjustments."}}
{"id": "2505.10322", "pdf": "https://arxiv.org/pdf/2505.10322", "abs": "https://arxiv.org/abs/2505.10322", "authors": ["Yijie Zhou", "Shi Pu"], "title": "Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks.", "AI": {"tldr": "The paper introduces a refined model of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) which converges under computation-delay-independent step sizes, making it efficient and suitable for real-world decentralized learning tasks.", "motivation": "Decentralized optimization is crucial for leveraging distributed data without central control. However, practical deployments face challenges due to heterogeneous computation speeds and unpredictable communication delays.", "method": "The authors analyze Asynchronous Stochastic Block Coordinate Descent (ASBCD) as a tool to understand the convergence of ADSGD and show that ADSGD converges under computation-delay-independent step sizes without assuming bounded data heterogeneity.", "result": "Empirical experiments reveal that ADSGD outperforms existing methods in wall-clock convergence time across various scenarios.", "conclusion": "ADSGD is simple, efficient in memory and communication, and resilient to communication and computation delays, making it well-suited for real-world decentralized learning tasks."}}
{"id": "2505.09831", "pdf": "https://arxiv.org/pdf/2505.09831", "abs": "https://arxiv.org/abs/2505.09831", "authors": ["Tushar Kataria", "Beatrice Knudsen", "Shireen Y. Elhabian"], "title": "ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hematoxylin and eosin (H&E) staining is a gold standard for microscopic\ndiagnosis in pathology. However, H&E staining does not capture all the\ndiagnostic information that may be needed. To obtain additional molecular\ninformation, immunohistochemical (IHC) stains highlight proteins that mark\nspecific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.\nWhile IHC stains are vital for prognosis and treatment guidance, they are\ntypically only available at specialized centers and time consuming to acquire,\nleading to treatment delays for patients. Virtual staining, enabled by deep\nlearning-based image translation models, provides a promising alternative by\ncomputationally generating IHC stains from H&E stained images. Although many\nGAN and diffusion based image to image (I2I) translation methods have been used\nfor virtual staining, these models treat image patches as independent data\npoints, which results in increased and more diverse data requirements for\neffective generation. We present ImplicitStainer, a novel approach that\nleverages local implicit functions to improve image translation, specifically\nvirtual staining performance, by focusing on pixel-level predictions. This\nmethod enhances robustness to variations in dataset sizes, delivering\nhigh-quality results even with limited data. We validate our approach on two\ndatasets using a comprehensive set of metrics and benchmark it against over\nfifteen state-of-the-art GAN- and diffusion based models. Full Code and models\ntrained will be released publicly via Github upon acceptance.", "AI": {"tldr": "The paper introduces ImplicitStainer, a new method using local implicit functions for virtual staining to generate IHC stains from H&E images. It addresses the limitations of existing GAN and diffusion models by improving performance with limited data and shows superior results through extensive validation.", "motivation": "H&E staining is the standard in pathology but lacks comprehensive diagnostic information. Current IHC staining methods are time-consuming and only available at specialized centers, causing delays in patient treatment. Virtual staining via deep learning offers an alternative solution.", "method": "ImplicitStainer uses local implicit functions to enhance pixel-level predictions in image translation tasks, specifically focusing on generating high-quality IHC stains from H&E images even when data is limited.", "result": "Validated on two datasets using multiple metrics, ImplicitStainer outperforms over fifteen state-of-the-art GAN and diffusion-based models, demonstrating robustness and effectiveness with limited data.", "conclusion": "ImplicitStainer provides a promising advancement in virtual staining technology, offering improved performance with less data compared to existing methods. Full code and trained models will be made publicly available."}}
{"id": "2505.10325", "pdf": "https://arxiv.org/pdf/2505.10325", "abs": "https://arxiv.org/abs/2505.10325", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "AI": {"tldr": "The paper introduces ALERT, a method detecting feature distribution changes in AI models for wireless networks, ensuring model re-training when needed.", "motivation": "AI is crucial for next-gen wireless networks but feature distribution changes can degrade model performance and cause issues. There's a need to detect these changes to ensure model effectiveness.", "method": "ALERT includes three components: representation learning (using MLP), statistical testing (Kolmogorov-Smirnov and Population Stability Index tests), and utility assessment (via a new function).", "result": "ALERT outperforms ten standard drift detection methods in two wireless network use cases: wireless fingerprinting and link anomaly detection.", "conclusion": "ALERT successfully detects feature distribution changes and triggers necessary model re-training in wireless network applications."}}
{"id": "2505.09985", "pdf": "https://arxiv.org/pdf/2505.09985", "abs": "https://arxiv.org/abs/2505.09985", "authors": ["Pengfei Yu", "Bin Huang", "Minghui Zhang", "Weiwen Wu", "Shaoyu Wang", "Qiegen Liu"], "title": "Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Score-based diffusion models have shown significant promise in the field of\nsparse-view CT reconstruction. However, the projection dataset is large and\nriddled with redundancy. Consequently, applying the diffusion model to\nunprocessed data results in lower learning effectiveness and higher learning\ndifficulty, frequently leading to reconstructed images that lack fine details.\nTo address these issues, we propose the ordered-subsets multi-diffusion model\n(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT\nprojection data into equal subsets and employs multi-subsets diffusion model\n(MSDM) to learn from each subset independently. This targeted learning approach\nreduces complexity and enhances the reconstruction of fine details.\nFurthermore, the integration of one-whole diffusion model (OWDM) with complete\nsinogram data acts as a global information constraint, which can reduce the\npossibility of generating erroneous or inconsistent sinogram information.\nMoreover, the OSMM's unsupervised learning framework provides strong robustness\nand generalizability, adapting seamlessly to varying sparsity levels of CT\nsinograms. This ensures consistent and reliable performance across different\nclinical scenarios. Experimental results demonstrate that OSMM outperforms\ntraditional diffusion models in terms of image quality and noise resilience,\noffering a powerful and versatile solution for advanced CT imaging in\nsparse-view scenarios.", "AI": {"tldr": "OSMM is a new model that divides CT projection data into subsets for independent learning, integrates OWDM for global information constraint, and uses an unsupervised learning framework to improve sparse-view CT reconstruction.", "motivation": "Current score-based diffusion models face challenges with large, redundant projection datasets in sparse-view CT reconstruction, leading to lower learning effectiveness, higher difficulty, and reconstructed images lacking fine details.", "method": "The OSMM divides CT projection data into equal subsets and applies MSDM to learn from each subset independently. It also integrates OWDM with complete sinogram data as a global information constraint.", "result": "Experimental results show that OSMM surpasses traditional diffusion models in image quality and noise resilience for sparse-view CT reconstruction.", "conclusion": "OSMM offers a robust and versatile solution for advanced CT imaging in sparse-view scenarios."}}
{"id": "2505.10330", "pdf": "https://arxiv.org/pdf/2505.10330", "abs": "https://arxiv.org/abs/2505.10330", "authors": ["Jonathan Clifford Balloch"], "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change", "categories": ["cs.LG", "cs.AI"], "comment": "PhD Dissertation, 131 pages", "summary": "Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.", "AI": {"tldr": "This paper addresses the challenge of adapting reinforcement learning (RL) agents to changing environments without forgetting prior knowledge.", "motivation": "Real-world autonomous decision-making systems must operate in environments that change over time, but conventional RL methods struggle to adapt when conditions change.", "method": "The dissertation proposes two key capabilities for efficient online adaptation: prioritized exploration and sampling strategies to identify and learn from relevant experiences, and selective preservation of prior knowledge through structured representations.", "result": "Demonstrates that with these two capabilities, RL agents can efficiently adapt their behavior when encountering novel environmental changes during deployment without catastrophically forgetting useful prior knowledge.", "conclusion": "Efficient online adaptation in RL requires prioritized exploration/sampling strategies and selective preservation of prior knowledge through structured representations."}}
{"id": "2505.10075", "pdf": "https://arxiv.org/pdf/2505.10075", "abs": "https://arxiv.org/abs/2505.10075", "authors": ["Jun Guo", "Xiaojian Ma", "Yikai Wang", "Min Yang", "Huaping Liu", "Qing Li"], "title": "FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: see https://sharinka0715.github.io/FlowDreamer/", "summary": "This paper investigates training better visual world models for robot\nmanipulation, i.e., models that can predict future visual observations by\nconditioning on past frames and robot actions. Specifically, we consider world\nmodels that operate on RGB-D frames (RGB-D world models). As opposed to\ncanonical approaches that handle dynamics prediction mostly implicitly and\nreconcile it with visual rendering in a single model, we introduce FlowDreamer,\nwhich adopts 3D scene flow as explicit motion representations. FlowDreamer\nfirst predicts 3D scene flow from past frame and action conditions with a\nU-Net, and then a diffusion model will predict the future frame utilizing the\nscene flow. FlowDreamer is trained end-to-end despite its modularized nature.\nWe conduct experiments on 4 different benchmarks, covering both video\nprediction and visual planning tasks. The results demonstrate that FlowDreamer\nachieves better performance compared to other baseline RGB-D world models by 7%\non semantic similarity, 11% on pixel quality, and 6% on success rate in various\nrobot manipulation domains.", "AI": {"tldr": "The paper presents FlowDreamer, a model that uses 3D scene flow for predicting future frames in robot manipulation tasks, showing improvements over baselines.", "motivation": "To develop better visual world models for robot manipulation that can accurately predict future visual observations using past frames and robot actions.", "method": "FlowDreamer predicts 3D scene flow using a U-Net and then uses a diffusion model to predict future frames, trained end-to-end despite its modular nature.", "result": "FlowDreamer outperforms baseline RGB-D world models by 7% on semantic similarity, 11% on pixel quality, and 6% on success rate across various robot manipulation domains.", "conclusion": "FlowDreamer, with its explicit motion representation through 3D scene flow, provides enhanced performance in video prediction and visual planning tasks for robot manipulation."}}
{"id": "2505.09974", "pdf": "https://arxiv.org/pdf/2505.09974", "abs": "https://arxiv.org/abs/2505.09974", "authors": ["Adel ElZemity", "Budi Arief", "Shujun Li"], "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. We present a systematic evaluation of safety risks in fine-tuned\nLLMs for cyber security applications. Using the OWASP Top 10 for LLM\nApplications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,\nMistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.\nOur evaluation shows that fine-tuning reduces safety resilience across all\ntested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection\ndrops from 0.95 to 0.15). We propose and evaluate a safety alignment approach\nthat carefully rewords instruction-response pairs to include explicit safety\nprecautions and ethical considerations. This approach demonstrates that it is\npossible to maintain or even improve model safety while preserving technical\nutility, offering a practical path forward for developing safer fine-tuning\nmethodologies. This work offers a systematic evaluation for safety risks in\nLLMs, enabling safer adoption of generative AI in sensitive domains, and\ncontributing towards the development of secure, trustworthy, and ethically\naligned LLMs.", "AI": {"tldr": "The integration of LLMs in cyber security applications has opportunities and risks. Fine-tuning reduces safety resilience across tested LLMs. A safety alignment approach rewords instruction-response pairs to include safety precautions and ethical considerations, maintaining or improving model safety while preserving technical utility.", "motivation": "To systematically evaluate the safety risks in fine-tuned LLMs for cyber security applications and propose a method to maintain or improve model safety while preserving technical utility.", "method": "Using the OWASP Top 10 for LLM Applications framework, seven open-source LLMs were assessed. A safety alignment approach was proposed and evaluated, which rewords instruction-response pairs to include explicit safety precautions and ethical considerations.", "result": "Fine-tuning reduces safety resilience across all tested LLMs. The proposed safety alignment approach can maintain or even improve model safety while preserving technical utility.", "conclusion": "This work provides a systematic evaluation of safety risks in LLMs, facilitating safer adoption of generative AI in sensitive domains and contributing to the development of secure, trustworthy, and ethically aligned LLMs."}}
{"id": "2505.10331", "pdf": "https://arxiv.org/pdf/2505.10331", "abs": "https://arxiv.org/abs/2505.10331", "authors": ["Luca Muscarnera", "Luigi Loreti", "Giovanni Todeschini", "Alessio Fumagalli", "Francesco Regazzoni"], "title": "Emergence of Structure in Ensembles of Random Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.", "AI": {"tldr": "\u5728\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\uff0c\u901a\u8fc7\u91c7\u7528\u5206\u7c7b\u635f\u5931\u4f5c\u4e3a\u80fd\u91cf\u7684\u5409\u5e03\u65af\u6d4b\u5ea6\u5bf9\u96c6\u5408\u8fdb\u884c\u52a0\u6743\uff0c\u5b58\u5728\u4e00\u4e2a\u6709\u9650\u6e29\u5ea6\u53c2\u6570\u53ef\u4ee5\u4f7f\u5206\u7c7b\u6548\u679c\u8fbe\u5230\u6700\u4f18\u3002\u6b64\u6e29\u5ea6\u5bf9\u4e8e\u6559\u5e08\u5206\u7c7b\u5668\u548c\u968f\u673a\u5206\u7c7b\u5668\u7684\u6570\u91cf\u5177\u6709\u666e\u9002\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u3001\u65e0\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e2d\u5177\u6709\u91cd\u8981\u6027\u3002", "motivation": "\u8bb8\u591a\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u90fd\u5b58\u5728\u968f\u673a\u6027\u3002\u7531\u968f\u673a\u7ec4\u4ef6\u7ec4\u6210\u7684\u7cfb\u7edf\u901a\u5e38\u8868\u73b0\u51fa\u786e\u5b9a\u6027\u7684\u5168\u5c40\u884c\u4e3a\uff0c\u8fd9\u6807\u5fd7\u7740\u4ece\u5fae\u89c2\u65e0\u5e8f\u5230\u5b8f\u89c2\u7ec4\u7ec7\u7684\u8f6c\u53d8\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u662f\u5341\u5206\u6709\u610f\u4e49\u7684\u3002", "method": "\u5f15\u5165\u7406\u8bba\u6a21\u578b\u6765\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u3002\u901a\u8fc7\u5bf9\u96c6\u5408\u91c7\u7528\u7531\u5206\u7c7b\u635f\u5931\u5b9a\u4e49\u4e3a\u80fd\u91cf\u7684\u5409\u5e03\u65af\u6d4b\u5ea6\u8fdb\u884c\u52a0\u6743\uff0c\u5e76\u5206\u6790\u662f\u5426\u5b58\u5728\u4f7f\u5206\u7c7b\u6548\u679c\u8fbe\u5230\u6700\u4f18\u7684\u6709\u9650\u6e29\u5ea6\u53c2\u6570\u3002\u540c\u65f6\uff0c\u5728\u6837\u672c\u7531\u9ad8\u65af\u5206\u5e03\u751f\u6210\u4e14\u6807\u7b7e\u7531\u6559\u5e08\u611f\u77e5\u673a\u6784\u5efa\u7684\u60c5\u51b5\u4e0b\uff0c\u8fdb\u884c\u89e3\u6790\u8bc1\u660e\u548c\u6570\u503c\u786e\u8ba4\u3002", "result": "\u89e3\u6790\u8bc1\u660e\u548c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5b58\u5728\u4e00\u4e2a\u4e0d\u4f9d\u8d56\u4e8e\u6559\u5e08\u5206\u7c7b\u5668\u548c\u968f\u673a\u5206\u7c7b\u5668\u6570\u91cf\u7684\u6700\u4f18\u6e29\u5ea6\u53c2\u6570\uff0c\u4ece\u800c\u63ed\u793a\u4e86\u6240\u89c2\u5bdf\u884c\u4e3a\u7684\u666e\u904d\u6027\u8d28\u3002\u5728MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u5f3a\u8c03\u4e86\u8fd9\u4e00\u73b0\u8c61\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7406\u8bba\u6a21\u578b\u5e76\u7ed3\u5408\u89e3\u6790\u548c\u6570\u503c\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u5b58\u5728\u4e00\u4e2a\u666e\u9002\u7684\u6700\u4f18\u6e29\u5ea6\u53c2\u6570\uff0c\u4f7f\u5f97\u5206\u7c7b\u6548\u679c\u8fbe\u5230\u6700\u4f18\u3002\u6b64\u53d1\u73b0\u63ed\u793a\u4e86\u7cfb\u7edf\u81ea\u7ec4\u7ec7\u6027\u8d28\u7684\u7269\u7406\u7c7b\u6bd4\u3002"}}
{"id": "2505.10144", "pdf": "https://arxiv.org/pdf/2505.10144", "abs": "https://arxiv.org/abs/2505.10144", "authors": ["Xuechang Tu", "Lukas Radl", "Michael Steiner", "Markus Steinberger", "Bernhard Kerbl", "Fernando de la Torre"], "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality", "categories": ["cs.GR", "cs.CV"], "comment": "I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/", "summary": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters.", "AI": {"tldr": "3DGS\u5728VR\u4e2d\u5b58\u5728\u65f6\u95f4\u4f2a\u5f71\u3001\u6295\u5f71\u5931\u771f\u548c\u5e27\u7387\u964d\u4f4e\u7b49\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faVRSplat\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e76\u6269\u5c55\u4e863DGS\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u6ce8\u89c6\u70b9\u5149\u6805\u5316\u5668\u548c\u4f18\u5316\u6b65\u9aa4\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8672+ FPS\u7684\u540c\u65f6\u6d88\u9664\u4e86\u4f2a\u5f71\u548c\u5e72\u6270\u6f02\u6d6e\u7269\u3002", "motivation": "3DGS\u6280\u672f\u5728\u65b0\u89c6\u56fe\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u5b58\u5728\u65f6\u95f4\u4f2a\u5f71\u3001\u6295\u5f71\u5931\u771f\u548c\u5e27\u7387\u964d\u4f4e\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u5934\u6234\u5f0f\u663e\u793a\u5668\u4e2d\u88ab\u653e\u5927\u3002", "method": "\u7ed3\u5408\u5e76\u6269\u5c55\u4e86Mini-Splatting\u3001StopThePop\u548cOptimal Projection\u7b49\u6280\u672f\uff0c\u4fee\u6539\u4e86\u4e2a\u522b\u6280\u672f\u548c\u6838\u5fc33DGS\u5149\u6805\u5316\u5668\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6ce8\u89c6\u70b9\u5149\u6805\u5316\u5668\uff0c\u53ef\u4ee5\u4e00\u6b21\u6027\u5904\u7406\u7126\u70b9\u548c\u5916\u56f4\u533a\u57df\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\uff1b\u8fd8\u5305\u542b\u4e00\u4e2a\u5fae\u8c03\u6b65\u9aa4\uff0c\u6839\u636eStopThePop\u6df1\u5ea6\u8bc4\u4f30\u548cOptimal Projection\u4f18\u5316\u9ad8\u65af\u53c2\u6570\u3002", "result": "\u901a\u8fc725\u540d\u53c2\u4e0e\u8005\u7684\u5bf9\u7167\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u5bf9\u5176\u4ed6Mini-Splatting\u914d\u7f6e\u7684\u5f3a\u70c8\u504f\u597d\u3002VRSplat\u662f\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u76843DGS\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u73b0\u4ee3VR\u5e94\u7528\uff0c\u8fbe\u523072+ FPS\uff0c\u540c\u65f6\u6d88\u9664\u5f39\u51fa\u548c\u7acb\u4f53\u7834\u574f\u6f02\u6d6e\u7269\u3002", "conclusion": "VRSplat\u662f\u9996\u4e2a\u7ecf\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e14\u80fd\u591f\u652f\u6301\u73b0\u4ee3VR\u5e94\u7528\u76843DGS\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86VR\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2505.09989", "pdf": "https://arxiv.org/pdf/2505.09989", "abs": "https://arxiv.org/abs/2505.09989", "authors": ["Tella Rajashekhar Reddy", "Palak", "Rohan Gandhi", "Anjaly Parayil", "Chaojie Zhang", "Mike Shepperd", "Liangcheng Yu", "Jayashree Mohan", "Srinivasan Iyengar", "Shivkumar Kalyanaraman", "Debopam Bhattacherjee"], "title": "AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron", "categories": ["cs.DC", "cs.AI", "cs.NI"], "comment": null, "summary": "AI power demand is growing unprecedentedly thanks to the high power density\nof AI compute and the emerging inferencing workload. On the supply side,\nabundant wind power is waiting for grid access in interconnection queues. In\nthis light, this paper argues bringing AI workload to modular compute clusters\nco-located in wind farms. Our deployment right-sizing strategy makes it\neconomically viable to deploy more than 6 million high-end GPUs today that\ncould consume cheap, green power at its source. We built Heron, a cross-site\nsoftware router, that could efficiently leverage the complementarity of power\ngeneration across wind farms by routing AI inferencing workload around power\ndrops. Using 1-week ofcoding and conversation production traces from Azure and\n(real) variable wind power traces, we show how Heron improves aggregate goodput\nof AI compute by up to 80% compared to the state-of-the-art.", "AI": {"tldr": "This paper proposes to bring AI workload to modular compute clusters co-located in wind farms, and introduces Heron, a cross-site software router that routes AI inferencing workload according to power generation, improving the aggregate goodput of AI compute by up to 80%.", "motivation": "AI power demand is growing unprecedentedly while abundant wind power awaits grid access. This drives the motivation to explore bringing AI workloads closer to renewable energy sources.", "method": "Deploying AI workloads in modular compute clusters co-located in wind farms, using a deployment right-sizing strategy. Developing Heron, a cross-site software router, to leverage complementarity of power generation across wind farms by routing AI inferencing workload around power drops.", "result": "Heron improves aggregate goodput of AI compute by up to 80% compared to the state-of-the-art when tested with 1-week production traces from Azure and real variable wind power traces.", "conclusion": "Bringing AI workloads to wind farms through modular compute clusters and leveraging Heron for efficient workload routing can significantly improve AI compute goodput while utilizing cheap, green power."}}
{"id": "2505.10344", "pdf": "https://arxiv.org/pdf/2505.10344", "abs": "https://arxiv.org/abs/2505.10344", "authors": ["Alan Jeffares", "Liyuan Liu"], "title": "An Introduction to Discrete Variational Autoencoders", "categories": ["cs.LG"], "comment": "Tutorial paper", "summary": "Variational Autoencoders (VAEs) are well-established as a principled approach\nto probabilistic unsupervised learning with neural networks. Typically, an\nencoder network defines the parameters of a Gaussian distributed latent space\nfrom which we can sample and pass realizations to a decoder network. This model\nis trained to reconstruct its inputs and is optimized through the evidence\nlower bound. In recent years, discrete latent spaces have grown in popularity,\nsuggesting that they may be a natural choice for many data modalities (e.g.\ntext). In this tutorial, we provide a rigorous, yet practical, introduction to\ndiscrete variational autoencoders -- specifically, VAEs in which the latent\nspace is made up of latent variables that follow a categorical distribution. We\nassume only a basic mathematical background with which we carefully derive each\nstep from first principles. From there, we develop a concrete training recipe\nand provide an example implementation, hosted at\nhttps://github.com/alanjeffares/discreteVAE.", "AI": {"tldr": "The paper provides a tutorial on discrete variational autoencoders with categorical latent variables, offering a training recipe and example implementation.", "motivation": "To provide a clear and practical introduction to discrete variational autoencoders for those with basic mathematical knowledge.", "method": "Introduces VAEs with discrete latent spaces where the latent variables follow a categorical distribution, deriving each step from first principles.", "result": "Develops a concrete training recipe and offers an example implementation available on GitHub.", "conclusion": "Discrete variational autoencoders with categorical latent variables are presented as a natural choice for certain data modalities, such as text."}}
{"id": "2505.10012", "pdf": "https://arxiv.org/pdf/2505.10012", "abs": "https://arxiv.org/abs/2505.10012", "authors": ["Tadashi Kadowaki"], "title": "Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering", "categories": ["quant-ph", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Recent advances in artificial intelligence (AI) and quantum computing are\naccelerating automation in scientific and engineering processes, fundamentally\nreshaping research methodologies. This perspective highlights parallels between\nscientific automation and established Computer-Aided Engineering (CAE)\npractices, introducing Quantum CAE as a framework that leverages quantum\nalgorithms for simulation, optimization, and machine learning within\nengineering design. Practical implementations of Quantum CAE are illustrated\nthrough case studies for combinatorial optimization problems. Further\ndiscussions include advancements toward higher automation levels, highlighting\nthe critical role of specialized AI agents proficient in quantum algorithm\ndesign. The integration of quantum computing with AI raises significant\nquestions about the collaborative dynamics among human scientists and\nengineers, AI systems, and quantum computational resources, underscoring a\ntransformative future for automated discovery and innovation.", "AI": {"tldr": "This paper explores the integration of quantum computing and AI in scientific automation, introducing Quantum CAE for tasks like simulation and optimization, with case studies on combinatorial problems. It emphasizes the role of AI agents in quantum algorithm design and questions the future collaboration between humans, AI, and quantum resources.", "motivation": "The motivation lies in the recent advances in AI and quantum computing that are reshaping research methodologies, necessitating a framework that can leverage these technologies for enhanced automation in engineering processes.", "method": "The method involves using Quantum CAE as a framework which applies quantum algorithms for simulation, optimization, and machine learning within engineering design, supported by practical implementations through case studies focused on combinatorial optimization problems.", "result": "The results include successful illustrations of Quantum CAE applications in solving combinatorial optimization problems and discussions on advancements towards higher automation levels, highlighting the importance of specialized AI agents in proficient quantum algorithm design.", "conclusion": "The conclusion underscores the transformative potential of integrating quantum computing with AI, posing significant questions about future collaborative dynamics among human scientists, engineers, AI systems, and quantum computational resources."}}
{"id": "2505.10347", "pdf": "https://arxiv.org/pdf/2505.10347", "abs": "https://arxiv.org/abs/2505.10347", "authors": ["Gabriel S. Gama", "Valdir Grassi Jr"], "title": "Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The code will be made publicly available.", "AI": {"tldr": "Specialized Multi-Task Optimizers (SMTOs) are evaluated on complex multi-task problems, revealing their competitive performance against uniform loss and fixed weights.", "motivation": "To address critiques suggesting that equally weighted tasks can achieve results as good as SMTOs due to poor hyperparameter optimization and lack of regularization in previous studies.", "method": "Conduct an extensive empirical evaluation of SMTOs, including recent methods, on more complex multi-task problems.", "result": "SMTOs perform well compared to uniform loss; fixed weights can achieve competitive performance compared to SMTOs. Uniform loss performs similarly to SMTOs in some cases.", "conclusion": "SMTOs show competitive performance, but fixed weights can also be effective. The reasons for uniform loss performing similarly to SMTOs in certain scenarios are demonstrated."}}
{"id": "2505.10312", "pdf": "https://arxiv.org/pdf/2505.10312", "abs": "https://arxiv.org/abs/2505.10312", "authors": ["Anh Tuan Ha", "Hoang Khang Phan", "Thai Minh Tien Ngo", "Anh Phan Truong", "Nhat Tan Le"], "title": "SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "In the realm of Human Activity Recognition (HAR), obtaining high quality and\nvariance data is still a persistent challenge due to high costs and the\ninherent variability of real-world activities. This study introduces a\ngeneration dataset by deep learning approaches (Attention Autoencoder and\nconditional Generative Adversarial Networks). Another problem that data\nheterogeneity is a critical challenge, one of the solutions is to shuffle the\ndata to homogenize the distribution. Experimental results demonstrate that the\nrandom sequence strategy significantly improves classification performance,\nachieving an accuracy of up to 0.70 $\\pm$ 0.03 and a macro F1 score of 0.64\n$\\pm$ 0.01. For that, disrupting temporal dependencies through random sequence\nreordering compels the model to focus on instantaneous recognition, thereby\nimproving robustness against activity transitions. This approach not only\nbroadens the effective training dataset but also offers promising avenues for\nenhancing HAR systems in complex, real-world scenarios.", "AI": {"tldr": "In Human Activity Recognition (HAR), generating high-quality data and handling data heterogeneity are challenges. This study uses deep learning methods (Attention Autoencoder and conditional GANs) to create a generation dataset, and shuffles data sequences to homogenize distribution. Experiments show this random sequence strategy improves classification performance with accuracy up to 0.70 \u00b1 0.03 and macro F1 score of 0.64 \u00b1 0.01. Disrupting temporal dependencies forces the model to focus on instant recognition, improving robustness.", "motivation": "To address the challenges of obtaining high-quality and diverse data in HAR, as well as dealing with data heterogeneity.", "method": "Used Attention Autoencoder and conditional Generative Adversarial Networks to generate a dataset, and applied a random sequence strategy to shuffle data for homogenizing distribution.", "result": "Classification performance significantly improved with accuracy up to 0.70 \u00b1 0.03 and macro F1 score of 0.64 \u00b1 0.01.", "conclusion": "This approach not only broadens the effective training dataset but also enhances HAR systems' robustness in complex real-world scenarios."}}
{"id": "2505.10360", "pdf": "https://arxiv.org/pdf/2505.10360", "abs": "https://arxiv.org/abs/2505.10360", "authors": ["Victor Petr\u00e9n Bach Hansen", "Lasse Krogsb\u00f8ll", "Jonas Lyngs\u00f8", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maal\u00f8e"], "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": null, "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFactsR\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u7597\u54a8\u8be2\u671f\u95f4\u5b9e\u65f6\u63d0\u53d6\u91cd\u8981\u4e34\u5e8a\u4fe1\u606f\uff08\u79f0\u4e3aFacts\uff09\uff0c\u5e76\u901a\u8fc7\u9012\u5f52\u4f7f\u7528\u8fd9\u4e9b\u4fe1\u606f\u751f\u6210\u6700\u7ec8\u7684\u533b\u7597\u8bb0\u5f55\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u8ba9\u533b\u751f\u53c2\u4e0e\u8bb0\u5f55\u751f\u6210\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u3001\u9519\u8bef\u8868\u793a\u548c\u8fc7\u5ea6\u4f9d\u8d56\u533b\u751f\u6821\u5bf9\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bb0\u5f55\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u540c\u65f6\u4e3a\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u5f00\u8f9f\u4e86\u65b0\u7684\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u8bb8\u591aAI\u4e66\u8bb0\u5458\u89e3\u51b3\u65b9\u6848\u5728\u533b\u7597\u54a8\u8be2\u7ed3\u675f\u540e\u751f\u6210\u7b14\u8bb0\u65f6\uff0c\u4f9d\u8d56\u4e8e\u4e00\u6b21\u6027\u6216\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\uff0c\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u80fd\u5bfc\u81f4\u957f\u7bc7\u7b14\u8bb0\u4e2d\u51fa\u73b0\u5e7b\u89c9\u3001\u8bef\u89e3\u533b\u751f\u610f\u56fe\u4ee5\u53ca\u9700\u8981\u533b\u751f\u6821\u5bf9\u6355\u6349\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u5de5\u4f5c\u91cf\u548c\u75b2\u52b3\u5f71\u54cd\u8b66\u89c9\u6027\u65f6\u4f1a\u5bf9\u60a3\u8005\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002", "method": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aFactsR\u7684\u65b9\u6cd5\uff0c\u5728\u533b\u7597\u54a8\u8be2\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u63d0\u53d6\u91cd\u8981\u4e34\u5e8a\u4fe1\u606f\uff08\u79f0\u4e3aFacts\uff09\uff0c\u5e76\u9012\u5f52\u5730\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u751f\u6210\u6700\u7ec8\u7684\u7b14\u8bb0\u3002\u8be5\u65b9\u6cd5\u5c06\u533b\u751f\u7eb3\u5165\u5230\u7b14\u8bb0\u751f\u6210\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u7b14\u8bb0\u7684\u8d28\u91cf\u3002", "result": "FactsR\u65b9\u6cd5\u751f\u6210\u7684\u7b14\u8bb0\u66f4\u51c6\u786e\u3001\u66f4\u7b80\u6d01\uff0c\u5e76\u4e14\u4e3a\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u7684\u5e94\u7528\u53ef\u80fd\u6027\u3002", "conclusion": "FactsR\u65b9\u6cd5\u901a\u8fc7\u5b9e\u65f6\u63d0\u53d6\u4e34\u5e8a\u4fe1\u606f\u548c\u9012\u5f52\u751f\u6210\u7b14\u8bb0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u7597\u8bb0\u5f55\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u60a3\u8005\u5b89\u5168\u6027\uff0c\u4e3a\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.10405", "pdf": "https://arxiv.org/pdf/2505.10405", "abs": "https://arxiv.org/abs/2505.10405", "authors": ["Jianhao Huang", "Qunsong Zeng", "Kaibin Huang"], "title": "Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.", "AI": {"tldr": "The paper proposes a hybrid generative semantic communication (Gen-SemCom) system with critical information embedding framework for 6G networks. It introduces the GVIF metric to evaluate visual quality and designs a channel-adaptive system.", "motivation": "To overcome the issues of losing fine-grained visual details in purely prompt-driven generation and the lack of systematic metrics to evaluate Gen-SemCom systems.", "method": "Developed a hybrid Gen-SemCom system using a critical information embedding (CIE) framework which transmits both text prompts and semantically critical features. Proposed the generative visual information fidelity (GVIF) metric to assess visual quality. Designed a channel-adaptive Gen-SemCom system based on maximizing the GVIF metric.", "result": "Validated that the GVIF metric is sensitive to visual fidelity, correlating with PSNR and critical information volume. The optimized system outperforms benchmarking schemes with higher PSNR and lower FID scores.", "conclusion": "The hybrid Gen-SemCom system with CIE framework and GVIF metric effectively addresses the limitations of current Gen-SemCom systems and shows superior performance."}}
{"id": "2505.10392", "pdf": "https://arxiv.org/pdf/2505.10392", "abs": "https://arxiv.org/abs/2505.10392", "authors": ["Aryan Mishra", "Lizhen Lin"], "title": "Schreier-Coset Graph Propagation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure , preprint", "summary": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.", "AI": {"tldr": "Schrier-Coset Graph Propagation (SCGP) is a new method in GNNs that improves long-range message passing without altering graph topology, showing advantages in hierarchical and modular graphs.", "motivation": "To solve the over-squashing problem in GNNs while avoiding scalability bottlenecks.", "method": "Introduces SCGP, which uses Schreier-coset embeddings to enrich node features without changing the input graph topology.", "result": "Empirical evaluations show SCGP performs as well as or better than existing methods, particularly excelling with hierarchical and modular graphs.", "conclusion": "SCGP offers improved performance, scalability, and efficiency for real-time and resource-constrained applications."}}
{"id": "2505.10441", "pdf": "https://arxiv.org/pdf/2505.10441", "abs": "https://arxiv.org/abs/2505.10441", "authors": ["Filippo Leveni", "Luca Magri", "Giacomo Boracchi", "Cesare Alippi"], "title": "PIF: Anomaly detection via preference embedding", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Pattern Recognition (ICPR\n  2020)", "summary": "We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.", "AI": {"tldr": "The paper introduces PIF, a new anomaly detection method that merges adaptive isolation techniques with preference embedding, showing superior performance in experiments.", "motivation": "To improve anomaly detection by leveraging structured patterns through combining adaptive isolation methods and preference embedding.", "method": "PIF embeds data into high dimensional space and uses PI-Forest, a tree-based method, to calculate anomaly scores.", "result": "PIF outperforms state-of-the-art anomaly detection methods in experiments on both synthetic and real datasets. PI-Forest is more effective at measuring distances and isolating points in the preference space.", "conclusion": "PIF presents a promising approach for anomaly detection by integrating adaptive isolation and preference embedding."}}
{"id": "2505.10407", "pdf": "https://arxiv.org/pdf/2505.10407", "abs": "https://arxiv.org/abs/2505.10407", "authors": ["Wenhao Ding", "Choon Hwai Yap", "Kangjun Ji", "Sim\u00e3o Castro"], "title": "Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning", "categories": ["cs.LG", "68T07"], "comment": "10 pages, 2 figures", "summary": "A generative model for the mesh geometry of intracranial aneurysms (IA) is\ncrucial for training networks to predict blood flow forces in real time, which\nis a key factor affecting disease progression. This need is necessitated by the\nabsence of a large IA image datasets. Existing shape generation methods\nstruggle to capture realistic IA features and ignore the relationship between\nIA pouches and parent vessels, limiting physiological realism and their\ngeneration cannot be controlled to have specific morphological measurements. We\npropose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh\ngenerator. In the first stage, AneuG generates low-dimensional Graph Harmonic\nDeformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,\nconstrained to morphing energy statistics truths. GHD enables more accurate\nshape encoding than alternatives. In the second stage, AneuG generates parent\nvessels conditioned on GHD tokens, by generating vascular centreline and\npropagating the cross-section. AneuG's IA shape generation can further be\nconditioned to have specific clinically relevant morphological measurements.\nThis is useful for studies to understand shape variations represented by\nclinical measurements, and for flow simulation studies to understand effects of\nspecific clinical shape parameters on fluid dynamics. Source code and\nimplementation details are available at\nhttps://github.com/anonymousaneug/AneuG.", "AI": {"tldr": "AneuG is a two-stage VAE-based IA mesh generator that creates realistic aneurysm shapes and parent vessels, conditioned on specific morphological measurements, to aid in flow simulation studies.", "motivation": "The lack of large IA image datasets makes it difficult to train networks for predicting blood flow forces in real time. Existing shape generation methods fail to capture realistic IA features and ignore the relationship between IA pouches and parent vessels.", "method": "AneuG uses a two-stage VAE approach. In the first stage, it generates low-dimensional GHD tokens to encode and reconstruct aneurysm pouch shapes with constraints on morphing energy statistics. In the second stage, it generates parent vessels conditioned on GHD tokens by creating vascular centreline and propagating the cross-section.", "result": "AneuG can generate IA shapes with specific clinically relevant morphological measurements, improving physiological realism and enabling controlled generation.", "conclusion": "AneuG provides a valuable tool for understanding shape variations and the effects of specific clinical shape parameters on fluid dynamics, contributing to both research and potential clinical applications."}}
{"id": "2505.10457", "pdf": "https://arxiv.org/pdf/2505.10457", "abs": "https://arxiv.org/abs/2505.10457", "authors": ["Matteo Gambella", "Vicente Javier Castro Solar", "Manuel Roveri"], "title": "SEAL: Searching Expandable Architectures for Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "8 pages, 5 figures", "summary": "Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.", "AI": {"tldr": "SEAL is a NAS-based framework designed for data-incremental learning that dynamically adapts model structure, reduces forgetting, and enhances accuracy while maintaining a lower model size.", "motivation": "Incremental learning requires balancing plasticity (learning new tasks) and stability (preserving past knowledge), but existing NAS-based approaches often expand the model at every task, making them impractical in resource-constrained environments.", "method": "SEAL uses a capacity estimation metric to determine when to expand the model structure, preserving stability through cross-distillation training after each expansion step. The NAS component searches for both the architecture and the optimal expansion policy.", "result": "Experiments show SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods across multiple benchmarks.", "conclusion": "Combining NAS and selective expansion holds promise for efficient, adaptive learning in incremental scenarios."}}
{"id": "2505.10043", "pdf": "https://arxiv.org/pdf/2505.10043", "abs": "https://arxiv.org/abs/2505.10043", "authors": ["Yifan Wu", "Lutao Yan", "Yizhang Zhu", "Yinan Mei", "Jiannan Wang", "Nan Tang", "Yuyu Luo"], "title": "Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Charts are crucial for data analysis and decision-making.Text-to-chart\nretrieval systems have become increasingly important for Business Intelligence\n(BI), where users need to find relevant charts that match their analytical\nneeds. These needs can be categorized into precise queries that are\nwell-specified and fuzzy queries that are more exploratory -- both require\nunderstanding the semantics and context of the charts. However, existing\ntext-to-chart retrieval solutions often fail to capture the semantic content\nand contextual information of charts, primarily due to the lack of\ncomprehensive metadata (or semantic insights). To address this limitation, we\npropose a training data development pipeline that automatically synthesizes\nhierarchical semantic insights for charts, covering visual patterns\n(visual-oriented), statistical properties (statistics-oriented), and practical\napplications (task-oriented), which produces 207,498 semantic insights for\n69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to\nlearn better representations of charts for text-to-chart retrieval. Our method\nleverages rich semantic insights during the training phase to develop a model\nthat understands both visual and semantic aspects of charts.To evaluate\ntext-to-chart retrieval performance, we curate the first benchmark, CRBench,\nfor this task with 21,862 charts and 326 text queries from real-world BI\napplications, with ground-truth labels verified by the crowd\nworkers.Experiments show that ChartFinder significantly outperforms existing\nmethods in text-to-chart retrieval tasks across various settings. For precise\nqueries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than\nstate-of-the-art models. In fuzzy query tasks, our method also demonstrates\nconsistent improvements, with an average increase of 5% across nearly all\nmetrics.", "AI": {"tldr": "\u4e3a\u4e86\u63d0\u5347\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5c42\u6b21\u8bed\u4e49\u4fe1\u606f\u7684\u8bad\u7ec3\u6570\u636e\u7ba1\u9053\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u4e86\u540d\u4e3aChartFinder\u7684\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u7cbe\u786e\u67e5\u8be2\u548c\u6a21\u7cca\u67e5\u8be2\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u56fe\u8868\u7684\u8bed\u4e49\u5185\u5bb9\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u5168\u9762\u7684\u5143\u6570\u636e\uff08\u6216\u8bed\u4e49\u6d1e\u89c1\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u5f00\u53d1\u7ba1\u9053\uff0c\u81ea\u52a8\u5408\u6210\u56fe\u8868\u7684\u5c42\u6b21\u8bed\u4e49\u6d1e\u89c1\uff0c\u5305\u62ec\u89c6\u89c9\u6a21\u5f0f\u3001\u7edf\u8ba1\u7279\u6027\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aChartFinder\u7684CLIP-based\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChartFinder\u5728\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5bf9\u4e8e\u7cbe\u786e\u67e5\u8be2\uff0cChartFinder\u7684NDCG@10\u8fbe\u523066.9%\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u6a21\u578b\u9ad8\u51fa11.58%\u3002\u5728\u6a21\u7cca\u67e5\u8be2\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e5f\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u51e0\u4e4e\u6240\u6709\u6307\u6807\u5e73\u5747\u63d0\u9ad8\u4e865%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684ChartFinder\u6a21\u578b\u901a\u8fc7\u5229\u7528\u4e30\u5bcc\u7684\u8bed\u4e49\u6d1e\u89c1\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u5230\u4e86\u66f4\u597d\u7684\u56fe\u8868\u8868\u793a\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10422", "pdf": "https://arxiv.org/pdf/2505.10422", "abs": "https://arxiv.org/abs/2505.10422", "authors": ["Daniel Weitekamp", "Christopher MacLellan", "Erik Harpstead", "Kenneth Koedinger"], "title": "Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency", "categories": ["cs.LG"], "comment": "To appear in CogSci 2025", "summary": "Human learning relies on specialization -- distinct cognitive mechanisms\nworking together to enable rapid learning. In contrast, most modern neural\nnetworks rely on a single mechanism: gradient descent over an objective\nfunction. This raises the question: might human learners' relatively rapid\nlearning from just tens of examples instead of tens of thousands in data-driven\ndeep learning arise from our ability to use multiple specialized mechanisms of\nlearning in combination? We investigate this question through an ablation\nanalysis of inductive human learning simulations in online tutoring\nenvironments. Comparing reinforcement learning to a more data-efficient\n3-mechanism symbolic rule induction approach, we find that decomposing learning\ninto multiple distinct mechanisms significantly improves data efficiency,\nbringing it in line with human learning. Furthermore, we show that this\ndecomposition has a greater impact on efficiency than the distinction between\nsymbolic and subsymbolic learning alone. Efforts to align data-driven machine\nlearning with human learning often overlook the stark difference in learning\nefficiency. Our findings suggest that integrating multiple specialized learning\nmechanisms may be key to bridging this gap.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u5b66\u4e60\u5206\u89e3\u4e3a\u591a\u4e2a\u4e0d\u540c\u7684\u673a\u5236\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4f7f\u5176\u4e0e\u4eba\u7c7b\u5b66\u4e60\u76f8\u4e00\u81f4\u3002\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u5355\u72ec\u533a\u5206\u7b26\u53f7\u548c\u5b50\u7b26\u53f7\u5b66\u4e60\u5bf9\u6548\u7387\u7684\u5f71\u54cd\u66f4\u5927\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6574\u5408\u591a\u79cd\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u662f\u5f25\u5408\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u4e0e\u4eba\u7c7b\u5b66\u4e60\u6548\u7387\u5dee\u8ddd\u7684\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u5b66\u4e60\u8005\u80fd\u591f\u4ece\u5c11\u91cf\u4f8b\u5b50\u4e2d\u5feb\u901f\u5b66\u4e60\u7684\u539f\u56e0\uff0c\u662f\u5426\u6e90\u4e8e\u6211\u4eec\u80fd\u591f\u7ed3\u5408\u4f7f\u7528\u591a\u79cd\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5bf9\u5728\u7ebf\u8f85\u5bfc\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u4eba\u7c7b\u5b66\u4e60\u6a21\u62df\u8fdb\u884c\u6d88\u878d\u5206\u6790\uff0c\u6bd4\u8f83\u5f3a\u5316\u5b66\u4e60\u4e0e\u66f4\u9ad8\u6548\u7684\u6570\u636e3\u673a\u5236\u7b26\u53f7\u89c4\u5219\u5f52\u7eb3\u65b9\u6cd5\u3002", "result": "\u5c06\u5b66\u4e60\u5206\u89e3\u4e3a\u591a\u4e2a\u4e0d\u540c\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u4e0e\u4eba\u7c7b\u5b66\u4e60\u6c34\u5e73\u76f8\u5f53\uff0c\u5e76\u4e14\u8fd9\u79cd\u5206\u89e3\u5bf9\u6548\u7387\u7684\u5f71\u54cd\u5927\u4e8e\u7b26\u53f7\u4e0e\u5b50\u7b26\u53f7\u5b66\u4e60\u7684\u533a\u522b\u3002", "conclusion": "\u6574\u5408\u591a\u79cd\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u662f\u5f25\u5408\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u4e0e\u4eba\u7c7b\u5b66\u4e60\u6548\u7387\u5dee\u8ddd\u7684\u5173\u952e\u3002"}}
{"id": "2505.10464", "pdf": "https://arxiv.org/pdf/2505.10464", "abs": "https://arxiv.org/abs/2505.10464", "authors": ["Jiaming Liang", "Lihuan Dai", "Xiaoqi Sheng", "Xiangguang Chen", "Chun Yao", "Guihua Tao", "Qibin Leng", "Honming Cai", "Xi Zhong"], "title": "HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been provisionally accepted for MICCAI 2025", "summary": "Multimodal medical image segmentation faces significant challenges in the\ncontext of gastric cancer lesion analysis. This clinical context is defined by\nthe scarcity of independent multimodal datasets and the imperative to\namalgamate inherently misaligned modalities. As a result, algorithms are\nconstrained to train on approximate data and depend on application migration,\nleading to substantial resource expenditure and a potential decline in analysis\naccuracy. To address those challenges, we have made two major contributions:\nFirst, we publicly disseminate the GCM 2025 dataset, which serves as the first\nlarge-scale, open-source collection of gastric cancer multimodal MRI scans,\nfeaturing professionally annotated FS-T2W, CE-T1W, and ADC images from 500\npatients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework\nthat employs an original HWA block with learnable window aggregation layers to\nestablish dynamic feature correspondences between different modalities'\nanatomical structures, and leverages the innovative tri-orientated fusion mamba\nmechanism for context modeling and capturing long-range spatial dependencies.\nExtensive experiments on our GCM 2025 dataset and the publicly BraTS 2021\ndataset validate the performance of our framework, demonstrating that the new\napproach surpasses existing methods by up to 1.68\\% in the Dice score while\nmaintaining solid robustness. The dataset and code are public via\nhttps://github.com/JeMing-creater/HWA-UNETR.", "AI": {"tldr": "In the field of gastric cancer lesion analysis, multimodal medical image segmentation encounters challenges due to lack of datasets and difficulty in aligning modalities. To solve these problems, the paper presents GCM 2025, a new large-scale dataset, and HWA-UNETR, a novel 3D segmentation framework. Experiments show that HWA-UNETR outperforms existing methods with a Dice score improvement of up to 1.68%.", "motivation": "Multimodal medical image segmentation for gastric cancer lesion analysis is constrained by the scarcity of independent multimodal datasets and the challenge of amalgamating misaligned modalities, which leads to substantial resource expenditure and potential decline in accuracy.", "method": "The paper introduces HWA-UNETR, a 3D segmentation framework incorporating an HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities' anatomical structures. It also utilizes a tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies.", "result": "Experiments on the GCM 2025 and BraTS 2021 datasets demonstrate that HWA-UNETR surpasses existing methods by up to 1.68% in the Dice score while maintaining robustness.", "conclusion": "The introduction of the GCM 2025 dataset and the HWA-UNETR framework provides a valuable resource and effective solution for multimodal medical image segmentation in gastric cancer lesion analysis."}}
{"id": "2505.10423", "pdf": "https://arxiv.org/pdf/2505.10423", "abs": "https://arxiv.org/abs/2505.10423", "authors": ["Ari Karchmer", "Eran Malach"], "title": "The Power of Random Features and the Limits of Distribution-Free Gradient Descent", "categories": ["cs.LG"], "comment": null, "summary": "We study the relationship between gradient-based optimization of parametric\nmodels (e.g., neural networks) and optimization of linear combinations of\nrandom features. Our main result shows that if a parametric model can be\nlearned using mini-batch stochastic gradient descent (bSGD) without making\nassumptions about the data distribution, then with high probability, the target\nfunction can also be approximated using a polynomial-sized combination of\nrandom features. The size of this combination depends on the number of gradient\nsteps and numerical precision used in the bSGD process. This finding reveals\nfundamental limitations of distribution-free learning in neural networks\ntrained by gradient descent, highlighting why making assumptions about data\ndistributions is often crucial in practice. Along the way, we also introduce a\nnew theoretical framework called average probabilistic dimension complexity\n(adc), which extends the probabilistic dimension complexity developed by Kamath\net al. (2020). We prove that adc has a polynomial relationship with statistical\nquery dimension, and use this relationship to demonstrate an infinite\nseparation between adc and standard dimension complexity.", "AI": {"tldr": "This paper explores the connection between gradient-based optimization in parametric models like neural networks and linear combinations of random features. It demonstrates that if a model can be learned via mini-batch stochastic gradient descent without data distribution assumptions, then with high probability, the target function can be approximated using a polynomial-sized combination of random features.", "motivation": "To understand the relationship between gradient-based optimization of parametric models (such as neural networks) and optimization of linear combinations of random features, especially under scenarios where no assumptions about data distribution are made.", "method": "The authors prove that if a parametric model can be learned through mini-batch stochastic gradient descent (bSGD) without making assumptions on the data distribution, then the target function can also be approximated by a polynomial-sized combination of random features. They introduce average probabilistic dimension complexity (adc), which extends probabilistic dimension complexity, and demonstrate its polynomial relationship with statistical query dimension.", "result": "The size of the combination of random features needed to approximate the target function depends on the number of gradient steps and numerical precision used in bSGD. This reveals fundamental limitations of distribution-free learning in neural networks trained by gradient descent. The study also shows an infinite separation between adc and standard dimension complexity.", "conclusion": "This work highlights the importance of making assumptions about data distributions when training neural networks with gradient descent due to the inherent limitations of distribution-free learning. Additionally, it introduces a new theoretical framework, adc, which provides deeper insights into the complexities involved in such optimizations."}}
{"id": "2505.10492", "pdf": "https://arxiv.org/pdf/2505.10492", "abs": "https://arxiv.org/abs/2505.10492", "authors": ["Taylor L. Bobrow", "Mayank Golhar", "Suchapa Arayakarnkul", "Anthony A. Song", "Saowanee Ngamruengphong", "Nicholas J. Durr"], "title": "Multi-contrast laser endoscopy for in vivo gastrointestinal imaging", "categories": ["eess.IV", "cs.CV", "physics.optics"], "comment": null, "summary": "White light endoscopy is the clinical gold standard for detecting diseases in\nthe gastrointestinal tract. Most applications involve identifying visual\nabnormalities in tissue color, texture, and shape. Unfortunately, the contrast\nof these features is often subtle, causing many clinically relevant cases to go\nundetected. To overcome this challenge, we introduce Multi-contrast Laser\nEndoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable\nspectral, coherent, and directional illumination. We demonstrate three\ncapabilities of MLE: enhancing tissue chromophore contrast with multispectral\ndiffuse reflectance, quantifying blood flow using laser speckle contrast\nimaging, and characterizing mucosal topography using photometric stereo. We\nvalidate MLE with benchtop models, then demonstrate MLE in vivo during clinical\ncolonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold\nimprovement in contrast and a five-fold improvement in color difference\ncompared to white light and narrow band imaging. With the ability to reveal\nmultiple complementary types of tissue contrast while seamlessly integrating\ninto the clinical environment, MLE shows promise as an investigative tool to\nimprove gastrointestinal imaging.", "AI": {"tldr": "The paper introduces Multi-contrast Laser Endoscopy (MLE) for better gastrointestinal imaging, showing improved contrast and color difference in detecting polyps compared to traditional methods.", "motivation": "To improve the detection of diseases in the gastrointestinal tract by enhancing the contrast of visual abnormalities that are often subtle in white light endoscopy.", "method": "Developed MLE platform with rapidly tunable spectral, coherent, and directional illumination. Demonstrated capabilities include multispectral diffuse reflectance, laser speckle contrast imaging, and photometric stereo.", "result": "MLE achieved a three-fold improvement in contrast and five-fold improvement in color difference when imaging 31 polyps compared to white light and narrow band imaging.", "conclusion": "MLE shows potential as an investigative tool to enhance gastrointestinal imaging by revealing multiple types of tissue contrast while integrating into the clinical environment."}}
{"id": "2505.10425", "pdf": "https://arxiv.org/pdf/2505.10425", "abs": "https://arxiv.org/abs/2505.10425", "authors": ["Jingyao Wang", "Wenwen Qiang", "Zeen Song", "Changwen Zheng", "Hui Xiong"], "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.", "AI": {"tldr": "The paper introduces Learning to Think (L2T), a framework for fine-tuning large language models (LLMs) to optimize reasoning effectiveness and computational efficiency. L2T uses an information-theoretic reinforcement approach with a novel reward mechanism based on PAC-Bayes bounds and the Fisher information matrix, reducing computational complexity while maintaining estimation accuracy. Empirical results show improvements in reasoning tasks.", "motivation": "Existing methods for improving reasoning abilities in LLMs do not adequately balance reasoning effectiveness with computational efficiency, often leading to unnecessarily long reasoning chains and wasted tokens.", "method": "L2T treats each query-response interaction as a hierarchical session of multiple episodes and proposes a universal dense process reward that quantifies episode-wise information gain in parameters without needing extra annotations or task-specific evaluators. A method is proposed to quickly estimate this reward using PAC-Bayes bounds and the Fisher information matrix, theoretically reducing computational complexity with high accuracy. Reinforcement learning optimizes the model by rewarding each episode's contribution and penalizing excessive updates.", "result": "Empirical results across various reasoning benchmarks and base models demonstrate L2T's advantage in different tasks, enhancing both reasoning effectiveness and efficiency.", "conclusion": "Learning to Think (L2T) provides a promising approach to fine-tune LLMs for achieving optimal reasoning with fewer tokens, thereby improving both the effectiveness and efficiency of reasoning tasks."}}
{"id": "2505.10432", "pdf": "https://arxiv.org/pdf/2505.10432", "abs": "https://arxiv.org/abs/2505.10432", "authors": ["Randy J. Chase", "Katherine Haynes", "Lander Ver Hoef", "Imme Ebert-Uphoff"], "title": "Score-based diffusion nowcasting of GOES imagery", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4e91\u548c\u964d\u6c34\u77ed\u65f6\u9884\u62a5\uff080\u81f33\u5c0f\u65f6\uff09\u7684\u6f5c\u529b\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u63a8\u8fdb\u73b0\u6709\u4e91\u5c42\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\u5c42\uff0c\u5305\u62ec\u5bf9\u6d41\u542f\u52a8\u3002\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e09\u79cd\u6269\u6563\u6a21\u578b\uff0c\u53d1\u73b0\u6b8b\u5dee\u6821\u6b63\u6269\u6563\u6a21\u578b\uff08CorrDiff\uff09\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u4f20\u7edfU-Net\u548c\u5176\u4ed6\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6269\u6563\u6a21\u578b\u8fd8\u5177\u5907\u751f\u6210\u96c6\u5408\u9884\u62a5\u7684\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u8bef\u5dee\u4f30\u8ba1\u3002", "motivation": "\u4e91\u548c\u964d\u6c34\u5bf9\u4e8e\u7406\u89e3\u5929\u6c14\u548c\u6c14\u5019\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u6b21\u7f51\u683c\u53c2\u6570\u5316\uff0c\u4f20\u7edf\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u3002\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u5df2\u88ab\u7528\u4e8e\u9884\u6d4b\u4e91\u548c\u964d\u6c34\uff0c\u4f46\u65e9\u671f\u65b9\u6cd5\u5e38\u4ea7\u751f\u6a21\u7cca\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u2014\u2014\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6539\u5584\u77ed\u65f6\u9884\u62a5\u7cbe\u5ea6\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e09\u79cd\u4e3b\u8981\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\uff1a\u6807\u51c6\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\uff08Diff\uff09\u3001\u6b8b\u5dee\u6821\u6b63\u6269\u6563\u6a21\u578b\uff08CorrDiff\uff09\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u3002\u8fd9\u4e9b\u6a21\u578b\u5229\u7528\u8fc7\u53bb20\u5206\u949f\u7684\u5730\u7403\u540c\u6b65\u7ea2\u5916\u536b\u661f\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u8fdb\u884c\u4e91\u548c\u964d\u6c34\u7684\u77ed\u65f6\u9884\u62a5\u3002", "result": "\u6269\u6563\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u63a8\u8fdb\u73b0\u6709\u7684\u4e91\u5c42\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\u5c42\uff0c\u5305\u62ec\u5bf9\u6d41\u542f\u52a8\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0cCorrDiff\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5176\u5747\u65b9\u6839\u8bef\u5dee\u6bd4\u5176\u4ed6\u6269\u6563\u6a21\u578b\u3001\u4f20\u7edfU-Net\u4ee5\u53ca\u6301\u7eed\u6027\u9884\u62a5\u4f4e1\u81f32\u5f00\u5c14\u6587\u3002\u6b64\u5916\uff0c\u6269\u6563\u6a21\u578b\u8fd8\u80fd\u591f\u81ea\u52a8\u751f\u6210\u96c6\u5408\u9884\u62a5\uff0c\u5176\u96c6\u5408\u79bb\u6563\u5ea6\u4e0e\u8bef\u5dee\u76f8\u5173\u6027\u826f\u597d\uff0c\u663e\u793a\u51fa\u6709\u6548\u7684\u6821\u51c6\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u5728\u4e91\u548c\u964d\u6c34\u77ed\u65f6\u9884\u62a5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662fCorrDiff\u6a21\u578b\uff0c\u5176\u7cbe\u5ea6\u548c\u6821\u51c6\u80fd\u529b\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u5929\u6c14\u9884\u62a5\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.10073", "pdf": "https://arxiv.org/pdf/2505.10073", "abs": "https://arxiv.org/abs/2505.10073", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering", "categories": ["cs.RO", "cs.AI"], "comment": "5 pages, 4 figures, Scheduled for presentation at an upcoming\n  conference", "summary": "In this paper, a novel framework is presented that achieves a combined\nsolution based on Multi-Robot Task Allocation (MRTA) and collision avoidance\nwith respect to homogeneous measurement tasks taking place in industrial\nenvironments. The spatial clustering we propose offers to simultaneously solve\nthe task allocation problem and deal with collision risks by cutting the\nworkspace into distinguishable operational zones for each robot. To divide task\nsites and to schedule robot routes within corresponding clusters, we use\nK-means clustering and the 2-Opt algorithm. The presented framework shows\nsatisfactory performance, where up to 93\\% time reduction (1.24s against\n17.62s) with a solution quality improvement of up to 7\\% compared to the best\nperforming method is demonstrated. Our method also completely eliminates\ncollision points that persist in comparative methods in a most significant\nsense. Theoretical analysis agrees with the claim that spatial partitioning\nunifies the apparently disjoint tasks allocation and collision avoidance\nproblems under conditions of many identical tasks to be distributed over sparse\ngeographical areas. Ultimately, the findings in this work are of substantial\nimportance for real world applications where both computational efficiency and\noperation free from collisions is of paramount importance.", "AI": {"tldr": "A novel framework for Multi-Robot Task Allocation (MRTA) and collision avoidance is presented, using spatial clustering, K-means, and 2-Opt algorithm to reduce time by up to 93% and eliminate collisions in industrial environments.", "motivation": "To create a combined solution for MRTA and collision avoidance in industrial environments with homogeneous measurement tasks, improving computational efficiency and ensuring operation free from collisions.", "method": "The method uses spatial clustering to divide the workspace into operational zones for each robot, applying K-means clustering to divide task sites and the 2-Opt algorithm to schedule robot routes within corresponding clusters.", "result": "The framework achieves up to 93% time reduction and a 7% improvement in solution quality compared to the best performing method, while completely eliminating collision points.", "conclusion": "The findings indicate that spatial partitioning unifies task allocation and collision avoidance problems, making this approach highly significant for real-world applications requiring both computational efficiency and collision-free operations."}}
{"id": "2505.10438", "pdf": "https://arxiv.org/pdf/2505.10438", "abs": "https://arxiv.org/abs/2505.10438", "authors": ["David Grasev"], "title": "Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "51 pages, 28 figures", "summary": "Gas turbine engines represent complex highly nonlinear dynamical systems.\nDeriving their physics-based models can be challenging as it requires\nperformance characteristics, that are not always available, and one often has\nto make many simplifying assumptions. In this paper, the limitations of\nconventional experimental methods used to derive component-level and locally\nlinear parameter-varying models are discussed and addressed by employing\nidentification techniques based on data collected from standard engine\noperation under closed-loop control. The rotor dynamics were estimated using\nthe sparse identification of nonlinear dynamics. Subsequently, the autonomous\npart of the dynamics was mapped into an optimally constructed Koopman\neigenfunction space. The process included eigenvalue optimization using\nmetaheuristic algorithms and temporal projection, followed by gradient-based\neigenfunction identification. The resulting Koopman model was validated against\nan in-house reference component-level model. A globally optimal nonlinear\nfeedback controller and a Kalman estimator were then designed in the\neigenfunction space and compared to the classical and gain-scheduled\nproportional-integral controllers, as well as a proposed internal model control\napproach. The eigenmode structure allowed targeting individual modes during the\noptimization process, resulting in a better performance tuning. The results\nshowed that the Koopman-based controller outperformed the other benchmark\ncontrollers in both reference tracking and disturbance rejection, under\nsea-level and varying flight conditions, due to its global nature.", "AI": {"tldr": "The paper discusses the limitations of conventional methods for deriving gas turbine engine models and proposes a new approach using Koopman eigenfunction space. The resulting Koopman model was validated against an in-house reference model, and the Koopman-based controller outperformed other benchmark controllers.", "motivation": "Gas turbine engines are complex systems with nonlinear dynamics, making it challenging to derive physics-based models. Conventional experimental methods have limitations when creating component-level and locally linear parameter-varying models.", "method": "Employ identification techniques based on data from standard engine operation under closed-loop control. Use sparse identification of nonlinear dynamics for rotor dynamics, map autonomous dynamics into Koopman eigenfunction space via eigenvalue optimization and temporal projection, followed by gradient-based eigenfunction identification.", "result": "The Koopman-based controller outperformed classical and gain-scheduled proportional-integral controllers, as well as an internal model control approach, in both reference tracking and disturbance rejection under varying conditions due to its global nature.", "conclusion": "Koopman eigenfunction space allows targeting individual modes during optimization, leading to better performance tuning. The proposed method provides a globally optimal nonlinear feedback controller."}}
{"id": "2505.10558", "pdf": "https://arxiv.org/pdf/2505.10558", "abs": "https://arxiv.org/abs/2505.10558", "authors": ["Peiying Zhang", "Nanxuan Zhao", "Jing Liao"], "title": "Style Customization of Text-to-Vector Generation with Image Diffusion Priors", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Paper). Project page:\n  https://customsvg.github.io", "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.", "AI": {"tldr": "This paper proposes a two-stage style customization pipeline for SVG generation that leverages both feed-forward T2V models and T2I image priors, ensuring structural regularity while enabling diverse and high-quality custom styles based on text prompts.", "motivation": "Existing text-to-vector (T2V) methods for creating SVGs from text prompts often lack the ability to perform style customization, which is crucial for practical applications requiring consistent visual appearance and coherent aesthetics in vector graphics collections.", "method": "The method involves a novel two-stage pipeline. In stage one, a T2V diffusion model with path-level representation is trained to ensure structural regularity of SVGs. In stage two, this model is customized to different styles through knowledge distillation from customized T2I models.", "result": "Through extensive experiments, the proposed pipeline has been shown to generate high-quality and diverse SVGs in custom styles efficiently using text prompts.", "conclusion": "The authors conclude that their two-stage style customization pipeline successfully addresses the challenges faced by existing T2V methods, providing an effective solution for generating SVGs with both structural regularity and customizable styles."}}
{"id": "2505.10101", "pdf": "https://arxiv.org/pdf/2505.10101", "abs": "https://arxiv.org/abs/2505.10101", "authors": ["Jongmin Jung", "Dasaem Jeong"], "title": "LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.MM", "eess.AS"], "comment": "Paper accepted at ISEA 2025, The 30th International Symposium on\n  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025", "summary": "This paper introduces LAV (Latent Audio-Visual), a system that integrates\nEnCodec's neural audio compression with StyleGAN2's generative capabilities to\nproduce visually dynamic outputs driven by pre-recorded audio. Unlike previous\nworks that rely on explicit feature mappings, LAV uses EnCodec embeddings as\nlatent representations, directly transformed into StyleGAN2's style latent\nspace via randomly initialized linear mapping. This approach preserves semantic\nrichness in the transformation, enabling nuanced and semantically coherent\naudio-visual translations. The framework demonstrates the potential of using\npretrained audio compression models for artistic and computational\napplications.", "AI": {"tldr": "LAV\u7cfb\u7edf\u7ed3\u5408\u4e86EnCodec\u7684\u795e\u7ecf\u97f3\u9891\u538b\u7f29\u4e0eStyleGAN2\u7684\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u968f\u673a\u521d\u59cb\u5316\u7ebf\u6027\u6620\u5c04\u5c06EnCodec\u5d4c\u5165\u8f6c\u6362\u4e3aStyleGAN2\u7684\u6837\u5f0f\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u800c\u751f\u6210\u7531\u9884\u5f55\u97f3\u9891\u9a71\u52a8\u7684\u89c6\u89c9\u52a8\u6001\u8f93\u51fa\u3002\u6b64\u65b9\u6cd5\u4fdd\u7559\u4e86\u8bed\u4e49\u4e30\u5bcc\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u97f3\u9891\u538b\u7f29\u6a21\u578b\u5728\u827a\u672f\u548c\u8ba1\u7b97\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u591a\u4f9d\u8d56\u663e\u5f0f\u7279\u5f81\u6620\u5c04\u8fdb\u884c\u97f3\u89c6\u9891\u8f6c\u6362\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528EnCodec\u7684\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u8868\u793a\uff0c\u76f4\u63a5\u8f6c\u6362\u5230StyleGAN2\u7684\u6837\u5f0f\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7ec6\u81f4\u548c\u8bed\u4e49\u8fde\u8d2f\u7684\u97f3\u89c6\u9891\u8f6c\u6362\u3002", "method": "LAV\u7cfb\u7edf\u4f7f\u7528EnCodec\u7684\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u521d\u59cb\u5316\u7684\u7ebf\u6027\u6620\u5c04\u5c06\u5176\u76f4\u63a5\u8f6c\u6362\u5230StyleGAN2\u7684\u6837\u5f0f\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u751f\u6210\u89c6\u89c9\u52a8\u6001\u8f93\u51fa\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u663e\u5f0f\u7684\u7279\u5f81\u6620\u5c04\uff0c\u800c\u662f\u4fdd\u7559\u4e86\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u7531\u9884\u5f55\u97f3\u9891\u9a71\u52a8\u7684\u89c6\u89c9\u52a8\u6001\u8f93\u51fa\uff0c\u5c55\u73b0\u4e86\u8bed\u4e49\u8fde\u8d2f\u4e14\u7ec6\u817b\u7684\u97f3\u89c6\u9891\u8f6c\u6362\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u97f3\u9891\u538b\u7f29\u6a21\u578b\uff08\u5982EnCodec\uff09\u8fdb\u884c\u827a\u672f\u548c\u8ba1\u7b97\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u97f3\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2505.10105", "pdf": "https://arxiv.org/pdf/2505.10105", "abs": "https://arxiv.org/abs/2505.10105", "authors": ["Zibin Dong", "Fei Ni", "Yifu Yuan", "Yinchuan Li", "Jianye Hao"], "title": "EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We present EmbodiedMAE, a unified 3D multi-modal representation for robot\nmanipulation. Current approaches suffer from significant domain gaps between\ntraining datasets and robot manipulation tasks, while also lacking model\narchitectures that can effectively incorporate 3D information. To overcome\nthese limitations, we enhance the DROID dataset with high-quality depth maps\nand point clouds, constructing DROID-3D as a valuable supplement for 3D\nembodied vision research. Then we develop EmbodiedMAE, a multi-modal masked\nautoencoder that simultaneously learns representations across RGB, depth, and\npoint cloud modalities through stochastic masking and cross-modal fusion.\nTrained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art\nvision foundation models (VFMs) in both training efficiency and final\nperformance across 70 simulation tasks and 20 real-world robot manipulation\ntasks on two robot platforms. The model exhibits strong scaling behavior with\nsize and promotes effective policy learning from 3D inputs. Experimental\nresults establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for\nembodied AI systems, particularly in precise tabletop manipulation settings\nwhere spatial perception is critical.", "AI": {"tldr": "The paper introduces EmbodiedMAE, a multi-modal masked autoencoder that learns representations across RGB, depth, and point cloud modalities to improve robot manipulation tasks.", "motivation": "Current approaches suffer from significant domain gaps between training datasets and robot manipulation tasks, while also lacking model architectures that can effectively incorporate 3D information.", "method": "Enhance the DROID dataset with high-quality depth maps and point clouds to construct DROID-3D, then develop EmbodiedMAE which is a multi-modal masked autoencoder that simultaneously learns representations across RGB, depth, and point cloud modalities through stochastic masking and cross-modal fusion.", "result": "EmbodiedMAE consistently outperforms state-of-the-art vision foundation models (VFMs) in both training efficiency and final performance across 70 simulation tasks and 20 real-world robot manipulation tasks on two robot platforms.", "conclusion": "EmbodiedMAE is established as a reliable unified 3D multi-modal VFM for embodied AI systems, particularly effective in precise tabletop manipulation settings where spatial perception is critical."}}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472", "abs": "https://arxiv.org/abs/2505.10472", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "categories": ["cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "AI": {"tldr": "Effective communication about breast and cervical cancers is challenging. This study evaluates LLMs' capabilities and limitations in generating cancer-related information. Results show general-purpose LLMs have higher linguistic quality, while medical LLMs have greater accessibility but also more potential harm. There's a duality between domain-specific knowledge and safety in health communications. Intentional model design is needed to improve safety and affectiveness.", "motivation": "Effective communication about breast and cervical cancers remains a persistent health challenge, with significant gaps in public understanding of cancer prevention, screening, and treatment.", "method": "Evaluated five general-purpose and three medical LLMs using a mixed-methods evaluation framework across linguistic quality, safety and trustworthiness, and communication accessibility and affectiveness. Approach utilized quantitative metrics, qualitative expert ratings, and statistical analysis using Welch's ANOVA, Games-Howell, and Hedges' g.", "result": "General-purpose LLMs produced outputs of higher linguistic quality and affectiveness, while medical LLMs demonstrate greater communication accessibility. However, medical LLMs tend to exhibit higher levels of potential harm, toxicity, and bias, reducing their performance in safety and trustworthiness.", "conclusion": "There's a duality between domain-specific knowledge and safety in health communications. Intentional model design is needed to improve safety and affectiveness."}}
{"id": "2505.10134", "pdf": "https://arxiv.org/pdf/2505.10134", "abs": "https://arxiv.org/abs/2505.10134", "authors": ["Guangjin Pan", "Kaixuan Huang", "Hui Chen", "Shunqing Zhang", "Christian H\u00e4ger", "Henk Wymeersch"], "title": "Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "13 pages,16 figures.This work has been submitted to the IEEE for\n  possible publication", "summary": "Accurate and robust localization is a critical enabler for emerging 5G and 6G\napplications, including autonomous driving, extended reality (XR), and smart\nmanufacturing. While data-driven approaches have shown promise, most existing\nmodels require large amounts of labeled data and struggle to generalize across\ndeployment scenarios and wireless configurations. To address these limitations,\nwe propose a foundation-model-based solution tailored for wireless\nlocalization. We first analyze how different self-supervised learning (SSL)\ntasks acquire general-purpose and task-specific semantic features based on\ninformation bottleneck (IB) theory. Building on this foundation, we design a\npretraining methodology for the proposed Large Wireless Localization Model\n(LWLM). Specifically, we propose an SSL framework that jointly optimizes three\ncomplementary objectives: (i) spatial-frequency masked channel modeling\n(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)\nposition-invariant contrastive learning (PICL). These objectives jointly\ncapture the underlying semantics of wireless channel from multiple\nperspectives. We further design lightweight decoders for key downstream tasks,\nincluding time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,\nsingle base station (BS) localization, and multiple BS localization.\nComprehensive experimental results confirm that LWLM consistently surpasses\nboth model-based and supervised learning baselines across all localization\ntasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer\nmodels without pretraining, and exhibits strong generalization under\nlabel-limited fine-tuning and unseen BS configurations, confirming its\npotential as a foundation model for wireless localization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u65e0\u7ebf\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848LWLM\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u9884\u8bad\u7ec3\u5e76\u4f18\u5316\u4e09\u4e2a\u76ee\u6807\uff1a\u7a7a\u95f4\u9891\u7387\u5c4f\u853d\u4fe1\u9053\u5efa\u6a21(SF-MCM)\u3001\u57df\u53d8\u6362\u4e0d\u53d8\u6027(DTI)\u548c\u4f4d\u7f6e\u4e0d\u53d8\u5bf9\u6bd4\u5b66\u4e60(PICL)\uff0c\u5728\u591a\u4e2a\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u5b9a\u4f4d\u5bf9\u4e8e5G\u548c6G\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u548c\u65e0\u7ebf\u914d\u7f6e\u4e2d\u3002", "method": "\u9996\u5148\u5206\u6790\u4e86\u4e0d\u540c\u7684\u81ea\u76d1\u7763\u5b66\u4e60(SSL)\u4efb\u52a1\u5982\u4f55\u6839\u636e\u4fe1\u606f\u74f6\u9888(IB)\u7406\u8bba\u83b7\u53d6\u901a\u7528\u548c\u7279\u5b9a\u4efb\u52a1\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u7136\u540e\u8bbe\u8ba1\u4e86LWLM\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u62ec\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u4e92\u8865\u76ee\u6807\uff1aSF-MCM\u3001DTI\u548cPICL\uff0c\u5e76\u4e3a\u5173\u952e\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLWLM\u5728\u6240\u6709\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4e00\u81f4\u8d85\u8d8a\u57fa\u4e8e\u6a21\u578b\u548c\u76d1\u7763\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8e\u65e0\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u63d0\u5347\u4e8626.0%-87.5%\uff0c\u5e76\u5728\u6807\u6ce8\u6709\u9650\u548c\u672a\u89c1\u8fc7\u7684\u57fa\u7ad9\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LWLM\u4f5c\u4e3a\u4e00\u79cd\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u89e3\u51b3\u65e0\u7ebf\u5b9a\u4f4d\u95ee\u9898\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.10482", "pdf": "https://arxiv.org/pdf/2505.10482", "abs": "https://arxiv.org/abs/2505.10482", "authors": ["Ningyuan Yang", "Jiaxuan Gao", "Feng Gao", "Yi Wu", "Chao Yu"], "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures", "summary": "Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.", "AI": {"tldr": "NCDPO is a new framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy, enabling efficient likelihood evaluation and gradient backpropagation. It matches MLP+PPO in sample efficiency and surpasses existing methods in performance.", "motivation": "Diffusion policies can learn diverse skills but suffer from sub-optimal demonstration data which may lead to catastrophic failures. RL-based fine-tuning is promising but adapting PPO to diffusion models is challenging due to computational intractability.", "method": "NCDPO reformulates Diffusion Policy as a noise-conditioned deterministic policy, making likelihood evaluation and gradient backpropagation tractable through all diffusion timesteps by treating each denoising step as a differentiable transformation.", "result": "NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch and outperforms existing methods in both sample efficiency and final performance across various benchmarks. It is also robust to the number of denoising timesteps.", "conclusion": "NCDPO addresses the challenges in adapting PPO to diffusion models, providing a more efficient and effective method for training diffusion policies."}}
{"id": "2505.10484", "pdf": "https://arxiv.org/pdf/2505.10484", "abs": "https://arxiv.org/abs/2505.10484", "authors": ["Andrea Baisero", "Rupali Bhati", "Shuo Liu", "Aathira Pillai", "Christopher Amato"], "title": "Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Value function decomposition methods for cooperative multi-agent\nreinforcement learning compose joint values from individual per-agent\nutilities, and train them using a joint objective. To ensure that the action\nselection process between individual utilities and joint values remains\nconsistent, it is imperative for the composition to satisfy the\nindividual-global max (IGM) property. Although satisfying IGM itself is\nstraightforward, most existing methods (e.g., VDN, QMIX) have limited\nrepresentation capabilities and are unable to represent the full class of IGM\nvalues, and the one exception that has no such limitation (QPLEX) is\nunnecessarily complex. In this work, we present a simple formulation of the\nfull class of IGM values that naturally leads to the derivation of QFIX, a\nnovel family of value function decomposition models that expand the\nrepresentation capabilities of prior models by means of a thin \"fixing\" layer.\nWe derive multiple variants of QFIX, and implement three variants in two\nwell-known multi-agent frameworks. We perform an empirical evaluation on\nmultiple SMACv2 and Overcooked environments, which confirms that QFIX (i)\nsucceeds in enhancing the performance of prior methods, (ii) learns more stably\nand performs better than its main competitor QPLEX, and (iii) achieves this\nwhile employing the simplest and smallest mixing models.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQFIX\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u7b80\u5355\u7684'fixing'\u5c42\u6269\u5c55\u4e86\u5148\u524d\u6a21\u578b\u7684\u8868\u73b0\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9a8c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u7b80\u6d01\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u503c\u51fd\u6570\u5206\u89e3\u65b9\u6cd5\uff08\u5982VDN\u3001QMIX\uff09\u5c3d\u7ba1\u80fd\u4fdd\u8bc1\u4e2a\u4f53-\u5168\u5c40\u6700\u5927\u503c\uff08IGM\uff09\u5c5e\u6027\uff0c\u4f46\u8868\u73b0\u80fd\u529b\u6709\u9650\uff1b\u800c\u552f\u4e00\u4e0d\u53d7\u6b64\u9650\u5236\u7684\u65b9\u6cd5QPLEX\u53c8\u8fc7\u4e8e\u590d\u6742\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u8868\u793a\u5b8c\u6574\u7684IGM\u503c\u7c7b\uff0c\u53c8\u4fdd\u6301\u7b80\u5355\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684IGM\u503c\u7c7b\u7684\u7b80\u5355\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u7531\u6b64\u63a8\u5bfc\u51faQFIX\u8fd9\u4e00\u65b0\u578b\u503c\u51fd\u6570\u5206\u89e3\u6a21\u578b\u5bb6\u65cf\u3002QFIX\u901a\u8fc7\u4e00\u4e2a\u8584\u7684'fixing'\u5c42\u6269\u5c55\u4e86\u5148\u524d\u6a21\u578b\u7684\u8868\u73b0\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u4ece\u8be5\u65b9\u6cd5\u4e2d\u884d\u751f\u51fa\u4e86\u591a\u4e2aQFIX\u53d8\u4f53\uff0c\u5e76\u5728\u4e24\u4e2a\u77e5\u540d\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u5176\u4e2d\u4e09\u79cd\u53d8\u4f53\u3002", "result": "\u5728SMACv2\u548cOvercooked\u7b49\u591a\u4e2a\u73af\u5883\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff1a(i) QFIX\u6210\u529f\u63d0\u5347\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\uff1b(ii) \u4e0e\u4e3b\u8981\u7ade\u4e89\u5bf9\u624bQPLEX\u76f8\u6bd4\uff0cQFIX\u5b66\u4e60\u66f4\u7a33\u5b9a\u4e14\u8868\u73b0\u66f4\u597d\uff1b(iii) QFIX\u5728\u91c7\u7528\u6700\u7b80\u5355\u548c\u6700\u5c0f\u7684\u6df7\u5408\u6a21\u578b\u65f6\u8fbe\u5230\u4e86\u4e0a\u8ff0\u6548\u679c\u3002", "conclusion": "QFIX\u662f\u4e00\u79cd\u6709\u6548\u7684\u503c\u51fd\u6570\u5206\u89e3\u6a21\u578b\uff0c\u5b83\u4e0d\u4ec5\u6269\u5c55\u4e86\u5148\u524d\u6a21\u578b\u7684\u8868\u73b0\u80fd\u529b\uff0c\u800c\u4e14\u4ee5\u66f4\u7b80\u5355\u548c\u66f4\u5c0f\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.10183", "pdf": "https://arxiv.org/pdf/2505.10183", "abs": "https://arxiv.org/abs/2505.10183", "authors": ["Jieke Lin", "Wanyu Wang", "Longxiang Yin", "Yinhe Han"], "title": "KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems", "categories": ["cs.DC", "cs.AI"], "comment": "9 pages, 4 figures. Jieke Lin and Wanyu Wang contributed equally to\n  this work", "summary": "Embodied Artificial Intelligence (AI) systems, such as autonomous robots and\nintelligent vehicles, are increasingly reliant on diverse heterogeneous\naccelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing\nand energy-efficiency demands. However, the proliferation of vendor-specific\nproprietary communication libraries creates significant interoperability\nbarriers, hindering seamless collaboration between different accelerator types\nand leading to suboptimal resource utilization and performance bottlenecks in\ndistributed AI workloads. This paper introduces KAITIAN, a novel distributed\ncommunication framework designed to bridge this gap. KAITIAN provides a unified\nabstraction layer that intelligently integrates vendor-optimized communication\nlibraries for intra-group efficiency with general-purpose communication\nprotocols for inter-group interoperability. Crucially, it incorporates a\nload-adaptive scheduling mechanism that dynamically balances computational\ntasks across heterogeneous devices based on their real-time performance\ncharacteristics. Implemented as an extension to PyTorch and rigorously\nevaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN\ndemonstrates significant improvements in resource utilization and scalability\nfor distributed training tasks. Experimental results show that KAITIAN can\naccelerate training time by up to 42% compared to baseline homogeneous systems,\nwhile incurring minimal communication overhead (2.8--4.3%) and maintaining\nmodel accuracy. KAITIAN paves the way for more flexible and powerful\nheterogeneous computing in complex embodied AI applications.", "AI": {"tldr": "KAITIAN is a new distributed communication framework that improves resource utilization and scalability in heterogeneous AI systems.", "motivation": "Embodied AI systems require diverse accelerators to meet real-time and energy-efficiency demands, but vendor-specific libraries create interoperability barriers hindering performance.", "method": "KAITIAN provides a unified abstraction layer integrating vendor-optimized libraries for intra-group efficiency and general-purpose protocols for inter-group interoperability, with a load-adaptive scheduling mechanism.", "result": "KAITIAN accelerates training time by up to 42% compared to baseline homogeneous systems with minimal communication overhead (2.8--4.3%) and maintained model accuracy.", "conclusion": "KAITIAN enhances flexible and powerful heterogeneous computing for complex embodied AI applications."}}
{"id": "2505.10515", "pdf": "https://arxiv.org/pdf/2505.10515", "abs": "https://arxiv.org/abs/2505.10515", "authors": ["Seongun Kim", "Sol A Kim", "Geonhyeong Kim", "Enver Menadjiev", "Chanwoo Lee", "Seongwook Chung", "Nari Kim", "Jaesik Choi"], "title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.", "AI": {"tldr": "To overcome the limitations of current XAI frameworks, PnPXAI is introduced as a universal framework that supports diverse data modalities and neural network models in a Plug-and-Play manner.", "motivation": "Current XAI frameworks face challenges such as limited flexibility to diverse model architectures and data modalities, restricted number of supported XAI methods, and sub-optimal recommendations of explanations.", "method": "PnPXAI automatically detects model architectures, recommends applicable explanation methods, and optimizes hyperparameters for optimal explanations.", "result": "The framework's effectiveness is validated through user surveys and its versatility is showcased across various domains, including medicine and finance.", "conclusion": "PnPXAI addresses the limitations of existing XAI frameworks and enhances the adoption of XAI technology in real-world applications."}}
{"id": "2505.10191", "pdf": "https://arxiv.org/pdf/2505.10191", "abs": "https://arxiv.org/abs/2505.10191", "authors": ["Qingyu Zheng", "Qi Shao", "Guijun Han", "Wei Li", "Hong Li", "Xuan Wang"], "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting", "categories": ["physics.ao-ph", "cs.AI", "cs.LG", "nlin.CD"], "comment": "22 pages, 6 figures", "summary": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the\nocean, and their impact on the energy cascade of the global ocean cannot be\nignored. Eddy-resolving ocean forecasting is providing more reliable protection\nfor fisheries and navigational safety, but also presents significant scientific\nchallenges and high computational costs for traditional numerical models.\nArtificial intelligence (AI)-based weather and ocean forecasting systems are\nbecoming powerful tools that balance forecast performance with computational\nefficiency. However, the complex multiscale features in the ocean dynamical\nsystem make AI models still face many challenges in mesoscale eddy forecasting\n(especially regional modelling). Here, we develop LanTu, a regional\neddy-resolving ocean forecasting system based on dynamics-enhanced deep\nlearning. We incorporate cross-scale interactions into LanTu and construct\nmultiscale physical constraint for optimising LanTu guided by knowledge of eddy\ndynamics in order to improve the forecasting skill of LanTu for mesoscale\nevolution. The results show that LanTu outperforms the existing advanced\noperational numerical ocean forecasting system (NOFS) and AI-based ocean\nforecasting system (AI-OFS) in temperature, salinity, sea level anomaly and\ncurrent prediction, with a lead time of more than 10 days. Our study highlights\nthat dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for\neddy-resolving ocean forecasting.", "AI": {"tldr": "LanTu, a dynamics-enhanced deep learning regional eddy-resolving ocean forecasting system incorporating cross-scale interactions and multiscale physical constraints, outperforms existing operational numerical and AI-based systems in predicting temperature, salinity, sea level anomaly, and currents with a lead time of over 10 days.", "motivation": "Eddy-resolving ocean forecasting is crucial for fisheries and navigational safety but presents scientific challenges and high computational costs. AI-based systems offer a balance between forecast performance and efficiency, yet still face challenges in mesoscale eddy forecasting.", "method": "Developed LanTu, a regional eddy-resolving ocean forecasting system based on dynamics-enhanced deep learning. Incorporated cross-scale interactions and constructed multiscale physical constraints guided by eddy dynamics knowledge to improve mesoscale evolution forecasting skill.", "result": "LanTu outperforms the existing advanced operational numerical ocean forecasting system (NOFS) and AI-based ocean forecasting system (AI-OFS) in temperature, salinity, sea level anomaly, and current prediction with a lead time of more than 10 days.", "conclusion": "Dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for eddy-resolving ocean forecasting."}}
{"id": "2505.10545", "pdf": "https://arxiv.org/pdf/2505.10545", "abs": "https://arxiv.org/abs/2505.10545", "authors": ["Amira Alakhdar", "Barnabas Poczos", "Newell Washburn"], "title": "Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design", "categories": ["cs.LG"], "comment": null, "summary": "Developing bioactive molecules remains a central, time- and cost-heavy\nchallenge in drug discovery, particularly for novel targets lacking structural\nor functional data. Pharmacophore modeling presents an alternative for\ncapturing the key features required for molecular bioactivity against a\nbiological target. In this work, we present PharmaDiff, a\npharmacophore-conditioned diffusion model for 3D molecular generation.\nPharmaDiff employs a transformer-based architecture to integrate an atom-based\nrepresentation of the 3D pharmacophore into the generative process, enabling\nthe precise generation of 3D molecular graphs that align with predefined\npharmacophore hypotheses. Through comprehensive testing, PharmaDiff\ndemonstrates superior performance in matching 3D pharmacophore constraints\ncompared to ligand-based drug design methods. Additionally, it achieves higher\ndocking scores across a range of proteins in structure-based drug design,\nwithout the need for target protein structures. By integrating pharmacophore\nmodeling with 3D generative techniques, PharmaDiff offers a powerful and\nflexible framework for rational drug design.", "AI": {"tldr": "PharmaDiff is a pharmacophore-conditioned diffusion model for 3D molecular generation that integrates an atom-based representation of the 3D pharmacophore into the generative process, offering a powerful and flexible framework for rational drug design.", "motivation": "Developing bioactive molecules is a central challenge in drug discovery, especially for novel targets lacking structural or functional data.", "method": "PharmaDiff employs a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process, enabling the precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses.", "result": "PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods and achieves higher docking scores across a range of proteins in structure-based drug design, without the need for target protein structures.", "conclusion": "By integrating pharmacophore modeling with 3D generative techniques, PharmaDiff offers a powerful and flexible framework for rational drug design."}}
{"id": "2505.10197", "pdf": "https://arxiv.org/pdf/2505.10197", "abs": "https://arxiv.org/abs/2505.10197", "authors": ["Anjali de Silva", "Gang Chen", "Hui Ma", "Seyed Mohammad Nekooei", "Xingquan Zuo"], "title": "Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion", "categories": ["cs.SI", "cs.AI"], "comment": "This paper has been accepted by IJCAI (International Joint Conference\n  on Artificial Intelligence) 2025", "summary": "Community detection, a vital technology for real-world applications, uncovers\ncohesive node groups (communities) by leveraging both topological and attribute\nsimilarities in social networks. However, existing Graph Convolutional Networks\n(GCNs) trained to maximize modularity often converge to suboptimal solutions.\nAdditionally, directly using human-labeled communities for training can\nundermine topological cohesiveness by grouping disconnected nodes based solely\non node attributes. We address these issues by proposing a novel Topological\nand Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com\nintroduces a novel loss function that exploits the highly effective and\nscalable Leiden algorithm to detect community structures with global optimal\nmodularity. Leiden is further utilized to refine human-labeled communities to\nensure connectivity within each community, enabling TAS-Com to detect community\nstructures with desirable trade-offs between modularity and compliance with\nhuman labels. Experimental results on multiple benchmark networks confirm that\nTAS-Com can significantly outperform several state-of-the-art algorithms.", "AI": {"tldr": "\u793e\u533a\u68c0\u6d4b\u6280\u672f\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u62d3\u6251\u548c\u5c5e\u6027\u76f8\u4f3c\u6027\u6765\u53d1\u73b0\u8fde\u8d2f\u7684\u8282\u70b9\u7ec4\uff08\u793e\u533a\uff09\u3002\u73b0\u6709\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u5728\u6700\u5927\u5316\u6a21\u5757\u5ea6\u65f6\u901a\u5e38\u6536\u655b\u5230\u6b21\u4f18\u89e3\u3002\u6b64\u5916\uff0c\u76f4\u63a5\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u7684\u793e\u533a\u8fdb\u884c\u8bad\u7ec3\u53ef\u80fd\u4f1a\u901a\u8fc7\u4ec5\u57fa\u4e8e\u8282\u70b9\u5c5e\u6027\u5bf9\u4e0d\u76f8\u8fde\u7684\u8282\u70b9\u8fdb\u884c\u5206\u7ec4\u800c\u524a\u5f31\u62d3\u6251\u8fde\u8d2f\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e\u62d3\u6251\u548c\u5c5e\u6027\u76f8\u4f3c\u6027\u7684\u793e\u533a\u68c0\u6d4b\uff08TAS-Com\uff09\u65b9\u6cd5\u3002TAS-Com\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684Leiden\u7b97\u6cd5\u6765\u68c0\u6d4b\u5177\u6709\u5168\u5c40\u6700\u4f18\u6a21\u5757\u5ea6\u7684\u793e\u533a\u7ed3\u6784\u3002\u8fdb\u4e00\u6b65\u5229\u7528Leiden\u7b97\u6cd5\u4f18\u5316\u4eba\u5de5\u6807\u6ce8\u7684\u793e\u533a\uff0c\u4ee5\u786e\u4fdd\u6bcf\u4e2a\u793e\u533a\u5185\u7684\u8fde\u901a\u6027\uff0c\u4f7fTAS-Com\u80fd\u591f\u68c0\u6d4b\u5230\u5728\u6a21\u5757\u5ea6\u548c\u7b26\u5408\u4eba\u5de5\u6807\u7b7e\u4e4b\u95f4\u5177\u6709\u826f\u597d\u6743\u8861\u7684\u793e\u533a\u7ed3\u6784\u3002\u591a\u4e2a\u57fa\u51c6\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0cTAS-Com\u53ef\u4ee5\u663e\u8457\u4f18\u4e8e\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u5728\u6700\u5927\u5316\u6a21\u5757\u5ea6\u65f6\u901a\u5e38\u6536\u655b\u5230\u6b21\u4f18\u89e3\uff0c\u5e76\u4e14\u76f4\u63a5\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u7684\u793e\u533a\u8fdb\u884c\u8bad\u7ec3\u53ef\u80fd\u4f1a\u901a\u8fc7\u4ec5\u57fa\u4e8e\u8282\u70b9\u5c5e\u6027\u5bf9\u4e0d\u76f8\u8fde\u7684\u8282\u70b9\u8fdb\u884c\u5206\u7ec4\u800c\u524a\u5f31\u62d3\u6251\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u62d3\u6251\u548c\u5c5e\u6027\u76f8\u4f3c\u6027\u7684\u793e\u533a\u68c0\u6d4b\uff08TAS-Com\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684Leiden\u7b97\u6cd5\u6765\u68c0\u6d4b\u5177\u6709\u5168\u5c40\u6700\u4f18\u6a21\u5757\u5ea6\u7684\u793e\u533a\u7ed3\u6784\uff0c\u5e76\u8fdb\u4e00\u6b65\u5229\u7528Leiden\u7b97\u6cd5\u4f18\u5316\u4eba\u5de5\u6807\u6ce8\u7684\u793e\u533a\uff0c\u4ee5\u786e\u4fdd\u6bcf\u4e2a\u793e\u533a\u5185\u7684\u8fde\u901a\u6027\u3002", "result": "\u591a\u4e2a\u57fa\u51c6\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0cTAS-Com\u53ef\u4ee5\u663e\u8457\u4f18\u4e8e\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002", "conclusion": "TAS-Com\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709GCNs\u6536\u655b\u5230\u6b21\u4f18\u89e3\u4ee5\u53ca\u4eba\u5de5\u6807\u6ce8\u793e\u533a\u53ef\u80fd\u524a\u5f31\u62d3\u6251\u8fde\u8d2f\u6027\u7684\u95ee\u9898\uff0c\u5728\u793e\u533a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.10556", "pdf": "https://arxiv.org/pdf/2505.10556", "abs": "https://arxiv.org/abs/2505.10556", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "categories": ["cs.LG", "physics.ao-ph"], "comment": "Kermani and Naderi share first authorship. 20 pages, 6 figures and 1\n  table", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u5c06\u53ef\u7a7f\u6234\u5065\u8eab\u8bbe\u5907\u7684\u751f\u7406\u6570\u636e\u4e0e\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u76f8\u7ed3\u5408\uff0c\u5229\u7528AI\u6a21\u578b\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u7684\u5065\u5eb7\u53cd\u5e94\u3002", "motivation": "\u7a7a\u6c14\u6c61\u67d3\u548c\u6c14\u5019\u53d8\u5316\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u800c\u4e2a\u4eba\u4f20\u611f\u6280\u672f\u548cAI\u6280\u672f\u7684\u8fdb\u6b65\u4e3a\u76d1\u6d4b\u548c\u9884\u6d4b\u4e2a\u4eba\u5065\u5eb7\u7ed3\u679c\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u6574\u5408\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u751f\u7406\u6570\u636e\u548c\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u6570\u636e\uff0c\u4f7f\u7528\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3AI\u6a21\u578b\uff0c\u4ee5\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u66b4\u9732\u7684\u5065\u5eb7\u53cd\u5e94\uff0c\u5e76\u5e94\u7528\u8fc1\u79fb\u5b66\u4e60\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "AI\u6a21\u578b\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u65f6\u95f4\u4f9d\u8d56\u7684\u5065\u5eb7\u4fe1\u53f7\uff0c\u5e76\u6355\u6349\u5bf9\u6c61\u67d3\u7684\u975e\u7ebf\u6027\u53cd\u5e94\uff0c\u8fc1\u79fb\u5b66\u4e60\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u4e2a\u6027\u5316\u533b\u7597\u9886\u57df\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u7684\u5065\u5eb7\u53cd\u5e94\u65b9\u9762\u3002"}}
{"id": "2505.10201", "pdf": "https://arxiv.org/pdf/2505.10201", "abs": "https://arxiv.org/abs/2505.10201", "authors": ["Victor Lagerkvist", "Mohamed Maizia", "Johannes Schmidt"], "title": "A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds", "categories": ["cs.CC", "cs.AI", "F.2.2"], "comment": null, "summary": "The Boolean satisfiability problem (SAT) is a well-known example of monotonic\nreasoning, of intense practical interest due to fast solvers, complemented by\nrigorous fine-grained complexity results. However, for non-monotonic reasoning,\ne.g., abductive reasoning, comparably little is known outside classic\ncomplexity theory. In this paper we take a first step of bridging the gap\nbetween monotonic and non-monotonic reasoning by analyzing the complexity of\nintractable abduction problems under the seemingly overlooked but natural\nparameter n: the number of variables in the knowledge base. We obtain several\npositive results for $\\Sigma^P_2$- as well as NP- and coNP-complete fragments,\nwhich implies the first example of beating exhaustive search for a\n$\\Sigma^P_2$-complete problem (to the best of our knowledge). We complement\nthis with lower bounds and for many fragments rule out improvements under the\n(strong) exponential-time hypothesis.", "AI": {"tldr": "The paper explores the complexity of non-monotonic reasoning, specifically abductive reasoning, by analyzing intractable abduction problems with respect to the number of variables in the knowledge base. It presents positive results for certain problem fragments and shows improvements over exhaustive search for a $\\Sigma^P_2$-complete problem.", "motivation": "To bridge the gap between monotonic and non-monotonic reasoning, especially focusing on abductive reasoning which lacks detailed complexity analysis beyond classic theory.", "method": "Analyze the complexity of intractable abduction problems using the parameter n (number of variables in the knowledge base) and provide both positive results and lower bounds for various problem fragments.", "result": "Achieved several positive results for different complexity classes including $\\Sigma^P_2$, NP, and coNP-complete fragments. Demonstrated an improvement over exhaustive search for a $\\Sigma^P_2$-complete problem. Provided lower bounds under the strong exponential-time hypothesis.", "conclusion": "This work marks a significant step in understanding the complexity of non-monotonic reasoning, providing both advancements and limitations in solving abductive reasoning problems."}}
{"id": "2505.10559", "pdf": "https://arxiv.org/pdf/2505.10559", "abs": "https://arxiv.org/abs/2505.10559", "authors": ["Ziming Liu", "Yizhou Liu", "Jeff Gore", "Max Tegmark"], "title": "Neural Thermodynamic Laws for Large Language Model Training", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "comment": "18 pages, 10 figures", "summary": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.", "AI": {"tldr": "The paper introduces Neural Thermodynamic Laws (NTL), a framework providing insights into LLM training dynamics, showing thermodynamic quantities and principles emerge under certain assumptions and offering guidelines for learning rate schedules.", "motivation": "To explore the underlying laws of large language models beyond neural scaling laws.", "method": "Demonstrate that key thermodynamic quantities and classical thermodynamic principles naturally emerge under river-valley loss landscape assumptions, and provide intuitive guidelines for designing learning rate schedules from this scientific perspective.", "result": "Neural Thermodynamic Laws framework successfully connects thermodynamic concepts with LLM training dynamics, providing theoretical and practical contributions.", "conclusion": "NTL offers fresh insights into LLM training dynamics by linking thermodynamic principles with LLMs, contributing both theoretically and practically."}}
{"id": "2505.10212", "pdf": "https://arxiv.org/pdf/2505.10212", "abs": "https://arxiv.org/abs/2505.10212", "authors": ["Dario Di Palma", "Felice Antonio Merra", "Maurizio Sfilio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector", "AI": {"tldr": "Large Language Models (LLMs) may have memorized public recommendation datasets, which affects their generalizability and fairness. This study investigates the extent of memorization in LLMs using MovieLens-1M dataset.", "motivation": "To verify whether Large Language Models (LLMs) have memorized public recommendation datasets as part of their training data, which is crucial for ensuring the generalizability of research findings and preventing bias amplification.", "method": "Define dataset memorization as the ability to retrieve item attributes, user profiles, and user-item interactions by prompting LLMs. Analyze two model families (GPT and Llama) across multiple sizes on the MovieLens-1M dataset. Examine the impact of memorization on recommendation performance and whether it varies across model families and sizes.", "result": "All examined models exhibit some degree of memorization of the MovieLens-1M dataset. Recommendation performance is related to the extent of memorization.", "conclusion": "LLMs have memorized parts of the MovieLens-1M dataset, impacting recommendation performance. The code for this study has been made publicly available."}}
{"id": "2505.09633", "pdf": "https://arxiv.org/pdf/2505.09633", "abs": "https://arxiv.org/abs/2505.09633", "authors": ["Nick Sunday"], "title": "Detecting Musical Deepfakes", "categories": ["cs.SD", "cs.LG"], "comment": "Submitted as part of coursework at UT Austin. Accompanying code\n  available at: https://github.com/nicksunday/deepfake-music-detector", "summary": "The proliferation of Text-to-Music (TTM) platforms has democratized music\ncreation, enabling users to effortlessly generate high-quality compositions.\nHowever, this innovation also presents new challenges to musicians and the\nbroader music industry. This study investigates the detection of AI-generated\nsongs using the FakeMusicCaps dataset by classifying audio as either deepfake\nor human. To simulate real-world adversarial conditions, tempo stretching and\npitch shifting were applied to the dataset. Mel spectrograms were generated\nfrom the modified audio, then used to train and evaluate a convolutional neural\nnetwork. In addition to presenting technical results, this work explores the\nethical and societal implications of TTM platforms, arguing that carefully\ndesigned detection systems are essential to both protecting artists and\nunlocking the positive potential of generative AI in music.", "AI": {"tldr": "This study explores the detection of AI-generated songs using the FakeMusicCaps dataset and mel spectrograms with a CNN model, while also discussing ethical and societal implications of TTM platforms.", "motivation": "The rise of Text-to-Music platforms has made music creation more accessible but also poses challenges to musicians and the music industry, necessitating research into detecting AI-generated music.", "method": "Using the FakeMusicCaps dataset, tempo stretching and pitch shifting were applied to simulate adversarial conditions. Mel spectrograms were generated from the modified audio for training and evaluating a convolutional neural network.", "result": "Technical results on detecting AI-generated songs are presented, showing the effectiveness of the approach under adversarial conditions.", "conclusion": "Carefully designed detection systems are crucial for protecting artists and harnessing the positive potential of generative AI in music."}}
{"id": "2505.09643", "pdf": "https://arxiv.org/pdf/2505.09643", "abs": "https://arxiv.org/abs/2505.09643", "authors": ["Zhixuan Wang"], "title": "A Computational Approach to Epilepsy Treatment: An AI-optimized Global Natural Product Prescription System", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "Epilepsy is a prevalent neurological disease with millions of patients\nworldwide. Many patients have turned to alternative medicine due to the limited\nefficacy and side effects of conventional antiepileptic drugs. In this study,\nwe developed a computational approach to optimize herbal epilepsy treatment\nthrough AI-driven analysis of global natural products and statistically\nvalidated randomized controlled trials (RCTs). Our intelligent prescription\nsystem combines machine learning (ML) algorithms for herb-efficacy\ncharacterization, Bayesian optimization for personalized dosing, and\nmeta-analysis of RCTs for evidence-based recommendations. The system analyzed\n1,872 natural compounds from traditional Chinese medicine (TCM), Ayurveda, and\nethnopharmacological databases, integrating their bioactive properties with\nclinical outcomes from 48 RCTs covering 48 epilepsy conditions (n=5,216). Using\nLASSO regression and SHAP value analysis, we identified 17 high-efficacy herbs\n(e.g., Gastrodia elata [using \\'e for accented characters], Withania\nsomnifera), showing significant seizure reduction (p$<$0.01, Cohen's d=0.89)\nwith statistical significance confirmed by multiple testing (p$<$0.001). A\nrandomized double-blind validation trial (n=120) demonstrated 28.5\\% greater\nseizure frequency reduction with AI-optimized herbal prescriptions compared to\nconventional protocols (95\\% CI: 18.7-37.3\\%, p=0.003).", "AI": {"tldr": "Epilepsy patients often use alternative medicine due to the limitations of conventional drugs. This study developed an AI-driven system that analyzes natural compounds and RCTs, identifying 17 high-efficacy herbs for epilepsy treatment with significant seizure reduction confirmed in a validation trial.", "motivation": "To optimize herbal epilepsy treatment by leveraging AI-driven analysis of global natural products and statistically validated RCTs, addressing the limitations of conventional antiepileptic drugs.", "method": "The intelligent prescription system integrates machine learning algorithms for herb-efficacy characterization, Bayesian optimization for personalized dosing, and meta-analysis of RCTs for evidence-based recommendations. It analyzed 1,872 natural compounds and clinical outcomes from 48 RCTs covering 48 epilepsy conditions.", "result": "Identified 17 high-efficacy herbs showing significant seizure reduction (p<0.01, Cohen's d=0.89) with statistical significance confirmed by multiple testing (p<0.001). A validation trial demonstrated 28.5% greater seizure frequency reduction compared to conventional protocols.", "conclusion": "AI-optimized herbal prescriptions offer a promising alternative for epilepsy treatment, significantly reducing seizure frequency."}}
{"id": "2505.09647", "pdf": "https://arxiv.org/pdf/2505.09647", "abs": "https://arxiv.org/abs/2505.09647", "authors": ["Leighton Pate Barnes", "Stephen Cameron", "Benjamin Howard"], "title": "On Unbiased Low-Rank Approximation with Minimum Distortion", "categories": ["cs.DS", "cs.IT", "cs.LG", "math.IT", "math.PR", "math.ST", "stat.TH"], "comment": null, "summary": "We describe an algorithm for sampling a low-rank random matrix $Q$ that best\napproximates a fixed target matrix $P\\in\\mathbb{C}^{n\\times m}$ in the\nfollowing sense: $Q$ is unbiased, i.e., $\\mathbb{E}[Q] = P$;\n$\\mathsf{rank}(Q)\\leq r$; and $Q$ minimizes the expected Frobenius norm error\n$\\mathbb{E}\\|P-Q\\|_F^2$. Our algorithm mirrors the solution to the efficient\nunbiased sparsification problem for vectors, except applied to the singular\ncomponents of the matrix $P$. Optimality is proven by showing that our\nalgorithm matches the error from an existing lower bound.", "AI": {"tldr": "The paper presents an algorithm for sampling a low-rank random matrix that optimally approximates a fixed target matrix in terms of expected Frobenius norm error.", "motivation": "To develop an optimal method for approximating a fixed target matrix with a low-rank random matrix, ensuring unbiasedness and minimizing the expected Frobenius norm error.", "method": "The algorithm mirrors the solution to the efficient unbiased sparsification problem for vectors but is applied to the singular components of the matrix P. It ensures that Q is unbiased, has a rank less than or equal to r, and minimizes the expected Frobenius norm error.", "result": "The algorithm achieves optimality by matching the error from an existing lower bound.", "conclusion": "The presented algorithm provides an optimal way to sample a low-rank random matrix that best approximates a given target matrix."}}
{"id": "2505.10273", "pdf": "https://arxiv.org/pdf/2505.10273", "abs": "https://arxiv.org/abs/2505.10273", "authors": ["Hexu Li", "Konstantinos Kalogiannis", "Ahmed Mohamed Hussain", "Panos Papadimitratos"], "title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": "Author's version; Accepted for presentation at the ACM Workshop on\n  Wireless Security and Machine Learning (WiseML 2025)", "summary": "Vehicle platooning, with vehicles traveling in close formation coordinated\nthrough Vehicle-to-Everything (V2X) communications, offers significant benefits\nin fuel efficiency and road utilization. However, it is vulnerable to\nsophisticated falsification attacks by authenticated insiders that can\ndestabilize the formation and potentially cause catastrophic collisions. This\npaper addresses this challenge: misbehavior detection in vehicle platooning\nsystems. We present AttentionGuard, a transformer-based framework for\nmisbehavior detection that leverages the self-attention mechanism to identify\nanomalous patterns in mobility data. Our proposal employs a multi-head\ntransformer-encoder to process sequential kinematic information, enabling\neffective differentiation between normal mobility patterns and falsification\nattacks across diverse platooning scenarios, including steady-state\n(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an\nextensive simulation dataset featuring various attack vectors (constant,\ngradual, and combined falsifications) and operational parameters (controller\ntypes, vehicle speeds, and attacker positions). Experimental results\ndemonstrate that AttentionGuard achieves up to 0.95 F1-score in attack\ndetection, with robust performance maintained during complex maneuvers.\nNotably, our system performs effectively with minimal latency (100ms decision\nintervals), making it suitable for real-time transportation safety\napplications. Comparative analysis reveals superior detection capabilities and\nestablishes the transformer-encoder as a promising approach for securing\nCooperative Intelligent Transport Systems (C-ITS) against sophisticated insider\nthreats.", "AI": {"tldr": "Vehicle platooning via V2X communication improves fuel efficiency and road use but is susceptible to insider attacks. This paper introduces AttentionGuard, a transformer-based framework using self-attention to detect anomalies in mobility data, achieving up to 0.95 F1-score with minimal latency.", "motivation": "Vehicle platooning systems are vulnerable to falsification attacks by authenticated insiders, which can destabilize formations and cause collisions. There is a need for effective misbehavior detection mechanisms to ensure safety and stability.", "method": "AttentionGuard leverages a multi-head transformer-encoder and the self-attention mechanism to process sequential kinematic information from vehicles. It differentiates between normal patterns and falsification attacks across various platooning scenarios including steady-state operation, join, and exit maneuvers.", "result": "AttentionGuard achieves an F1-score of up to 0.95 in detecting attacks within diverse attack vectors and operational parameters. The system maintains robust performance during complex maneuvers and operates with minimal latency (100ms decision intervals).", "conclusion": "AttentionGuard demonstrates superior detection capabilities for securing Cooperative Intelligent Transport Systems (C-ITS) against insider threats, making it a promising solution for real-time transportation safety applications."}}
{"id": "2505.10300", "pdf": "https://arxiv.org/pdf/2505.10300", "abs": "https://arxiv.org/abs/2505.10300", "authors": ["Muzhe Wu", "Yanzhi Zhao", "Shuyi Han", "Michael Xieyang Liu", "Hong Shen"], "title": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Responsible AI (RAI) efforts increasingly emphasize the importance of\naddressing potential harms early in the AI development lifecycle through\nsocial-technical lenses. However, in cross-functional industry teams, this work\nis often stalled by a persistent knowledge handoff challenge: the difficulty of\ntransferring high-level, early-stage technical design rationales from technical\nexperts to non-technical or user-facing roles for ethical evaluation and harm\nidentification. Through literature review and a co-design study with 8\npractitioners, we unpack how this challenge manifests -- technical design\nchoices are rarely handed off in ways that support meaningful engagement by\nnon-technical roles; collaborative workflows lack shared, visual structures to\nsupport mutual understanding; and non-technical practitioners are left without\nscaffolds for systematic harm evaluation. Existing tools like JIRA or Google\nDocs, while useful for product tracking, are ill-suited for supporting joint\nharm identification across roles, often requiring significant extra effort to\nalign understanding. To address this, we developed AI LEGO, a web-based\nprototype that supports cross-functional AI practitioners in effectively\nfacilitating knowledge handoff and identifying harmful design choices in the\nearly design stages. Technical roles use interactive blocks to draft\ndevelopment plans, while non-technical roles engage with those blocks through\nstage-specific checklists and LLM-driven persona simulations to surface\npotential harms. In a study with 18 cross-functional practitioners, AI LEGO\nincreased the volume and likelihood of harms identified compared to baseline\nworksheets. Participants found that its modular structure and persona prompts\nmade harm identification more accessible, fostering clearer and more\ncollaborative RAI practices in early design.", "AI": {"tldr": "To address the challenge of transferring technical design rationales for ethical evaluation, the authors developed AI LEGO, a web-based prototype that facilitates knowledge handoff and helps identify harmful design choices in early AI development stages. It uses interactive blocks and persona simulations, leading to more effective harm identification compared to traditional methods.", "motivation": "The motivation is to solve the persistent knowledge handoff challenge in cross-functional industry teams working on Responsible AI (RAI). This involves transferring high-level technical design rationales from technical experts to non-technical roles for ethical evaluation and harm identification early in the AI development lifecycle.", "method": "The method involved literature review and a co-design study with 8 practitioners to understand the current challenges. Based on insights, they developed AI LEGO, a web-based prototype. Technical roles use interactive blocks to draft development plans, while non-technical roles engage through checklists and LLM-driven persona simulations to identify potential harms.", "result": "In a study with 18 cross-functional practitioners, AI LEGO increased the volume and likelihood of harms identified compared to baseline worksheets. Participants found its modular structure and persona prompts made harm identification more accessible and fostered clearer RAI practices.", "conclusion": "AI LEGO effectively supports cross-functional AI practitioners in facilitating knowledge handoff and identifying harmful design choices early in the development process, leading to more collaborative RAI practices."}}
{"id": "2505.09660", "pdf": "https://arxiv.org/pdf/2505.09660", "abs": "https://arxiv.org/abs/2505.09660", "authors": ["Saptarshi Saha", "Dhruv Vansraj Rathore", "Soumadeep Saha", "Utpal Garain", "David Doermann"], "title": "On Measuring Intrinsic Causal Attributions in Deep Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Quantifying the causal influence of input features within neural networks has\nbecome a topic of increasing interest. Existing approaches typically assess\ndirect, indirect, and total causal effects. This work treats NNs as structural\ncausal models (SCMs) and extends our focus to include intrinsic causal\ncontributions (ICC). We propose an identifiable generative post-hoc framework\nfor quantifying ICC. We also draw a relationship between ICC and Sobol'\nindices. Our experiments on synthetic and real-world datasets demonstrate that\nICC generates more intuitive and reliable explanations compared to existing\nglobal explanation techniques.", "AI": {"tldr": "The paper explores intrinsic causal contributions (ICC) in neural networks using a generative post-hoc framework, showing ICC provides more intuitive and reliable explanations than existing methods.", "motivation": "Quantifying the causal influence of input features within neural networks has become a topic of increasing interest.", "method": "Treat NNs as structural causal models (SCMs) and extend focus to include intrinsic causal contributions (ICC), proposing an identifiable generative post-hoc framework for quantifying ICC and drawing a relationship between ICC and Sobol' indices.", "result": "Experiments on synthetic and real-world datasets demonstrate that ICC generates more intuitive and reliable explanations compared to existing global explanation techniques.", "conclusion": "ICC offers a new perspective in understanding causal influences within neural networks."}}
{"id": "2505.10315", "pdf": "https://arxiv.org/pdf/2505.10315", "abs": "https://arxiv.org/abs/2505.10315", "authors": ["Yang Li", "Xinyu Zhou", "Yitong Wang", "Liangxin Qian", "Jun Zhao"], "title": "Private Transformer Inference in MLaaS: A Survey", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.", "AI": {"tldr": "Transformer models, despite their success in AI applications, pose privacy risks when deployed in MLaaS due to centralized data processing. Private Transformer Inference (PTI) mitigates these risks through cryptographic techniques like secure multi-party computation and homomorphic encryption, allowing for private inference. This paper reviews PTI advancements, provides a taxonomy, and evaluates solutions focusing on balancing resource efficiency with privacy.", "motivation": "The motivation of this paper stems from the increasing privacy concerns associated with deploying transformer models in MLaaS environments, where sensitive user data is processed centrally.", "method": "The paper reviews recent advancements in Private Transformer Inference (PTI), which uses cryptographic techniques such as secure multi-party computation and homomorphic encryption to enable private inference. It also introduces a structured taxonomy and evaluation framework to assess PTI solutions.", "result": "The paper highlights state-of-the-art PTI solutions and challenges, providing insights into balancing resource efficiency with privacy.", "conclusion": "Private Transformer Inference offers a promising approach to address privacy concerns in transformer model deployments, and the introduced taxonomy and evaluation framework will aid in advancing PTI solutions."}}
{"id": "2505.09706", "pdf": "https://arxiv.org/pdf/2505.09706", "abs": "https://arxiv.org/abs/2505.09706", "authors": ["Hugo Gobato Souto", "Francisco Louzada Neto"], "title": "Forests for Differences: Robust Causal Inference Beyond Parametric DiD", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces the Difference-in-Differences Bayesian Causal Forest\n(DiD-BCF), a novel non-parametric model addressing key challenges in DiD\nestimation, such as staggered adoption and heterogeneous treatment effects.\nDiD-BCF provides a unified framework for estimating Average (ATE),\nGroup-Average (GATE), and Conditional Average Treatment Effects (CATE). A core\ninnovation, its Parallel Trends Assumption (PTA)-based reparameterization,\nenhances estimation accuracy and stability in complex panel data settings.\nExtensive simulations demonstrate DiD-BCF's superior performance over\nestablished benchmarks, particularly under non-linearity, selection biases, and\neffect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers\nsignificant conditional treatment effect heterogeneity related to county\npopulation, insights obscured by traditional methods. DiD-BCF offers a robust\nand versatile tool for more nuanced causal inference in modern DiD\napplications.", "AI": {"tldr": "This paper introduces DiD-BCF, a novel model for Difference-in-Differences estimation that addresses challenges like staggered adoption and heterogeneous treatment effects. It provides a unified framework for estimating various treatment effects and demonstrates superior performance in simulations and real-world applications.", "motivation": "To improve upon existing methods for Difference-in-Differences (DiD) estimation by addressing issues such as staggered adoption of treatments and heterogeneous treatment effects, which traditional methods struggle with.", "method": "The paper proposes the Difference-in-Differences Bayesian Causal Forest (DiD-BCF), a non-parametric model that incorporates a Parallel Trends Assumption-based reparameterization to enhance estimation accuracy and stability. This model offers a unified framework for estimating Average Treatment Effects (ATE), Group-Average Treatment Effects (GATE), and Conditional Average Treatment Effects (CATE).", "result": "Through extensive simulations, the DiD-BCF model shows superior performance compared to established benchmarks, especially in scenarios involving non-linearity, selection biases, and effect heterogeneity. When applied to U.S. minimum wage policy, it reveals significant conditional treatment effect heterogeneity related to county population, insights not captured by traditional methods.", "conclusion": "DiD-BCF is presented as a robust and versatile tool for more nuanced causal inference in modern DiD applications, offering improvements over traditional methods in terms of accuracy, stability, and ability to uncover complex treatment effect heterogeneity."}}
{"id": "2505.10321", "pdf": "https://arxiv.org/pdf/2505.10321", "abs": "https://arxiv.org/abs/2505.10321", "authors": ["Julius Henke"], "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents", "categories": ["cs.CR", "cs.AI"], "comment": "24 pages, 1 figure, for implementation, see\n  https://github.com/JuliusHenke/autopentest", "summary": "A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.09718", "pdf": "https://arxiv.org/pdf/2505.09718", "abs": "https://arxiv.org/abs/2505.09718", "authors": ["Daniel Dylewsky", "Sonia K\u00e9fi", "Madhur Anand", "Chris T. Bauch"], "title": "Neural models for prediction of spatially patterned phase transitions: methods and challenges", "categories": ["physics.comp-ph", "cs.LG"], "comment": null, "summary": "Dryland vegetation ecosystems are known to be susceptible to critical\ntransitions between alternative stable states when subjected to external\nforcing. Such transitions are often discussed through the framework of\nbifurcation theory, but the spatial patterning of vegetation, which is\ncharacteristic of drylands, leads to dynamics that are much more complex and\ndiverse than local bifurcations. Recent methodological developments in Early\nWarning Signal (EWS) detection have shown promise in identifying dynamical\nsignatures of oncoming critical transitions, with particularly strong\npredictive capabilities being demonstrated by deep neural networks. However, a\nmachine learning model trained on synthetic examples is only useful if it can\neffectively transfer to a test case of practical interest. These models'\ncapacity to generalize in this manner has been demonstrated for bifurcation\ntransitions, but it is not as well characterized for high-dimensional phase\ntransitions. This paper explores the successes and shortcomings of neural EWS\ndetection for spatially patterned phase transitions, and shows how these models\ncan be used to gain insight into where and how EWS-relevant information is\nencoded in spatiotemporal dynamics. A few paradigmatic test systems are used to\nillustrate how the capabilities of such models can be probed in a number of\nways, with particular attention to the performances of a number of proposed\nstatistical indicators for EWS and to the supplementary task of distinguishing\nbetween abrupt and continuous transitions. Results reveal that model\nperformance often changes dramatically when training and test data sources are\ninterchanged, which offers new insight into the criteria for model\ngeneralization.", "AI": {"tldr": "Dryland vegetation ecosystems are susceptible to critical transitions between alternative stable states when subjected to external forcing. Recent methodological developments in Early Warning Signal (EWS) detection have shown promise in identifying dynamical signatures of oncoming critical transitions, with particularly strong predictive capabilities being demonstrated by deep neural networks.", "motivation": "Dryland vegetation ecosystems are known to be susceptible to critical transitions between alternative stable states when subjected to external forcing.", "method": "This paper explores the successes and shortcomings of neural EWS detection for spatially patterned phase transitions, and shows how these models can be used to gain insight into where and how EWS-relevant information is encoded in spatiotemporal dynamics.", "result": "Results reveal that model performance often changes dramatically when training and test data sources are interchanged, which offers new insight into the criteria for model generalization.", "conclusion": "The study reveals the potential and limitations of using neural networks for detecting early warning signals in dryland vegetation ecosystems."}}
{"id": "2505.09734", "pdf": "https://arxiv.org/pdf/2505.09734", "abs": "https://arxiv.org/abs/2505.09734", "authors": ["Babak Esmaeili", "Nariman Niknejad", "Hamidreza Modares"], "title": "Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "comment": "Submitted to Asian Journal of Control", "summary": "This paper presents a risk-aware safe reinforcement learning (RL) control\ndesign for stochastic discrete-time linear systems. Rather than using a safety\ncertifier to myopically intervene with the RL controller, a risk-informed safe\ncontroller is also learned besides the RL controller, and the RL and safe\ncontrollers are combined together. Several advantages come along with this\napproach: 1) High-confidence safety can be certified without relying on a\nhigh-fidelity system model and using limited data available, 2) Myopic\ninterventions and convergence to an undesired equilibrium can be avoided by\ndeciding on the contribution of two stabilizing controllers, and 3) highly\nefficient and computationally tractable solutions can be provided by optimizing\nover a scalar decision variable and linear programming polyhedral sets. To\nlearn safe controllers with a large invariant set, piecewise affine controllers\nare learned instead of linear controllers. To this end, the closed-loop system\nis first represented using collected data, a decision variable, and noise. The\neffect of the decision variable on the variance of the safe violation of the\nclosed-loop system is formalized. The decision variable is then designed such\nthat the probability of safety violation for the learned closed-loop system is\nminimized. It is shown that this control-oriented approach reduces the data\nrequirements and can also reduce the variance of safety violations. Finally, to\nintegrate the safe and RL controllers, a new data-driven interpolation\ntechnique is introduced. This method aims to maintain the RL agent's optimal\nimplementation while ensuring its safety within environments characterized by\nnoise. The study concludes with a simulation example that serves to validate\nthe theoretical results.", "AI": {"tldr": "This paper proposes a risk-aware safe reinforcement learning control design for stochastic discrete-time linear systems, integrating safety and optimality through a novel interpolation technique.", "motivation": "To address the limitations of existing methods that rely on myopic interventions or high-fidelity models in ensuring safety for RL-controlled stochastic systems.", "method": "Learning a risk-informed safe controller alongside an RL controller and combining them using a data-driven interpolation technique. Utilizing piecewise affine controllers to expand the invariant set and optimizing over a scalar decision variable to minimize safety violations.", "result": "Achieves high-confidence safety without needing a high-fidelity model, avoids undesired equilibria, provides efficient solutions, reduces data requirements, and minimizes variance of safety violations.", "conclusion": "The proposed approach successfully integrates safety and optimality in RL control for stochastic systems, validated through a simulation example."}}
{"id": "2505.09748", "pdf": "https://arxiv.org/pdf/2505.09748", "abs": "https://arxiv.org/abs/2505.09748", "authors": ["Jitendra K Tugnait"], "title": "Learning Multi-Attribute Differential Graphs with Non-Convex Penalties", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "14 pages, 1 figures, 2 tables, published in IEEE Access, pp.\n  67065-67078, 2025", "summary": "We consider the problem of estimating differences in two multi-attribute\nGaussian graphical models (GGMs) which are known to have similar structure,\nusing a penalized D-trace loss function with non-convex penalties. The GGM\nstructure is encoded in its precision (inverse covariance) matrix. Existing\nmethods for multi-attribute differential graph estimation are based on a group\nlasso penalized loss function. In this paper, we consider a penalized D-trace\nloss function with non-convex (log-sum and smoothly clipped absolute deviation\n(SCAD)) penalties. Two proximal gradient descent methods are presented to\noptimize the objective function. Theoretical analysis establishing sufficient\nconditions for consistency in support recovery, convexity and estimation in\nhigh-dimensional settings is provided. We illustrate our approaches with\nnumerical examples based on synthetic and real data.", "AI": {"tldr": "The paper proposes a method for estimating differences in two multi-attribute Gaussian graphical models (GGMs) using a penalized D-trace loss function with non-convex penalties, providing theoretical analysis and numerical examples.", "motivation": "Existing methods for multi-attribute differential graph estimation rely on group lasso penalized loss functions, but this paper aims to improve upon that by introducing a penalized D-trace loss function with non-convex penalties.", "method": "The method involves using a penalized D-trace loss function with non-convex penalties such as log-sum and SCAD. Two proximal gradient descent methods are used to optimize the objective function.", "result": "Theoretical analysis is provided that establishes sufficient conditions for consistency in support recovery, convexity, and estimation in high-dimensional settings. The approaches are demonstrated through numerical examples based on synthetic and real data.", "conclusion": "This approach offers an alternative to existing methods for multi-attribute differential graph estimation, potentially providing better performance due to the use of non-convex penalties."}}
{"id": "2505.09783", "pdf": "https://arxiv.org/pdf/2505.09783", "abs": "https://arxiv.org/abs/2505.09783", "authors": ["Jianfeng Jiao", "Xi Gao", "Jie Li"], "title": "Pure Component Property Estimation Framework Using Explainable Machine Learning Methods", "categories": ["stat.AP", "cs.LG"], "comment": null, "summary": "Accurate prediction of pure component physiochemical properties is crucial\nfor process integration, multiscale modeling, and optimization. In this work,\nan enhanced framework for pure component property prediction by using\nexplainable machine learning methods is proposed. In this framework, the\nmolecular representation method based on the connectivity matrix effectively\nconsiders atomic bonding relationships to automatically generate features. The\nsupervised machine learning model random forest is applied for feature ranking\nand pooling. The adjusted R2 is introduced to penalize the inclusion of\nadditional features, providing an assessment of the true contribution of\nfeatures. The prediction results for normal boiling point (Tb), liquid molar\nvolume, critical temperature (Tc) and critical pressure (Pc) obtained using\nArtificial Neural Network and Gaussian Process Regression models confirm the\naccuracy of the molecular representation method. Comparison with GC based\nmodels shows that the root-mean-square error on the test set can be reduced by\nup to 83.8%. To enhance the interpretability of the model, a feature analysis\nmethod based on Shapley values is employed to determine the contribution of\neach feature to the property predictions. The results indicate that using the\nfeature pooling method reduces the number of features from 13316 to 100 without\ncompromising model accuracy. The feature analysis results for Tb, Tc, and Pc\nconfirms that different molecular properties are influenced by different\nstructural features, aligning with mechanistic interpretations. In conclusion,\nthe proposed framework is demonstrated to be feasible and provides a solid\nfoundation for mixture component reconstruction and process integration\nmodelling.", "AI": {"tldr": "An enhanced framework using explainable machine learning for predicting pure component physiochemical properties is proposed, reducing feature count without compromising accuracy and aligning with mechanistic interpretations.", "motivation": "Accurate prediction of pure component physiochemical properties is crucial for process integration, multiscale modeling, and optimization.", "method": "The framework uses molecular representation based on connectivity matrix for feature generation, random forest for feature ranking and pooling, and employs Shapley values for feature analysis. Models like Artificial Neural Network and Gaussian Process Regression are used for predictions.", "result": "The root-mean-square error on the test set was reduced by up to 83.8% compared to GC based models. The number of features was reduced from 13316 to 100 without loss in model accuracy.", "conclusion": "The proposed framework is feasible and provides a solid foundation for mixture component reconstruction and process integration modelling."}}
{"id": "2505.09798", "pdf": "https://arxiv.org/pdf/2505.09798", "abs": "https://arxiv.org/abs/2505.09798", "authors": ["Bojan Ristov", "Stefan Eftimov", "Milena Trajanoska", "Dimitar Trajanov"], "title": "Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Public procurement plays a critical role in government operations, ensuring\nthe efficient allocation of resources and fostering economic growth. However,\ntraditional procurement data is often stored in rigid, tabular formats,\nlimiting its analytical potential and hindering transparency. This research\npresents a methodological framework for transforming structured procurement\ndata into a semantic knowledge graph, leveraging ontological modeling and\nautomated data transformation techniques. By integrating RDF and SPARQL-based\nquerying, the system enhances the accessibility and interpretability of\nprocurement records, enabling complex semantic queries and advanced analytics.\nFurthermore, by incorporating machine learning-driven predictive modeling, the\nsystem extends beyond conventional data analysis, offering insights into\nprocurement trends and risk assessment. This work contributes to the broader\nfield of public procurement intelligence by improving data transparency,\nsupporting evidence-based decision-making, and enabling in-depth analysis of\nprocurement activities in North Macedonia.", "AI": {"tldr": "The paper explores transforming traditional procurement data into a semantic knowledge graph using ontological modeling and automated data transformation techniques, enhancing data transparency, decision-making, and analysis in public procurement.", "motivation": "Traditional procurement data is often stored in rigid, tabular formats which limits its analytical potential and hinders transparency.", "method": "Transform structured procurement data into a semantic knowledge graph by leveraging ontological modeling and automated data transformation techniques. Integrate RDF and SPARQL-based querying and incorporate machine learning-driven predictive modeling.", "result": "Improved data transparency, support for evidence-based decision-making, and enabled in-depth analysis of procurement activities in North Macedonia.", "conclusion": "This work contributes to the field of public procurement intelligence by demonstrating methods to enhance accessibility, interpretability, and analytics of procurement records."}}
{"id": "2505.09803", "pdf": "https://arxiv.org/pdf/2505.09803", "abs": "https://arxiv.org/abs/2505.09803", "authors": ["Antony Sikorski", "Michael Ivanitskiy", "Nathan Lenssen", "Douglas Nychka", "Daniel McKenzie"], "title": "LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In many scientific and industrial applications, we are given a handful of\ninstances (a 'small ensemble') of a spatially distributed quantity (a 'field')\nbut would like to acquire many more. For example, a large ensemble of global\ntemperature sensitivity fields from a climate model can help farmers, insurers,\nand governments plan appropriately. When acquiring more data is prohibitively\nexpensive -- as is the case with climate models -- statistical emulation offers\nan efficient alternative for simulating synthetic yet realistic fields.\nHowever, parameter inference using maximum likelihood estimation (MLE) is\ncomputationally prohibitive, especially for large, non-stationary fields. Thus,\nmany recent works train neural networks to estimate parameters given spatial\nfields as input, sidestepping MLE completely. In this work we focus on a\npopular class of parametric, spatially autoregressive (SAR) models. We make a\nsimple yet impactful observation; because the SAR parameters can be arranged on\na regular grid, both inputs (spatial fields) and outputs (model parameters) can\nbe viewed as images. Using this insight, we demonstrate that image-to-image\n(I2I) networks enable faster and more accurate parameter estimation for a class\nof non-stationary SAR models with unprecedented complexity.", "AI": {"tldr": "The paper explores the use of image-to-image (I2I) networks for parameter estimation in spatially autoregressive (SAR) models, offering faster and more accurate results compared to traditional methods.", "motivation": "In many applications, acquiring a large ensemble of data instances is expensive or impractical. Statistical emulation using SAR models can generate synthetic fields, but parameter inference via MLE is computationally prohibitive for large, non-stationary fields.", "method": "By recognizing that SAR parameters can be arranged on a regular grid, both inputs (spatial fields) and outputs (model parameters) are treated as images. This allows the application of image-to-image (I2I) networks for parameter estimation.", "result": "I2I networks enable faster and more accurate parameter estimation for non-stationary SAR models with high complexity.", "conclusion": "Image-to-image networks provide an efficient alternative for parameter estimation in complex SAR models, outperforming traditional methods in terms of speed and accuracy."}}
{"id": "2505.10371", "pdf": "https://arxiv.org/pdf/2505.10371", "abs": "https://arxiv.org/abs/2505.10371", "authors": ["Kai Sun", "Peibo Duan", "Levin Kuhlmann", "Beilun Wang", "Bin Zhang"], "title": "ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "The Spiking Neural Network (SNN) has drawn increasing attention for its\nenergy-efficient, event-driven processing and biological plausibility. To train\nSNNs via backpropagation, surrogate gradients are used to approximate the\nnon-differentiable spike function, but they only maintain nonzero derivatives\nwithin a narrow range of membrane potentials near the firing threshold,\nreferred to as the surrogate gradient support width gamma. We identify a major\nchallenge, termed the dilemma of gamma: a relatively large gamma leads to\noveractivation, characterized by excessive neuron firing, which in turn\nincreases energy consumption, whereas a small gamma causes vanishing gradients\nand weakens temporal dependencies. To address this, we propose a temporal\nInhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological\ninhibitory mechanisms. This model incorporates interconnected inhibitory units\nfor membrane potential and current, effectively mitigating overactivation while\npreserving gradient propagation. Theoretical analysis demonstrates ILIF\neffectiveness in overcoming the gamma dilemma, and extensive experiments on\nmultiple datasets show that ILIF improves energy efficiency by reducing firing\nrates, stabilizes training, and enhances accuracy. The code is available at\ngithub.com/kaisun1/ILIF.", "AI": {"tldr": "A temporal Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model is proposed to solve the dilemma of gamma in Spiking Neural Networks (SNNs), which improves energy efficiency, stabilizes training and enhances accuracy.", "motivation": "The non-differentiable spike function in SNNs is approximated by surrogate gradients with a narrow range of nonzero derivatives near the firing threshold. This leads to a dilemma: large gamma causes overactivation while small gamma results in vanishing gradients and weakened temporal dependencies.", "method": "The ILIF neuron model incorporates interconnected inhibitory units for membrane potential and current, inspired by biological inhibitory mechanisms, to mitigate overactivation while preserving gradient propagation.", "result": "Theoretical analysis shows that ILIF effectively overcomes the gamma dilemma. Experiments on multiple datasets demonstrate reduced firing rates, stabilized training, enhanced accuracy, and improved energy efficiency.", "conclusion": "The ILIF neuron model successfully addresses the gamma dilemma in SNNs, leading to more energy-efficient, stable, and accurate networks."}}
{"id": "2505.10375", "pdf": "https://arxiv.org/pdf/2505.10375", "abs": "https://arxiv.org/abs/2505.10375", "authors": ["Rui Melo", "Claudia Mamede", "Andre Catarino", "Rui Abreu", "Henrique Lopes Cardoso"], "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "10 pages, 10 figures", "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.", "AI": {"tldr": "The paper explores the use of Sparse Autoencoders (SAEs) as a lightweight, interpretable alternative for bug detection in Java functions using representations from pretrained LLMs like GPT-2 Small and Gemma 2B. SAE-derived features achieved an F1 score of up to 89%, outperforming fine-tuned transformer encoder baselines without requiring fine-tuning or task-specific supervision.", "motivation": "Software vulnerabilities pose significant security risks, and traditional detection methods face challenges such as high false positive rates and scalability issues. AI-based approaches, particularly LLMs, offer new possibilities but encounter interpretability and deployment problems due to their complexity.", "method": "The researchers applied Sparse Autoencoders (SAEs) to internal representations from pretrained LLMs (GPT-2 Small and Gemma 2B) to detect bugs in Java functions. The method did not involve fine-tuning the LLMs or applying task-specific supervision.", "result": "SAE-derived features achieved an F1 score of up to 89% in bug detection, consistently surpassing fine-tuned transformer encoder baselines.", "conclusion": "This study provides the first empirical evidence that SAEs can effectively detect software bugs directly from the internal representations of pretrained LLMs without fine-tuning or additional supervision."}}
{"id": "2505.10387", "pdf": "https://arxiv.org/pdf/2505.10387", "abs": "https://arxiv.org/abs/2505.10387", "authors": ["Artem Agafonov", "Konstantin Yakovlev"], "title": "Multi-Agent Path Finding For Large Agents Is Intractable", "categories": ["cs.MA", "cs.AI", "cs.CC"], "comment": null, "summary": "The multi-agent path finding (MAPF) problem asks to find a set of paths on a\ngraph such that when synchronously following these paths the agents never\nencounter a conflict. In the most widespread MAPF formulation, the so-called\nClassical MAPF, the agents sizes are neglected and two types of conflicts are\nconsidered: occupying the same vertex or using the same edge at the same time\nstep. Meanwhile in numerous practical applications, e.g. in robotics, taking\ninto account the agents' sizes is vital to ensure that the MAPF solutions can\nbe safely executed. Introducing large agents yields an additional type of\nconflict arising when one agent follows an edge and its body overlaps with the\nbody of another agent that is actually not using this same edge (e.g. staying\nstill at some distinct vertex of the graph). Until now it was not clear how\nharder the problem gets when such conflicts are to be considered while\nplanning. Specifically, it was known that Classical MAPF problem on an\nundirected graph can be solved in polynomial time, however no complete\npolynomial-time algorithm was presented to solve MAPF with large agents. In\nthis paper we, for the first time, establish that the latter problem is NP-hard\nand, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be\npresented. Our proof is based on the prevalent in the field technique of\nreducing the seminal 3SAT problem (which is known to be an NP-complete problem)\nto the problem at hand. In particular, for an arbitrary 3SAT formula we\nprocedurally construct a dedicated graph with specific start and goal vertices\nand show that the given 3SAT formula is satisfiable iff the corresponding path\nfinding instance has a solution.", "AI": {"tldr": "\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e\uff08MAPF\uff09\u95ee\u9898\u4e2d\uff0c\u8003\u8651\u667a\u80fd\u4f53\u5927\u5c0f\u4f1a\u4f7f\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u3002\u672c\u6587\u8bc1\u660e\u4e86\u5e26\u6709\u5927\u578b\u667a\u80fd\u4f53\u7684MAPF\u95ee\u9898\u662fNP\u96be\u95ee\u9898\uff0c\u8fd9\u610f\u5473\u7740\u5982\u679cP!=NP\uff0c\u5219\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u53ef\u4ee5\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684MAPF\u95ee\u9898\u5047\u8bbe\u667a\u80fd\u4f53\u6ca1\u6709\u5927\u5c0f\uff0c\u5e76\u4ec5\u8003\u8651\u4e24\u79cd\u51b2\u7a81\u7c7b\u578b\uff1a\u5728\u540c\u4e00\u65f6\u95f4\u6b65\u5360\u7528\u540c\u4e00\u9876\u70b9\u6216\u4f7f\u7528\u540c\u4e00\u8fb9\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u673a\u5668\u4eba\u6280\u672f\uff09\uff0c\u8003\u8651\u667a\u80fd\u4f53\u7684\u5927\u5c0f\u5bf9\u4e8e\u786e\u4fddMAPF\u89e3\u51b3\u65b9\u6848\u7684\u5b89\u5168\u6267\u884c\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5f15\u5165\u5927\u578b\u667a\u80fd\u4f53\u540e\u5bf9\u95ee\u9898\u590d\u6742\u6027\u7684\u5f71\u54cd\u662f\u6709\u610f\u4e49\u7684\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5c06\u8457\u540d\u76843SAT\u95ee\u9898\uff08\u5df2\u77e5\u4e3aNP\u5b8c\u5168\u95ee\u9898\uff09\u5f52\u7ea6\u5230\u5e26\u6709\u5927\u578b\u667a\u80fd\u4f53\u7684MAPF\u95ee\u9898\u6765\u8bc1\u660e\u5176NP\u96be\u5ea6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4ed6\u4eec\u4e3a\u4efb\u610f\u76843SAT\u516c\u5f0f\u6784\u9020\u4e00\u4e2a\u4e13\u95e8\u7684\u56fe\uff0c\u5e76\u5c55\u793a\u51fa\u7ed9\u5b9a\u76843SAT\u516c\u5f0f\u662f\u53ef\u6ee1\u8db3\u7684\u5f53\u4e14\u4ec5\u5f53\u76f8\u5e94\u7684\u8def\u5f84\u5bfb\u627e\u5b9e\u4f8b\u6709\u89e3\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u5e26\u6709\u5927\u578b\u667a\u80fd\u4f53\u7684MAPF\u95ee\u9898\u662fNP\u96be\u95ee\u9898\u3002\u8fd9\u610f\u5473\u7740\u5982\u679cP!=NP\uff0c\u5219\u4e0d\u5b58\u5728\u80fd\u591f\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u5b8c\u6574\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u5e26\u6709\u5927\u578b\u667a\u80fd\u4f53\u7684MAPF\u95ee\u9898\u662fNP\u96be\u95ee\u9898\uff0c\u8fd9\u8868\u660e\u6211\u4eec\u9700\u8981\u5bfb\u6c42\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u5176\u4ed6\u8fd1\u4f3c\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u671f\u671b\u5b58\u5728\u4e00\u4e2a\u901a\u7528\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002"}}
{"id": "2505.09833", "pdf": "https://arxiv.org/pdf/2505.09833", "abs": "https://arxiv.org/abs/2505.09833", "authors": ["Tuba Girgin", "Emre Girgin", "Cagri Kilic"], "title": "Learning Rock Pushability on Rough Planetary Terrain", "categories": ["cs.RO", "cs.LG"], "comment": "Paper presented at the Workshop on Field Robotics, ICRA 2025,\n  Atlanta, GA, United States", "summary": "In the context of mobile navigation in unstructured environments, the\npredominant approach entails the avoidance of obstacles. The prevailing path\nplanning algorithms are contingent upon deviating from the intended path for an\nindefinite duration and returning to the closest point on the route after the\nobstacle is left behind spatially. However, avoiding an obstacle on a path that\nwill be used repeatedly by multiple agents can hinder long-term efficiency and\nlead to a lasting reliance on an active path planning system. In this study, we\npropose an alternative approach to mobile navigation in unstructured\nenvironments by leveraging the manipulation capabilities of a robotic\nmanipulator mounted on top of a mobile robot. Our proposed framework integrates\nexteroceptive and proprioceptive feedback to assess the push affordance of\nobstacles, facilitating their repositioning rather than avoidance. While our\npreliminary visual estimation takes into account the characteristics of both\nthe obstacle and the surface it relies on, the push affordance estimation\nmodule exploits the force feedback obtained by interacting with the obstacle\nvia a robotic manipulator as the guidance signal. The objective of our\nnavigation approach is to enhance the efficiency of routes utilized by multiple\nagents over extended periods by reducing the overall time spent by a fleet in\nenvironments where autonomous infrastructure development is imperative, such as\nlunar or Martian surfaces.", "AI": {"tldr": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u79fb\u52a8\u969c\u788d\u7269\u800c\u975e\u907f\u5f00\u5b83\u6765\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u91cd\u590d\u4f7f\u7528\u7684\u8def\u5f84\u6548\u7387\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u907f\u969c\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u957f\u671f\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u9700\u8981\u6301\u7eed\u7684\u8def\u5f84\u89c4\u5212\u7cfb\u7edf\u652f\u6301\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5b89\u88c5\u5728\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u7684\u673a\u68b0\u81c2\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u7ed3\u5408\u5916\u90e8\u548c\u672c\u4f53\u611f\u89c9\u53cd\u9988\u6765\u8bc4\u4f30\u969c\u788d\u7269\u7684\u53ef\u63a8\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u969c\u788d\u7269\u7684\u91cd\u65b0\u5b9a\u4f4d\u3002\u521d\u6b65\u89c6\u89c9\u4f30\u8ba1\u8003\u8651\u4e86\u969c\u788d\u7269\u53ca\u5176\u6240\u5728\u8868\u9762\u7684\u7279\u6027\uff0c\u800c\u63a8\u52a8\u53ef\u8d1f\u62c5\u6027\u8bc4\u4f30\u6a21\u5757\u5219\u5229\u7528\u4e0e\u969c\u788d\u7269\u4ea4\u4e92\u65f6\u901a\u8fc7\u673a\u68b0\u81c2\u83b7\u5f97\u7684\u529b\u53cd\u9988\u4f5c\u4e3a\u5f15\u5bfc\u4fe1\u53f7\u3002", "result": "\u8be5\u5bfc\u822a\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u8f66\u961f\u5728\u9700\u8981\u81ea\u4e3b\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u7684\u73af\u5883\uff08\u5982\u6708\u7403\u6216\u706b\u661f\u8868\u9762\uff09\u4e2d\u6240\u82b1\u8d39\u7684\u603b\u4f53\u65f6\u95f4\uff0c\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u957f\u65f6\u95f4\u4f7f\u7528\u8def\u5f84\u7684\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u673a\u68b0\u81c2\u64cd\u4f5c\u548c\u53cd\u9988\u673a\u5236\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u6539\u5584\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u8def\u5f84\u7684\u957f\u671f\u6548\u7387\u3002"}}
{"id": "2505.10393", "pdf": "https://arxiv.org/pdf/2505.10393", "abs": "https://arxiv.org/abs/2505.10393", "authors": ["Agustin Medina", "Marcelo Arlego", "Carlos A. Lamas"], "title": "Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training", "categories": ["cond-mat.str-el", "cs.AI"], "comment": "25 pages, 14 figures", "summary": "We investigate the efficient learning of magnetic phases using artificial\nneural networks trained on synthetic data, combining computational simplicity\nwith physics-informed strategies. Focusing on the diluted Ising model, which\nlacks an exact analytical solution, we explore two complementary approaches: a\nsupervised classification using simple dense neural networks, and an\nunsupervised detection of phase transitions using convolutional autoencoders\ntrained solely on idealized spin configurations.\n  To enhance model performance, we incorporate two key forms of\nphysics-informed guidance. First, we exploit architectural biases which\npreferentially amplify features related to symmetry breaking. Second, we\ninclude training configurations that explicitly break $\\mathbb{Z}_2$ symmetry,\nreinforcing the network's ability to detect ordered phases. These mechanisms,\nacting in tandem, increase the network's sensitivity to phase structure even in\nthe absence of explicit labels. We validate the machine learning predictions\nthrough comparison with direct numerical estimates of critical temperatures and\npercolation thresholds.\n  Our results show that synthetic, structured, and computationally efficient\ntraining schemes can reveal physically meaningful phase boundaries, even in\ncomplex systems. This framework offers a low-cost and robust alternative to\nconventional methods, with potential applications in broader condensed matter\nand statistical physics contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u9ad8\u6548\u5b66\u4e60\u78c1\u76f8\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u8ba1\u7b97\u7b80\u5355\u6027\u548c\u7269\u7406\u4fe1\u606f\u7b56\u7565\u3002\u901a\u8fc7\u76d1\u7763\u5206\u7c7b\u548c\u975e\u76d1\u7763\u68c0\u6d4b\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u52a0\u5165\u7269\u7406\u4fe1\u606f\u6307\u5bfc\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e0e\u6570\u503c\u4f30\u8ba1\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u3001\u7ed3\u6784\u5316\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\u53ef\u4ee5\u63ed\u793a\u7269\u7406\u610f\u4e49\u7684\u76f8\u8fb9\u754c\uff0c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u7531\u4e8e\u7a00\u91ca\u4f0a\u8f9b\u6a21\u578b\u7f3a\u4e4f\u7cbe\u786e\u89e3\u6790\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u5b66\u4e60\u65b9\u6cd5\u6765\u7406\u89e3\u5176\u76f8\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u76d1\u7763\u5206\u7c7b\uff0c\u4ee5\u53ca\u4ec5\u57fa\u4e8e\u7406\u60f3\u81ea\u65cb\u914d\u7f6e\u8bad\u7ec3\u7684\u5377\u79ef\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u975e\u76d1\u7763\u68c0\u6d4b\u3002\u540c\u65f6\uff0c\u5229\u7528\u67b6\u6784\u504f\u5dee\u653e\u5927\u5bf9\u79f0\u6027\u7834\u574f\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u5305\u542b\u663e\u5f0f\u7834\u574f$\\mathbb{Z}_2$\u5bf9\u79f0\u6027\u7684\u8bad\u7ec3\u914d\u7f6e\u4ee5\u589e\u5f3a\u7f51\u7edc\u68c0\u6d4b\u6709\u5e8f\u76f8\u7684\u80fd\u529b\u3002", "result": "\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e0e\u76f4\u63a5\u6570\u503c\u4f30\u8ba1\u7684\u5173\u952e\u6e29\u5ea6\u548c\u6e17\u900f\u9608\u503c\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63ed\u793a\u590d\u6742\u7cfb\u7edf\u4e2d\u7269\u7406\u610f\u4e49\u76f8\u8fb9\u754c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5408\u6210\u3001\u7ed3\u6784\u5316\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\u53ef\u4ee5\u63ed\u793a\u7269\u7406\u610f\u4e49\u7684\u76f8\u8fb9\u754c\uff0c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2505.09843", "pdf": "https://arxiv.org/pdf/2505.09843", "abs": "https://arxiv.org/abs/2505.09843", "authors": ["Melissa Turcotte", "Fran\u00e7ois Labr\u00e8che", "Serge-Olivier Paquette"], "title": "Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts", "categories": ["cs.CR", "cs.LG", "stat.AP"], "comment": null, "summary": "Enterprise networks are growing ever larger with a rapidly expanding attack\nsurface, increasing the volume of security alerts generated from security\ncontrols. Security Operations Centre (SOC) analysts triage these alerts to\nidentify malicious activity, but they struggle with alert fatigue due to the\noverwhelming number of benign alerts. Organisations are turning to managed SOC\nproviders, where the problem is amplified by context switching and limited\nvisibility into business processes.\n  A novel system, named AACT, is introduced that automates SOC workflows by\nlearning from analysts' triage actions on cybersecurity alerts. It accurately\npredicts triage decisions in real time, allowing benign alerts to be closed\nautomatically and critical ones prioritised. This reduces the SOC queue\nallowing analysts to focus on the most severe, relevant or ambiguous threats.\nThe system has been trained and evaluated on both real SOC data and an open\ndataset, obtaining high performance in identifying malicious alerts from benign\nalerts.\n  Additionally, the system has demonstrated high accuracy in a real SOC\nenvironment, reducing alerts shown to analysts by 61% over six months, with a\nlow false negative rate of 1.36% over millions of alerts.", "AI": {"tldr": "AACT\u7cfb\u7edf\u901a\u8fc7\u5b66\u4e60\u5206\u6790\u5e08\u5bf9\u7f51\u7edc\u5b89\u5168\u8b66\u62a5\u7684\u5206\u7c7b\u64cd\u4f5c\uff0c\u81ea\u52a8\u5316SOC\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u5c55\u793a\u7ed9\u5206\u6790\u5e08\u7684\u8b66\u62a5\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8bef\u62a5\u7387\u3002", "motivation": "\u4f01\u4e1a\u7f51\u7edc\u89c4\u6a21\u65e5\u76ca\u6269\u5927\uff0c\u5b89\u5168\u8b66\u62a5\u6570\u91cf\u6fc0\u589e\uff0c\u5bfc\u81f4SOC\u5206\u6790\u5e08\u9762\u4e34\u4e25\u91cd\u7684\u8b66\u62a5\u75b2\u52b3\u95ee\u9898\u3002\u6b64\u5916\uff0c\u5728\u7ba1\u7406SOC\u670d\u52a1\u4e2d\uff0c\u4e0a\u4e0b\u6587\u5207\u6362\u548c\u4e1a\u52a1\u6d41\u7a0b\u53ef\u89c1\u6027\u4e0d\u8db3\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u540d\u4e3aAACT\u7684\u65b0\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u5206\u6790\u5206\u6790\u5e08\u5bf9\u7f51\u7edc\u5b89\u5168\u8b66\u62a5\u7684\u5206\u7c7b\u884c\u4e3a\uff0c\u5b9e\u65f6\u9884\u6d4b\u5206\u7c7b\u51b3\u7b56\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u52a8\u5173\u95ed\u826f\u6027\u8b66\u62a5\u5e76\u4f18\u5148\u5904\u7406\u5173\u952e\u8b66\u62a5\u3002", "result": "\u5728\u771f\u5b9eSOC\u6570\u636e\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u540e\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u80fd\u6709\u6548\u533a\u5206\u6076\u610f\u548c\u826f\u6027\u8b66\u62a5\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cAACT\u7cfb\u7edf\u5728\u516d\u4e2a\u6708\u5185\u51cf\u5c11\u4e8661%\u5c55\u793a\u7ed9\u5206\u6790\u5e08\u7684\u8b66\u62a5\uff0c\u540c\u65f6\u8bef\u62a5\u7387\u4ec5\u4e3a1.36%\u3002", "conclusion": "AACT\u7cfb\u7edf\u6210\u529f\u51cf\u5c11\u4e86SOC\u5206\u6790\u5e08\u7684\u5de5\u4f5c\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6548\u7387\uff0c\u5e76\u4e14\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u8bef\u62a5\u7387\u3002"}}
{"id": "2505.10394", "pdf": "https://arxiv.org/pdf/2505.10394", "abs": "https://arxiv.org/abs/2505.10394", "authors": ["Meghyn Bienvenu", "Camille Bourgaux", "Atefe Khodadaditaghanaki"], "title": "Inconsistency Handling in DatalogMTL", "categories": ["cs.LO", "cs.AI", "cs.DB"], "comment": "This is an extended version of a paper appearing at the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI 2025). 24\n  pages", "summary": "In this paper, we explore the issue of inconsistency handling in DatalogMTL,\nan extension of Datalog with metric temporal operators. Since facts are\nassociated with time intervals, there are different manners to restore\nconsistency when they contradict the rules, such as removing facts or modifying\ntheir time intervals. Our first contribution is the definition of relevant\nnotions of conflicts (minimal explanations for inconsistency) and repairs\n(possible ways of restoring consistency) for this setting and the study of the\nproperties of these notions and the associated inconsistency-tolerant\nsemantics. Our second contribution is a data complexity analysis of the tasks\nof generating a single conflict / repair and query entailment under\nrepair-based semantics.", "AI": {"tldr": "The paper explores inconsistency handling in DatalogMTL, defines conflict and repair notions, and analyzes data complexity for generating conflicts/repairs and query entailment.", "motivation": "Inconsistency handling is crucial in DatalogMTL as facts associated with time intervals may contradict rules; thus, understanding how to restore consistency through various methods like removing facts or altering time intervals is important.", "method": "The authors define relevant notions of conflicts (minimal explanations for inconsistency) and repairs (ways to restore consistency) specific to the DatalogMTL setting. They also study the properties of these notions and their inconsistency-tolerant semantics.", "result": "The first contribution includes definitions and analysis of conflicts and repairs in DatalogMTL. The second contribution involves a detailed data complexity analysis for tasks such as generating a single conflict/repair and query entailment under repair-based semantics.", "conclusion": "This work provides essential insights into inconsistency handling in DatalogMTL by defining key concepts and analyzing computational complexities, which will help advance the field."}}
{"id": "2505.09972", "pdf": "https://arxiv.org/pdf/2505.09972", "abs": "https://arxiv.org/abs/2505.09972", "authors": ["Anchen Sun", "Tiantian Feng", "Gabriela Gutierrez", "Juan J Londono", "Anfeng Xu", "Batya Elbaum", "Shrikanth Narayanan", "Lynn K Perry", "Daniel S Messinger"], "title": "Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech", "categories": ["eess.AS", "cs.LG"], "comment": "8 pages, 2 figures, 5 tables", "summary": "This paper introduces an automated framework WSW2.0 for analyzing vocal\ninteractions in preschool classrooms, enhancing both accuracy and scalability\nthrough the integration of wav2vec2-based speaker classification and Whisper\n(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio\nrecordings (160 minutes from 12 children and 75 minutes from 5 teachers), were\nused to compare system outputs to expert human annotations. WSW2.0 achieves a\nweighted F1 score of .845, accuracy of .846, and an error-corrected kappa of\n.672 for speaker classification (child vs. teacher). Transcription quality is\nmoderate to high with word error rates of .119 for teachers and .238 for\nchildren. WSW2.0 exhibits relatively high absolute agreement intraclass\ncorrelations (ICC) with expert transcriptions for a range of classroom language\nfeatures. These include teacher and child mean utterance length, lexical\ndiversity, question asking, and responses to questions and other utterances,\nwhich show absolute agreement intraclass correlations between .64 and .98. To\nestablish scalability, we apply the framework to an extensive dataset spanning\ntwo years and over 1,592 hours of classroom audio recordings, demonstrating the\nframework's robustness for broad real-world applications. These findings\nhighlight the potential of deep learning and natural language processing\ntechniques to revolutionize educational research by providing accurate measures\nof key features of preschool classroom speech, ultimately guiding more\neffective intervention strategies and supporting early childhood language\ndevelopment.", "AI": {"tldr": "This paper introduces an automated framework WSW2.0 for analyzing vocal interactions in preschool classrooms, enhancing both accuracy and scalability through the integration of wav2vec2-based speaker classification and Whisper speech transcription.", "motivation": "To provide accurate measures of key features of preschool classroom speech using deep learning and natural language processing techniques, ultimately guiding more effective intervention strategies and supporting early childhood language development.", "method": "Integration of wav2vec2-based speaker classification and Whisper (large-v2 and large-v3) speech transcription to create WSW2.0 framework. Comparison of system outputs to expert human annotations on 235 minutes of audio recordings.", "result": "WSW2.0 achieves a weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of .672 for speaker classification. Transcription quality is moderate to high with word error rates of .119 for teachers and .238 for children. High absolute agreement intraclass correlations (ICC) with expert transcriptions for a range of classroom language features.", "conclusion": "The findings highlight the potential of deep learning and natural language processing techniques to revolutionize educational research by providing accurate measures of key features of preschool classroom speech."}}
{"id": "2505.10442", "pdf": "https://arxiv.org/pdf/2505.10442", "abs": "https://arxiv.org/abs/2505.10442", "authors": ["Dechen Gao", "Hang Wang", "Hanchu Zhou", "Nejib Ammar", "Shatadal Mishra", "Ahmadreza Moradipari", "Iman Soltani", "Junshan Zhang"], "title": "IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Imitation learning (IL) and reinforcement learning (RL) each offer distinct\nadvantages for robotics policy learning: IL provides stable learning from\ndemonstrations, and RL promotes generalization through exploration. While\nexisting robot learning approaches using IL-based pre-training followed by\nRL-based fine-tuning are promising, this two-step learning paradigm often\nsuffers from instability and poor sample efficiency during the RL fine-tuning\nphase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning\nand Imitation Learning, for policy fine-tuning, which periodically injects IL\nupdates after multiple RL updates and hence can benefit from the stability of\nIL and the guidance of expert data for more efficient exploration throughout\nthe entire fine-tuning process. Since IL and RL involve different optimization\nobjectives, we develop gradient separation mechanisms to prevent destructive\ninterference during \\ABBR fine-tuning, by separating possibly conflicting\ngradient updates in orthogonal subspaces. Furthermore, we conduct rigorous\nanalysis, and our findings shed light on why interleaving IL with RL stabilizes\nlearning and improves sample-efficiency. Extensive experiments on 14 robot\nmanipulation and locomotion tasks across 3 benchmarks, including\nFurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \\ABBR can\nsignificantly improve sample efficiency and mitigate performance collapse\nduring online finetuning in both long- and short-horizon tasks with either\nsparse or dense rewards. IN-RIL, as a general plug-in compatible with various\nstate-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,\nfrom 12\\% to 88\\% with 6.3x improvement in the success rate on Robomimic\nTransport. Project page: https://github.com/ucd-dare/IN-RIL.", "AI": {"tldr": "IN-RIL\u662f\u4e00\u79cd\u5c06\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ea4\u7ec7\u8fdb\u884c\u7684\u7b56\u7565\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6ce8\u5165IL\u66f4\u65b0\u6765\u63d0\u9ad8\u7a33\u5b9a\u6027\u53ca\u91c7\u6837\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u5206\u79bb\u673a\u5236\u9632\u6b62\u4f18\u5316\u51b2\u7a81\u3002\u5b9e\u9a8c\u8868\u660e\uff0cIN-RIL\u5728\u591a\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u8fd0\u52a8\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\u5e76\u7f13\u89e3\u4e86\u6027\u80fd\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u91c7\u7528IL\u9884\u8bad\u7ec3\u52a0RL\u5fae\u8c03\u7684\u4e24\u6b65\u8303\u5f0f\uff0c\u4f46\u8fd9\u79cd\u8303\u5f0f\u5728RL\u5fae\u8c03\u9636\u6bb5\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5728\u6574\u4e2a\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u90fd\u80fd\u4eceIL\u7684\u7a33\u5b9a\u6027\u548c\u4e13\u5bb6\u6570\u636e\u6307\u5bfc\u4e2d\u53d7\u76ca\uff0c\u540c\u65f6\u4fdd\u6301RL\u7684\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIN-RIL\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u591a\u6b21RL\u66f4\u65b0\u540e\u5b9a\u671f\u6ce8\u5165IL\u66f4\u65b0\uff0c\u4f7f\u7b56\u7565\u5728\u6574\u4e2a\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u65e2\u80fd\u4eceIL\u7684\u7a33\u5b9a\u6027\u4e2d\u53d7\u76ca\uff0c\u53c8\u80fd\u5f97\u5230\u4e13\u5bb6\u6570\u636e\u7684\u6307\u5bfc\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u68af\u5ea6\u5206\u79bb\u673a\u5236\uff0c\u4ee5\u9632\u6b62\u7531\u4e8eIL\u548cRL\u7684\u4e0d\u540c\u4f18\u5316\u76ee\u6807\u800c\u4ea7\u751f\u7684\u7834\u574f\u6027\u5e72\u6270\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIN-RIL\u53ef\u4ee5\u5728\u957f\u89c6\u57df\u548c\u77ed\u89c6\u57df\u4efb\u52a1\u4e2d\u3001\u7a00\u758f\u6216\u5bc6\u96c6\u5956\u52b1\u7684\u4efb\u52a1\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u5e76\u51cf\u8f7b\u5728\u7ebf\u5fae\u8c03\u671f\u95f4\u7684\u6027\u80fd\u5d29\u6e83\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728Robomimic Transport\u4efb\u52a1\u4e0a\uff0c\u6210\u529f\u7387\u4ece12%\u63d0\u5347\u523088%\uff0c\u63d0\u5347\u4e866.3\u500d\u3002", "conclusion": "IN-RIL\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u63d2\u4ef6\uff0c\u4e0e\u5404\u79cd\u6700\u5148\u8fdb\u7684RL\u7b97\u6cd5\u517c\u5bb9\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584RL\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u91c7\u6837\u6548\u7387\u3002"}}
{"id": "2505.10033", "pdf": "https://arxiv.org/pdf/2505.10033", "abs": "https://arxiv.org/abs/2505.10033", "authors": ["Luis F. W. Batista", "St\u00e9phanie Aravecchia", "Seth Hutchinson", "C\u00e9dric Pradalier"], "title": "Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests", "categories": ["cs.RO", "cs.LG"], "comment": "Workshop on Field Robotics at ICRA 2025", "summary": "Despite significant advancements in Deep Reinforcement Learning (DRL) for\nAutonomous Surface Vehicles (ASVs), their robustness in real-world conditions,\nparticularly under external disturbances, remains insufficiently explored. In\nthis paper, we evaluate the resilience of a DRL-based agent designed to capture\nfloating waste under various perturbations. We train the agent using domain\nrandomization and evaluate its performance in real-world field tests, assessing\nits ability to handle unexpected disturbances such as asymmetric drag and an\noff-center payload. We assess the agent's performance under these perturbations\nin both simulation and real-world experiments, quantifying performance\ndegradation and benchmarking it against an MPC baseline. Results indicate that\nthe DRL agent performs reliably despite significant disturbances. Along with\nthe open-source release of our implementation, we provide insights into\neffective training strategies, real-world challenges, and practical\nconsiderations for deploying DRLbased ASV controllers.", "AI": {"tldr": "This paper evaluates the resilience of a DRL-based agent for Autonomous Surface Vehicles (ASVs) under various perturbations, finding that it performs reliably despite significant disturbances.", "motivation": "To address the insufficient exploration of robustness in real-world conditions for DRL-based ASVs, especially under external disturbances.", "method": "The agent is trained using domain randomization and its performance is evaluated in both simulation and real-world experiments under unexpected disturbances like asymmetric drag and off-center payload. Performance degradation is quantified and benchmarked against an MPC baseline.", "result": "Results show that the DRL agent performs reliably even with significant disturbances.", "conclusion": "The study provides insights into effective training strategies, real-world challenges, and practical considerations for deploying DRL-based ASV controllers, alongside releasing the implementation open-source."}}
{"id": "2505.10443", "pdf": "https://arxiv.org/pdf/2505.10443", "abs": "https://arxiv.org/abs/2505.10443", "authors": ["Pedro Orvalho", "Marta Kwiatkowska"], "title": "Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?", "categories": ["cs.SE", "cs.AI"], "comment": "10 pages, 5 tables, 1 figure", "summary": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u7684\u4ee3\u7801\u53d8\u5f02\u65b9\u6cd5\u8bc4\u4f30\u4e86\u516d\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9Python\u7a0b\u5e8f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5982Llama3.2\u7b49\u6a21\u578b\u5728\u9ad8\u8fbe61%\u7684\u60c5\u51b5\u4e0b\u57fa\u4e8e\u6709\u7f3a\u9677\u7684\u63a8\u7406\u5f97\u51fa\u6b63\u786e\u9884\u6d4b\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6a21\u578b\u5728\u9762\u5bf9\u4ee3\u7801\u53d8\u5f02\u65f6\u9884\u6d4b\u4e0d\u7a33\u5b9a\uff0c\u8868\u660e\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u5df2\u77e5LLMs\u53ef\u80fd\u901a\u8fc7\u9519\u8bef\u903b\u8f91\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u8fdb\u4e00\u6b65\u63a2\u8ba8LLMs\u662f\u5426\u771f\u6b63\u80fd\u591f\u7406\u89e3\u4ee3\u7801\u903b\u8f91\u8fd8\u662f\u4ec5\u51ed\u731c\u6d4b\u3002", "method": "\u7814\u7a76\u8005\u91c7\u7528\u4e86\u4e94\u79cd\u8bed\u4e49\u4fdd\u6301\u7684\u4ee3\u7801\u53d8\u5f02\u6280\u672f\uff1a\u53d8\u91cf\u91cd\u547d\u540d\u3001\u955c\u50cf\u6bd4\u8f83\u8868\u8fbe\u5f0f\u3001\u4ea4\u6362if-else\u5206\u652f\u3001for\u5faa\u73af\u8f6c\u6362\u4e3awhile\u5faa\u73af\u4ee5\u53ca\u5faa\u73af\u5c55\u5f00\u3002\u901a\u8fc7LiveCodeBench\u548cCruxEval\u5e73\u53f0\uff0c\u4ed6\u4eec\u8bc4\u4f30\u4e86\u516d\u4e2aLLM\u6a21\u578b\u5728\u9762\u5bf9\u8fd9\u4e9b\u53d8\u5f02\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u7531\u4eba\u7c7b\u4e13\u5bb6\u5206\u6790\u6b63\u786e\u9884\u6d4b\u80cc\u540e\u7684\u63a8\u7406\u4f9d\u636e\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u90e8\u5206LLM\uff08\u4f8b\u5982Llama3.2\uff09\u5728\u9ad8\u8fbe61%\u7684\u60c5\u51b5\u4e0b\u57fa\u4e8e\u6709\u7f3a\u9677\u7684\u63a8\u7406\u5f97\u51fa\u4e86\u6b63\u786e\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u5f53\u4ee3\u7801\u53d1\u751f\u53d8\u5f02\u65f6\uff0cLLM\u7ecf\u5e38\u6539\u53d8\u9884\u6d4b\u7ed3\u679c\uff0c\u8868\u660e\u5b83\u4eec\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u8f83\u4e3a\u6709\u9650\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u4e00\u4e9bLLM\u53ef\u4ee5\u751f\u6210\u6b63\u786e\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u4e14\u6a21\u578b\u5728\u4ee3\u7801\u53d8\u5f02\u9762\u524d\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027\uff0c\u8bf4\u660e\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u5c1a\u9700\u6539\u8fdb\u3002"}}
{"id": "2505.10080", "pdf": "https://arxiv.org/pdf/2505.10080", "abs": "https://arxiv.org/abs/2505.10080", "authors": ["Weijie Xiong", "Zo\u00eb Holmes", "Armando Angrisani", "Yudai Suzuki", "Thiparat Chotibut", "Supanut Thanasilp"], "title": "Role of scrambling and noise in temporal information processing with quantum systems", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "comment": "14+35 pages, 6+5 figures, 1 table", "summary": "Scrambling quantum systems have been demonstrated as effective substrates for\ntemporal information processing. While their role in providing rich feature\nmaps has been widely studied, a theoretical understanding of their performance\nin temporal tasks is still lacking. Here we consider a general quantum\nreservoir processing framework that captures a broad range of physical\ncomputing models with quantum systems. We examine the scalability and memory\nretention of the model with scrambling reservoirs modelled by high-order\nunitary designs in both noiseless and noisy settings. In the former regime, we\nshow that measurement readouts become exponentially concentrated with\nincreasing reservoir size, yet strikingly do not worsen with the reservoir\niterations. Thus, while repeatedly reusing a small scrambling reservoir with\nquantum data might be viable, scaling up the problem size deteriorates\ngeneralization unless one can afford an exponential shot overhead. In contrast,\nthe memory of early inputs and initial states decays exponentially in both\nreservoir size and reservoir iterations. In the noisy regime, we also prove\nexponential memory decays with iterations for local noisy channels. Proving\nthese results required us to introduce new proof techniques for bounding\nconcentration in temporal quantum learning models.", "AI": {"tldr": "Quantum scrambling systems are effective for temporal information processing. This paper explores their scalability and memory retention using high-order unitary designs, showing exponential concentration of readouts with reservoir size but without worsening over iterations in noiseless settings. However, scaling problem size deteriorates generalization unless an exponential shot overhead is affordable. Memory of early inputs decays exponentially in both reservoir size and iterations, also proven for noisy channels.", "motivation": "To understand the theoretical performance of quantum scrambling systems in temporal tasks, specifically focusing on scalability and memory retention.", "method": "A general quantum reservoir processing framework was used to examine models with quantum systems. High-order unitary designs were applied to model reservoirs in both noiseless and noisy settings. New proof techniques were introduced to bound concentration in temporal quantum learning models.", "result": "In noiseless settings, measurement readouts concentrate exponentially with increasing reservoir size without worsening over iterations. Memory of early inputs decays exponentially in both reservoir size and iterations. In noisy settings, exponential memory decay with iterations was proven for local noisy channels.", "conclusion": "Quantum scrambling systems show potential for temporal information processing but face challenges in scalability and memory retention. Theoretical analysis reveals that while small reservoir reusability is possible, scaling up requires significant resources."}}
{"id": "2505.10099", "pdf": "https://arxiv.org/pdf/2505.10099", "abs": "https://arxiv.org/abs/2505.10099", "authors": ["Sarat Moka", "Matias Quiroz", "Vali Asimit", "Samuel Muller"], "title": "A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection", "categories": ["stat.ML", "cs.LG", "math.OC", "q-fin.PM"], "comment": null, "summary": "Portfolio optimization involves selecting asset weights to minimize a\nrisk-reward objective, such as the portfolio variance in the classical\nminimum-variance framework. Sparse portfolio selection extends this by imposing\na cardinality constraint: only $k$ assets from a universe of $p$ may be\nincluded. The standard approach models this problem as a mixed-integer\nquadratic program and relies on commercial solvers to find the optimal\nsolution. However, the computational costs of such methods increase\nexponentially with $k$ and $p$, making them too slow for problems of even\nmoderate size. We propose a fast and scalable gradient-based approach that\ntransforms the combinatorial sparse selection problem into a constrained\ncontinuous optimization task via Boolean relaxation, while preserving\nequivalence with the original problem on the set of binary points. Our\nalgorithm employs a tunable parameter that transmutes the auxiliary objective\nfrom a convex to a concave function. This allows a stable convex starting\npoint, followed by a controlled path toward a sparse binary solution as the\ntuning parameter increases and the objective moves toward concavity. In\npractice, our method matches commercial solvers in asset selection for most\ninstances and, in rare instances, the solution differs by a few assets whilst\nshowing a negligible error in portfolio variance.", "AI": {"tldr": "The paper proposes a gradient-based approach for sparse portfolio selection that transforms the combinatorial problem into a continuous optimization task, providing a fast and scalable solution with negligible error compared to commercial solvers.", "motivation": "To address the computational inefficiency of current methods for sparse portfolio selection which rely on mixed-integer quadratic programming and become slow as the number of assets and constraints increase.", "method": "A gradient-based approach is introduced, converting the sparse selection problem into a constrained continuous optimization task through Boolean relaxation. A tunable parameter is used to transition the objective function from convex to concave, enabling a stable starting point and progressing towards a sparse binary solution.", "result": "The method matches commercial solvers in asset selection for most instances, and when it differs, the solution varies by only a few assets with negligible error in portfolio variance.", "conclusion": "The proposed gradient-based method offers a fast, scalable alternative to commercial solvers for sparse portfolio selection with comparable accuracy."}}
{"id": "2505.10139", "pdf": "https://arxiv.org/pdf/2505.10139", "abs": "https://arxiv.org/abs/2505.10139", "authors": ["Lorenz Vaitl", "Leon Klein"], "title": "Path Gradients after Flow Matching", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Boltzmann Generators have emerged as a promising machine learning tool for\ngenerating samples from equilibrium distributions of molecular systems using\nNormalizing Flows and importance weighting. Recently, Flow Matching has helped\nspeed up Continuous Normalizing Flows (CNFs), scale them to more complex\nmolecular systems, and minimize the length of the flow integration\ntrajectories. We investigate the benefits of using path gradients to fine-tune\nCNFs initially trained by Flow Matching, in the setting where a target energy\nis known. Our experiments show that this hybrid approach yields up to a\nthreefold increase in sampling efficiency for molecular systems, all while\nusing the same model, a similar computational budget and without the need for\nadditional sampling. Furthermore, by measuring the length of the flow\ntrajectories during fine-tuning, we show that path gradients largely preserve\nthe learned structure of the flow.", "AI": {"tldr": "Boltzmann Generators\u5229\u7528Normalizing Flows\u548c\u91cd\u8981\u6027\u52a0\u6743\u4ece\u5206\u5b50\u7cfb\u7edf\u7684\u5e73\u8861\u5206\u5e03\u751f\u6210\u6837\u672c\u3002Flow Matching\u52a0\u901f\u4e86Continuous Normalizing Flows\uff08CNFs\uff09\uff0c\u4f7f\u5176\u80fd\u591f\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u5206\u5b50\u7cfb\u7edf\uff0c\u5e76\u6700\u5c0f\u5316\u6d41\u79ef\u5206\u8f68\u8ff9\u7684\u957f\u5ea6\u3002\u672c\u6587\u7814\u7a76\u4e86\u5728\u5df2\u77e5\u76ee\u6807\u80fd\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u8def\u5f84\u68af\u5ea6\u5fae\u8c03\u901a\u8fc7Flow Matching\u521d\u6b65\u8bad\u7ec3\u7684CNFs\u7684\u597d\u5904\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u6027\u548c\u8ba1\u7b97\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5206\u5b50\u7cfb\u7edf\u7684\u91c7\u6837\u6548\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6d4b\u91cf\u5fae\u8c03\u671f\u95f4\u6d41\u8f68\u8ff9\u7684\u957f\u5ea6\uff0c\u8bc1\u660e\u8def\u5f84\u68af\u5ea6\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4fdd\u7559\u4e86\u6d41\u7684\u5b66\u4e60\u7ed3\u6784\u3002", "motivation": "\u5c3d\u7ba1Boltzmann Generators\u5df2\u7ecf\u53ef\u4ee5\u901a\u8fc7Normalizing Flows\u548c\u91cd\u8981\u6027\u52a0\u6743\u751f\u6210\u5206\u5b50\u7cfb\u7edf\u7684\u5e73\u8861\u5206\u5e03\u6837\u672c\uff0c\u4f46\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\u3002Flow Matching\u867d\u7136\u52a0\u901f\u4e86CNFs\u5e76\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u5206\u5b50\u7cfb\u7edf\uff0c\u4f46\u5728\u5df2\u77e5\u76ee\u6807\u80fd\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u4e9b\u521d\u6b65\u8bad\u7ec3\u7684CNFs\u4ecd\u9700\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u8def\u5f84\u68af\u5ea6\u5bf9\u901a\u8fc7Flow Matching\u521d\u6b65\u8bad\u7ec3\u7684Continuous Normalizing Flows\uff08CNFs\uff09\u8fdb\u884c\u5fae\u8c03\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86Flow Matching\u548c\u8def\u5f84\u68af\u5ea6\u7684\u4f18\u52bf\uff0c\u5728\u4e0d\u9700\u8981\u989d\u5916\u91c7\u6837\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u5316CNFs\u4ee5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u6d4b\u91cf\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6d41\u8f68\u8ff9\u7684\u957f\u5ea6\uff0c\u8bc4\u4f30\u8def\u5f84\u68af\u5ea6\u5bf9\u6d41\u5b66\u4e60\u7ed3\u6784\u7684\u4fdd\u7559\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528Flow Matching\u76f8\u6bd4\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u5c06\u5206\u5b50\u7cfb\u7edf\u7684\u91c7\u6837\u6548\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\u3002\u6b64\u5916\uff0c\u8def\u5f84\u68af\u5ea6\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u80fd\u591f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4fdd\u7559\u6d41\u7684\u5b66\u4e60\u7ed3\u6784\uff0c\u8fd9\u8bf4\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u4f7f\u7528\u8def\u5f84\u68af\u5ea6\u5fae\u8c03\u7531Flow Matching\u521d\u6b65\u8bad\u7ec3\u7684CNFs\u662f\u4e00\u79cd\u6709\u6548\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u6027\u548c\u8ba1\u7b97\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u5206\u5b50\u7cfb\u7edf\u7684\u91c7\u6837\u6548\u7387\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u5206\u5b50\u6a21\u62df\u548c\u5176\u4ed6\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.10160", "pdf": "https://arxiv.org/pdf/2505.10160", "abs": "https://arxiv.org/abs/2505.10160", "authors": ["Yannis Montreuil", "Axel Carlier", "Lai Xing Ng", "Wei Tsang Ooi"], "title": "One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which\nunifies prediction and deferral by learning a shared score-based model that\nselects the $k$ most cost-effective entities-labels or experts-per input. While\nexisting one-stage L2D methods are limited to deferring to a single expert, our\napproach jointly optimizes prediction and deferral across multiple entities\nthrough a single end-to-end objective. We define a cost-sensitive loss and\nderive a novel convex surrogate that is independent of the cardinality\nparameter $k$, enabling generalization across Top-$k$ regimes without\nretraining. Our formulation recovers the Top-1 deferral policy of prior\nscore-based methods as a special case, and we prove that our surrogate is both\nBayes-consistent and $\\mathcal{H}$-consistent under mild assumptions. We\nfurther introduce an adaptive variant, Top-$k(x)$, which dynamically selects\nthe number of consulted entities per input to balance predictive accuracy and\nconsultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage\nTop-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves\nsuperior accuracy-cost trade-offs by tailoring allocations to input complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u9636\u6bb5Top-k\u5b66\u4e60\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u9884\u6d4b\u548c\u5ef6\u8fdf\u51b3\u7b56\uff0c\u901a\u8fc7\u5171\u4eab\u7684\u57fa\u4e8e\u5f97\u5206\u7684\u6a21\u578b\u9009\u62e9\u6bcf\u4e2a\u8f93\u5165\u7684k\u4e2a\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u5b9e\u4f53\uff08\u6807\u7b7e\u6216\u4e13\u5bb6\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u5355\u4e00\u7aef\u5230\u7aef\u76ee\u6807\u4e0b\u8054\u5408\u4f18\u5316\u9884\u6d4b\u548c\u5ef6\u8fdf\u51b3\u7b56\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u4e0ek\u65e0\u5173\u7684\u6210\u672c\u654f\u611f\u635f\u5931\u51fd\u6570\u53ca\u5176\u51f8\u66ff\u4ee3\u51fd\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u53d8\u4f53Top-k(x)\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u8f93\u5165\u54a8\u8be2\u7684\u5b9e\u4f53\u6570\u91cf\u4ee5\u5e73\u8861\u9884\u6d4b\u51c6\u786e\u6027\u548c\u54a8\u8be2\u6210\u672c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684Top-1\u5ef6\u8fdf\u7b56\u7565\uff0c\u800cTop-k(x)\u901a\u8fc7\u6839\u636e\u8f93\u5165\u590d\u6742\u6027\u8c03\u6574\u5206\u914d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51c6\u786e\u6027-\u6210\u672c\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u9636\u6bb5\u5b66\u4e60\u5230\u5ef6\u8fdf\uff08L2D\uff09\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5c06\u51b3\u7b56\u5ef6\u8fdf\u7ed9\u5355\u4e00\u4e13\u5bb6\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u5b9e\u4f53\u60c5\u51b5\u4e0b\u7684\u9884\u6d4b\u548c\u5ef6\u8fdf\u51b3\u7b56\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4f18\u5316\u9884\u6d4b\u548c\u5ef6\u8fdf\u5e76\u9002\u7528\u4e8e\u591a\u4e2a\u5b9e\u4f53\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u9636\u6bb5Top-k Learning-to-Defer\u6846\u67b6\uff0c\u4f7f\u7528\u5171\u4eab\u7684\u57fa\u4e8e\u5f97\u5206\u7684\u6a21\u578b\u9009\u62e9k\u4e2a\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u5b9e\u4f53\uff08\u6807\u7b7e\u6216\u4e13\u5bb6\uff09\u30022. \u5b9a\u4e49\u4e86\u4e00\u4e2a\u6210\u672c\u654f\u611f\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u4e2a\u4e0d\u4f9d\u8d56\u4e8ek\u7684\u65b0\u578b\u51f8\u66ff\u4ee3\u51fd\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u4e0d\u540cTop-k\u573a\u666f\u7684\u6cdb\u5316\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u30023. \u5f15\u5165\u4e86\u81ea\u9002\u5e94\u53d8\u4f53Top-k(x)\uff0c\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u8f93\u5165\u54a8\u8be2\u7684\u5b9e\u4f53\u6570\u91cf\uff0c\u4ee5\u5e73\u8861\u9884\u6d4b\u51c6\u786e\u6027\u548c\u54a8\u8be2\u6210\u672c\u3002", "result": "1. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5355\u9636\u6bb5Top-k\u65b9\u6cd5\u5728CIFAR-10\u548cSVHN\u6570\u636e\u96c6\u4e0a\u4e25\u683c\u4f18\u4e8eTop-1\u5ef6\u8fdf\u7b56\u7565\u30022. Top-k(x)\u901a\u8fc7\u6839\u636e\u8f93\u5165\u590d\u6742\u6027\u8c03\u6574\u5206\u914d\uff0c\u5728\u51c6\u786e\u6027-\u6210\u672c\u6743\u8861\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5355\u9636\u6bb5Top-k Learning-to-Defer\u6846\u67b6\u53ca\u5176\u81ea\u9002\u5e94\u53d8\u4f53Top-k(x)\u4e3a\u591a\u5b9e\u4f53\u60c5\u51b5\u4e0b\u7684\u9884\u6d4b\u548c\u5ef6\u8fdf\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u5728\u51c6\u786e\u6027\u4e0e\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6743\u8861\u3002"}}
{"id": "2505.10522", "pdf": "https://arxiv.org/pdf/2505.10522", "abs": "https://arxiv.org/abs/2505.10522", "authors": ["Xinrui Wang", "Yan Jin"], "title": "Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has demonstrated remarkable potential in robotic\nmanipulation but faces challenges in sample inefficiency and lack of\ninterpretability, limiting its applicability in real world scenarios. Enabling\nthe agent to gain a deeper understanding and adapt more efficiently to diverse\nworking scenarios is crucial, and strategic knowledge utilization is a key\nfactor in this process. This paper proposes a Knowledge Capture, Adaptation,\nand Composition (KCAC) framework to systematically integrate knowledge transfer\ninto RL through cross-task curriculum learning. KCAC is evaluated using a two\nblock stacking task in the CausalWorld benchmark, a complex robotic\nmanipulation environment. To our knowledge, existing RL approaches fail to\nsolve this task effectively, reflecting deficiencies in knowledge capture. In\nthis work, we redesign the benchmark reward function by removing rigid\nconstraints and strict ordering, allowing the agent to maximize total rewards\nconcurrently and enabling flexible task completion. Furthermore, we define two\nself-designed sub-tasks and implement a structured cross-task curriculum to\nfacilitate efficient learning. As a result, our KCAC approach achieves a 40\npercent reduction in training time while improving task success rates by 10\npercent compared to traditional RL methods. Through extensive evaluation, we\nidentify key curriculum design parameters subtask selection, transition timing,\nand learning rate that optimize learning efficiency and provide conceptual\nguidance for curriculum based RL frameworks. This work offers valuable insights\ninto curriculum design in RL and robotic learning.", "AI": {"tldr": "An abstract about a Knowledge Capture, Adaptation, and Composition (KCAC) framework to improve reinforcement learning in robotic manipulation.", "motivation": "Reinforcement learning has shown great potential but is limited by sample inefficiency and lack of interpretability. To address these challenges and enhance the agent's understanding and adaptability, there is a need for strategic knowledge utilization.", "method": "The KCAC framework integrates knowledge transfer into RL via cross-task curriculum learning. It involves redesigning the benchmark reward function for flexibility, defining sub-tasks, and implementing a structured curriculum to facilitate efficient learning.", "result": "KCAC achieves a 40% reduction in training time and improves task success rates by 10% compared to traditional RL methods. Key curriculum design parameters are identified for optimizing learning efficiency.", "conclusion": "This work provides valuable insights into curriculum design in reinforcement learning and robotic learning, offering conceptual guidance for future curriculum-based RL frameworks."}}
{"id": "2505.10537", "pdf": "https://arxiv.org/pdf/2505.10537", "abs": "https://arxiv.org/abs/2505.10537", "authors": ["Filippo Olimpieri", "Noemi Giustini", "Andrea Lacava", "Salvatore D'Oro", "Tommaso Melodia", "Francesca Cuomo"], "title": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps", "categories": ["cs.NI", "cs.AI"], "comment": "6 pages, 5 figures, 2 tables", "summary": "The O-RAN architecture is transforming cellular networks by adopting RAN\nsoftwarization and disaggregation concepts to enable data-driven monitoring and\ncontrol of the network. Such management is enabled by RICs, which facilitate\nnear-real-time and non-real-time network control through xApps and rApps.\nHowever, they face limitations, including latency overhead in data exchange\nbetween the RAN and RIC, restricting real-time monitoring, and the inability to\naccess user plain data due to privacy and security constraints, hindering use\ncases like beamforming and spectrum classification. In this paper, we leverage\nthe dApps concept to enable real-time RF spectrum classification with LibIQ, a\nnovel library for RF signals that facilitates efficient spectrum monitoring and\nsignal classification by providing functionalities to read I/Q samples as\ntime-series, create datasets and visualize time-series data through plots and\nspectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to\ndetect external RF signals, which are subsequently classified using a CNN\ninside the library. To achieve accurate spectrum analysis, we created an\nextensive dataset of time-series-based I/Q samples, representing distinct\nsignal types captured using a custom dApp running on a 5G deployment over the\nColosseum network emulator and an OTA testbed. We evaluate our model by\ndeploying LibIQ in heterogeneous scenarios with varying center frequencies,\ntime windows, and external RF signals. In real-time analysis, the model\nclassifies the processed I/Q samples, achieving an average accuracy of\napproximately 97.8\\% in identifying signal types across all scenarios. We\npledge to release both LibIQ and the dataset created as a publicly available\nframework upon acceptance.", "AI": {"tldr": "The paper introduces LibIQ, a library for RF signals that uses dApps concept to enable real-time RF spectrum classification. It processes I/Q samples to detect and classify external RF signals using a CNN, achieving an average accuracy of 97.8%. The authors plan to release LibIQ and the dataset publicly.", "motivation": "To overcome limitations in current RICs, such as latency overhead in data exchange and inability to access user plain data, which restrict real-time monitoring and certain use cases like beamforming and spectrum classification.", "method": "Leverage the dApps concept and develop LibIQ, a novel library for RF signals that provides functionalities to read I/Q samples as time-series, create datasets, visualize data through plots and spectrograms, and classify signals using a CNN.", "result": "Achieved an average accuracy of approximately 97.8% in identifying signal types across all scenarios in real-time analysis when deploying LibIQ in heterogeneous scenarios with varying conditions.", "conclusion": "LibIQ enables efficient and accurate real-time RF spectrum classification and the authors intend to release it along with the created dataset as a public framework."}}
{"id": "2505.10547", "pdf": "https://arxiv.org/pdf/2505.10547", "abs": "https://arxiv.org/abs/2505.10547", "authors": ["Milan Ganai", "Rohan Sinha", "Christopher Agia", "Daniel Morton", "Marco Pavone"], "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "Website: https://milanganai.github.io/fortress/", "summary": "Foundation models can provide robust high-level reasoning on appropriate\nsafety interventions in hazardous scenarios beyond a robot's training data,\ni.e. out-of-distribution (OOD) failures. However, due to the high inference\nlatency of Large Vision and Language Models, current methods rely on manually\ndefined intervention policies to enact fallbacks, thereby lacking the ability\nto plan generalizable, semantically safe motions. To overcome these challenges\nwe present FORTRESS, a framework that generates and reasons about semantically\nsafe fallback strategies in real time to prevent OOD failures. At a low\nfrequency in nominal operations, FORTRESS uses multi-modal reasoners to\nidentify goals and anticipate failure modes. When a runtime monitor triggers a\nfallback response, FORTRESS rapidly synthesizes plans to fallback goals while\ninferring and avoiding semantically unsafe regions in real time. By bridging\nopen-world, multi-modal reasoning with dynamics-aware planning, we eliminate\nthe need for hard-coded fallbacks and human safety interventions. FORTRESS\noutperforms on-the-fly prompting of slow reasoning models in safety\nclassification accuracy on synthetic benchmarks and real-world ANYmal robot\ndata, and further improves system safety and planning success in simulation and\non quadrotor hardware for urban navigation.", "AI": {"tldr": "FORTRESS is a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures, outperforming slow reasoning models in safety classification accuracy and improving system safety and planning success.", "motivation": "Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, but current methods lack the ability to plan generalizable, semantically safe motions due to high inference latency of Large Vision and Language Models.", "method": "FORTRESS uses multi-modal reasoners at a low frequency in nominal operations to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time.", "result": "FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation.", "conclusion": "By bridging open-world, multi-modal reasoning with dynamics-aware planning, FORTRESS eliminates the need for hard-coded fallbacks and human safety interventions."}}
{"id": "2505.10279", "pdf": "https://arxiv.org/pdf/2505.10279", "abs": "https://arxiv.org/abs/2505.10279", "authors": ["Gabriel R. Palma", "Sally McClean", "Brahim Allan", "Zeeshan Tariq", "Rafael A. Moral"], "title": "Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging", "categories": ["stat.ME", "cs.LG"], "comment": "21 pages", "summary": "TV customers today face many choices from many live channels and on-demand\nservices. Providing a personalised experience that saves customers time when\ndiscovering content is essential for TV providers. However, a reliable\nunderstanding of their behaviour and preferences is key. When creating\npersonalised recommendations for TV, the biggest challenge is understanding\nviewing behaviour within households when multiple people are watching. The\nobjective is to detect and combine individual profiles to make\nbetter-personalised recommendations for group viewing. Our challenge is that we\nhave little explicit information about who is watching the devices at any time\n(individuals or groups). Also, we do not have a way to combine more than one\nindividual profile to make better recommendations for group viewing. We propose\na novel framework using a Gaussian mixture model averaging to obtain point\nestimates for the number of household TV profiles and a Bayesian random walk\nmodel to introduce uncertainty. We applied our approach using data from real\ncustomers whose TV-watching data totalled approximately half a million\nobservations. Our results indicate that combining our framework with the\nselected features provides a means to estimate the number of household TV\nprofiles and their characteristics, including shifts over time and\nquantification of uncertainty.", "AI": {"tldr": "TV providers need to offer personalized experiences for customers. A key challenge is understanding group viewing behavior within households. This paper proposes a novel framework using Gaussian mixture model averaging and Bayesian random walk to estimate the number of household TV profiles and their characteristics from real customer data.", "motivation": "To provide better-personalised recommendations for group viewing in households, it is essential to understand the behaviour and preferences of multiple people watching TV together.", "method": "The method involves using a Gaussian mixture model averaging to obtain point estimates for the number of household TV profiles and a Bayesian random walk model to introduce uncertainty.", "result": "The results show that the framework can estimate the number of household TV profiles and their characteristics, including shifts over time and quantification of uncertainty, when applied to real customer data with approximately half a million observations.", "conclusion": "Combining the proposed framework with selected features provides an effective way to understand and predict group viewing behavior."}}
{"id": "2505.10319", "pdf": "https://arxiv.org/pdf/2505.10319", "abs": "https://arxiv.org/abs/2505.10319", "authors": ["John Nicol", "Markus Frohme"], "title": "Deconstructing Subset Construction -- Reducing While Determinizing", "categories": ["cs.FL", "cs.LG"], "comment": "19 pages, 2 figures", "summary": "We present a novel perspective on the NFA canonization problem, which\nintroduces intermediate minimization steps to reduce the exploration space\non-the-fly. Essential to our approach are so-called equivalence registries\nwhich manage information about equivalent states and allow for incorporating\nfurther optimization techniques such as convexity closures or simulation to\nboost performance. Due to the generality of our approach, these concepts can be\nembedded in classic subset construction or Brzozowski's approach. We evaluate\nour approach on a set of real-world examples from automatic sequences and\nobserve that we are able to improve especially worst-case scenarios. We\nimplement our approach in an open-source library for users to experiment with.", "AI": {"tldr": "The paper presents a new method for NFA canonization using intermediate minimization steps and equivalence registries, which can be integrated into existing approaches, showing improvements in worst-case scenarios.", "motivation": "To reduce the exploration space during NFA canonization and improve performance, especially in worst-case scenarios.", "method": "Introducing intermediate minimization steps and utilizing equivalence registries to manage equivalent states, allowing for additional optimizations like convexity closures or simulation.", "result": "Evaluation on real-world examples shows improvement in worst-case scenarios for NFA canonization.", "conclusion": "The novel approach is effective and can be embedded in classic methods, with an open-source implementation available for experimentation."}}
{"id": "2505.10367", "pdf": "https://arxiv.org/pdf/2505.10367", "abs": "https://arxiv.org/abs/2505.10367", "authors": ["Chuanqing Pu", "Feilong Fan", "Nengling Tai", "Songyuan Liu", "Jinming Yu"], "title": "A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy Trading in HEFTCom2024", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "Solution description of IEEE Hybrid Energy Forecasting and Trading\n  Competition (HEFTCom)", "summary": "Obtaining accurate probabilistic energy forecasts and making effective\ndecisions amid diverse uncertainties are routine challenges in future energy\nsystems. This paper presents the solution of team GEB, which ranked 3rd in\ntrading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid\nEnergy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution\nprovides accurate probabilistic forecasts for a wind-solar hybrid system, and\nachieves substantial trading revenue in the day-ahead electricity market. Key\ncomponents include: (1) a stacking-based approach combining sister forecasts\nfrom various Numerical Weather Predictions (NWPs) to provide wind power\nforecasts, (2) an online solar post-processing model to address the\ndistribution shift in the online test set caused by increased solar capacity,\n(3) a probabilistic aggregation method for accurate quantile forecasts of\nhybrid generation, and (4) a stochastic trading strategy to maximize expected\ntrading revenue considering uncertainties in electricity prices. This paper\nalso explores the potential of end-to-end learning to further enhance the\ntrading revenue by adjusting the distribution of forecast errors. Detailed case\nstudies are provided to validate the effectiveness of these proposed methods.\nCode for all mentioned methods is available for reproduction and further\nresearch in both industry and academia.", "AI": {"tldr": "This paper presents team GEB's solution that ranked 3rd in trading, 4th in forecasting, and 1st among student teams in HEFTCom2024. They provide accurate probabilistic forecasts for a wind-solar hybrid system and achieve substantial trading revenue.", "motivation": "To address the challenges of obtaining accurate probabilistic energy forecasts and making effective decisions amid diverse uncertainties in future energy systems.", "method": "Key components include: (1) a stacking-based approach combining sister forecasts from various NWPs to provide wind power forecasts; (2) an online solar post-processing model to address the distribution shift in the online test set caused by increased solar capacity; (3) a probabilistic aggregation method for accurate quantile forecasts of hybrid generation; (4) a stochastic trading strategy to maximize expected trading revenue considering uncertainties in electricity prices.", "result": "Substantial trading revenue in the day-ahead electricity market was achieved.", "conclusion": "The proposed methods were validated through detailed case studies, and code is available for reproduction and further research."}}
{"id": "2505.10398", "pdf": "https://arxiv.org/pdf/2505.10398", "abs": "https://arxiv.org/abs/2505.10398", "authors": ["Alexandre Banks", "Randy Moore", "Sayem Nazmuz Zaman", "Alaa Eldin Abdelaal", "Septimiu E. Salcudean"], "title": "AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera in Surgical Robotics", "categories": ["cs.RO", "cs.HC", "cs.LG", "cs.SY", "eess.SP", "eess.SY", "J.3.2; J.2.7; I.2.9"], "comment": "13 pages, 9 figures", "summary": "Incorporating an autonomous auxiliary camera into robot-assisted minimally\ninvasive surgery (RAMIS) enhances spatial awareness and eliminates manual\nviewpoint control. Existing path planning methods for auxiliary cameras track\ntwo-dimensional surgical features but do not simultaneously account for camera\norientation, workspace constraints, and robot joint limits. This study presents\nAutoCam: an automatic auxiliary camera placement method to improve\nvisualization in RAMIS. Implemented on the da Vinci Research Kit, the system\nuses a priority-based, workspace-constrained control algorithm that combines\nheuristic geometric placement with nonlinear optimization to ensure robust\ncamera tracking. A user study (N=6) demonstrated that the system maintained\n99.84% visibility of a salient feature and achieved a pose error of 4.36 $\\pm$\n2.11 degrees and 1.95 $\\pm$ 5.66 mm. The controller was computationally\nefficient, with a loop time of 6.8 $\\pm$ 12.8 ms. An additional pilot study\n(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training\ntask, suggests that users can teleoperate just as effectively from AutoCam's\nviewpoint as from the endoscope's while still benefiting from AutoCam's\nimproved visual coverage of the scene. These results indicate that an auxiliary\ncamera can be autonomously controlled using the da Vinci patient-side\nmanipulators to track a salient feature, laying the groundwork for new\nmulti-camera visualization methods in RAMIS.", "AI": {"tldr": "The paper introduces AutoCam, an automatic auxiliary camera placement method for RAMIS using da Vinci Research Kit. It ensures robust camera tracking with high visibility and low pose error while being computationally efficient.", "motivation": "To enhance spatial awareness in RAMIS by incorporating an autonomous auxiliary camera that eliminates manual viewpoint control and addresses limitations of existing path planning methods which do not simultaneously account for camera orientation, workspace constraints, and robot joint limits.", "method": "AutoCam uses a priority-based, workspace-constrained control algorithm combining heuristic geometric placement with nonlinear optimization. Implemented on the da Vinci Research Kit, it tracks a salient feature autonomously.", "result": "Maintained 99.84% visibility of a salient feature with a pose error of 4.36 \u00b1 2.11 degrees and 1.95 \u00b1 5.66 mm. The controller was computationally efficient with a loop time of 6.8 \u00b1 12.8 ms. Novices performed equally well using AutoCam's viewpoint as with the endoscope's during a training task.", "conclusion": "An auxiliary camera can be autonomously controlled using the da Vinci patient-side manipulators to track a salient feature, paving the way for new multi-camera visualization methods in RAMIS."}}
{"id": "2505.10444", "pdf": "https://arxiv.org/pdf/2505.10444", "abs": "https://arxiv.org/abs/2505.10444", "authors": ["Miguel Aguilera", "Sosuke Ito", "Artemy Kolchinsky"], "title": "Inferring entropy production in many-body systems using nonequilibrium MaxEnt", "categories": ["cond-mat.stat-mech", "cs.LG", "nlin.AO", "q-bio.NC"], "comment": null, "summary": "We propose a method for inferring entropy production (EP) in high-dimensional\nstochastic systems, including many-body systems and non-Markovian systems with\nlong memory. Standard techniques for estimating EP become intractable in such\nsystems due to computational and statistical limitations. We infer\ntrajectory-level EP and lower bounds on average EP by exploiting a\nnonequilibrium analogue of the Maximum Entropy principle, along with convex\nduality. Our approach uses only samples of trajectory observables (such as\nspatiotemporal correlation functions). It does not require reconstruction of\nhigh-dimensional probability distributions or rate matrices, nor any special\nassumptions such as discrete states or multipartite dynamics. It may be used to\ncompute a hierarchical decomposition of EP, reflecting contributions from\ndifferent kinds of interactions, and it has an intuitive physical\ninterpretation as a thermodynamic uncertainty relation. We demonstrate its\nnumerical performance on a disordered nonequilibrium spin model with 1000 spins\nand a large neural spike-train dataset.", "AI": {"tldr": "A method for inferring entropy production in high-dimensional stochastic systems is proposed, which uses trajectory observables and doesn't require probability distributions or special assumptions. It has intuitive physical interpretation and can be used for hierarchical decomposition of EP.", "motivation": "Standard techniques for estimating entropy production become intractable in high-dimensional stochastic systems due to computational and statistical limitations.", "method": "Infer trajectory-level EP and lower bounds on average EP by exploiting a nonequilibrium analogue of the Maximum Entropy principle, along with convex duality, using samples of trajectory observables.", "result": "Demonstrated numerical performance on a disordered nonequilibrium spin model with 1000 spins and a large neural spike-train dataset.", "conclusion": "The proposed method provides an intuitive physical interpretation as a thermodynamic uncertainty relation and may be used to compute a hierarchical decomposition of EP."}}
{"id": "2505.10448", "pdf": "https://arxiv.org/pdf/2505.10448", "abs": "https://arxiv.org/abs/2505.10448", "authors": ["Conor Rosato", "Harvinder Lehal", "Simon Maskell", "Lee Devlin", "Malcolm Strens"], "title": "Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods", "categories": ["stat.ML", "cs.LG"], "comment": "45 pages", "summary": "Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when\nthe likelihood function is irregular and expensive to compute. We explore\nseveral sampling algorithms that make use of subset evaluations to reduce\ncomputational overhead. We adapt the subset samplers for this setting where\ngradient information is not available or is unreliable. To achieve this, we\nintroduce data-driven proxies in place of Taylor expansions and define a novel\ncomputation-cost aware adaptive controller. We undertake an extensive\nevaluation for a challenging disease modelling task and a configurable task\nwith similar irregularity in the likelihood surface. We find our improved\nversion of Hierarchical Importance with Nested Training Samples (HINTS), with\nadaptive proposals and a data-driven proxy, obtains the best sampling error in\na fixed computational budget. We conclude that subset evaluations can provide\ncheap and naturally-tempered exploration, while a data-driven proxy can\npre-screen proposals successfully in explored regions of the state space. These\ntwo elements combine through hierarchical delayed acceptance to achieve\nefficient, exact sampling.", "AI": {"tldr": "The paper investigates sampling algorithms using subset evaluations to reduce computational overhead in Bayesian inference with MCMC for irregular and expensive likelihood functions. An improved version of HINTS with adaptive proposals and a data-driven proxy shows the best performance in a fixed computational budget.", "motivation": "Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when dealing with irregular and costly likelihood functions.", "method": "Adapt subset samplers without gradient information, introduce data-driven proxies instead of Taylor expansions, and define a novel computation-cost aware adaptive controller. Use hierarchical delayed acceptance for efficient exact sampling.", "result": "Improved HINTS obtains the best sampling error within a fixed computational budget. Subset evaluations provide cost-effective exploration and data-driven proxies successfully pre-screen proposals.", "conclusion": "Subset evaluations can offer cheap and tempered exploration while data-driven proxies can effectively pre-screen proposals leading to efficient exact sampling through hierarchical delayed acceptance."}}
{"id": "2505.10466", "pdf": "https://arxiv.org/pdf/2505.10466", "abs": "https://arxiv.org/abs/2505.10466", "authors": ["Juehang Qin", "Shixiao Liang", "Christopher Tunnell"], "title": "FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": "10 pages, 5 figures, and 2 tables in main text, two appendices", "summary": "Multi-modal and high-dimensional posteriors present significant challenges\nfor variational inference, causing mode-seeking behavior and collapse despite\nthe theoretical expressiveness of normalizing flows. Traditional annealing\nmethods require temperature schedules and hyperparameter tuning, falling short\nof the goal of truly black-box variational inference. We introduce FlowVAT, a\nconditional tempering approach for normalizing flow variational inference that\naddresses these limitations. Our method tempers both the base and target\ndistributions simultaneously, maintaining affine-invariance under tempering. By\nconditioning the normalizing flow on temperature, we leverage overparameterized\nneural networks' generalization capabilities to train a single flow\nrepresenting the posterior across a range of temperatures. This preserves modes\nidentified at higher temperatures when sampling from the variational posterior\nat $T = 1$, mitigating standard variational methods' mode-seeking behavior. In\nexperiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT\noutperforms traditional and adaptive annealing methods, finding more modes and\nachieving better ELBO values, particularly in higher dimensions where existing\napproaches fail. Our method requires minimal hyperparameter tuning and does not\nrequire an annealing schedule, advancing toward fully-automatic black-box\nvariational inference for complicated posteriors.", "AI": {"tldr": "FlowVAT is a new method for normalizing flow variational inference that tempers both base and target distributions simultaneously, overcoming mode-seeking behavior and posterior collapse in multi-modal and high-dimensional problems. It outperforms traditional methods in experiments with multi-modal distributions and moves toward fully-automatic black-box variational inference.", "motivation": "Variational inference faces challenges with multi-modal and high-dimensional posteriors, leading to mode-seeking behavior and posterior collapse. Traditional annealing methods require temperature schedules and hyperparameter tuning, which hinders the goal of truly black-box variational inference.", "method": "FlowVAT uses a conditional tempering approach where both the base and target distributions are tempered simultaneously while maintaining affine-invariance. The normalizing flow is conditioned on temperature, allowing it to generalize across a range of temperatures using overparameterized neural networks. This single flow represents the posterior across different temperatures.", "result": "In experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT finds more modes and achieves better ELBO values compared to traditional and adaptive annealing methods, particularly excelling in higher dimensions where existing approaches fail.", "conclusion": "FlowVAT advances the field towards fully-automatic black-box variational inference for complicated posteriors by requiring minimal hyperparameter tuning and no annealing schedule."}}
{"id": "2505.10498", "pdf": "https://arxiv.org/pdf/2505.10498", "abs": "https://arxiv.org/abs/2505.10498", "authors": ["Sakshi Arya"], "title": "Batched Nonparametric Bandits via k-Nearest Neighbor UCB", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "68T05, 62L05, 62G08, 68Q32", "F.2.2; I.2.6"], "comment": "25 pages, 6 figures", "summary": "We study sequential decision-making in batched nonparametric contextual\nbandits, where actions are selected over a finite horizon divided into a small\nnumber of batches. Motivated by constraints in domains such as medicine and\nmarketing -- where online feedback is limited -- we propose a nonparametric\nalgorithm that combines adaptive k-nearest neighbor (k-NN) regression with the\nupper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully\nnonparametric, adapts to the context dimension, and is simple to implement.\nUnlike prior work relying on parametric or binning-based estimators, BaNk-UCB\nuses local geometry to estimate rewards and adaptively balances exploration and\nexploitation. We provide near-optimal regret guarantees under standard\nLipschitz smoothness and margin assumptions, using a theoretically motivated\nbatch schedule that balances regret across batches and achieves minimax-optimal\nrates. Empirical evaluations on synthetic and real-world datasets demonstrate\nthat BaNk-UCB consistently outperforms binning-based baselines.", "AI": {"tldr": "The paper introduces BaNk-UCB, a nonparametric algorithm combining k-NN regression with UCB principle for batched contextual bandits, achieving near-optimal regret and outperforming baselines in experiments.", "motivation": "To address the challenges of sequential decision-making in contexts like medicine and marketing where online feedback is limited, especially when actions are taken over a finite horizon divided into a small number of batches.", "method": "Proposes BaNk-UCB, which uses adaptive k-nearest neighbor regression combined with the upper confidence bound principle. The method leverages local geometry for reward estimation and adaptively balances exploration and exploitation.", "result": "Provides near-optimal regret guarantees under standard assumptions and demonstrates superior performance compared to binning-based methods through empirical evaluations on both synthetic and real-world datasets.", "conclusion": "BaNk-UCB is a fully nonparametric algorithm that adapts to context dimensions, achieves minimax-optimal rates, and performs well across different datasets."}}
{"id": "2505.10511", "pdf": "https://arxiv.org/pdf/2505.10511", "abs": "https://arxiv.org/abs/2505.10511", "authors": ["Victor Zheleznov", "Stefan Bilbao", "Alec Wright", "Simon King"], "title": "Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations", "categories": ["cs.SD", "cs.LG", "eess.AS", "physics.comp-ph"], "comment": "Accepted for publication in Proceedings of the 28th International\n  Conference on Digital Audio Effects (DAFx25), Ancona, Italy, September 2025", "summary": "Modal synthesis methods are a long-standing approach for modelling\ndistributed musical systems. In some cases extensions are possible in order to\nhandle geometric nonlinearities. One such case is the high-amplitude vibration\nof a string, where geometric nonlinear effects lead to perceptually important\neffects including pitch glides and a dependence of brightness on striking\namplitude. A modal decomposition leads to a coupled nonlinear system of\nordinary differential equations. Recent work in applied machine learning\napproaches (in particular neural ordinary differential equations) has been used\nto model lumped dynamic systems such as electronic circuits automatically from\ndata. In this work, we examine how modal decomposition can be combined with\nneural ordinary differential equations for modelling distributed musical\nsystems. The proposed model leverages the analytical solution for linear\nvibration of system's modes and employs a neural network to account for\nnonlinear dynamic behaviour. Physical parameters of a system remain easily\naccessible after the training without the need for a parameter encoder in the\nnetwork architecture. As an initial proof of concept, we generate synthetic\ndata for a nonlinear transverse string and show that the model can be trained\nto reproduce the nonlinear dynamics of the system. Sound examples are\npresented.", "AI": {"tldr": "The paper explores combining modal decomposition with neural ordinary differential equations to model distributed musical systems, demonstrating its ability to reproduce nonlinear dynamics using synthetic data of a nonlinear string.", "motivation": "To address the challenge of modeling geometric nonlinearities in distributed musical systems, such as pitch glides and brightness changes in high-amplitude string vibrations.", "method": "Combine modal decomposition with neural ordinary differential equations. Use an analytical solution for linear vibration modes and a neural network for nonlinear dynamics. Avoid the need for a parameter encoder in the network architecture.", "result": "Successfully trained the model on synthetic data of a nonlinear transverse string to reproduce the system's nonlinear dynamics.", "conclusion": "This approach provides a proof of concept for modeling nonlinear musical systems while keeping physical parameters accessible."}}
{"id": "2505.08828", "pdf": "https://arxiv.org/pdf/2505.08828", "abs": "https://arxiv.org/abs/2505.08828", "authors": ["Eduardo Araujo Oliveira", "Madhavi Mohoni", "Sonsoles L\u00f3pez-Pernas", "Mohammed Saqr"], "title": "Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 pages, 10 figures, 11 tables", "summary": "As human-AI collaboration becomes increasingly prevalent in educational\ncontexts, understanding and measuring the extent and nature of such\ninteractions pose significant challenges. This research investigates the use of\nauthorship verification (AV) techniques not as a punitive measure, but as a\nmeans to quantify AI assistance in academic writing, with a focus on promoting\ntransparency, interpretability, and student development. Building on prior\nwork, we structured our investigation into three stages: dataset selection and\nexpansion, AV method development, and systematic evaluation. Using three\ndatasets - including a public dataset (PAN-14) and two from University of\nMelbourne students from various courses - we expanded the data to include\nLLM-generated texts, totalling 1,889 documents and 540 authorship problems from\n506 students. We developed an adapted Feature Vector Difference AV methodology\nto construct robust academic writing profiles for students, designed to capture\nmeaningful, individual characteristics of their writing. The method's\neffectiveness was evaluated across multiple scenarios, including distinguishing\nbetween student-authored and LLM-generated texts and testing resilience against\nLLMs' attempts to mimic student writing styles. Results demonstrate the\nenhanced AV classifier's ability to identify stylometric discrepancies and\nmeasure human-AI collaboration at word and sentence levels while providing\neducators with a transparent tool to support academic integrity investigations.\nThis work advances AV technology, offering actionable insights into the\ndynamics of academic writing in an AI-driven era.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f5c\u8005\u8eab\u4efd\u9a8c\u8bc1\u6280\u672f\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u6d4b\u91cf\u4eba\u673a\u534f\u4f5c\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6784\u5efa\u5b66\u751f\u5199\u4f5c\u6863\u6848\u5e76\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u4e3aAI\u9a71\u52a8\u65f6\u4ee3\u7684\u5b66\u672f\u8bda\u4fe1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u968f\u7740\u4eba\u673a\u534f\u4f5c\u5728\u6559\u80b2\u73af\u5883\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u7406\u89e3\u5e76\u8861\u91cf\u8fd9\u79cd\u4e92\u52a8\u7684\u7a0b\u5ea6\u548c\u6027\u8d28\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f5c\u8005\u8eab\u4efd\u9a8c\u8bc1\uff08AV\uff09\u6280\u672f\u7684\u4f7f\u7528\uff0c\u4e0d\u4ec5\u4f5c\u4e3a\u60e9\u7f5a\u63aa\u65bd\uff0c\u800c\u4e14\u4f5c\u4e3a\u91cf\u5316AI\u8f85\u52a9\u5b66\u672f\u5199\u4f5c\u7684\u624b\u6bb5\uff0c\u91cd\u70b9\u5173\u6ce8\u4fc3\u8fdb\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b66\u751f\u53d1\u5c55\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u7684\u7279\u5f81\u5411\u91cf\u5dee\u5f02AV\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u5b66\u751f\u7a33\u5065\u7684\u5b66\u672f\u5199\u4f5c\u6863\u6848\uff0c\u65e8\u5728\u6355\u6349\u5176\u5199\u4f5c\u4e2d\u7684\u6709\u610f\u4e49\u7684\u3001\u4e2a\u4eba\u7684\u7279\u5f81\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u7684AV\u5206\u7c7b\u5668\u80fd\u591f\u8bc6\u522b\u98ce\u683c\u5b66\u5dee\u5f02\uff0c\u5e76\u5728\u5355\u8bcd\u548c\u53e5\u5b50\u5c42\u9762\u6d4b\u91cf\u4eba\u673a\u534f\u4f5c\uff0c\u540c\u65f6\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u7684\u5de5\u5177\u6765\u652f\u6301\u5b66\u672f\u8bda\u4fe1\u8c03\u67e5\u3002", "conclusion": "\u672c\u7814\u7a76\u63a8\u8fdb\u4e86AV\u6280\u672f\uff0c\u4e3aAI\u9a71\u52a8\u65f6\u4ee3\u7684\u5b66\u672f\u5199\u4f5c\u52a8\u6001\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.08891", "pdf": "https://arxiv.org/pdf/2505.08891", "abs": "https://arxiv.org/abs/2505.08891", "authors": ["Daeun Hwang", "Samuel Shields", "Alex Calderwood", "Shi Johnson-Bey", "Michael Mateas", "Noah Wardrip-Fruin", "Edward F. Melcer"], "title": "Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives", "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, 1 appendix. Workshop paper, CHI 2025\n  Augmented Educators and AI", "summary": "Motivation is an important factor underlying successful learning. Previous\nresearch has demonstrated the positive effects that static interactive\nnarrative games can have on motivation. Concurrently, advances in AI have made\ndynamic and adaptive approaches to interactive narrative increasingly\naccessible. However, limited work has explored the impact that dynamic\nnarratives can have on learner motivation. In this paper, we compare two\nversions of Academical, a choice-based educational interactive narrative game\nabout research ethics. One version employs a traditional hand-authored\nbranching plot (i.e., static narrative) while the other dynamically sequences\nplots during play (i.e., dynamic narrative). Results highlight the importance\nof responsive content and a variety of choices for player engagement, while\nalso illustrating the challenge of balancing pedagogical goals with the dynamic\naspects of narrative. We also discuss design implications that arise from these\nfindings. Ultimately, this work provides initial steps to illuminate the\nemerging potential of AI-driven dynamic narrative in educational games.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u9759\u6001\u548c\u52a8\u6001\u53d9\u4e8b\u5bf9\u5b66\u4e60\u8005\u52a8\u673a\u7684\u5f71\u54cd\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u4e0a\u7684\u542f\u793a\u3002", "motivation": "\u52a8\u673a\u662f\u6210\u529f\u5b66\u4e60\u7684\u91cd\u8981\u56e0\u7d20\u3002\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u8bc1\u660e\u4e86\u9759\u6001\u4e92\u52a8\u53d9\u4e8b\u6e38\u620f\u5bf9\u52a8\u673a\u7684\u79ef\u6781\u5f71\u54cd\u3002\u540c\u65f6\uff0cAI\u7684\u8fdb\u6b65\u4f7f\u5f97\u52a8\u6001\u548c\u81ea\u9002\u5e94\u7684\u4e92\u52a8\u53d9\u4e8b\u65b9\u6cd5\u53d8\u5f97\u66f4\u52a0\u53ef\u884c\u3002\u7136\u800c\uff0c\u6709\u9650\u7684\u5de5\u4f5c\u63a2\u8ba8\u4e86\u52a8\u6001\u53d9\u4e8b\u5bf9\u5b66\u4e60\u8005\u52a8\u673a\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u7248\u672c\u7684Academical\uff0c\u4e00\u79cd\u662f\u4f20\u7edf\u7684\u624b\u5de5\u7f16\u5199\u7684\u5206\u652f\u5267\u60c5\uff08\u9759\u6001\u53d9\u4e8b\uff09\uff0c\u53e6\u4e00\u79cd\u662f\u5728\u6e38\u620f\u4e2d\u52a8\u6001\u5e8f\u5217\u5267\u60c5\uff08\u52a8\u6001\u53d9\u4e8b\uff09\u3002", "result": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u54cd\u5e94\u5f0f\u5185\u5bb9\u548c\u591a\u6837\u9009\u62e9\u5bf9\u4e8e\u73a9\u5bb6\u53c2\u4e0e\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u4e5f\u8bf4\u660e\u4e86\u5e73\u8861\u6559\u5b66\u76ee\u6807\u4e0e\u53d9\u4e8b\u52a8\u6001\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3aAI\u9a71\u52a8\u7684\u52a8\u6001\u53d9\u4e8b\u5728\u6559\u80b2\u6e38\u620f\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u63d0\u4f9b\u4e86\u521d\u6b65\u6b65\u9aa4\u3002"}}
{"id": "2505.08996", "pdf": "https://arxiv.org/pdf/2505.08996", "abs": "https://arxiv.org/abs/2505.08996", "authors": ["Adele E Goldberg", "Supantho Rakshit", "Jennifer Hu", "Kyle Mahowald"], "title": "A suite of LMs comprehend puzzle statements as well as humans", "categories": ["cs.CL"], "comment": null, "summary": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53ef\u80fd\u5e76\u4e0d\u900a\u8272\u4e8e\u4eba\u7c7b\uff0c\u4e14\u73b0\u6709\u6a21\u578b\u7684\u80fd\u529b\u53ef\u80fd\u88ab\u4f4e\u4f30\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6539\u8fdb\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u6700\u8fd1\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728\u7406\u89e3\u7b80\u5355\u82f1\u8bed\u9648\u8ff0\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u7684\u4e3b\u5f20\uff0c\u5e76\u63a2\u8ba8\u4eba\u7c7b\u8868\u73b0\u662f\u5426\u88ab\u9ad8\u4f30\uff0c\u800c\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u662f\u5426\u88ab\u4f4e\u4f30\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u76f8\u540c\u7684\u523a\u6fc0\u6750\u6599\uff0c\u8fdb\u884c\u4e86\u4e00\u9879\u9884\u6ce8\u518c\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4eba\u7c7b\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u7684\u53cd\u5e94\uff1a\u4e00\u79cd\u5141\u8bb8\u91cd\u8bfb\uff08\u590d\u5236\u539f\u59cb\u7814\u7a76\uff09\uff0c\u53e6\u4e00\u79cd\u9650\u5236\u91cd\u8bfb\uff08\u66f4\u81ea\u7136\u7684\u9605\u8bfb\u6d4b\u8bd5\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528Llama-2-70B\u7684\u65e5\u5fd7\u6982\u7387\u3001\u5bf9\u5f00\u653e\u5f0f\u6a21\u578b\u54cd\u5e94\u7684\u91cd\u65b0\u7f16\u7801\u4ee5\u53ca\u5bf9\u5176\u4ed6\u53e5\u5b50\u7684\u8bed\u6cd5\u6027\u8bc4\u5206\u8fdb\u884c\u4e86\u989d\u5916\u5206\u6790\u3002", "result": "\u5f53\u5141\u8bb8\u91cd\u8bfb\u65f6\uff0c\u4eba\u7c7b\u7684\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0873%\uff09\uff0c\u4f4e\u4e8eFalcon-180B-Chat\uff0876%\uff09\u548cGPT-4\uff0881%\uff09\u3002\u6700\u65b0\u7684GPT-o1\u6a21\u578b\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u4eba\u7c7b\u548c\u6a21\u578b\u5728\u6d89\u53ca\u53ef\u80fd\u4e92\u60e0\u884c\u4e3a\u7684\u67e5\u8be2\uff08\u5982\u63a5\u543b\uff09\u4e0a\u90fd\u9762\u4e34\u66f4\u5927\u7684\u6311\u6218\uff0c\u8fd9\u8868\u660e\u5171\u4eab\u7684\u8bed\u7528\u654f\u611f\u6027\u800c\u975e\u6a21\u578b\u7279\u5b9a\u7f3a\u9677\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u5e76\u4e0d\u4e00\u5b9a\u6bd4\u4eba\u7c7b\u5f31\uff0c\u8fd9\u6311\u6218\u4e86\u73b0\u6709\u7684\u5047\u8bbe\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u65f6\u9700\u8981\u66f4\u8c28\u614e\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7f16\u7801\u5b9e\u8df5\u3002"}}
{"id": "2505.09005", "pdf": "https://arxiv.org/pdf/2505.09005", "abs": "https://arxiv.org/abs/2505.09005", "authors": ["Nicole Cuneo", "Eleanor Graves", "Supantho Rakshit", "Adele E. Goldberg"], "title": "For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies", "categories": ["cs.CL"], "comment": null, "summary": "It remains debated how well any LM understands natural language or generates\nreliable metalinguistic judgments. Moreover, relatively little work has\ndemonstrated that LMs can represent and respect subtle relationships between\nform and function proposed by linguists. We here focus on a particular such\nrelationship established in recent work: English speakers' judgments about the\ninformation structure of canonical sentences predicts independently collected\nacceptability ratings on corresponding 'long distance dependency' [LDD]\nconstructions, across a wide array of base constructions and multiple types of\nLDDs. To determine whether any LM captures this relationship, we probe GPT-4 on\nthe same tasks used with humans and new extensions.Results reveal reliable\nmetalinguistic skill on the information structure and acceptability tasks,\nreplicating a striking interaction between the two, despite the zero-shot,\nexplicit nature of the tasks, and little to no chance of contamination [Studies\n1a, 1b]. Study 2 manipulates the information structure of base sentences and\nconfirms a causal relationship: increasing the prominence of a constituent in a\ncontext sentence increases the subsequent acceptability ratings on an LDD\nconstruction. The findings suggest a tight relationship between natural and\nGPT-4 generated English, and between information structure and syntax, which\nbegs for further exploration.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-4\u80fd\u591f\u6355\u6349\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u4fe1\u606f\u7ed3\u6784\u548c\u53ef\u63a5\u53d7\u6027\u5173\u7cfb\uff0c\u5e76\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5143\u8bed\u8a00\u6280\u80fd\u3002", "motivation": "\u63a2\u8ba8LM\u5982\u4f55\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6216\u751f\u6210\u53ef\u9760\u7684\u5143\u8bed\u8a00\u5224\u65ad\uff0c\u4ee5\u53caLM\u662f\u5426\u80fd\u8868\u793a\u548c\u5c0a\u91cd\u8bed\u8a00\u5b66\u5bb6\u63d0\u51fa\u7684\u5f62\u5f0f\u4e0e\u529f\u80fd\u4e4b\u95f4\u7684\u5fae\u5999\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u63a2\u6d4bGPT-4\u5728\u4e0e\u4eba\u7c7b\u76f8\u540c\u7684\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u65b0\u7684\u6269\u5c55\u6765\u786e\u5b9a\u4efb\u4f55LM\u662f\u5426\u80fd\u6355\u6349\u8fd9\u79cd\u5173\u7cfb\u3002", "result": "GPT-4\u5728\u4fe1\u606f\u7ed3\u6784\u548c\u53ef\u63a5\u53d7\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u53ef\u9760\u7684\u5143\u8bed\u8a00\u6280\u80fd\uff0c\u590d\u5236\u4e86\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e4b\u95f4\u7684\u663e\u8457\u76f8\u4e92\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u7814\u7a762\u786e\u8ba4\u4e86\u4fe1\u606f\u7ed3\u6784\u5bf9LDD\u6784\u9020\u7684\u53ef\u63a5\u53d7\u6027\u8bc4\u5206\u6709\u56e0\u679c\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u81ea\u7136\u8bed\u8a00\u548cGPT-4\u751f\u6210\u7684\u82f1\u8bed\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u5173\u7cfb\uff0c\u4ee5\u53ca\u4fe1\u606f\u7ed3\u6784\u548c\u53e5\u6cd5\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2505.08896", "pdf": "https://arxiv.org/pdf/2505.08896", "abs": "https://arxiv.org/abs/2505.08896", "authors": ["Pankaj Kumar", "Aditya Mishra", "Pranamesh Chakraborty", "Subrahmanya Swamy Peruru"], "title": "Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Developing an autonomous vehicle control strategy for signalised\nintersections (SI) is one of the challenging tasks due to its inherently\ncomplex decision-making process. This study proposes a Deep Reinforcement\nLearning (DRL) based longitudinal vehicle control strategy at SI. A\ncomprehensive reward function has been formulated with a particular focus on\n(i) distance headway-based efficiency reward, (ii) decision-making criteria\nduring amber light, and (iii) asymmetric acceleration/ deceleration response,\nalong with the traditional safety and comfort criteria. This reward function\nhas been incorporated with two popular DRL algorithms, Deep Deterministic\nPolicy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the\ncontinuous action space of acceleration/deceleration. The proposed models have\nbeen trained on the combination of real-world leader vehicle (LV) trajectories\nand simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.\nThe overall performance of the proposed models has been tested using Cumulative\nDistribution Function (CDF) plots and compared with the real-world trajectory\ndata. The results show that the RL models successfully maintain lower distance\nheadway (i.e., higher efficiency) and jerk compared to human-driven vehicles\nwithout compromising safety. Further, to assess the robustness of the proposed\nmodels, we evaluated the model performance on diverse safety-critical\nscenarios, in terms of car-following and traffic signal compliance. Both DDPG\nand SAC models successfully handled the critical scenarios, while the DDPG\nmodel showed smoother action profiles compared to the SAC model. Overall, the\nresults confirm that DRL-based longitudinal vehicle control strategy at SI can\nhelp to improve traffic safety, efficiency, and comfort.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7eb5\u5411\u8f66\u8f86\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u7efc\u5408\u5956\u52b1\u51fd\u6570\u548c\u4e24\u79cdDRL\u7b97\u6cd5\uff08DDPG\u548cSAC\uff09\uff0c\u63d0\u9ad8\u4e86\u4ea4\u901a\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u81ea\u4e3b\u8f66\u8f86\u63a7\u5236\u7b56\u7565\u4ee5\u5e94\u5bf9\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7684\u590d\u6742\u51b3\u7b56\u8fc7\u7a0b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7eb5\u5411\u8f66\u8f86\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e86\u4e24\u79cd\u6d41\u884c\u7684DRL\u7b97\u6cd5\uff08DDPG\u548cSAC\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRL\u6a21\u578b\u80fd\u591f\u4fdd\u6301\u8f83\u4f4e\u7684\u8f66\u8ddd\uff08\u5373\u66f4\u9ad8\u7684\u6548\u7387\uff09\u548c\u8f83\u5c0f\u7684\u6025\u52a0\u901f/\u6025\u51cf\u901f\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u5b89\u5168\u6027\u3002\u6b64\u5916\uff0cDDPG\u6a21\u578b\u5728\u5173\u952e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u5e73\u6ed1\u7684\u52a8\u4f5c\u7279\u6027\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u7eb5\u5411\u8f66\u8f86\u63a7\u5236\u7b56\u7565\u5728\u4fe1\u53f7\u4ea4\u53c9\u53e3\u53ef\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u3002"}}
{"id": "2505.08792", "pdf": "https://arxiv.org/pdf/2505.08792", "abs": "https://arxiv.org/abs/2505.08792", "authors": ["Michelle Nashla Turcios", "Alicia E. Boyd", "Angela D. R. Smith", "Brittany Johnson"], "title": "A Preliminary Framework for Intersectionality in ML Pipelines", "categories": ["cs.LG", "cs.CY"], "comment": "Accepted for the 1st International Intersectionality and Software\n  Engineering Workshop, colocated with FSE 2025", "summary": "Machine learning (ML) has become a go-to solution for improving how we use,\nexperience, and interact with technology (and the world around us).\nUnfortunately, studies have repeatedly shown that machine learning technologies\nmay not provide adequate support for societal identities and experiences.\nIntersectionality is a sociological framework that provides a mechanism for\nexplicitly considering complex social identities, focusing on social justice\nand power. While the framework of intersectionality can support the development\nof technologies that acknowledge and support all members of society, it has\nbeen adopted and adapted in ways that are not always true to its foundations,\nthereby weakening its potential for impact. To support the appropriate adoption\nand use of intersectionality for more equitable technological outcomes, we\namplify the foundational intersectionality scholarship--Crenshaw, Combahee, and\nCollins (three C's), to create a socially relevant preliminary framework in\ndeveloping machine-learning solutions. We use this framework to evaluate and\nreport on the (mis)alignments of intersectionality application in machine\nlearning literature.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4ea4\u53c9\u6027\u7406\u8bba\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u7ed3\u679c\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u652f\u6301\u793e\u4f1a\u8eab\u4efd\u548c\u4f53\u9a8c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8003\u8651\u590d\u6742\u793e\u4f1a\u8eab\u4efd\u5e76\u5173\u6ce8\u793e\u4f1a\u6b63\u4e49\u548c\u6743\u529b\u7684\u6846\u67b6\u3002\u4ea4\u53c9\u6027\u7406\u8bba\u867d\u7136\u53ef\u4ee5\u652f\u6301\u5f00\u53d1\u5305\u5bb9\u6240\u6709\u793e\u4f1a\u6210\u5458\u7684\u6280\u672f\uff0c\u4f46\u5176\u5e94\u7528\u65b9\u5f0f\u5e76\u4e0d\u603b\u662f\u7b26\u5408\u5176\u57fa\u7840\uff0c\u4ece\u800c\u524a\u5f31\u4e86\u5176\u5f71\u54cd\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5f3a\u8c03\u57fa\u7840\u7684\u4ea4\u53c9\u6027\u7814\u7a76\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u793e\u4f1a\u76f8\u5173\u7684\u521d\u6b65\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u62a5\u544a\u673a\u5668\u5b66\u4e60\u6587\u732e\u4e2d\u4ea4\u53c9\u6027\u5e94\u7528\u7684\uff08\u4e0d\uff09\u5339\u914d\u60c5\u51b5\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e09\u4e2aC\u7684\u521d\u6b65\u6846\u67b6\uff0c\u5e76\u7528\u5b83\u6765\u8bc4\u4f30\u548c\u62a5\u544a\u673a\u5668\u5b66\u4e60\u6587\u732e\u4e2d\u4ea4\u53c9\u6027\u5e94\u7528\u7684\uff08\u4e0d\uff09\u5339\u914d\u60c5\u51b5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCrenshaw\u3001Combahee\u548cCollins\uff08\u4e09\u4e2aC\uff09\u7684\u521d\u6b65\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u9002\u5f53\u91c7\u7528\u4ea4\u53c9\u6027\u7406\u8bba\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u6280\u672f\u7ed3\u679c\u3002"}}
{"id": "2505.08800", "pdf": "https://arxiv.org/pdf/2505.08800", "abs": "https://arxiv.org/abs/2505.08800", "authors": ["Olivia Nocentini", "Marta Lagomarsino", "Gokhan Solak", "Younggeol Cho", "Qiyi Tong", "Marta Lorenzini", "Arash Ajoudani"], "title": "Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Driver fatigue poses a significant challenge to railway safety, with\ntraditional systems like the dead-man switch offering limited and basic\nalertness checks. This study presents an online behavior-based monitoring\nsystem utilizing a customised Directed-Graph Neural Network (DGNN) to classify\ntrain driver's states into three categories: alert, not alert, and\npathological. To optimize input representations for the model, an ablation\nstudy was performed, comparing three feature configurations: skeletal-only,\nfacial-only, and a combination of both. Experimental results show that\ncombining facial and skeletal features yields the highest accuracy (80.88%) in\nthe three-class model, outperforming models using only facial or skeletal\nfeatures. Furthermore, this combination achieves over 99% accuracy in the\nbinary alertness classification. Additionally, we introduced a novel dataset\nthat, for the first time, incorporates simulated pathological conditions into\ntrain driver monitoring, broadening the scope for assessing risks related to\nfatigue and health. This work represents a step forward in enhancing railway\nsafety through advanced online monitoring using vision-based technologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u7684\u5728\u7ebf\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u5b9a\u5236\u7684\u6709\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u5bf9\u5217\u8f66\u53f8\u673a\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u6a21\u62df\u75c5\u7406\u72b6\u51b5\uff0c\u589e\u5f3a\u94c1\u8def\u5b89\u5168\u3002", "motivation": "\u4f20\u7edf\u7cfb\u7edf\u5982\u6b7b\u4eba\u5f00\u5173\u63d0\u4f9b\u7684\u8b66\u89c9\u6027\u68c0\u67e5\u6709\u9650\u4e14\u57fa\u7840\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5148\u8fdb\u7684\u5728\u7ebf\u884c\u4e3a\u76d1\u6d4b\u7cfb\u7edf\u6765\u63d0\u9ad8\u94c1\u8def\u5b89\u5168\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u5b9a\u5236\u7684\u6709\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u6765\u5206\u7c7b\u5217\u8f66\u53f8\u673a\u7684\u72b6\u6001\uff0c\u5e76\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u7279\u5f81\u914d\u7f6e\uff1a\u4ec5\u9aa8\u9abc\u3001\u4ec5\u9762\u90e8\u4ee5\u53ca\u4e24\u8005\u7684\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u5728\u4e09\u7c7b\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\uff0880.88%\uff09\uff0c\u5e76\u4e14\u5728\u4e8c\u5143\u8b66\u89c9\u6027\u5206\u7c7b\u4e2d\u8fbe\u5230\u4e86\u8d85\u8fc799%\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5c06\u6a21\u62df\u7684\u75c5\u7406\u72b6\u51b5\u7eb3\u5165\u5217\u8f66\u53f8\u673a\u76d1\u63a7\u4e2d\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u5b9a\u5236\u7684\u6709\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7ebf\u884c\u4e3a\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u94c1\u8def\u5b89\u5168\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u53ef\u4ee5\u5b9e\u73b0\u6700\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u6765\u6a21\u62df\u75c5\u7406\u72b6\u51b5\uff0c\u4e3a\u8bc4\u4f30\u75b2\u52b3\u548c\u5065\u5eb7\u76f8\u5173\u7684\u98ce\u9669\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u8303\u56f4\u3002"}}
{"id": "2505.09039", "pdf": "https://arxiv.org/pdf/2505.09039", "abs": "https://arxiv.org/abs/2505.09039", "authors": ["Jingfeng Chen", "Raghuveer Thirukovalluru", "Junlin Wang", "Kaiwei Luo", "Bhuwan Dhingra"], "title": "Atomic Consistency Preference Optimization for Long-Form Question Answering", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases.", "AI": {"tldr": "ACPO\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u6211\u76d1\u7763\u504f\u597d\u8c03\u6574\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u66f4\u5f3a\u7684\u6a21\u578b\uff08\u5982GPT-4\uff09\u6216\u5916\u90e8\u77e5\u8bc6\u5e93\u6765\u8bc4\u4f30\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u8fd9\u53ef\u80fd\u5e76\u4e0d\u603b\u662f\u53ef\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u9700\u8981\u5916\u90e8\u76d1\u7763\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "ACPO\u5229\u7528\u539f\u5b50\u4e00\u81f4\u6027\u4fe1\u53f7\uff0c\u5373\u591a\u4e2a\u968f\u673a\u54cd\u5e94\u4e2d\u5355\u4e2a\u4e8b\u5b9e\u7684\u4e00\u81f4\u6027\uff0c\u6765\u8bc6\u522b\u9ad8\u8d28\u91cf\u548c\u4f4e\u8d28\u91cf\u7684\u6570\u636e\u5bf9\u4ee5\u8fdb\u884c\u6a21\u578b\u5bf9\u9f50\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cACPO\u5728LongFact\u548cBioGen\u6570\u636e\u96c6\u4e0a\u6bd4FactAlign\uff08\u4e00\u4e2a\u5f3a\u5927\u7684\u76d1\u7763\u5bf9\u9f50\u57fa\u7ebf\uff09\u9ad8\u51fa1.95\u5206\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8\u4e8b\u5b9e\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "ACPO\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u6211\u76d1\u7763\u504f\u597d\u8c03\u6574\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u6216\u77e5\u8bc6\u5e93\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e8b\u5b9e\u53ef\u9760\u6027\u3002"}}
{"id": "2505.08905", "pdf": "https://arxiv.org/pdf/2505.08905", "abs": "https://arxiv.org/abs/2505.08905", "authors": ["Michael Majurski", "Cynthia Matuszek"], "title": "Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language Models (LMs) continue to advance, improving response quality and\ncoherence. Given Internet-scale training datasets, LMs have likely encountered\nmuch of what users might ask them to generate in some form during their\ntraining. A plethora of evaluation benchmarks have been constructed to assess\nmodel quality, response appropriateness, and reasoning capabilities. However,\nthe human effort required for benchmark construction is limited and being\nrapidly outpaced by the size and scope of the models under evaluation.\nAdditionally, having humans build a benchmark for every possible domain of\ninterest is impractical. Therefore, we propose a methodology for automating the\nconstruction of fact-based synthetic data model evaluations grounded in\ndocument populations. This work leverages those very same LMs to evaluate\ndomain-specific knowledge automatically, using only grounding documents (e.g.,\na textbook) as input. This synthetic data benchmarking approach corresponds\nwell with human curated questions with a Spearman ranking correlation of 0.96\nand a benchmark evaluation Pearson accuracy correlation of 0.79. This novel\ntool supports generating both multiple choice and open-ended synthetic data\nquestions to gain diagnostic insight of LM capability. We apply this\nmethodology to evaluate model performance on a recent relevant arXiv preprint,\ndiscovering a surprisingly strong performance from Gemma3 models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6784\u5efa\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5408\u6210\u6570\u636e\u6a21\u578b\u8bc4\u4f30\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u6765\u8bc4\u4f30\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\uff0c\u4ec5\u9700\u57fa\u7840\u6587\u6863\u4f5c\u4e3a\u8f93\u5165\u3002\u8be5\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6784\u5efa\u7684\u95ee\u9898\u6709\u5f88\u9ad8\u7684\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u591a\u79cd\u7c7b\u578b\u7684\u5408\u6210\u95ee\u9898\u4ee5\u8bca\u65ad\u6a21\u578b\u80fd\u529b\u3002\u5728\u8bc4\u4f30Gemma3\u6a21\u578b\u65f6\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u975e\u5e38\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\u9700\u8981\u5927\u91cf\u7684\u4eba\u529b\uff0c\u800c\u6a21\u578b\u7684\u89c4\u6a21\u548c\u8303\u56f4\u8fc5\u901f\u6269\u5927\uff0c\u4f7f\u5f97\u624b\u52a8\u6784\u5efa\u6bcf\u4e2a\u9886\u57df\u57fa\u51c6\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u6863\u7fa4\u4f53\u7684\u81ea\u52a8\u5316\u5408\u6210\u6570\u636e\u57fa\u51c6\u6784\u5efa\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u6765\u8bc4\u4f30\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\uff0c\u4ec5\u9700\u57fa\u7840\u6587\u6863\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6784\u5efa\u7684\u95ee\u9898\u5177\u6709\u5f88\u9ad8\u7684\u76f8\u5173\u6027\uff08Spearman\u6392\u540d\u76f8\u5173\u7cfb\u6570\u4e3a0.96\uff0cPearson\u51c6\u786e\u7387\u76f8\u5173\u7cfb\u6570\u4e3a0.79\uff09\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u591a\u79cd\u7c7b\u578b\u7684\u5408\u6210\u95ee\u9898\u4ee5\u8bca\u65ad\u6a21\u578b\u80fd\u529b\u3002\u5728\u8bc4\u4f30Gemma3\u6a21\u578b\u65f6\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u975e\u5e38\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6784\u5efa\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5408\u6210\u6570\u636e\u6a21\u578b\u8bc4\u4f30\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u6765\u81ea\u52a8\u8bc4\u4f30\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\uff0c\u4ec5\u9700\u63d0\u4f9b\u57fa\u7840\u6587\u6863\uff08\u5982\u6559\u79d1\u4e66\uff09\u4f5c\u4e3a\u8f93\u5165\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6784\u5efa\u7684\u95ee\u9898\u6709\u5f88\u9ad8\u7684\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u591a\u79cd\u7c7b\u578b\u7684\u5408\u6210\u95ee\u9898\u4ee5\u8bca\u65ad\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2505.08793", "pdf": "https://arxiv.org/pdf/2505.08793", "abs": "https://arxiv.org/abs/2505.08793", "authors": ["Monirul Islam Pavel", "Siyi Hu", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Onboard Optimization and Learning: A Survey", "categories": ["cs.LG", "cs.AR"], "comment": "36 pages, 5 figures, 3 tables", "summary": "Onboard learning is a transformative approach in edge AI, enabling real-time\ndata processing, decision-making, and adaptive model training directly on\nresource-constrained devices without relying on centralized servers. This\nparadigm is crucial for applications demanding low latency, enhanced privacy,\nand energy efficiency. However, onboard learning faces challenges such as\nlimited computational resources, high inference costs, and security\nvulnerabilities. This survey explores a comprehensive range of methodologies\nthat address these challenges, focusing on techniques that optimize model\nefficiency, accelerate inference, and support collaborative learning across\ndistributed devices. Approaches for reducing model complexity, improving\ninference speed, and ensuring privacy-preserving computation are examined\nalongside emerging strategies that enhance scalability and adaptability in\ndynamic environments. By bridging advancements in hardware-software co-design,\nmodel compression, and decentralized learning, this survey provides insights\ninto the current state of onboard learning to enable robust, efficient, and\nsecure AI deployment at the edge.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86 onboard learning \u7684\u5404\u79cd\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5176\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u4f18\u5316\u6a21\u578b\u6548\u7387\u3001\u52a0\u901f\u63a8\u7406\u548c\u5728\u5206\u5e03\u5f0f\u8bbe\u5907\u4e0a\u652f\u6301\u534f\u4f5c\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u51cf\u5c11\u6a21\u578b\u590d\u6742\u6027\u3001\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u548c\u786e\u4fdd\u9690\u79c1\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u7684\u65b0\u5174\u7b56\u7565\u3002", "motivation": "Onboard learning \u662f\u4e00\u79cd\u53d8\u9769\u6027\u7684\u65b9\u6cd5\uff0c\u5728\u8fb9\u7f18 AI \u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6570\u636e\u5904\u7406\u3001\u51b3\u7b56\u548c\u81ea\u9002\u5e94\u6a21\u578b\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u670d\u52a1\u5668\u3002\u8fd9\u79cd\u8303\u5f0f\u5bf9\u4e8e\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u589e\u5f3a\u9690\u79c1\u548c\u80fd\u6548\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0conboard learning \u9762\u4e34\u7740\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3001\u9ad8\u63a8\u7406\u6210\u672c\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u4e86\u5404\u79cd\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3 onboard learning \u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u4f18\u5316\u6a21\u578b\u6548\u7387\u3001\u52a0\u901f\u63a8\u7406\u548c\u5728\u5206\u5e03\u5f0f\u8bbe\u5907\u4e0a\u652f\u6301\u534f\u4f5c\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u51cf\u5c11\u6a21\u578b\u590d\u6742\u6027\u3001\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u548c\u786e\u4fdd\u9690\u79c1\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u7684\u65b0\u5174\u7b56\u7565\u3002", "result": "\u672c\u6587\u7efc\u8ff0\u4e86\u5404\u79cd\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3 onboard learning \u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u4f18\u5316\u6a21\u578b\u6548\u7387\u3001\u52a0\u901f\u63a8\u7406\u548c\u5728\u5206\u5e03\u5f0f\u8bbe\u5907\u4e0a\u652f\u6301\u534f\u4f5c\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u51cf\u5c11\u6a21\u578b\u590d\u6742\u6027\u3001\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u548c\u786e\u4fdd\u9690\u79c1\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u7684\u65b0\u5174\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u5e7f\u6cdb\u65b9\u6cd5\uff0c\u91cd\u70b9\u662f\u4f18\u5316\u6a21\u578b\u6548\u7387\u3001\u52a0\u901f\u63a8\u7406\u548c\u5728\u5206\u5e03\u5f0f\u8bbe\u5907\u4e0a\u652f\u6301\u534f\u4f5c\u5b66\u4e60\u3002\u540c\u65f6\u63a2\u8ba8\u4e86\u51cf\u5c11\u6a21\u578b\u590d\u6742\u6027\u3001\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u548c\u786e\u4fdd\u9690\u79c1\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u7684\u65b0\u5174\u7b56\u7565\u3002\u901a\u8fc7\u7ed3\u5408\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u3001\u6a21\u578b\u538b\u7f29\u548c\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u8fdb\u5c55\uff0c\u672c\u6587\u4e3a\u5728\u8fb9\u7f18\u8fdb\u884c\u7a33\u5065\u3001\u9ad8\u6548\u548c\u5b89\u5168\u7684AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2505.08801", "pdf": "https://arxiv.org/pdf/2505.08801", "abs": "https://arxiv.org/abs/2505.08801", "authors": ["Md. Sakib Hassan Chowdhury", "Md. Hafiz Ahamed", "Bishowjit Paul", "Sarafat Hussain Abhi", "Abu Bakar Siddique", "Md. Robius Sany"], "title": "OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "12 pages, 17 figures", "summary": "Gait recognition, known for its ability to identify individuals from a\ndistance, has gained significant attention in recent times due to its\nnon-intrusive verification. While video-based gait identification systems\nperform well on large public datasets, their performance drops when applied to\nreal-world, unconstrained gait data due to various factors. Among these,\nuncontrolled outdoor environments, non-overlapping camera views, varying\nillumination, and computational efficiency are core challenges in gait-based\nauthentication. Currently, no dataset addresses all these challenges\nsimultaneously. In this paper, we propose an OptiGait-LGBM model capable of\nrecognizing person re-identification under these constraints using a skeletal\nmodel approach, which helps mitigate inconsistencies in a person's appearance.\nThe model constructs a dataset from landmark positions, minimizing memory usage\nby using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to\nrepresent uncontrolled gait sequences in complex outdoor environments. The\nprocess involves extracting skeletal joint landmarks, generating numerical\ndatasets, and developing an OptiGait-LGBM gait classification model. Our aim is\nto address the aforementioned challenges with minimal computational cost\ncompared to existing methods. A comparative analysis with ensemble techniques\nsuch as Random Forest and CatBoost demonstrates that the proposed approach\noutperforms them in terms of accuracy, memory usage, and training time. This\nmethod provides a novel, low-cost, and memory-efficient video-based gait\nrecognition solution for real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9aa8\u9abc\u6a21\u578b\u7684OptiGait-LGBM\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u9ad8\u6548\u5185\u5b58\u4f7f\u7528\u7684\u7279\u70b9\u3002", "motivation": "\u76ee\u524d\u6ca1\u6709\u6570\u636e\u96c6\u80fd\u540c\u65f6\u89e3\u51b3\u6b65\u6001\u8bc6\u522b\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5982\u4e0d\u53d7\u63a7\u7684\u6237\u5916\u73af\u5883\u3001\u975e\u91cd\u53e0\u6444\u50cf\u5934\u89c6\u89d2\u3001\u5149\u7167\u53d8\u5316\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u91c7\u7528\u9aa8\u9abc\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u9aa8\u9abc\u5173\u8282\u5173\u952e\u70b9\u6784\u5efa\u6570\u636e\u96c6\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6RUET-GAIT\uff0c\u7528\u4e8e\u8868\u793a\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u975e\u53d7\u63a7\u6b65\u6001\u5e8f\u5217\u3002", "result": "\u4e0e\u96c6\u6210\u6280\u672f\u5982\u968f\u673a\u68ee\u6797\u548cCatBoost\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdOptiGait-LGBM\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5728\u5404\u79cd\u7ea6\u675f\u6761\u4ef6\u4e0b\u8fdb\u884c\u4eba\u5458\u518d\u8bc6\u522b\uff0c\u5e76\u4e14\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u8be5\u65b9\u6cd5\u4e3a\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u4f4e\u6210\u672c\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u89c6\u9891\u6b65\u6001\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09056", "pdf": "https://arxiv.org/pdf/2505.09056", "abs": "https://arxiv.org/abs/2505.09056", "authors": ["Brandon Smith", "Mohamed Reda Bouadjenek", "Tahsin Alamgir Kheya", "Phillip Dawson", "Sunil Aryal"], "title": "A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e8612\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f93\u51fa\uff0c\u53d1\u73b0\u540c\u4e00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u6bd4\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u672c\u66f4\u76f8\u4f3c\uff0c\u800cGPT-4\u7684\u8f93\u51fa\u66f4\u5177\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u5199\u4f5c\u98ce\u683c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\u6027\u522b\u5e73\u8861\u548c\u51cf\u5c11\u504f\u89c1\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u7814\u7a76LLM\u751f\u6210\u6587\u672c\u7684\u76f8\u4f3c\u6027\u3001\u53d8\u5f02\u6027\u4ee5\u53ca\u4f26\u7406\u5f71\u54cd\uff0c\u4f8b\u5982\u540c\u4e00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u6709\u591a\u76f8\u4f3c\uff0c\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u5982\u4f55\u6bd4\u8f83\uff0c\u4ee5\u53ca\u54ea\u4e9b\u6a21\u578b\u6700\u7b26\u5408\u4f26\u7406\u6807\u51c6\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e865000\u4e2a\u8de8\u4e0d\u540c\u4efb\u52a1\u7684\u63d0\u793a\uff0c\u5305\u62ec\u751f\u6210\u3001\u89e3\u91ca\u548c\u91cd\u5199\uff0c\u4ea7\u751f\u4e86\u7ea6300\u4e07\u6761\u6765\u81ea12\u4e2aLLM\u7684\u6587\u672c\uff0c\u5305\u62ec\u6765\u81eaOpenAI\u3001Google\u3001Microsoft\u3001Meta\u548cMistral\u7684\u4e13\u6709\u548c\u5f00\u6e90\u7cfb\u7edf\u3002", "result": "\uff081\uff09\u540c\u4e00LLM\u7684\u8f93\u51fa\u6bd4\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u672c\u66f4\u76f8\u4f3c\uff1b\uff082\uff09\u50cfWizardLM-2-8x22b\u8fd9\u6837\u7684\u6a21\u578b\u751f\u6210\u9ad8\u5ea6\u76f8\u4f3c\u7684\u8f93\u51fa\uff0c\u800cGPT-4\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u54cd\u5e94\uff1b\uff083\uff09LLM\u7684\u5199\u4f5c\u98ce\u683c\u5dee\u5f02\u663e\u8457\uff0cLlama 3\u548cMistral\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u76f8\u4f3c\u6027\uff0c\u800cGPT-4\u5219\u56e0\u5176\u72ec\u7279\u6027\u8131\u9896\u800c\u51fa\uff1b\uff084\uff09\u8bcd\u6c47\u548c\u8bed\u6c14\u7684\u5dee\u5f02\u7a81\u663e\u4e86LLM\u751f\u6210\u5185\u5bb9\u7684\u8bed\u8a00\u72ec\u7279\u6027\uff1b\uff085\uff09\u4e00\u4e9bLLM\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6027\u522b\u5e73\u8861\u548c\u51cf\u5c11\u7684\u504f\u89c1\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3aLLM\u8f93\u51fa\u7684\u884c\u4e3a\u548c\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7684\u53d1\u5c55\u548c\u4f26\u7406\u8bc4\u4f30\u3002"}}
{"id": "2505.08988", "pdf": "https://arxiv.org/pdf/2505.08988", "abs": "https://arxiv.org/abs/2505.08988", "authors": ["Montaser Mohammedalamen", "Michael Bowling"], "title": "Generalization in Monitored Markov Decision Processes (Mon-MDPs)", "categories": ["cs.AI"], "comment": "Under Review", "summary": "Reinforcement learning (RL) typically models the interaction between the\nagent and environment as a Markov decision process (MDP), where the rewards\nthat guide the agent's behavior are always observable. However, in many\nreal-world scenarios, rewards are not always observable, which can be modeled\nas a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have\nbeen limited to simple, tabular cases, restricting their applicability to\nreal-world problems. This work explores Mon-MDPs using function approximation\n(FA) and investigates the challenges involved. We show that combining function\napproximation with a learned reward model enables agents to generalize from\nmonitored states with observable rewards, to unmonitored environment states\nwith unobservable rewards. Therefore, we demonstrate that such generalization\nwith a reward model achieves near-optimal policies in environments formally\ndefined as unsolvable. However, we identify a critical limitation of such\nfunction approximation, where agents incorrectly extrapolate rewards due to\novergeneralization, resulting in undesirable behaviors. To mitigate\novergeneralization, we propose a cautious police optimization method leveraging\nreward uncertainty. This work serves as a step towards bridging this gap\nbetween Mon-MDP theory and real-world applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u51fd\u6570\u903c\u8fd1\u7684\u76d1\u63a7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Mon-MDP\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8c28\u614e\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u56e0\u8fc7\u5ea6\u6cdb\u5316\u800c\u5bfc\u81f4\u7684\u4e0d\u826f\u884c\u4e3a\u3002", "motivation": "\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5956\u52b1\u5e76\u4e0d\u603b\u662f\u53ef\u89c2\u5bdf\u7684\uff0c\u8fd9\u53ef\u4ee5\u5efa\u6a21\u4e3a\u4e00\u4e2a\u76d1\u63a7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Mon-MDP\uff09\u3002\u7136\u800c\uff0c\u4e4b\u524d\u5173\u4e8eMon-MDP\u7684\u7814\u7a76\u4ec5\u9650\u4e8e\u7b80\u5355\u7684\u8868\u683c\u6848\u4f8b\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u51fd\u6570\u903c\u8fd1\u7684Mon-MDP\uff0c\u5e76\u8c03\u67e5\u4e86\u5176\u4e2d\u7684\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8c28\u614e\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5956\u52b1\u4e0d\u786e\u5b9a\u6027\u6765\u51cf\u8f7b\u8fc7\u5ea6\u6cdb\u5316\u7684\u95ee\u9898\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86\u5c06\u51fd\u6570\u903c\u8fd1\u4e0e\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4ece\u5177\u6709\u53ef\u89c2\u5bdf\u5956\u52b1\u7684\u76d1\u63a7\u72b6\u6001\u63a8\u5e7f\u5230\u5177\u6709\u4e0d\u53ef\u89c2\u5bdf\u5956\u52b1\u7684\u975e\u76d1\u63a7\u73af\u5883\u72b6\u6001\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u5e26\u6709\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u5728\u5f62\u5f0f\u4e0a\u5b9a\u4e49\u4e3a\u65e0\u6cd5\u89e3\u51b3\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u79cd\u51fd\u6570\u903c\u8fd1\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u9650\u5236\uff0c\u5373\u667a\u80fd\u4f53\u7531\u4e8e\u8fc7\u5ea6\u6cdb\u5316\u800c\u9519\u8bef\u5730\u5916\u63a8\u5956\u52b1\uff0c\u5bfc\u81f4\u4e0d\u826f\u884c\u4e3a\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5f25\u5408Mon-MDP\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.08795", "pdf": "https://arxiv.org/pdf/2505.08795", "abs": "https://arxiv.org/abs/2505.08795", "authors": ["Andres Anabalon", "Hugo Garces", "Julio Oliva", "Jose Cifuentes"], "title": "The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "We show that there is a fast algorithm that embeds hierarchical structures in\nthree-dimensional Minkowski spacetime. The correlation of data ends up purely\nencoded in the causal structure. Our model relies solely on oriented token\npairs -- local hierarchical signals -- with no access to global symbolic\nstructure. We apply our method to the corpus of \\textit{WordNet}. We provide a\nperfect embedding of the mammal sub-tree including ambiguities (more than one\nhierarchy per node) in such a way that the hierarchical structures get\ncompletely codified in the geometry and exactly reproduce the ground-truth. We\nextend this to a perfect embedding of the maximal unambiguous subset of the\n\\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We\nintroduce a novel retrieval mechanism in which causality, not distance, governs\nhierarchical access. Our results seem to indicate that all discrete data has a\nperfect geometrical representation that is three-dimensional. The resulting\nembeddings are nearly conformally invariant, indicating deep connections with\ngeneral relativity and field theory. These results suggest that concepts,\ncategories, and their interrelations, namely hierarchical meaning itself, is\ngeometric.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5206\u5c42\u7ed3\u6784\u5d4c\u5165\u4e09\u7ef4\u95f5\u53ef\u592b\u65af\u57fa\u65f6\u7a7a\u7684\u5feb\u901f\u7b97\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u8fdb\u884c\u5c42\u6b21\u8bbf\u95ee\uff0c\u5e76\u5728WordNet\u8bed\u6599\u5e93\u4e0a\u53d6\u5f97\u4e86\u5b8c\u7f8e\u5d4c\u5165\u7684\u7ed3\u679c\u3002", "motivation": "\u6211\u4eec\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u7684\u5c42\u6b21\u7ed3\u6784\u5b8c\u5168\u7f16\u7801\u5728\u51e0\u4f55\u7ed3\u6784\u4e2d\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u8fdb\u884c\u8bbf\u95ee\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7b97\u6cd5\uff0c\u5c06\u5206\u5c42\u7ed3\u6784\u5d4c\u5165\u5230\u4e09\u7ef4\u95f5\u53ef\u592b\u65af\u57fa\u65f6\u7a7a\u3002\u6211\u4eec\u7684\u6a21\u578b\u4ec5\u4f9d\u8d56\u4e8e\u5b9a\u5411\u6807\u8bb0\u5bf9\u2014\u2014\u5c40\u90e8\u5206\u5c42\u4fe1\u53f7\u2014\u2014\u6ca1\u6709\u8bbf\u95ee\u5168\u5c40\u7b26\u53f7\u7ed3\u6784\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u673a\u5236\uff0c\u5176\u4e2d\u56e0\u679c\u5173\u7cfb\u800c\u4e0d\u662f\u8ddd\u79bb\u51b3\u5b9a\u4e86\u5c42\u6b21\u8bbf\u95ee\u3002", "result": "\u6211\u4eec\u5728WordNet\u8bed\u6599\u5e93\u4e0a\u5e94\u7528\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u54fa\u4e73\u52a8\u7269\u5b50\u6811\u7684\u5b8c\u7f8e\u5d4c\u5165\uff0c\u5305\u62ec\u6b67\u4e49\uff08\u6bcf\u4e2a\u8282\u70b9\u6709\u591a\u4e2a\u5c42\u6b21\uff09\uff0c\u5e76\u5b8c\u5168\u7f16\u7801\u4e86\u5c42\u6b21\u7ed3\u6784\u3002\u6211\u4eec\u8fd8\u6269\u5c55\u5230\u4e86WordNet\u6700\u5927\u65e0\u6b67\u4e49\u5b50\u96c6\u7684\u5b8c\u7f8e\u5d4c\u5165\uff0c\u5305\u542b82,115\u4e2a\u540d\u8bcd\u6807\u8bb0\u548c\u6bcf\u4e2a\u6807\u8bb0\u4e00\u4e2a\u5c42\u6b21\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u79bb\u6563\u6570\u636e\u90fd\u6709\u4e00\u4e2a\u5b8c\u7f8e\u7684\u51e0\u4f55\u8868\u793a\uff0c\u5b83\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u3002\u8fd9\u4e9b\u5d4c\u5165\u51e0\u4e4e\u662f\u5171\u5f62\u4e0d\u53d8\u7684\uff0c\u8868\u660e\u4e0e\u5e7f\u4e49\u76f8\u5bf9\u8bba\u548c\u573a\u8bba\u6709\u6df1\u523b\u7684\u8054\u7cfb\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u6982\u5ff5\u3001\u7c7b\u522b\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\uff0c\u5373\u5c42\u6b21\u610f\u4e49\u672c\u8eab\uff0c\u662f\u51e0\u4f55\u7684\u3002"}}
{"id": "2505.08808", "pdf": "https://arxiv.org/pdf/2505.08808", "abs": "https://arxiv.org/abs/2505.08808", "authors": ["Anqing Jiang", "Jinhao Chai", "Yu Gao", "Yiru Wang", "Yuwen Heng", "Zhigang Sun", "Hao Sun", "Zezhong Zhao", "Li Sun", "Jian Zhou", "Lijuan Zhu", "Shugong Xu", "Hao Zhao"], "title": "SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in high-definition \\emph{HD} map construction have\ndemonstrated the effectiveness of dense representations, which heavily rely on\ncomputationally intensive bird's-eye view \\emph{BEV} features. While sparse\nrepresentations offer a more efficient alternative by avoiding dense BEV\nprocessing, existing methods often lag behind due to the lack of tailored\ndesigns. These limitations have hindered the competitiveness of sparse\nrepresentations in online HD map construction. In this work, we systematically\nrevisit and enhance sparse representation techniques, identifying key\narchitectural and algorithmic improvements that bridge the gap with--and\nultimately surpass--dense approaches. We introduce a dedicated network\narchitecture optimized for sparse map feature extraction, a sparse-dense\nsegmentation auxiliary task to better leverage geometric and semantic cues, and\na denoising module guided by physical priors to refine predictions. Through\nthese enhancements, our method achieves state-of-the-art performance on the\nnuScenes dataset, significantly advancing HD map construction and centerline\ndetection. Specifically, SparseMeXt-Tiny reaches a mean average precision\n\\emph{mAP} of 55.5% at 32 frames per second \\emph{fps}, while SparseMeXt-Base\nattains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large\nachieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for\nsparse representations in HD map construction. These results underscore the\nuntapped potential of sparse methods, challenging the conventional reliance on\ndense representations and redefining efficiency-performance trade-offs in the\nfield.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5730\u56de\u987e\u5e76\u6539\u8fdb\u4e86\u7a00\u758f\u8868\u793a\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u7f51\u7edc\u67b6\u6784\u3001\u4e00\u4e2a\u7a00\u758f-\u5bc6\u96c6\u7684\u5206\u5272\u8f85\u52a9\u4efb\u52a1\u4ee5\u53ca\u4e00\u4e2a\u7531\u7269\u7406\u5148\u9a8c\u5f15\u5bfc\u7684\u53bb\u566a\u6a21\u5757\uff0c\u4ece\u800c\u5728HD\u5730\u56fe\u6784\u5efa\u548c\u4e2d\u5fc3\u7ebf\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u5b9a\u5236\u8bbe\u8ba1\u800c\u5f80\u5f80\u843d\u540e\u4e8e\u5bc6\u96c6\u8868\u793a\uff0c\u8fd9\u9650\u5236\u4e86\u7a00\u758f\u8868\u793a\u5728\u5728\u7ebfHD\u5730\u56fe\u6784\u5efa\u4e2d\u7684\u7ade\u4e89\u529b\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u7a00\u758f\u5730\u56fe\u7279\u5f81\u63d0\u53d6\uff0c\u4e00\u4e2a\u7a00\u758f-\u5bc6\u96c6\u7684\u5206\u5272\u8f85\u52a9\u4efb\u52a1\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u51e0\u4f55\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u4e14\u6709\u4e00\u4e2a\u7531\u7269\u7406\u5148\u9a8c\u5f15\u5bfc\u7684\u53bb\u566a\u6a21\u5757\u6765\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63a8\u8fdb\u4e86HD\u5730\u56fe\u6784\u5efa\u548c\u4e2d\u5fc3\u7ebf\u68c0\u6d4b\u3002SparseMeXt-Tiny\u572832\u5e27/\u79d2\u65f6\u8fbe\u523055.5%\u7684mAP\uff0cSparseMeXt-Base\u8fbe\u523065.2%\u7684mAP\u3002\u6269\u5927\u4e3b\u5e72\u548c\u89e3\u7801\u5668\u540e\uff0cSparseMeXt-Large\u5728\u8d85\u8fc720\u5e27/\u79d2\u65f6\u8fbe\u523068.9%\u7684mAP\uff0c\u4e3a\u7a00\u758f\u8868\u793a\u5728HD\u5730\u56fe\u6784\u5efa\u4e2d\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u7a00\u758f\u65b9\u6cd5\u7684\u672a\u5f00\u53d1\u6f5c\u529b\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4f9d\u8d56\u5bc6\u96c6\u8868\u793a\u7684\u505a\u6cd5\uff0c\u5e76\u91cd\u65b0\u5b9a\u4e49\u4e86\u8be5\u9886\u57df\u4e2d\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\u3002"}}
{"id": "2505.09068", "pdf": "https://arxiv.org/pdf/2505.09068", "abs": "https://arxiv.org/abs/2505.09068", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "title": "S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/.", "AI": {"tldr": "This paper introduces S-DAT, a scalable, multilingual framework for automated assessment of divergent thinking, which uses large language models and advanced multilingual embeddings to compute semantic distance as a language-agnostic proxy for DT.", "motivation": "Traditional creativity assessments are often labor-intensive, language-specific, and reliant on subjective human ratings, limiting their scalability and cross-cultural applicability.", "method": "S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance, which serves as a language-agnostic proxy for divergent thinking (DT).", "result": "S-DAT was evaluated across eleven diverse languages, demonstrating robust and consistent scoring across linguistic contexts. It shows convergent validity with other DT measures and correct discriminant validity with convergent thinking.", "conclusion": "S-DAT provides a powerful tool for fairer, more comprehensive evaluation of cognitive flexibility in diverse populations and can be freely assessed online."}}
{"id": "2505.08995", "pdf": "https://arxiv.org/pdf/2505.08995", "abs": "https://arxiv.org/abs/2505.08995", "authors": ["Ardian Selmonaj", "Oleg Szehr", "Giacomo Del Rio", "Alessandro Antonucci", "Adrian Schneider", "Michael R\u00fcegsegger"], "title": "Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "comment": "Published as journal chapter in Deep Learning Applications, Vol. 1,\n  by Taylor & Francis", "summary": "This work presents a Hierarchical Multi-Agent Reinforcement Learning\nframework for analyzing simulated air combat scenarios involving heterogeneous\nagents. The objective is to identify effective Courses of Action that lead to\nmission success within preset simulations, thereby enabling the exploration of\nreal-world defense scenarios at low cost and in a safe-to-fail setting.\nApplying deep Reinforcement Learning in this context poses specific challenges,\nsuch as complex flight dynamics, the exponential size of the state and action\nspaces in multi-agent systems, and the capability to integrate real-time\ncontrol of individual units with look-ahead planning. To address these\nchallenges, the decision-making process is split into two levels of\nabstraction: low-level policies control individual units, while a high-level\ncommander policy issues macro commands aligned with the overall mission\ntargets. This hierarchical structure facilitates the training process by\nexploiting policy symmetries of individual agents and by separating control\nfrom command tasks. The low-level policies are trained for individual combat\ncontrol in a curriculum of increasing complexity. The high-level commander is\nthen trained on mission targets given pre-trained control policies. The\nempirical validation confirms the advantages of the proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u6a21\u62df\u7a7a\u6218\u573a\u666f\uff0c\u4ee5\u8bc6\u522b\u5bfc\u81f4\u4efb\u52a1\u6210\u529f\u7684\u6709\u6548\u884c\u52a8\u65b9\u6848\u3002", "motivation": "\u5728\u6a21\u62df\u7a7a\u6218\u573a\u666f\u4e2d\u5e94\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u590d\u6742\u98de\u884c\u52a8\u529b\u5b66\u3001\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u6307\u6570\u7ea7\u589e\u957f\u4ee5\u53ca\u5b9e\u65f6\u63a7\u5236\u4e0e\u524d\u77bb\u89c4\u5212\u7684\u6574\u5408\u7b49\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u51b3\u7b56\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u62bd\u8c61\u5c42\u6b21\uff1a\u4f4e\u5c42\u6b21\u7b56\u7565\u63a7\u5236\u5355\u4e2a\u5355\u4f4d\uff0c\u9ad8\u5c42\u6b21\u6307\u6325\u7b56\u7565\u53d1\u5e03\u4e0e\u6574\u4f53\u4efb\u52a1\u76ee\u6807\u5bf9\u9f50\u7684\u5b8f\u89c2\u547d\u4ee4\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u786e\u8ba4\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6a21\u62df\u7a7a\u6218\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5bfc\u81f4\u4efb\u52a1\u6210\u529f\u7684\u884c\u52a8\u65b9\u6848\u3002"}}
{"id": "2505.08803", "pdf": "https://arxiv.org/pdf/2505.08803", "abs": "https://arxiv.org/abs/2505.08803", "authors": ["Zizhao Hu", "Mohammad Rostami", "Jesse Thomason"], "title": "Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent research has highlighted the risk of generative model collapse, where\nperformance progressively degrades when continually trained on self-generated\ndata. However, existing exploration on model collapse is limited to single,\nunimodal models, limiting our understanding in more realistic scenarios, such\nas diverse multi-modal AI agents interacting autonomously through synthetic\ndata and continually evolving. We expand the synthetic data training and model\ncollapse study to multi-modal vision-language generative systems, such as\nvision-language models (VLMs) and text-to-image diffusion models, as well as\nrecursive generate-train loops with multiple models. We find that model\ncollapse, previously observed in single-modality generative models, exhibits\ndistinct characteristics in the multi-modal context, such as improved\nvision-language alignment and increased variance in VLM image-captioning task.\nAdditionally, we find that general approaches such as increased decoding\nbudgets, greater model diversity, and relabeling with frozen models can\neffectively mitigate model collapse. Our findings provide initial insights and\npractical guidelines for reducing the risk of model collapse in self-improving\nmulti-agent AI systems and curating robust multi-modal synthetic datasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u7684\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u73b0\u6709\u7684\u5173\u4e8e\u6a21\u578b\u5d29\u6e83\u7684\u7814\u7a76\u4ec5\u9650\u4e8e\u5355\u4e00\u3001\u5355\u6a21\u6001\u6a21\u578b\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5728\u66f4\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u7406\u89e3\uff0c\u4f8b\u5982\u901a\u8fc7\u5408\u6210\u6570\u636e\u81ea\u4e3b\u4ea4\u4e92\u7684\u591a\u6837\u5316\u591a\u6a21\u6001AI\u4ee3\u7406\u548c\u6301\u7eed\u8fdb\u5316\u7684\u573a\u666f\u3002", "method": "\u6211\u4eec\u5c06\u5408\u6210\u6570\u636e\u8bad\u7ec3\u548c\u6a21\u578b\u5d29\u6e83\u7684\u7814\u7a76\u6269\u5c55\u5230\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\uff0c\u5982\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u53ca\u5177\u6709\u591a\u4e2a\u6a21\u578b\u7684\u9012\u5f52\u751f\u6210-\u8bad\u7ec3\u5faa\u73af\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\uff0c\u6a21\u578b\u5d29\u6e83\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u6539\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u548cVLM\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u65b9\u5dee\u589e\u52a0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u8bf8\u5982\u589e\u52a0\u89e3\u7801\u9884\u7b97\u3001\u66f4\u5927\u7684\u6a21\u578b\u591a\u6837\u6027\u548c\u4f7f\u7528\u51bb\u7ed3\u6a21\u578b\u91cd\u65b0\u6807\u8bb0\u7b49\u901a\u7528\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u51cf\u5c11\u81ea\u6211\u6539\u8fdb\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u6a21\u578b\u5d29\u6e83\u98ce\u9669\u4ee5\u53ca\u6784\u5efa\u7a33\u5065\u7684\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\u96c6\u7684\u521d\u6b65\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2505.08811", "pdf": "https://arxiv.org/pdf/2505.08811", "abs": "https://arxiv.org/abs/2505.08811", "authors": ["Shijie Lian", "Ziyi Zhang", "Laurence Tianruo Yang and", "Mengyu Ren", "Debin Liu", "Hua Li"], "title": "TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Underwater 3D scene reconstruction is crucial for undewater robotic\nperception and navigation. However, the task is significantly challenged by the\ncomplex interplay between light propagation, water medium, and object surfaces,\nwith existing methods unable to model their interactions accurately.\nAdditionally, expensive training and rendering costs limit their practical\napplication in underwater robotic systems. Therefore, we propose Tensorized\nUnderwater Gaussian Splatting (TUGS), which can effectively solve the modeling\nchallenges of the complex interactions between object geometries and water\nmedia while achieving significant parameter reduction. TUGS employs lightweight\ntensorized higher-order Gaussians with a physics-based underwater Adaptive\nMedium Estimation (AME) module, enabling accurate simulation of both light\nattenuation and backscatter effects in underwater environments. Compared to\nother NeRF-based and GS-based methods designed for underwater, TUGS is able to\nrender high-quality underwater images with faster rendering speeds and less\nmemory usage. Extensive experiments on real-world underwater datasets have\ndemonstrated that TUGS can efficiently achieve superior reconstruction quality\nusing a limited number of parameters, making it particularly suitable for\nmemory-constrained underwater UAV applications", "AI": {"tldr": "TUGS is a method for underwater 3D scene reconstruction that uses lightweight tensorized higher-order Gaussians and a physics-based AME module to simulate light attenuation and backscatter effects. It achieves high-quality images with faster rendering speeds and less memory usage compared to other NeRF-based and GS-based methods.", "motivation": "Underwater 3D scene reconstruction is crucial for underwater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems.", "method": "TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments.", "result": "Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications.", "conclusion": "TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications."}}
{"id": "2505.09082", "pdf": "https://arxiv.org/pdf/2505.09082", "abs": "https://arxiv.org/abs/2505.09082", "authors": ["Sophie Zhang", "Zhiming Lin"], "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6CEC-Zero\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u81ea\u4e3b\u9519\u8bef\u7b56\u7565\u5b66\u4e60\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\uff0c\u800c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e2d\u6587\u6587\u672c\u6821\u6b63\u4e2d\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8eBERT\u7684\u6a21\u578b\uff0c\u4f46\u5728\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8LLMs\u5728\u4e2d\u6587\u6587\u672c\u6821\u6b63\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u79f0\u4e3aCEC-Zero\uff0c\u5b83\u7ed3\u5408\u4e86RL\u548cLLMs\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f7fLLMs\u80fd\u591f\u901a\u8fc7\u81ea\u4e3b\u9519\u8bef\u7b56\u7565\u5b66\u4e60\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u6ce8\u91ca\u6570\u636e\u6216\u8f85\u52a9\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3aRL\u7684LLMs\u5b9e\u73b0\u4e86\u884c\u4e1a\u53ef\u884c\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u7684\u53ef\u9760\u6027\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86CEC-Zero\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u901a\u8fc7\u81ea\u4e3b\u9519\u8bef\u7b56\u7565\u5b66\u4e60\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\uff0c\u800c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3aRL\u7684LLMs\u5b9e\u73b0\u4e86\u884c\u4e1a\u53ef\u884c\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u7684\u53ef\u9760\u6027\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u4e00\u7a81\u7834\u4fc3\u8fdb\u4e86LLMs\u5728\u5b9e\u9645\u4e2d\u6587\u6587\u672c\u6821\u6b63\u573a\u666f\u4e2d\u7684\u90e8\u7f72\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u81ea\u6211\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u8303\u5f0f\u3002"}}
{"id": "2505.09012", "pdf": "https://arxiv.org/pdf/2505.09012", "abs": "https://arxiv.org/abs/2505.09012", "authors": ["Bo Meng", "Chenghao Xu", "Yongli Zhu"], "title": "Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "Cascading failures in power grids can lead to grid collapse, causing severe\ndisruptions to social operations and economic activities. In certain cases,\nmulti-stage cascading failures can occur. However, existing\ncascading-failure-mitigation strategies are usually single-stage-based,\noverlooking the complexity of the multi-stage scenario. This paper treats the\nmulti-stage cascading failure problem as a reinforcement learning task and\ndevelops a simulation environment. The reinforcement learning agent is then\ntrained via the deterministic policy gradient algorithm to achieve continuous\nactions. Finally, the effectiveness of the proposed approach is validated on\nthe IEEE 14-bus and IEEE 118-bus systems.", "AI": {"tldr": "\u672c\u6587\u5c06\u591a\u9636\u6bb5\u7ea7\u8054\u6545\u969c\u95ee\u9898\u89c6\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eff\u771f\u73af\u5883\u3002\u901a\u8fc7\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u6700\u7ec8\u5728IEEE 14-bus\u548cIEEE 118-bus\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7ea7\u8054\u6545\u969c\u7f13\u89e3\u7b56\u7565\u901a\u5e38\u662f\u5355\u9636\u6bb5\u7684\uff0c\u5ffd\u7565\u4e86\u591a\u9636\u6bb5\u573a\u666f\u7684\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u9636\u6bb5\u7ea7\u8054\u6545\u969c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5c06\u591a\u9636\u6bb5\u7ea7\u8054\u6545\u969c\u95ee\u9898\u89c6\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eff\u771f\u73af\u5883\u3002\u7136\u540e\u901a\u8fc7\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4ee5\u5b9e\u73b0\u8fde\u7eed\u52a8\u4f5c\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u5728IEEE 14-bus\u548cIEEE 118-bus\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u5728IEEE 14-bus\u548cIEEE 118-bus\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u591a\u9636\u6bb5\u7ea7\u8054\u6545\u969c\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08823", "pdf": "https://arxiv.org/pdf/2505.08823", "abs": "https://arxiv.org/abs/2505.08823", "authors": ["Cody Steinmetz", "Gavin Childress", "Aaron Herbst", "Gavin Jones", "Jasdeep Singh", "Eli Vang", "Keagan Weinstock"], "title": "An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural-language processing,\nyet their scale makes real-world deployment costly. Post-training quantization\nreduces memory and computation but often degrades accuracy, while\nquantization-aware training can recover performance at the cost of extra\ntraining. Pushing quantization to the ternary (2-bit) regime yields even larger\nsavings but is notoriously unstable. Building on recent work showing that a\nbias-free, RMS-normalized Transformer with straight-through estimation can\nreach 1.58-bit precision, we demonstrate that simply inserting RMS\nnormalization before every linear projection and applying a gradual, layer-wise\nquantization schedule stably fine-tunes full-precision checkpoints into ternary\nLLMs. Our approach matches or surpasses more elaborate knowledge-distillation\npipelines on standard language-modeling benchmarks without adding model\ncomplexity. These results indicate that careful normalization alone can close\nmuch of the accuracy gap between ternary and full-precision LLMs, making\nultra-low-bit inference practical.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u7ebf\u6027\u6295\u5f71\u524d\u63d2\u5165RMS\u5f52\u4e00\u5316\u5e76\u5e94\u7528\u6e10\u8fdb\u7684\u3001\u9010\u5c42\u7684\u91cf\u5316\u8ba1\u5212\uff0c\u7a33\u5b9a\u5730\u5c06\u5168\u7cbe\u5ea6\u68c0\u67e5\u70b9\u5fae\u8c03\u4e3a\u4e09\u8fdb\u5236LLM\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u4e0a\u4e0e\u66f4\u590d\u6742\u7684\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\u76f8\u5339\u914d\u6216\u8d85\u8d8a\uff0c\u800c\u6ca1\u6709\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u4ed4\u7ec6\u7684\u5f52\u4e00\u5316\u672c\u8eab\u5c31\u53ef\u4ee5\u5f25\u5408\u4e8c\u8fdb\u5236\u548c\u5168\u7cbe\u5ea6LLM\u4e4b\u95f4\u7684\u51c6\u786e\u5ea6\u5dee\u8ddd\uff0c\u4f7f\u8d85\u4f4e\u6bd4\u7279\u63a8\u7406\u6210\u4e3a\u53ef\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f46\u5b83\u4eec\u7684\u89c4\u6a21\u4f7f\u5f97\u5b9e\u9645\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002\u540e\u8bad\u7ec3\u91cf\u5316\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\uff0c\u4f46\u901a\u5e38\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u800c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u53ef\u4ee5\u5728\u989d\u5916\u8bad\u7ec3\u7684\u6210\u672c\u4e0b\u6062\u590d\u6027\u80fd\u3002\u5c06\u91cf\u5316\u63a8\u5411\u4e09\u8fdb\u5236\uff082\u4f4d\uff09\u5236\u5ea6\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5927\u7684\u8282\u7701\uff0c\u4f46\u4f17\u6240\u5468\u77e5\u8fd9\u662f\u4e0d\u7a33\u5b9a\u7684\u3002", "method": "\u6211\u4eec\u5728\u6bcf\u4e2a\u7ebf\u6027\u6295\u5f71\u524d\u63d2\u5165RMS\u5f52\u4e00\u5316\uff0c\u5e76\u5e94\u7528\u6e10\u8fdb\u7684\u3001\u9010\u5c42\u7684\u91cf\u5316\u8ba1\u5212\uff0c\u4ee5\u7a33\u5b9a\u5fae\u8c03\u5168\u7cbe\u5ea6\u68c0\u67e5\u70b9\u4e3a\u4e09\u8fdb\u5236LLM\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u4e0a\u4e0e\u66f4\u590d\u6742\u7684\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\u76f8\u5339\u914d\u6216\u8d85\u8d8a\uff0c\u800c\u6ca1\u6709\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u4ed4\u7ec6\u7684\u5f52\u4e00\u5316\u672c\u8eab\u5c31\u53ef\u4ee5\u5f25\u5408\u4e8c\u8fdb\u5236\u548c\u5168\u7cbe\u5ea6LLM\u4e4b\u95f4\u7684\u51c6\u786e\u5ea6\u5dee\u8ddd\uff0c\u4f7f\u8d85\u4f4e\u6bd4\u7279\u63a8\u7406\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2505.08814", "pdf": "https://arxiv.org/pdf/2505.08814", "abs": "https://arxiv.org/abs/2505.08814", "authors": ["Wenkai Li", "Xiaoqi Li", "Yingjie Mao", "Yishun Wang"], "title": "Towards Understanding Deep Learning Model in Image Recognition via Coverage Test", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) play a crucial role in the field of artificial\nintelligence, and their security-related testing has been a prominent research\nfocus. By inputting test cases, the behavior of models is examined for\nanomalies, and coverage metrics are utilized to determine the extent of neurons\ncovered by these test cases. With the widespread application and advancement of\nDNNs, different types of neural behaviors have garnered attention, leading to\nthe emergence of various coverage metrics for neural networks. However, there\nis currently a lack of empirical research on these coverage metrics,\nspecifically in analyzing the relationships and patterns between model depth,\nconfiguration information, and neural network coverage. This paper aims to\ninvestigate the relationships and patterns of four coverage metrics: primary\nfunctionality, boundary, hierarchy, and structural coverage. A series of\nempirical experiments were conducted, selecting LeNet, VGG, and ResNet as\ndifferent DNN architectures, along with 10 models of varying depths ranging\nfrom 5 to 54 layers, to compare and study the relationships between different\ndepths, configuration information, and various neural network coverage metrics.\nAdditionally, an investigation was carried out on the relationships between\nmodified decision/condition coverage and dataset size. Finally, three potential\nfuture directions are proposed to further contribute to the security testing of\nDNN Models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4fc3\u8fdbDNN\u6a21\u578b\u7684\u5b89\u5168\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740DNN\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u8fdb\u6b65\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u795e\u7ecf\u884c\u4e3a\u5f15\u8d77\u4e86\u5173\u6ce8\uff0c\u5bfc\u81f4\u4e86\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u5ea6\u91cf\u7684\u51fa\u73b0\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u8986\u76d6\u5ea6\u91cf\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u5206\u6790\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u4e4b\u95f4\u7684\u5173\u7cfb\u65b9\u9762\u3002", "method": "\u672c\u6587\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u9009\u62e9\u4e86LeNet\u3001VGG\u548cResNet\u4f5c\u4e3a\u4e0d\u540c\u7684DNN\u67b6\u6784\uff0c\u5e76\u9009\u62e9\u4e8610\u4e2a\u4e0d\u540c\u6df1\u5ea6\u7684\u6a21\u578b\uff08\u4ece5\u5c42\u523054\u5c42\uff09\u6765\u6bd4\u8f83\u548c\u7814\u7a76\u4e0d\u540c\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u4fee\u6539\u540e\u7684\u51b3\u7b56/\u6761\u4ef6\u8986\u76d6\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u672c\u6587\u7814\u7a76\u4e86\u56db\u79cd\u8986\u76d6\u5ea6\u91cf\uff1a\u4e3b\u8981\u529f\u80fd\u3001\u8fb9\u754c\u3001\u5c42\u6b21\u548c\u7ed3\u6784\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u548c\u6a21\u5f0f\u3002\u5b9e\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u548c\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u4fee\u6539\u540e\u7684\u51b3\u7b56/\u6761\u4ef6\u8986\u76d6\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u4e4b\u95f4\u4e5f\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4fc3\u8fdbDNN\u6a21\u578b\u7684\u5b89\u5168\u6d4b\u8bd5\u3002"}}
{"id": "2505.09269", "pdf": "https://arxiv.org/pdf/2505.09269", "abs": "https://arxiv.org/abs/2505.09269", "authors": ["Ulrich Frank", "Pierre Maier"], "title": "How an unintended Side Effect of a Research Project led to Boosting the Power of UML", "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the design, implementation and use of a new UML modeling\ntool that represents a significant advance over conventional tools. Among other\nthings, it allows the integration of class diagrams and object diagrams as well\nas the execution of objects. This not only enables new software architectures\ncharacterized by the integration of software with corresponding object models,\nbut is also ideal for use in teaching, as it provides students with a\nparticularly stimulating learning experience. A special feature of the project\nis that it has emerged from a long-standing international research project,\nwhich is aimed at a comprehensive multi-level architecture. The project is\ntherefore an example of how research can lead to valuable results that arise as\na side effect of other work.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684UML\u5efa\u6a21\u5de5\u5177\uff0c\u5b83\u5728\u4f20\u7edf\u5de5\u5177\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u91cd\u5927\u6539\u8fdb\uff0c\u53ef\u4ee5\u96c6\u6210\u7c7b\u56fe\u548c\u5bf9\u8c61\u56fe\u5e76\u6267\u884c\u5bf9\u8c61\uff0c\u9002\u7528\u4e8e\u65b0\u7684\u8f6f\u4ef6\u67b6\u6784\u548c\u6559\u5b66\u3002", "motivation": "\u5f00\u53d1\u65b0\u7684UML\u5efa\u6a21\u5de5\u5177\u662f\u4e3a\u4e86\u6539\u8fdb\u4f20\u7edf\u7684\u5de5\u5177\uff0c\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u529f\u80fd\uff0c\u5982\u96c6\u6210\u7c7b\u56fe\u548c\u5bf9\u8c61\u56fe\u4ee5\u53ca\u6267\u884c\u5bf9\u8c61\uff0c\u4ece\u800c\u652f\u6301\u65b0\u7684\u8f6f\u4ef6\u67b6\u6784\u548c\u6559\u5b66\u5e94\u7528\u3002", "method": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u65b0\u7684UML\u5efa\u6a21\u5de5\u5177\u7684\u8bbe\u8ba1\u3001\u5b9e\u73b0\u548c\u4f7f\u7528\uff0c\u901a\u8fc7\u96c6\u6210\u7c7b\u56fe\u548c\u5bf9\u8c61\u56fe\u4ee5\u53ca\u6267\u884c\u5bf9\u8c61\u6765\u5b9e\u73b0\u5176\u529f\u80fd\u3002", "result": "\u8be5\u5de5\u5177\u5141\u8bb8\u96c6\u6210\u7c7b\u56fe\u548c\u5bf9\u8c61\u56fe\u4ee5\u53ca\u6267\u884c\u5bf9\u8c61\uff0c\u4e3a\u8f6f\u4ef6\u67b6\u6784\u548c\u6559\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86\u4e00\u4e2a\u65b0\u7684UML\u5efa\u6a21\u5de5\u5177\u7684\u8bbe\u8ba1\u3001\u5b9e\u73b0\u548c\u4f7f\u7528\uff0c\u8be5\u5de5\u5177\u5728\u4f20\u7edf\u5de5\u5177\u4e0a\u6709\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u5b83\u5141\u8bb8\u96c6\u6210\u7c7b\u56fe\u548c\u5bf9\u8c61\u56fe\u4ee5\u53ca\u6267\u884c\u5bf9\u8c61\uff0c\u8fd9\u4e0d\u4ec5\u80fd\u591f\u5b9e\u73b0\u65b0\u7684\u8f6f\u4ef6\u67b6\u6784\uff0c\u800c\u4e14\u5728\u6559\u5b66\u4e2d\u4e5f\u975e\u5e38\u6709\u7528\u3002\u8be5\u9879\u76ee\u6e90\u4e8e\u4e00\u4e2a\u957f\u671f\u7684\u56fd\u9645\u7814\u7a76\u9879\u76ee\uff0c\u5c55\u793a\u4e86\u7814\u7a76\u5982\u4f55\u4ea7\u751f\u6709\u4ef7\u503c\u7684\u526f\u4ea7\u54c1\u3002"}}
{"id": "2505.09024", "pdf": "https://arxiv.org/pdf/2505.09024", "abs": "https://arxiv.org/abs/2505.09024", "authors": ["Aaron Baughman", "Rahul Agarwal", "Eduardo Morales", "Gozde Akay"], "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, 3 tables", "summary": "We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5143\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u4eba\u7c7b\u5fc3\u7406\u9884\u671f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u5904\u7406\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u63d0\u9ad8\u590d\u6742\u4efb\u52a1\u7684\u6587\u672c\u751f\u6210\u8d28\u91cf\u3002\u5e94\u7528\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4f7fLLMaaJ\u80fd\u591f\u9884\u6d4b\u5e76\u5305\u542b\u4eba\u7c7b\u7f16\u8f91\uff0c\u4ece\u800c\u89e3\u51b3\u5fc3\u667a\u7406\u8bba\u5bf9\u9f50\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u5185\u5bb9\u8bc4\u5ba1\u7684\u671f\u671b\u4e0eAI\u5bf9\u9f50\u7387\u8fbe\u523053.8%\u3002", "motivation": "\u4e3a\u4e86\u8861\u91cf\u4eba\u7c7b\u5728\u5185\u5bb9\u751f\u4ea7\u4e2d\u7684\u5fc3\u7406\u4fe1\u5ff5\uff0c\u7528\u6237\u57282024\u5e74\u7f8e\u56fd\u516c\u5f00\u8d5b\u524d\u4fee\u6539\u957f\u7bc7AI\u751f\u6210\u7684\u6587\u7ae0\u3002LLMaaJ\u53ef\u4ee5\u901a\u8fc7\u9884\u6d4b\u5e76\u5305\u542b\u4eba\u7c7b\u7f16\u8f91\u6765\u89e3\u51b3\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u521b\u5efa\u6587\u672c\u65f6\u8003\u8651\u8fd9\u4e9b\u7f16\u8f91\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5143\u63d0\u793a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8054\u5408\u751f\u6210\u6d41\u7545\u6587\u672c\u4ee5\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u540c\u65f6\u4f18\u5316\u4eba\u7c7b\u5fc3\u7406\u9884\u671f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u795e\u7ecf\u5904\u7406\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5e94\u7528\u4e86\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u5176\u4e2dLLMaaJ\u4f5c\u4e3a\u88c1\u5224\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6559\u53e6\u4e00\u4e2aLLM\u5982\u4f55\u901a\u8fc7\u89e3\u91ca\u751f\u6210\u6587\u672c\u7684\u610f\u56fe\u548c\u975e\u610f\u56fe\u7279\u5f81\u6765\u751f\u6210\u5185\u5bb9\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u5bf9\u5b9e\u65f6\u751f\u4ea7\u7cfb\u7edf\u7684\u89e3\u8bfb\uff0c\u4eba\u7c7b\u5185\u5bb9\u8bc4\u5ba1\u7684\u671f\u671b\u6709100%\u4e0eAI\u5bf9\u9f50\uff0c\u5176\u4e2d53.8%\u7684\u65f6\u95f4\u5e73\u5747\u8fed\u4ee3\u6b21\u6570\u4e3a4.38\u3002\u5185\u5bb9\u7279\u5f81\u5982\u4e8b\u5b9e\u6027\u3001\u65b0\u9896\u6027\u3001\u91cd\u590d\u6027\u548c\u76f8\u5173\u6027\u7684\u51e0\u4f55\u89e3\u91ca\u5728\u5e0c\u5c14\u4f2f\u7279\u5411\u91cf\u7a7a\u95f4\u4e2d\u7ed3\u5408\u4e86\u7a7a\u95f4\u4f53\u79ef\uff08\u6240\u6709\u7279\u5f81\u91cd\u8981\u6027\uff09\u548c\u9876\u70b9\u5bf9\u9f50\uff08\u5355\u4e2a\u7279\u5f81\u76f8\u5173\u6027\uff09\uff0c\u4f7fLLMaaJ\u80fd\u591f\u4f18\u5316\u4eba\u7c7b\u5fc3\u667a\u7406\u8bba\u3002\u8fd9\u5bfc\u81f4\u4e86\u5185\u5bb9\u8d28\u91cf\u7684\u63d0\u5347\uff0c\u6269\u5c55\u4e86\u7f51\u7403\u52a8\u4f5c\u7684\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u57282024\u5e74\u7f8e\u56fd\u516c\u5f00\u8d5b\u4e0a\u90e8\u7f72\uff0c\u5e76\u5df2\u7528\u4e8e\u5176\u4ed6\u4f53\u80b2\u548c\u5a31\u4e50\u9886\u57df\u7684\u5b9e\u65f6\u4e8b\u4ef6\u3002"}}
{"id": "2505.08827", "pdf": "https://arxiv.org/pdf/2505.08827", "abs": "https://arxiv.org/abs/2505.08827", "authors": ["Toby Simonds", "Kevin Lopez", "Akira Yoshiyama", "Dominique Garmier"], "title": "Self Rewarding Self Improving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We demonstrate that large language models can effectively self-improve\nthrough self-judging without requiring reference solutions, leveraging the\ninherent asymmetry between generating and verifying solutions. Our experiments\non Countdown puzzles and MIT Integration Bee problems show that models can\nprovide reliable reward signals without ground truth answers, enabling\nreinforcement learning in domains previously not possible. By implementing\nself-judging, we achieve significant performance gains maintaining alignment\nwith formal verification. When combined with synthetic question generation, we\nestablish a complete self-improvement loop where models generate practice\nproblems, solve them, and evaluate their own performance-achieving an 8%\nimprovement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on\nintegration tasks. Our findings demonstrate that LLM judges can provide\neffective reward signals for training models, unlocking many reinforcement\nlearning environments previously limited by the difficulty of creating\nprogrammatic rewards. This suggests a potential paradigm shift toward AI\nsystems that continuously improve through self-directed learning rather than\nhuman-guided training, potentially accelerating progress in domains with scarce\ntraining data or complex evaluation requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u5373\u53ef\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u4f9b\u5956\u52b1\u4fe1\u53f7\uff0c\u4f46\u5728\u67d0\u4e9b\u9886\u57df\u4e2d\uff0c\u83b7\u53d6\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u662f\u56f0\u96be\u7684\u3002\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u6ca1\u6709\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u81ea\u6211\u6539\u8fdb\uff0c\u4ece\u800c\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\u5b9e\u73b0\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb\uff0c\u5229\u7528\u751f\u6210\u548c\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u56fa\u6709\u4e0d\u5bf9\u79f0\u6027\u3002\u6211\u4eec\u5b9e\u73b0\u4e86\u81ea\u6211\u8bc4\u5224\uff0c\u5e76\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\uff0c\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u81ea\u6211\u6539\u8fdb\u5faa\u73af\uff0c\u8ba9\u6a21\u578b\u751f\u6210\u7ec3\u4e60\u9898\u3001\u89e3\u51b3\u95ee\u9898\u5e76\u8bc4\u4f30\u81ea\u5df1\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u6ca1\u6709\u771f\u5b9e\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u5728Countdown\u8c1c\u9898\u548cMIT\u79ef\u5206\u871c\u8702\u95ee\u9898\u4e0a\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\u540e\uff0c\u6a21\u578b\u5b9e\u73b0\u4e868%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u79ef\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u7684\u8868\u73b0\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u88c1\u5224\u53ef\u4ee5\u4e3a\u8bad\u7ec3\u6a21\u578b\u63d0\u4f9b\u6709\u6548\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u89e3\u9501\u4e86\u8bb8\u591a\u4ee5\u524d\u56e0\u521b\u5efa\u7a0b\u5e8f\u5316\u5956\u52b1\u7684\u96be\u5ea6\u800c\u53d7\u9650\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u3002\u8fd9\u6697\u793a\u7740\u4e00\u79cd\u53ef\u80fd\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5373AI\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u81ea\u6211\u5f15\u5bfc\u7684\u5b66\u4e60\u6301\u7eed\u6539\u8fdb\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u6307\u5bfc\u7684\u8bad\u7ec3\uff0c\u8fd9\u53ef\u80fd\u52a0\u901f\u5728\u6570\u636e\u7a00\u7f3a\u6216\u8bc4\u4f30\u8981\u6c42\u590d\u6742\u7684\u9886\u57df\u4e2d\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.08817", "pdf": "https://arxiv.org/pdf/2505.08817", "abs": "https://arxiv.org/abs/2505.08817", "authors": ["Camilo Carvajal Reyes", "Joaqu\u00edn Fontbona", "Felipe Tobar"], "title": "Towards SFW sampling for diffusion models via external conditioning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepcted at IJCNN 2025", "summary": "Score-based generative models (SBM), also known as diffusion models, are the\nde facto state of the art for image synthesis. Despite their unparalleled\nperformance, SBMs have recently been in the spotlight for being tricked into\ncreating not-safe-for-work (NSFW) content, such as violent images and\nnon-consensual nudity. Current approaches that prevent unsafe generation are\nbased on the models' own knowledge, and the majority of them require\nfine-tuning. This article explores the use of external sources for ensuring\nsafe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional\nTrajectory Correction step that guides the samples away from undesired regions\nin the ambient space using multimodal models as the source of conditioning.\nFurthermore, using Contrastive Language Image Pre-training (CLIP), our method\nadmits user-defined NSFW classes, which can vary in different settings. Our\nexperiments on the text-to-image SBM Stable Diffusion validate that the\nproposed SFW sampler effectively reduces the generation of explicit content\nwhile being competitive with other fine-tuning-based approaches, as assessed\nvia independent NSFW detectors. Moreover, we evaluate the impact of the SFW\nsampler on image quality and show that the proposed correction scheme comes at\na minor cost with negligible effect on samples not needing correction. Our\nstudy confirms the suitability of the SFW sampler towards aligned SBM models\nand the potential of using model-agnostic conditioning for the prevention of\nunwanted images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5916\u90e8\u6761\u4ef6\u7684SFW\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u9632\u6b62\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6548\u679c\u826f\u597d\u4e14\u5bf9\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u5c0f\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u57fa\u4e8e\u6a21\u578b\u81ea\u8eab\u7684\u77e5\u8bc6\uff0c\u5927\u591a\u6570\u9700\u8981\u5fae\u8c03\u3002\u672c\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u5916\u90e8\u6e90\u6765\u786e\u4fddSBM\u7684\u5b89\u5168\u8f93\u51fa\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b89\u5168\u7684SFW\u91c7\u6837\u5668\uff0c\u8be5\u91c7\u6837\u5668\u5b9e\u73b0\u4e86\u6761\u4ef6\u8f68\u8ff9\u6821\u6b63\u6b65\u9aa4\uff0c\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a\u6761\u4ef6\u6765\u6e90\uff0c\u5f15\u5bfc\u6837\u672c\u8fdc\u79bb\u4e0d\u9700\u8981\u7684\u533a\u57df\u3002\u6b64\u5916\uff0c\u4f7f\u7528CLIP\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5141\u8bb8\u7528\u6237\u5b9a\u4e49NSFW\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SFW\u91c7\u6837\u5668\u6709\u6548\u51cf\u5c11\u4e86\u663e\u5f0f\u5185\u5bb9\u7684\u751f\u6210\uff0c\u540c\u65f6\u4e0e\u5176\u4ed6\u57fa\u4e8e\u5fae\u8c03\u7684\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0c\u8bc4\u4f30\u4e86SFW\u91c7\u6837\u5668\u5bf9\u56fe\u50cf\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u63d0\u51fa\u7684\u6821\u6b63\u65b9\u6848\u6210\u672c\u8f83\u4f4e\uff0c\u5bf9\u4e0d\u9700\u8981\u6821\u6b63\u7684\u6837\u672c\u5f71\u54cd\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u672c\u7814\u7a76\u786e\u8ba4\u4e86SFW\u91c7\u6837\u5668\u9002\u7528\u4e8e\u5bf9\u9f50\u7684SBM\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528\u6a21\u578b\u65e0\u5173\u7684\u6761\u4ef6\u6765\u9632\u6b62\u4e0d\u60f3\u8981\u7684\u56fe\u50cf\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09286", "pdf": "https://arxiv.org/pdf/2505.09286", "abs": "https://arxiv.org/abs/2505.09286", "authors": ["Jiin Park", "Misuk Kim"], "title": "A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data", "categories": ["cs.CL"], "comment": "36 pages, 3 figures", "summary": "Effectively analyzing online review data is essential across industries.\nHowever, many existing studies are limited to specific domains and languages or\ndepend on supervised learning approaches that require large-scale labeled\ndatasets. To address these limitations, we propose a multilingual, scalable,\nand unsupervised framework for cross-domain aspect detection. This framework is\ndesigned for multi-aspect labeling of multilingual and multi-domain review\ndata. In this study, we apply automatic labeling to Korean and English review\ndatasets spanning various domains and assess the quality of the generated\nlabels through extensive experiments. Aspect category candidates are first\nextracted through clustering, and each review is then represented as an\naspect-aware embedding vector using negative sampling. To evaluate the\nframework, we conduct multi-aspect labeling and fine-tune several pretrained\nlanguage models to measure the effectiveness of the automatically generated\nlabels. Results show that these models achieve high performance, demonstrating\nthat the labels are suitable for training. Furthermore, comparisons with\npublicly available large language models highlight the framework's superior\nconsistency and scalability when processing large-scale data. A human\nevaluation also confirms that the quality of the automatic labels is comparable\nto those created manually. This study demonstrates the potential of a robust\nmulti-aspect labeling approach that overcomes limitations of supervised methods\nand is adaptable to multilingual, multi-domain environments. Future research\nwill explore automatic review summarization and the integration of artificial\nintelligence agents to further improve the efficiency and depth of review\nanalysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u53ef\u6269\u5c55\u4e14\u65e0\u76d1\u7763\u7684\u8de8\u9886\u57df\u65b9\u9762\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\u8bc4\u8bba\u6570\u636e\u8fdb\u884c\u591a\u65b9\u9762\u6807\u8bb0\u3002\u901a\u8fc7\u805a\u7c7b\u63d0\u53d6\u65b9\u9762\u7c7b\u522b\u5019\u9009\uff0c\u5e76\u4f7f\u7528\u8d1f\u91c7\u6837\u751f\u6210\u65b9\u9762\u611f\u77e5\u5d4c\u5165\u5411\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u81ea\u52a8\u751f\u6210\u7684\u6807\u7b7e\u8d28\u91cf\u9ad8\uff0c\u80fd\u591f\u6709\u6548\u7528\u4e8e\u8bad\u7ec3\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u5177\u6709\u66f4\u597d\u7684\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u6709\u6548\u5206\u6790\u5728\u7ebf\u8bc4\u8bba\u6570\u636e\u5bf9\u4e8e\u5404\u4e2a\u884c\u4e1a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8bb8\u591a\u73b0\u6709\u7814\u7a76\u4ec5\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u548c\u8bed\u8a00\uff0c\u6216\u8005\u4f9d\u8d56\u9700\u8981\u5927\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u96c6\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u3001\u591a\u9886\u57df\u8bc4\u8bba\u6570\u636e\u7684\u591a\u65b9\u9762\u6807\u8bb0\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u53ef\u6269\u5c55\u4e14\u65e0\u76d1\u7763\u7684\u8de8\u9886\u57df\u65b9\u9762\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u65e8\u5728\u5bf9\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\u8bc4\u8bba\u6570\u636e\u8fdb\u884c\u591a\u65b9\u9762\u6807\u8bb0\u3002\u9996\u5148\u901a\u8fc7\u805a\u7c7b\u63d0\u53d6\u65b9\u9762\u7c7b\u522b\u5019\u9009\uff0c\u7136\u540e\u4f7f\u7528\u8d1f\u91c7\u6837\u5c06\u6bcf\u6761\u8bc4\u8bba\u8868\u793a\u4e3a\u65b9\u9762\u611f\u77e5\u5d4c\u5165\u5411\u91cf\u3002\u4e3a\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u591a\u65b9\u9762\u6807\u8bb0\u5e76\u5fae\u8c03\u4e86\u51e0\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u8861\u91cf\u81ea\u52a8\u751f\u6210\u6807\u7b7e\u7684\u6709\u6548\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u751f\u6210\u7684\u6807\u7b7e\u9002\u5408\u7528\u4e8e\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u4e0e\u516c\u5f00\u53ef\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u7a81\u663e\u4e86\u8be5\u6846\u67b6\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u7684\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u4eba\u5de5\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u81ea\u52a8\u6807\u7b7e\u7684\u8d28\u91cf\u4e0e\u624b\u52a8\u521b\u5efa\u7684\u6807\u7b7e\u76f8\u5f53\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u591a\u65b9\u9762\u6807\u8bb0\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u514b\u670d\u4e86\u76d1\u7763\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5e76\u9002\u5e94\u591a\u8bed\u8a00\u3001\u591a\u9886\u57df\u73af\u5883\u3002\u672a\u6765\u7684\u7814\u7a76\u5c06\u63a2\u7d22\u81ea\u52a8\u8bc4\u8bba\u6458\u8981\u548c\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u6574\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8bc4\u8bba\u5206\u6790\u7684\u6548\u7387\u548c\u6df1\u5ea6\u3002"}}
{"id": "2505.09029", "pdf": "https://arxiv.org/pdf/2505.09029", "abs": "https://arxiv.org/abs/2505.09029", "authors": ["Hazim Alzorgan", "Abolfazl Razi"], "title": "Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient\n(TD3), depend on basic noise-based exploration, which can result in less than\noptimal policy convergence. In this study, we introduce Monte Carlo Beam Search\n(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts\nwith TD3 to improve exploration and action selection. MCBS produces several\ncandidate actions around the policy's output and assesses them through\nshort-horizon rollouts, enabling the agent to make better-informed choices. We\ntest MCBS across various continuous-control benchmarks, including\nHalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency\nand performance compared to standard TD3 and other baseline methods like SAC,\nPPO, and A2C. Our findings emphasize MCBS's capability to enhance policy\nlearning through structured look-ahead search while ensuring computational\nefficiency. Additionally, we offer a detailed analysis of crucial\nhyperparameters, such as beam width and rollout depth, and explore adaptive\nstrategies to optimize MCBS for complex control tasks. Our method shows a\nhigher convergence rate across different environments compared to TD3, SAC,\nPPO, and A2C. For instance, we achieved 90% of the maximum achievable reward\nwithin around 200 thousand timesteps compared to 400 thousand timesteps for the\nsecond-best method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u65b9\u6cd5MCBS\uff0c\u7ed3\u5408\u4e86\u675f\u641c\u7d22\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e0eTD3\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u548c\u52a8\u4f5c\u9009\u62e9\u7684\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMCBS\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u566a\u58f0\u7684\u63a2\u7d22\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7b56\u7565\u6536\u655b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u63a2\u7d22\u548c\u52a8\u4f5c\u9009\u62e9\u65b9\u6cd5\u3002", "method": "MCBS\u7ed3\u5408\u4e86\u675f\u641c\u7d22\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e0eTD3\uff0c\u4ee5\u6539\u8fdb\u63a2\u7d22\u548c\u52a8\u4f5c\u9009\u62e9\u3002MCBS\u751f\u6210\u653f\u7b56\u8f93\u51fa\u5468\u56f4\u7684\u51e0\u4e2a\u5019\u9009\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u77ed\u65f6\u7a0b\u6a21\u62df\u8bc4\u4f30\u5b83\u4eec\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u505a\u51fa\u66f4\u597d\u7684\u51b3\u7b56\u3002", "result": "MCBS\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ecHalfCheetah-v4\u3001Walker2d-v5\u548cSwimmer-v5\uff0c\u663e\u793a\u4e86\u6bd4\u6807\u51c6TD3\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982SAC\u3001PPO\u548cA2C\uff09\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002\u6b64\u5916\uff0cMCBS\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "MCBS\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u6bd4TD3\u3001SAC\u3001PPO\u548cA2C\u7b49\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2505.08829", "pdf": "https://arxiv.org/pdf/2505.08829", "abs": "https://arxiv.org/abs/2505.08829", "authors": ["David Kinney"], "title": "Aggregating Concepts of Fairness and Accuracy in Predictive Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "An algorithm that outputs predictions about the state of the world will\nalmost always be designed with the implicit or explicit goal of outputting\naccurate predictions (i.e., predictions that are likely to be true). In\naddition, the rise of increasingly powerful predictive algorithms brought about\nby the recent revolution in artificial intelligence has led to an emphasis on\nbuilding predictive algorithms that are fair, in the sense that their\npredictions do not systematically evince bias or bring about harm to certain\nindividuals or groups. This state of affairs presents two conceptual\nchallenges. First, the goals of accuracy and fairness can sometimes be in\ntension, and there are no obvious normative guidelines for managing the\ntrade-offs between these two desiderata when they arise. Second, there are many\ndistinct ways of measuring both the accuracy and fairness of a predictive\nalgorithm; here too, there are no obvious guidelines on how to aggregate our\npreferences for predictive algorithms that satisfy disparate measures of\nfairness and accuracy to various extents. The goal of this paper is to address\nthese challenges by arguing that there are good reasons for using a linear\ncombination of accuracy and fairness metrics to measure the\nall-things-considered value of a predictive algorithm for agents who care about\nboth accuracy and fairness. My argument depends crucially on a classic result\nin the preference aggregation literature due to Harsanyi. After making this\nformal argument, I apply my result to an analysis of accuracy-fairness\ntrade-offs using the COMPAS dataset compiled by Angwin et al.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9884\u6d4b\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u7ebf\u6027\u7ec4\u5408\u6765\u8861\u91cf\u5176\u603b\u4f53\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u9884\u6d4b\u7b97\u6cd5\u9700\u8981\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u6307\u5bfc\u539f\u5219\u3002", "method": "\u672c\u6587\u4f9d\u8d56\u4e8eHarsanyi\u5728\u504f\u597d\u805a\u5408\u6587\u732e\u4e2d\u7684\u7ecf\u5178\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6b63\u5f0f\u7684\u8bba\u70b9\uff0c\u5e76\u5e94\u7528\u8be5\u7ed3\u679c\u5bf9COMPAS\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u672c\u6587\u8bba\u8bc1\u4e86\u4f7f\u7528\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u5ea6\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\u4f5c\u4e3a\u9884\u6d4b\u7b97\u6cd5\u603b\u4f53\u4ef7\u503c\u7684\u8861\u91cf\u65b9\u6cd5\u662f\u5408\u7406\u7684\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u4f7f\u7528\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u5ea6\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u8861\u91cf\u9884\u6d4b\u7b97\u6cd5\u7684\u603b\u4f53\u4ef7\u503c\u662f\u6709\u9053\u7406\u7684\u3002"}}
{"id": "2505.08833", "pdf": "https://arxiv.org/pdf/2505.08833", "abs": "https://arxiv.org/abs/2505.08833", "authors": ["Qingyi Wang", "Yuebing Liang", "Yunhan Zheng", "Kaiyuan Xu", "Jinhua Zhao", "Shenhao Wang"], "title": "Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Generative AI offers new opportunities for automating urban planning by\ncreating site-specific urban layouts and enabling flexible design exploration.\nHowever, existing approaches often struggle to produce realistic and practical\ndesigns at scale. Therefore, we adapt a state-of-the-art Stable Diffusion\nmodel, extended with ControlNet, to generate high-fidelity satellite imagery\nconditioned on land use descriptions, infrastructure, and natural environments.\nTo overcome data availability limitations, we spatially link satellite imagery\nwith structured land use and constraint information from OpenStreetMap. Using\ndata from three major U.S. cities, we demonstrate that the proposed diffusion\nmodel generates realistic and diverse urban landscapes by varying land-use\nconfigurations, road networks, and water bodies, facilitating cross-city\nlearning and design diversity. We also systematically evaluate the impacts of\nvarying language prompts and control imagery on the quality of satellite\nimagery generation. Our model achieves high FID and KID scores and demonstrates\nrobustness across diverse urban contexts. Qualitative assessments from urban\nplanners and the general public show that generated images align closely with\ndesign descriptions and constraints, and are often preferred over real images.\nThis work establishes a benchmark for controlled urban imagery generation and\nhighlights the potential of generative AI as a tool for enhancing planning\nworkflows and public engagement.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4f7f\u7528\u751f\u6210\u5f0fAI\u6765\u81ea\u52a8\u5316\u57ce\u5e02\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u5e94Stable Diffusion\u6a21\u578b\u5e76\u7ed3\u5408ControlNet\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u536b\u661f\u56fe\u50cf\u3002\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u6a21\u578b\u5728\u751f\u6210\u903c\u771f\u548c\u591a\u6837\u5316\u57ce\u5e02\u666f\u89c2\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u5728\u589e\u5f3a\u89c4\u5212\u5de5\u4f5c\u6d41\u7a0b\u548c\u516c\u4f17\u53c2\u4e0e\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5e38\u5e38\u96be\u4ee5\u5728\u5927\u89c4\u6a21\u4e0a\u751f\u6210\u73b0\u5b9e\u4e14\u5b9e\u7528\u7684\u8bbe\u8ba1\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u751f\u6210\u5f0fAI\u6765\u81ea\u52a8\u5316\u57ce\u5e02\u89c4\u5212\uff0c\u521b\u9020\u7279\u5b9a\u4e8e\u573a\u5730\u7684\u57ce\u5e02\u5e03\u5c40\u5e76\u5b9e\u73b0\u7075\u6d3b\u7684\u8bbe\u8ba1\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u9002\u5e94\u4e86\u6700\u5148\u8fdb\u7684Stable Diffusion\u6a21\u578b\uff0c\u5e76\u901a\u8fc7ControlNet\u8fdb\u884c\u6269\u5c55\uff0c\u4ee5\u751f\u6210\u57fa\u4e8e\u571f\u5730\u5229\u7528\u63cf\u8ff0\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u81ea\u7136\u73af\u5883\u7684\u9ad8\u4fdd\u771f\u536b\u661f\u56fe\u50cf\u3002\u6211\u4eec\u8fd8\u5c06\u536b\u661f\u56fe\u50cf\u4e0e\u6765\u81eaOpenStreetMap\u7684\u7ed3\u6784\u5316\u571f\u5730\u5229\u7528\u548c\u7ea6\u675f\u4fe1\u606f\u8fdb\u884c\u7a7a\u95f4\u5173\u8054\uff0c\u4ee5\u514b\u670d\u6570\u636e\u53ef\u7528\u6027\u7684\u9650\u5236\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u4e09\u4e2a\u4e3b\u8981\u7f8e\u56fd\u57ce\u5e02\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u80fd\u591f\u901a\u8fc7\u6539\u53d8\u571f\u5730\u5229\u7528\u914d\u7f6e\u3001\u9053\u8def\u7f51\u7edc\u548c\u6c34\u4f53\u751f\u6210\u903c\u771f\u4e14\u591a\u6837\u7684\u57ce\u5e02\u666f\u89c2\uff0c\u4fc3\u8fdb\u8de8\u57ce\u5e02\u5b66\u4e60\u548c\u8bbe\u8ba1\u591a\u6837\u6027\u3002\u6211\u4eec\u8fd8\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e0d\u540c\u8bed\u8a00\u63d0\u793a\u548c\u63a7\u5236\u56fe\u50cf\u5bf9\u536b\u661f\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8FID\u548cKID\u5206\u6570\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\u3002\u57ce\u5e02\u89c4\u5212\u8005\u548c\u666e\u901a\u516c\u4f17\u7684\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u8bbe\u8ba1\u63cf\u8ff0\u548c\u7ea6\u675f\u975e\u5e38\u63a5\u8fd1\uff0c\u5e76\u4e14\u901a\u5e38\u4f18\u4e8e\u771f\u5b9e\u56fe\u50cf\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u53d7\u63a7\u57ce\u5e02\u56fe\u50cf\u751f\u6210\u7684\u57fa\u51c6\uff0c\u5e76\u7a81\u663e\u4e86\u751f\u6210\u5f0fAI\u4f5c\u4e3a\u589e\u5f3a\u89c4\u5212\u5de5\u4f5c\u6d41\u7a0b\u548c\u516c\u4f17\u53c2\u4e0e\u5de5\u5177\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09316", "pdf": "https://arxiv.org/pdf/2505.09316", "abs": "https://arxiv.org/abs/2505.09316", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging", "categories": ["cs.CL", "cs.IR"], "comment": "16 pages", "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.", "AI": {"tldr": "InForage is a reinforcement learning framework that enhances large language models by enabling dynamic, adaptive retrieval during inference, leading to improved performance on complex tasks.", "motivation": "Traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval.", "method": "InForage is a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. It explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors.", "result": "Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods.", "conclusion": "InForage's results highlight its effectiveness in building robust, adaptive, and efficient reasoning agents."}}
{"id": "2505.09031", "pdf": "https://arxiv.org/pdf/2505.09031", "abs": "https://arxiv.org/abs/2505.09031", "authors": ["Adarsh Kumar", "Hwiyoon Kim", "Jawahar Sai Nathani", "Neil Roy"], "title": "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Hallucination, where large language models (LLMs) generate confident but\nincorrect or irrelevant information, remains a key limitation in their\napplication to complex, open-ended tasks. Chain-of-thought (CoT) prompting has\nemerged as a promising method for improving multistep reasoning by guiding\nmodels through intermediate steps. However, CoT alone does not fully address\nthe hallucination problem. In this work, we investigate how combining CoT with\nretrieval-augmented generation (RAG), as well as applying self-consistency and\nself-verification strategies, can reduce hallucinations and improve factual\naccuracy. By incorporating external knowledge sources during reasoning and\nenabling models to verify or revise their own outputs, we aim to generate more\naccurate and coherent responses. We present a comparative evaluation of\nbaseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification\ntechniques. Our results highlight the effectiveness of each method and identify\nthe most robust approach for minimizing hallucinations while preserving fluency\nand reasoning depth.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u601d\u7ef4\u94fe\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u4ee5\u53ca\u5e94\u7528\u81ea\u4e00\u81f4\u6027\u548c\u81ea\u9a8c\u8bc1\u7b56\u7565\u6765\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5e7b\u89c9\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e94\u7528\u4e8e\u590d\u6742\u3001\u5f00\u653e\u6027\u4efb\u52a1\u65f6\u7684\u4e00\u4e2a\u5173\u952e\u9650\u5236\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6539\u5584\u591a\u6b65\u9aa4\u63a8\u7406\uff0c\u4f46\u5355\u72ec\u7684CoT\u5e76\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u6211\u4eec\u7814\u7a76\u4e86\u5c06\u601d\u7ef4\u94fe\uff08CoT\uff09\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u53ca\u5e94\u7528\u81ea\u4e00\u81f4\u6027\u548c\u81ea\u9a8c\u8bc1\u7b56\u7565\u5982\u4f55\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "result": "\u6211\u4eec\u5bf9\u57fa\u7ebfLLM\u4e0eCoT\u3001CoT+RAG\u3001\u81ea\u4e00\u81f4\u6027\u4ee5\u53ca\u81ea\u9a8c\u8bc1\u6280\u672f\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u51fa\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u4fdd\u6301\u6d41\u7545\u6027\u548c\u63a8\u7406\u6df1\u5ea6\u7684\u540c\u65f6\u6700\u5c0f\u5316\u5e7b\u89c9\u7684\u6700\u7a33\u5065\u65b9\u6cd5\u3002"}}
{"id": "2505.08846", "pdf": "https://arxiv.org/pdf/2505.08846", "abs": "https://arxiv.org/abs/2505.08846", "authors": ["Felix Marti-Perez", "Brigt H\u00e5vardstun", "C\u00e8sar Ferri", "Carlos Monserrat", "Jan Arne Telle"], "title": "Evaluating Simplification Algorithms for Interpretability of Time Series Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we introduce metrics to evaluate the use of simplified time\nseries in the context of interpretability of a TSC - a Time Series Classifier.\nSuch simplifications are important because time series data, in contrast to\ntext and image data, are not intuitively understandable to humans. These\nmetrics are related to the complexity of the simplifications - how many\nsegments they contain - and to their loyalty - how likely they are to maintain\nthe classification of the original time series. We employ these metrics to\nevaluate four distinct simplification algorithms, across several TSC algorithms\nand across datasets of varying characteristics, from seasonal or stationary to\nshort or long. Our findings suggest that using simplifications for\ninterpretability of TSC is much better than using the original time series,\nparticularly when the time series are seasonal, non-stationary and/or with low\nentropy.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u7b80\u5316\u5728TSC\u89e3\u91ca\u4e2d\u4f7f\u7528\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u53d1\u73b0\u7b80\u5316\u65b9\u6cd5\u5728\u7279\u5b9a\u7c7b\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0d\u50cf\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u90a3\u6837\u76f4\u89c2\u6613\u61c2\uff0c\u56e0\u6b64\u9700\u8981\u7b80\u5316\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u4e86\u8bc4\u4f30\u7b80\u5316\u65f6\u95f4\u5e8f\u5217\u5728TSC\u89e3\u91ca\u4e2d\u7684\u4f7f\u7528\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5305\u62ec\u7b80\u5316\u7684\u590d\u6742\u6027\u548c\u5fe0\u8bda\u5ea6\u3002\u8fd9\u4e9b\u5ea6\u91cf\u6807\u51c6\u7528\u4e8e\u8bc4\u4f30\u56db\u79cd\u4e0d\u540c\u7684\u7b80\u5316\u7b97\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u7279\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u7b80\u5316\u7684\u65f6\u95f4\u5e8f\u5217\u8fdb\u884cTSC\u7684\u89e3\u91ca\u6548\u679c\u4f18\u4e8e\u4f7f\u7528\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u5e8f\u5217\u5177\u6709\u5b63\u8282\u6027\u3001\u975e\u5e73\u7a33\u6027\u548c/\u6216\u4f4e\u71b5\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u4f7f\u7528\u7b80\u5316\u7684\u65f6\u95f4\u5e8f\u5217\u5728\u89e3\u91caTSC\u65b9\u9762\u6bd4\u4f7f\u7528\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u66f4\u597d\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u5e8f\u5217\u5177\u6709\u5b63\u8282\u6027\u3001\u975e\u5e73\u7a33\u6027\u548c/\u6216\u4f4e\u71b5\u65f6\u3002"}}
{"id": "2505.08834", "pdf": "https://arxiv.org/pdf/2505.08834", "abs": "https://arxiv.org/abs/2505.08834", "authors": ["Muhammad Junaid Asif"], "title": "Crowd Scene Analysis using Deep Learning Techniques", "categories": ["cs.CV", "cs.AI"], "comment": "MS Graduate Research Thesis", "summary": "Our research is focused on two main applications of crowd scene analysis\ncrowd counting and anomaly detection In recent years a large number of\nresearches have been presented in the domain of crowd counting We addressed two\nmain challenges in this domain 1 Deep learning models are datahungry paradigms\nand always need a large amount of annotated data for the training of algorithm\nIt is timeconsuming and costly task to annotate such large amount of data\nSelfsupervised training is proposed to deal with this challenge 2 MCNN consists\nof multicolumns of CNN with different sizes of filters by presenting a novel\napproach based on a combination of selfsupervised training and MultiColumn CNN\nThis enables the model to learn features at different levels and makes it\neffective in dealing with challenges of occluded scenes nonuniform density\ncomplex backgrounds and scale invariation The proposed model was evaluated on\npublicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE\nand MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly\ndetection addressing challenges like lighting environmental conditions\nunexpected objects and scalability The model extracts spatial and temporal\nfeatures allowing it to be generalized to realworld scenes Spatial features are\nlearned using CNN while temporal features are learned using LSTM blocks The\nmodel works on binary classification and can detect normal or abnormal behavior\nThe models performance is improved by replacing fully connected layers with\ndense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset\nshow our models outperform other stateoftheart approaches", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7528\u4e8e\u4eba\u7fa4\u573a\u666f\u5206\u6790\u7684\u65b9\u6cd5\uff1a\u4e00\u79cd\u662f\u57fa\u4e8e\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\u7684\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\uff0c\u53e6\u4e00\u79cd\u662f\u57fa\u4e8eVGG19\u7684\u65f6\u7a7a\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u6807\u6ce8\u8fc7\u7a0b\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\u6765\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5e0c\u671b\u89e3\u51b3\u4eba\u7fa4\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5149\u7167\u3001\u73af\u5883\u6761\u4ef6\u548c\u53ef\u6269\u5c55\u6027\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\u6765\u89e3\u51b3\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u6570\u636e\u9700\u6c42\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eVGG19\u7684\u65f6\u7a7a\u6a21\u578b\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u5757\u6765\u63d0\u53d6\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728ShanghaiTech\u548cUCFQNRF\u6570\u636e\u96c6\u4e0a\u901a\u8fc7MAE\u548cMSE\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u4eba\u7fa4\u8ba1\u6570\u4efb\u52a1\u4e2d\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u57fa\u4e8eVGG19\u7684\u65f6\u7a7a\u6a21\u578b\u5728Hockey Fight\u548cSCVD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7528\u4e8e\u4eba\u7fa4\u573a\u666f\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u5373\u57fa\u4e8e\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\u7684\u8ba1\u6570\u6a21\u578b\u4ee5\u53ca\u57fa\u4e8eVGG19\u7684\u65f6\u7a7a\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.09338", "pdf": "https://arxiv.org/pdf/2505.09338", "abs": "https://arxiv.org/abs/2505.09338", "authors": ["Jingcheng Niu", "Xingdi Yuan", "Tong Wang", "Hamidreza Saghir", "Amir H. Abdi"], "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u73b0\u8c61\uff0c\u5373\u4e0a\u4e0b\u6587\u540c\u6b65\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u8bc6\u522b\u4e0e\u8be5\u73b0\u8c61\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\u3002\u901a\u8fc7\u5173\u95ed\u8fd9\u4e9b\u5934\uff0c\u53ef\u4ee5\u51cf\u8f7b\u8bed\u8a00\u6a21\u578b\u53d7\u5230\u7684\u5e72\u6270\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u53d7\u5230\u8f93\u5165\u63d0\u793a\u4e2d\u7684\u65e0\u5173\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u51cf\u8f7b\u8fd9\u79cd\u5e72\u6270\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u53d1\u73b0\u4e86\u4e0a\u4e0b\u6587\u540c\u6b65\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u63a9\u7801\u7684\u65b0\u65b9\u6cd5\u6765\u8bc6\u522b\u4e0e\u8be5\u73b0\u8c61\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\u3002", "result": "\u672c\u6587\u53d1\u73b0\uff0c\u8bed\u8a00\u6a21\u578b\u4f1a\u4e3a\u4e4b\u524d\u51fa\u73b0\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u4efb\u4f55\u6807\u8bb0\u5206\u914d\u66f4\u9ad8\u7684logits\uff08\u6216\u6982\u7387\uff09\uff0c\u5373\u4f7f\u8fd9\u4e9b\u6807\u8bb0\u662f\u968f\u673a\u7684\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u53d1\u73b0\u901a\u8fc7\u5173\u95ed\u4e0e\u4e0a\u4e0b\u6587\u540c\u6b65\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8fd9\u79cd\u5e72\u6270\u3002", "conclusion": "\u672c\u6587\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u73b0\u8c61\uff0c\u5373\u4e0a\u4e0b\u6587\u540c\u6b65\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8fd9\u79cd\u73b0\u8c61\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u4e0e\u4e0a\u4e0b\u6587\u540c\u6b65\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u5173\u95ed\u8fd9\u4e9b\u5934\u6765\u51cf\u8f7b\u6a21\u578b\u7684\u5e72\u6270\u95ee\u9898\u3002"}}
{"id": "2505.09114", "pdf": "https://arxiv.org/pdf/2505.09114", "abs": "https://arxiv.org/abs/2505.09114", "authors": ["Minh Hoang Nguyen", "Linh Le Pham Van", "Thommen George Karimpanal", "Sunil Gupta", "Hung Le"], "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Decision Transformers (DT) play a crucial role in modern reinforcement\nlearning, leveraging offline datasets to achieve impressive results across\nvarious domains. However, DT requires high-quality, comprehensive data to\nperform optimally. In real-world applications, the lack of training data and\nthe scarcity of optimal behaviours make training on offline datasets\nchallenging, as suboptimal data can hinder performance. To address this, we\npropose the Counterfactual Reasoning Decision Transformer (CRDT), a novel\nframework inspired by counterfactual reasoning. CRDT enhances DT ability to\nreason beyond known data by generating and utilizing counterfactual\nexperiences, enabling improved decision-making in unseen scenarios. Experiments\nacross Atari and D4RL benchmarks, including scenarios with limited data and\naltered dynamics, demonstrate that CRDT outperforms conventional DT approaches.\nAdditionally, reasoning counterfactually allows the DT agent to obtain\nstitching abilities, combining suboptimal trajectories, without architectural\nmodifications. These results highlight the potential of counterfactual\nreasoning to enhance reinforcement learning agents' performance and\ngeneralization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b0\u578b\u51b3\u7b56\u53d8\u538b\u5668\uff08CRDT\uff09\uff0c\u4ee5\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u6709\u9650\u6570\u636e\u548c\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "DT\u9700\u8981\u9ad8\u8d28\u91cf\u3001\u5168\u9762\u7684\u6570\u636e\u624d\u80fd\u53d1\u6325\u6700\u4f73\u6027\u80fd\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u548c\u6700\u4f18\u884c\u4e3a\u4f7f\u5f97\u5728\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u5177\u6709\u6311\u6218\u6027\u3002", "method": "CRDT\u662f\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u548c\u5229\u7528\u53cd\u4e8b\u5b9e\u7ecf\u9a8c\u6765\u589e\u5f3a\u51b3\u7b56\u53d8\u538b\u5668\uff08DT\uff09\u7684\u80fd\u529b\u3002", "result": "CRDT\u5728Atari\u548cD4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u548c\u52a8\u6001\u53d8\u5316\u7684\u573a\u666f\u4e2d\u3002\u6b64\u5916\uff0c\u53cd\u4e8b\u5b9e\u63a8\u7406\u4f7fDT\u4ee3\u7406\u80fd\u591f\u83b7\u5f97\u62fc\u63a5\u80fd\u529b\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u5373\u53ef\u7ec4\u5408\u6b21\u4f18\u8f68\u8ff9\u3002", "conclusion": "CRDT\u5c55\u793a\u4e86\u53cd\u4e8b\u5b9e\u63a8\u7406\u5728\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.08915", "pdf": "https://arxiv.org/pdf/2505.08915", "abs": "https://arxiv.org/abs/2505.08915", "authors": ["Jialin Mao", "Itay Griniasty", "Yan Sun", "Mark K. Transtrum", "James P. Sethna", "Pratik Chaudhari"], "title": "An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": null, "summary": "Recent experiments have shown that training trajectories of multiple deep\nneural networks with different architectures, optimization algorithms,\nhyper-parameter settings, and regularization methods evolve on a remarkably\nlow-dimensional \"hyper-ribbon-like\" manifold in the space of probability\ndistributions. Inspired by the similarities in the training trajectories of\ndeep networks and linear networks, we analytically characterize this phenomenon\nfor the latter. We show, using tools in dynamical systems theory, that the\ngeometry of this low-dimensional manifold is controlled by (i) the decay rate\nof the eigenvalues of the input correlation matrix of the training data, (ii)\nthe relative scale of the ground-truth output to the weights at the beginning\nof training, and (iii) the number of steps of gradient descent. By analytically\ncomputing and bounding the contributions of these quantities, we characterize\nphase boundaries of the region where hyper-ribbons are to be expected. We also\nextend our analysis to kernel machines and linear models that are trained with\nstochastic gradient descent.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.08854", "pdf": "https://arxiv.org/pdf/2505.08854", "abs": "https://arxiv.org/abs/2505.08854", "authors": ["Yuping Wang", "Shuo Xing", "Cui Can", "Renjie Li", "Hongyuan Hua", "Kexin Tian", "Zhaobin Mo", "Xiangbo Gao", "Keshu Wu", "Sulong Zhou", "Hengxu You", "Juntong Peng", "Junge Zhang", "Zehao Wang", "Rui Song", "Mingxuan Yan", "Walter Zimmer", "Xingcheng Zhou", "Peiran Li", "Zhaohan Lu", "Chia-Ju Chen", "Yue Huang", "Ryan A. Rossi", "Lichao Sun", "Hongkai Yu", "Zhiwen Fan", "Frank Hao Yang", "Yuhao Kang", "Ross Greer", "Chenxi Liu", "Eun Hak Lee", "Xuan Di", "Xinyue Ye", "Liu Ren", "Alois Knoll", "Xiaopeng Li", "Shuiwang Ji", "Masayoshi Tomizuka", "Marco Pavone", "Tianbao Yang", "Jing Du", "Ming-Hsuan Yang", "Hua Wei", "Ziran Wang", "Yang Zhou", "Jiachen Li", "Zhengzhong Tu"], "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative\ntechnological wave that reconfigures industries through its unparalleled\ncapabilities for content creation, reasoning, planning, and multimodal\nunderstanding. This revolutionary force offers the most promising path yet\ntoward solving one of engineering's grandest challenges: achieving reliable,\nfully autonomous driving, particularly the pursuit of Level 5 autonomy. This\nsurvey delivers a comprehensive and critical synthesis of the emerging role of\nGenAI across the autonomous driving stack. We begin by distilling the\nprinciples and trade-offs of modern generative modeling, encompassing VAEs,\nGANs, Diffusion Models, and Large Language Models (LLMs). We then map their\nfrontier applications in image, LiDAR, trajectory, occupancy, video generation\nas well as LLM-guided reasoning and decision making. We categorize practical\napplications, such as synthetic data workflows, end-to-end driving strategies,\nhigh-fidelity digital twin systems, smart transportation networks, and\ncross-domain transfer to embodied AI. We identify key obstacles and\npossibilities such as comprehensive generalization across rare cases,\nevaluation and safety checks, budget-limited implementation, regulatory\ncompliance, ethical concerns, and environmental effects, while proposing\nresearch plans across theoretical assurances, trust metrics, transport\nintegration, and socio-technical influence. By unifying these threads, the\nsurvey provides a forward-looking reference for researchers, engineers, and\npolicymakers navigating the convergence of generative AI and advanced\nautonomous mobility. An actively maintained repository of cited works is\navailable at https://github.com/taco-group/GenAI4AD.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u662f\u4e00\u79cd\u53d8\u9769\u6027\u7684\u6280\u672f\u6d6a\u6f6e\uff0c\u901a\u8fc7\u5176\u65e0\u4e0e\u4f26\u6bd4\u7684\u5185\u5bb9\u521b\u4f5c\u3001\u63a8\u7406\u3001\u89c4\u5212\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u91cd\u65b0\u914d\u7f6e\u4e86\u884c\u4e1a\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u800c\u6279\u5224\u6027\u5730\u7efc\u8ff0\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u65b0\u5174\u4f5c\u7528\u3002", "method": "\u672c\u6587\u9996\u5148\u603b\u7ed3\u4e86\u73b0\u4ee3\u751f\u6210\u5efa\u6a21\u7684\u539f\u7406\u548c\u6743\u8861\uff0c\u5305\u62ecVAEs\u3001GANs\u3001\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u7136\u540e\u6620\u5c04\u4e86\u5b83\u4eec\u5728\u56fe\u50cf\u3001LiDAR\u3001\u8f68\u8ff9\u3001\u5360\u7528\u3001\u89c6\u9891\u751f\u6210\u4ee5\u53caLLM\u5f15\u5bfc\u7684\u63a8\u7406\u548c\u51b3\u7b56\u4e2d\u7684\u524d\u6cbf\u5e94\u7528\u3002", "result": "\u672c\u6587\u5206\u7c7b\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u5982\u5408\u6210\u6570\u636e\u5de5\u4f5c\u6d41\u3001\u7aef\u5230\u7aef\u9a7e\u9a76\u7b56\u7565\u3001\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u3001\u667a\u80fd\u4ea4\u901a\u7f51\u7edc\u548c\u8de8\u9886\u57df\u8fc1\u79fb\u81f3\u5177\u8eab\u4eba\u5de5\u667a\u80fd\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u969c\u788d\u548c\u53ef\u80fd\u6027\uff0c\u5982\u5728\u7f55\u89c1\u60c5\u51b5\u4e0b\u7684\u5168\u9762\u6cdb\u5316\u3001\u8bc4\u4f30\u548c\u5b89\u5168\u68c0\u67e5\u3001\u9884\u7b97\u9650\u5236\u7684\u5b9e\u65bd\u3001\u76d1\u7ba1\u5408\u89c4\u3001\u4f26\u7406\u95ee\u9898\u548c\u73af\u5883\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u65b0\u5174\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u53c2\u8003\u3002"}}
{"id": "2505.09388", "pdf": "https://arxiv.org/pdf/2505.09388", "abs": "https://arxiv.org/abs/2505.09388", "authors": ["An Yang", "Anfeng Li", "Baosong Yang", "Beichen Zhang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Gao", "Chengen Huang", "Chenxu Lv", "Chujie Zheng", "Dayiheng Liu", "Fan Zhou", "Fei Huang", "Feng Hu", "Hao Ge", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Yang", "Jiaxi Yang", "Jing Zhou", "Jingren Zhou", "Junyang Lin", "Kai Dang", "Keqin Bao", "Kexin Yang", "Le Yu", "Lianghao Deng", "Mei Li", "Mingfeng Xue", "Mingze Li", "Pei Zhang", "Peng Wang", "Qin Zhu", "Rui Men", "Ruize Gao", "Shixuan Liu", "Shuang Luo", "Tianhao Li", "Tianyi Tang", "Wenbiao Yin", "Xingzhang Ren", "Xinyu Wang", "Xinyu Zhang", "Xuancheng Ren", "Yang Fan", "Yang Su", "Yichang Zhang", "Yinger Zhang", "Yu Wan", "Yuqiong Liu", "Zekun Wang", "Zeyu Cui", "Zhenru Zhang", "Zhipeng Zhou", "Zihan Qiu"], "title": "Qwen3 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.", "AI": {"tldr": "Qwen3 is the latest version of the Qwen model family, featuring advanced performance, efficiency, and multilingual capabilities. It integrates thinking and non-thinking modes into a unified framework, introduces a thinking budget mechanism, and offers competitive results across various benchmarks.", "motivation": "To advance performance, efficiency, and multilingual capabilities of large language models (LLMs) while eliminating the need to switch between different models for complex reasoning and rapid responses.", "method": "Qwen3 integrates thinking mode and non-thinking mode into a unified framework, introduces a thinking budget mechanism, and leverages knowledge from flagship models to reduce computational resources for smaller-scale models.", "result": "Qwen3 demonstrates competitive performance against larger MoE models and proprietary models in tasks such as code generation, mathematical reasoning, and agent tasks, with expanded multilingual support.", "conclusion": "Qwen3 achieves state-of-the-art results across diverse benchmarks and expands multilingual support, enhancing global accessibility."}}
{"id": "2505.09289", "pdf": "https://arxiv.org/pdf/2505.09289", "abs": "https://arxiv.org/abs/2505.09289", "authors": ["Pedro M. P. Curvo", "Mara Dragomir", "Salvador Torpes", "Mohammadmahdi Rahimi"], "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"", "categories": ["cs.AI"], "comment": "11 Tables, 9 Figures", "summary": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u9a8c\u8bc1\u5e76\u6269\u5c55\u4e86GovSim\u6846\u67b6\uff0c\u8bc1\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u5171\u4eab\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u7684\u5408\u4f5c\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u8bbe\u7f6e\u4e2d\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u548c\u6269\u5c55Piatti\u7b49\u4eba\u63d0\u51fa\u7684GovSim\u6846\u67b6\uff0c\u4ee5\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u5171\u4eab\u573a\u666f\u4e2d\u7684\u5408\u4f5c\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u65b0\u8bbe\u7f6e\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u590d\u5236Piatti\u7b49\u4eba\u7684\u5173\u952e\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u5927\u578b\u6a21\u578b\uff08\u5982GPT-4-turbo\uff09\u4e0e\u5c0f\u578b\u6a21\u578b\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u5e76\u63a2\u8ba8\u4e86\u666e\u904d\u5316\u539f\u5219\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6269\u5c55\u4e86\u6846\u67b6\u4ee5\u9002\u5e94\u65b0\u7684\u8bbe\u7f6e\uff0c\u5305\u62ec\u8bc4\u4f30\u5176\u4ed6\u6a21\u578b\u3001\u521b\u5efa\u5f02\u6784\u591a\u667a\u80fd\u4f53\u73af\u5883\u3001\u7814\u7a76\u4f7f\u7528\u65e5\u8bed\u6307\u4ee4\u7684\u573a\u666f\u4ee5\u53ca\u63a2\u7d22\u201c\u9006\u5411\u73af\u5883\u201d\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u6a21\u578b\u5373\u4f7f\u5728\u6ca1\u6709\u666e\u904d\u5316\u539f\u5219\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u53ef\u6301\u7eed\u5408\u4f5c\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u5219\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u9ad8\u6027\u80fd\u6a21\u578b\u53ef\u4ee5\u5f71\u54cd\u4f4e\u6027\u80fd\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5728\u65b0\u6a21\u578b\u3001\u573a\u666f\u548c\u8bed\u8a00\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u6e90\u5171\u4eab\u573a\u666f\u4e2d\u7684\u5408\u4f5c\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u3001\u89c4\u6a21\u548c\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u9002\u5e94\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\u9ad8\u6027\u80fd\u6a21\u578b\u53ef\u4ee5\u5f71\u54cd\u4f4e\u6027\u80fd\u6a21\u578b\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u5408\u4f5cAI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.08940", "pdf": "https://arxiv.org/pdf/2505.08940", "abs": "https://arxiv.org/abs/2505.08940", "authors": ["Jeremie Blanchard", "Lisa Casino", "Jordan Gierschendorf"], "title": "NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach", "categories": ["cs.LG", "astro-ph.IM"], "comment": "12 pages", "summary": "The characterization of exoplanetary atmospheres through spectral analysis is\na complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration\nwith the European Space Agency's (ESA) Ariel mission, provided an opportunity\nto explore machine learning techniques for extracting atmospheric compositions\nfrom simulated spectral data. In this work, we focus on a data-centric business\napproach, prioritizing generalization over competition-specific optimization.\nWe briefly outline multiple experimental axes, including feature extraction,\nsignal transformation, and heteroskedastic uncertainty modeling. Our\nexperiments demonstrate that uncertainty estimation plays a crucial role in the\nGaussian Log-Likelihood (GLL) score, impacting performance by several\npercentage points. Despite improving the GLL score by 11%, our results\nhighlight the inherent limitations of tabular modeling and feature engineering\nfor this task, as well as the constraints of a business-driven approach within\na Kaggle-style competition framework. Our findings emphasize the trade-offs\nbetween model simplicity, interpretability, and generalization in astrophysical\ndata analysis.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728NeurIPS 2024 Ariel\u6570\u636e\u6311\u6218\u4e2d\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u4ece\u6a21\u62df\u5149\u8c31\u6570\u636e\u4e2d\u63d0\u53d6\u5927\u6c14\u6210\u5206\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6a21\u578b\u7b80\u5355\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u6307\u51fa\u5728\u5929\u4f53\u7269\u7406\u6570\u636e\u5206\u6790\u4e2d\u5b58\u5728\u4e00\u4e9b\u9650\u5236\u3002", "motivation": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\u4ece\u6a21\u62df\u5149\u8c31\u6570\u636e\u4e2d\u63d0\u53d6\u5927\u6c14\u6210\u5206\uff0c\u662f\u89e3\u51b3\u7cfb\u5916\u884c\u661f\u5927\u6c14\u8868\u5f81\u8fd9\u4e00\u590d\u6742\u6311\u6218\u7684\u673a\u4f1a\u3002", "method": "\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5546\u4e1a\u65b9\u6cd5\uff0c\u4f18\u5148\u8003\u8651\u6cdb\u5316\u800c\u975e\u7ade\u4e89\u7279\u5b9a\u4f18\u5316\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u7279\u5f81\u63d0\u53d6\u3001\u4fe1\u53f7\u53d8\u6362\u548c\u5f02\u65b9\u5dee\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7b49\u591a\u4e2a\u5b9e\u9a8c\u8f74\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5728\u9ad8\u65af\u5bf9\u6570\u4f3c\u7136\uff08GLL\uff09\u5206\u6570\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5f71\u54cd\u6027\u80fd\u51e0\u4e2a\u767e\u5206\u70b9\u3002\u5c3d\u7ba1GLL\u5206\u6570\u63d0\u9ad8\u4e8611%\uff0c\u4f46\u7ed3\u679c\u7a81\u663e\u4e86\u8868\u683c\u5efa\u6a21\u548c\u7279\u5f81\u5de5\u7a0b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5728Kaggle\u5f0f\u7ade\u8d5b\u6846\u67b6\u5185\u5546\u4e1a\u9a71\u52a8\u65b9\u6cd5\u7684\u9650\u5236\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5929\u4f53\u7269\u7406\u6570\u636e\u5206\u6790\u4e2d\u6a21\u578b\u7b80\u5355\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2505.08882", "pdf": "https://arxiv.org/pdf/2505.08882", "abs": "https://arxiv.org/abs/2505.08882", "authors": ["Ali Almakhluk", "Uthman Baroudi", "Yasser El-Alfy"], "title": "Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety", "categories": ["cs.CV", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "This study aims to improve transportation safety, especially traffic safety.\nRoad damage anomalies such as potholes and cracks have emerged as a significant\nand recurring cause for accidents. To tackle this problem and improve road\nsafety, a comprehensive system has been developed to detect potholes, cracks\n(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit\nthis data to the cloud for appropriate action by authorities. The system also\nbroadcasts warning signals to nearby vehicles warning them if a severe anomaly\nis detected on the road. Moreover, the system can count road anomalies in\nreal-time. It is emulated through the utilization of Raspberry Pi, a camera\nmodule, deep learning model, laptop, and cloud service. Deploying this\ninnovative solution aims to proactively enhance road safety by notifying\nrelevant authorities and drivers about the presence of potholes and cracks to\ntake actions, thereby mitigating potential accidents arising from this\nprevalent road hazard leading to safer road conditions for the whole community.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7efc\u5408\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u7c7b\u9053\u8def\u635f\u574f\u5f02\u5e38\uff08\u5982\u5751\u6d3c\u548c\u88c2\u7f1d\uff09\uff0c\u5e76\u5c06\u6570\u636e\u4f20\u8f93\u5230\u4e91\u7aef\u4ee5\u91c7\u53d6\u9002\u5f53\u884c\u52a8\u3002\u8be5\u7cfb\u7edf\u8fd8\u80fd\u5b9e\u65f6\u7edf\u8ba1\u9053\u8def\u5f02\u5e38\u60c5\u51b5\uff0c\u5e76\u5411\u9644\u8fd1\u8f66\u8f86\u5e7f\u64ad\u8b66\u544a\u4fe1\u53f7\uff0c\u4ee5\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u6027\u3002", "motivation": "\u9053\u8def\u635f\u574f\u5f02\u5e38\uff08\u5982\u5751\u6d3c\u548c\u88c2\u7f1d\uff09\u5df2\u6210\u4e3a\u4e8b\u6545\u7684\u91cd\u8981\u4e14\u53cd\u590d\u51fa\u73b0\u7684\u539f\u56e0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5e76\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7efc\u5408\u7cfb\u7edf\u6765\u68c0\u6d4b\u5751\u6d3c\u548c\u88c2\u7f1d\uff0c\u5e76\u5411\u9644\u8fd1\u8f66\u8f86\u5e7f\u64ad\u8b66\u544a\u4fe1\u53f7\u3002", "method": "\u8be5\u7cfb\u7edf\u5229\u7528\u6811\u8393\u6d3e\u3001\u6444\u50cf\u5934\u6a21\u5757\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3001\u7b14\u8bb0\u672c\u7535\u8111\u548c\u4e91\u670d\u52a1\u8fdb\u884c\u6a21\u62df\uff0c\u4ee5\u68c0\u6d4b\u5751\u6d3c\u548c\u88c2\u7f1d\uff0c\u5206\u7c7b\u5176\u5927\u5c0f\uff0c\u5e76\u5c06\u6570\u636e\u4f20\u8f93\u5230\u4e91\u7aef\u4ee5\u91c7\u53d6\u9002\u5f53\u884c\u52a8\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u8fd8\u80fd\u5b9e\u65f6\u7edf\u8ba1\u9053\u8def\u5f02\u5e38\u60c5\u51b5\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u5751\u6d3c\u548c\u88c2\u7f1d\uff0c\u5206\u7c7b\u5176\u5927\u5c0f\uff0c\u5e76\u5c06\u6570\u636e\u4f20\u8f93\u5230\u4e91\u7aef\u4ee5\u4f9b\u76f8\u5173\u90e8\u95e8\u91c7\u53d6\u884c\u52a8\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u8fd8\u80fd\u5b9e\u65f6\u7edf\u8ba1\u9053\u8def\u5f02\u5e38\u60c5\u51b5\uff0c\u5e76\u5411\u9644\u8fd1\u8f66\u8f86\u5e7f\u64ad\u8b66\u544a\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u901a\u77e5\u76f8\u5173\u90e8\u95e8\u548c\u9a7e\u9a76\u5458\u6709\u5173\u5751\u6d3c\u548c\u88c2\u7f1d\u7684\u5b58\u5728\uff0c\u4ece\u800c\u51cf\u8f7b\u7531\u6b64\u4ea7\u751f\u7684\u6f5c\u5728\u4e8b\u6545\uff0c\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u6027\u3002"}}
{"id": "2505.09407", "pdf": "https://arxiv.org/pdf/2505.09407", "abs": "https://arxiv.org/abs/2505.09407", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits", "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": "12 pages, 12 figures", "summary": "Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations.", "AI": {"tldr": "QEDACVC \u662f\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5728 OPUS \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 82% \u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u7684\u4e91\u670d\u52a1\u5982 Google Translate \u548c Microsoft Translator \u4f7f\u7528\u5927\u578b\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7ecf\u5178\u8ba1\u7b97\u3002QEDACVC \u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\uff0c\u4ee5\u7814\u7a76\u548c\u5c55\u793a\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u3002", "method": "QEDACVC \u5f15\u5165\u4e86\u91cf\u5b50\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u91cf\u5b50\u5377\u79ef\u3001\u91cf\u5b50\u6c60\u5316\u3001\u91cf\u5b50\u53d8\u5206\u7535\u8def\u548c\u91cf\u5b50\u6ce8\u610f\u529b\u4f5c\u4e3a\u8f6f\u4ef6\u4fee\u6539\uff0c\u5728\u91cf\u5b50\u8ba1\u7b97\u786c\u4ef6\u4e0a\u8fdb\u884c\u6a21\u62df\u548c\u8fd0\u884c\u3002", "result": "QEDACVC \u5728 OPUS \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 82% \u7684\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "QEDACVC \u662f\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u5728 OPUS \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 82% \u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2505.09341", "pdf": "https://arxiv.org/pdf/2505.09341", "abs": "https://arxiv.org/abs/2505.09341", "authors": ["Ev\u017een Wybitul"], "title": "Access Controls Will Solve the Dual-Use Dilemma", "categories": ["cs.AI"], "comment": null, "summary": "AI safety systems face a dual-use dilemma. Since the same request can be\neither harmless or harmful depending on who made it and why, if the system\nmakes decisions based solely on the request's content, it will refuse some\nlegitimate queries and let pass harmful ones. To address this, we propose a\nconceptual access control framework, based on verified user credentials (such\nas institutional affiliation) and classifiers that assign model outputs to risk\ncategories (such as advanced virology). The system permits responses only when\nthe user's verified credentials match the category's requirements. For\nimplementation of the model output classifiers, we introduce a theoretical\napproach utilizing small, gated expert modules integrated into the generator\nmodel, trained with gradient routing, that enable efficient risk detection\nwithout the capability gap problems of external monitors. While open questions\nremain about the verification mechanisms, risk categories, and the technical\nimplementation, our framework makes the first step toward enabling granular\ngovernance of AI capabilities: verified users gain access to specialized\nknowledge without arbitrary restrictions, while adversaries are blocked from\nit. This contextual approach reconciles model utility with robust safety,\naddressing the dual-use dilemma.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u51ed\u8bc1\u548c\u98ce\u9669\u5206\u7c7b\u7684AI\u5b89\u5168\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u53cc\u91cd\u7528\u9014\u56f0\u5883\uff0c\u4f7f\u5408\u6cd5\u7528\u6237\u80fd\u591f\u8bbf\u95ee\u4e13\u4e1a\u77e5\u8bc6\uff0c\u540c\u65f6\u963b\u6b62\u6076\u610f\u884c\u4e3a\u3002", "motivation": "AI\u5b89\u5168\u7cfb\u7edf\u9762\u4e34\u53cc\u91cd\u7528\u9014\u56f0\u5883\u3002\u7531\u4e8e\u540c\u4e00\u8bf7\u6c42\u53ef\u80fd\u5bf9\u4e0d\u540c\u7684\u4eba\u6216\u539f\u56e0\u800c\u8a00\u662f\u65e0\u5bb3\u6216\u6709\u5bb3\u7684\uff0c\u5982\u679c\u7cfb\u7edf\u4ec5\u6839\u636e\u8bf7\u6c42\u5185\u5bb9\u505a\u51fa\u51b3\u7b56\uff0c\u5b83\u5c06\u62d2\u7edd\u4e00\u4e9b\u5408\u6cd5\u67e5\u8be2\u5e76\u8ba9\u6709\u5bb3\u67e5\u8be2\u901a\u8fc7\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u7528\u6237\u8eab\u4efd\u548c\u610f\u56fe\u8fdb\u884c\u533a\u5206\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a8c\u8bc1\u7528\u6237\u51ed\u8bc1\uff08\u5982\u673a\u6784\u96b6\u5c5e\u5173\u7cfb\uff09\u548c\u5206\u7c7b\u5668\u7684\u8bbf\u95ee\u63a7\u5236\u6846\u67b6\uff0c\u8fd9\u4e9b\u5206\u7c7b\u5668\u5c06\u6a21\u578b\u8f93\u51fa\u5206\u914d\u5230\u98ce\u9669\u7c7b\u522b\uff08\u5982\u9ad8\u7ea7\u75c5\u6bd2\u5b66\uff09\u3002\u7cfb\u7edf\u4ec5\u5728\u7528\u6237\u7684\u9a8c\u8bc1\u51ed\u8bc1\u4e0e\u7c7b\u522b\u8981\u6c42\u5339\u914d\u65f6\u624d\u5141\u8bb8\u54cd\u5e94\u3002\u4e3a\u4e86\u5b9e\u73b0\u6a21\u578b\u8f93\u51fa\u5206\u7c7b\u5668\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7406\u8bba\u65b9\u6cd5\uff0c\u5229\u7528\u96c6\u6210\u5230\u751f\u6210\u5668\u6a21\u578b\u4e2d\u7684\u5c0f\u578b\u95e8\u63a7\u4e13\u5bb6\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u8def\u7531\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u9ad8\u6548\u68c0\u6d4b\u98ce\u9669\uff0c\u800c\u4e0d\u4f1a\u51fa\u73b0\u5916\u90e8\u76d1\u63a7\u5668\u7684\u80fd\u529b\u5dee\u8ddd\u95ee\u9898\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7528\u6237\u51ed\u8bc1\u548c\u98ce\u9669\u5206\u7c7b\u7684\u8bbf\u95ee\u63a7\u5236\uff0c\u4f7f\u5f97\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7528\u6237\u53ef\u4ee5\u8bbf\u95ee\u7279\u5b9a\u77e5\u8bc6\uff0c\u800c\u5bf9\u624b\u5219\u88ab\u963b\u6b62\u8bbf\u95ee\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u6a21\u578b\u6548\u7528\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3aAI\u80fd\u529b\u7684\u7ec6\u7c92\u5ea6\u6cbb\u7406\u63d0\u4f9b\u4e86\u7b2c\u4e00\u6b65\uff0c\u4f7f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7528\u6237\u80fd\u591f\u5728\u6ca1\u6709\u4efb\u610f\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e13\u4e1a\u77e5\u8bc6\uff0c\u540c\u65f6\u963b\u6b62\u5bf9\u624b\u83b7\u5f97\u8fd9\u4e9b\u77e5\u8bc6\u3002\u8fd9\u79cd\u4e0a\u4e0b\u6587\u65b9\u6cd5\u8c03\u548c\u4e86\u6a21\u578b\u6548\u7528\u4e0e\u5b89\u5168\u6027\u7684\u77db\u76fe\uff0c\u89e3\u51b3\u4e86\u53cc\u91cd\u7528\u9014\u56f0\u5883\u3002"}}
{"id": "2505.08941", "pdf": "https://arxiv.org/pdf/2505.08941", "abs": "https://arxiv.org/abs/2505.08941", "authors": ["Gavin Hull", "Alex Bihlo"], "title": "ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers", "categories": ["cs.LG", "cs.CL"], "comment": "16 pages, 13 figures", "summary": "Predicting the future citation rates of academic papers is an important step\ntoward the automation of research evaluation and the acceleration of scientific\nprogress. We present $\\textbf{ForeCite}$, a simple but powerful framework to\nappend pre-trained causal language models with a linear head for average\nmonthly citation rate prediction. Adapting transformers for regression tasks,\nForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of\n900K+ biomedical papers published between 2000 and 2024, a 27-point improvement\nover the previous state-of-the-art. Comprehensive scaling-law analysis reveals\nconsistent gains across model sizes and data volumes, while temporal holdout\nexperiments confirm practical robustness. Gradient-based saliency heatmaps\nsuggest a potentially undue reliance on titles and abstract texts. These\nresults establish a new state-of-the-art in forecasting the long-term influence\nof academic research and lay the groundwork for the automated, high-fidelity\nevaluation of scientific contributions.", "AI": {"tldr": "ForeCite\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5b66\u672f\u8bba\u6587\u7684\u672a\u6765\u5f15\u7528\u7387\uff0c\u5b83\u5728900K+\u751f\u7269\u533b\u5b66\u8bba\u6587\u7684\u7cbe\u5fc3\u6311\u9009\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6d4b\u8bd5\u76f8\u5173\u6027\u03c1=0.826\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u63d0\u9ad8\u4e8627\u5206\u3002", "motivation": "\u9884\u6d4b\u5b66\u672f\u8bba\u6587\u7684\u672a\u6765\u5f15\u7528\u7387\u662f\u5b9e\u73b0\u7814\u7a76\u8bc4\u4f30\u81ea\u52a8\u5316\u548c\u52a0\u901f\u79d1\u5b66\u8fdb\u6b65\u7684\u91cd\u8981\u6b65\u9aa4\u3002", "method": "ForeCite\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f46\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u9644\u52a0\u4e86\u4e00\u4e2a\u7ebf\u6027\u5934\u90e8\uff0c\u7528\u4e8e\u5e73\u5747\u6708\u5f15\u7528\u7387\u9884\u6d4b\u3002\u9002\u5e94\u53d8\u538b\u5668\u8fdb\u884c\u56de\u5f52\u4efb\u52a1\uff0cForeCite\u5728900K+\u751f\u7269\u533b\u5b66\u8bba\u6587\u7684\u7cbe\u5fc3\u6311\u9009\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6d4b\u8bd5\u76f8\u5173\u6027\u03c1=0.826\u3002", "result": "ForeCite\u5728900K+\u751f\u7269\u533b\u5b66\u8bba\u6587\u7684\u7cbe\u5fc3\u6311\u9009\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6d4b\u8bd5\u76f8\u5173\u6027\u03c1=0.826\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u63d0\u9ad8\u4e8627\u5206\u3002\u7efc\u5408\u5c3a\u5ea6\u5b9a\u5f8b\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u91cf\u7684\u4e00\u81f4\u6536\u76ca\uff0c\u800c\u65f6\u95f4\u4fdd\u7559\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5b9e\u9645\u7684\u7a33\u5065\u6027\u3002\u68af\u5ea6\u57fa\u4e8e\u663e\u8457\u6027\u70ed\u56fe\u8868\u660e\u5bf9\u6807\u9898\u548c\u6458\u8981\u6587\u672c\u7684\u6f5c\u5728\u8fc7\u5ea6\u4f9d\u8d56\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u786e\u7acb\u4e86\u9884\u6d4b\u5b66\u672f\u7814\u7a76\u957f\u671f\u5f71\u54cd\u7684\u65b0\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e76\u4e3a\u79d1\u5b66\u8d21\u732e\u7684\u81ea\u52a8\u5316\u3001\u9ad8\u4fdd\u771f\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.08886", "pdf": "https://arxiv.org/pdf/2505.08886", "abs": "https://arxiv.org/abs/2505.08886", "authors": ["Hamideh Khaleghpour", "Brett McKinney"], "title": "Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images", "categories": ["cs.CV", "cs.LG"], "comment": "7 pages, 10 figures. Accepted at the 2nd Asia Pacific Computer\n  Systems Conference (APCS 2024), March 15-17, 2024", "summary": "The rising incidence of skin cancer, coupled with limited public awareness\nand a shortfall in clinical expertise, underscores an urgent need for advanced\ndiagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool\nin this domain, particularly for distinguishing malignant from benign skin\nlesions. Leveraging publicly available datasets of skin lesions, researchers\nhave been developing AI-based diagnostic solutions. However, the integration of\nsuch computer systems in clinical settings is still nascent. This study aims to\nbridge this gap by employing a fusion of image processing techniques and\nmachine learning algorithms, specifically neuro-fuzzy and colonial competition\napproaches. Applied to dermoscopic images from the ISIC database, our method\nachieved a notable accuracy of 94% on a dataset of 560 images. These results\nunderscore the potential of our approach in aiding clinicians in the early\ndetection of melanoma, thereby contributing significantly to skin cancer\ndiagnostics.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u5904\u7406\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u76ae\u80a4\u764c\u8bca\u65ad\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8694%\u7684\u51c6\u786e\u7387\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u65e9\u671f\u68c0\u6d4b\u5de5\u5177\u3002", "motivation": "\u76ae\u80a4\u764c\u53d1\u75c5\u7387\u4e0a\u5347\uff0c\u516c\u4f17\u610f\u8bc6\u6709\u9650\uff0c\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u4e0d\u8db3\uff0c\u8feb\u5207\u9700\u8981\u5148\u8fdb\u7684\u8bca\u65ad\u8f85\u52a9\u5de5\u5177\u3002\u4eba\u5de5\u667a\u80fd\u5728\u533a\u5206\u6076\u6027\u4e0e\u826f\u6027\u76ae\u80a4\u75c5\u53d8\u65b9\u9762\u663e\u793a\u51fa\u524d\u666f\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u5904\u7406\u6280\u672f\u4e0e\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u7279\u522b\u662f\u795e\u7ecf\u6a21\u7cca\u548c\u6b96\u6c11\u7ade\u4e89\u65b9\u6cd5\uff09\u7684\u878d\u5408\u3002", "result": "\u5728ISIC\u6570\u636e\u5e93\u7684560\u5f20\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e8694%\u7684\u663e\u8457\u51c6\u786e\u7387\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u76ae\u80a4\u764c\u8bca\u65ad\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u533b\u751f\u65e9\u671f\u68c0\u6d4b\u9ed1\u8272\u7d20\u7624\u3002"}}
{"id": "2505.09519", "pdf": "https://arxiv.org/pdf/2505.09519", "abs": "https://arxiv.org/abs/2505.09519", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting\nlarge language models, yet existing approaches exhibit counter-intuitive\nphenomena: integrating router into prompt tuning (PT) increases training\nefficiency yet does not improve performance universally; parameter reduction\nthrough matrix decomposition can improve performance in specific domains.\nMotivated by these observations and the modular nature of PT, we propose\nPT-MoE, a novel framework that integrates matrix decomposition with\nmixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets\ndemonstrate that PT-MoE achieves state-of-the-art performance in both question\nanswering (QA) and mathematical problem solving tasks, improving F1 score by\n1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing\nmathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all\nwhile using 25% fewer parameters than LoRA. Our analysis reveals that while PT\nmethods generally excel in QA tasks and LoRA-based methods in math datasets,\nthe integration of matrix decomposition and MoE in PT-MoE yields complementary\nbenefits: decomposition enables efficient parameter sharing across experts\nwhile MoE provides dynamic adaptation, collectively enabling PT-MoE to\ndemonstrate cross-task consistency and generalization abilities. These\nfindings, along with ablation studies on routing mechanisms and architectural\ncomponents, provide insights for future PEFT methods.", "AI": {"tldr": "PT-MoE is a novel framework that integrates matrix decomposition with MoE routing for efficient PT, achieving state-of-the-art performance in QA and math tasks with fewer parameters.", "motivation": "Existing PEFT methods exhibit counter-intuitive phenomena, such as increased training efficiency without universal performance improvement and parameter reduction through matrix decomposition that improves performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE.", "method": "PT-MoE integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient prompt tuning.", "result": "PT-MoE achieves state-of-the-art performance in both QA and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA.", "conclusion": "PT-MoE achieves state-of-the-art performance in both question answering and mathematical problem solving tasks, while using fewer parameters than LoRA. The integration of matrix decomposition and MoE yields complementary benefits, enabling cross-task consistency and generalization abilities."}}
{"id": "2505.09396", "pdf": "https://arxiv.org/pdf/2505.09396", "abs": "https://arxiv.org/abs/2505.09396", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4ee3\u7406\u590d\u6742\u6027\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u4e86\u5e95\u5c42LLM\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fc5\u901f\u5d1b\u8d77\u5df2\u7ecf\u5c06\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7814\u7a76\u8f6c\u5411\u4e86\u4ee3\u7406\u7cfb\u7edf\uff0c\u4fc3\u4f7f\u4f7f\u7528\u66f4\u5f31\u548c\u66f4\u7075\u6d3b\u7684\u4ee3\u7406\u6982\u5ff5\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u8f6c\u53d8\u5f15\u53d1\u4e86\u5173\u4e8eLLM\u4ee3\u7406\u5728\u535a\u5f08\u8bba\u73af\u5883\u4e2d\u590d\u5236\u4eba\u7c7b\u6218\u7565\u63a8\u7406\u7a0b\u5ea6\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u6211\u4eec\u8bc4\u4f30\u4e86\u4e09\u79cd\u4ee3\u7406\u8bbe\u8ba1\uff1a\u4e00\u4e2a\u7b80\u5355\u7684\u535a\u5f08\u8bba\u6a21\u578b\u3001\u4e00\u4e2a\u65e0\u7ed3\u6784\u7684LLM\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u96c6\u6210\u5230\u4f20\u7edf\u4ee3\u7406\u6846\u67b6\u4e2d\u7684LLM\u3002\u4f7f\u7528\u731c\u6d4b\u6e38\u620f\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e9b\u4ee3\u7406\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u5728\u4e00\u822c\u63a8\u7406\u6a21\u5f0f\u548c\u57fa\u4e8e\u4e2a\u4f53\u89d2\u8272\u7684\u76ee\u6807\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6a21\u7cca\u7684\u535a\u5f08\u573a\u666f\u6765\u8bc4\u4f30\u4ee3\u7406\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u6db5\u76d6\u4e8625\u79cd\u4ee3\u7406\u914d\u7f6e\u4e0b\u76842000\u591a\u4e2a\u63a8\u7406\u6837\u672c\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u4eba\u7c7b\u542f\u53d1\u7684\u8ba4\u77e5\u7ed3\u6784\u53ef\u4ee5\u589e\u5f3aLLM\u4ee3\u7406\u4e0e\u4eba\u7c7b\u6218\u7565\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u4eba\u7c7b\u542f\u53d1\u7684\u8ba4\u77e5\u7ed3\u6784\u53ef\u4ee5\u589e\u5f3aLLM\u4ee3\u7406\u4e0e\u4eba\u7c7b\u6218\u7565\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u4ee3\u7406\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u975e\u7ebf\u6027\u7684\uff0c\u8fd9\u8868\u660e\u5bf9\u5e95\u5c42LLM\u80fd\u529b\u7684\u4f9d\u8d56\uff0c\u5e76\u6697\u793a\u4e86\u7b80\u5355\u67b6\u6784\u589e\u5f3a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.08964", "pdf": "https://arxiv.org/pdf/2505.08964", "abs": "https://arxiv.org/abs/2505.08964", "authors": ["Majed Jaber", "Julien Michel", "Nicolas Boutry", "Pierre Parrend"], "title": "GPML: Graph Processing for Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The dramatic increase of complex, multi-step, and rapidly evolving attacks in\ndynamic networks involves advanced cyber-threat detectors. The GPML (Graph\nProcessing for Machine Learning) library addresses this need by transforming\nraw network traffic traces into graph representations, enabling advanced\ninsights into network behaviors. The library provides tools to detect anomalies\nin interaction and community shifts in dynamic networks. GPML supports\ncommunity and spectral metrics extraction, enhancing both real-time detection\nand historical forensics analysis. This library supports modern cybersecurity\nchallenges with a robust, graph-based approach.", "AI": {"tldr": "GPML \u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u56fe\u5904\u7406\u5e93\uff0c\u80fd\u591f\u5c06\u7f51\u7edc\u6d41\u91cf\u8f6c\u6362\u4e3a\u56fe\u8868\u793a\uff0c\u4ece\u800c\u5e2e\u52a9\u68c0\u6d4b\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\u548c\u793e\u533a\u53d8\u5316\u3002", "motivation": "\u7531\u4e8e\u52a8\u6001\u7f51\u7edc\u4e2d\u590d\u6742\u3001\u591a\u6b65\u9aa4\u548c\u5feb\u901f\u6f14\u53d8\u7684\u653b\u51fb\u663e\u8457\u589e\u52a0\uff0c\u9700\u8981\u5148\u8fdb\u7684\u7f51\u7edc\u5a01\u80c1\u68c0\u6d4b\u5668\u3002", "method": "GPML \u5e93\u901a\u8fc7\u5c06\u539f\u59cb\u7f51\u7edc\u6d41\u91cf\u75d5\u8ff9\u8f6c\u6362\u4e3a\u56fe\u8868\u793a\uff0c\u4ece\u800c\u63d0\u4f9b\u5de5\u5177\u6765\u68c0\u6d4b\u4ea4\u4e92\u5f02\u5e38\u548c\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u793e\u533a\u53d8\u5316\u3002", "result": "GPML \u652f\u6301\u793e\u533a\u548c\u9891\u8c31\u5ea6\u91cf\u63d0\u53d6\uff0c\u589e\u5f3a\u4e86\u5b9e\u65f6\u68c0\u6d4b\u548c\u5386\u53f2\u53d6\u8bc1\u5206\u6790\u3002", "conclusion": "GPML \u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u7f51\u7edc\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2505.08909", "pdf": "https://arxiv.org/pdf/2505.08909", "abs": "https://arxiv.org/abs/2505.08909", "authors": ["Deliang Wei", "Peng Chen", "Haobo Xu", "Jiale Yao", "Fang Li", "Tieyong Zeng"], "title": "Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems", "categories": ["cs.CV", "cs.LG", "math.FA", "math.OC", "94A08, 47H10, 47J26, 46N10, 47N10"], "comment": "31 pages", "summary": "Plug-and-play (PnP) methods with deep denoisers have shown impressive results\nin imaging problems. They typically require strong convexity or smoothness of\nthe fidelity term and a (residual) non-expansive denoiser for convergence.\nThese assumptions, however, are violated in Poisson inverse problems, and\nnon-expansiveness can hinder denoising performance. To address these\nchallenges, we propose a cocoercive conservative (CoCo) denoiser, which may be\n(residual) expansive, leading to improved denoising. By leveraging the\ngeneralized Helmholtz decomposition, we introduce a novel training strategy\nthat combines Hamiltonian regularization to promote conservativeness and\nspectral regularization to ensure cocoerciveness. We prove that CoCo denoiser\nis a proximal operator of a weakly convex function, enabling a restoration\nmodel with an implicit weakly convex prior. The global convergence of PnP\nmethods to a stationary point of this restoration model is established.\nExtensive experimental results demonstrate that our approach outperforms\nclosely related methods in both visual quality and quantitative metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CoCo\u53bb\u566a\u5668\uff0c\u7528\u4e8e\u89e3\u51b3Poisson\u9006\u95ee\u9898\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "Poisson\u9006\u95ee\u9898\u4e2d\u7684\u4f20\u7edf\u5047\u8bbe\u88ab\u8fdd\u53cd\uff0c\u975e\u6269\u5f20\u6027\u4f1a\u963b\u788d\u53bb\u566a\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2acocoercive conservative (CoCo)\u53bb\u566a\u5668\uff0c\u7ed3\u5408\u4e86Hamiltonian\u6b63\u5219\u5316\u548c\u9891\u8c31\u6b63\u5219\u5316\u6765\u4fc3\u8fdb\u4fdd\u5b88\u6027\u548ccocoerciveness\u3002", "result": "CoCo\u53bb\u566a\u5668\u662f\u5f31\u51f8\u51fd\u6570\u7684\u8fd1\u4f3c\u7b97\u5b50\uff0c\u53ef\u4ee5\u5efa\u7acb\u5177\u6709\u9690\u5f0f\u5f31\u51f8\u5148\u9a8c\u7684\u6062\u590d\u6a21\u578b\uff0c\u5e76\u4e14PnP\u65b9\u6cd5\u53ef\u4ee5\u5168\u5c40\u6536\u655b\u5230\u8be5\u6a21\u578b\u7684\u9a7b\u70b9\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u76f8\u5173\u65b9\u6cd5\u3002"}}
{"id": "2505.09595", "pdf": "https://arxiv.org/pdf/2505.09595", "abs": "https://arxiv.org/abs/2505.09595", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025", "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WorldView-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5168\u7403\u6587\u5316\u5305\u5bb9\u6027\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u5206\u6790\u5176\u5bb9\u7eb3\u591a\u6837\u4e16\u754c\u89c2\u7684\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5e94\u7528\u591a\u91cd\u89c6\u89d2\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6587\u5316\u5305\u5bb9\u6027\uff0c\u5e76\u51cf\u5c11\u6587\u5316\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u672a\u80fd\u5145\u5206\u6355\u6349\u8fd9\u79cd\u504f\u89c1\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u521a\u6027\u3001\u5c01\u95ed\u5f62\u5f0f\u7684\u8bc4\u4f30\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u5305\u5bb9\u6027\u7684\u590d\u6742\u6027\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86WorldView-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5168\u7403\u6587\u5316\u5305\u5bb9\u6027\uff08GCI\uff09\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u5206\u6790\u5b83\u4eec\u5bb9\u7eb3\u591a\u6837\u4e16\u754c\u89c2\u7684\u80fd\u529b\u6765\u8861\u91cf\u6587\u5316\u6781\u5316\uff0c\u5373\u6392\u9664\u5176\u4ed6\u89c2\u70b9\u7684\u7a0b\u5ea6\u3002\u6211\u4eec\u901a\u8fc7\u4e24\u79cd\u5e72\u9884\u7b56\u7565\u5b9e\u73b0\u5e94\u7528\u7684\u591a\u91cd\u89c6\u89d2\uff1a(1) \u5d4c\u5165\u591a\u91cd\u89c6\u89d2\u539f\u5219\u7684\u4e0a\u4e0b\u6587\u5b9e\u65bd\u591a\u91cd\u89c6\u89d2LLMs\uff0c\u4ee5\u53ca(2) \u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5b9e\u65bd\u7684\u591a\u91cd\u89c6\u89d2LLMs\uff0c\u5176\u4e2d\u4ee3\u8868\u4e0d\u540c\u6587\u5316\u89c6\u89d2\u7684\u591a\u4e2aLLM\u4ee3\u7406\u534f\u4f5c\u751f\u6210\u54cd\u5e94\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cPerspectives Distribution Score\uff08PDS\uff09\u71b5\u4ece\u57fa\u7ebf\u768413%\u589e\u52a0\u5230MAS-Implementation\u591a\u91cd\u89c6\u89d2LLMs\u768494%\uff0c\u540c\u65f6\u8f6c\u5411\u79ef\u6781\u60c5\u7eea\uff0867.7%\uff09\u5e76\u589e\u5f3a\u4e86\u6587\u5316\u5e73\u8861\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5177\u6709\u591a\u91cd\u89c6\u89d2\u610f\u8bc6\u7684AI\u8bc4\u4f30\u5728\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u66f4\u5305\u5bb9\u548c\u7b26\u5408\u4f26\u7406\u7684AI\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.09412", "pdf": "https://arxiv.org/pdf/2505.09412", "abs": "https://arxiv.org/abs/2505.09412", "authors": ["Paul Kobialka", "Lina Gerlach", "Francesco Leofante", "Erika \u00c1brah\u00e1m", "Silvia Lizeth Tapia Tarifa", "Einar Broch Johnsen"], "title": "Counterfactual Strategies for Markov Decision Processes", "categories": ["cs.AI", "I.2.m"], "comment": null, "summary": "Counterfactuals are widely used in AI to explain how minimal changes to a\nmodel's input can lead to a different output. However, established methods for\ncomputing counterfactuals typically focus on one-step decision-making, and are\nnot directly applicable to sequential decision-making tasks. This paper fills\nthis gap by introducing counterfactual strategies for Markov Decision Processes\n(MDPs). During MDP execution, a strategy decides which of the enabled actions\n(with known probabilistic effects) to execute next. Given an initial strategy\nthat reaches an undesired outcome with a probability above some limit, we\nidentify minimal changes to the initial strategy to reduce that probability\nbelow the limit. We encode such counterfactual strategies as solutions to\nnon-linear optimization problems, and further extend our encoding to synthesize\ndiverse counterfactual strategies. We evaluate our approach on four real-world\ndatasets and demonstrate its practical viability in sophisticated sequential\ndecision-making tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u7b56\u7565\u7f16\u7801\u4e3a\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6269\u5c55\u4ee5\u5408\u6210\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u53cd\u4e8b\u5b9e\u7684\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u6b65\u51b3\u7b56\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9002\u7528\u4e8eMDP\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\u3002", "method": "\u5c06\u53cd\u4e8b\u5b9e\u7b56\u7565\u7f16\u7801\u4e3a\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6269\u5c55\u4ee5\u5408\u6210\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\u4e3a\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.08977", "pdf": "https://arxiv.org/pdf/2505.08977", "abs": "https://arxiv.org/abs/2505.08977", "authors": ["Hossein Babaei", "Mel White", "Sina Alemohammad", "Richard G. Baraniuk"], "title": "SaFARi: State-Space Models for Frame-Agnostic Representation", "categories": ["cs.LG", "eess.AS", "eess.IV", "eess.SP"], "comment": "13 pages, 5 figures", "summary": "State-Space Models (SSMs) have re-emerged as a powerful tool for online\nfunction approximation, and as the backbone of machine learning models for\nlong-range dependent data. However, to date, only a few polynomial bases have\nbeen explored for this purpose, and the state-of-the-art implementations were\nbuilt upon the best of a few limited options. In this paper, we present a\ngeneralized method for building an SSM with any frame or basis, rather than\nbeing restricted to polynomials. This framework encompasses the approach known\nas HiPPO, but also permits an infinite diversity of other possible \"species\"\nwithin the SSM architecture. We dub this approach SaFARi: SSMs for\nFrame-Agnostic Representation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6784\u5efa\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u5e27\u6216\u57fa\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u591a\u9879\u5f0f\u3002", "motivation": "\u76ee\u524d\uff0c\u7528\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u57fa\u51fd\u6570\u4ec5\u9650\u4e8e\u5c11\u6570\u51e0\u79cd\u591a\u9879\u5f0f\uff0c\u800c\u6700\u5148\u8fdb\u7684\u5b9e\u73b0\u57fa\u4e8e\u8fd9\u4e9b\u6709\u9650\u9009\u9879\u4e2d\u7684\u6700\u4f73\u9009\u62e9\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u6765\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u5e27\u6216\u57fa\u6765\u6784\u5efa\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u800c\u4e0d\u662f\u88ab\u9650\u5236\u5728\u591a\u9879\u5f0f\u4e0a\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u5e27\u6216\u57fa\u6765\u6784\u5efa\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u800c\u4e0d\u662f\u88ab\u9650\u5236\u5728\u591a\u9879\u5f0f\u4e0a\u3002\u8be5\u6846\u67b6\u5305\u62ec\u5df2\u77e5\u7684HiPPO\u65b9\u6cd5\uff0c\u8fd8\u5141\u8bb8\u5728SSM\u67b6\u6784\u4e2d\u5b58\u5728\u65e0\u9650\u591a\u6837\u7684\u5176\u4ed6\u53ef\u80fd\u7684\u201c\u7269\u79cd\u201d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u5e27\u6216\u57fa\u6765\u6784\u5efa\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u800c\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u591a\u9879\u5f0f\u3002\u8be5\u6846\u67b6\u5305\u62ec\u5df2\u77e5\u7684HiPPO\u65b9\u6cd5\uff0c\u8fd8\u5141\u8bb8\u5728SSM\u67b6\u6784\u4e2d\u5b58\u5728\u65e0\u9650\u591a\u6837\u7684\u5176\u4ed6\u53ef\u80fd\u7684\u201c\u7269\u79cd\u201d\u3002"}}
{"id": "2505.08910", "pdf": "https://arxiv.org/pdf/2505.08910", "abs": "https://arxiv.org/abs/2505.08910", "authors": ["Nahid Alam", "Karthik Reddy Kanjula", "Surya Guthikonda", "Timothy Chung", "Bala Krishna S Vegesna", "Abhipsha Das", "Anthony Susevski", "Ryan Sze-Yin Chan", "S M Iftekhar Uddin", "Shayekh Bin Islam", "Roshan Santhosh", "Snegha A", "Drishti Sharma", "Chen Liu", "Isha Chaturvedi", "Genta Indra Winata", "Ashvanth. S", "Snehanshu Mukherjee", "Alham Fikri Aji"], "title": "Behind Maya: Building a Multilingual Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at VLM4ALL CVPR 2025 Workshop", "summary": "In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Maya\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5927\u578bVLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u591a\u6837\u5316\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u6d41\u8bed\u8a00\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u591a\u6837\u5316\u7684\u6587\u5316\u80cc\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Maya\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u8bed\u8a00VLM\uff0c\u5305\u542b\u516b\u4e2a\u8bed\u8a00\u7684\u56fe\u50cf-\u6587\u672c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u652f\u6301\u8fd9\u4e9b\u8bed\u8a00\u7684\u56fe\u50cf-\u6587\u672c\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6587\u5316\u548c\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "result": "Maya\u63d0\u4f9b\u4e86\u591a\u8bed\u8a00\u56fe\u50cf-\u6587\u672c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u652f\u6301\u591a\u79cd\u8bed\u8a00\u7684\u6a21\u578b\u589e\u5f3a\u4e86\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6587\u5316\u548c\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "Maya\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u8bed\u8a00VLM\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u591a\u6837\u5316\u6587\u5316\u80cc\u666f\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2505.09518", "pdf": "https://arxiv.org/pdf/2505.09518", "abs": "https://arxiv.org/abs/2505.09518", "authors": ["Maris F. L. Galesloot", "Roman Andriushchenko", "Milan \u010ce\u0161ka", "Sebastian Junges", "Nils Jansen"], "title": "\\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Partially observable Markov decision processes (POMDPs) model specific\nenvironments in sequential decision-making under uncertainty. Critically,\noptimal policies for POMDPs may not be robust against perturbations in the\nenvironment. Hidden-model POMDPs (HM-POMDPs) capture sets of different\nenvironment models, that is, POMDPs with a shared action and observation space.\nThe intuition is that the true model is hidden among a set of potential models,\nand it is unknown which model will be the environment at execution time. A\npolicy is robust for a given HM-POMDP if it achieves sufficient performance for\neach of its POMDPs. We compute such robust policies by combining two orthogonal\ntechniques: (1) a deductive formal verification technique that supports\ntractable robust policy evaluation by computing a worst-case POMDP within the\nHM-POMDP and (2) subgradient ascent to optimize the candidate policy for a\nworst-case POMDP. The empirical evaluation shows that, compared to various\nbaselines, our approach (1) produces policies that are more robust and\ngeneralize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of\nover a hundred thousand environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9690\u85cf\u6a21\u578bPOMDP\uff08HM-POMDP\uff09\u4e2d\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5f62\u5f0f\u9a8c\u8bc1\u548c\u68af\u5ea6\u4e0a\u5347\u6280\u672f\uff0c\u80fd\u591f\u5728\u5927\u91cf\u73af\u5883\u4e2d\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6700\u4f18\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u62b5\u5fa1\u73af\u5883\u4e2d\u7684\u6270\u52a8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u540c\u73af\u5883\u6a21\u578b\u4e2d\u4fdd\u6301\u6027\u80fd\u7684\u9c81\u68d2\u7b56\u7565\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u79cd\u6b63\u4ea4\u6280\u672f\uff1a(1) \u4e00\u79cd\u6f14\u7ece\u5f62\u5f0f\u9a8c\u8bc1\u6280\u672f\uff0c\u901a\u8fc7\u8ba1\u7b97HM-POMDP\u4e2d\u7684\u6700\u574f\u60c5\u51b5POMDP\u6765\u652f\u6301\u53ef\u6269\u5c55\u7684\u9c81\u68d2\u7b56\u7565\u8bc4\u4f30\uff1b(2) \u68af\u5ea6\u4e0a\u5347\u4f18\u5316\u5019\u9009\u7b56\u7565\u4ee5\u9002\u5e94\u6700\u574f\u60c5\u51b5POMDP\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5404\u79cd\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u7b56\u7565\u66f4\u52a0\u9c81\u68d2\uff0c\u5e76\u4e14\u80fd\u591f\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684POMDP\uff0c\u540c\u65f6\u53ef\u4ee5\u6269\u5c55\u5230\u5305\u542b\u8d85\u8fc7\u5341\u4e07\u4e2a\u73af\u5883\u7684HM-POMDP\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u9690\u85cf\u6a21\u578bPOMDP\uff08HM-POMDP\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u5927\u91cf\u73af\u5883\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u8fc7\u7684POMDP\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.08982", "pdf": "https://arxiv.org/pdf/2505.08982", "abs": "https://arxiv.org/abs/2505.08982", "authors": ["Jiachen Qian", "Yang Zheng"], "title": "Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": null, "summary": "We consider the problem of online prediction for an unknown, non-explosive\nlinear stochastic system. With a known system model, the optimal predictor is\nthe celebrated Kalman filter. In the case of unknown systems, existing\napproaches based on recursive least squares and its variants may suffer from\ndegraded performance due to the highly imbalanced nature of the regression\nmodel. This imbalance can easily lead to overfitting and thus degrade\nprediction accuracy. We tackle this problem by injecting an inductive bias into\nthe regression model via {exponential forgetting}. While exponential forgetting\nis a common wisdom in online learning, it is typically used for re-weighting\ndata. In contrast, our approach focuses on balancing the regression model. This\nachieves a better trade-off between {regression} and {regularization errors},\nand simultaneously reduces the {accumulation error}. With new proof techniques,\nwe also provide a sharper logarithmic regret bound of $O(\\log^3 N)$, where $N$\nis the number of observations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u5f52\u7eb3\u504f\u5dee\u6765\u5e73\u8861\u56de\u5f52\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5728\u7ebf\u9884\u6d4b\u4e2d\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u56de\u5f52\u548c\u6b63\u5219\u5316\u8bef\u5dee\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u5e76\u51cf\u5c11\u4e86\u7d2f\u79ef\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5bf9\u6570\u9057\u61be\u754cO(log^3 N)\u3002", "motivation": "\u5728\u5df2\u77e5\u7cfb\u7edf\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u4f18\u9884\u6d4b\u5668\u662f\u8457\u540d\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u3002\u7136\u800c\uff0c\u5728\u7cfb\u7edf\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u53ca\u5176\u53d8\u4f53\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u56e0\u56de\u5f52\u6a21\u578b\u7684\u9ad8\u5ea6\u4e0d\u5e73\u8861\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5728\u56de\u5f52\u6a21\u578b\u4e2d\u6ce8\u5165\u5f52\u7eb3\u504f\u5dee\uff08\u5373\u6307\u6570\u9057\u5fd8\uff09\u6765\u89e3\u51b3\u5728\u7ebf\u9884\u6d4b\u4e2d\u7684\u95ee\u9898\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u6570\u636e\u52a0\u6743\u65b9\u6cd5\uff0c\u800c\u662f\u4e13\u6ce8\u4e8e\u5e73\u8861\u56de\u5f52\u6a21\u578b\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u56de\u5f52\u548c\u6b63\u5219\u5316\u8bef\u5dee\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u5e76\u51cf\u5c11\u4e86\u7d2f\u79ef\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5bf9\u6570\u9057\u61be\u754cO(log^3 N)\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u5f52\u7eb3\u504f\u5dee\u6765\u5e73\u8861\u56de\u5f52\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5728\u7ebf\u9884\u6d4b\u4e2d\u7531\u4e8e\u56de\u5f52\u6a21\u578b\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u56de\u5f52\u548c\u6b63\u5219\u5316\u8bef\u5dee\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u5e76\u51cf\u5c11\u4e86\u7d2f\u79ef\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5bf9\u6570\u9057\u61be\u754cO(log^3 N)\u3002"}}
{"id": "2505.08961", "pdf": "https://arxiv.org/pdf/2505.08961", "abs": "https://arxiv.org/abs/2505.08961", "authors": ["Yancheng Wang", "Nebojsa Jojic", "Yingzhen Yang"], "title": "Differentiable Channel Selection in Self-Attention For Person Re-Identification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we propose a novel attention module termed the Differentiable\nChannel Selection Attention module, or the DCS-Attention module. In contrast\nwith conventional self-attention, the DCS-Attention module features selection\nof informative channels in the computation of the attention weights. The\nselection of the feature channels is performed in a differentiable manner,\nenabling seamless integration with DNN training. Our DCS-Attention is\ncompatible with either fixed neural network backbones or learnable backbones\nwith Differentiable Neural Architecture Search (DNAS), leading to DCS with\nFixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our\nDCS-Attention is motivated by the principle of Information Bottleneck (IB), and\na novel variational upper bound for the IB loss, which can be optimized by SGD,\nis derived and incorporated into the training loss of the networks with the\nDCS-Attention modules. In this manner, a neural network with DCS-Attention\nmodules is capable of selecting the most informative channels for feature\nextraction so that it enjoys state-of-the-art performance for the Re-ID task.\nExtensive experiments on multiple person Re-ID benchmarks using both DCS-FB and\nDCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy\nof DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention\nin learning discriminative features critical to identifying person identities.\nThe code of our work is available at\nhttps://github.com/Statistical-Deep-Learning/DCS-Attention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDCS-Attention\u7684\u65b0\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5b83\u901a\u8fc7\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u901a\u9053\u6765\u6539\u8fdb\u4f20\u7edf\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u9009\u62e9\u6700\u76f8\u5173\u4fe1\u606f\u7684\u901a\u9053\u3002\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u673a\u5236\u65e0\u6cd5\u6709\u6548\u5730\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u901a\u9053\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u79f0\u4e3a\u53ef\u5fae\u901a\u9053\u9009\u62e9\u6ce8\u610f\u529b\u6a21\u5757\uff08DCS-Attention\u6a21\u5757\uff09\u3002\u8be5\u6a21\u5757\u901a\u8fc7\u5728\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u65f6\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u901a\u9053\u6765\u6539\u8fdb\u4f20\u7edf\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4ee5\u53ef\u5fae\u7684\u65b9\u5f0f\u8fdb\u884c\u7279\u5f81\u901a\u9053\u7684\u9009\u62e9\uff0c\u4ece\u800c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u4e2d\u3002DCS-Attention\u517c\u5bb9\u56fa\u5b9a\u795e\u7ecf\u7f51\u7edc\u4e3b\u5e72\u6216\u53ef\u5b66\u4e60\u7684\u4e3b\u5e72\u4e0e\u53ef\u5fae\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08DNAS\uff09\uff0c\u5206\u522b\u79f0\u4e3aDCS-FB\u548cDCS-DNAS\u3002\u6b64\u5916\uff0cDCS-Attention\u53d7\u5230\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u539f\u7406\u7684\u542f\u53d1\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u79cd\u65b0\u7684\u53d8\u5206\u4e0a\u754c\u7528\u4e8eIB\u635f\u5931\uff0c\u8be5\u635f\u5931\u53ef\u4ee5\u901a\u8fc7SGD\u4f18\u5316\uff0c\u5e76\u88ab\u6574\u5408\u5230\u5e26\u6709DCS-Attention\u6a21\u5757\u7684\u7f51\u7edc\u7684\u8bad\u7ec3\u635f\u5931\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528DCS-FB\u548cDCS-DNAS\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDCS-Attention\u663e\u8457\u63d0\u9ad8\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b66\u4e60\u5224\u522b\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "DCS-Attention\u6a21\u5757\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b66\u4e60\u5224\u522b\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09614", "pdf": "https://arxiv.org/pdf/2505.09614", "abs": "https://arxiv.org/abs/2505.09614", "authors": ["Anthony GX-Chen", "Dongyan Lin", "Mandana Samiei", "Doina Precup", "Blake A. Richards", "Rob Fergus", "Kenneth Marino"], "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5b83\u4eec\u5b58\u5728\u6790\u53d6\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u95f4\u91c7\u6837\u65b9\u6cd5\u6765\u51cf\u5c11\u8fd9\u79cd\u504f\u5dee\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u8005\uff0c\u9700\u8981\u4e3b\u52a8\u6536\u96c6\u4fe1\u606f\u6765\u6307\u5bfc\u4ed6\u4eec\u7684\u51b3\u7b56\u3002\u7136\u800c\uff0c\u5c1a\u4e0d\u6e05\u695a\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u8fd9\u79cd\u80fd\u529b\u6216\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u504f\u5dee\u5bfc\u81f4\u9519\u8bef\u7ed3\u8bba\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86\u53d1\u5c55\u5fc3\u7406\u5b66\u4e2d\u5df2\u5efa\u7acb\u7684\u201cBlicket Test\u201d\u8303\u5f0f\u6765\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u548c\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u95f4\u91c7\u6837\u65b9\u6cd5\uff0c\u4ee5\u663e\u5f0f\u5730\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u91c7\u6837\u5e76\u6d88\u9664\u5173\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u5047\u8bbe\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u5730\u63a8\u65ad\u51fa\u5e38\u89c1\u7684\u3001\u76f4\u89c2\u7684\u6790\u53d6\u56e0\u679c\u5173\u7cfb\uff0c\u4f46\u7cfb\u7edf\u6027\u5730\u96be\u4ee5\u5904\u7406\u4e0d\u5bfb\u5e38\u7684\u3001\u4f46\u540c\u6837\uff08\u6709\u65f6\u751a\u81f3\u66f4\u591a\uff09\u6709\u8bc1\u636e\u652f\u6301\u7684\u5408\u53d6\u56e0\u679c\u5173\u7cfb\u3002\u8fd9\u79cd\u201c\u6790\u53d6\u504f\u5dee\u201d\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u3001\u5927\u5c0f\u548c\u63d0\u793a\u7b56\u7565\u4e2d\u90fd\u5b58\u5728\uff0c\u5e76\u4e14\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u7684\u589e\u52a0\u800c\u8fdb\u4e00\u6b65\u4e0b\u964d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u95f4\u91c7\u6837\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u6790\u53d6\u504f\u5dee\uff0c\u5e76\u4f7f\u5176\u66f4\u63a5\u8fd1\u79d1\u5b66\u3001\u56e0\u679c\u4e25\u8c28\u7684\u63a8\u7406\u76ee\u6807\u3002"}}
{"id": "2505.09003", "pdf": "https://arxiv.org/pdf/2505.09003", "abs": "https://arxiv.org/abs/2505.09003", "authors": ["Zeki Doruk Erden", "Donia Gasmi", "Boi Faltings"], "title": "Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition", "categories": ["cs.LG", "cs.AI"], "comment": "Published in the Autonomous Robots and Multirobot Systems (ARMS)\n  workshop at AAMAS 2025", "summary": "Continual learning for reinforcement learning agents remains a significant\nchallenge, particularly in preserving and leveraging existing information\nwithout an external signal to indicate changes in tasks or environments. In\nthis study, we explore the effectiveness of autoencoders in detecting new tasks\nand matching observed environments to previously encountered ones. Our approach\nintegrates policy optimization with familiarity autoencoders within an\nend-to-end continual learning system. This system can recognize and learn new\ntasks or environments while preserving knowledge from earlier experiences and\ncan selectively retrieve relevant knowledge when re-encountering a known\nenvironment. Initial results demonstrate successful continual learning without\nexternal signals to indicate task changes or reencounters, showing promise for\nthis methodology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u81ea\u52a8\u7f16\u7801\u5668\u5728\u68c0\u6d4b\u65b0\u4efb\u52a1\u548c\u5339\u914d\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u7b56\u7565\u4f18\u5316\u548c\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u6765\u8bf4\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6ca1\u6709\u5916\u90e8\u4fe1\u53f7\u6307\u793a\u4efb\u52a1\u6216\u73af\u5883\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86\u7b56\u7565\u4f18\u5316\u548c\u719f\u6089\u5ea6\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6ca1\u6709\u5916\u90e8\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728\u6ca1\u6709\u5916\u90e8\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.08971", "pdf": "https://arxiv.org/pdf/2505.08971", "abs": "https://arxiv.org/abs/2505.08971", "authors": ["Yangyi Chen", "Hao Peng", "Tong Zhang", "Heng Ji"], "title": "Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The code will be available at https://github.com/Yangyi-Chen/PRIOR", "summary": "In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data.", "AI": {"tldr": "PRIOR is a vision-language pre-training approach that prioritizes image-related tokens through differential weighting in the NTP loss, leading to improved performance and scalability.", "motivation": "The naive NTP in standard large vision-language models (LVLMs) pre-training unintentionally fits the model to noise and increases the risk of hallucination because only a small subset of caption tokens directly relates to the visual content.", "method": "PRIOR introduces a reference model, a text-only LLM, to weight each token based on its probability for LVLMs training. It uses a token-specific re-weighting term based on importance scores to adjust each token's loss.", "result": "PRIOR achieves 19% and 8% average relative improvement on several vision-language benchmarks compared to NTP. It also shows higher scaling coefficients, indicating greater potential for performance gains with increasing compute and data.", "conclusion": "PRIOR exhibits superior scaling properties and shows significant improvements in vision-language benchmarks compared to NTP."}}
{"id": "2505.08842", "pdf": "https://arxiv.org/pdf/2505.08842", "abs": "https://arxiv.org/abs/2505.08842", "authors": ["Zekun Wu", "Seonglae Cho", "Umar Mohammed", "Cristian Munoz", "Kleyton Costa", "Xin Guan", "Theo King", "Ze Wang", "Emre Kazim", "Adriano Koshiyama"], "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Open-source AI libraries are foundational to modern AI systems but pose\nsignificant, underexamined risks across security, licensing, maintenance,\nsupply chain integrity, and regulatory compliance. We present LibVulnWatch, a\ngraph-based agentic assessment framework that performs deep, source-grounded\nevaluations of these libraries. Built on LangGraph, the system coordinates a\ndirected acyclic graph of specialized agents to extract, verify, and quantify\nrisk using evidence from trusted sources such as repositories, documentation,\nand vulnerability databases. LibVulnWatch generates reproducible,\ngovernance-aligned scores across five critical domains, publishing them to a\npublic leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely\nused libraries, including ML frameworks, LLM inference engines, and agent\norchestration tools, our system covers up to 88% of OpenSSF Scorecard checks\nwhile uncovering up to 19 additional risks per library. These include critical\nRemote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials\n(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in\nregulatory documentation and auditability. By translating high-level governance\nprinciples into practical, verifiable metrics, LibVulnWatch advances technical\nAI governance with a scalable, transparent mechanism for continuous supply\nchain risk assessment and informed library selection.", "AI": {"tldr": "LibVulnWatch is a framework that evaluates open-source AI libraries for risks and provides scores for governance alignment, helping in informed library selection.", "motivation": "Open-source AI libraries pose significant risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance, which are underexamined.", "method": "LibVulnWatch is a graph-based agentic assessment framework that performs deep, source-grounded evaluations of open-source AI libraries using evidence from trusted sources such as repositories, documentation, and vulnerability databases.", "result": "LibVulnWatch covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library, including critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and gaps in regulatory documentation and auditability.", "conclusion": "LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection."}}
{"id": "2412.15404", "pdf": "https://arxiv.org/pdf/2412.15404", "abs": "https://arxiv.org/abs/2412.15404", "authors": ["Ahmet Yasin Aytar", "Kemal Kilic", "Kamer Kaya"], "title": "A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "In the rapidly evolving field of data science, efficiently navigating the\nexpansive body of academic literature is crucial for informed decision-making\nand innovation. This paper presents an enhanced Retrieval-Augmented Generation\n(RAG) application, an artificial intelligence (AI)-based system designed to\nassist data scientists in accessing precise and contextually relevant academic\nresources. The AI-powered application integrates advanced techniques, including\nthe GeneRation Of BIbliographic Data (GROBID) technique for extracting\nbibliographic information, fine-tuned embedding models, semantic chunking, and\nan abstract-first retrieval method, to significantly improve the relevance and\naccuracy of the retrieved information. This implementation of AI specifically\naddresses the challenge of academic literature navigation. A comprehensive\nevaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)\nframework demonstrates substantial improvements in key metrics, particularly\nContext Relevance, underscoring the system's effectiveness in reducing\ninformation overload and enhancing decision-making processes. Our findings\nhighlight the potential of this enhanced Retrieval-Augmented Generation system\nto transform academic exploration within data science, ultimately advancing the\nworkflow of research and innovation in the field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u5e2e\u52a9\u6570\u636e\u79d1\u5b66\u5bb6\u66f4\u6709\u6548\u5730\u8bbf\u95ee\u76f8\u5173\u7684\u5b66\u672f\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u9ad8\u4fe1\u606f\u68c0\u7d22\u51c6\u786e\u6027\u548c\u51cf\u5c11\u4fe1\u606f\u8fc7\u8f7d\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u6570\u636e\u79d1\u5b66\u9886\u57df\uff0c\u9ad8\u6548\u5730\u5bfc\u822a\u5e9e\u5927\u7684\u5b66\u672f\u6587\u732e\u662f\u505a\u51fa\u660e\u667a\u51b3\u7b56\u548c\u521b\u65b0\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u7cfb\u7edf\u5728\u68c0\u7d22\u76f8\u5173\u5b66\u672f\u8d44\u6e90\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u589e\u5f3a\u578bRAG\u5e94\u7528\uff0c\u96c6\u6210\u4e86GROBID\u6280\u672f\u3001\u5fae\u8c03\u5d4c\u5165\u6a21\u578b\u3001\u8bed\u4e49\u5206\u5757\u548c\u4ee5\u6458\u8981\u4f18\u5148\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u68c0\u7d22\u4fe1\u606f\u7684\u76f8\u5173\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u901a\u8fc7RAGAS\u6846\u67b6\u8fdb\u884c\u7684\u5168\u9762\u8bc4\u4f30\u663e\u793a\uff0c\u5173\u952e\u6307\u6807\u5982\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u589e\u5f3a\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5728\u6570\u636e\u79d1\u5b66\u9886\u57df\u5b66\u672f\u63a2\u7d22\u4e2d\u7684\u6f5c\u529b\uff0c\u8868\u660e\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4fe1\u606f\u8fc7\u8f7d\u5e76\u63d0\u9ad8\u51b3\u7b56\u8fc7\u7a0b\u7684\u6548\u7387\u3002"}}
{"id": "2505.09011", "pdf": "https://arxiv.org/pdf/2505.09011", "abs": "https://arxiv.org/abs/2505.09011", "authors": ["Antonio Candito", "Matthew D Blackledge", "Richard Holbrey", "Nuria Porta", "Ana Ribeiro", "Fabio Zugni", "Luca D'Erme", "Francesca Castagnoli", "Alina Dragan", "Ricardo Donners", "Christina Messiou", "Nina Tunariu", "Dow-Mu Koh"], "title": "Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer", "categories": ["cs.LG"], "comment": null, "summary": "We developed an AI-driven software solution to quantify metastatic bone\ndisease from WB-DWI scans. Core technologies include: (i) a weakly-supervised\nResidual U-Net model generating a skeleton probability map to isolate bone;\n(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a\nsignal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional\nneural network that processes outputs from (i) and (ii) to generate a mask of\nsuspected bone lesions, characterised by higher b900 signal intensity due to\nrestricted water diffusion. This mask is applied to the gADC map to extract TDV\nand gADC statistics. We tested the tool using expert-defined metastatic bone\ndisease delineations on 66 datasets, assessed repeatability of imaging\nbiomarkers (N=10), and compared software-based response assessment with a\nconstruct reference standard based on clinical, laboratory and imaging\nassessments (N=118). Dice score between manual and automated delineations was\n0.6 for lesions within pelvis and spine, with an average surface distance of\n2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC\nwere below 9% and 5%, respectively. Repeatability analysis showed coefficients\nof variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass\ncorrelation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%\nsensitivity, and 85.7% specificity in assessing response to treatment compared\nto the construct reference standard. Computation time generating a mask\naveraged 90 seconds per scan. Our software enables reproducible TDV and gADC\nquantification from WB-DWI scans for monitoring metastatic bone disease\nresponse, thus providing potentially useful measurements for clinical\ndecision-making in APC patients.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4ece\u5168\u8eab\u6269\u6563\u52a0\u6743\u6210\u50cf\uff08WB-DWI\uff09\u626b\u63cf\u4e2d\u91cf\u5316\u9aa8\u8f6c\u79fb\u75be\u75c5\u3002\u8be5\u8f6f\u4ef6\u901a\u8fc7\u5f31\u76d1\u7763\u7684\u6b8b\u5deeU-Net\u6a21\u578b\u3001\u7edf\u8ba1\u5f52\u4e00\u5316\u6846\u67b6\u548c\u6d45\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\uff0c\u5177\u6709\u826f\u597d\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u80fd\u6709\u6548\u8bc4\u4f30\u6cbb\u7597\u53cd\u5e94\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u5b9a\u91cf\u65b9\u6cd5\u6765\u76d1\u6d4b\u9aa8\u8f6c\u79fb\u75be\u75c5\u7684\u53cd\u5e94\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4eceWB-DWI\u626b\u63cf\u4e2d\u91cf\u5316\u9aa8\u8f6c\u79fb\u75be\u75c5\u3002\u6838\u5fc3\u6280\u672f\u5305\u62ec\uff1a(i) \u4e00\u79cd\u5f31\u76d1\u7763\u7684\u6b8b\u5deeU-Net\u6a21\u578b\u751f\u6210\u9aa8\u9abc\u6982\u7387\u56fe\u4ee5\u9694\u79bb\u9aa8\u9abc\uff1b(ii) \u4e00\u79cd\u7edf\u8ba1\u6846\u67b6\u7528\u4e8eWB-DWI\u5f3a\u5ea6\u5f52\u4e00\u5316\uff0c\u83b7\u5f97\u4fe1\u53f7\u5f52\u4e00\u5316\u7684b=900s/mm^2\uff08b900\uff09\u56fe\u50cf\uff1b\u4ee5\u53ca(iii) \u4e00\u4e2a\u6d45\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5904\u7406(i)\u548c(ii)\u7684\u8f93\u51fa\u4ee5\u751f\u6210\u7591\u4f3c\u9aa8\u75c5\u53d8\u7684\u63a9\u7801\uff0c\u8fd9\u4e9b\u75c5\u53d8\u7531\u4e8e\u53d7\u9650\u7684\u6c34\u6269\u6563\u8868\u73b0\u51fa\u66f4\u9ad8\u7684b900\u4fe1\u53f7\u5f3a\u5ea6\u3002\u6b64\u63a9\u7801\u5e94\u7528\u4e8egADC\u56fe\u4ee5\u63d0\u53d6TDV\u548cgADC\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "\u624b\u52a8\u548c\u81ea\u52a8\u5206\u5272\u4e4b\u95f4\u7684Dice\u5206\u6570\u4e3a0.6\uff0c\u5e73\u5747\u8868\u9762\u8ddd\u79bb\u4e3a2mm\u3002log-TDV\u548c\u4e2d\u4f4d\u6570gADC\u7684\u76f8\u5bf9\u5dee\u5f02\u5206\u522b\u4f4e\u4e8e9%\u548c5%\u3002\u53ef\u91cd\u590d\u6027\u5206\u6790\u663e\u793alog-TDV\u548c\u4e2d\u4f4d\u6570gADC\u7684\u53d8\u5f02\u7cfb\u6570\u5206\u522b\u4e3a4.57%\u548c3.54%\uff0c\u7ec4\u5185\u76f8\u5173\u7cfb\u6570\u9ad8\u4e8e0.9\u3002\u8f6f\u4ef6\u5728\u4e0e\u6784\u5efa\u53c2\u8003\u6807\u51c6\u6bd4\u8f83\u65f6\uff0c\u5728\u8bc4\u4f30\u6cbb\u7597\u53cd\u5e94\u65b9\u9762\u8fbe\u5230\u4e8680.5%\u7684\u51c6\u786e\u7387\u300184.3%\u7684\u7075\u654f\u5ea6\u548c85.7%\u7684\u7279\u5f02\u6027\u3002\u751f\u6210\u63a9\u7801\u7684\u8ba1\u7b97\u65f6\u95f4\u5e73\u5747\u4e3a\u6bcf\u626b\u63cf90\u79d2\u3002", "conclusion": "\u6211\u4eec\u7684\u8f6f\u4ef6\u80fd\u591f\u4ece\u5168\u8eab\u6269\u6563\u52a0\u6743\u6210\u50cf\uff08WB-DWI\uff09\u626b\u63cf\u4e2d\u53ef\u91cd\u590d\u5730\u91cf\u5316\u603b\u75c5\u7076\u4f53\u79ef\uff08TDV\uff09\u548c\u5168\u5c40\u8868\u89c2\u6269\u6563\u7cfb\u6570\uff08gADC\uff09\uff0c\u4ece\u800c\u4e3a\u76d1\u6d4b\u9aa8\u8f6c\u79fb\u75be\u75c5\u7684\u53cd\u5e94\u63d0\u4f9b\u6f5c\u5728\u6709\u7528\u7684\u6d4b\u91cf\u503c\uff0c\u8fd9\u53ef\u80fd\u5bf9\u524d\u5217\u817a\u764c\uff08APC\uff09\u60a3\u8005\u7684\u4e34\u5e8a\u51b3\u7b56\u6709\u5e2e\u52a9\u3002"}}
{"id": "2505.08999", "pdf": "https://arxiv.org/pdf/2505.08999", "abs": "https://arxiv.org/abs/2505.08999", "authors": ["Wei-Long Tian", "Peng Gao", "Xiao Liu", "Long Xu", "Hamido Fujita", "Hanan Aljuai", "Mao-Li Wang"], "title": "Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, visual tracking methods based on convolutional neural\nnetworks and Transformers have achieved remarkable performance and have been\nsuccessfully applied in fields such as autonomous driving. However, the\nnumerous security issues exposed by deep learning models have gradually\naffected the reliable application of visual tracking methods in real-world\nscenarios. Therefore, how to reveal the security vulnerabilities of existing\nvisual trackers through effective adversarial attacks has become a critical\nproblem that needs to be addressed. To this end, we propose an adaptive\nmeta-gradient adversarial attack (AMGA) method for visual tracking. This method\nintegrates multi-model ensembles and meta-learning strategies, combining\nmomentum mechanisms and Gaussian smoothing, which can significantly enhance the\ntransferability and attack effectiveness of adversarial examples. AMGA randomly\nselects models from a large model repository, constructs diverse tracking\nscenarios, and iteratively performs both white- and black-box adversarial\nattacks in each scenario, optimizing the gradient directions of each model.\nThis paradigm minimizes the gap between white- and black-box adversarial\nattacks, thus achieving excellent attack performance in black-box scenarios.\nExtensive experimental results on large-scale datasets such as OTB2015, LaSOT,\nand GOT-10k demonstrate that AMGA significantly improves the attack\nperformance, transferability, and deception of adversarial examples. Codes and\ndata are available at https://github.com/pgao-lab/AMGA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5143\u68af\u5ea6\u5bf9\u6297\u653b\u51fb\uff08AMGA\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u6a21\u578b\u96c6\u6210\u548c\u5143\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5bf9\u6297\u6837\u672c\u7684\u653b\u51fb\u6027\u80fd\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u66b4\u9732\u7684\u4f17\u591a\u5b89\u5168\u95ee\u9898\u9010\u6e10\u5f71\u54cd\u4e86\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u6765\u63ed\u793a\u73b0\u6709\u89c6\u89c9\u8ddf\u8e2a\u5668\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "AMGA\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u6a21\u578b\u96c6\u6210\u548c\u5143\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u52a8\u91cf\u673a\u5236\u548c\u9ad8\u65af\u5e73\u6ed1\uff0c\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u653b\u51fb\u6548\u679c\u3002", "result": "AMGA\u5728OTB2015\u3001LaSOT\u548cGOT-10k\u7b49\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u6837\u672c\u7684\u653b\u51fb\u6027\u80fd\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u6b3a\u9a97\u6027\u3002", "conclusion": "AMGA\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u6837\u672c\u7684\u653b\u51fb\u6027\u80fd\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u6b3a\u9a97\u6027\u3002"}}
{"id": "2505.08902", "pdf": "https://arxiv.org/pdf/2505.08902", "abs": "https://arxiv.org/abs/2505.08902", "authors": ["Lucas McCullum", "Pelagie Ami Agassi", "Leo Anthony Celi", "Daniel K. Ebner", "Chrystinne Oliveira Fernandes", "Rachel S. Hicklen", "Mkliwa Koumbia", "Lisa Soleymani Lehmann", "David Restrepo"], "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner.", "AI": {"tldr": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u5e94\u5173\u6ce8\u4eba\u7c7b\u4e0eLLM\u7684\u534f\u4f5c\u800c\u975e\u6bd4\u8f83\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u4f7f\u7528\u3002", "motivation": "\u5f53\u524d\u7684\u7814\u7a76\u52aa\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u5c06LLM\u4e0e\u4e00\u7ec4\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46'\u4e13\u5bb6'\u8fd9\u4e00\u6982\u5ff5\u5f80\u5f80\u4e0d\u660e\u786e\u6216\u4e0d\u65ad\u53d8\u5316\u3002\u5982\u679c\u6ca1\u6709\u9002\u5f53\u7684\u4fdd\u969c\u63aa\u65bd\uff0cLLM\u53ef\u80fd\u4f1a\u5a01\u80c1\u5230\u5b89\u5168\u4ea4\u4ed8\u60a3\u8005\u62a4\u7406\u7684\u65e2\u5b9a\u7ed3\u6784\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u7c7b\u4e0eLLM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u534f\u4f5c\uff0c\u800c\u4e0d\u662f\u5c06LLM\u4e0e\u4eba\u7c7b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u9700\u8981\u5f00\u53d1\u7b56\u7565\uff0c\u4f7f\u4eba\u7c7b\u4e0eLLM\u80fd\u591f\u4ee5\u51e0\u4e4e\u5171\u751f\u7684\u65b9\u5f0f\u9ad8\u6548\u5de5\u4f5c\u3002", "conclusion": "\u7814\u7a76\u5e94\u4e13\u6ce8\u4e8e\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u6709\u6548\u8868\u5f81LLM\u7684\u5b89\u5168\u4f7f\u7528\uff0c\u8fd9\u79cd\u4f7f\u7528\u65b9\u5f0f\u80fd\u9002\u5e94\u65b0\u578bLLM\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u3002"}}
{"id": "2505.09017", "pdf": "https://arxiv.org/pdf/2505.09017", "abs": "https://arxiv.org/abs/2505.09017", "authors": ["Bizhan Alipour Pijan", "Serdar Bozdag"], "title": "DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Most of the dynamic graph representation learning methods involve dividing a\ndynamic graph into discrete snapshots to capture the evolving behavior of nodes\nover time. Existing methods primarily capture only local or global structures\nof each node within a snapshot using message-passing and random walk-based\nmethods. Then, they utilize sequence-based models (e.g., transformers) to\nencode the temporal evolution of node embeddings, and meta-learning techniques\nto update the model parameters. However, these approaches have two limitations.\nFirst, they neglect the extraction of global and local information\nsimultaneously in each snapshot. Second, they fail to consider the model's\nperformance in the current snapshot during parameter updates, resulting in a\nlack of temporal dependency management. Recently, HiPPO (High-order Polynomial\nProjection Operators) algorithm has gained attention for their ability to\noptimize and preserve sequence history in State Space Model (SSM). To address\nthe aforementioned limitations in dynamic graph representation learning, we\npropose a novel method called Multi-view Dynamic Graph Embeddings with State\nSpace Model Gradient Update (DyGSSM). Our approach combines Graph Convolution\nNetworks (GCN) for local feature extraction and random walk with Gated\nRecurrent Unit (GRU) for global feature extraction in each snapshot. We then\nintegrate the local and global features using a cross-attention mechanism.\nAdditionally, we incorporate an SSM based on HiPPO algorithm to account for\nlong-term dependencies when updating model parameters, ensuring that model\nperformance in each snapshot informs subsequent updates. Experiments on five\npublic datasets show that our method outperforms existing baseline and\nstate-of-the-art (SOTA) methods in 17 out of 20 cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u56fe\u5d4c\u5165\u65b9\u6cd5DyGSSM\uff0c\u7ed3\u5408\u4e86GCN\u548cGRU\u8fdb\u884c\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8eHiPPO\u7684SSM\u6765\u5904\u7406\u957f\u671f\u4f9d\u8d56\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6bcf\u4e2a\u5feb\u7167\u4e2d\u4ec5\u63d0\u53d6\u5c40\u90e8\u6216\u5168\u5c40\u7ed3\u6784\uff0c\u672a\u80fd\u540c\u65f6\u63d0\u53d6\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u53c2\u6570\u66f4\u65b0\u65f6\u672a\u8003\u8651\u5f53\u524d\u5feb\u7167\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u65f6\u95f4\u4f9d\u8d56\u6027\u7ba1\u7406\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDyGSSM\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u8fdb\u884c\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u548c\u968f\u673a\u6e38\u8d70\u4e0e\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u8fdb\u884c\u5168\u5c40\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eHiPPO\u7b97\u6cd5\u7684SSM\u6765\u5904\u7406\u957f\u671f\u4f9d\u8d56\u6027\uff0c\u5728\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u65f6\u786e\u4fdd\u6bcf\u4e2a\u5feb\u7167\u7684\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u540e\u7eed\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u572820\u4e2a\u6848\u4f8b\u4e2d\u768417\u4e2a\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u572820\u4e2a\u6848\u4f8b\u4e2d\u768417\u4e2a\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.09018", "pdf": "https://arxiv.org/pdf/2505.09018", "abs": "https://arxiv.org/abs/2505.09018", "authors": ["Adarsh Kumar"], "title": "Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective dietary monitoring is critical for managing Type 2 diabetes, yet\naccurately estimating caloric intake remains a major challenge. While\ncontinuous glucose monitors (CGMs) offer valuable physiological data, they\noften fall short in capturing the full nutritional profile of meals due to\ninter-individual and meal-specific variability. In this work, we introduce a\nmultimodal deep learning framework that jointly leverages CGM time-series data,\nDemographic/Microbiome, and pre-meal food images to enhance caloric estimation.\nOur model utilizes attention based encoding and a convolutional feature\nextraction for meal imagery, multi-layer perceptrons for CGM and Microbiome\ndata followed by a late fusion strategy for joint reasoning. We evaluate our\napproach on a curated dataset of over 40 participants, incorporating\nsynchronized CGM, Demographic and Microbiome data and meal photographs with\nstandardized caloric labels. Our model achieves a Root Mean Squared Relative\nError (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These\nfindings demonstrate the potential of multimodal sensing to improve automated\ndietary assessment tools for chronic disease management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408CGM\u6570\u636e\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66/\u5fae\u751f\u7269\u7ec4\u6570\u636e\u548c\u9910\u524d\u98df\u7269\u56fe\u50cf\u6765\u63d0\u9ad8\u5361\u8def\u91cc\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002\u5728\u5305\u542b40\u591a\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u6709\u6548\u7684\u996e\u98df\u76d1\u6d4b\u5bf9\u4e8e\u7ba1\u74062\u578b\u7cd6\u5c3f\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u51c6\u786e\u4f30\u7b97\u5361\u8def\u91cc\u6444\u5165\u91cf\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u867d\u7136\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u4eea\uff08CGMs\uff09\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u751f\u7406\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u4e2a\u4f53\u5dee\u5f02\u548c\u9910\u98df\u7279\u5f02\u6027\u53d8\u5316\uff0c\u5b83\u4eec\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5230\u9910\u98df\u7684\u5b8c\u6574\u8425\u517b\u6982\u51b5\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8054\u5408\u5229\u7528CGM\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66/\u5fae\u751f\u7269\u7ec4\u6570\u636e\u548c\u9910\u524d\u98df\u7269\u56fe\u50cf\u6765\u589e\u5f3a\u5361\u8def\u91cc\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u6a21\u578b\u4f7f\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7f16\u7801\u548c\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u7528\u4e8e\u9910\u98df\u56fe\u50cf\uff0c\u591a\u5c42\u611f\u77e5\u5668\u7528\u4e8eCGM\u548c\u5fae\u751f\u7269\u7ec4\u6570\u636e\uff0c\u7136\u540e\u91c7\u7528\u665a\u671f\u878d\u5408\u7b56\u7565\u8fdb\u884c\u8054\u5408\u63a8\u7406\u3002", "result": "\u6211\u4eec\u5728\u4e00\u4e2a\u5305\u542b40\u591a\u540d\u53c2\u4e0e\u8005\u7684\u7cbe\u5fc3\u6574\u7406\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u540c\u6b65\u7684CGM\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u5fae\u751f\u7269\u7ec4\u6570\u636e\u4ee5\u53ca\u5e26\u6709\u6807\u51c6\u5316\u5361\u8def\u91cc\u6807\u7b7e\u7684\u9910\u98df\u7167\u7247\u3002\u6211\u4eec\u7684\u6a21\u578b\u5b9e\u73b0\u4e860.2544\u7684\u5747\u65b9\u6839\u76f8\u5bf9\u8bef\u5dee\uff08RMSRE\uff09\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad8\u51fa50%\u4ee5\u4e0a\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u591a\u6a21\u6001\u4f20\u611f\u5728\u6539\u5584\u6162\u6027\u75c5\u7ba1\u7406\u7684\u81ea\u52a8\u5316\u996e\u98df\u8bc4\u4f30\u5de5\u5177\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.08798", "pdf": "https://arxiv.org/pdf/2505.08798", "abs": "https://arxiv.org/abs/2505.08798", "authors": ["Mobina Shrestha", "Bishwas Mandal", "Vishal Mandal", "Asis Shrestha"], "title": "In-Context Learning for Label-Efficient Cancer Image Classification in Oncology", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The application of AI in oncology has been limited by its reliance on large,\nannotated datasets and the need for retraining models for domain-specific\ndiagnostic tasks. Taking heed of these limitations, we investigated in-context\nlearning as a pragmatic alternative to model retraining by allowing models to\nadapt to new diagnostic tasks using only a few labeled examples at inference,\nwithout the need for retraining. Using four vision-language models\n(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across\nthree oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our\nknowledge, this is the first study to compare the performance of multiple VLMs\non different oncology classification tasks. Without any parameter updates, all\nmodels showed significant gains with few-shot prompting, with GPT-4o reaching\nan F1 score of 0.81 in binary classification and 0.60 in multi-class\nclassification settings. While these results remain below the ceiling of fully\nfine-tuned systems, they highlight the potential of ICL to approximate\ntask-specific behavior using only a handful of examples, reflecting how\nclinicians often reason from prior cases. Notably, open-source models like\nPaligemma and CLIP demonstrated competitive gains despite their smaller size,\nsuggesting feasibility for deployment in computing constrained clinical\nenvironments. Overall, these findings highlight the potential of ICL as a\npractical solution in oncology, particularly for rare cancers and\nresource-limited contexts where fine-tuning is infeasible and annotated data is\ndifficult to obtain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5c11\u91cf\u6807\u8bb0\u793a\u4f8b\u6765\u9002\u5e94\u65b0\u7684\u8bca\u65ad\u4efb\u52a1\uff0c\u4ece\u800c\u66ff\u4ee3\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u6f5c\u5728\u65b9\u6cd5\u3002", "motivation": "AI\u5728\u80bf\u7624\u5b66\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u5176\u5bf9\u5927\u578b\u6ce8\u91ca\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u4ee5\u53ca\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u4ee5\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u8bca\u65ad\u4efb\u52a1\u7684\u9650\u5236\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u56db\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09-Paligemma\u3001CLIP\u3001ALIGN\u548cGPT-4o\uff0c\u8bc4\u4f30\u4e86\u5728\u4e09\u4e2a\u80bf\u7624\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff1aMHIST\u3001PatchCamelyon\u548cHAM10000\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u90fd\u663e\u793a\u51fa\u663e\u8457\u7684\u63d0\u5347\uff0cGPT-4o\u5728\u4e8c\u5206\u7c7b\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e860.81\u7684F1\u5206\u6570\uff0c\u5728\u591a\u7c7b\u5206\u7c7b\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e860.60\u3002\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u578b\u5982Paligemma\u548cCLIP\u5c3d\u7ba1\u89c4\u6a21\u8f83\u5c0f\uff0c\u4f46\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u63d0\u5347\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86ICL\u5728\u80bf\u7624\u5b66\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7f55\u89c1\u764c\u75c7\u548c\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\uff0c\u5176\u4e2d\u5fae\u8c03\u4e0d\u53ef\u884c\u4e14\u6ce8\u91ca\u6570\u636e\u96be\u4ee5\u83b7\u5f97\u3002"}}
{"id": "2505.09022", "pdf": "https://arxiv.org/pdf/2505.09022", "abs": "https://arxiv.org/abs/2505.09022", "authors": ["Annan Yu", "N. Benjamin Erichson"], "title": "Block-Biased Mamba for Long-Range Sequence Processing", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Mamba extends earlier state space models (SSMs) by introducing\ninput-dependent dynamics, and has demonstrated strong empirical performance\nacross a range of domains, including language modeling, computer vision, and\nfoundation models. However, a surprising weakness remains: despite being built\non architectures designed for long-range dependencies, Mamba performs poorly on\nlong-range sequential tasks. Understanding and addressing this gap is important\nfor improving Mamba's universality and versatility. In this work, we analyze\nMamba's limitations through three perspectives: expressiveness, inductive bias,\nand training stability. Our theoretical results show how Mamba falls short in\neach of these aspects compared to earlier SSMs such as S4D. To address these\nissues, we propose $\\text{B}_2\\text{S}_6$, a simple extension of Mamba's S6\nunit that combines block-wise selective dynamics with a channel-specific bias.\nWe prove that these changes equip the model with a better-suited inductive bias\nand improve its expressiveness and stability. Empirically,\n$\\text{B}_2\\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks\nwhile maintaining Mamba's performance on language modeling benchmarks.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Mamba\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86B_2S_6\u6765\u6539\u8fdb\u5176\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8Mamba\u7684\u901a\u7528\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u9700\u8981\u7406\u89e3\u5e76\u89e3\u51b3\u5176\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u89c6\u89d2\u5206\u6790Mamba\u7684\u5c40\u9650\u6027\uff1a\u8868\u8fbe\u80fd\u529b\u3001\u5f52\u7eb3\u504f\u5dee\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u51faB_2S_6\u6765\u6539\u8fdb\u8fd9\u4e9b\u65b9\u9762\u3002", "result": "B_2S_6\u5728LRA\u4efb\u52a1\u4e0a\u4f18\u4e8eS4\u548cS4D\uff0c\u540c\u65f6\u4fdd\u6301\u4e86Mamba\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u5206\u6790\u4e86Mamba\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86B_2S_6\u6765\u6539\u8fdb\u5176\u6027\u80fd\u3002"}}
{"id": "2505.09073", "pdf": "https://arxiv.org/pdf/2505.09073", "abs": "https://arxiv.org/abs/2505.09073", "authors": ["J. Brennan Peace", "Shuowen Hu", "Benjamin S. Riggan"], "title": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition", "categories": ["cs.CV"], "comment": "To appear at the IEEE International Conference on Automatic Face and\n  Gesture 2025 (FG2025)", "summary": "Despite recent advances in facial recognition, there remains a fundamental\nissue concerning degradations in performance due to substantial perspective\n(pose) differences between enrollment and query (probe) imagery. Therefore, we\npropose a novel domain adaptive framework to facilitate improved performances\nacross large discrepancies in pose by enabling image-based (2D) representations\nto infer properties of inherently pose invariant point cloud (3D)\nrepresentations. Specifically, our proposed framework achieves better pose\ninvariance by using (1) a shared (joint) attention mapping to emphasize common\npatterns that are most correlated between 2D facial images and 3D facial data\nand (2) a joint entropy regularizing loss to promote better\nconsistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D\nand 3D representations$\\unicode{x2014}$by leveraging both attention maps. This\nframework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms\ncompetitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$)\nTAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and\n1.57$\\unicode{x0025}$, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5728\u5927\u59ff\u6001\u5dee\u5f02\u4e0b\u7684\u9762\u90e8\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5728\u6ce8\u518c\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u59ff\u6001\u5dee\u5f02\uff0c\u5bfc\u81f4\u9762\u90e8\u8bc6\u522b\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u5171\u4eab\u7684\uff08\u8054\u5408\uff09\u6ce8\u610f\u529b\u6620\u5c04\u548c\u8054\u5408\u71b5\u6b63\u5219\u5316\u635f\u5931\u6765\u63d0\u9ad8\u59ff\u6001\u4e0d\u53d8\u6027\u3002", "result": "\u8be5\u6846\u67b6\u5728Profile\uff0890\u00b0\u00b1\uff09TAR @ 1% FAR\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e86\u81f3\u5c117.1%\u548c1.57%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728FaceScape\u548cARL-VTF\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.09063", "pdf": "https://arxiv.org/pdf/2505.09063", "abs": "https://arxiv.org/abs/2505.09063", "authors": ["Khalid Rafiq", "Wenjing Liao", "Aditya G. Nair"], "title": "Single-shot prediction of parametric partial differential equations", "categories": ["cs.LG", "cs.NA", "math.NA", "68T07"], "comment": "35 pages, 17 figures", "summary": "We introduce Flexi-VAE, a data-driven framework for efficient single-shot\nforecasting of nonlinear parametric partial differential equations (PDEs),\neliminating the need for iterative time-stepping while maintaining high\naccuracy and stability. Flexi-VAE incorporates a neural propagator that\nadvances latent representations forward in time, aligning latent evolution with\nphysical state reconstruction in a variational autoencoder setting. We evaluate\ntwo propagation strategies, the Direct Concatenation Propagator (DCP) and the\nPositional Encoding Propagator (PEP), and demonstrate, through\nrepresentation-theoretic analysis, that DCP offers superior long-term\ngeneralization by fostering disentangled and physically meaningful latent\nspaces. Geometric diagnostics, including Jacobian spectral analysis, reveal\nthat propagated latent states reside in regions of lower decoder sensitivity\nand more stable local geometry than those derived via direct encoding,\nenhancing robustness for long-horizon predictions. We validate Flexi-VAE on\ncanonical PDE benchmarks, the 1D viscous Burgers equation and the 2D\nadvection-diffusion equation, achieving accurate forecasts across wide\nparametric ranges. The model delivers over 50x CPU and 90x GPU speedups\ncompared to autoencoder-LSTM baselines for large temporal shifts. These results\nposition Flexi-VAE as a scalable and interpretable surrogate modeling tool for\naccelerating high-fidelity simulations in computational fluid dynamics (CFD)\nand other parametric PDE-driven applications, with extensibility to\nhigher-dimensional and more complex systems.", "AI": {"tldr": "Flexi-VAE \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e00\u6b21\u6027\u9884\u6d4b\u975e\u7ebf\u6027\u53c2\u6570\u5316\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u4f20\u64ad\u5668\u5b9e\u73b0\u6f5c\u5728\u8868\u793a\u7684\u65f6\u95f4\u63a8\u8fdb\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8fed\u4ee3\u65f6\u95f4\u6b65\u8fdb\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u6548\u7387\u4f4e\u4e0b\u3002Flexi-VAE \u7684\u76ee\u6807\u662f\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u7a33\u5b9a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u52a0\u901f\u9ad8\u4fdd\u771f\u6a21\u62df\u3002", "method": "Flexi-VAE \u5f15\u5165\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u4e00\u6b21\u6027\u9884\u6d4b\u975e\u7ebf\u6027\u53c2\u6570\u5316\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\uff0c\u6d88\u9664\u4e86\u8fed\u4ee3\u65f6\u95f4\u6b65\u8fdb\u7684\u9700\u8981\u3002\u5b83\u7ed3\u5408\u4e86\u4e00\u4e2a\u795e\u7ecf\u4f20\u64ad\u5668\uff0c\u8be5\u4f20\u64ad\u5668\u5728\u65f6\u95f4\u4e0a\u63a8\u8fdb\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u5728\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u8bbe\u7f6e\u4e2d\u5bf9\u9f50\u6f5c\u5728\u6f14\u5316\u4e0e\u7269\u7406\u72b6\u6001\u91cd\u5efa\u3002", "result": "Flexi-VAE \u5728\u7ecf\u5178\u7684 PDE \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5305\u62ec 1D \u7c98\u6027 Burgers \u65b9\u7a0b\u548c 2D \u5bf9\u6d41\u6269\u6563\u65b9\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u53c2\u6570\u8303\u56f4\u5185\u7684\u51c6\u786e\u9884\u6d4b\u3002\u6a21\u578b\u5728\u5927\u89c4\u6a21\u65f6\u95f4\u504f\u79fb\u60c5\u51b5\u4e0b\u6bd4\u81ea\u7f16\u7801\u5668-LSTM \u57fa\u7ebf\u5feb 50 \u500d CPU \u548c 90 \u500d GPU\u3002", "conclusion": "Flexi-VAE \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u5efa\u6a21\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u52a0\u901f\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u548c\u5176\u4ed6\u53c2\u6570\u5316\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u9a71\u52a8\u7684\u5e94\u7528\u4e2d\u7684\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u66f4\u9ad8\u7ef4\u548c\u66f4\u590d\u6742\u7684\u7cfb\u7edf\u3002"}}
{"id": "2505.09092", "pdf": "https://arxiv.org/pdf/2505.09092", "abs": "https://arxiv.org/abs/2505.09092", "authors": ["Yuhang Wang", "Abdulaziz Alhuraish", "Shengming Yuan", "Hao Zhou"], "title": "OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its\nreal-world performance remains underexplored due to proprietary systems and\nlimited data access. This paper presents OpenLKA, the first open, large-scale\ndataset for LKA evaluation and improvement. It includes 400 hours of driving\ndata from 50+ production vehicle models, collected through extensive road\ntesting in Tampa, Florida and global contributions from the Comma.ai driving\ncommunity. The dataset spans a wide range of challenging scenarios, including\ncomplex road geometries, degraded lane markings, adverse weather, lighting\nconditions and surrounding traffic. The dataset is multimodal, comprising: i)\nfull CAN bus streams, decoded using custom reverse-engineered DBC files to\nextract key LKA events (e.g., system disengagements, lane detection failures);\nii) synchronized high-resolution dash-cam video; iii) real-time outputs from\nOpenpilot, providing accurate estimates of road curvature and lane positioning;\niv) enhanced scene annotations generated by Vision Language Models, describing\nlane visibility, pavement quality, weather, lighting, and traffic conditions.\nBy integrating vehicle-internal signals with high-fidelity perception and rich\nsemantic context, OpenLKA provides a comprehensive platform for benchmarking\nthe real-world performance of production LKA systems, identifying\nsafety-critical operational scenarios, and assessing the readiness of current\nroad infrastructure for autonomous driving. The dataset is publicly available\nat: https://github.com/OpenLKA/OpenLKA.", "AI": {"tldr": "This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement, which includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing and global contributions. The dataset is multimodal, comprising full CAN bus streams, synchronized high-resolution dash-cam video, real-time outputs from Openpilot, and enhanced scene annotations generated by Vision Language Models.", "motivation": "Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access.", "method": "The paper introduces OpenLKA, an open, large-scale dataset for LKA evaluation and improvement, which includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing and global contributions. It also describes the multimodal nature of the dataset, comprising full CAN bus streams, synchronized high-resolution dash-cam video, real-time outputs from Openpilot, and enhanced scene annotations generated by Vision Language Models.", "result": "OpenLKA is a comprehensive dataset that spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions, and surrounding traffic. It integrates vehicle-internal signals with high-fidelity perception and rich semantic context, providing a platform for benchmarking the real-world performance of production LKA systems.", "conclusion": "OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving."}}
{"id": "2505.09076", "pdf": "https://arxiv.org/pdf/2505.09076", "abs": "https://arxiv.org/abs/2505.09076", "authors": ["Berkay Guler", "Hamid Jafarkhani"], "title": "AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Deep learning models for channel estimation in Orthogonal Frequency Division\nMultiplexing (OFDM) systems often suffer from performance degradation under\nfast-fading channels and low-SNR scenarios. To address these limitations, we\nintroduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model\nspecifically designed to enhance channel estimation in challenging\nenvironments. Our approach employs convolutional layers that exploit locality\nbias to capture strong correlations between neighboring channel elements,\ncombined with a transformer encoder that applies the global Attention mechanism\nto channel patches. This approach effectively models both long-range\ndependencies and spectro-temporal interactions within single OFDM frames. We\nfurther augment the model's adaptability by integrating nonlinear\nrepresentations of available channel statistics SNR, delay spread, and Doppler\nshift as priors. A residual connection is employed to merge global features\nfrom the transformer with local features from early convolutional processing,\nfollowed by final convolutional layers to refine the hierarchical channel\nrepresentation. Despite its compact architecture, AdaFortiTran achieves up to 6\ndB reduction in mean squared error (MSE) compared to state-of-the-art models.\nTested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),\nand delay spreads (50-300 ns), it demonstrates superior robustness in\nhigh-mobility environments.", "AI": {"tldr": "AdaFortiTran is a novel model that improves channel estimation in OFDM systems by combining convolutional layers with a transformer encoder and integrating channel statistics as priors, achieving significant performance improvements.", "motivation": "Deep learning models for channel estimation in OFDM systems often suffer from performance degradation under fast-fading channels and low-SNR scenarios. The goal is to enhance channel estimation in challenging environments.", "method": "AdaFortiTran combines convolutional layers that exploit locality bias with a transformer encoder that applies the global Attention mechanism to channel patches. It also integrates nonlinear representations of channel statistics as priors and uses a residual connection to merge global and local features.", "result": "AdaFortiTran achieves up to 6 dB reduction in MSE compared to state-of-the-art models and shows superior robustness across a wide range of Doppler shifts, SNRs, and delay spreads.", "conclusion": "AdaFortiTran achieves up to 6 dB reduction in mean squared error (MSE) compared to state-of-the-art models and demonstrates superior robustness in high-mobility environments."}}
{"id": "2505.09118", "pdf": "https://arxiv.org/pdf/2505.09118", "abs": "https://arxiv.org/abs/2505.09118", "authors": ["Dayong Liang", "Changmeng Zheng", "Zhiyuan Wen", "Yi Cai", "Xiao-Yong Wei", "Qing Li"], "title": "Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Traditional scene graphs primarily focus on spatial relationships, limiting\nvision-language models' (VLMs) ability to reason about complex interactions in\nvisual scenes. This paper addresses two key challenges: (1) conventional\ndetection-to-construction methods produce unfocused, contextually irrelevant\nrelationship sets, and (2) existing approaches fail to form persistent memories\nfor generalizing interaction reasoning to new scenes. We propose\nInteraction-augmented Scene Graph Reasoning (ISGR), a framework that enhances\nVLMs' interactional reasoning through three complementary components. First,\nour dual-stream graph constructor combines SAM-powered spatial relation\nextraction with interaction-aware captioning to generate functionally salient\nscene graphs with spatial grounding. Second, we employ targeted interaction\nqueries to activate VLMs' latent knowledge of object functionalities,\nconverting passive recognition into active reasoning about how objects work\ntogether. Finally, we introduce a lone-term memory reinforcement learning\nstrategy with a specialized interaction-focused reward function that transforms\ntransient patterns into long-term reasoning heuristics. Extensive experiments\ndemonstrate that our approach significantly outperforms baseline methods on\ninteraction-heavy reasoning benchmarks, with particularly strong improvements\non complex scene understanding tasks. The source code can be accessed at\nhttps://github.com/open_upon_acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6ISGR\uff0c\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u89c6\u89c9\u573a\u666f\u4e2d\u590d\u6742\u4ea4\u4e92\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff1a\u53cc\u6d41\u56fe\u6784\u9020\u5668\u3001\u9488\u5bf9\u6027\u7684\u4ea4\u4e92\u67e5\u8be2\u4ee5\u53ca\u957f\u671f\u8bb0\u5fc6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ea4\u4e92\u5bc6\u96c6\u7684\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u573a\u666f\u56fe\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u89c6\u89c9\u573a\u666f\u4e2d\u590d\u6742\u4ea4\u4e92\u7684\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u89e3\u51b3\u4e86\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1) \u4f20\u7edf\u7684\u68c0\u6d4b\u5230\u6784\u9020\u65b9\u6cd5\u4ea7\u751f\u4e0d\u96c6\u4e2d\u3001\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u5173\u7cfb\u96c6\uff1b(2) \u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5f62\u6210\u6301\u4e45\u7684\u8bb0\u5fc6\uff0c\u4ee5\u5c06\u4ea4\u4e92\u63a8\u7406\u63a8\u5e7f\u5230\u65b0\u573a\u666f\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Interaction-augmented Scene Graph Reasoning (ISGR)\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\u589e\u5f3aVLMs\u7684\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u4eec\u7684\u53cc\u6d41\u56fe\u6784\u9020\u5668\u7ed3\u5408\u4e86SAM\u9a71\u52a8\u7684\u7a7a\u95f4\u5173\u7cfb\u63d0\u53d6\u548c\u4ea4\u4e92\u611f\u77e5\u7684\u63cf\u8ff0\u751f\u6210\uff0c\u4ee5\u751f\u6210\u529f\u80fd\u663e\u8457\u7684\u573a\u666f\u56fe\u3002\u5176\u6b21\uff0c\u6211\u4eec\u4f7f\u7528\u9488\u5bf9\u6027\u7684\u4ea4\u4e92\u67e5\u8be2\u6765\u6fc0\u6d3bVLMs\u4e2d\u7269\u4f53\u529f\u80fd\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u5c06\u88ab\u52a8\u8bc6\u522b\u8f6c\u5316\u4e3a\u5bf9\u7269\u4f53\u5982\u4f55\u534f\u540c\u5de5\u4f5c\u7684\u4e3b\u52a8\u63a8\u7406\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u957f\u671f\u8bb0\u5fc6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u4e13\u95e8\u7684\u4ea4\u4e92\u805a\u7126\u5956\u52b1\u51fd\u6570\uff0c\u5c06\u77ed\u6682\u6a21\u5f0f\u8f6c\u5316\u4e3a\u957f\u671f\u63a8\u7406\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4ea4\u4e92\u5bc6\u96c6\u7684\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4ea4\u4e92\u5bc6\u96c6\u7684\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2505.08807", "pdf": "https://arxiv.org/pdf/2505.08807", "abs": "https://arxiv.org/abs/2505.08807", "authors": ["Yuntao Wang", "Yanghe Pan", "Shaolong Guo", "Zhou Su"], "title": "Security of Internet of Agents: Attacks and Countermeasures", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 5 figures, 3 tables, submitted to IEEE OJCS", "summary": "With the rise of large language and vision-language models, AI agents have\nevolved into autonomous, interactive systems capable of perception, reasoning,\nand decision-making. As they proliferate across virtual and physical domains,\nthe Internet of Agents (IoA) has emerged as a key infrastructure for enabling\nscalable and secure coordination among heterogeneous agents. This survey offers\na comprehensive examination of the security and privacy landscape in IoA\nsystems. We begin by outlining the IoA architecture and its distinct\nvulnerabilities compared to traditional networks, focusing on four critical\naspects: identity authentication threats, cross-agent trust issues, embodied\nsecurity, and privacy risks. We then review existing and emerging defense\nmechanisms and highlight persistent challenges. Finally, we identify open\nresearch directions to advance the development of resilient and\nprivacy-preserving IoA ecosystems.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u5173\u4e8eIoA\u7cfb\u7edf\u4e2d\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u7684\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u67b6\u6784\u3001\u6f0f\u6d1e\u3001\u9632\u5fa1\u673a\u5236\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u865a\u62df\u548c\u7269\u7406\u9886\u57df\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u5bf9IoA\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u4ee5\u786e\u4fdd\u5176\u53ef\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6982\u8ff0IoA\u67b6\u6784\u53ca\u5176\u4e0e\u4f20\u7edf\u7f51\u7edc\u7684\u4e0d\u540c\u6f0f\u6d1e\uff0c\u56de\u987e\u4e86\u73b0\u6709\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5e76\u6307\u51fa\u4e86\u6301\u7eed\u7684\u6311\u6218\u3002", "result": "\u672c\u6587\u5168\u9762\u5ba1\u89c6\u4e86IoA\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u548c\u9690\u79c1\u666f\u89c2\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u8eab\u4efd\u8ba4\u8bc1\u5a01\u80c1\u3001\u8de8\u4ee3\u7406\u4fe1\u4efb\u95ee\u9898\u3001\u5b9e\u4f53\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u8ba8\u8bba\u4e86\u73b0\u6709\u7684\u9632\u5fa1\u673a\u5236\u548c\u6301\u7eed\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86IoA\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u5177\u6709\u5f39\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684IoA\u751f\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.09085", "pdf": "https://arxiv.org/pdf/2505.09085", "abs": "https://arxiv.org/abs/2505.09085", "authors": ["Jiaxuan Chen", "Yu Qi", "Yueming Wang", "Gang Pan"], "title": "Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in deep neural networks (DNNs), particularly large-scale\nlanguage models, have demonstrated remarkable capabilities in image and natural\nlanguage understanding. Although scaling up model parameters with increasing\nvolume of training data has progressively improved DNN capabilities, achieving\ncomplex cognitive abilities - such as understanding abstract concepts,\nreasoning, and adapting to novel scenarios, which are intrinsic to human\ncognition - remains a major challenge. In this study, we show that\nbrain-in-the-loop supervised learning, utilizing a small set of brain signals,\ncan effectively transfer human conceptual structures to DNNs, significantly\nenhancing their comprehension of abstract and even unseen concepts.\nExperimental results further indicate that the enhanced cognitive capabilities\nlead to substantial performance gains in challenging tasks, including\nfew-shot/zero-shot learning and out-of-distribution recognition, while also\nyielding highly interpretable concept representations. These findings highlight\nthat human-in-the-loop supervision can effectively augment the complex\ncognitive abilities of large models, offering a promising pathway toward\ndeveloping more human-like cognitive abilities in artificial systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8111\u4fe1\u53f7\u5c06\u4eba\u7c7b\u6982\u5ff5\u7ed3\u6784\u8f6c\u79fb\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5176\u7406\u89e3\u548c\u5904\u7406\u62bd\u8c61\u53ca\u672a\u89c1\u8fc7\u6982\u5ff5\u7684\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u5c3d\u7ba1\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u91cf\u6765\u6269\u5927\u6a21\u578b\u53c2\u6570\u5df2\u9010\u6b65\u63d0\u9ad8\u4e86DNN\u7684\u80fd\u529b\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u7684\u8ba4\u77e5\u80fd\u529b\uff08\u5982\u7406\u89e3\u62bd\u8c61\u6982\u5ff5\u3001\u63a8\u7406\u548c\u9002\u5e94\u65b0\u573a\u666f\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u8111\u4e2d\u5faa\u73af\u76d1\u7763\u5b66\u4e60\uff0c\u5229\u7528\u5c11\u91cf\u8111\u4fe1\u53f7\u5c06\u4eba\u7c7b\u6982\u5ff5\u7ed3\u6784\u8f6c\u79fb\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u7684\u8ba4\u77e5\u80fd\u529b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5305\u62ec\u5c11\u6837\u672c/\u96f6\u6837\u672c\u5b66\u4e60\u548c\u5206\u5e03\u5916\u8bc6\u522b\uff0c\u540c\u65f6\u4ea7\u751f\u4e86\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u8868\u793a\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u4eba\u673a\u4ea4\u4e92\u76d1\u7763\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u5927\u578b\u6a21\u578b\u7684\u590d\u6742\u8ba4\u77e5\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u8ba4\u77e5\u80fd\u529b\u7684\u4eba\u5de5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2505.09123", "pdf": "https://arxiv.org/pdf/2505.09123", "abs": "https://arxiv.org/abs/2505.09123", "authors": ["Guoying Liang", "Su Yang"], "title": "Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Big model has emerged as a new research paradigm that can be applied to\nvarious down-stream tasks with only minor effort for domain adaption.\nCorrespondingly, this study tackles Camouflaged Object Detection (COD)\nleveraging the Segment Anything Model (SAM). The previous studies declared that\nSAM is not workable for COD but this study reveals that SAM works if promoted\nproperly, for which we devise a new framework to render point promotions:\nFirst, we develop the Promotion Point Targeting Network (PPT-net) to leverage\nmulti-scale features in predicting the probabilities of camouflaged objects'\npresences at given candidate points over the image. Then, we develop a key\npoint selection (KPS) algorithm to deploy both positive and negative point\npromotions contrastively to SAM to guide the segmentation. It is the first work\nto facilitate big model for COD and achieves plausible results experimentally\nover the existing methods on 3 data sets under 6 metrics. This study\ndemonstrates an off-the-shelf methodology for COD by leveraging SAM, which\ngains advantage over designing professional models from scratch, not only in\nperformance, but also in turning the problem to a less challenging task, that\nis, seeking informative but not exactly precise promotions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528Segment Anything Model (SAM)\u8fdb\u884c\u4f2a\u88c5\u7269\u4f53\u68c0\u6d4b\uff08COD\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u63d0\u793a\u6765\u4fc3\u8fdbSAM\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u8ba4\u4e3aSAM\u65e0\u6cd5\u7528\u4e8eCOD\uff0c\u4f46\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5982\u679c\u9002\u5f53\u4fc3\u8fdb\uff0cSAM\u662f\u53ef\u884c\u7684\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5927\u6a21\u578b\u8fdb\u884cCOD\uff0c\u5e76\u5b9e\u73b0\u4ee4\u4eba\u6ee1\u610f\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u63d0\u793a\u6765\u4fc3\u8fdbSAM\uff0c\u5305\u62ec\u5f00\u53d1\u4e86Promotion Point Targeting Network (PPT-net)\u6765\u9884\u6d4b\u7ed9\u5b9a\u5019\u9009\u70b9\u5728\u56fe\u50cf\u4e2d\u5b58\u5728\u4f2a\u88c5\u7269\u4f53\u7684\u6982\u7387\uff0c\u4ee5\u53ca\u5f00\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u70b9\u9009\u62e9(KPS)\u7b97\u6cd5\uff0c\u4ee5\u5bf9\u6bd4\u65b9\u5f0f\u90e8\u7f72\u6b63\u8d1f\u70b9\u63d0\u793a\u6765\u5f15\u5bfc\u5206\u5272\u3002", "result": "\u8be5\u7814\u7a76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u516d\u4e2a\u6307\u6807\u4e0b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd9\u662f\u9996\u6b21\u5229\u7528\u5927\u6a21\u578b\u8fdb\u884cCOD\u7684\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u5229\u7528SAM\u8fdb\u884cCOD\u7684\u73b0\u6210\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4ece\u5934\u8bbe\u8ba1\u4e13\u4e1a\u6a21\u578b\uff0c\u800c\u4e14\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u4e2a\u4e0d\u592a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5373\u5bfb\u627e\u6709\u4fe1\u606f\u91cf\u4f46\u4e0d\u7cbe\u786e\u7684\u63d0\u793a\u3002"}}
{"id": "2505.09089", "pdf": "https://arxiv.org/pdf/2505.09089", "abs": "https://arxiv.org/abs/2505.09089", "authors": ["Philipp Hess", "Maximilian Gelbrecht", "Christof Sch\u00f6tz", "Michael Aich", "Yu Huang", "Shangshang Yang", "Niklas Boers"], "title": "Generating time-consistent dynamics with discriminator-guided image diffusion models", "categories": ["cs.LG"], "comment": null, "summary": "Realistic temporal dynamics are crucial for many video generation, processing\nand modelling applications, e.g. in computational fluid dynamics, weather\nprediction, or long-term climate simulations. Video diffusion models (VDMs) are\nthe current state-of-the-art method for generating highly realistic dynamics.\nHowever, training VDMs from scratch can be challenging and requires large\ncomputational resources, limiting their wider application. Here, we propose a\ntime-consistency discriminator that enables pretrained image diffusion models\nto generate realistic spatiotemporal dynamics. The discriminator guides the\nsampling inference process and does not require extensions or finetuning of the\nimage diffusion model. We compare our approach against a VDM trained from\nscratch on an idealized turbulence simulation and a real-world global\nprecipitation dataset. Our approach performs equally well in terms of temporal\nconsistency, shows improved uncertainty calibration and lower biases compared\nto the VDM, and achieves stable centennial-scale climate simulations at daily\ntime steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u4e00\u81f4\u6027\u5224\u522b\u5668\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u80fd\u591f\u751f\u6210\u73b0\u5b9e\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u65e0\u9700\u5bf9\u6a21\u578b\u8fdb\u884c\u6269\u5c55\u6216\u5fae\u8c03\u3002\u5728\u7406\u60f3\u5316\u7684\u6e4d\u6d41\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5168\u7403\u964d\u6c34\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u5e76\u964d\u4f4e\u4e86\u504f\u5dee\uff0c\u5e76\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u767e\u5e74\u5c3a\u5ea6\u6c14\u5019\u6a21\u62df\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u662f\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u52a8\u6001\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3VDMs\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u4e00\u81f4\u6027\u5224\u522b\u5668\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u80fd\u591f\u751f\u6210\u73b0\u5b9e\u7684\u65f6\u7a7a\u52a8\u6001\u3002\u8be5\u5224\u522b\u5668\u6307\u5bfc\u91c7\u6837\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e0d\u9700\u8981\u5bf9\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6269\u5c55\u6216\u5fae\u8c03\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u76f8\u5f53\uff0c\u4e0eVDM\u76f8\u6bd4\u663e\u793a\u51fa\u6539\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u8f83\u4f4e\u7684\u504f\u5dee\uff0c\u5e76\u4e14\u5728\u6bcf\u5929\u7684\u65f6\u95f4\u6b65\u957f\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u767e\u5e74\u5c3a\u5ea6\u6c14\u5019\u6a21\u62df\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u76f8\u5f53\uff0c\u4e0eVDM\u76f8\u6bd4\u663e\u793a\u51fa\u6539\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u8f83\u4f4e\u7684\u504f\u5dee\uff0c\u5e76\u4e14\u5728\u6bcf\u5929\u7684\u65f6\u95f4\u6b65\u957f\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u767e\u5e74\u5c3a\u5ea6\u6c14\u5019\u6a21\u62df\u3002"}}
{"id": "2505.09129", "pdf": "https://arxiv.org/pdf/2505.09129", "abs": "https://arxiv.org/abs/2505.09129", "authors": ["Wei Meng"], "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes", "categories": ["cs.CV", "cs.AI", "es: 68T10, 68T05, 62H35, 68U10", "I.4.9; I.5.1; I.2.10"], "comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight\n  weakly-supervised color intelligence model for tactical video anomaly\n  detection, tested on anonymized African surveillance data", "summary": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u989c\u8272\u7279\u5f81\u7684\u8f7b\u91cf\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u654f\u611f\u6761\u4ef6\u4e0b\u5feb\u901f\u8bc6\u522b\u6f5c\u5728\u5a01\u80c1\u4e8b\u4ef6\u3002\u8be5\u65b9\u6cd5\u878d\u5408\u4e86\u65e0\u76d1\u7763KMeans\u805a\u7c7b\u548cRGB\u901a\u9053\u76f4\u65b9\u56fe\u5efa\u6a21\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u6218\u672f\u6697\u6740\u9884\u8b66\u3001\u53ef\u7591\u7269\u4f53\u7b5b\u67e5\u548c\u73af\u5883\u5267\u70c8\u53d8\u5316\u76d1\u6d4b\u4e2d\u5177\u6709\u826f\u597d\u7684\u6548\u679c\u548c\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65e0\u6807\u7b7e\u3001\u6570\u636e\u4e0d\u53ef\u83b7\u53d6\u7684\u89c6\u9891\u667a\u80fd\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u98ce\u9669\u5b89\u5168\u4efb\u52a1\u90e8\u7f72\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u989c\u8272\u7279\u5f81\u7684\u8f7b\u91cf\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u4ee5\u5feb\u901f\u8bc6\u522b\u548c\u89e3\u91ca\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u654f\u611f\u6761\u4ef6\u4e0b\u7684\u6f5c\u5728\u5a01\u80c1\u4e8b\u4ef6\u3002", "method": "\u8be5\u65b9\u6cd5\u878d\u5408\u4e86\u65e0\u76d1\u7763KMeans\u805a\u7c7b\u548cRGB\u901a\u9053\u76f4\u65b9\u56fe\u5efa\u6a21\uff0c\u4ee5\u5b9e\u73b0\u5173\u952e\u5e27\u4e2d\u7ed3\u6784\u5f02\u5e38\u548c\u989c\u8272\u7a81\u53d8\u4fe1\u53f7\u7684\u590d\u5408\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u4ee5\u4e00\u4e2a\u975e\u6d32\u56fd\u5bb6\u53d1\u751f\u7684\u64cd\u4f5c\u76d1\u63a7\u89c6\u9891\u4e3a\u7814\u7a76\u6837\u672c\uff0c\u5728\u65e0\u6cd5\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u8bc6\u522b\u51fa\u4e0e\u9ad8\u80fd\u5149\u6e90\u3001\u76ee\u6807\u5b58\u5728\u548c\u53cd\u5c04\u5e72\u6270\u76f8\u5173\u7684\u591a\u4e2a\u9ad8\u5ea6\u5f02\u5e38\u5e27\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6218\u672f\u6697\u6740\u9884\u8b66\u3001\u53ef\u7591\u7269\u4f53\u7b5b\u67e5\u548c\u73af\u5883\u5267\u70c8\u53d8\u5316\u76d1\u6d4b\u4e2d\u5177\u6709\u5f88\u5f3a\u7684\u53ef\u90e8\u7f72\u6027\u548c\u6218\u672f\u89e3\u91ca\u4ef7\u503c\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u989c\u8272\u7279\u5f81\u4f5c\u4e3a\u4f4e\u8bed\u4e49\u6218\u573a\u4fe1\u53f7\u8f7d\u4f53\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c06\u5728\u672a\u6765\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u65f6\u5e8f\u5efa\u6a21\u8fdb\u4e00\u6b65\u6269\u5c55\u6218\u573a\u667a\u80fd\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2505.08809", "pdf": "https://arxiv.org/pdf/2505.08809", "abs": "https://arxiv.org/abs/2505.08809", "authors": ["Shixi Qin", "Zhiyong Yang", "Shilong Bao", "Shi Wang", "Qianqian Xu", "Qingming Huang"], "title": "MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schr\u00f6dinger Bridges", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper focuses on implanting multiple heterogeneous backdoor triggers in\nbridge-based diffusion models designed for complex and arbitrary input\ndistributions. Existing backdoor formulations mainly address single-attack\nscenarios and are limited to Gaussian noise input models. To fill this gap, we\npropose MixBridge, a novel diffusion Schr\\\"odinger bridge (DSB) framework to\ncater to arbitrary input distributions (taking I2I tasks as special cases).\nBeyond this trait, we demonstrate that backdoor triggers can be injected into\nMixBridge by directly training with poisoned image pairs. This eliminates the\nneed for the cumbersome modifications to stochastic differential equations\nrequired in previous studies, providing a flexible tool to study backdoor\nbehavior for bridge models. However, a key question arises: can a single DSB\nmodel train multiple backdoor triggers? Unfortunately, our theory shows that\nwhen attempting this, the model ends up following the geometric mean of benign\nand backdoored distributions, leading to performance conflict across backdoor\ntasks. To overcome this, we propose a Divide-and-Merge strategy to mix\ndifferent bridges, where models are independently pre-trained for each specific\nobjective (Divide) and then integrated into a unified model (Merge). In\naddition, a Weight Reallocation Scheme (WRS) is also designed to enhance the\nstealthiness of MixBridge. Empirical studies across diverse generation tasks\nspeak to the efficacy of MixBridge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MixBridge\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6865\u63a5\u6269\u6563\u6a21\u578b\u4e2d\u690d\u5165\u591a\u4e2a\u5f02\u6784\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u5e76\u901a\u8fc7Divide-and-Merge\u7b56\u7565\u548cWRS\u63d0\u5347\u6548\u679c\u4e0e\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u6b21\u653b\u51fb\u573a\u666f\uff0c\u5e76\u4e14\u5c40\u9650\u4e8e\u9ad8\u65af\u566a\u58f0\u8f93\u5165\u6a21\u578b\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u548c\u4efb\u610f\u8f93\u5165\u5206\u5e03\u7684\u60c5\u51b5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u859b\u5b9a\u8c14\u6865\uff08DSB\uff09\u6846\u67b6MixBridge\uff0c\u901a\u8fc7\u76f4\u63a5\u4f7f\u7528\u4e2d\u6bd2\u56fe\u50cf\u5bf9\u8fdb\u884c\u8bad\u7ec3\u6765\u6ce8\u5165\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u5e76\u91c7\u7528Divide-and-Merge\u7b56\u7565\u548cWRS\u6765\u589e\u5f3a\u6027\u80fd\u548c\u9690\u853d\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMixBridge\u5728\u5404\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u690d\u5165\u591a\u4e2a\u540e\u95e8\u89e6\u53d1\u5668\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86MixBridge\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5728\u6865\u63a5\u6269\u6563\u6a21\u578b\u4e2d\u690d\u5165\u591a\u4e2a\u5f02\u6784\u540e\u95e8\u89e6\u53d1\u5668\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Divide-and-Merge\u7b56\u7565\u548cWeight Reallocation Scheme (WRS)\u6765\u63d0\u9ad8\u5176\u6709\u6548\u6027\u4e0e\u9690\u853d\u6027\u3002"}}
{"id": "2505.09106", "pdf": "https://arxiv.org/pdf/2505.09106", "abs": "https://arxiv.org/abs/2505.09106", "authors": ["Ya Liu", "Kai Yang", "Yu Zhu", "Keying Yang", "Haibo Zhao"], "title": "Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network", "categories": ["cs.LG", "68T07", "I.2"], "comment": "17 pages, 11 figures", "summary": "The space-air-ground integrated network (SAGIN) has recently emerged as a\ncore element in the 6G networks. However, traditional centralized and\nsynchronous optimization algorithms are unsuitable for SAGIN due to\ninfrastructureless and time-varying environments. This paper aims to develop a\nnovel Asynchronous algorithm a.k.a. Argus for tackling non-convex and\nnon-smooth decentralized federated bilevel learning over SAGIN. The proposed\nalgorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle\nbilevel learning problems in time-varying networks asynchronously, thereby\naverting stragglers from impeding the overall training speed. We provide a\ntheoretical analysis of the iteration complexity, communication complexity, and\ncomputational complexity of Argus. Its effectiveness is further demonstrated\nthrough numerical experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6b65\u7b97\u6cd5Argus\uff0c\u7528\u4e8e\u89e3\u51b3SAGIN\u4e2d\u7684\u975e\u51f8\u548c\u975e\u5149\u6ed1\u5206\u5e03\u5f0f\u8054\u90a6\u53cc\u5c42\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u548c\u540c\u6b65\u4f18\u5316\u7b97\u6cd5\u4e0d\u9002\u7528\u4e8eSAGIN\uff0c\u56e0\u4e3a\u5176\u5177\u6709\u65e0\u57fa\u7840\u8bbe\u65bd\u548c\u65f6\u95f4\u53d8\u5316\u7684\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6b65\u7b97\u6cd5Argus\uff0c\u7528\u4e8e\u89e3\u51b3SAGIN\u4e2d\u7684\u975e\u51f8\u548c\u975e\u5149\u6ed1\u5206\u5e03\u5f0f\u8054\u90a6\u53cc\u5c42\u5b66\u4e60\u95ee\u9898\u3002", "result": "\u63d0\u4f9b\u4e86Argus\u7b97\u6cd5\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u3001\u901a\u4fe1\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7406\u8bba\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Argus\u7b97\u6cd5\u5728SAGIN\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u51f8\u548c\u975e\u5149\u6ed1\u7684\u5206\u5e03\u5f0f\u8054\u90a6\u53cc\u5c42\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2505.09139", "pdf": "https://arxiv.org/pdf/2505.09139", "abs": "https://arxiv.org/abs/2505.09139", "authors": ["Lucas Choi", "Ross Greer"], "title": "Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) offer flexible object detection through natural\nlanguage prompts but suffer from performance variability depending on prompt\nphrasing. In this paper, we introduce a method for automated prompt refinement\nusing a novel metric called the Contrastive Class Alignment Score (CCAS), which\nranks prompts based on their semantic alignment with a target object class\nwhile penalizing similarity to confounding classes. Our method generates\ndiverse prompt candidates via a large language model and filters them through\nCCAS, computed using prompt embeddings from a sentence transformer. We evaluate\nour approach on challenging object categories, demonstrating that our automatic\nselection of high-precision prompts improves object detection accuracy without\nthe need for additional model training or labeled data. This scalable and\nmodel-agnostic pipeline offers a principled alternative to manual prompt\nengineering for VLM-based detection systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u7c7b\u5bf9\u9f50\u5206\u6570\uff08CCAS\uff09\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u6216\u6807\u8bb0\u6570\u636e\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0\u7075\u6d3b\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u4f46\u5176\u6027\u80fd\u4f1a\u56e0\u63d0\u793a\u7684\u63aa\u8f9e\u800c\u6709\u6240\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u63d0\u793a\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u7c7b\u5bf9\u9f50\u5206\u6570\uff08CCAS\uff09\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684\u63d0\u793a\u5019\u9009\uff0c\u5e76\u5229\u7528CCAS\u8fdb\u884c\u8fc7\u6ee4\uff0c\u5176\u4e2dCCAS\u901a\u8fc7\u53e5\u5b50\u8f6c\u6362\u5668\u7684\u63d0\u793a\u5d4c\u5165\u8ba1\u7b97\u5f97\u51fa\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u76ee\u6807\u7c7b\u522b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u52a8\u9009\u62e9\u9ad8\u7cbe\u5ea6\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u76ee\u6807\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u6216\u6807\u8bb0\u6570\u636e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u6bd4\u7c7b\u5bf9\u9f50\u5206\u6570\uff08CCAS\uff09\u6765\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u6216\u6807\u8bb0\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8eVLM\u7684\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.09083", "pdf": "https://arxiv.org/pdf/2505.09083", "abs": "https://arxiv.org/abs/2505.09083", "authors": ["Dominic Zaun Eu Jones"], "title": "Ornithologist: Towards Trustworthy \"Reasoning\" about Central Bank Communications", "categories": ["econ.GN", "cs.CL", "q-fin.EC", "J.4; I.2.7"], "comment": "16 pages, 6 figures", "summary": "I develop Ornithologist, a weakly-supervised textual classification system\nand measure the hawkishness and dovishness of central bank text. Ornithologist\nuses ``taxonomy-guided reasoning'', guiding a large language model with\nhuman-authored decision trees. This increases the transparency and\nexplainability of the system and makes it accessible to non-experts. It also\nreduces hallucination risk. Since it requires less supervision than traditional\nclassification systems, it can more easily be applied to other problems or\nsources of text (e.g. news) without much modification. Ornithologist\nmeasurements of hawkishness and dovishness of RBA communication carry\ninformation about the future of the cash rate path and of market expectations.", "AI": {"tldr": "Ornithologist is a weakly-supervised system for classifying central bank texts, using taxonomy-guided reasoning to improve transparency and reduce hallucination risks.", "motivation": "To develop a weakly-supervised textual classification system that increases transparency, explainability, and reduces hallucination risk.", "method": "Ornithologist uses 'taxonomy-guided reasoning', guiding a large language model with human-authored decision trees.", "result": "Ornithologist can be applied to other problems or sources of text without much modification.", "conclusion": "Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations."}}
{"id": "2505.08810", "pdf": "https://arxiv.org/pdf/2505.08810", "abs": "https://arxiv.org/abs/2505.08810", "authors": ["Bappa Muktar", "Vincent Fono", "Adama Nouboukpo"], "title": "Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent\nTransportation Systems (ITS), particularly in enabling real-time communication\nfor emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,\nwhich interfere with safety-critical communication channels, can severely\nimpair their reliability. This study introduces a robust and scalable framework\nto detect DDoS attacks in highway-based VANET environments. A synthetic dataset\nwas constructed using Network Simulator 3 (NS-3) in conjunction with the\nSimulation of Urban Mobility (SUMO) and further enriched with real-world\nmobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).\nThree traffic categories were simulated: DDoS, VoIP, and TCP-based video\nstreaming (VideoTCP). The data preprocessing pipeline included normalization,\nsignal-to-noise ratio (SNR) feature engineering, missing value imputation, and\nclass balancing using the Synthetic Minority Over-sampling Technique (SMOTE).\nFeature importance was assessed using SHapley Additive exPlanations (SHAP).\nEleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),\nAdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).\nXGB and CB achieved the best performance, each attaining an F1-score of 96%.\nThese results highlight the robustness of the proposed framework and its\npotential for real-time deployment in VANETs to secure critical emergency\ncommunications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u57fa\u4e8e\u9ad8\u901f\u516c\u8def\u7684VANET\u73af\u5883\u4e2dDDoS\u653b\u51fb\u7684\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002\u901a\u8fc7\u4f7f\u7528NS-3\u548cSUMO\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u771f\u5b9e\u79fb\u52a8\u8f68\u8ff9\u8fdb\u884c\u4e30\u5bcc\uff0c\u6570\u636e\u9884\u5904\u7406\u5305\u62ec\u5f52\u4e00\u5316\u3001SNR\u7279\u5f81\u5de5\u7a0b\u3001\u7f3a\u5931\u503c\u63d2\u8865\u548cSMOTE\u7c7b\u522b\u5e73\u8861\u3002\u901a\u8fc7SHAP\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u5bf9\u591a\u4e2a\u5206\u7c7b\u5668\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002XGB\u548cCB\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe\u523096%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8eVANET\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\uff0c\u4ee5\u4fdd\u62a4\u5173\u952e\u7684\u5e94\u6025\u901a\u4fe1\u3002", "motivation": "VANETs\u5728ITS\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u4e3a\u5e94\u6025\u8f66\u8f86\u63d0\u4f9b\u5b9e\u65f6\u901a\u4fe1\u65b9\u9762\u3002\u7136\u800c\uff0cDDoS\u653b\u51fb\u4f1a\u4e25\u91cd\u635f\u5bb3\u5176\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u68c0\u6d4bDDoS\u653b\u51fb\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u68c0\u6d4b\u57fa\u4e8e\u9ad8\u901f\u516c\u8def\u7684VANET\u73af\u5883\u4e2d\u7684DDoS\u653b\u51fb\u3002\u4f7f\u7528NS-3\u548cSUMO\u6784\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u4e00\u6b65\u7528\u5fb7\u56fdA81\u9ad8\u901f\u516c\u8def\u7684\u771f\u5b9e\u79fb\u52a8\u8f68\u8ff9\u8fdb\u884c\u4e30\u5bcc\u3002\u6570\u636e\u9884\u5904\u7406\u5305\u62ec\u5f52\u4e00\u5316\u3001\u4fe1\u566a\u6bd4\uff08SNR\uff09\u7279\u5f81\u5de5\u7a0b\u3001\u7f3a\u5931\u503c\u63d2\u8865\u548c\u4f7f\u7528SMOTE\u8fdb\u884c\u7c7b\u522b\u5e73\u8861\u3002\u901a\u8fc7SHAP\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u5bf9\u5341\u4e00\u4e2a\u5206\u7c7b\u5668\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "XGB\u548cCB\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u6bcf\u4e2a\u90fd\u83b7\u5f97\u4e8696%\u7684F1\u5206\u6570\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u7a33\u5065\u6027\u53ca\u5176\u5728VANET\u4e2d\u5b9e\u65f6\u90e8\u7f72\u4ee5\u4fdd\u62a4\u5173\u952e\u5e94\u6025\u901a\u4fe1\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09113", "pdf": "https://arxiv.org/pdf/2505.09113", "abs": "https://arxiv.org/abs/2505.09113", "authors": ["Yingrong Wang", "Anpeng Wu", "Baohong Li", "Ziyang Xiao", "Ruoxuan Xiong", "Qing Han", "Kun Kuang"], "title": "Sequential Treatment Effect Estimation with Unmeasured Confounders", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "This paper studies the cumulative causal effects of sequential treatments in\nthe presence of unmeasured confounders. It is a critical issue in sequential\ndecision-making scenarios where treatment decisions and outcomes dynamically\nevolve over time. Advanced causal methods apply transformer as a backbone to\nmodel such time sequences, which shows superiority in capturing long time\ndependence and periodic patterns via attention mechanism. However, even they\ncontrol the observed confounding, these estimators still suffer from unmeasured\nconfounders, which influence both treatment assignments and outcomes. How to\nadjust the latent confounding bias in sequential treatment effect estimation\nremains an open challenge. Therefore, we propose a novel Decomposing Sequential\nInstrumental Variable framework for CounterFactual Regression (DSIV-CFR),\nrelying on a common negative control assumption. Specifically, an instrumental\nvariable (IV) is a special negative control exposure, while the previous\noutcome serves as a negative control outcome. This allows us to recover the IVs\nlatent in observation variables and estimate sequential treatment effects via a\ngeneralized moment condition. We conducted experiments on 4 datasets and\nachieved significant performance in one- and multi-step prediction, supported\nby which we can identify optimal treatments for dynamic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6DSIV-CFR\uff0c\u7528\u4e8e\u89e3\u51b3\u5e8f\u5217\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4e2d\u7684\u6f5c\u5728\u6df7\u6742\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\uff0c\u5e8f\u5217\u6cbb\u7597\u7684\u7d2f\u79ef\u56e0\u679c\u6548\u5e94\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u7684\u5148\u8fdb\u56e0\u679c\u65b9\u6cd5\u867d\u7136\u80fd\u63a7\u5236\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u56e0\u7d20\uff0c\u4f46\u4ecd\u53d7\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8c03\u6574\u8fd9\u79cd\u6f5c\u5728\u7684\u6df7\u6742\u504f\u5dee\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u540c\u8d1f\u63a7\u5236\u5047\u8bbe\u7684\u65b0\u6846\u67b6DSIV-CFR\uff0c\u5229\u7528\u5de5\u5177\u53d8\u91cf\uff08IV\uff09\u4f5c\u4e3a\u7279\u6b8a\u7684\u8d1f\u63a7\u5236\u66b4\u9732\uff0c\u540c\u65f6\u4ee5\u524d\u671f\u7ed3\u679c\u4f5c\u4e3a\u8d1f\u63a7\u5236\u7ed3\u679c\uff0c\u4ece\u800c\u6062\u590d\u89c2\u5bdf\u53d8\u91cf\u4e2d\u7684IV\uff0c\u5e76\u901a\u8fc7\u5e7f\u4e49\u77e9\u6761\u4ef6\u4f30\u8ba1\u5e8f\u5217\u6cbb\u7597\u6548\u679c\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDSIV-CFR\u5728\u5355\u6b65\u548c\u591a\u6b65\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u652f\u6301\u4e86\u5bf9\u52a8\u6001\u7cfb\u7edf\u4e2d\u6700\u4f18\u6cbb\u7597\u65b9\u6848\u7684\u8bc6\u522b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u89e3\u987a\u5e8f\u5de5\u5177\u53d8\u91cf\u6846\u67b6\uff08DSIV-CFR\uff09\uff0c\u7528\u4e8e\u53cd\u4e8b\u5b9e\u56de\u5f52\uff0c\u4ee5\u89e3\u51b3\u5e8f\u5217\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4e2d\u7684\u6f5c\u5728\u6df7\u6742\u504f\u5dee\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6b65\u548c\u591a\u6b65\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u8bc6\u522b\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u6700\u4f18\u6cbb\u7597\u65b9\u6848\u3002"}}
{"id": "2505.09140", "pdf": "https://arxiv.org/pdf/2505.09140", "abs": "https://arxiv.org/abs/2505.09140", "authors": ["Zechao Guan", "Feng Yan", "Shuai Du", "Lin Ma", "Qingshan Liu"], "title": "TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Diffusion Transformer (DiT) models have significantly\nimproved 3D point cloud generation. However, existing methods primarily focus\non local feature extraction while overlooking global topological information,\nsuch as voids, which are crucial for maintaining shape consistency and\ncapturing complex geometries. To address this limitation, we propose\nTopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure\nfor 3D point cloud generation. Specifically, we design the bottleneck structure\nutilizing Perceiver Resampler, which not only offers a mode to integrate\ntopological information extracted through persistent homology into feature\nlearning, but also adaptively filters out redundant local features to improve\ntraining efficiency. Experimental results demonstrate that TopoDiT-3D\noutperforms state-of-the-art models in visual quality, diversity, and training\nefficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich\ntopological information for 3D point cloud generation and its synergy with\nconventional local feature learning. Videos and code are available at\nhttps://github.com/Zechao-Guan/TopoDiT-3D.", "AI": {"tldr": "TopoDiT-3D is a new method for 3D point cloud generation that incorporates topological information to improve performance.", "motivation": "Existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries.", "method": "TopoDiT-3D is a Topology-Aware Diffusion Transformer with a bottleneck structure utilizing Perceiver Resampler to integrate topological information extracted through persistent homology into feature learning.", "result": "Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency.", "conclusion": "TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning."}}
{"id": "2505.09246", "pdf": "https://arxiv.org/pdf/2505.09246", "abs": "https://arxiv.org/abs/2505.09246", "authors": ["Derian Boer", "Stephen Roth", "Stefan Kramer"], "title": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .", "AI": {"tldr": "FocusedRetriever\u662f\u4e00\u4e2a\u57fa\u4e8e\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u7684\u591a\u8df3\u95ee\u7b54\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5176\u4ed6\u6280\u672f\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u4ea4\u4e92\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u7ed3\u6784\u5316\u77e5\u8bc6\u6216\u975e\u7ed3\u6784\u5316\u5185\u5bb9\uff0c\u800c\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff08SKB\uff09\u80fd\u591f\u5c06\u975e\u7ed3\u6784\u5316\u5185\u5bb9\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u7684\u8282\u70b9\u8fde\u63a5\u8d77\u6765\uff0c\u4ece\u800c\u63d0\u4f9b\u65b0\u7684\u77e5\u8bc6\u8bbf\u95ee\u548c\u4f7f\u7528\u7b56\u7565\u3002", "method": "FocusedRetriever\u662f\u4e00\u4e2a\u57fa\u4e8e\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff08SKB\uff09\u7684\u6846\u67b6\uff0c\u6574\u5408\u4e86\u57fa\u4e8eVSS\u7684\u5b9e\u4f53\u641c\u7d22\u3001\u57fa\u4e8eLLM\u7684Cypher\u67e5\u8be2\u751f\u6210\u4ee5\u53ca\u6210\u5bf9\u91cd\u65b0\u6392\u5e8f\u7ec4\u4ef6\u3002", "result": "FocusedRetriever\u5728\u6240\u6709\u4e09\u4e2aSTaRK\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u9996\u6b21\u547d\u4e2d\u7387\u6bd4\u7b2c\u4e8c\u597d\u7684\u65b9\u6cd5\u9ad8\u51fa25.7%\u3002", "conclusion": "FocusedRetriever\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u80fd\u529b\uff0c\u7ed3\u5408\u8282\u70b9\u96c6\u8fde\u63a5\u3001\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.09131", "pdf": "https://arxiv.org/pdf/2505.09131", "abs": "https://arxiv.org/abs/2505.09131", "authors": ["Kunwoong Kim", "Jihu Lee", "Sangchul Park", "Yongdai Kim"], "title": "Fair Clustering via Alignment", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025. This is the version submitted for review and\n  will be replaced by the camera-ready version soon", "summary": "Algorithmic fairness in clustering aims to balance the proportions of\ninstances assigned to each cluster with respect to a given sensitive attribute.\nWhile recently developed fair clustering algorithms optimize clustering\nobjectives under specific fairness constraints, their inherent complexity or\napproximation often results in suboptimal clustering utility or numerical\ninstability in practice. To resolve these limitations, we propose a new fair\nclustering algorithm based on a novel decomposition of the fair K-means\nclustering objective function. The proposed algorithm, called Fair Clustering\nvia Alignment (FCA), operates by alternately (i) finding a joint probability\ndistribution to align the data from different protected groups, and (ii)\noptimizing cluster centers in the aligned space. A key advantage of FCA is that\nit theoretically guarantees approximately optimal clustering utility for any\ngiven fairness level without complex constraints, thereby enabling high-utility\nfair clustering in practice. Experiments show that FCA outperforms existing\nmethods by (i) attaining a superior trade-off between fairness level and\nclustering utility, and (ii) achieving near-perfect fairness without numerical\ninstability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u805a\u7c7b\u7b97\u6cd5 FCA\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5206\u89e3\u516c\u5e73 K-means \u805a\u7c7b\u76ee\u6807\u51fd\u6570\uff0c\u80fd\u591f\u5728\u4e0d\u4f7f\u7528\u590d\u6742\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u7406\u8bba\u4fdd\u8bc1\u8fd1\u4f3c\u6700\u4f18\u7684\u805a\u7c7b\u6548\u7528\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u516c\u5e73\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u516c\u5e73\u805a\u7c7b\u7b97\u6cd5\u7531\u4e8e\u5176\u56fa\u6709\u7684\u590d\u6742\u6027\u6216\u8fd1\u4f3c\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u805a\u7c7b\u6548\u7528\u4e0d\u4f73\u6216\u6570\u503c\u4e0d\u7a33\u5b9a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u805a\u7c7b\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "FCA \u7b97\u6cd5\u901a\u8fc7\u4ea4\u66ff\u5730 (i) \u627e\u5230\u4e00\u4e2a\u8054\u5408\u6982\u7387\u5206\u5e03\u4ee5\u5bf9\u9f50\u4e0d\u540c\u53d7\u4fdd\u62a4\u7fa4\u4f53\u7684\u6570\u636e\uff0c\u4ee5\u53ca (ii) \u5728\u5bf9\u9f50\u7a7a\u95f4\u4e2d\u4f18\u5316\u805a\u7c7b\u4e2d\u5fc3\u6765\u5b9e\u73b0\u516c\u5e73\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFCA \u5728 (i) \u516c\u5e73\u6c34\u5e73\u548c\u805a\u7c7b\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee5\u53ca (ii) \u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u516c\u5e73\u6027\u800c\u6ca1\u6709\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "FCA \u5728\u5b9e\u8df5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u5b9e\u7528\u6027\u516c\u5e73\u805a\u7c7b\uff0c\u56e0\u4e3a\u5b83\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u5728\u4efb\u4f55\u7ed9\u5b9a\u7684\u516c\u5e73\u6c34\u5e73\u4e0b\u8fd1\u4f3c\u6700\u4f18\u7684\u805a\u7c7b\u6548\u7528\uff0c\u5e76\u4e14\u907f\u514d\u4e86\u590d\u6742\u7684\u7ea6\u675f\u3002"}}
{"id": "2505.09155", "pdf": "https://arxiv.org/pdf/2505.09155", "abs": "https://arxiv.org/abs/2505.09155", "authors": ["Yichen Shi", "Zhuofu Tao", "Yuhao Gao", "Li Huang", "Hongyang Wang", "Zhiping Yu", "Ting-Jung Lin", "Lei He"], "title": "AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection", "categories": ["cs.CV"], "comment": "accepted by LAD25", "summary": "Current multimodal large language models (MLLMs) struggle to understand\ncircuit schematics due to their limited recognition capabilities. This could be\nattributed to the lack of high-quality schematic-netlist training data.\nExisting work such as AMSnet applies schematic parsing to generate netlists.\nHowever, these methods rely on hard-coded heuristics and are difficult to apply\nto complex or noisy schematics in this paper. We therefore propose a novel net\ndetection mechanism based on segmentation with high robustness. The proposed\nmethod also recovers positional information, allowing digital reconstruction of\nschematics. We then expand AMSnet dataset with schematic images from various\nsources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with\nschematic images, Spectre-formatted netlists, OpenAccess digital schematics,\nand positional information for circuit components and nets, whereas AMSnet only\nincludes 792 circuits with SPICE netlists but no digital schematics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u65b0\u578b\u7f51\u7edc\u68c0\u6d4b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7535\u8def\u56fe\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u66f4\u5927\u7684\u6570\u636e\u96c6AMSnet 2.0\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7535\u8def\u56fe\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u7535\u8def\u56fe-\u7f51\u8868\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u786c\u7f16\u7801\u7684\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u590d\u6742\u6216\u566a\u58f0\u8f83\u5927\u7684\u7535\u8def\u56fe\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u65b0\u578b\u7f51\u7edc\u68c0\u6d4b\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7535\u8def\u56fe\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u540c\u65f6\uff0c\u672c\u6587\u6269\u5c55\u4e86AMSnet\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86AMSnet 2.0\uff0c\u5305\u542b\u66f4\u591a\u7535\u8def\u548c\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5206\u5272\u7684\u7f51\u7edc\u68c0\u6d4b\u673a\u5236\u5177\u6709\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6062\u590d\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u7535\u8def\u56fe\u7684\u6570\u5b57\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u672c\u6587\u521b\u5efa\u7684AMSnet 2.0\u6570\u636e\u96c6\u5305\u542b\u66f4\u591a\u7684\u7535\u8def\u548c\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5206\u5272\u7684\u65b0\u578b\u7f51\u7edc\u68c0\u6d4b\u673a\u5236\uff0c\u8be5\u673a\u5236\u5177\u6709\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u6062\u590d\u4e86\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5141\u8bb8\u7535\u8def\u56fe\u7684\u6570\u5b57\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u672c\u6587\u6269\u5c55\u4e86AMSnet\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86AMSnet 2.0\uff0c\u5305\u542b\u66f4\u591a\u7535\u8def\u548c\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002"}}
{"id": "2505.09436", "pdf": "https://arxiv.org/pdf/2505.09436", "abs": "https://arxiv.org/abs/2505.09436", "authors": ["Raghav Garg", "Kapil Sharma", "Karan Gupta"], "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CXMArena\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u5728\u64cd\u4f5c\u6027\u5ba2\u6237\u4f53\u9a8c\u7ba1\u7406\uff08CXM\uff09\u573a\u666f\u4e2d\u7684\u5927\u89c4\u6a21\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u4e94\u4e2a\u5173\u952e\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u590d\u6742\u7684\u89e3\u51b3\u65b9\u6848\u6765\u514b\u670d\u4f20\u7edf\u6280\u672f\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u7f3a\u4e4f\u73b0\u5b9e\u6027\uff0c\u672a\u80fd\u6574\u5408\u6df1\u5ea6\u77e5\u8bc6\u5e93\uff08KB\uff09\u96c6\u6210\u3001\u73b0\u5b9e\u4e16\u754c\u566a\u58f0\u6216\u8d85\u51fa\u5bf9\u8bdd\u6d41\u7545\u6027\u7684\u5173\u952e\u64cd\u4f5c\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u771f\u5b9e\u3001\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86CXMArena\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30AI\u5728\u64cd\u4f5c\u6027CXM\u573a\u666f\u4e2d\u7684\u5927\u89c4\u6a21\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u7684\u6d41\u7a0b\uff0c\u6a21\u62df\u54c1\u724cCXM\u5b9e\u4f53\uff0c\u5982\u77e5\u8bc6\u6587\u7ae0\u3001\u4ea7\u54c1\u89c4\u683c\u3001\u95ee\u9898\u5206\u7c7b\u548c\u5ba2\u670d\u5bf9\u8bdd\u3002\u901a\u8fc7\u53d7\u63a7\u566a\u58f0\u6ce8\u5165\u548c\u4e25\u683c\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\uff0c\u8fd9\u4e9b\u5b9e\u4f53\u7d27\u5bc6\u4ee3\u8868\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u5206\u5e03\u3002", "result": "CXMArena \u63d0\u4f9b\u4e86\u9488\u5bf9\u4e94\u4e2a\u91cd\u8981\u64cd\u4f5c\u4efb\u52a1\u7684\u4e13\u7528\u57fa\u51c6\uff1a\u77e5\u8bc6\u5e93\u4f18\u5316\u3001\u610f\u56fe\u9884\u6d4b\u3001\u4ee3\u7406\u8d28\u91cf\u9075\u5b88\u3001\u6587\u7ae0\u641c\u7d22\u4ee5\u53ca\u96c6\u6210\u5de5\u5177\u7684\u591a\u8f6eRAG\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u5d4c\u5165\u548c\u751f\u6210\u6a21\u578b\u5728\u6587\u7ae0\u641c\u7d22\u4efb\u52a1\u4e2d\u4e5f\u53ea\u8fbe\u5230\u4e8668%\u7684\u51c6\u786e\u7387\uff0c\u800c\u6807\u51c6\u5d4c\u5165\u65b9\u6cd5\u5728\u77e5\u8bc6\u5e93\u4f18\u5316\u4efb\u52a1\u4e2d\u7684F1\u5206\u6570\u4ec5\u4e3a0.3\uff0c\u8fd9\u8868\u660e\u5f53\u524d\u6a21\u578b\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u590d\u6742\u7684\u7ba1\u9053\u548c\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "CXMArena \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u5728\u64cd\u4f5c\u6027\u5ba2\u6237\u4f53\u9a8c\u7ba1\u7406\uff08CXM\uff09\u573a\u666f\u4e2d\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5b83\u63d0\u4f9b\u4e86\u9488\u5bf9\u4e94\u4e2a\u91cd\u8981\u64cd\u4f5c\u4efb\u52a1\u7684\u4e13\u7528\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u5d4c\u5165\u548c\u751f\u6210\u6a21\u578b\u5728\u6587\u7ae0\u641c\u7d22\u4efb\u52a1\u4e2d\u4e5f\u53ea\u8fbe\u5230\u4e8668%\u7684\u51c6\u786e\u7387\uff0c\u800c\u6807\u51c6\u5d4c\u5165\u65b9\u6cd5\u5728\u77e5\u8bc6\u5e93\u4f18\u5316\u4efb\u52a1\u4e2d\u7684F1\u5206\u6570\u4ec5\u4e3a0.3\uff0c\u8fd9\u7a81\u663e\u4e86\u5f53\u524d\u6a21\u578b\u9762\u4e34\u7684\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u590d\u6742\u7684\u7ba1\u9053\u548c\u89e3\u51b3\u65b9\u6848\u6765\u514b\u670d\u4f20\u7edf\u6280\u672f\u7684\u9650\u5236\u3002"}}
{"id": "2505.08818", "pdf": "https://arxiv.org/pdf/2505.08818", "abs": "https://arxiv.org/abs/2505.08818", "authors": ["Amara Tariq", "Rimita Lahiri", "Charles Kahn", "Imon Banerjee"], "title": "Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "15 pages, 2, tables, 3 figures", "summary": "The intricate and multifaceted nature of vision language model (VLM)\ndevelopment, adaptation, and application necessitates the establishment of\nclear and standardized reporting protocols, particularly within the high-stakes\ncontext of healthcare. Defining these reporting standards is inherently\nchallenging due to the diverse nature of studies involving VLMs, which vary\nsignificantly from the development of all new VLMs or finetuning for domain\nalignment to off-the-shelf use of VLM for targeted diagnosis and prediction\ntasks. In this position paper, we argue that traditional machine learning\nreporting standards and evaluation guidelines must be restructured to\naccommodate multiphase VLM studies; it also has to be organized for intuitive\nunderstanding of developers while maintaining rigorous standards for\nreproducibility. To facilitate community adoption, we propose a categorization\nframework for VLM studies and outline corresponding reporting standards that\ncomprehensively address performance evaluation, data reporting protocols, and\nrecommendations for manuscript composition. These guidelines are organized\naccording to the proposed categorization scheme. Lastly, we present a checklist\nthat consolidates reporting standards, offering a standardized tool to ensure\nconsistency and quality in the publication of VLM-related research.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86VLM\u7814\u7a76\u7684\u591a\u6837\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6846\u67b6\u548c\u62a5\u544a\u6807\u51c6\uff0c\u4ee5\u63d0\u9ad8VLM\u76f8\u5173\u7814\u7a76\u7684\u51fa\u7248\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u7531\u4e8eVLM\u7814\u7a76\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u62a5\u544a\u6807\u51c6\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u6784\u5efa\u8fd9\u4e9b\u6807\u51c6\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u9002\u5e94VLM\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790VLM\u7814\u7a76\u7684\u591a\u6837\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6839\u636e\u8be5\u6846\u67b6\u5236\u5b9a\u4e86\u5168\u9762\u7684\u62a5\u544a\u6807\u51c6\uff0c\u5305\u62ec\u6027\u80fd\u8bc4\u4f30\u3001\u6570\u636e\u62a5\u544a\u534f\u8bae\u548c\u624b\u7a3f\u64b0\u5199\u5efa\u8bae\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u68c0\u67e5\u6e05\u5355\uff0c\u4ee5\u786e\u4fdd\u6807\u51c6\u5316\u7684\u62a5\u544a\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aVLM\u7814\u7a76\u7684\u5206\u7c7b\u6846\u67b6\u548c\u76f8\u5e94\u7684\u62a5\u544a\u6807\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u68c0\u67e5\u6e05\u5355\uff0c\u4ee5\u786e\u4fddVLM\u76f8\u5173\u7814\u7a76\u7684\u51fa\u7248\u4e00\u81f4\u6027\u4e0e\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u4e3b\u5f20\u9700\u8981\u91cd\u65b0\u6784\u5efa\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u62a5\u544a\u6807\u51c6\u548c\u8bc4\u4f30\u6307\u5357\uff0c\u4ee5\u9002\u5e94\u591a\u9636\u6bb5\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\u548c\u76f8\u5e94\u7684\u62a5\u544a\u6807\u51c6\uff0c\u4ee5\u786e\u4fddVLM\u76f8\u5173\u7814\u7a76\u7684\u51fa\u7248\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2505.09134", "pdf": "https://arxiv.org/pdf/2505.09134", "abs": "https://arxiv.org/abs/2505.09134", "authors": ["Daniel Huang"], "title": "Scaling Gaussian Process Regression with Full Derivative Observations", "categories": ["cs.LG", "stat.ML"], "comment": "12 pages", "summary": "We present a scalable Gaussian Process (GP) method that can fit and predict\nfull derivative observations called DSoftKI. It extends SoftKI, a method that\napproximates a kernel via softmax interpolation from learned interpolation\npoint locations, to the setting with derivatives. DSoftKI enhances SoftKI's\ninterpolation scheme to incorporate the directional orientation of\ninterpolation points relative to the data. This enables the construction of a\nscalable approximate kernel, including its first and second-order derivatives,\nthrough interpolation. We evaluate DSoftKI on a synthetic function benchmark\nand high-dimensional molecular force field prediction (100-1000 dimensions),\ndemonstrating that DSoftKI is accurate and can scale to larger datasets with\nfull derivative observations than previously possible.", "AI": {"tldr": "DSoftKI is a scalable Gaussian Process method that can fit and predict full derivative observations, extending SoftKI to the setting with derivatives and demonstrating accuracy and scalability on various benchmarks.", "motivation": "The paper aims to present a scalable Gaussian Process (GP) method that can fit and predict full derivative observations, which is more accurate and can scale to larger datasets with full derivative observations than previously possible.", "method": "DSoftKI extends SoftKI, a method that approximates a kernel via softmax interpolation from learned interpolation point locations, to the setting with derivatives. It enhances SoftKI's interpolation scheme to incorporate the directional orientation of interpolation points relative to the data, enabling the construction of a scalable approximate kernel, including its first and second-order derivatives, through interpolation.", "result": "DSoftKI was evaluated on a synthetic function benchmark and high-dimensional molecular force field prediction (100-1000 dimensions), demonstrating its accuracy and scalability.", "conclusion": "DSoftKI is accurate and can scale to larger datasets with full derivative observations than previously possible."}}
{"id": "2505.09168", "pdf": "https://arxiv.org/pdf/2505.09168", "abs": "https://arxiv.org/abs/2505.09168", "authors": ["Jianlin Sun", "Xiaolin Fang", "Juwei Guan", "Dongdong Gui", "Teqi Wang", "Tongxin Zhu"], "title": "DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The core challenge in Camouflage Object Detection (COD) lies in the\nindistinguishable similarity between targets and backgrounds in terms of color,\ntexture, and shape. This causes existing methods to either lose edge details\n(such as hair-like fine structures) due to over-reliance on global semantic\ninformation or be disturbed by similar backgrounds (such as vegetation\npatterns) when relying solely on local features. We propose DRRNet, a\nfour-stage architecture characterized by a \"context-detail-fusion-refinement\"\npipeline to address these issues. Specifically, we introduce an Omni-Context\nFeature Extraction Module to capture global camouflage patterns and a Local\nDetail Extraction Module to supplement microstructural information for the\nfull-scene context module. We then design a module for forming dual\nrepresentations of scene understanding and structural awareness, which fuses\npanoramic features and local features across various scales. In the decoder, we\nalso introduce a reverse refinement module that leverages spatial edge priors\nand frequency-domain noise suppression to perform a two-stage inverse\nrefinement of the output. By applying two successive rounds of inverse\nrefinement, the model effectively suppresses background interference and\nenhances the continuity of object boundaries. Experimental results demonstrate\nthat DRRNet significantly outperforms state-of-the-art methods on benchmark\ndatasets. Our code is available at https://github.com/jerrySunning/DRRNet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DRRNet \u7684\u56db\u9636\u6bb5\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u5ea6\u4f9d\u8d56\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u800c\u4e22\u5931\u8fb9\u7f18\u7ec6\u8282\uff08\u5982\u53d1\u4e1d\u72b6\u7ec6\u7ed3\u6784\uff09\uff0c\u8981\u4e48\u4ec5\u4f9d\u8d56\u5c40\u90e8\u7279\u5f81\u65f6\u53d7\u5230\u76f8\u4f3c\u80cc\u666f\u7684\u5e72\u6270\uff08\u5982\u690d\u88ab\u56fe\u6848\uff09\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 DRRNet\uff0c\u8fd9\u662f\u4e00\u79cd\u56db\u9636\u6bb5\u67b6\u6784\uff0c\u5177\u6709\u201c\u4e0a\u4e0b\u6587-\u7ec6\u8282-\u878d\u5408-\u7cbe\u70bc\u201d\u6d41\u7a0b\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a Omni-Context \u7279\u5f81\u63d0\u53d6\u6a21\u5757\u6765\u6355\u6349\u5168\u5c40\u4f2a\u88c5\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u6765\u5f62\u6210\u573a\u666f\u7406\u89e3\u548c\u7ed3\u6784\u611f\u77e5\u7684\u53cc\u91cd\u8868\u793a\uff0c\u4ee5\u53ca\u4e00\u4e2a\u53cd\u5411\u7cbe\u70bc\u6a21\u5757\u6765\u6267\u884c\u4e24\u9636\u6bb5\u7684\u9006\u7cbe\u70bc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDRRNet \u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "DRRNet \u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.09610", "pdf": "https://arxiv.org/pdf/2505.09610", "abs": "https://arxiv.org/abs/2505.09610", "authors": ["Nicolas Dupuis", "Ravi Nair", "Shyam Ramji", "Sean McClintock", "Nishant Chauhan", "Priyanka Nagpal", "Bart Blaner", "Ken Valk", "Leon Stok", "Ruchir Puri"], "title": "Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3aVHDL\u4ee3\u7801\u89e3\u91ca\u5f00\u53d1LLM\u7684\u8fc7\u7a0b\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u6269\u5c55\u9884\u8bad\u7ec3\u548c\u4e13\u5bb6\u8bc4\u4f30\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5229\u7528\u751f\u6210\u5f0fAI\u6539\u8fdb\u786c\u4ef6\u8bbe\u8ba1LLM\u7684\u5efa\u8bae\u3002", "motivation": "\u5728\u9ad8\u6027\u80fd\u5904\u7406\u5668\u8bbe\u8ba1\u9886\u57df\uff0cLLM\u5728VHDL\u4ee3\u7801\u89e3\u91ca\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u800cVHDL\u5728\u5de5\u4e1a\u754c\u4ecd\u7136\u5f88\u53d7\u6b22\u8fce\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u9488\u5bf9\u7279\u5b9a\u9700\u6c42\u7684\u6d4b\u8bd5\u96c6\uff0c\u5e76\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\uff0c\u8fdb\u884c\u4e86\u6269\u5c55\u9884\u8bad\u7ec3\uff08EPT\uff09\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2aLLM-as-a-judge\u6765\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u7ecf\u8fc7EPT\u6a21\u578b\u7684\u4e13\u5bb6\u8bc4\u4f30\u5f97\u5206\u4ece43%\u63d0\u9ad8\u5230\u4e8669%\uff0c\u5e76\u4e14\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u7684EPT\u6a21\u578b\u9884\u8ba1\u4e13\u5bb6\u8bc4\u4f30\u5f97\u5206\u53ef\u8fbe71%\u3002\u4f7f\u7528\u66f4\u65b0\u7684\u57fa\u6a21\u578b\u53ef\u4ee5\u5c06\u5f97\u5206\u63d0\u5347\u523085%\u4ee5\u4e0a\u3002", "conclusion": "\u6211\u4eec\u8ba8\u8bba\u4e86\u5982\u4f55\u8fdb\u4e00\u6b65\u5229\u7528\u751f\u6210\u5f0fAI\u9886\u57df\u7684\u6700\u65b0\u53d1\u5c55\u6765\u63d0\u9ad8\u786c\u4ef6\u8bbe\u8ba1LLM\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.08821", "pdf": "https://arxiv.org/pdf/2505.08821", "abs": "https://arxiv.org/abs/2505.08821", "authors": ["Meryem Altin Karagoz", "Marc D. Breton", "Anas El Fathi"], "title": "A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction", "categories": ["q-bio.QM", "cs.AI", "stat.AP"], "comment": "7 pages, 2 figures, 1 table, 1st IFAC Workshop on Engineering\n  Diabetes Technologies (EDT 2025)", "summary": "Accurate blood glucose prediction can enable novel interventions for type 1\ndiabetes treatment, including personalized insulin and dietary adjustments.\nAlthough recent advances in transformer-based architectures have demonstrated\nthe power of attention mechanisms in complex multivariate time series\nprediction, their potential for blood glucose (BG) prediction remains\nunderexplored. We present a comparative analysis of transformer models for\nmulti-horizon BG prediction, examining forecasts up to 4 hours and input\nhistory up to 1 week. The publicly available DCLP3 dataset (n=112) was split\n(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset\n(n=12) served as an external test set. We trained networks with point-wise,\npatch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal\ndata. For short-term blood glucose prediction, Crossformer, a patch-wise\ntransformer architecture, achieved a superior 30-minute prediction of RMSE\n(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),\nPatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6\nmg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used\ntokenization through patches demonstrated improved accuracy with larger input\nsizes, with the best results obtained with a one-week history. These findings\nhighlight the promise of transformer-based architectures for BG prediction by\ncapturing and leveraging seasonal patterns in multivariate time-series data to\nimprove accuracy.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u53d8\u538b\u5668\u6a21\u578b\u5728\u591a\u65f6\u95f4\u8303\u56f4\u8840\u7cd6\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5757\u5f0f\u53d8\u538b\u5668\u5728\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u4e00\u5468\u7684\u5386\u53f2\u6570\u636e\u65f6\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u51c6\u786e\u7684\u8840\u7cd6\u9884\u6d4b\u53ef\u4ee5\u4e3a1\u578b\u7cd6\u5c3f\u75c5\u6cbb\u7597\u63d0\u4f9b\u65b0\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u5305\u62ec\u4e2a\u6027\u5316\u7684\u80f0\u5c9b\u7d20\u548c\u996e\u98df\u8c03\u6574\u3002\u5c3d\u7ba1\u6700\u8fd1\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u67b6\u6784\u5728\u590d\u6742\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5c55\u793a\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u529b\u91cf\uff0c\u4f46\u5b83\u4eec\u5728\u8840\u7cd6\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u53d8\u538b\u5668\u6a21\u578b\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u8fdb\u884c\u591a\u65f6\u95f4\u8303\u56f4\u7684\u8840\u7cd6\u9884\u6d4b\uff0c\u68c0\u67e5\u6700\u591a4\u5c0f\u65f6\u7684\u9884\u6d4b\u548c\u6700\u591a1\u5468\u7684\u8f93\u5165\u5386\u53f2\u3002\u6211\u4eec\u4f7f\u7528\u4e86CGM\u3001\u80f0\u5c9b\u7d20\u548c\u9910\u98df\u6570\u636e\u8bad\u7ec3\u4e86\u5177\u6709\u70b9\u5f0f\u3001\u5757\u5f0f\u3001\u5e8f\u5217\u5f0f\u548c\u6df7\u5408\u5d4c\u5165\u7684\u7f51\u7edc\u3002", "result": "\u5bf9\u4e8e\u77ed\u671f\u8840\u7cd6\u9884\u6d4b\uff0cCrossformer\uff08\u4e00\u79cd\u5757\u5f0f\u53d8\u538b\u5668\u67b6\u6784\uff09\u5728OhioT1DM\u4e0a\u5b9e\u73b0\u4e86RMSE\u4e3a15.6 mg/dL\u7684\u4f18\u8d8a30\u5206\u949f\u9884\u6d4b\u3002\u5bf9\u4e8e\u66f4\u957f\u671f\u7684\u9884\u6d4b\uff081\u5c0f\u65f6\u30012\u5c0f\u65f6\u548c4\u5c0f\u65f6\uff09\uff0cPatchTST\uff08\u53e6\u4e00\u79cd\u5757\u5f0f\u53d8\u538b\u5668\uff09\u5728OhioT1DM\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684RMSE\uff0824.6 mg/dL\u300136.1 mg/dL\u548c46.5 mg/dL\uff09\u3002\u4e00\u822c\u6765\u8bf4\uff0c\u901a\u8fc7\u5757\u8fdb\u884c\u6807\u8bb0\u5316\u7684\u6a21\u578b\u5728\u8f93\u5165\u89c4\u6a21\u8f83\u5927\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u6700\u4f73\u7ed3\u679c\u662f\u5728\u4e00\u5468\u7684\u5386\u53f2\u6570\u636e\u4e0b\u83b7\u5f97\u7684\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u67b6\u6784\u5728\u8840\u7cd6\u9884\u6d4b\u4e2d\u5177\u6709\u524d\u666f\uff0c\u901a\u8fc7\u6355\u6349\u548c\u5229\u7528\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5b63\u8282\u6027\u6a21\u5f0f\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2505.09160", "pdf": "https://arxiv.org/pdf/2505.09160", "abs": "https://arxiv.org/abs/2505.09160", "authors": ["Berkay Guler", "Giovanni Geraci", "Hamid Jafarkhani"], "title": "A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Current applications of self-supervised learning to wireless channel\nrepresentation often borrow paradigms developed for text and image processing,\nwithout fully addressing the unique characteristics and constraints of wireless\ncommunications. Aiming to fill this gap, we first propose WiMAE (Wireless\nMasked Autoencoder), a transformer-based encoder-decoder foundation model\npretrained on a realistic open-source multi-antenna wireless channel dataset.\nBuilding upon this foundation, we develop ContraWiMAE, which enhances WiMAE by\nincorporating a contrastive learning objective alongside the reconstruction\ntask in a unified multi-task framework. By warm-starting from pretrained WiMAE\nweights and generating positive pairs via noise injection, the contrastive\ncomponent enables the model to capture both structural and discriminative\nfeatures, enhancing representation quality beyond what reconstruction alone can\nachieve. Through extensive evaluation on unseen scenarios, we demonstrate the\neffectiveness of both approaches across multiple downstream tasks, with\nContraWiMAE showing further improvements in linear separability and\nadaptability in diverse wireless environments. Comparative evaluations against\na state-of-the-art wireless channel foundation model confirm the superior\nperformance and data efficiency of our models, highlighting their potential as\npowerful baselines for future research in self-supervised wireless channel\nrepresentation learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u7ebf\u4fe1\u9053\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u5f53\u524d\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u5e94\u7528\u4e8e\u65e0\u7ebf\u4fe1\u9053\u8868\u793a\u7684\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u501f\u9274\u4e86\u4e3a\u6587\u672c\u548c\u56fe\u50cf\u5904\u7406\u5f00\u53d1\u7684\u8303\u5f0f\uff0c\u4f46\u6ca1\u6709\u5145\u5206\u8003\u8651\u65e0\u7ebf\u901a\u4fe1\u7684\u72ec\u7279\u7279\u6027\u548c\u7ea6\u675f\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86WiMAE\uff08Wireless Masked Autoencoder\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u57fa\u7840\u6a21\u578b\uff0c\u5728\u73b0\u5b9e\u7684\u5f00\u6e90\u591a\u5929\u7ebf\u65e0\u7ebf\u4fe1\u9053\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86ContraWiMAE\uff0c\u5b83\u901a\u8fc7\u5728\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u6846\u67b6\u4e2d\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u548c\u91cd\u5efa\u4efb\u52a1\u6765\u589e\u5f3aWiMAE\u3002", "result": "\u901a\u8fc7\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e24\u79cd\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5176\u4e2dContraWiMAE\u5728\u7ebf\u6027\u53ef\u5206\u6027\u548c\u5728\u4e0d\u540c\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8fdb\u4e00\u6b65\u7684\u6539\u8fdb\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u65e0\u7ebf\u4fe1\u9053\u57fa\u7840\u6a21\u578b\u7684\u6bd4\u8f83\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6211\u4eec\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u81ea\u76d1\u7763\u65e0\u7ebf\u4fe1\u9053\u8868\u793a\u5b66\u4e60\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff0c\u5177\u6709\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u5f3a\u5927\u57fa\u7ebf\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09178", "pdf": "https://arxiv.org/pdf/2505.09178", "abs": "https://arxiv.org/abs/2505.09178", "authors": ["Yitao Zhu", "Yuan Yin", "Zhenrong Shen", "Zihao Zhao", "Haiyu Song", "Sheng Wang", "Dinggang Shen", "Qian Wang"], "title": "UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System", "categories": ["cs.CV"], "comment": "14 pages", "summary": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.", "AI": {"tldr": "UniCAD \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u67b6\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u6765\u5904\u7406 2D \u548c 3D \u533b\u7597\u56fe\u50cf\uff0c\u540c\u65f6\u4ec5\u9700\u8981\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u3002\u5b83\u5f15\u5165\u4e86\u6548\u7387\u548c\u5373\u63d2\u5373\u7528\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff0c\u5e76\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u548c\u90e8\u7f72\u591a\u4efb\u52a1\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad (CAD) \u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\u548c\u8d44\u6e90\u5bc6\u96c6\u578b\uff0c\u540c\u65f6\u533b\u7597\u5f71\u50cf\u793e\u533a\u7f3a\u4e4f\u4e00\u4e2a\u5f00\u6e90 CAD \u5e73\u53f0\u6765\u4fc3\u8fdb\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u8bca\u65ad\u6a21\u578b\u7684\u5feb\u901f\u521b\u5efa\u3002", "method": "UniCAD \u5f15\u5165\u4e86\u4e24\u79cd\u5173\u952e\u521b\u65b0\uff1a(1) \u6548\u7387\uff1a\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u9002\u5e94\u5230\u533b\u5b66\u56fe\u50cf\u9886\u57df\uff1b(2) \u5373\u63d2\u5373\u7528\uff1a\u4e00\u4e2a\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7ed3\u5408\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\u548c\u591a\u4e2a\u5373\u63d2\u5373\u7528\u4e13\u5bb6\uff0c\u4ee5\u5b9e\u73b0\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u65e0\u7f1d\u529f\u80fd\u6269\u5c55\u3002", "result": "\u5728 12 \u4e2a\u4e0d\u540c\u7684\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0cUniCAD \u5728\u51c6\u786e\u6027\u548c\u90e8\u7f72\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UniCAD \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u67b6\u6784\uff0c\u80fd\u591f\u5904\u7406 2D \u548c 3D \u533b\u7597\u56fe\u50cf\uff0c\u5e76\u4e14\u5728\u51c6\u786e\u6027\u548c\u90e8\u7f72\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.09174", "pdf": "https://arxiv.org/pdf/2505.09174", "abs": "https://arxiv.org/abs/2505.09174", "authors": ["Xinyu You", "Xiang Liu", "Chuan-Shen Hu", "Kelin Xia", "Tze Chien Sum"], "title": "Quotient Complex Transformer (QCformer) for Perovskite Data Analysis", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": null, "summary": "The discovery of novel functional materials is crucial in addressing the\nchallenges of sustainable energy generation and climate change. Hybrid\norganic-inorganic perovskites (HOIPs) have gained attention for their\nexceptional optoelectronic properties in photovoltaics. Recently, geometric\ndeep learning, particularly graph neural networks (GNNs), has shown strong\npotential in predicting material properties and guiding material design.\nHowever, traditional GNNs often struggle to capture the periodic structures and\nhigher-order interactions prevalent in such systems. To address these\nlimitations, we propose a novel representation based on quotient complexes\n(QCs) and introduce the Quotient Complex Transformer (QCformer) for material\nproperty prediction. A material structure is modeled as a quotient complex,\nwhich encodes both pairwise and many-body interactions via simplices of varying\ndimensions and captures material periodicity through a quotient operation. Our\nmodel leverages higher-order features defined on simplices and processes them\nusing a simplex-based Transformer module. We pretrain QCformer on benchmark\ndatasets such as the Materials Project and JARVIS, and fine-tune it on HOIP\ndatasets. The results show that QCformer outperforms state-of-the-art models in\nHOIP property prediction, demonstrating its effectiveness. The quotient complex\nrepresentation and QCformer model together contribute a powerful new tool for\npredictive modeling of perovskite materials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5546\u590d\u5f62\u7684\u65b0\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86QCformer\u6a21\u578b\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u9499\u949b\u77ff\u6750\u6599\u7684\u5c5e\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u96be\u4ee5\u6355\u6349\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u7684\u5468\u671f\u7ed3\u6784\u548c\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8868\u793a\u65b9\u6cd5\u548c\u6a21\u578b\u6765\u63d0\u9ad8\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5546\u590d\u5f62\uff08QC\uff09\u7684\u65b0\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86Quotient Complex Transformer\uff08QCformer\uff09\u6a21\u578b\u3002\u6750\u6599\u7ed3\u6784\u88ab\u5efa\u6a21\u4e3a\u5546\u590d\u5f62\uff0c\u901a\u8fc7\u4e0d\u540c\u7ef4\u5ea6\u7684\u5355\u5f62\u6765\u7f16\u7801\u6210\u5bf9\u548c\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u5546\u64cd\u4f5c\u6355\u6349\u6750\u6599\u5468\u671f\u6027\u3002\u6a21\u578b\u5229\u7528\u5b9a\u4e49\u5728\u5355\u5f62\u4e0a\u7684\u9ad8\u9636\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5355\u5f62\u7684Transformer\u6a21\u5757\u8fdb\u884c\u5904\u7406\u3002", "result": "QCformer\u5728HOIP\u5c5e\u6027\u9884\u6d4b\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u5546\u590d\u5f62\u8868\u793a\u548cQCformer\u6a21\u578b\u5171\u540c\u4e3a\u9499\u949b\u77ff\u6750\u6599\u7684\u9884\u6d4b\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65b0\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5546\u590d\u5f62\uff08QC\uff09\u7684\u65b0\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86Quotient Complex Transformer\uff08QCformer\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728HOIP\u5c5e\u6027\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002\u5546\u590d\u5f62\u8868\u793a\u548cQCformer\u6a21\u578b\u5171\u540c\u4e3a\u9499\u949b\u77ff\u6750\u6599\u7684\u9884\u6d4b\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.09188", "pdf": "https://arxiv.org/pdf/2505.09188", "abs": "https://arxiv.org/abs/2505.09188", "authors": ["Minjun Kim", "Jaehyeon Choi", "Jongkeun Lee", "Wonjin Cho", "U Kang"], "title": "Zero-shot Quantization: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "IJCAI 2025 Survey Track", "summary": "Network quantization has proven to be a powerful approach to reduce the\nmemory and computational demands of deep learning models for deployment on\nresource-constrained devices. However, traditional quantization methods often\nrely on access to training data, which is impractical in many real-world\nscenarios due to privacy, security, or regulatory constraints. Zero-shot\nQuantization (ZSQ) emerges as a promising solution, achieving quantization\nwithout requiring any real data. In this paper, we provide a comprehensive\noverview of ZSQ methods and their recent advancements. First, we provide a\nformal definition of the ZSQ problem and highlight the key challenges. Then, we\ncategorize the existing ZSQ methods into classes based on data generation\nstrategies, and analyze their motivations, core ideas, and key takeaways.\nLastly, we suggest future research directions to address the remaining\nlimitations and advance the field of ZSQ. To the best of our knowledge, this\npaper is the first in-depth survey on ZSQ.", "AI": {"tldr": "\u672c\u6587\u662f\u5bf9\u96f6\u6837\u672c\u91cf\u5316\uff08ZSQ\uff09\u65b9\u6cd5\u53ca\u5176\u6700\u65b0\u8fdb\u5c55\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u5176\u5b9a\u4e49\u3001\u6311\u6218\u3001\u5206\u7c7b\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u8bbf\u95ee\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u7531\u4e8e\u9690\u79c1\u3001\u5b89\u5168\u6216\u76d1\u7ba1\u9650\u5236\u800c\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u96f6\u6837\u672c\u91cf\u5316\uff08ZSQ\uff09\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u4efb\u4f55\u771f\u5b9e\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u91cf\u5316\u7684\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u88ab\u63d0\u51fa\u3002", "method": "\u672c\u6587\u9996\u5148\u5bf9ZSQ\u95ee\u9898\u8fdb\u884c\u4e86\u6b63\u5f0f\u5b9a\u4e49\uff0c\u7136\u540e\u6839\u636e\u6570\u636e\u751f\u6210\u7b56\u7565\u5bf9\u73b0\u6709\u7684ZSQ\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u52a8\u673a\u3001\u6838\u5fc3\u601d\u60f3\u548c\u5173\u952e\u89c1\u89e3\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9ZSQ\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u89e3\u51b3\u5269\u4f59\u7684\u5c40\u9650\u6027\u5e76\u63a8\u52a8ZSQ\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u96f6\u6837\u672c\u91cf\u5316\uff08ZSQ\uff09\u65b9\u6cd5\u53ca\u5176\u6700\u65b0\u8fdb\u5c55\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u4ee5\u89e3\u51b3\u5269\u4f59\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8ZSQ\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.08825", "pdf": "https://arxiv.org/pdf/2505.08825", "abs": "https://arxiv.org/abs/2505.08825", "authors": ["Pedro Antonio Alarcon Granadeno", "Theodore Chambers", "Jane Cleland-Huang"], "title": "Multi-source Plume Tracing via Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI"], "comment": "13 pages, 7 figures", "summary": "Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon\ngas leak (2015) demonstrate the urgent need for rapid and reliable plume\ntracing algorithms to protect public health and the environment. Traditional\nmethods, such as gradient-based or biologically inspired approaches, often fail\nin realistic, turbulent conditions. To address these challenges, we present a\nMulti-Agent Reinforcement Learning (MARL) algorithm designed for localizing\nmultiple airborne pollution sources using a swarm of small uncrewed aerial\nsystems (sUAS). Our method models the problem as a Partially Observable Markov\nGame (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific\nDouble Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical\naction-observation pairs, effectively approximating latent states. Unlike prior\nwork, we use a general-purpose simulation environment based on the Gaussian\nPlume Model (GPM), incorporating realistic elements such as a three-dimensional\nenvironment, sensor noise, multiple interacting agents, and multiple plume\nsources. The incorporation of action histories as part of the inputs further\nenhances the adaptability of our model in complex, partially observable\nenvironments. Extensive simulations show that our algorithm significantly\noutperforms conventional approaches. Specifically, our model allows agents to\nexplore only 1.29\\% of the environment to successfully locate pollution\nsources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f7f\u7528\u5c0f\u578b\u65e0\u4eba\u822a\u7a7a\u7cfb\u7edf\u5b9a\u4f4d\u591a\u4e2a\u7a7a\u6c14\u6c61\u67d3\u6e90\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5de5\u4e1a\u707e\u96be\u8868\u660e\uff0c\u9700\u8981\u5feb\u901f\u53ef\u9760\u7684\u70df\u7fbd\u8ffd\u8e2a\u7b97\u6cd5\u6765\u4fdd\u62a4\u516c\u5171\u5065\u5eb7\u548c\u73af\u5883\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u73b0\u5b9e\u7684\u6e4d\u6d41\u6761\u4ef6\u4e0b\u5f80\u5f80\u5931\u8d25\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f7f\u7528\u5c0f\u578b\u65e0\u4eba\u822a\u7a7a\u7cfb\u7edf\uff08sUAS\uff09\u5b9a\u4f4d\u591a\u4e2a\u7a7a\u6c14\u6c61\u67d3\u6e90\u3002\u8be5\u65b9\u6cd5\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff08POMG\uff09\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7684\u7279\u5b9a\u52a8\u4f5c\u53cc\u6df1\u5ea6\u9012\u5f52Q\u7f51\u7edc\uff08ADDRQN\uff09\u3002", "result": "\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u5e7f\u6cdb\u7684\u6a21\u62df\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5141\u8bb8\u4ee3\u7406\u4ec5\u63a2\u7d221.29%\u7684\u73af\u5883\u5373\u53ef\u6210\u529f\u5b9a\u4f4d\u6c61\u67d3\u6e90\u3002", "conclusion": "\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u590d\u6742\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b9a\u4f4d\u6c61\u67d3\u6e90\u3002"}}
{"id": "2505.09175", "pdf": "https://arxiv.org/pdf/2505.09175", "abs": "https://arxiv.org/abs/2505.09175", "authors": ["Mohammad Ganjirad", "Mahmoud Reza Delavar", "Hossein Bagheri", "Mohammad Mehdi Azizi"], "title": "Optimizing Urban Critical Green Space Development Using Machine Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper presents a novel framework for prioritizing urban green space\ndevelopment in Tehran using diverse socio-economic, environmental, and\nsensitivity indices. The indices were derived from various sources including\nGoogle Earth Engine, air pollution measurements, municipal reports and the\nWeather Research & Forecasting (WRF) model. The WRF model was used to estimate\nthe air temperature at a 1 km resolution due to insufficient meteorological\nstations, yielding RMSE and MAE values of 0.96{\\deg}C and 0.92{\\deg}C,\nrespectively. After data preparation, several machine learning models were used\nfor binary vegetation cover classification including XGBoost, LightGBM, Random\nForest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%\nin Overall Accuracy, Recall, and F1-score. Then, the probability of areas\nlacking vegetation cover was assessed using socio-economic, environmental and\nsensitivity indices. This resulted in the RF generating an urban green space\ndevelopment prioritization map. Feature Importance Analysis revealed that the\nmost significant indices were nightly land surface temperature (LST) and\nsensitive population. Finally, the framework performance was validated through\nmicroclimate simulation to assess the critical areas after and before the green\nspace development by green roofs. The simulation demonstrated reducing air\ntemperature by up to 0.67{\\deg}C after utilizing the green roof technology in\ncritical areas. As a result, this framework provides a valuable tool for urban\nplanners to develop green spaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5fb7\u9ed1\u5170\u4f18\u5148\u53d1\u5c55\u57ce\u5e02\u7eff\u8272\u7a7a\u95f4\uff0c\u5229\u7528\u591a\u79cd\u6570\u636e\u6e90\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u6709\u6548\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u7531\u4e8e\u6c14\u8c61\u7ad9\u4e0d\u8db3\uff0c\u9700\u8981\u4f30\u8ba11\u516c\u91cc\u5206\u8fa8\u7387\u7684\u6c14\u6e29\uff0c\u5e76\u4e14\u9700\u8981\u786e\u5b9a\u54ea\u4e9b\u533a\u57df\u7f3a\u4e4f\u690d\u88ab\u8986\u76d6\uff0c\u4ee5\u4fbf\u4f18\u5148\u53d1\u5c55\u57ce\u5e02\u7eff\u8272\u7a7a\u95f4\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u793e\u4f1a\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u654f\u611f\u6027\u6307\u6570\uff0c\u7ed3\u5408Google Earth Engine\u3001\u7a7a\u6c14\u6c61\u67d3\u6d4b\u91cf\u3001\u5e02\u653f\u62a5\u544a\u548cWRF\u6a21\u578b\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\uff09\u8fdb\u884c\u4e8c\u5143\u690d\u88ab\u8986\u76d6\u5206\u7c7b\uff0c\u5e76\u751f\u6210\u4f18\u5148\u53d1\u5c55\u57ce\u5e02\u7eff\u8272\u7a7a\u95f4\u7684\u5730\u56fe\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u603b\u4f53\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u5747\u8d85\u8fc794%\u3002\u5fae\u6c14\u5019\u6a21\u62df\u663e\u793a\uff0c\u5728\u5173\u952e\u533a\u57df\u4f7f\u7528\u7eff\u8272\u5c4b\u9876\u6280\u672f\u540e\uff0c\u7a7a\u6c14\u6e29\u5ea6\u53ef\u964d\u4f4e\u9ad8\u8fbe0.67\u00b0C\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u4ee5\u5f00\u53d1\u7eff\u8272\u7a7a\u95f4\u3002"}}
{"id": "2505.09196", "pdf": "https://arxiv.org/pdf/2505.09196", "abs": "https://arxiv.org/abs/2505.09196", "authors": ["Tong Li", "Lizhi Wang", "Hansen Feng", "Lin Zhu", "Hua Huang"], "title": "PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement", "categories": ["cs.CV"], "comment": "11 pages, 9 tables, 9 figures", "summary": "Low-light image enhancement (LLIE) is a fundamental task in computational\nphotography, aiming to improve illumination, reduce noise, and enhance image\nquality. While recent advancements focus on designing increasingly complex\nneural network models, we observe a peculiar phenomenon: resetting certain\nparameters to random values unexpectedly improves enhancement performance for\nsome images. Drawing inspiration from biological genes, we term this phenomenon\nthe gene effect. The gene effect limits enhancement performance, as even random\nparameters can sometimes outperform learned ones, preventing models from fully\nutilizing their capacity. In this paper, we investigate the reason and propose\na solution. Based on our observations, we attribute the gene effect to static\nparameters, analogous to how fixed genetic configurations become maladaptive\nwhen environments change. Inspired by biological evolution, where adaptation to\nnew environments relies on gene mutation and recombination, we propose\nparameter dynamic evolution (PDE) to adapt to different images and mitigate the\ngene effect. PDE employs a parameter orthogonal generation technique and the\ncorresponding generated parameters to simulate gene recombination and gene\nmutation, separately. Experiments validate the effectiveness of our techniques.\nThe code will be released to the public.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u57fa\u56e0\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u53c2\u6570\u52a8\u6001\u8fdb\u5316\uff08PDE\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002PDE\u901a\u8fc7\u6a21\u62df\u57fa\u56e0\u91cd\u7ec4\u548c\u7a81\u53d8\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u589e\u5f3a\u7684\u6548\u679c\u3002", "motivation": "\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08LLIE\uff09\u662f\u8ba1\u7b97\u6444\u5f71\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u65e8\u5728\u6539\u5584\u7167\u660e\u3001\u51cf\u5c11\u566a\u58f0\u548c\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u8bbe\u8ba1\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u65f6\uff0c\u53d1\u73b0\u67d0\u4e9b\u53c2\u6570\u91cd\u7f6e\u4e3a\u968f\u673a\u503c\u53ef\u4ee5\u610f\u5916\u5730\u63d0\u9ad8\u589e\u5f3a\u6027\u80fd\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u57fa\u56e0\u6548\u5e94\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u6b63\u4ea4\u751f\u6210\u6280\u672f\u7684\u53c2\u6570\u52a8\u6001\u8fdb\u5316\uff08PDE\uff09\u65b9\u6cd5\uff0c\u6a21\u62df\u57fa\u56e0\u91cd\u7ec4\u548c\u7a81\u53d8\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u56fe\u50cf\u5e76\u51cf\u8f7b\u57fa\u56e0\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660ePDE\u80fd\u591f\u6709\u6548\u7f13\u89e3\u57fa\u56e0\u6548\u5e94\uff0c\u63d0\u9ad8\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u53c2\u6570\u52a8\u6001\u8fdb\u5316\uff08PDE\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u57fa\u56e0\u6548\u5e94\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09214", "pdf": "https://arxiv.org/pdf/2505.09214", "abs": "https://arxiv.org/abs/2505.09214", "authors": ["Zhonghao Lyu", "Ming Xiao", "Jie Xu", "Mikael Skoglund", "Marco Di Renzo"], "title": "The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks", "categories": ["cs.LG"], "comment": null, "summary": "The growing demand for large artificial intelligence model (LAIM) services is\ndriving a paradigm shift from traditional cloud-based inference to edge-based\ninference for low-latency, privacy-preserving applications. In particular,\nedge-device co-inference, which partitions LAIMs between edge devices and\nservers, has emerged as a promising strategy for resource-efficient LAIM\nexecution in wireless networks. In this paper, we investigate a pruning-aware\nLAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned\ninto on-device and on-server sub-models for deployment. For analysis, we first\nprove that the LAIM output distortion is upper bounded by its parameter\ndistortion. Then, we derive a lower bound on parameter distortion via\nrate-distortion theory, analytically capturing the relationship between pruning\nratio and co-inference performance. Next, based on the analytical results, we\nformulate an LAIM co-inference distortion bound minimization problem by jointly\noptimizing the pruning ratio, transmit power, and computation frequency under\nsystem latency, energy, and available resource constraints. Moreover, we\npropose an efficient algorithm to tackle the considered highly non-convex\nproblem. Finally, extensive simulations demonstrate the effectiveness of the\nproposed design. In particular, model parameter distortion is shown to provide\na reliable bound on output distortion. Also, the proposed joint pruning ratio\nand resource management design achieves superior performance in balancing\ntrade-offs among inference performance, system latency, and energy consumption\ncompared with benchmark schemes, such as fully on-device and on-server\ninference. Moreover, the split point is shown to play a critical role in system\nperformance optimization under heterogeneous and resource-limited edge\nenvironments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u611f\u77e5\u526a\u679d\u7684LAIM\u534f\u540c\u63a8\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5e73\u8861\u63a8\u7406\u6027\u80fd\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u526a\u679d\u540e\u7684\u6a21\u578b\u53c2\u6570\u5931\u771f\u53ef\u4ee5\u4f5c\u4e3a\u8f93\u51fa\u5931\u771f\u7684\u53ef\u9760\u4e0a\u9650\uff0c\u5e76\u4e14\u5206\u5272\u70b9\u5728\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u5bf9\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u5bf9\u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAIM\uff09\u670d\u52a1\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f20\u7edf\u57fa\u4e8e\u4e91\u7684\u63a8\u7406\u6b63\u5728\u5411\u57fa\u4e8e\u8fb9\u7f18\u7684\u63a8\u7406\u8f6c\u53d8\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5e94\u7528\u3002\u8fb9\u7f18\u8bbe\u5907\u534f\u540c\u63a8\u7406\u4f5c\u4e3a\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684LAIM\u6267\u884c\u7b56\u7565\uff0c\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5177\u6709\u524d\u666f\u3002\u7136\u800c\uff0c\u5982\u4f55\u5728\u8fb9\u7f18\u8bbe\u5907\u548c\u670d\u52a1\u5668\u4e4b\u95f4\u5408\u7406\u5212\u5206\u6a21\u578b\u4ee5\u4f18\u5316\u6027\u80fd\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u672c\u6587\u9996\u5148\u8bc1\u660e\u4e86LAIM\u8f93\u51fa\u5931\u771f\u7531\u53c2\u6570\u5931\u771f\u4e0a\u754c\u9650\u5236\uff0c\u7136\u540e\u901a\u8fc7\u7387\u5931\u771f\u7406\u8bba\u63a8\u5bfc\u51fa\u53c2\u6570\u5931\u771f\u7684\u4e0b\u754c\uff0c\u5206\u6790\u4e86\u526a\u679d\u6bd4\u4f8b\u4e0e\u534f\u540c\u63a8\u7406\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u63a5\u7740\uff0c\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u8054\u5408\u4f18\u5316\u526a\u679d\u6bd4\u4f8b\u3001\u4f20\u8f93\u529f\u7387\u548c\u8ba1\u7b97\u9891\u7387\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316LAIM\u534f\u540c\u63a8\u7406\u5931\u771f\u754c\u9650\u7684\u95ee\u9898\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u9ad8\u5ea6\u975e\u51f8\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u53c2\u6570\u5931\u771f\u53ef\u4ee5\u4f5c\u4e3a\u8f93\u51fa\u5931\u771f\u7684\u53ef\u9760\u4e0a\u9650\u3002\u540c\u65f6\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u526a\u679d\u6bd4\u4f8b\u548c\u8d44\u6e90\u7ba1\u7406\u8bbe\u8ba1\u5728\u5e73\u8861\u63a8\u7406\u6027\u80fd\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u5982\u5b8c\u5168\u5728\u8bbe\u5907\u7aef\u6216\u670d\u52a1\u5668\u7aef\u7684\u63a8\u7406\u3002\u6b64\u5916\uff0c\u5206\u5272\u70b9\u5728\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u5bf9\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5\u526a\u679d\u7684LAIM\u534f\u540c\u63a8\u7406\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5e73\u8861\u63a8\u7406\u6027\u80fd\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002\u6b64\u5916\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u526a\u679d\u540e\u7684\u6a21\u578b\u53c2\u6570\u5931\u771f\u53ef\u4ee5\u4f5c\u4e3a\u8f93\u51fa\u5931\u771f\u7684\u53ef\u9760\u4e0a\u9650\uff0c\u5e76\u4e14\u5206\u5272\u70b9\u5728\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u5bf9\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2505.09251", "pdf": "https://arxiv.org/pdf/2505.09251", "abs": "https://arxiv.org/abs/2505.09251", "authors": ["Vineetha Joy", "Aditya Anand", "Nidhi", "Anshuman Kumar", "Amit Sethi", "Hema Singh"], "title": "A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures", "categories": ["cs.CV"], "comment": null, "summary": "Metasurface-based radar absorbing structures (RAS) are highly preferred for\napplications like stealth technology, electromagnetic (EM) shielding, etc. due\nto their capability to achieve frequency selective absorption characteristics\nwith minimal thickness and reduced weight penalty. However, the conventional\napproach for the EM design and optimization of these structures relies on\nforward simulations, using full wave simulation tools, to predict the\nelectromagnetic (EM) response of candidate meta atoms. This process is\ncomputationally intensive, extremely time consuming and requires exploration of\nlarge design spaces. To overcome this challenge, we propose a surrogate model\nthat significantly accelerates the prediction of EM responses of multi-layered\nmetasurface-based RAS. A convolutional neural network (CNN) based architecture\nwith Huber loss function has been employed to estimate the reflection\ncharacteristics of the RAS model. The proposed model achieved a cosine\nsimilarity of 99.9% and a mean square error of 0.001 within 1000 epochs of\ntraining. The efficiency of the model has been established via full wave\nsimulations as well as experiment where it demonstrated significant reduction\nin computational time while maintaining high predictive accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u4ee5\u52a0\u901f\u591a\u5c42\u8d85\u8868\u9762\u96f7\u8fbe\u5438\u6ce2\u7ed3\u6784\u7684\u7535\u78c1\u54cd\u5e94\u9884\u6d4b\uff0c\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u7535\u78c1\u8bbe\u8ba1\u548c\u4f18\u5316\u4e2d\u8ba1\u7b97\u91cf\u5927\u3001\u8017\u65f6\u957f\u4e14\u9700\u8981\u63a2\u7d22\u5927\u91cf\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u67b6\u6784\u548cHuber\u635f\u5931\u51fd\u6570\u6765\u4f30\u8ba1RAS\u6a21\u578b\u7684\u53cd\u5c04\u7279\u6027\u3002", "result": "\u8be5\u6a21\u578b\u57281000\u4e2a\u8bad\u7ec3\u5468\u671f\u5185\u5b9e\u73b0\u4e8699.9%\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c0.001\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u5e76\u5728\u8ba1\u7b97\u65f6\u95f4\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u5168\u6ce2\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8ba1\u7b97\u65f6\u95f4\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u7684\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2505.09218", "pdf": "https://arxiv.org/pdf/2505.09218", "abs": "https://arxiv.org/abs/2505.09218", "authors": ["Alexander Tyurin", "Danil Sivtsov"], "title": "Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods", "categories": ["cs.LG", "cs.DC", "math.OC"], "comment": null, "summary": "We propose a new unifying framework, Birch SGD, for analyzing and designing\ndistributed SGD methods. The central idea is to represent each method as a\nweighted directed tree, referred to as a computation tree. Leveraging this\nrepresentation, we introduce a general theoretical result that reduces\nconvergence analysis to studying the geometry of these trees. This perspective\nyields a purely graph-based interpretation of optimization dynamics, offering a\nnew and intuitive foundation for method development. Using Birch SGD, we design\neight new methods and analyze them alongside previously known ones, with at\nleast six of the new methods shown to have optimal computational time\ncomplexity. Our research leads to two key insights: (i) all methods share the\nsame \"iteration rate\" of $O\\left(\\frac{(R + 1) L \\Delta}{\\varepsilon} +\n\\frac{\\sigma^2 L \\Delta}{\\varepsilon^2}\\right)$, where $R$ the maximum \"tree\ndistance\" along the main branch of a tree; and (ii) different methods exhibit\ndifferent trade-offs-for example, some update iterates more frequently,\nimproving practical performance, while others are more communication-efficient\nor focus on other aspects. Birch SGD serves as a unifying framework for\nnavigating these trade-offs. We believe these results provide a unified\nfoundation for understanding, analyzing, and designing efficient asynchronous\nand parallel optimization methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBirch SGD\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u8bbe\u8ba1\u5206\u5e03\u5f0fSGD\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u65b9\u6cd5\u8868\u793a\u4e3a\u52a0\u6743\u6709\u5411\u6811\uff0c\u6211\u4eec\u5f97\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u516b\u79cd\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u5f02\u6b65\u548c\u5e76\u884c\u4f18\u5316\u65b9\u6cd5\u7684\u7edf\u4e00\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0fSGD\u65b9\u6cd5\u5728\u5206\u6790\u548c\u8bbe\u8ba1\u4e0a\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u7edf\u4e00\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u57fa\u7840\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBirch SGD\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u8bbe\u8ba1\u5206\u5e03\u5f0fSGD\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u6bcf\u79cd\u65b9\u6cd5\u8868\u793a\u4e3a\u52a0\u6743\u6709\u5411\u6811\uff0c\u79f0\u4e3a\u8ba1\u7b97\u6811\u3002", "result": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u516b\u79cd\u65b0\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u4ee5\u53ca\u5df2\u77e5\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u81f3\u5c11\u516d\u79cd\u65b0\u65b9\u6cd5\u88ab\u8bc1\u660e\u5177\u6709\u6700\u4f18\u7684\u8ba1\u7b97\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u6211\u4eec\u5f97\u51fa\u4e24\u4e2a\u5173\u952e\u89c1\u89e3\uff1a(i) \u6240\u6709\u65b9\u6cd5\u5171\u4eab\u76f8\u540c\u7684\u201c\u8fed\u4ee3\u7387\u201d\uff1b(ii) \u4e0d\u540c\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6743\u8861\uff0c\u4f8b\u5982\u4e00\u4e9b\u65b9\u6cd5\u66f4\u65b0\u8fed\u4ee3\u5668\u66f4\u9891\u7e41\uff0c\u63d0\u9ad8\u4e86\u5b9e\u9645\u6027\u80fd\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\u5219\u66f4\u901a\u4fe1\u9ad8\u6548\u6216\u5173\u6ce8\u5176\u4ed6\u65b9\u9762\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u7406\u89e3\u3001\u5206\u6790\u548c\u8bbe\u8ba1\u9ad8\u6548\u7684\u5f02\u6b65\u548c\u5e76\u884c\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u3002"}}
{"id": "2505.09252", "pdf": "https://arxiv.org/pdf/2505.09252", "abs": "https://arxiv.org/abs/2505.09252", "authors": ["Yinuo Wang", "Yue Zeng", "Kai Chen", "Cai Meng", "Chao Pan", "Zhouping Tang"], "title": "Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping", "categories": ["cs.CV"], "comment": null, "summary": "Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes\non non-contrast computed tomography is critical for prognosis prediction and\ntherapeutic decision-making, yet remains challenging due to low contrast and\nblurring boundaries. This study evaluates the performance of zero-shot\nmulti-modal large language models (MLLMs) compared to traditional deep learning\nmethods in ICH binary classification and subtyping. Methods: We utilized a\ndataset provided by RSNA, comprising 192 NCCT volumes. The study compares\nvarious MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,\nwith conventional deep learning models, including ResNet50 and Vision\nTransformer. Carefully crafted prompts were used to guide MLLMs in tasks such\nas ICH presence, subtype classification, localization, and volume estimation.\nResults: The results indicate that in the ICH binary classification task,\ntraditional deep learning models outperform MLLMs comprehensively. For subtype\nclassification, MLLMs also exhibit inferior performance compared to traditional\ndeep learning models, with Gemini 2.0 Flash achieving an macro-averaged\nprecision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While\nMLLMs excel in interactive capabilities, their overall accuracy in ICH\nsubtyping is inferior to deep networks. However, MLLMs enhance interpretability\nthrough language interactions, indicating potential in medical imaging\nanalysis. Future efforts will focus on model refinement and developing more\nprecise MLLMs to improve performance in three-dimensional medical image\nprocessing.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u9885\u5185\u51fa\u8840\uff08ICH\uff09\u4e8c\u5206\u7c7b\u548c\u4e9a\u578b\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c06\u5176\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u800cMLLMs\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u53ca\u65f6\u8bc6\u522b\u975e\u5bf9\u6bd4CT\u4e0a\u7684\u9885\u5185\u51fa\u8840\uff08ICH\uff09\u4e9a\u578b\u5bf9\u4e8e\u9884\u540e\u9884\u6d4b\u548c\u6cbb\u7597\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5bf9\u6bd4\u5ea6\u4f4e\u548c\u8fb9\u754c\u6a21\u7cca\uff0c\u8fd9\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728ICH\u4e8c\u5206\u7c7b\u548c\u4e9a\u578b\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528RSNA\u63d0\u4f9b\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b192\u4e2a\u975e\u5bf9\u6bd4CT\u4f53\u79ef\uff0c\u6bd4\u8f83\u4e86\u5404\u79cdMLLM\uff08\u5305\u62ecGPT-4o\u3001Gemini 2.0 Flash\u548cClaude 3.5 Sonnet V2\uff09\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecResNet50\u548cVision Transformer\uff09\u7684\u6027\u80fd\u3002\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7528\u4e8e\u6307\u5bfcMLLM\u5b8c\u6210ICH\u5b58\u5728\u6027\u3001\u4e9a\u578b\u5206\u7c7b\u3001\u5b9a\u4f4d\u548c\u4f53\u79ef\u4f30\u8ba1\u7b49\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728ICH\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5168\u9762\u4f18\u4e8eMLLMs\u3002\u5728\u4e9a\u578b\u5206\u7c7b\u4e2d\uff0cMLLMs\u7684\u8868\u73b0\u4e5f\u900a\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0cGemini 2.0 Flash\u5728\u5b8f\u5e73\u5747\u7cbe\u5ea6\u4e0a\u8fbe\u52300.41\uff0c\u5728\u5b8f\u5e73\u5747F1\u5206\u6570\u4e0a\u8fbe\u52300.31\u3002", "conclusion": "\u867d\u7136MLLM\u5728\u4ea4\u4e92\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728ICH\u4e9a\u578b\u5206\u7c7b\u7684\u6574\u4f53\u51c6\u786e\u6027\u4ecd\u4f4e\u4e8e\u6df1\u5ea6\u7f51\u7edc\u3002\u7136\u800c\uff0cMLLM\u901a\u8fc7\u8bed\u8a00\u4ea4\u4e92\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u8868\u660e\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u96c6\u4e2d\u5728\u6a21\u578b\u4f18\u5316\u548c\u5f00\u53d1\u66f4\u7cbe\u786e\u7684MLLM\u4ee5\u63d0\u9ad8\u4e09\u7ef4\u533b\u5b66\u56fe\u50cf\u5904\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09239", "pdf": "https://arxiv.org/pdf/2505.09239", "abs": "https://arxiv.org/abs/2505.09239", "authors": ["Faruk Alpay"], "title": "Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories", "categories": ["cs.LG", "68T05, 90C25, 94A15", "I.2.6; G.1.6; H.1.1"], "comment": "23 pages, 11 figures, includes analytical proofs, sensitivity\n  analysis (95% CI), and JAX-based open-source implementation available at:\n  https://github.com/farukalpay/information-bottleneck-beta-optimization", "summary": "The Information Bottleneck (IB) method frequently suffers from unstable\noptimization, characterized by abrupt representation shifts near critical\npoints of the IB trade-off parameter, beta. In this paper, I introduce a novel\napproach to achieve stable and convex IB optimization through symbolic\ncontinuation and entropy-regularized trajectories. I analytically prove\nconvexity and uniqueness of the IB solution path when an entropy regularization\nterm is included, and demonstrate how this stabilizes representation learning\nacross a wide range of \\b{eta} values. Additionally, I provide extensive\nsensitivity analyses around critical points (beta) with statistically robust\nuncertainty quantification (95% confidence intervals). The open-source\nimplementation, experimental results, and reproducibility framework included in\nthis work offer a clear path for practical deployment and future extension of\nmy proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b26\u53f7\u5ef6\u7eed\u548c\u71b5\u6b63\u5219\u5316\u8f68\u8ff9\u5b9e\u73b0\u7a33\u5b9a\u4e14\u51f8\u7684IB\u4f18\u5316\u3002", "motivation": "\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u9047\u5230\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728IB\u6743\u8861\u53c2\u6570beta\u7684\u5173\u952e\u70b9\u9644\u8fd1\u4f1a\u51fa\u73b0\u7a81\u7136\u7684\u8868\u793a\u53d8\u5316\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u7b26\u53f7\u5ef6\u7eed\u548c\u71b5\u6b63\u5219\u5316\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684IB\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5206\u6790\u8bc1\u660e\u4e86\u5728\u5305\u542b\u71b5\u6b63\u5219\u5316\u9879\u65f6\uff0cIB\u89e3\u8def\u5f84\u5177\u6709\u51f8\u6027\u548c\u552f\u4e00\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5982\u4f55\u5728\u5e7f\u6cdb\u7684eta\u503c\u8303\u56f4\u5185\u7a33\u5b9a\u8868\u793a\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u56f4\u7ed5\u5173\u952e\u70b9\uff08beta\uff09\u7684\u654f\u611f\u6027\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4e0a\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0895%\u7f6e\u4fe1\u533a\u95f4\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b26\u53f7\u5ef6\u7eed\u548c\u71b5\u6b63\u5219\u5316\u8f68\u8ff9\u5b9e\u73b0\u7a33\u5b9a\u4e14\u51f8\u7684IB\u4f18\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u548c\u53ef\u91cd\u590d\u6027\u6846\u67b6\u4e3a\u5b9e\u9645\u90e8\u7f72\u548c\u672a\u6765\u6269\u5c55\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u5f84\u3002"}}
{"id": "2505.09256", "pdf": "https://arxiv.org/pdf/2505.09256", "abs": "https://arxiv.org/abs/2505.09256", "authors": ["Jaemin Jung", "Youngjoon Jang", "Joon Son Chung"], "title": "Test-Time Augmentation for Pose-invariant Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "The goal of this paper is to enhance face recognition performance by\naugmenting head poses during the testing phase. Existing methods often rely on\ntraining on frontalised images or learning pose-invariant representations, yet\nboth approaches typically require re-training and testing for each dataset,\ninvolving a substantial amount of effort. In contrast, this study proposes\nPose-TTA, a novel approach that aligns faces at inference time without\nadditional training. To achieve this, we employ a portrait animator that\ntransfers the source image identity into the pose of a driving image. Instead\nof frontalising a side-profile face -- which can introduce distortion --\nPose-TTA generates matching side-profile images for comparison, thereby\nreducing identity information loss. Furthermore, we propose a weighted feature\naggregation strategy to address any distortions or biases arising from the\nsynthetic data, thus enhancing the reliability of the augmented images.\nExtensive experiments on diverse datasets and with various pre-trained face\nrecognition models demonstrate that Pose-TTA consistently improves inference\nperformance. Moreover, our method is straightforward to integrate into existing\nface recognition pipelines, as it requires no retraining or fine-tuning of the\nunderlying recognition models.", "AI": {"tldr": "This paper introduces Pose-TTA, an approach that enhances face recognition performance by aligning faces at inference time without additional training, using a portrait animator and a weighted feature aggregation strategy.", "motivation": "Existing methods for enhancing face recognition performance often require re-training and testing for each dataset, which involves substantial effort. The study aims to propose a novel approach that aligns faces at inference time without additional training.", "method": "Pose-TTA aligns faces at inference time without additional training by employing a portrait animator that transfers the source image identity into the pose of a driving image, along with a weighted feature aggregation strategy to address distortions or biases from synthetic data.", "result": "Extensive experiments on diverse datasets and with various pre-trained face recognition models demonstrate that Pose-TTA consistently improves inference performance.", "conclusion": "Pose-TTA consistently improves inference performance and is straightforward to integrate into existing face recognition pipelines."}}
{"id": "2505.08830", "pdf": "https://arxiv.org/pdf/2505.08830", "abs": "https://arxiv.org/abs/2505.08830", "authors": ["Wenhao Jiang", "Yuchuan Luo", "Guilin Deng", "Silong Chen", "Xu Yang", "Shihong Wu", "Xinwen Gao", "Lin Liu", "Shaojing Fu"], "title": "Federated Large Language Models: Feasibility, Robustness, Security and Future Directions", "categories": ["cs.CR", "cs.AI"], "comment": "35 pages", "summary": "The integration of Large Language Models (LLMs) and Federated Learning (FL)\npresents a promising solution for joint training on distributed data while\npreserving privacy and addressing data silo issues. However, this emerging\nfield, known as Federated Large Language Models (FLLM), faces significant\nchallenges, including communication and computation overheads, heterogeneity,\nprivacy and security concerns. Current research has primarily focused on the\nfeasibility of FLLM, but future trends are expected to emphasize enhancing\nsystem robustness and security. This paper provides a comprehensive review of\nthe latest advancements in FLLM, examining challenges from four critical\nperspectives: feasibility, robustness, security, and future directions. We\npresent an exhaustive survey of existing studies on FLLM feasibility, introduce\nmethods to enhance robustness in the face of resource, data, and task\nheterogeneity, and analyze novel risks associated with this integration,\nincluding privacy threats and security challenges. We also review the latest\ndevelopments in defense mechanisms and explore promising future research\ndirections, such as few-shot learning, machine unlearning, and IP protection.\nThis survey highlights the pressing need for further research to enhance system\nrobustness and security while addressing the unique challenges posed by the\nintegration of FL and LLM.", "AI": {"tldr": "\u672c\u6587\u5bf9FLLM\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u5206\u6790\u4e86\u5176\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "FLLM\u9762\u4e34\u7740\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3001\u5f02\u6784\u6027\u3001\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\u7b49\u91cd\u5927\u6311\u6218\uff0c\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728FLLM\u7684\u53ef\u884c\u6027\u4e0a\uff0c\u672a\u6765\u8d8b\u52bf\u5c06\u66f4\u52a0\u6ce8\u91cd\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u672c\u6587\u5bf9FLLM\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u4ece\u53ef\u884c\u6027\u3001\u9c81\u68d2\u6027\u3001\u5b89\u5168\u6027\u548c\u672a\u6765\u65b9\u5411\u56db\u4e2a\u5173\u952e\u89d2\u5ea6\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u672c\u6587\u5bf9FLLM\u7684\u53ef\u884c\u6027\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u8c03\u67e5\uff0c\u4ecb\u7ecd\u4e86\u5728\u8d44\u6e90\u3001\u6570\u636e\u548c\u4efb\u52a1\u5f02\u6784\u6027\u9762\u524d\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u79cd\u96c6\u6210\u5e26\u6765\u7684\u65b0\u98ce\u9669\uff0c\u5305\u62ec\u9690\u79c1\u5a01\u80c1\u548c\u5b89\u5168\u6311\u6218\u3002\u6b64\u5916\uff0c\u8fd8\u56de\u987e\u4e86\u6700\u65b0\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5e76\u63a2\u8ba8\u4e86\u6709\u524d\u666f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u89e3\u51b3FL\u548cLLM\u96c6\u6210\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u3002"}}
{"id": "2505.09284", "pdf": "https://arxiv.org/pdf/2505.09284", "abs": "https://arxiv.org/abs/2505.09284", "authors": ["Panqi Chen", "Yifan Sun", "Lei Cheng", "Yang Yang", "Weichang Li", "Yang Liu", "Weiqing Liu", "Jiang Bian", "Shikai Fang"], "title": "Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Modeling and reconstructing multidimensional physical dynamics from sparse\nand off-grid observations presents a fundamental challenge in scientific\nresearch. Recently, diffusion-based generative modeling shows promising\npotential for physical simulation. However, current approaches typically\noperate on on-grid data with preset spatiotemporal resolution, but struggle\nwith the sparsely observed and continuous nature of real-world physical\ndynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in\nFunctional Tucker space, a novel framework that generates full-field evolution\nof physical dynamics from irregular sparse observations. SDIFT leverages the\nfunctional Tucker model as the latent space representer with proven universal\napproximation property, and represents observations as latent functions and\nTucker core sequences. We then construct a sequential diffusion model with\ntemporally augmented UNet in the functional Tucker space, denoising noise drawn\nfrom a Gaussian process to generate the sequence of core tensors.\n  At the posterior sampling stage, we propose a Message-Passing Posterior\nSampling mechanism, enabling conditional generation of the entire sequence\nguided by observations at limited time steps. We validate SDIFT on three\nphysical systems spanning astronomical (supernova explosions, light-year\nscale), environmental (ocean sound speed fields, kilometer scale), and\nmolecular (organic liquid, millimeter scale) domains, demonstrating significant\nimprovements in both reconstruction accuracy and computational efficiency\ncompared to state-of-the-art approaches.", "AI": {"tldr": "SDIFT\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7a00\u758f\u89c2\u6d4b\u4e2d\u751f\u6210\u5b8c\u6574\u7684\u7269\u7406\u52a8\u529b\u5b66\u6f14\u53d8\uff0c\u5e76\u5728\u591a\u4e2a\u7269\u7406\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u9884\u5b9a\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u7f51\u683c\u6570\u636e\u4e0a\u8fd0\u884c\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u7269\u7406\u52a8\u529b\u5b66\u7684\u7a00\u758f\u89c2\u6d4b\u548c\u8fde\u7eed\u6027\u8d28\u3002", "method": "SDIFT\u5229\u7528\u529f\u80fdTucker\u6a21\u578b\u4f5c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u5668\uff0c\u5e76\u5728\u529f\u80fdTucker\u7a7a\u95f4\u4e2d\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u65f6\u95f4\u589e\u5f3aUNet\u7684\u987a\u5e8f\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4ece\u9ad8\u65af\u8fc7\u7a0b\u4e2d\u62bd\u53d6\u566a\u58f0\u6765\u751f\u6210\u6838\u5fc3\u5f20\u91cf\u5e8f\u5217\u3002", "result": "SDIFT\u80fd\u591f\u4ece\u4e0d\u89c4\u5219\u7a00\u758f\u89c2\u6d4b\u4e2d\u751f\u6210\u5b8c\u6574\u7684\u7269\u7406\u52a8\u529b\u5b66\u6f14\u53d8\uff0c\u5e76\u5728\u591a\u4e2a\u7269\u7406\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SDIFT\u5728\u4e09\u4e2a\u7269\u7406\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2505.09263", "pdf": "https://arxiv.org/pdf/2505.09263", "abs": "https://arxiv.org/abs/2505.09263", "authors": ["Guan Gui", "Bin-Bin Gao", "Jun Liu", "Chengjie Wang", "Yunsheng Wu"], "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c11\u6837\u672c\u5f02\u5e38\u9a71\u52a8\u751f\u6210\u65b9\u6cd5\uff08AnoGen\uff09\uff0c\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u73b0\u5b9e\u4e14\u591a\u6837\u7684\u5f02\u5e38\uff0c\u4ece\u800c\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\uff0c\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u8fc7\u566a\u58f0\u6216\u5916\u90e8\u6570\u636e\u5408\u6210\u5f02\u5e38\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u7136\u800c\uff0c\u5408\u6210\u5f02\u5e38\u4e0e\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u4e4b\u95f4\u5b58\u5728\u8f83\u5927\u7684\u8bed\u4e49\u5dee\u8ddd\uff0c\u5bfc\u81f4\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u8f83\u5f31\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u73b0\u5b9e\u4e14\u591a\u6837\u5f02\u5e38\u7684\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c11\u6837\u672c\u5f02\u5e38\u9a71\u52a8\u751f\u6210\uff08AnoGen\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ec5\u4f7f\u7528\u5c11\u91cf\u771f\u5b9e\u5f02\u5e38\u6765\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u73b0\u5b9e\u4e14\u591a\u6837\u7684\u5f02\u5e38\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u8bad\u7ec3\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u57fa\u4e8e\u5c11\u91cf\u7ed9\u5b9a\u7684\u771f\u5b9e\u5f02\u5e38\u5b66\u4e60\u5f02\u5e38\u5206\u5e03\uff0c\u5e76\u5c06\u6240\u5b66\u77e5\u8bc6\u6ce8\u5165\u5d4c\u5165\u4e2d\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u4f7f\u7528\u5d4c\u5165\u548c\u7ed9\u5b9a\u7684\u8fb9\u754c\u6846\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\u5728\u7279\u5b9a\u5bf9\u8c61\uff08\u6216\u7eb9\u7406\uff09\u4e0a\u751f\u6210\u73b0\u5b9e\u4e14\u591a\u6837\u7684\u5f02\u5e38\uff1b\u7b2c\u4e09\u9636\u6bb5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u7684\u5f02\u5e38\u8bad\u7ec3\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u751f\u6210\u5f02\u5e38\u6709\u6548\u63d0\u9ad8\u4e86DRAEM\u548cDesTSeg\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u7684AU-PR\u6307\u6807\uff0c\u5206\u522b\u63d0\u9ad8\u4e865.8%\u548c1.5%\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6MVTec\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u751f\u6210\u7684\u5f02\u5e38\u6709\u6548\u63d0\u9ad8\u4e86\u5f02\u5e38\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.09287", "pdf": "https://arxiv.org/pdf/2505.09287", "abs": "https://arxiv.org/abs/2505.09287", "authors": ["Shunsuke Yoneda", "Valdemar \u0160v\u00e1bensk\u00fd", "Gen Li", "Daisuke Deguchi", "Atsushi Shimada"], "title": "Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features", "categories": ["cs.LG", "cs.CY", "I.2; I.6; K.3"], "comment": "To appear in the Proceedings of the 18th Educational Data Mining\n  Conference (EDM 2025)", "summary": "Digital textbooks are widely used in various educational contexts, such as\nuniversity courses and online lectures. Such textbooks yield learning log data\nthat have been used in numerous educational data mining (EDM) studies for\nstudent behavior analysis and performance prediction. However, these studies\nhave faced challenges in integrating confidential data, such as academic\nrecords and learning logs, across schools due to privacy concerns.\nConsequently, analyses are often conducted with data limited to a single\nschool, which makes developing high-performing and generalizable models\ndifficult. This study proposes a method that combines federated learning and\ndifferential features to address these issues. Federated learning enables model\ntraining without centralizing data, thereby preserving student privacy.\nDifferential features, which utilize relative values instead of absolute\nvalues, enhance model performance and generalizability. To evaluate the\nproposed method, a model for predicting at-risk students was trained using data\nfrom 1,136 students across 12 courses conducted over 4 years, and validated on\nhold-out test data from 5 other courses. Experimental results demonstrated that\nthe proposed method addresses privacy concerns while achieving performance\ncomparable to that of models trained via centralized learning in terms of Top-n\nprecision, nDCG, and PR-AUC. Furthermore, using differential features improved\nprediction performance across all evaluation datasets compared to\nnon-differential approaches. The trained models were also applicable for early\nprediction, achieving high performance in detecting at-risk students in earlier\nstages of the semester within the validation datasets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u96c6\u4e2d\u5f0f\u5b66\u4e60\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6240\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u975e\u5dee\u5206\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6559\u80b2\u6570\u636e\u6316\u6398\u7814\u7a76\u9762\u4e34\u6574\u5408\u5b66\u6821\u95f4\u7684\u673a\u5bc6\u6570\u636e\uff08\u5982\u5b66\u672f\u8bb0\u5f55\u548c\u5b66\u4e60\u65e5\u5fd7\uff09\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u9690\u79c1\u95ee\u9898\u5bfc\u81f4\u5206\u6790\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e2a\u5b66\u6821\u7684\u6570\u636e\uff0c\u8fd9\u4f7f\u5f97\u5f00\u53d1\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u7279\u5f81\u7684\u65b9\u6cd5\u3002\u8054\u90a6\u5b66\u4e60\u5141\u8bb8\u5728\u4e0d\u96c6\u4e2d\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u4ece\u800c\u4fdd\u62a4\u5b66\u751f\u9690\u79c1\u3002\u5dee\u5206\u7279\u5f81\u5229\u7528\u76f8\u5bf9\u503c\u800c\u4e0d\u662f\u7edd\u5bf9\u503c\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u5728Top-n\u7cbe\u5ea6\u3001nDCG\u548cPR-AUC\u65b9\u9762\u8fbe\u5230\u4e86\u4e0e\u96c6\u4e2d\u5f0f\u5b66\u4e60\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5dee\u5206\u7279\u5f81\u5728\u6240\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u90fd\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u3002\u8bad\u7ec3\u7684\u6a21\u578b\u8fd8\u9002\u7528\u4e8e\u65e9\u671f\u9884\u6d4b\uff0c\u5728\u9a8c\u8bc1\u6570\u636e\u96c6\u4e2d\u68c0\u6d4b\u51fa\u65e9\u671f\u5904\u4e8e\u98ce\u9669\u4e2d\u7684\u5b66\u751f\u65f6\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u96c6\u4e2d\u5f0f\u5b66\u4e60\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6240\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u975e\u5dee\u5206\u65b9\u6cd5\u3002"}}
{"id": "2505.09264", "pdf": "https://arxiv.org/pdf/2505.09264", "abs": "https://arxiv.org/abs/2505.09264", "authors": ["Bin-Bin Gao"], "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5373\u4f7f\u7528\u4e00\u4e2a\u6b63\u5e38\u56fe\u50cf\u63d0\u793a\uff08OneNIP\uff09\u6765\u91cd\u5efa\u6b63\u5e38\u7279\u5f81\u5e76\u6062\u590d\u5f02\u5e38\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u7cbe\u70bc\u5668\uff0c\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u7684\u6b63\u5e38\u56fe\u50cf\u548c\u5408\u6210\u7684\u5f02\u5e38\u56fe\u50cf\u6765\u56de\u5f52\u91cd\u5efa\u8bef\u5dee\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u3002OneNIP\u5728\u4e09\u4e2a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002", "motivation": "Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space.", "method": "We propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). Additionally, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images.", "result": "OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. The supervised refiner significantly improves pixel-level anomaly segmentation.", "conclusion": "OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA."}}
{"id": "2505.08835", "pdf": "https://arxiv.org/pdf/2505.08835", "abs": "https://arxiv.org/abs/2505.08835", "authors": ["Hyunsik Na", "Wonho Lee", "Seungdeok Roh", "Sohee Park", "Daeseon Choi"], "title": "Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of convenient and efficient fully unmanned stores equipped with\nartificial intelligence-based automated checkout systems marks a new era in\nretail. However, these systems have inherent artificial intelligence security\nvulnerabilities, which are exploited via adversarial patch attacks,\nparticularly in physical environments. This study demonstrated that adversarial\npatches can severely disrupt object detection models used in unmanned stores,\nleading to issues such as theft, inventory discrepancies, and interference. We\ninvestigated three types of adversarial patch attacks -- Hiding, Creating, and\nAltering attacks -- and highlighted their effectiveness. We also introduce the\nnovel color histogram similarity loss function by leveraging attacker knowledge\nof the color information of a target class object. Besides the traditional\nconfusion-matrix-based attack success rate, we introduce a new\nbounding-boxes-based metric to analyze the practical impact of these attacks.\nStarting with attacks on object detection models trained on snack and fruit\ndatasets in a digital environment, we evaluated the effectiveness of\nadversarial patches in a physical testbed that mimicked a real unmanned store\nwith RGB cameras and realistic conditions. Furthermore, we assessed the\nrobustness of these attacks in black-box scenarios, demonstrating that shadow\nattacks can enhance success rates of attacks even without direct access to\nmodel parameters. Our study underscores the necessity for robust defense\nstrategies to protect unmanned stores from adversarial threats. Highlighting\nthe limitations of the current defense mechanisms in real-time detection\nsystems and discussing various proactive measures, we provide insights into\nimproving the robustness of object detection models and fortifying unmanned\nretail environments against these attacks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u5bf9\u65e0\u4eba\u5546\u5e97\u4e2d\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u65e0\u4eba\u5546\u5e97\u4e2d\u7684\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u653b\u51fb\u7684\u6709\u6548\u6027\u4ee5\u53ca\u5982\u4f55\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u4e09\u79cd\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\uff08\u9690\u85cf\u3001\u521b\u5efa\u548c\u4fee\u6539\u653b\u51fb\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u989c\u8272\u76f4\u65b9\u56fe\u76f8\u4f3c\u6027\u635f\u5931\u51fd\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u754c\u6846\u7684\u5ea6\u91cf\u6807\u51c6\u6765\u5206\u6790\u8fd9\u4e9b\u653b\u51fb\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6297\u6027\u8865\u4e01\u53ef\u4ee5\u4e25\u91cd\u5e72\u6270\u65e0\u4eba\u5546\u5e97\u4e2d\u4f7f\u7528\u7684\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\uff0c\u5bfc\u81f4\u76d7\u7a83\u3001\u5e93\u5b58\u5dee\u5f02\u548c\u5e72\u6270\u7b49\u95ee\u9898\u3002\u6b64\u5916\uff0c\u5728\u9ed1\u76d2\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u9634\u5f71\u653b\u51fb\u53ef\u4ee5\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4e3a\u4fdd\u62a4\u65e0\u4eba\u5546\u5e97\u514d\u53d7\u5bf9\u6297\u6027\u5a01\u80c1\u800c\u9700\u8981\u7a33\u5065\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u52a0\u5f3a\u65e0\u4eba\u96f6\u552e\u73af\u5883\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.09294", "pdf": "https://arxiv.org/pdf/2505.09294", "abs": "https://arxiv.org/abs/2505.09294", "authors": ["Fan Xu", "Wuyang Chen", "Wei Gao"], "title": "On the Learning with Augmented Class via Forests", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Decision trees and forests have achieved successes in various real\napplications, most working with all testing classes known in training data. In\nthis work, we focus on learning with augmented class via forests, where an\naugmented class may appear in testing data yet not in training data. We\nincorporate information of augmented class into trees' splitting, i.e., a new\nsplitting criterion, called augmented Gini impurity, is introduced to exploit\nsome unlabeled data from testing distribution. We then develop the approach\nnamed Learning with Augmented Class via Forests (LACForest), which constructs\nshallow forests based on the augmented Gini impurity and then splits forests\nwith pseudo-labeled augmented instances for better performance. We also develop\ndeep neural forests with a novel optimization objective based on our augmented\nGini impurity, so as to utilize the representation power of neural networks for\nforests. Theoretically, we present the convergence analysis for augmented Gini\nimpurity, and finally conduct experiments to verify the effectiveness of our\napproaches. The code is available at https://github.com/nju-xuf/LACForest/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5LACForest\uff0c\u7528\u4e8e\u901a\u8fc7\u68ee\u6797\u8fdb\u884c\u5e26\u6709\u589e\u5f3a\u7c7b\u7684\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u589e\u5f3aGini\u4e0d\u7eaf\u5ea6\u6765\u6539\u8fdb\u51b3\u7b56\u6811\u7684\u5206\u88c2\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u4f2a\u6807\u8bb0\u7684\u589e\u5f3a\u5b9e\u4f8b\u5206\u5272\u68ee\u6797\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u6df1\u5ea6\u795e\u7ecf\u68ee\u6797\uff0c\u4ee5\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u80fd\u529b\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u51b3\u7b56\u6811\u548c\u68ee\u6797\u901a\u5e38\u5904\u7406\u6240\u6709\u6d4b\u8bd5\u7c7b\u522b\u90fd\u51fa\u73b0\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u60c5\u51b5\u3002\u7136\u800c\uff0c\u5728\u672c\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5173\u6ce8\u4e8e\u901a\u8fc7\u68ee\u6797\u8fdb\u884c\u5e26\u6709\u589e\u5f3a\u7c7b\u7684\u5b66\u4e60\uff0c\u5176\u4e2d\u589e\u5f3a\u7c7b\u53ef\u80fd\u5728\u6d4b\u8bd5\u6570\u636e\u4e2d\u51fa\u73b0\u4f46\u4e0d\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u88c2\u51c6\u5219\uff0c\u79f0\u4e3a\u589e\u5f3aGini\u4e0d\u7eaf\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u540d\u4e3aLACForest\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u589e\u5f3aGini\u4e0d\u7eaf\u5ea6\u6784\u5efa\u6d45\u5c42\u68ee\u6797\uff0c\u5e76\u901a\u8fc7\u4f2a\u6807\u8bb0\u7684\u589e\u5f3a\u5b9e\u4f8b\u5206\u5272\u68ee\u6797\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u6df1\u5ea6\u795e\u7ecf\u68ee\u6797\uff0c\u5176\u4f18\u5316\u76ee\u6807\u57fa\u4e8e\u6211\u4eec\u7684\u589e\u5f3aGini\u4e0d\u7eaf\u5ea6\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u589e\u5f3aGini\u4e0d\u7eaf\u5ea6\u7684\u6536\u655b\u5206\u6790\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u5df2\u53d1\u5e03\u5728https://github.com/nju-xuf/LACForest/\u3002"}}
{"id": "2505.09265", "pdf": "https://arxiv.org/pdf/2505.09265", "abs": "https://arxiv.org/abs/2505.09265", "authors": ["Bin-Bin Gao"], "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by NeurIPS 2024", "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eaf\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u8bed\u8a00\u5f15\u5bfc\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5730\u5206\u5272\u4efb\u4f55\u5f02\u5e38\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u89c6\u89c9\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4f7f\u7528\u624b\u52a8\u8bbe\u8ba1\u7684\u6587\u672c\u63d0\u793a\u68c0\u6d4b\u672a\u89c1\u8fc7\u7684\u5f02\u5e38\u3002\u7136\u800c\uff0c\u89c6\u89c9\u8868\u793a\u672c\u8d28\u4e0a\u4e0e\u8bed\u8a00\u65e0\u5173\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u7eaf\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMetaUAS\u7684\u4e00\u6b21\u63d0\u793a\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u771f\u5b9e\u4e16\u754c\u4e2d\u5206\u5272\u4efb\u4f55\u65b0\u9896\u6216\u672a\u89c1\u8fc7\u7684\u89c6\u89c9\u5f02\u5e38\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u6765\u5904\u7406\u63d0\u793a\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4\u7684\u51e0\u4f55\u53d8\u5316\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u751a\u81f3\u5168\u6837\u672c\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eaf\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u548c\u9ad8\u6548\u5730\u5206\u5272\u4efb\u4f55\u5f02\u5e38\uff0c\u4ec5\u9700\u4e00\u4e2a\u6b63\u5e38\u56fe\u50cf\u63d0\u793a\uff0c\u5e76\u4e14\u65e0\u9700\u8bed\u8a00\u5f15\u5bfc\u3002"}}
{"id": "2505.08838", "pdf": "https://arxiv.org/pdf/2505.08838", "abs": "https://arxiv.org/abs/2505.08838", "authors": ["Peixuan Ge", "Tongkun Su", "Faqin Lv", "Baoliang Zhao", "Peng Zhang", "Chi Hong Wong", "Liang Yao", "Yu Sun", "Zenan Wang", "Pak Kin Wong", "Ying Hu"], "title": "Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Ultrasound (US) report generation is a challenging task due to the\nvariability of US images, operator dependence, and the need for standardized\ntext. Unlike X-ray and CT, US imaging lacks consistent datasets, making\nautomation difficult. In this study, we propose a unified framework for\nmulti-organ and multilingual US report generation, integrating fragment-based\nmultilingual training and leveraging the standardized nature of US reports. By\naligning modular text fragments with diverse imaging data and curating a\nbilingual English-Chinese dataset, the method achieves consistent and\nclinically accurate text generation across organ sites and languages.\nFine-tuning with selective unfreezing of the vision transformer (ViT) further\nimproves text-image alignment. Compared to the previous state-of-the-art KMVE\nmethod, our approach achieves relative gains of about 2\\% in BLEU scores,\napproximately 3\\% in ROUGE-L, and about 15\\% in CIDEr, while significantly\nreducing errors such as missing or incorrect content. By unifying multi-organ\nand multi-language report generation into a single, scalable framework, this\nwork demonstrates strong potential for real-world clinical workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5668\u5b98\u548c\u591a\u8bed\u8a00\u8d85\u58f0\u62a5\u544a\u751f\u6210\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u7247\u6bb5\u7684\u591a\u8bed\u8a00\u8bad\u7ec3\u548c\u6807\u51c6\u5316\u62a5\u544a\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u8de8\u5668\u5b98\u548c\u8bed\u8a00\u7684\u4e00\u81f4\u4e14\u4e34\u5e8a\u51c6\u786e\u7684\u6587\u672c\u751f\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8d85\u58f0\u62a5\u544a\u751f\u6210\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u7531\u4e8e\u8d85\u58f0\u56fe\u50cf\u7684\u53d8\u5f02\u6027\u3001\u64cd\u4f5c\u5458\u4f9d\u8d56\u6027\u548c\u6807\u51c6\u5316\u6587\u672c\u7684\u9700\u6c42\uff0c\u81ea\u52a8\u5316\u53d8\u5f97\u56f0\u96be\u3002\u4e0eX\u5c04\u7ebf\u548cCT\u4e0d\u540c\uff0c\u8d85\u58f0\u6210\u50cf\u7f3a\u4e4f\u4e00\u81f4\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u5f97\u81ea\u52a8\u5316\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5668\u5b98\u548c\u591a\u8bed\u8a00\u8d85\u58f0\u62a5\u544a\u751f\u6210\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u7247\u6bb5\u7684\u591a\u8bed\u8a00\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u4e86\u8d85\u58f0\u62a5\u544a\u7684\u6807\u51c6\u5316\u7279\u6027\u3002\u901a\u8fc7\u5c06\u6a21\u5757\u5316\u6587\u672c\u7247\u6bb5\u4e0e\u591a\u6837\u5316\u7684\u6210\u50cf\u6570\u636e\u5bf9\u9f50\uff0c\u5e76\u6574\u7406\u4e86\u4e00\u4e2a\u53cc\u8bed\u82f1\u8bed-\u4e2d\u6587\u6570\u636e\u96c6\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8de8\u5668\u5b98\u90e8\u4f4d\u548c\u8bed\u8a00\u7684\u4e00\u81f4\u4e14\u4e34\u5e8a\u51c6\u786e\u7684\u6587\u672c\u751f\u6210\u3002\u9009\u62e9\u6027\u89e3\u51bb\u89c6\u89c9\u53d8\u538b\u5668\uff08ViT\uff09\u7684\u5fae\u8c03\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u4e0e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5KMVE\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728BLEU\u5206\u6570\u4e0a\u76f8\u5bf9\u63d0\u5347\u4e86\u7ea62%\uff0cROUGE-L\u7ea63%\uff0cCIDEr\u7ea615%\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u7f3a\u5931\u6216\u9519\u8bef\u5185\u5bb9\u7b49\u9519\u8bef\u3002", "conclusion": "\u901a\u8fc7\u5c06\u591a\u5668\u5b98\u548c\u591a\u8bed\u8a00\u62a5\u544a\u751f\u6210\u7edf\u4e00\u5230\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u4e2d\uff0c\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2505.09308", "pdf": "https://arxiv.org/pdf/2505.09308", "abs": "https://arxiv.org/abs/2505.09308", "authors": ["George Andriopoulos", "Soyuj Jung Basnet", "Juan Guevara", "Li Guo", "Keith Ross"], "title": "Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model", "categories": ["cs.LG"], "comment": "31 pages, 8 figures", "summary": "The Unconstrained Feature Model (UFM) is a mathematical framework that\nenables closed-form approximations for minimal training loss and related\nperformance measures in deep neural networks (DNNs). This paper leverages the\nUFM to provide qualitative insights into neural multivariate regression, a\ncritical task in imitation learning, robotics, and reinforcement learning.\nSpecifically, we address two key questions: (1) How do multi-task models\ncompare to multiple single-task models in terms of training performance? (2)\nCan whitening and normalizing regression targets improve training performance?\nThe UFM theory predicts that multi-task models achieve strictly smaller\ntraining MSE than multiple single-task models when the same or stronger\nregularization is applied to the latter, and our empirical results confirm\nthese findings. Regarding whitening and normalizing regression targets, the UFM\ntheory predicts that they reduce training MSE when the average variance across\nthe target dimensions is less than one, and our empirical results once again\nconfirm these findings. These findings highlight the UFM as a powerful\nframework for deriving actionable insights into DNN design and data\npre-processing strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528UFM\u6846\u67b6\u6765\u63d0\u4f9b\u5bf9\u795e\u7ecf\u591a\u5143\u56de\u5f52\u7684\u5b9a\u6027\u89c1\u89e3\uff0c\u5e76\u9a8c\u8bc1\u4e86\u591a\u4efb\u52a1\u6a21\u578b\u4f18\u4e8e\u5355\u4efb\u52a1\u6a21\u578b\u4ee5\u53ca\u767d\u5316\u548c\u6807\u51c6\u5316\u56de\u5f52\u76ee\u6807\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6027\u80fd\u7684\u7406\u8bba\u9884\u6d4b\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a(1) \u591a\u4efb\u52a1\u6a21\u578b\u4e0e\u591a\u4e2a\u5355\u4efb\u52a1\u6a21\u578b\u5728\u8bad\u7ec3\u6027\u80fd\u65b9\u9762\u5982\u4f55\u6bd4\u8f83\uff1f(2) \u767d\u5316\u548c\u6807\u51c6\u5316\u56de\u5f52\u76ee\u6807\u662f\u5426\u80fd\u63d0\u9ad8\u8bad\u7ec3\u6027\u80fd\uff1f", "method": "\u8be5\u8bba\u6587\u5229\u7528UFM\u6765\u63d0\u4f9b\u5bf9\u795e\u7ecf\u591a\u5143\u56de\u5f52\u7684\u5b9a\u6027\u89c1\u89e3\u3002", "result": "UFM\u7406\u8bba\u9884\u6d4b\uff0c\u5f53\u5bf9\u540e\u8005\u5e94\u7528\u76f8\u540c\u6216\u66f4\u5f3a\u7684\u6b63\u5219\u5316\u65f6\uff0c\u591a\u4efb\u52a1\u6a21\u578b\u7684\u8bad\u7ec3MSE\u4e25\u683c\u5c0f\u4e8e\u591a\u4e2a\u5355\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002\u5173\u4e8e\u767d\u5316\u548c\u6807\u51c6\u5316\u56de\u5f52\u76ee\u6807\uff0cUFM\u7406\u8bba\u9884\u6d4b\u5b83\u4eec\u4f1a\u964d\u4f4e\u8bad\u7ec3MSE\uff0c\u5f53\u76ee\u6807\u7ef4\u5ea6\u7684\u5e73\u5747\u65b9\u5dee\u5c0f\u4e8e1\u65f6\uff0c\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u518d\u6b21\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86UFM\u4f5c\u4e3a\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u5bfc\u5173\u4e8eDNN\u8bbe\u8ba1\u548c\u6570\u636e\u9884\u5904\u7406\u7b56\u7565\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002"}}
{"id": "2505.09274", "pdf": "https://arxiv.org/pdf/2505.09274", "abs": "https://arxiv.org/abs/2505.09274", "authors": ["Fares Bougourzi", "Abdenour Hadid"], "title": "Recent Advances in Medical Imaging Segmentation: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Medical imaging is a cornerstone of modern healthcare, driving advancements\nin diagnosis, treatment planning, and patient care. Among its various tasks,\nsegmentation remains one of the most challenging problem due to factors such as\ndata accessibility, annotation complexity, structural variability, variation in\nmedical imaging modalities, and privacy constraints. Despite recent progress,\nachieving robust generalization and domain adaptation remains a significant\nhurdle, particularly given the resource-intensive nature of some proposed\nmodels and their reliance on domain expertise. This survey explores\ncutting-edge advancements in medical image segmentation, focusing on\nmethodologies such as Generative AI, Few-Shot Learning, Foundation Models, and\nUniversal Models. These approaches offer promising solutions to longstanding\nchallenges. We provide a comprehensive overview of the theoretical foundations,\nstate-of-the-art techniques, and recent applications of these methods. Finally,\nwe discuss inherent limitations, unresolved issues, and future research\ndirections aimed at enhancing the practicality and accessibility of\nsegmentation models in medical imaging. We are maintaining a\n\\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub\nRepository} to continue tracking and updating innovations in this field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u533b\u5b66\u5f71\u50cf\u5206\u5272\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u751f\u6210\u5f0fAI\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\u548c\u901a\u7528\u6a21\u578b\u7b49\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u5206\u5272\u9762\u4e34\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u3001\u6ce8\u91ca\u590d\u6742\u6027\u3001\u7ed3\u6784\u53d8\u5f02\u3001\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u53d8\u5316\u548c\u9690\u79c1\u9650\u5236\u7b49\u6311\u6218\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b9e\u73b0\u7a33\u5065\u7684\u6cdb\u5316\u548c\u9886\u57df\u9002\u5e94\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u969c\u788d\uff0c\u7279\u522b\u662f\u7531\u4e8e\u4e00\u4e9b\u63d0\u51fa\u7684\u6a21\u578b\u8d44\u6e90\u5bc6\u96c6\u4e14\u4f9d\u8d56\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\u533b\u5b66\u5f71\u50cf\u5206\u5272\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u751f\u6210\u5f0fAI\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\u548c\u901a\u7528\u6a21\u578b\u7b49\u65b9\u6cd5\u7684\u7406\u8bba\u57fa\u7840\u3001\u6700\u5148\u8fdb\u7684\u6280\u672f\u548c\u8fd1\u671f\u5e94\u7528\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5185\u5728\u9650\u5236\u3001\u672a\u89e3\u51b3\u7684\u95ee\u9898\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u589e\u5f3a\u533b\u5b66\u5f71\u50cf\u4e2d\u5206\u5272\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u53ca\u6027\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u533b\u5b66\u5f71\u50cf\u5206\u5272\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\u548c\u901a\u7528\u6a21\u578b\u7b49\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63d0\u9ad8\u5206\u5272\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u53ca\u6027\u3002"}}
{"id": "2505.08841", "pdf": "https://arxiv.org/pdf/2505.08841", "abs": "https://arxiv.org/abs/2505.08841", "authors": ["Andrea Cremaschi", "Dae-Jin Lee", "Manuele Leonelli"], "title": "Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As artificial intelligence and robotics increasingly reshape the global labor\nmarket, understanding public perceptions of these technologies becomes\ncritical. We examine how these perceptions have evolved across Latin America,\nusing survey data from the 2017, 2018, 2020, and 2023 waves of the\nLatinobar\\'ometro. Drawing on responses from over 48,000 individuals across 16\ncountries, we analyze fear of job loss due to artificial intelligence and\nrobotics. Using statistical modeling and latent class analysis, we identify key\nstructural and ideological predictors of concern, with education level and\npolitical orientation emerging as the most consistent drivers. Our findings\nreveal substantial temporal and cross-country variation, with a notable peak in\nfear during 2018 and distinct attitudinal profiles emerging from latent\nsegmentation. These results offer new insights into the social and structural\ndimensions of AI anxiety in emerging economies and contribute to a broader\nunderstanding of public attitudes toward automation beyond the Global North.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u62c9\u4e01\u7f8e\u6d32\u5bf9\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u62c5\u5fe7\u53d8\u5316\uff0c\u53d1\u73b0\u6559\u80b2\u6c34\u5e73\u548c\u653f\u6cbb\u503e\u5411\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u63ed\u793a\u4e86\u65f6\u95f4\u4e0e\u56fd\u5bb6\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u65e5\u76ca\u91cd\u5851\u5168\u7403\u52b3\u52a8\u529b\u5e02\u573a\uff0c\u4e86\u89e3\u516c\u4f17\u5bf9\u8fd9\u4e9b\u6280\u672f\u7684\u770b\u6cd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e9b\u770b\u6cd5\u5728\u62c9\u4e01\u7f8e\u6d32\u7684\u6f14\u53d8\u60c5\u51b5\uff0c\u5e76\u8bc6\u522b\u5f71\u54cd\u62c5\u5fe7\u7684\u5173\u952e\u7ed3\u6784\u548c\u610f\u8bc6\u5f62\u6001\u56e0\u7d20\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u7edf\u8ba1\u5efa\u6a21\u548c\u6f5c\u5728\u7c7b\u522b\u5206\u6790\uff0c\u57fa\u4e8e\u6765\u81ea\u62c9\u4e01\u5df4\u7f57\u6885\u7279\u7f572017\u30012018\u30012020\u548c2023\u5e74\u6ce2\u6b21\u7684\u8c03\u67e5\u6570\u636e\uff0c\u5206\u6790\u4e86\u56e0\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u5bfc\u81f4\u7684\u5de5\u4f5c\u6d41\u5931\u6050\u60e7\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u57282018\u5e74\u5bf9\u5de5\u4f5c\u6d41\u5931\u7684\u6050\u60e7\u8fbe\u5230\u9ad8\u5cf0\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u5206\u6bb5\u51fa\u73b0\u4e86\u4e0d\u540c\u7684\u6001\u5ea6\u7279\u5f81\u3002\u6559\u80b2\u6c34\u5e73\u548c\u653f\u6cbb\u503e\u5411\u88ab\u786e\u5b9a\u4e3a\u6700\u4e00\u81f4\u7684\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u62c9\u4e01\u7f8e\u6d32\u5bf9\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u62c5\u5fe7\u5728\u65f6\u95f4\u548c\u56fd\u5bb6\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u5f3a\u8c03\u4e86\u6559\u80b2\u6c34\u5e73\u548c\u653f\u6cbb\u503e\u5411\u4f5c\u4e3a\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u7684\u91cd\u8981\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u7406\u89e3\u65b0\u5174\u7ecf\u6d4e\u4f53\u4e2d\u7684AI\u7126\u8651\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u5730\u4e86\u89e3\u5168\u7403\u5317\u65b9\u4ee5\u5916\u7684\u81ea\u52a8\u5316\u516c\u4f17\u6001\u5ea6\u3002"}}
{"id": "2505.09331", "pdf": "https://arxiv.org/pdf/2505.09331", "abs": "https://arxiv.org/abs/2505.09331", "authors": ["Cunlai Pu", "Fangrui Wu", "Rajput Ramiz Sharafat", "Guangzhao Dai", "Xiangbo Shu"], "title": "MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks", "categories": ["cs.LG"], "comment": null, "summary": "Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)\naims to predict the potential formation of future links between UAVs. In\nadversarial environments where the route information of UAVs is unavailable,\npredicting future links must rely solely on the observed historical topological\ninformation of UANETs. However, the highly dynamic and sparse nature of UANET\ntopologies presents substantial challenges in effectively capturing meaningful\nstructural and temporal patterns for accurate link prediction. Most existing\nlink prediction methods focus on temporal dynamics at a single structural scale\nwhile neglecting the effects of sparsity, resulting in insufficient information\ncapture and limited applicability to UANETs. In this paper, we propose a\nmulti-scale structural-temporal link prediction model (MUST) for UANETs.\nSpecifically, we first employ graph attention networks (GATs) to capture\nstructural features at multiple levels, including the individual UAV level, the\nUAV community level, and the overall network level. Then, we use long\nshort-term memory (LSTM) networks to learn the temporal dynamics of these\nmulti-scale structural features. Additionally, we address the impact of\nsparsity by introducing a sophisticated loss function during model\noptimization. We validate the performance of MUST using several UANET datasets\ngenerated through simulations. Extensive experimental results demonstrate that\nMUST achieves state-of-the-art link prediction performance in highly dynamic\nand sparse UANETs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u7ed3\u6784-\u65f6\u95f4\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\uff08MUST\uff09\uff0c\u7528\u4e8e\u89e3\u51b3UANET\u4e2d\u7531\u4e8e\u9ad8\u5ea6\u52a8\u6001\u548c\u7a00\u758f\u6027\u5e26\u6765\u7684\u94fe\u63a5\u9884\u6d4b\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u7ed3\u6784\u5c3a\u5ea6\u7684\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u7a00\u758f\u6027\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u4fe1\u606f\u6355\u6349\u4e0d\u8db3\u4e14\u9002\u7528\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u7ed3\u6784-\u65f6\u95f4\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\uff08MUST\uff09\uff0c\u9996\u5148\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GATs\uff09\u6355\u83b7\u591a\u4e2a\u5c42\u6b21\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u5b66\u4e60\u8fd9\u4e9b\u591a\u5c3a\u5ea6\u7ed3\u6784\u7279\u5f81\u7684\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u590d\u6742\u7684\u635f\u5931\u51fd\u6570\u6765\u89e3\u51b3\u7a00\u758f\u6027\u7684\u5f71\u54cd\u3002", "result": "MUST\u5728\u591a\u4e2a\u901a\u8fc7\u6a21\u62df\u751f\u6210\u7684UANET\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u5e76\u4e14\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u7a00\u758f\u7684UANET\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MUST\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u7a00\u758f\u7684UANET\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.09306", "pdf": "https://arxiv.org/pdf/2505.09306", "abs": "https://arxiv.org/abs/2505.09306", "authors": ["Thijs L van der Plas", "Stephen Law", "Michael JO Pocock"], "title": "Predicting butterfly species presence from satellite imagery using soft contrastive regularisation", "categories": ["cs.CV", "cs.LG"], "comment": "To be published in the 2025 CVPR FGVC12 workshop", "summary": "The growing demand for scalable biodiversity monitoring methods has fuelled\ninterest in remote sensing data, due to its widespread availability and\nextensive coverage. Traditionally, the application of remote sensing to\nbiodiversity research has focused on mapping and monitoring habitats, but with\nincreasing availability of large-scale citizen-science wildlife observation\ndata, recent methods have started to explore predicting multi-species presence\ndirectly from satellite images. This paper presents a new data set for\npredicting butterfly species presence from satellite data in the United\nKingdom. We experimentally optimise a Resnet-based model to predict\nmulti-species presence from 4-band satellite images, and find that this model\nespecially outperforms the mean rate baseline for locations with high species\nbiodiversity. To improve performance, we develop a soft, supervised contrastive\nregularisation loss that is tailored to probabilistic labels (such as\nspecies-presence data), and demonstrate that this improves prediction accuracy.\nIn summary, our new data set and contrastive regularisation method contribute\nto the open challenge of accurately predicting species biodiversity from remote\nsensing data, which is key for efficient biodiversity monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u536b\u661f\u6570\u636e\u9884\u6d4b\u82f1\u56fd\u8774\u8776\u7269\u79cd\u7684\u5b58\u5728\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u591a\u7269\u79cd\u5b58\u5728\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u9065\u611f\u6570\u636e\u7684\u5e7f\u6cdb\u53ef\u7528\u6027\u548c\u5e7f\u6cdb\u8986\u76d6\uff0c\u5bf9\u53ef\u6269\u5c55\u7684\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u65b9\u6cd5\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\u3002\u4f20\u7edf\u4e0a\uff0c\u9065\u611f\u5728\u751f\u7269\u591a\u6837\u6027\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u96c6\u4e2d\u5728\u7ed8\u5236\u548c\u76d1\u6d4b\u751f\u5883\u4e0a\uff0c\u4f46\u968f\u7740\u5927\u89c4\u6a21\u516c\u6c11\u79d1\u5b66\u91ce\u751f\u52a8\u7269\u89c2\u6d4b\u6570\u636e\u7684\u589e\u52a0\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5f00\u59cb\u63a2\u7d22\u76f4\u63a5\u4ece\u536b\u661f\u56fe\u50cf\u9884\u6d4b\u591a\u7269\u79cd\u5b58\u5728\u3002", "method": "\u6211\u4eec\u5b9e\u9a8c\u4f18\u5316\u4e86\u4e00\u4e2a\u57fa\u4e8eResnet\u7684\u6a21\u578b\uff0c\u4ee5\u4ece4\u6ce2\u6bb5\u536b\u661f\u56fe\u50cf\u4e2d\u9884\u6d4b\u591a\u7269\u79cd\u5b58\u5728\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u8f6f\u76d1\u7763\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\uff0c\u4e13\u95e8\u9488\u5bf9\u6982\u7387\u6807\u7b7e\uff08\u5982\u7269\u79cd\u5b58\u5728\u6570\u636e\uff09\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u8be5\u6a21\u578b\u5728\u9ad8\u7269\u79cd\u751f\u7269\u591a\u6837\u6027\u5730\u70b9\u7279\u522b\u4f18\u4e8e\u5e73\u5747\u7387\u57fa\u7ebf\uff0c\u5e76\u4e14\u901a\u8fc7\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b0\u6570\u636e\u96c6\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u52a9\u4e8e\u89e3\u51b3\u4ece\u9065\u611f\u6570\u636e\u51c6\u786e\u9884\u6d4b\u7269\u79cd\u751f\u7269\u591a\u6837\u6027\u7684\u5f00\u653e\u6027\u6311\u6218\uff0c\u8fd9\u5bf9\u4e8e\u9ad8\u6548\u7684\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.08844", "pdf": "https://arxiv.org/pdf/2505.08844", "abs": "https://arxiv.org/abs/2505.08844", "authors": ["Jiawen Chen", "Jianghao Zhang", "Huaxiu Yao", "Yun Li"], "title": "CellTypeAgent: Trustworthy cell type annotation with Large Language Models", "categories": ["q-bio.GN", "cs.AI", "68T20", "I.2.1"], "comment": null, "summary": "Cell type annotation is a critical yet laborious step in single-cell RNA\nsequencing analysis. We present a trustworthy large language model (LLM)-agent,\nCellTypeAgent, which integrates LLMs with verification from relevant databases.\nCellTypeAgent achieves higher accuracy than existing methods while mitigating\nhallucinations. We evaluated CellTypeAgent across nine real datasets involving\n303 cell types from 36 tissues. This combined approach holds promise for more\nefficient and reliable cell type annotation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aCellTypeAgent\u7684\u53ef\u4fe1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u76f8\u5173\u6570\u636e\u5e93\u7684\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u4e86\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u5206\u6790\u4e2d\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u662f\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u5206\u6790\u4e2d\u7684\u5173\u952e\u4f46\u8017\u65f6\u7684\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4fe1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0cCellTypeAgent\uff0c\u5b83\u5c06LLM\u4e0e\u76f8\u5173\u6570\u636e\u5e93\u7684\u9a8c\u8bc1\u76f8\u7ed3\u5408\u3002", "result": "CellTypeAgent\u5728\u4e5d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6d89\u53ca303\u79cd\u7ec6\u80de\u7c7b\u578b\u548c36\u79cd\u7ec4\u7ec7\uff0c\u5176\u51c6\u786e\u6027\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "conclusion": "\u8fd9\u79cd\u7ed3\u5408\u65b9\u6cd5\u6709\u671b\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u53ef\u9760\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u3002"}}
{"id": "2505.09344", "pdf": "https://arxiv.org/pdf/2505.09344", "abs": "https://arxiv.org/abs/2505.09344", "authors": ["Gabriel Cort\u00eas", "Nuno Louren\u00e7o", "Paolo Romano", "Penousal Machado"], "title": "GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Determining the performance of a Deep Neural Network during Neural\nArchitecture Search processes is essential for identifying optimal\narchitectures and hyperparameters. Traditionally, this process requires\ntraining and evaluation of each network, which is time-consuming and\nresource-intensive. Zero-cost proxies estimate performance without training,\nserving as an alternative to traditional training. However, recent proxies\noften lack generalization across diverse scenarios and provide only relative\nrankings rather than predicted accuracies. To address these limitations, we\npropose GreenFactory, an ensemble of zero-cost proxies that leverages a random\nforest regressor to combine multiple predictors' strengths and directly predict\nmodel test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust\nresults across multiple datasets. Specifically, GreenFactory achieves high\nKendall correlations on NATS-Bench-SSS, indicating substantial agreement\nbetween its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945\nfor CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we\nachieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for\nImageNet-16-120, showcasing its reliability in both search spaces.", "AI": {"tldr": "GreenFactory\u662f\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u7684\u96f6\u6210\u672c\u4ee3\u7406\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u9884\u6d4b\u6a21\u578b\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u548c\u8bc4\u4f30\u6bcf\u4e2a\u7f51\u7edc\uff0c\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002\u6700\u8fd1\u7684\u4ee3\u7406\u901a\u5e38\u7f3a\u4e4f\u8de8\u4e0d\u540c\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u53ea\u63d0\u4f9b\u76f8\u5bf9\u6392\u540d\u800c\u4e0d\u662f\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faGreenFactory\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u7684\u96f6\u6210\u672c\u4ee3\u7406\u96c6\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u76f4\u63a5\u9884\u6d4b\u6a21\u578b\u6d4b\u8bd5\u7cbe\u5ea6\u3002", "result": "GreenFactory\u5728NATS-Bench\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u7a33\u5065\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728NATS-Bench-SSS\u548cNATS-Bench-TSS\u4e0a\uff0c\u4e0e\u5b9e\u9645\u6027\u80fd\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "GreenFactory\u5728NATS-Bench\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8Kendall\u76f8\u5173\u6027\uff0c\u8868\u660e\u5176\u9884\u6d4b\u5206\u6570\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u6709\u663e\u8457\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.09324", "pdf": "https://arxiv.org/pdf/2505.09324", "abs": "https://arxiv.org/abs/2505.09324", "authors": ["Lakshya Gupta", "Imran N. Junejo"], "title": "Neural Video Compression using 2D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "9 pages, 8 figures", "summary": "The computer vision and image processing research community has been involved\nin standardizing video data communications for the past many decades, leading\nto standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent\ngroundbreaking works have focused on employing deep learning-based techniques\nto replace the traditional video codec pipeline to a greater affect. Neural\nvideo codecs (NVC) create an end-to-end ML-based solution that does not rely on\nany handcrafted features (motion or edge-based) and have the ability to learn\ncontent-aware compression strategies, offering better adaptability and higher\ncompression efficiency than traditional methods. This holds a great potential\nnot only for hardware design, but also for various video streaming platforms\nand applications, especially video conferencing applications such as MS-Teams\nor Zoom that have found extensive usage in classrooms and workplaces. However,\ntheir high computational demands currently limit their use in real-time\napplications like video conferencing. To address this, we propose a\nregion-of-interest (ROI) based neural video compression model that leverages 2D\nGaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable\nof real-time decoding and can be optimized using fewer data points, requiring\nonly thousands of Gaussians for decent quality outputs as opposed to millions\nin 3D scenes. In this work, we designed a video pipeline that speeds up the\nencoding time of the previous Gaussian splatting-based image codec by 88% by\nusing a content-aware initialization strategy paired with a novel Gaussian\ninter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be\nused for a video-codec solution, the first of its kind solution in this neural\nvideo codec space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u70b9\u56fe\u7684\u795e\u7ecf\u89c6\u9891\u538b\u7f29\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u7801\u901f\u5ea6\uff0c\u5e76\u4e3a\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u7f16\u89e3\u7801\u5668\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u5b58\u5728\u8ba1\u7b97\u9700\u6c42\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668\uff08NVC\uff09\u867d\u7136\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u66f4\u9ad8\u7684\u538b\u7f29\u6548\u7387\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u89c6\u9891\u5904\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u521d\u59cb\u5316\u7b56\u7565\u548c\u65b0\u9896\u7684\u9ad8\u65af\u5e27\u95f4\u5197\u4f59\u51cf\u5c11\u673a\u5236\uff0c\u5c06\u57fa\u4e8e\u9ad8\u65af\u70b9\u56fe\u7684\u56fe\u50cf\u7f16\u89e3\u7801\u5668\u7684\u7f16\u7801\u65f6\u95f4\u52a0\u5feb\u4e8688%\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u89c6\u9891\u5904\u7406\u6d41\u7a0b\u6210\u529f\u5730\u5c06\u57fa\u4e8e\u9ad8\u65af\u70b9\u56fe\u7684\u56fe\u50cf\u7f16\u89e3\u7801\u5668\u7684\u7f16\u7801\u65f6\u95f4\u52a0\u5feb\u4e8688%\uff0c\u5e76\u9996\u6b21\u5728\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668\u9886\u57df\u5b9e\u73b0\u4e86\u9ad8\u65af\u70b9\u56fe\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u611f\u5174\u8da3\uff08ROI\uff09\u7684\u795e\u7ecf\u89c6\u9891\u538b\u7f29\u6a21\u578b\uff0c\u5229\u75282D\u9ad8\u65af\u70b9\u56fe\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u7801\u901f\u5ea6\uff0c\u5e76\u4e3a\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08845", "pdf": "https://arxiv.org/pdf/2505.08845", "abs": "https://arxiv.org/abs/2505.08845", "authors": ["Misgina Tsighe Hagos", "Antti Suutala", "Dmitrii Bychkov", "Hakan K\u00fcc\u00fckel", "Joar von Bahr", "Milda Poceviciute", "Johan Lundin", "Nina Linder", "Claes Lundstr\u00f6m"], "title": "Validation of Conformal Prediction in Cervical Atypia Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Deep learning based cervical cancer classification can potentially increase\naccess to screening in low-resource regions. However, deep learning models are\noften overconfident and do not reliably reflect diagnostic uncertainty.\nMoreover, they are typically optimized to generate maximum-likelihood\npredictions, which fail to convey uncertainty or ambiguity in their results.\nSuch challenges can be addressed using conformal prediction, a model-agnostic\nframework for generating prediction sets that contain likely classes for\ntrained deep-learning models. The size of these prediction sets indicates model\nuncertainty, contracting as model confidence increases. However, existing\nconformal prediction evaluation primarily focuses on whether the prediction set\nincludes or covers the true class, often overlooking the presence of extraneous\nclasses. We argue that prediction sets should be truthful and valuable to end\nusers, ensuring that the listed likely classes align with human expectations\nrather than being overly relaxed and including false positives or unlikely\nclasses. In this study, we comprehensively validate conformal prediction sets\nusing expert annotation sets collected from multiple annotators. We evaluate\nthree conformal prediction approaches applied to three deep-learning models\ntrained for cervical atypia classification. Our expert annotation-based\nanalysis reveals that conventional coverage-based evaluations overestimate\nperformance and that current conformal prediction methods often produce\nprediction sets that are not well aligned with human labels. Additionally, we\nexplore the capabilities of the conformal prediction methods in identifying\nambiguous and out-of-distribution data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u5bab\u9888\u764c\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u9ad8\u4f30\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u4e0e\u4eba\u7c7b\u6807\u7b7e\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u4e0d\u80fd\u53ef\u9760\u5730\u53cd\u6620\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u901a\u5e38\u88ab\u4f18\u5316\u4e3a\u751f\u6210\u6700\u5927\u4f3c\u7136\u9884\u6d4b\uff0c\u8fd9\u65e0\u6cd5\u4f20\u8fbe\u7ed3\u679c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6b67\u4e49\u3002\u8fd9\u4e9b\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u5171\u5f62\u9884\u6d4b\u6765\u89e3\u51b3\uff0c\u8fd9\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5305\u542b\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u80fd\u7c7b\u522b\u7684\u9884\u6d4b\u96c6\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4ece\u591a\u4e2a\u6ce8\u91ca\u5668\u6536\u96c6\u7684\u4e13\u5bb6\u6ce8\u91ca\u96c6\u5168\u9762\u9a8c\u8bc1\u4e86\u5171\u5f62\u9884\u6d4b\u96c6\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u4e09\u79cd\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5e94\u7528\u4e8e\u4e09\u79cd\u7528\u4e8e\u5bab\u9888\u975e\u5178\u578b\u6027\u5206\u7c7b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6548\u679c\u3002", "result": "\u6211\u4eec\u7684\u4e13\u5bb6\u6ce8\u91ca\u5206\u6790\u663e\u793a\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u8bc4\u4f30\u9ad8\u4f30\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u5f53\u524d\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u7ecf\u5e38\u4ea7\u751f\u4e0e\u4eba\u7c7b\u6807\u7b7e\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u8bc6\u522b\u6a21\u7cca\u548c\u5206\u5e03\u5916\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u8bc4\u4f30\u9ad8\u4f30\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u5f53\u524d\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u7ecf\u5e38\u4ea7\u751f\u4e0e\u4eba\u7c7b\u6807\u7b7e\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u8bc6\u522b\u6a21\u7cca\u548c\u5206\u5e03\u5916\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2505.09354", "pdf": "https://arxiv.org/pdf/2505.09354", "abs": "https://arxiv.org/abs/2505.09354", "authors": ["Guangtai Wang", "Chi-Man Vong", "Jintao Huang"], "title": "Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning", "categories": ["cs.LG"], "comment": null, "summary": "Diminishing the impact of false-positive labels is critical for conducting\ndisambiguation in partial label learning. However, the existing disambiguation\nstrategies mainly focus on exploiting the characteristics of individual partial\nlabel instances while neglecting the strong supervision information of clean\nsamples randomly lying in the datasets. In this work, we show that clean\nsamples can be collected to offer guidance and enhance the confidence of the\nmost possible candidates. Motivated by the manner of the differentiable count\nloss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new\ncalibration strategy called CleanSE. Specifically, we attribute the most\nreliable candidates with higher significance under the assumption that for each\nclean sample, if its label is one of the candidates of its nearest neighbor in\nthe representation space, it is more likely to be the ground truth of its\nneighbor. Moreover, clean samples offer help in characterizing the sample\ndistributions by restricting the label counts of each label to a specific\ninterval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL\ndatasets showed this calibration strategy can be applied to most of the\nstate-of-the-art PLL methods as well as enhance their performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6821\u51c6\u7b56\u7565CleanSE\uff0c\u5229\u7528\u5e72\u51c0\u6837\u672c\u6765\u63d0\u9ad8\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u4e2d\u7684\u6d88\u6b67\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6d88\u6b67\u7b56\u7565\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u90e8\u5206\u6807\u7b7e\u5b9e\u4f8b\u7684\u7279\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u6570\u636e\u96c6\u4e2d\u968f\u673a\u5b58\u5728\u7684\u5e72\u51c0\u6837\u672c\u7684\u5f3a\u76d1\u7763\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6821\u51c6\u7b56\u7565\u79f0\u4e3aCleanSE\uff0c\u8be5\u7b56\u7565\u57fa\u4e8e\u53ef\u5fae\u8ba1\u6570\u635f\u5931\u7b56\u7565\u548cK-\u8fd1\u90bb\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u6536\u96c6\u5e72\u51c0\u6837\u672c\uff0c\u53ef\u4ee5\u63d0\u4f9b\u6307\u5bfc\u5e76\u589e\u5f3a\u6700\u53ef\u80fd\u5019\u9009\u8005\u7684\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6821\u51c6\u7b56\u7565\u53ef\u4ee5\u5e94\u7528\u4e8e\u5927\u591a\u6570\u6700\u5148\u8fdb\u7684PLL\u65b9\u6cd5\uff0c\u5e76\u63d0\u9ad8\u5176\u6027\u80fd\u3002"}}
{"id": "2505.09329", "pdf": "https://arxiv.org/pdf/2505.09329", "abs": "https://arxiv.org/abs/2505.09329", "authors": ["Jiarun Liu", "Hong-Yu Zhou", "Weijian Huang", "Hao Yang", "Dongning Song", "Tao Tan", "Yong Liang", "Shanshan Wang"], "title": "BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Scaling up model and data size have demonstrated impressive performance\nimprovement over a wide range of tasks. Despite extensive studies on scaling\nbehaviors for general-purpose tasks, medical images exhibit substantial\ndifferences from natural data. It remains unclear the key factors in developing\nmedical vision foundation models at scale due to the absence of an extensive\nunderstanding of scaling behavior in the medical domain. In this paper, we\nexplored the scaling behavior across model sizes, training algorithms, data\nsizes, and imaging modalities in developing scalable medical vision foundation\nmodels by self-supervised learning. To support scalable pretraining, we\nintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing a\nwide range of biomedical image modalities and anatomies. We observed that\nscaling up does provide benefits but varies across tasks. Additional analysis\nreveals several factors correlated with scaling benefits. Finally, we propose\nBioVFM, a large-scale medical vision foundation model pretrained on 21 million\nbiomedical images, which outperforms the previous state-of-the-art foundation\nmodels across 12 medical benchmarks. Our results highlight that while scaling\nup is beneficial for pursuing better performance, task characteristics, data\ndiversity, pretraining methods, and computational efficiency remain critical\nconsiderations for developing scalable medical foundation models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7f29\u653e\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578bBioVFM\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e0e\u81ea\u7136\u6570\u636e\u6709\u663e\u8457\u5dee\u5f02\uff0c\u56e0\u6b64\u5728\u533b\u5b66\u9886\u57df\u7f3a\u4e4f\u5bf9\u7f29\u653e\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\uff0c\u8fd9\u4f7f\u5f97\u5f00\u53d1\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63a2\u7d22\u4e86\u5728\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u7b97\u6cd5\u3001\u6570\u636e\u5927\u5c0f\u548c\u6210\u50cf\u6a21\u6001\u7684\u7f29\u653e\u884c\u4e3a\u3002\u4e3a\u4e86\u652f\u6301\u53ef\u6269\u5c55\u7684\u9884\u8bad\u7ec3\uff0c\u6211\u4eec\u5f15\u5165\u4e86BioVFM-21M\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u5e7f\u6cdb\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6a21\u6001\u548c\u89e3\u5256\u7ed3\u6784\u7684\u5927\u89c4\u6a21\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u3002", "result": "\u6211\u4eec\u89c2\u5bdf\u5230\u6269\u5927\u89c4\u6a21\u786e\u5b9e\u63d0\u4f9b\u4e86\u597d\u5904\uff0c\u4f46\u56e0\u4efb\u52a1\u800c\u5f02\u3002\u989d\u5916\u7684\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u7f29\u653e\u6536\u76ca\u76f8\u5173\u7684\u51e0\u4e2a\u56e0\u7d20\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86BioVFM\uff0c\u4e00\u4e2a\u57282100\u4e07\u5f20\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u572812\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u6269\u5927\u89c4\u6a21\u5bf9\u4e8e\u8ffd\u6c42\u66f4\u597d\u7684\u6027\u80fd\u662f\u6709\u76ca\u7684\uff0c\u4f46\u4efb\u52a1\u7279\u6027\u3001\u6570\u636e\u591a\u6837\u6027\u3001\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u8ba1\u7b97\u6548\u7387\u4ecd\u7136\u662f\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2505.09361", "pdf": "https://arxiv.org/pdf/2505.09361", "abs": "https://arxiv.org/abs/2505.09361", "authors": ["Samir Moustafa", "Nils M. Kriege", "Wilfried N. Gansterer"], "title": "Efficient Mixed Precision Quantization in Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have become essential for handling large-scale\ngraph applications. However, the computational demands of GNNs necessitate the\ndevelopment of efficient methods to accelerate inference. Mixed precision\nquantization emerges as a promising solution to enhance the efficiency of GNN\narchitectures without compromising prediction performance. Compared to\nconventional deep learning architectures, GNN layers contain a wider set of\ncomponents that can be quantized, including message passing functions,\naggregation functions, update functions, the inputs, learnable parameters, and\noutputs of these functions. In this paper, we introduce a theorem for efficient\nquantized message passing to aggregate integer messages. It guarantees\nnumerical equality of the aggregated messages using integer values with respect\nto those obtained with full (FP32) precision. Based on this theorem, we\nintroduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which\nflexibly selects effective integer bit-widths for all components within GNN\nlayers. Our approach systematically navigates the wide set of possible\nbit-width combinations, addressing the challenge of optimizing efficiency while\naiming at maintaining comparable prediction performance. MixQ-GNN integrates\nwith existing GNN quantization methods, utilizing their graph structure\nadvantages to achieve higher prediction performance. On average, MixQ-GNN\nachieved reductions in bit operations of 5.5x for node classification and 5.1x\nfor graph classification compared to architectures represented in FP32\nprecision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GNN\u91cf\u5316\u6846\u67b6MixQ-GNN\uff0c\u901a\u8fc7\u9009\u62e9\u6709\u6548\u7684\u6574\u6570\u4f4d\u5bbd\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "GNN\u7684\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u9700\u8981\u9ad8\u6548\u7684\u52a0\u901f\u65b9\u6cd5\u3002\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u63d0\u9ad8GNN\u67b6\u6784\u7684\u6548\u7387\u800c\u4e0d\u5f71\u54cd\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5b9a\u7406\uff0c\u7528\u4e8e\u9ad8\u6548\u91cf\u5316\u6d88\u606f\u4f20\u9012\u4ee5\u805a\u5408\u6574\u6570\u6d88\u606f\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86MixQ-GNN\u6846\u67b6\u3002", "result": "MixQ-GNN\u5728\u8282\u70b9\u5206\u7c7b\u548c\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u5b9e\u73b0\u4e865.5\u500d\u548c5.1\u500d\u7684\u4f4d\u64cd\u4f5c\u51cf\u5c11\u3002", "conclusion": "MixQ-GNN\u80fd\u591f\u901a\u8fc7\u9009\u62e9\u6709\u6548\u7684\u6574\u6570\u4f4d\u5bbd\u6765\u63d0\u9ad8GNN\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.09336", "pdf": "https://arxiv.org/pdf/2505.09336", "abs": "https://arxiv.org/abs/2505.09336", "authors": ["Muzammil Behzad"], "title": "Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce MultiviewVLM, a vision-language model designed\nfor unsupervised contrastive multiview representation learning of facial\nemotions from 3D/4D data. Our architecture integrates pseudo-labels derived\nfrom generated textual prompts to guide implicit alignment of emotional\nsemantics. To capture shared information across multi-views, we propose a joint\nembedding space that aligns multiview representations without requiring\nexplicit supervision. We further enhance the discriminability of our model\nthrough a novel multiview contrastive learning strategy that leverages stable\npositive-negative pair sampling. A gradient-friendly loss function is\nintroduced to promote smoother and more stable convergence, and the model is\noptimized for distributed training to ensure scalability. Extensive experiments\ndemonstrate that MultiviewVLM outperforms existing state-of-the-art methods and\ncan be easily adapted to various real-world applications with minimal\nmodifications.", "AI": {"tldr": "This paper introduces MultiviewVLM, a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. The model uses pseudo-labels and a joint embedding space to align emotional semantics across multiple views, enhancing discriminability through a novel contrastive learning strategy and a gradient-friendly loss function.", "motivation": "The paper aims to introduce a vision-language model for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data.", "method": "MultiviewVLM integrates pseudo-labels derived from generated textual prompts to guide implicit alignment of emotional semantics. It proposes a joint embedding space that aligns multiview representations without requiring explicit supervision. A novel multiview contrastive learning strategy is used to enhance the discriminability of the model, along with a gradient-friendly loss function and distributed training optimization.", "result": "Extensive experiments demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications.", "conclusion": "MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications."}}
{"id": "2505.08847", "pdf": "https://arxiv.org/pdf/2505.08847", "abs": "https://arxiv.org/abs/2505.08847", "authors": ["Fatima Ezzeddine", "Rinad Akel", "Ihab Sbeity", "Silvia Giordano", "Marc Langheinrich", "Omran Ayoub"], "title": "On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Machine Learning as a Service (MLaaS) has gained important attraction as a\nmeans for deploying powerful predictive models, offering ease of use that\nenables organizations to leverage advanced analytics without substantial\ninvestments in specialized infrastructure or expertise. However, MLaaS\nplatforms must be safeguarded against security and privacy attacks, such as\nmodel extraction (MEA) attacks. The increasing integration of explainable AI\n(XAI) within MLaaS has introduced an additional privacy challenge, as attackers\ncan exploit model explanations particularly counterfactual explanations (CFs)\nto facilitate MEA. In this paper, we investigate the trade offs among model\nperformance, privacy, and explainability when employing Differential Privacy\n(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two\ndistinct DP strategies: implemented during the classification model training\nand at the explainer during CF generation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\uff08MLaaS\uff09\u5e73\u53f0\u4e2d\uff0c\u4f7f\u7528\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u6280\u672f\u5728\u6a21\u578b\u6027\u80fd\u3001\u9690\u79c1\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "MLaaS\u5e73\u53f0\u9700\u8981\u9632\u8303\u5b89\u5168\u548c\u9690\u79c1\u653b\u51fb\uff0c\u5982\u6a21\u578b\u63d0\u53d6\u653b\u51fb\uff08MEA\uff09\u3002XAI\u7684\u5f15\u5165\u589e\u52a0\u4e86\u9690\u79c1\u98ce\u9669\uff0c\u56e0\u4e3a\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u6a21\u578b\u89e3\u91ca\uff0c\u7279\u522b\u662f\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFs\uff09\u6765\u4fc3\u8fdbMEA\u3002", "method": "\u8bc4\u4f30\u4e24\u79cd\u4e0d\u540c\u7684DP\u7b56\u7565\uff1a\u5728\u5206\u7c7b\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u5b9e\u65bd\u548c\u5728\u751f\u6210CF\u65f6\u5728\u89e3\u91ca\u5668\u4e2d\u5b9e\u65bd\u3002", "result": "\u7814\u7a76\u4e86DP\u5728\u7f13\u89e3CF\u4fc3\u6210\u7684MEA\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u3001\u9690\u79c1\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "DP\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u5728MLaaS\u4e2d\u5e73\u8861\u6a21\u578b\u6027\u80fd\u3001\u9690\u79c1\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.09366", "pdf": "https://arxiv.org/pdf/2505.09366", "abs": "https://arxiv.org/abs/2505.09366", "authors": ["SeyedMojtaba Mohasel", "Alireza Afzal Aghaei", "Corey Pew"], "title": "Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks", "categories": ["cs.LG"], "comment": null, "summary": "Objective: This paper investigates the potential of learnable activation\nfunctions in Kolmogorov-Arnold Networks (KANs) for personalized control in a\nlower-limb prosthesis. In addition, user-specific vs. pooled training data is\nevaluated to improve machine learning (ML) and Deep Learning (DL) performance\nfor turn intent prediction.\n  Method: Inertial measurement unit (IMU) data from the shank were collected\nfrom five individuals with lower-limb amputation performing turning tasks in a\nlaboratory setting. Ability to classify an upcoming turn was evaluated for\nMultilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional\nneural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The\ncomparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)\nassessed the effectiveness of learnable activation functions. Models were\ntrained separately on user-specific and pooled data to evaluate the impact of\ntraining data on their performance.\n  Results: Learnable activation functions in KAN and FKAN did not yield\nsignificant improvement compared to MLP and CNN, respectively. Training on\nuser-specific data yielded superior results compared to pooled data for ML\nmodels ($p < 0.05$). In contrast, no significant difference was observed\nbetween user-specific and pooled training for DL models.\n  Significance: These findings suggest that learnable activation functions may\ndemonstrate distinct advantages in datasets involving more complex tasks and\nlarger volumes. In addition, pooled training showed comparable performance to\nuser-specific training in DL models, indicating that model training for\nprosthesis control can utilize data from multiple participants.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u7528\u4e8e\u4e0b\u80a2\u5047\u80a2\u7684\u4e2a\u6027\u5316\u63a7\u5236\u3002\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728\u66f4\u590d\u6742\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u53ef\u80fd\u5177\u6709\u4f18\u52bf\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6df7\u5408\u8bad\u7ec3\u6570\u636e\u8868\u73b0\u4e0e\u4e2a\u4f53\u7279\u5b9a\u6570\u636e\u76f8\u5f53\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u7528\u4e8e\u4e0b\u80a2\u5047\u80a2\u7684\u4e2a\u6027\u5316\u63a7\u5236\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u7528\u6237\u7279\u5b9a\u4e0e\u6df7\u5408\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5728\u8f6c\u5f2f\u610f\u56fe\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u6536\u96c6\u4e86\u4e94\u540d\u4e0b\u80a2\u622a\u80a2\u8005\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u6267\u884c\u8f6c\u5f2f\u4efb\u52a1\u7684\u80eb\u9aa8\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u6570\u636e\u3002\u8bc4\u4f30\u4e86\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u3001Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u5206\u6570Kolmogorov-Arnold\u7f51\u7edc\uff08FKAN\uff09\u5bf9\u5373\u5c06\u53d1\u751f\u7684\u8f6c\u5f2f\u7684\u5206\u7c7b\u80fd\u529b\u3002\u6bd4\u8f83\u4e86MLP\u548cKAN\uff08\u5bf9\u4e8eML\u6a21\u578b\uff09\u4ee5\u53caFKAN\u548cCNN\uff08\u5bf9\u4e8eDL\u6a21\u578b\uff09\u4ee5\u8bc4\u4f30\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u7684\u6709\u6548\u6027\u3002\u6a21\u578b\u5206\u522b\u4f7f\u7528\u4e2a\u4f53\u7279\u5b9a\u548c\u6df7\u5408\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u8bc4\u4f30\u8bad\u7ec3\u6570\u636e\u5bf9\u5176\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "KAN\u548cFKAN\u4e2d\u7684\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u6ca1\u6709\u663e\u8457\u4f18\u4e8eMLP\u548cCNN\u3002\u5bf9\u4e8eML\u6a21\u578b\uff0c\u4f7f\u7528\u4e2a\u4f53\u7279\u5b9a\u6570\u636e\u8bad\u7ec3\u7684\u7ed3\u679c\u4f18\u4e8e\u6df7\u5408\u6570\u636e\uff08p < 0.05\uff09\u3002\u76f8\u53cd\uff0c\u5bf9\u4e8eDL\u6a21\u578b\uff0c\u4e2a\u4f53\u7279\u5b9a\u548c\u6df7\u5408\u8bad\u7ec3\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u53ef\u5b66\u4e60\u7684\u6fc0\u6d3b\u51fd\u6570\u53ef\u80fd\u5728\u6d89\u53ca\u66f4\u590d\u6742\u4efb\u52a1\u548c\u66f4\u5927\u6570\u636e\u91cf\u7684\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u72ec\u7279\u7684\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6df7\u5408\u8bad\u7ec3\u6570\u636e\u7684\u8868\u73b0\u4e0e\u4e2a\u4f53\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u76f8\u5f53\uff0c\u8fd9\u8868\u660e\u53ef\u4ee5\u5229\u7528\u591a\u4e2a\u53c2\u4e0e\u8005\u7684\u6570\u636e\u8fdb\u884c\u5047\u80a2\u63a7\u5236\u6a21\u578b\u7684\u8bad\u7ec3\u3002"}}
{"id": "2505.09358", "pdf": "https://arxiv.org/pdf/2505.09358", "abs": "https://arxiv.org/abs/2505.09358", "authors": ["Bingxin Ke", "Kevin Qu", "Tianfu Wang", "Nando Metzger", "Shengyu Huang", "Bo Li", "Anton Obukhov", "Konrad Schindler"], "title": "Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "Journal extension of our CVPR 2024 paper, featuring new tasks,\n  improved efficiency, high-resolution capabilities, and enhanced accessibility", "summary": "The success of deep learning in computer vision over the past decade has\nhinged on large labeled datasets and strong pretrained models. In data-scarce\nsettings, the quality of these pretrained models becomes crucial for effective\ntransfer learning. Image classification and self-supervised learning have\ntraditionally been the primary methods for pretraining CNNs and\ntransformer-based architectures. Recently, the rise of text-to-image generative\nmodels, particularly those using denoising diffusion in a latent space, has\nintroduced a new class of foundational models trained on massive, captioned\nimage datasets. These models' ability to generate realistic images of unseen\ncontent suggests they possess a deep understanding of the visual world. In this\nwork, we present Marigold, a family of conditional generative models and a\nfine-tuning protocol that extracts the knowledge from pretrained latent\ndiffusion models like Stable Diffusion and adapts them for dense image analysis\ntasks, including monocular depth estimation, surface normals prediction, and\nintrinsic decomposition. Marigold requires minimal modification of the\npre-trained latent diffusion model's architecture, trains with small synthetic\ndatasets on a single GPU over a few days, and demonstrates state-of-the-art\nzero-shot generalization. Project page:\nhttps://marigoldcomputervision.github.io", "AI": {"tldr": "Marigold is a method that adapts pretrained latent diffusion models for dense image analysis tasks, achieving state-of-the-art results with minimal modifications.", "motivation": "In data-scarce settings, the quality of pretrained models becomes crucial for effective transfer learning. The work aims to adapt pretrained latent diffusion models for dense image analysis tasks.", "method": "Marigold is a family of conditional generative models and a fine-tuning protocol that extracts knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks.", "result": "Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization.", "conclusion": "Marigold demonstrates state-of-the-art zero-shot generalization by extracting knowledge from pretrained latent diffusion models and adapting them for dense image analysis tasks."}}
{"id": "2505.08849", "pdf": "https://arxiv.org/pdf/2505.08849", "abs": "https://arxiv.org/abs/2505.08849", "authors": ["Keyu Chen", "Hao Tang", "Qinglin Liu", "Yizhao Xu"], "title": "Improved Algorithms for Differentially Private Language Model Alignment", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Language model alignment is crucial for ensuring that large language models\n(LLMs) align with human preferences, yet it often involves sensitive user data,\nraising significant privacy concerns. While prior work has integrated\ndifferential privacy (DP) with alignment techniques, their performance remains\nlimited. In this paper, we propose novel algorithms for privacy-preserving\nalignment and rigorously analyze their effectiveness across varying privacy\nbudgets and models. Our framework can be deployed on two celebrated alignment\ntechniques, namely direct preference optimization (DPO) and reinforcement\nlearning from human feedback (RLHF). Through systematic experiments on\nlarge-scale language models, we demonstrate that our approach achieves\nstate-of-the-art performance. Notably, one of our algorithms, DP-AdamW,\ncombined with DPO, surpasses existing methods, improving alignment quality by\nup to 15% under moderate privacy budgets ({\\epsilon}=2-5). We further\ninvestigate the interplay between privacy guarantees, alignment efficacy, and\ncomputational demands, providing practical guidelines for optimizing these\ntrade-offs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u7684\u65b0\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u5bf9\u4e8e\u786e\u4fdd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u6d89\u53ca\u654f\u611f\u7528\u6237\u6570\u636e\uff0c\u5f15\u53d1\u91cd\u5927\u9690\u79c1\u95ee\u9898\u3002\u5c3d\u7ba1\u4e4b\u524d\u7684\u5de5\u4f5c\u5df2\u5c06\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4e0e\u5bf9\u9f50\u6280\u672f\u7ed3\u5408\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u6709\u9650\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u66f4\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548c\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e24\u79cd\u8457\u540d\u7684\u5bf9\u9f50\u6280\u672f\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u7279\u522b\u662fDP-AdamW\u7b97\u6cd5\u7ed3\u5408DPO\uff0c\u5728\u4e2d\u7b49\u9690\u79c1\u9884\u7b97\u4e0b\uff08\u03b5=2-5\uff09\u63d0\u9ad8\u4e86\u5bf9\u9f50\u8d28\u91cf\u8fbe15%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u9690\u79c1\u9884\u7b97\u548c\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u5206\u6790\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u4f18\u5316\u9690\u79c1\u4fdd\u8bc1\u3001\u5bf9\u9f50\u6548\u679c\u548c\u8ba1\u7b97\u9700\u6c42\u4e4b\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2505.09427", "pdf": "https://arxiv.org/pdf/2505.09427", "abs": "https://arxiv.org/abs/2505.09427", "authors": ["Achref Doula", "Max M\u00fchl\u00e4user", "Alejandro Sanchez Guinea"], "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.", "AI": {"tldr": "SafePath is a framework that enhances the safety of LLM-based path planning by integrating conformal prediction, reducing planning uncertainty and collision rates significantly.", "motivation": "Large Language Models (LLMs) show promise in autonomous driving but face issues of overconfidence and hallucinations, which raise safety concerns. SafePath aims to address these issues by providing formal safety guarantees for LLM-based path planning.", "method": "SafePath is a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. It operates in three stages: generating diverse candidate paths, filtering out high-risk trajectories with a user-defined probability, and selecting the safest path or delegating control to a human based on uncertainty.", "result": "SafePath reduces planning uncertainty by 77% and collision rates by up to 70% on datasets like nuScenes and Highway-env, proving its effectiveness in enhancing the safety of LLM-driven path planning.", "conclusion": "SafePath effectively reduces planning uncertainty and collision rates, demonstrating its effectiveness in making LLM-driven path planning safer."}}
{"id": "2505.09368", "pdf": "https://arxiv.org/pdf/2505.09368", "abs": "https://arxiv.org/abs/2505.09368", "authors": ["Jenny Schmalfuss", "Victor Oei", "Lukas Mehl", "Madlen Bartsch", "Shashank Agnihotri", "Margret Keuper", "Andr\u00e9s Bruhn"], "title": "RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Standard benchmarks for optical flow, scene flow, and stereo vision\nalgorithms generally focus on model accuracy rather than robustness to image\ncorruptions like noise or rain. Hence, the resilience of models to such\nreal-world perturbations is largely unquantified. To address this, we present\nRobustSpring, a comprehensive dataset and benchmark for evaluating robustness\nto image corruptions for optical flow, scene flow, and stereo models.\nRobustSpring applies 20 different image corruptions, including noise, blur,\ncolor changes, quality degradations, and weather distortions, in a time-,\nstereo-, and depth-consistent manner to the high-resolution Spring dataset,\ncreating a suite of 20,000 corrupted images that reflect challenging\nconditions. RobustSpring enables comparisons of model robustness via a new\ncorruption robustness metric. Integration with the Spring benchmark enables\npublic two-axis evaluations of both accuracy and robustness. We benchmark a\ncurated selection of initial models, observing that accurate models are not\nnecessarily robust and that robustness varies widely by corruption type.\nRobustSpring is a new computer vision benchmark that treats robustness as a\nfirst-class citizen to foster models that combine accuracy with resilience. It\nwill be available at https://spring-benchmark.org.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RobustSpring\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u6a21\u578b\u5bf9\u56fe\u50cf\u9000\u5316\u7684\u9c81\u68d2\u6027\uff0c\u65e8\u5728\u4fc3\u8fdb\u540c\u65f6\u5177\u5907\u51c6\u786e\u6027\u548c\u97e7\u6027\u7684\u6a21\u578b\u3002", "motivation": "\u6807\u51c6\u7684\u5149\u5b66\u6d41\u3001\u573a\u666f\u6d41\u548c\u7acb\u4f53\u89c6\u89c9\u7b97\u6cd5\u57fa\u51c6\u901a\u5e38\u5173\u6ce8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u800c\u4e0d\u662f\u5bf9\u56fe\u50cf\u9000\u5316\uff08\u5982\u566a\u58f0\u6216\u96e8\uff09\u7684\u9c81\u68d2\u6027\u3002\u56e0\u6b64\uff0c\u6a21\u578b\u5bf9\u8fd9\u4e9b\u73b0\u5b9e\u4e16\u754c\u6270\u52a8\u7684\u5f39\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u91cf\u5316\u3002", "method": "\u63d0\u51faRobustSpring\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5149\u5b66\u6d41\u3001\u573a\u666f\u6d41\u548c\u7acb\u4f53\u6a21\u578b\u5bf9\u56fe\u50cf\u9000\u5316\u7684\u9c81\u68d2\u6027\u3002\u5e94\u752820\u79cd\u4e0d\u540c\u7684\u56fe\u50cf\u9000\u5316\uff0c\u5305\u62ec\u566a\u58f0\u3001\u6a21\u7cca\u3001\u989c\u8272\u53d8\u5316\u3001\u8d28\u91cf\u9000\u5316\u548c\u5929\u6c14\u5931\u771f\uff0c\u5e76\u521b\u5efa\u4e8620,000\u5f20\u9000\u5316\u7684\u56fe\u50cf\u3002", "result": "\u901a\u8fc7\u65b0\u7684\u56fe\u50cf\u9000\u5316\u9c81\u68d2\u6027\u5ea6\u91cf\uff0cRobustSpring\u4f7f\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6bd4\u8f83\u6210\u4e3a\u53ef\u80fd\u3002\u4e0eSpring\u57fa\u51c6\u7684\u96c6\u6210\u4f7f\u5f97\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u7684\u516c\u5171\u53cc\u8f74\u8bc4\u4f30\u6210\u4e3a\u53ef\u80fd\u3002\u57fa\u51c6\u6d4b\u8bd5\u4e86\u4e00\u7ec4\u7cbe\u9009\u7684\u521d\u59cb\u6a21\u578b\uff0c\u89c2\u5bdf\u5230\u51c6\u786e\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u9c81\u68d2\u6027\u56e0\u9000\u5316\u7c7b\u578b\u800c\u5f02\u3002", "conclusion": "RobustSpring\u662f\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u51c6\uff0c\u5c06\u9c81\u68d2\u6027\u4f5c\u4e3a\u9996\u8981\u8003\u8651\u56e0\u7d20\uff0c\u4ee5\u4fc3\u8fdb\u7ed3\u5408\u51c6\u786e\u6027\u548c\u97e7\u6027\u7684\u6a21\u578b\u3002"}}
{"id": "2505.09432", "pdf": "https://arxiv.org/pdf/2505.09432", "abs": "https://arxiv.org/abs/2505.09432", "authors": ["Yuzhou Cao", "Han Bao", "Lei Feng", "Bo An"], "title": "Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Surrogate regret bounds, also known as excess risk bounds, bridge the gap\nbetween the convergence rates of surrogate and target losses, with linear\nbounds favorable for their lossless regret transfer. While convex smooth\nsurrogate losses are appealing in particular due to the efficient estimation\nand optimization, the existence of a trade-off between the smoothness and\nlinear regret bound has been believed in the community. That being said, the\nbetter optimization and estimation properties of convex smooth surrogate losses\nmay inevitably deteriorate after undergoing the regret transfer onto a target\nloss. We overcome this dilemma for arbitrary discrete target losses by\nconstructing a convex smooth surrogate loss, which entails a linear surrogate\nregret bound composed with a tailored prediction link. The construction is\nbased on Fenchel-Young losses generated by the convolutional negentropy, which\nare equivalent to the infimal convolution of a generalized negentropy and the\ntarget Bayes risk. Consequently, the infimal convolution enables us to derive a\nsmooth loss while maintaining the surrogate regret bound linear. We\nadditionally benefit from the infimal convolution to have a consistent\nestimator of the underlying class probability. Our results are overall a novel\ndemonstration of how convex analysis penetrates into optimization and\nstatistical efficiency in risk minimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u6784\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7Fenchel-Young\u635f\u5931\u548c\u4e0b\u786e\u754c\u5377\u79ef\u5b9e\u73b0\u4e86\u7ebf\u6027\u4ee3\u7406\u9057\u61be\u754c\uff0c\u4ece\u800c\u514b\u670d\u4e86\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u5728\u9057\u61be\u8f6c\u79fb\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u5728\u4f18\u5316\u548c\u4f30\u8ba1\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u6027\u8d28\uff0c\u4f46\u5728\u8fdb\u884c\u9057\u61be\u8f6c\u79fb\u540e\u53ef\u80fd\u4f1a\u6076\u5316\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u56f0\u5883\uff0c\u4e3a\u4efb\u610f\u79bb\u6563\u76ee\u6807\u635f\u5931\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u4ee3\u7406\u635f\u5931\u6784\u9020\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u57fa\u4e8eFenchel-Young\u635f\u5931\uff0c\u5229\u7528\u5377\u79ef\u8d1f\u71b5\u751f\u6210\u4ee3\u7406\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u76ee\u6807\u8d1d\u53f6\u65af\u98ce\u9669\u7684\u4e0b\u786e\u754c\u5377\u79ef\u6765\u5b9e\u73b0\u7ebf\u6027\u4ee3\u7406\u9057\u61be\u754c\u3002", "result": "\u672c\u6587\u6784\u9020\u4e86\u4e00\u79cd\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\uff0c\u8be5\u635f\u5931\u5177\u6709\u7ebf\u6027\u4ee3\u7406\u9057\u61be\u754c\uff0c\u5e76\u4e14\u80fd\u591f\u4fdd\u6301\u4e00\u81f4\u7684\u7c7b\u6982\u7387\u4f30\u8ba1\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u51f8\u5206\u6790\u5728\u98ce\u9669\u6700\u5c0f\u5316\u4e2d\u6e17\u900f\u4f18\u5316\u548c\u7edf\u8ba1\u6548\u7387\uff0c\u4e3a\u4efb\u610f\u79bb\u6563\u76ee\u6807\u635f\u5931\u63d0\u4f9b\u4e86\u5177\u6709\u7ebf\u6027\u4ee3\u7406\u9057\u61be\u754c\u7684\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u3002"}}
{"id": "2505.09372", "pdf": "https://arxiv.org/pdf/2505.09372", "abs": "https://arxiv.org/abs/2505.09372", "authors": ["Siyuan Yan", "Xieji Li", "Ming Hu", "Yiwen Jiang", "Zhen Yu", "Zongyuan Ge"], "title": "MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment", "categories": ["cs.CV"], "comment": "MICCAI2025 early acceptance; First two authors contribute equally", "summary": "Dermatological diagnosis represents a complex multimodal challenge that\nrequires integrating visual features with specialized clinical knowledge. While\nvision-language pretraining (VLP) has advanced medical AI, its effectiveness in\ndermatology is limited by text length constraints and the lack of structured\ntexts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced\nvision-language pretraining framework for zero-shot dermatological tasks.\nRecognizing that comprehensive dermatological descriptions require multiple\nknowledge aspects that exceed standard text constraints, our framework\nintroduces: (1) a multi-aspect contrastive learning strategy that decomposes\nclinical narratives into knowledge-enhanced sub-texts through large language\nmodels, (2) a fine-grained alignment mechanism that connects subcaptions with\ndiagnostically relevant image features, and (3) a diagnosis-guided weighting\nscheme that adaptively prioritizes different sub-captions based on clinical\nsignificance prior. Through pretraining on 403,563 dermatological image-text\npairs collected from education resources, MAKE significantly outperforms\nstate-of-the-art VLP models on eight datasets across zero-shot skin disease\nclassification, concept annotation, and cross-modal retrieval tasks. Our code\nwill be made publicly available at https: //github.com/SiyuanYan1/MAKE.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MAKE\uff0c\u4e00\u79cd\u7528\u4e8e\u96f6\u6837\u672c\u76ae\u80a4\u75c5\u4efb\u52a1\u7684\u591a\u65b9\u9762\u77e5\u8bc6\u589e\u5f3a\u7684\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u65b9\u9762\u5bf9\u6bd4\u5b66\u4e60\u3001\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u8bca\u65ad\u5f15\u5bfc\u52a0\u6743\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u76ae\u80a4\u75c5\u5b66\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u76ae\u80a4\u75c5\u5b66\u4e2d\u7684\u6709\u6548\u6027\u53d7\u5230\u6587\u672c\u957f\u5ea6\u9650\u5236\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u6587\u672c\u7684\u9650\u5236\u3002\u5168\u9762\u7684\u76ae\u80a4\u75c5\u63cf\u8ff0\u9700\u8981\u591a\u4e2a\u77e5\u8bc6\u65b9\u9762\uff0c\u8fd9\u4e9b\u65b9\u9762\u8d85\u51fa\u4e86\u6807\u51c6\u6587\u672c\u7ea6\u675f\u3002", "method": "MAKE\u6846\u67b6\u5f15\u5165\u4e86\u591a\u65b9\u9762\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3001\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u673a\u5236\u548c\u8bca\u65ad\u5f15\u5bfc\u52a0\u6743\u65b9\u6848\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u4e34\u5e8a\u53d9\u8ff0\u5206\u89e3\u4e3a\u589e\u5f3a\u77e5\u8bc6\u7684\u5b50\u6587\u672c\uff0c\u5e76\u5c06\u5b50\u6807\u9898\u4e0e\u8bca\u65ad\u76f8\u5173\u7684\u56fe\u50cf\u7279\u5f81\u5bf9\u9f50\uff0c\u540c\u65f6\u6839\u636e\u4e34\u5e8a\u91cd\u8981\u6027\u4f18\u5148\u7ea7\u81ea\u9002\u5e94\u5730\u4f18\u5148\u8003\u8651\u4e0d\u540c\u7684\u5b50\u6807\u9898\u3002", "result": "MAKE\u5728\u96f6\u6837\u672c\u76ae\u80a4\u75be\u75c5\u5206\u7c7b\u3001\u6982\u5ff5\u6ce8\u91ca\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u7684\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "MAKE\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u76ae\u80a4\u75be\u75c5\u5206\u7c7b\u3001\u6982\u5ff5\u6ce8\u91ca\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u7684\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.08878", "pdf": "https://arxiv.org/pdf/2505.08878", "abs": "https://arxiv.org/abs/2505.08878", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Haim Permuter", "Flavio P. Calmon"], "title": "Optimized Couplings for Watermarking Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.IT", "math.IT"], "comment": "Accepted at ISIT25", "summary": "Large-language models (LLMs) are now able to produce text that is, in many\ncases, seemingly indistinguishable from human-generated content. This has\nfueled the development of watermarks that imprint a ``signal'' in LLM-generated\ntext with minimal perturbation of an LLM's output. This paper provides an\nanalysis of text watermarking in a one-shot setting. Through the lens of\nhypothesis testing with side information, we formulate and analyze the\nfundamental trade-off between watermark detection power and distortion in\ngenerated textual quality. We argue that a key component in watermark design is\ngenerating a coupling between the side information shared with the watermark\ndetector and a random partition of the LLM vocabulary. Our analysis identifies\nthe optimal coupling and randomization strategy under the worst-case LLM\nnext-token distribution that satisfies a min-entropy constraint. We provide a\nclosed-form expression of the resulting detection rate under the proposed\nscheme and quantify the cost in a max-min sense. Finally, we provide an array\nof numerical results, comparing the proposed scheme with the theoretical\noptimum and existing schemes, in both synthetic data and LLM watermarking. Our\ncode is available at https://github.com/Carol-Long/CC_Watermark", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6587\u672c\u6c34\u5370\u5728\u68c0\u6d4b\u80fd\u529b\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u7684\u6c34\u5370\u8bbe\u8ba1\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u7387\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u6587\u672c\u8d8a\u6765\u8d8a\u63a5\u8fd1\u4eba\u7c7b\u751f\u6210\u7684\u5185\u5bb9\uff0c\u6c34\u5370\u6280\u672f\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u80fd\u529b\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u7f3a\u4e4f\u7406\u8bba\u4e0a\u7684\u5206\u6790\u548c\u4f18\u5316\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u4e0e\u8f85\u52a9\u4fe1\u606f\u7684\u89c6\u89d2\uff0c\u5206\u6790\u4e86\u6587\u672c\u6c34\u5370\u5728\u68c0\u6d4b\u80fd\u529b\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u6c34\u5370\u8bbe\u8ba1\u7b56\u7565\uff0c\u8003\u8651\u4e86\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8bed\u8a00\u6a21\u578b\u4e0b\u4e00\u4e2a\u8bcd\u5206\u5e03\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u6c34\u5370\u8bbe\u8ba1\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u7387\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u672c\u6587\u5206\u6790\u4e86\u5728\u4e00\u6b21\u6027\u8bbe\u7f6e\u4e0b\u6587\u672c\u6c34\u5370\u7684\u7406\u8bba\u6781\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u7684\u6c34\u5370\u8bbe\u8ba1\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u7387\u548c\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2505.09379", "pdf": "https://arxiv.org/pdf/2505.09379", "abs": "https://arxiv.org/abs/2505.09379", "authors": ["Ali Rida Sahili", "Najett Neji", "Hedi Tabia"], "title": "Text-driven Motion Generation: Overview, Challenges and Directions", "categories": ["cs.CV"], "comment": "17 pages, 5 tables", "summary": "Text-driven motion generation offers a powerful and intuitive way to create\nhuman movements directly from natural language. By removing the need for\npredefined motion inputs, it provides a flexible and accessible approach to\ncontrolling animated characters. This makes it especially useful in areas like\nvirtual reality, gaming, human-computer interaction, and robotics. In this\nreview, we first revisit the traditional perspective on motion synthesis, where\nmodels focused on predicting future poses from observed initial sequences,\noften conditioned on action labels. We then provide a comprehensive and\nstructured survey of modern text-to-motion generation approaches, categorizing\nthem from two complementary perspectives: (i) architectural, dividing methods\ninto VAE-based, diffusion-based, and hybrid models; and (ii) motion\nrepresentation, distinguishing between discrete and continuous motion\ngeneration strategies. In addition, we explore the most widely used datasets,\nevaluation methods, and recent benchmarks that have shaped progress in this\narea. With this survey, we aim to capture where the field currently stands,\nbring attention to its key challenges and limitations, and highlight promising\ndirections for future exploration. We hope this work offers a valuable starting\npoint for researchers and practitioners working to push the boundaries of\nlanguage-driven human motion synthesis.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6db5\u76d6\u4e86\u65b9\u6cd5\u5206\u7c7b\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u76f4\u89c2\u7684\u65b9\u5f0f\u6765\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u521b\u5efa\u4eba\u7c7b\u8fd0\u52a8\uff0c\u8fd9\u4f7f\u5f97\u5b83\u5728\u865a\u62df\u73b0\u5b9e\u3001\u6e38\u620f\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u7279\u522b\u6709\u7528\u3002", "method": "\u672c\u6587\u9996\u5148\u56de\u987e\u4e86\u4f20\u7edf\u7684\u8fd0\u52a8\u5408\u6210\u89c2\u70b9\uff0c\u7136\u540e\u5bf9\u73b0\u4ee3\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u4e14\u7ed3\u6784\u5316\u7684\u8c03\u67e5\uff0c\u4ece\u4e24\u4e2a\u4e92\u8865\u7684\u89d2\u5ea6\u8fdb\u884c\u5206\u7c7b\uff1a(i) \u67b6\u6784\uff0c\u5c06\u65b9\u6cd5\u5206\u4e3a\u57fa\u4e8eVAE\u3001\u57fa\u4e8e\u6269\u6563\u548c\u6df7\u5408\u6a21\u578b\uff1b(ii) \u8fd0\u52a8\u8868\u793a\uff0c\u533a\u5206\u79bb\u6563\u548c\u8fde\u7eed\u8fd0\u52a8\u751f\u6210\u7b56\u7565\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u7684\u5168\u9762\u8c03\u67e5\uff0c\u5305\u62ec\u5e38\u7528\u7684\u6570\u636e\u5e93\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u6700\u8fd1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u603b\u7ed3\u5f53\u524d\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6307\u51fa\u5176\u5173\u952e\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2505.08894", "pdf": "https://arxiv.org/pdf/2505.08894", "abs": "https://arxiv.org/abs/2505.08894", "authors": ["Hiba Eltigani", "Rukhshan Haroon", "Asli Kocak", "Abdullah Bin Faisal", "Noah Martin", "Fahad Dogar"], "title": "WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in generative AI, such as ChatGPT, have transformed access to\ninformation in education, knowledge-seeking, and everyday decision-making.\nHowever, in many developing regions, access remains a challenge due to the\npersistent digital divide. To help bridge this gap, we developed WaLLM - a\ncustom AI chatbot over WhatsApp, a widely used communication platform in\ndeveloping regions. Beyond answering queries, WaLLM offers several features to\nenhance user engagement: a daily top question, suggested follow-up questions,\ntrending and recent queries, and a leaderboard-based reward system. Our service\nhas been operational for over 6 months, amassing over 14.7K queries from\napproximately 100 users. In this paper, we present WaLLM's design and a\nsystematic analysis of logs to understand user interactions. Our results show\nthat 55% of user queries seek factual information. \"Health and well-being\" was\nthe most popular topic (28%), including queries about nutrition and disease,\nsuggesting users view WaLLM as a reliable source. Two-thirds of users' activity\noccurred within 24 hours of the daily top question. Users who accessed the\n\"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by\ndiscussing implications for culture-based customization, user interface design,\nand appropriate calibration of users' trust in AI systems for developing\nregions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.09458", "pdf": "https://arxiv.org/pdf/2505.09458", "abs": "https://arxiv.org/abs/2505.09458", "authors": ["Jad Mounayer", "Alicia Tierz", "Jerome Tomezyk", "Chady Ghnatios", "Francisco Chinesta"], "title": "Variational Rank Reduction Autoencoder", "categories": ["cs.LG"], "comment": null, "summary": "Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a\nregularization on the latent space by applying a truncated SVD. While this\nregularization makes Autoencoders more powerful, using them for generative\npurposes is counter-intuitive due to their deterministic nature. On the other\nhand, Variational Autoencoders (VAEs) are well known for their generative\nabilities by learning a probabilistic latent space. In this paper, we present\nVariational Rank Reduction Autoencoders (VRRAEs), a model that leverages the\nadvantages of both RRAEs and VAEs. Our claims and results show that when\ncarefully sampling the latent space of RRAEs and further regularizing with the\nKullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs\nand VAEs. Additionally, we show that the regularization induced by the SVD not\nonly makes VRRAEs better generators than VAEs, but also reduces the possibility\nof posterior collapse. Our results include a synthetic dataset of a small size\nthat showcases the robustness of VRRAEs against collapse, and three real-world\ndatasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to\noutperform both VAEs and RRAEs on many random generation and interpolation\ntasks based on the FID score.", "AI": {"tldr": "VRRAEs combine the advantages of RRAEs and VAEs, leading to improved generative performance and reduced posterior collapse.", "motivation": "The deterministic nature of RRAEs makes them less suitable for generative purposes, while VAEs are known for their generative abilities. The paper aims to combine the strengths of both models.", "method": "VRRAEs combine the advantages of RRAEs and VAEs by carefully sampling the latent space of RRAEs and regularizing with the Kullback-Leibler (KL) divergence.", "result": "VRRAEs outperform RRAEs and VAEs on many random generation and interpolation tasks based on the FID score, and show robustness against collapse on a synthetic dataset and real-world datasets like MNIST, CelebA, and CIFAR-10.", "conclusion": "VRRAEs outperform RRAEs and VAEs, and are better generators than VAEs while reducing the possibility of posterior collapse."}}
{"id": "2505.09380", "pdf": "https://arxiv.org/pdf/2505.09380", "abs": "https://arxiv.org/abs/2505.09380", "authors": ["Qinghui Liu", "Jon Nesvold", "Hanna Raaum", "Elakkyen Murugesu", "Martin R\u00f8vang", "Bradley J Maclntosh", "Atle Bj\u00f8rnerud", "Karoline Skogen"], "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 11 figures, on submission to BMC Methods", "summary": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A pragmatic investigation was\ndeployed using clinical cases of patients presenting to the largest Emergency\nDepartment in Norway (site-1) with suspected traumatic brain injury (TBI) or\npatients with suspected stroke (site-2). We assessed ICH classification\nperformance as VIOLA-AI encountered new data and underwent pre-planned model\nretraining. Performance metrics included sensitivity, specificity, accuracy,\nand the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback.", "AI": {"tldr": "\u7814\u7a76\u63cf\u8ff0\u4e86\u4e00\u4e2a\u540d\u4e3aNeoMedSys\u7684\u653e\u5c04\u5b66\u8f6f\u4ef6\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u53ef\u4ee5\u5b9e\u73b0AI\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u4f18\u5316\u3002\u901a\u8fc7\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u8fd0\u884cNeoMedSys\u4e09\u4e2a\u6708\uff0c\u8bc4\u4f30\u4e86\u5176\u53ef\u884c\u6027\u4e0e\u6548\u679c\uff0c\u5e76\u4e13\u6ce8\u4e8e\u6539\u8fdb\u4e00\u4e2a\u5185\u90e8\u5f00\u53d1\u7684\u7528\u4e8e\u9885\u5185\u51fa\u8840\u68c0\u6d4b\u7684AI\u6a21\u578b\uff08VIOLA-AI\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0cNeoMedSys\u4fc3\u8fdb\u4e86AI\u6a21\u578b\u7684\u8fed\u4ee3\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models.", "method": "NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway with suspected traumatic brain injury or patients with suspected stroke. ICH classification performance was assessed as VIOLA-AI encountered new data and underwent pre-planned model retraining.", "result": "NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).", "conclusion": "NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. The iterative refinement process yielded a marked improvement in classification sensitivity and specificity, demonstrating the value of real-time radiologist feedback."}}
{"id": "2505.09486", "pdf": "https://arxiv.org/pdf/2505.09486", "abs": "https://arxiv.org/abs/2505.09486", "authors": ["Seyed Roozbeh Razavi Rohani", "Khashayar Khajavi", "Wesley Chung", "Mo Chen", "Sharan Vaswani"], "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity Injection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in 4th Conference on Lifelong Learning Agents (CoLLAs), 2025", "summary": "Loss of plasticity in deep neural networks is the gradual reduction in a\nmodel's capacity to incrementally learn and has been identified as a key\nobstacle to learning in non-stationary problem settings. Recent work has shown\nthat deep linear networks tend to be resilient towards loss of plasticity.\nMotivated by this observation, we propose Adaptive Linearization (AdaLin), a\ngeneral approach that dynamically adapts each neuron's activation function to\nmitigate plasticity loss. Unlike prior methods that rely on regularization or\nperiodic resets, AdaLin equips every neuron with a learnable parameter and a\ngating mechanism that injects linearity into the activation function based on\nits gradient flow. This adaptive modulation ensures sufficient gradient signal\nand sustains continual learning without introducing additional hyperparameters\nor requiring explicit task boundaries. When used with conventional activation\nfunctions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can\nsignificantly improve performance on standard benchmarks, including Random\nLabel and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split\nCIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such\nas class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in\nmitigating plasticity loss in off-policy reinforcement learning agents. We\nperform a systematic set of ablations that show that neuron-level adaptation is\ncrucial for good performance and analyze a number of metrics in the network\nthat might be correlated to loss of plasticity.", "AI": {"tldr": "AdaLin is a new approach for mitigating plasticity loss in deep neural networks by dynamically adapting each neuron's activation function.", "motivation": "The loss of plasticity in deep neural networks is a key obstacle to learning in non-stationary problem settings, and recent work has shown that deep linear networks tend to be resilient towards loss of plasticity.", "method": "AdaLin is a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss by equipping every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow.", "result": "AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. It is also effective in more complex scenarios such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone and mitigating plasticity loss in off-policy reinforcement learning agents.", "conclusion": "AdaLin can significantly improve performance on standard benchmarks and is effective in complex scenarios such as class-incremental learning and mitigating plasticity loss in off-policy reinforcement learning agents."}}
{"id": "2505.09385", "pdf": "https://arxiv.org/pdf/2505.09385", "abs": "https://arxiv.org/abs/2505.09385", "authors": ["Xiaoyang Yu", "Xiaoming Wu", "Xin Wang", "Dongrun Li", "Ming Yang", "Peng Cheng"], "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5206\u5272\u6846\u67b6FedSaaS\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u6837\u672c\u4f5c\u4e3a\u6807\u51c6\uff0c\u5e76\u5728\u670d\u52a1\u5668\u548c\u5ba2\u6237\u7aef\u91c7\u7528\u5bf9\u6297\u673a\u5236\u548c\u591a\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u901a\u5e38\u5ffd\u89c6\u4e86\u8bed\u4e49\u7a7a\u95f4\u5185\u7684\u7ec6\u7c92\u5ea6\u7c7b\u522b\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u9886\u57df\u8f6c\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u5ffd\u89c6\u5bfc\u81f4\u4e86\u7c7b\u522b\u8868\u793a\u4e4b\u95f4\u7684\u6a21\u7cca\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FedSaaS\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5206\u5272\u6846\u67b6\uff0c\u79f0\u4e3aFedSaaS\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u6837\u672c\u4f5c\u4e3a\u672c\u5730\u548c\u5168\u5c40\u7ea7\u522b\u7c7b\u8868\u793a\u7684\u6807\u51c6\uff0c\u4ee5\u53ca\u5728\u670d\u52a1\u5668\u7aef\u5229\u7528\u4e0a\u4f20\u7684\u7c7b\u6837\u672c\u5efa\u6a21\u7c7b\u539f\u578b\uff0c\u76d1\u7763\u5ba2\u6237\u7aef\u7684\u5168\u5c40\u5206\u652f\uff0c\u786e\u4fdd\u4e0e\u5168\u5c40\u7ea7\u522b\u8868\u793a\u5bf9\u9f50\u3002\u5728\u5ba2\u6237\u7aef\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5bf9\u6297\u673a\u5236\u6765\u534f\u8c03\u5168\u5c40\u548c\u5c40\u90e8\u5206\u652f\u7684\u8d21\u732e\uff0c\u4ece\u800c\u5b9e\u73b0\u4e00\u81f4\u7684\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u5728\u4e24\u4fa7\u90fd\u91c7\u7528\u4e86\u591a\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u5728\u76f8\u540c\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5f3a\u5236\u4e24\u4e2a\u7ea7\u522b\u8868\u793a\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u9a7e\u9a76\u573a\u666f\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u5206\u5272\u51c6\u786e\u7387\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u522b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u591a\u4e2a\u9a7e\u9a76\u573a\u666f\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u5206\u5272\u51c6\u786e\u7387\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\u3002"}}
{"id": "2505.08904", "pdf": "https://arxiv.org/pdf/2505.08904", "abs": "https://arxiv.org/abs/2505.08904", "authors": ["Varun Nagaraj Rao", "Samantha Dalal", "Andrew Schwartz", "Amna Liaqat", "Dana Calacci", "Andr\u00e9s Monroy-Hern\u00e1ndez"], "title": "FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "What happens when a rideshare driver is suddenly locked out of the platform\nconnecting them to riders, wages, and daily work? Deactivation-the abrupt\nremoval of gig workers' platform access-typically occurs through arbitrary AI\nand algorithmic decisions with little explanation or recourse. This represents\none of the most severe forms of algorithmic control and often devastates\nworkers' financial stability. Recent U.S. state policies now mandate appeals\nprocesses and recovering compensation during the period of wrongful\ndeactivation based on past earnings. Yet, labor organizers still lack effective\ntools to support these complex, error-prone workflows. We designed FareShare, a\ncomputational tool automating lost wage estimation for deactivated drivers,\nthrough a 6 month partnership with the State of Washington's largest rideshare\nlabor union. Over the following 3 months, our field deployment of FareShare\nregistered 178 account signups. We observed that the tool could reduce lost\nwage calculation time by over 95%, eliminate manual data entry errors, and\nenable legal teams to generate arbitration-ready reports more efficiently.\nBeyond these gains, the deployment also surfaced important socio-technical\nchallenges around trust, consent, and tool adoption in high-stakes labor\ncontexts.", "AI": {"tldr": "FareShare is a tool that automates lost wage estimation for deactivated rideshare drivers, reducing calculation time and errors, but also highlighting challenges around trust and adoption in labor contexts.", "motivation": "Labor organizers lack effective tools to support complex, error-prone workflows related to deactivation of gig workers, which can devastate their financial stability.", "method": "FareShare is a computational tool designed to automate lost wage estimation for deactivated drivers, developed through a 6-month partnership with the State of Washington's largest rideshare labor union.", "result": "FareShare reduced lost wage calculation time by over 95%, eliminated manual data entry errors, and enabled legal teams to generate arbitration-ready reports more efficiently. It registered 178 account signups during its field deployment.", "conclusion": "FareShare demonstrated significant improvements in lost wage calculation for deactivated drivers, but also highlighted socio-technical challenges around trust, consent, and tool adoption in high-stakes labor contexts."}}
{"id": "2505.09500", "pdf": "https://arxiv.org/pdf/2505.09500", "abs": "https://arxiv.org/abs/2505.09500", "authors": ["Timothy Qian", "Vinith Suriyakumar", "Ashia Wilson", "Dylan Hadfield-Menell"], "title": "Layered Unlearning for Adversarial Relearning", "categories": ["cs.LG"], "comment": "37 pages, 8 figures", "summary": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u540e\u8bad\u7ec3\u65b9\u6cd5\u5982\u4f55\u4fee\u6539\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u548c\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u5206\u5c42\u9057\u5fd8\uff08LU\uff09\u7684\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u6297\u6027\u91cd\u65b0\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6211\u4eec\u60f3\u8981\u7406\u89e3\u540e\u8bad\u7ec3\u65b9\u6cd5\u5982\u4f55\u4fee\u6539\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u548c\u8868\u793a\uff0c\u5e76\u7279\u522b\u5173\u6ce8\u8fd9\u4e9b\u4fee\u6539\u7684\u8106\u5f31\u6027\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5bb9\u6613\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6216\u91cd\u65b0\u5b66\u4e60\u88ab\u7ed5\u8fc7\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u79f0\u4e3a\u5206\u5c42\u9057\u5fd8\uff08LU\uff09\u7684\u9057\u5fd8\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4e3a\u6570\u636e\u7684\u4e00\u4e2a\u589e\u957f\u5b50\u96c6\u521b\u5efa\u4e86\u4e0d\u540c\u7684\u6291\u5236\u673a\u5236\u3002\u901a\u8fc7\u5728\u7b2ci\u4e2a\u9636\u6bb5\u9057\u5fd8\u524di\u4e2a\u6298\u53e0\u800c\u4fdd\u7559\u5176\u4f59k-i\u4e2a\u6298\u53e0\uff0cLU\u9650\u5236\u4e86\u5728\u6570\u636e\u5b50\u96c6\u4e0a\u91cd\u65b0\u5b66\u4e60\u6062\u590d\u5b8c\u6574\u6570\u636e\u96c6\u7684\u80fd\u529b\u3002", "result": "\u6211\u4eec\u53d1\u73b0LU\u63d0\u9ad8\u4e86\u5bf9\u6297\u6027\u91cd\u65b0\u5b66\u4e60\u7684\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u51e0\u79cd\u4e0d\u540c\u7684\u9057\u5fd8\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u4e3a\u673a\u5668\u9057\u5fd8\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u540e\u8bad\u7ec3\u66f4\u65b0\u5f71\u54cd\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.09406", "pdf": "https://arxiv.org/pdf/2505.09406", "abs": "https://arxiv.org/abs/2505.09406", "authors": ["Yue Wen", "Liang Song", "Yijia Liu", "Siting Zhu", "Yanzi Miao", "Lijun Han", "Hesheng Wang"], "title": "FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling", "categories": ["cs.CV"], "comment": "7 pages, 9 figures, accepted by ICRA2025", "summary": "Dynamic scene reconstruction for autonomous driving enables vehicles to\nperceive and interpret complex scene changes more precisely. Dynamic Neural\nRadiance Fields (NeRFs) have recently shown promising capability in scene\nmodeling. However, many existing methods rely heavily on accurate poses inputs\nand multi-sensor data, leading to increased system complexity. To address this,\nwe propose FreeDriveRF, which reconstructs dynamic driving scenes using only\nsequential RGB images without requiring poses inputs. We innovatively decouple\ndynamic and static parts at the early sampling level using semantic\nsupervision, mitigating image blurring and artifacts. To overcome the\nchallenges posed by object motion and occlusion in monocular camera, we\nintroduce a warped ray-guided dynamic object rendering consistency loss,\nutilizing optical flow to better constrain the dynamic modeling process.\nAdditionally, we incorporate estimated dynamic flow to constrain the pose\noptimization process, improving the stability and accuracy of unbounded scene\nreconstruction. Extensive experiments conducted on the KITTI and Waymo datasets\ndemonstrate the superior performance of our method in dynamic scene modeling\nfor autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFreeDriveRF\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ec5\u4f7f\u7528\u987a\u5e8fRGB\u56fe\u50cf\u91cd\u5efa\u52a8\u6001\u9a7e\u9a76\u573a\u666f\uff0c\u65e0\u9700\u59ff\u6001\u8f93\u5165\u3002\u901a\u8fc7\u8bed\u4e49\u76d1\u7763\u89e3\u8026\u52a8\u6001\u548c\u9759\u6001\u90e8\u5206\uff0c\u5e76\u5229\u7528\u5149\u6d41\u548c\u52a8\u6001\u6d41\u6765\u6539\u5584\u52a8\u6001\u5efa\u6a21\u548c\u59ff\u6001\u4f18\u5316\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u8bb8\u591a\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u51c6\u786e\u7684\u59ff\u6001\u8f93\u5165\u548c\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5bfc\u81f4\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u91cd\u5efa\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86FreeDriveRF\uff0c\u5b83\u4ec5\u4f7f\u7528\u987a\u5e8fRGB\u56fe\u50cf\u6765\u91cd\u5efa\u52a8\u6001\u9a7e\u9a76\u573a\u666f\uff0c\u800c\u4e0d\u9700\u8981\u59ff\u6001\u8f93\u5165\u3002\u6211\u4eec\u521b\u65b0\u5730\u5728\u65e9\u671f\u91c7\u6837\u7ea7\u522b\u4f7f\u7528\u8bed\u4e49\u76d1\u7763\u89e3\u8026\u52a8\u6001\u548c\u9759\u6001\u90e8\u5206\uff0c\u51cf\u8f7b\u4e86\u56fe\u50cf\u6a21\u7cca\u548c\u4f2a\u5f71\u3002\u4e3a\u4e86\u514b\u670d\u5355\u76ee\u76f8\u673a\u4e2d\u7684\u7269\u4f53\u8fd0\u52a8\u548c\u906e\u6321\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u626d\u66f2\u5c04\u7ebf\u5f15\u5bfc\u7684\u52a8\u6001\u7269\u4f53\u6e32\u67d3\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5229\u7528\u5149\u6d41\u66f4\u597d\u5730\u7ea6\u675f\u52a8\u6001\u5efa\u6a21\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7ed3\u5408\u4f30\u8ba1\u7684\u52a8\u6001\u6d41\u6765\u7ea6\u675f\u59ff\u6001\u4f18\u5316\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u65e0\u754c\u573a\u666f\u91cd\u5efa\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728KITTI\u548cWaymo\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u3002"}}
{"id": "2505.08916", "pdf": "https://arxiv.org/pdf/2505.08916", "abs": "https://arxiv.org/abs/2505.08916", "authors": ["Chan Le Duc", "Ludovic Brieulle"], "title": "A New Tractable Description Logic under Categorical Semantics", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "Biomedical ontologies contain numerous concept or role names involving\nnegative knowledge such as lacks_part, absence_of. Such a representation with\nlabels rather than logical constructors would not allow a reasoner to interpret\nlacks_part as a kind of negation of has_part. It is known that adding negation\nto the tractable Description Logic (DL) EL allowing for conjunction,\nexistential restriction and concept inclusion makes it intractable since the\nobtained logic includes implicitly disjunction and universal restriction which\ninteract with other constructors. In this paper, we propose a new extension of\nEL with a weakened negation allowing to represent negative knowledge while\nretaining tractability. To this end, we introduce categorical semantics of all\nlogical constructors of the DL SH including EL with disjunction, negation,\nuniversal restriction, role inclusion and transitive roles. The categorical\nsemantics of a logical constructor is usually described as a set of categorical\nproperties referring to several objects without using set membership. To\nrestore tractability, we have to weaken semantics of disjunction and universal\nrestriction by identifying \\emph{independent} categorical properties that are\nresponsible for intractability, and dropping them from the set of categorical\nproperties. We show that the logic resulting from weakening semantics is more\nexpressive than EL with the bottom concept, transitive roles and role\ninclusion.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u4fdd\u6301\u53ef\u89e3\u6027\u7684\u540c\u65f6\u6269\u5c55EL\u4ee5\u8868\u793a\u5426\u5b9a\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u8303\u7574\u8bed\u4e49\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u7269\u533b\u5b66\u672c\u4f53\u4e2d\u5305\u542b\u8bb8\u591a\u6d89\u53ca\u5426\u5b9a\u77e5\u8bc6\u7684\u6982\u5ff5\u6216\u89d2\u8272\u540d\u79f0\uff0c\u4f46\u4f20\u7edf\u7684\u903b\u8f91\u6784\u9020\u65e0\u6cd5\u8ba9\u63a8\u7406\u5668\u6b63\u786e\u89e3\u91ca\u8fd9\u4e9b\u5426\u5b9a\u77e5\u8bc6\u3002", "method": "\u5f15\u5165\u4e86SH\u903b\u8f91\u7684\u6240\u6709\u903b\u8f91\u6784\u9020\u7684\u8303\u7574\u8bed\u4e49\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u5220\u9664\u5bfc\u81f4\u4e0d\u53ef\u89e3\u6027\u7684\u72ec\u7acb\u8303\u7574\u5c5e\u6027\u6765\u6062\u590d\u53ef\u89e3\u6027\u3002", "result": "\u63d0\u51fa\u7684\u903b\u8f91\u6bd4\u5e26\u6709\u5e95\u6982\u5ff5\u3001\u4f20\u9012\u89d2\u8272\u548c\u89d2\u8272\u5305\u542b\u7684EL\u66f4\u5177\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684EL\u6269\u5c55\uff0c\u901a\u8fc7\u5f31\u5316\u5426\u5b9a\u6765\u8868\u793a\u8d1f\u9762\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u6027\u3002"}}
{"id": "2505.09503", "pdf": "https://arxiv.org/pdf/2505.09503", "abs": "https://arxiv.org/abs/2505.09503", "authors": ["Patrik Kenfack", "Samira Ebrahimi Kahou", "Ulrich A\u00efvodji"], "title": "Towards Fair In-Context Learning with Tabular Foundation Models", "categories": ["cs.LG"], "comment": "24 pages, 10 figures, 4 tables", "summary": "Tabular foundational models have exhibited strong in-context learning (ICL)\ncapabilities on structured data, allowing them to make accurate predictions on\ntest sets without parameter updates, using training examples as context. This\nemerging approach positions itself as a competitive alternative to traditional\ngradient-boosted tree methods. However, while biases in conventional machine\nlearning models are well documented, it remains unclear how these biases\nmanifest in tabular ICL. The paper investigates the fairness implications of\ntabular ICL and explores three preprocessing strategies--correlation removal,\ngroup-balanced demonstration selection, and uncertainty-based demonstration\nselection--to address bias. Comprehensive experiments indicate that\nuncertainty-based demonstration selection consistently enhances group fairness\nof in-context predictions. The source code for reproducing the results of this\nwork can be found at https://github.com/patrikken/Fair-TabICL.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8868\u683c\u578bICL\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6f14\u793a\u9009\u62e9\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u7fa4\u4f53\u516c\u5e73\u6027\u3002", "motivation": "\u867d\u7136\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u5df2\u88ab\u5e7f\u6cdb\u8bb0\u5f55\uff0c\u4f46\u8fd9\u4e9b\u504f\u89c1\u5728\u8868\u683c\u578bICL\u4e2d\u7684\u8868\u73b0\u4ecd\u4e0d\u6e05\u695a\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8868\u683c\u578bICL\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u8868\u683c\u578bICL\u7684\u516c\u5e73\u6027\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u4e09\u79cd\u9884\u5904\u7406\u7b56\u7565\uff1a\u76f8\u5173\u6027\u53bb\u9664\u3001\u7ec4\u5e73\u8861\u6f14\u793a\u9009\u62e9\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6f14\u793a\u9009\u62e9\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6f14\u793a\u9009\u62e9\u7b56\u7565\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u7fa4\u4f53\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6f14\u793a\u9009\u62e9\u7b56\u7565\u53ef\u4ee5\u6301\u7eed\u63d0\u9ad8\u4e0a\u4e0b\u6587\u9884\u6d4b\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u3002"}}
{"id": "2505.09413", "pdf": "https://arxiv.org/pdf/2505.09413", "abs": "https://arxiv.org/abs/2505.09413", "authors": ["Ma Changfeng", "Bi Ran", "Guo Jie", "Wang Chongjun", "Guo Yanwen"], "title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians", "categories": ["cs.CV"], "comment": "CVPR 2025 Accepted", "summary": "Current learning-based methods predict NeRF or 3D Gaussians from point clouds\nto achieve photo-realistic rendering but still depend on categorical priors,\ndense point clouds, or additional refinements. Hence, we introduce a novel\npoint cloud rendering method by predicting 2D Gaussians from point clouds. Our\nmethod incorporates two identical modules with an entire-patch architecture\nenabling the network to be generalized to multiple datasets. The module\nnormalizes and initializes the Gaussians utilizing the point cloud information\nincluding normals, colors and distances. Then, splitting decoders are employed\nto refine the initial Gaussians by duplicating them and predicting more\naccurate results, making our methodology effectively accommodate sparse point\nclouds as well. Once trained, our approach exhibits direct generalization to\npoint clouds across different categories. The predicted Gaussians are employed\ndirectly for rendering without additional refinement on the rendered images,\nretaining the benefits of 2D Gaussians. We conduct extensive experiments on\nvarious datasets, and the results demonstrate the superiority and\ngeneralization of our method, which achieves SOTA performance. The code is\navailable at\nhttps://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u70b9\u4e91\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u70b9\u4e91\u9884\u6d4b2D\u9ad8\u65af\uff0c\u65e0\u9700\u989d\u5916\u7ec6\u5316\u5373\u53ef\u76f4\u63a5\u7528\u4e8e\u6e32\u67d3\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4ece\u70b9\u4e91\u9884\u6d4bNeRF\u62163D\u9ad8\u65af\u4ee5\u5b9e\u73b0\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6e32\u67d3\uff0c\u4f46\u4ecd\u4f9d\u8d56\u4e8e\u7c7b\u522b\u5148\u9a8c\u3001\u5bc6\u96c6\u70b9\u4e91\u6216\u989d\u5916\u7684\u7ec6\u5316\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u70b9\u4e91\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u70b9\u4e91\u9884\u6d4b2D\u9ad8\u65af\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u70b9\u4e91\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u70b9\u4e91\u4e2d\u9884\u6d4b2D\u9ad8\u65af\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5177\u6709\u5b8c\u6574\u8865\u4e01\u67b6\u6784\u7684\u76f8\u540c\u6a21\u5757\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u63a8\u5e7f\u5230\u591a\u4e2a\u6570\u636e\u96c6\u3002\u6a21\u5757\u5229\u7528\u70b9\u4e91\u4fe1\u606f\uff08\u5305\u62ec\u6cd5\u7ebf\u3001\u989c\u8272\u548c\u8ddd\u79bb\uff09\u5bf9\u9ad8\u65af\u8fdb\u884c\u5f52\u4e00\u5316\u548c\u521d\u59cb\u5316\u3002\u7136\u540e\u4f7f\u7528\u5206\u5272\u89e3\u7801\u5668\u901a\u8fc7\u590d\u5236\u9ad8\u65af\u5e76\u9884\u6d4b\u66f4\u51c6\u786e\u7684\u7ed3\u679c\u6765\u7ec6\u5316\u521d\u59cb\u9ad8\u65af\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u9002\u5e94\u7a00\u758f\u70b9\u4e91\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002"}}
{"id": "2505.08918", "pdf": "https://arxiv.org/pdf/2505.08918", "abs": "https://arxiv.org/abs/2505.08918", "authors": ["Marina Popova", "Iaroslav Chelombitko", "Aleksey Komissarov"], "title": "When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes", "categories": ["q-bio.GN", "cs.AI"], "comment": "ICLR 2025 Workshop on Machine Learning for Genomics Explorations", "summary": "The emergence of telomere-to-telomere (T2T) genome assemblies has opened new\navenues for comparative genomics, yet effective tokenization strategies for\ngenomic sequences remain underexplored. In this pilot study, we apply Byte Pair\nEncoding (BPE) to nine T2T primate genomes including three human assemblies by\ntraining independent BPE tokenizers with a fixed vocabulary of 512,000 tokens\nusing our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are\nshared across all assemblies, while nearly 991,854 tokens are unique to a\nsingle genome, indicating a rapid decline in shared vocabulary with increasing\nassembly comparisons. Moreover, phylogenetic trees derived from token overlap\nfailed to recapitulate established primate relationships, a discrepancy\nattributed to the disproportionate influence of species-specific high-copy\nrepetitive elements. These findings underscore the dual nature of BPE\ntokenization: while it effectively compresses repetitive sequences, its\nsensitivity to high-copy elements limits its utility as a universal tool for\ncomparative genomics. We discuss potential hybrid strategies and repeat-masking\napproaches to refine genomic tokenization, emphasizing the need for\ndomain-specific adaptations in the development of large-scale genomic language\nmodels. The dnaBPE tool used in this study is open-source and available at\nhttps://github.com/aglabx/dnaBPE.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86BPE\u5206\u8bcd\u5728\u57fa\u56e0\u7ec4\u5e8f\u5217\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5728\u5904\u7406\u91cd\u590d\u5e8f\u5217\u65b9\u9762\u6709\u6548\uff0c\u4f46\u53d7\u9ad8\u62f7\u8d1d\u5143\u4ef6\u5f71\u54cd\u8f83\u5927\uff0c\u9650\u5236\u4e86\u5176\u4f5c\u4e3a\u901a\u7528\u5de5\u5177\u7684\u6548\u7528\u3002", "motivation": "\u7814\u7a76BPE\u5206\u8bcd\u7b56\u7565\u5728\u57fa\u56e0\u7ec4\u5e8f\u5217\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63a2\u7d22\u5176\u5728\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u5de5\u5177dnaBPE\u5bf9\u4e5d\u4e2aT2T\u7075\u957f\u7c7b\u57fa\u56e0\u7ec4\uff08\u5305\u62ec\u4e09\u4e2a\u4eba\u7c7b\u57fa\u56e0\u7ec4\uff09\u5e94\u7528\u5b57\u8282\u5bf9\u7f16\u7801\uff08BPE\uff09\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53ea\u670911,569\u4e2a\u6807\u8bb0\u5728\u6240\u6709\u57fa\u56e0\u7ec4\u4e2d\u5171\u4eab\uff0c\u800c\u8fd1991,854\u4e2a\u6807\u8bb0\u662f\u5355\u4e2a\u57fa\u56e0\u7ec4\u72ec\u6709\u7684\uff0c\u8868\u660e\u968f\u7740\u57fa\u56e0\u7ec4\u6bd4\u8f83\u7684\u589e\u52a0\uff0c\u5171\u4eab\u8bcd\u6c47\u8fc5\u901f\u51cf\u5c11\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6807\u8bb0\u91cd\u53e0\u7684\u7cfb\u7edf\u53d1\u80b2\u6811\u672a\u80fd\u518d\u73b0\u5df2\u77e5\u7684\u7075\u957f\u7c7b\u5173\u7cfb\u3002", "conclusion": "BPE\u5206\u8bcd\u5728\u538b\u7f29\u91cd\u590d\u5e8f\u5217\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5bf9\u9ad8\u62f7\u8d1d\u5143\u4ef6\u7684\u654f\u611f\u6027\u9650\u5236\u4e86\u5176\u4f5c\u4e3a\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u901a\u7528\u5de5\u5177\u7684\u6548\u7528\u3002\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u9002\u5e94\u6765\u5f00\u53d1\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2505.09572", "pdf": "https://arxiv.org/pdf/2505.09572", "abs": "https://arxiv.org/abs/2505.09572", "authors": ["Julian Kranz", "Davide Gallon", "Steffen Dereich", "Arnulf Jentzen"], "title": "SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures", "categories": ["cs.LG", "math.LO", "math.OC", "stat.ML", "Primary 68T05, Secondary 68T07, 26B40, 03C64, 03C98"], "comment": "27 pages, 4 figures", "summary": "We study gradient flows for loss landscapes of fully connected feed forward\nneural networks with commonly used continuously differentiable activation\nfunctions such as the logistic, hyperbolic tangent, softplus or GELU function.\nWe prove that the gradient flow either converges to a critical point or\ndiverges to infinity while the loss converges to an asymptotic critical value.\nMoreover, we prove the existence of a threshold $\\varepsilon>0$ such that the\nloss value of any gradient flow initialized at most $\\varepsilon$ above the\noptimal level converges to it. For polynomial target functions and sufficiently\nbig architecture and data set, we prove that the optimal loss value is zero and\ncan only be realized asymptotically. From this setting, we deduce our main\nresult that any gradient flow with sufficiently good initialization diverges to\ninfinity. Our proof heavily relies on the geometry of o-minimal structures. We\nconfirm these theoretical findings with numerical experiments and extend our\ninvestigation to real-world scenarios, where we observe an analogous behavior.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5168\u8fde\u63a5\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u4e2d\u68af\u5ea6\u6d41\u7684\u884c\u4e3a\uff0c\u8bc1\u660e\u4e86\u68af\u5ea6\u6d41\u8981\u4e48\u6536\u655b\u5230\u4e34\u754c\u70b9\uff0c\u8981\u4e48\u53d1\u6563\u5230\u65e0\u7a77\u5927\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5f97\u51fa\u521d\u59cb\u5316\u826f\u597d\u7684\u68af\u5ea6\u6d41\u4f1a\u53d1\u6563\u5230\u65e0\u7a77\u5927\u7684\u7ed3\u8bba\u3002", "motivation": "\u7814\u7a76\u5168\u8fde\u63a5\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u666f\u89c2\u4e2d\u7684\u68af\u5ea6\u6d41\uff0c\u4ee5\u7406\u89e3\u5176\u6536\u655b\u884c\u4e3a\u548c\u4f18\u5316\u8fc7\u7a0b\u3002", "method": "\u6211\u4eec\u5229\u7528o-\u6781\u5c0f\u7ed3\u6784\u7684\u51e0\u4f55\u7279\u6027\u8fdb\u884c\u8bc1\u660e\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u548c\u73b0\u5b9e\u573a\u666f\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u53d1\u73b0\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u68af\u5ea6\u6d41\u8981\u4e48\u6536\u655b\u5230\u4e34\u754c\u70b9\uff0c\u8981\u4e48\u53d1\u6563\u5230\u65e0\u7a77\u5927\uff0c\u540c\u65f6\u635f\u5931\u6536\u655b\u5230\u6e10\u8fd1\u4e34\u754c\u503c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u4e2a\u9608\u503c\u03b5>0\uff0c\u4f7f\u5f97\u5728\u6700\u4f18\u6c34\u5e73\u4ee5\u4e0a\u6700\u591a\u03b5\u521d\u59cb\u5316\u7684\u68af\u5ea6\u6d41\u4f1a\u6536\u655b\u5230\u6700\u4f18\u503c\u3002\u5bf9\u4e8e\u591a\u9879\u5f0f\u76ee\u6807\u51fd\u6570\u548c\u8db3\u591f\u5927\u7684\u67b6\u6784\u548c\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6700\u4f18\u635f\u5931\u503c\u4e3a\u96f6\uff0c\u53ea\u80fd\u6e10\u8fd1\u5b9e\u73b0\u3002", "conclusion": "\u6211\u4eec\u7684\u4e3b\u8981\u7ed3\u8bba\u662f\uff0c\u4efb\u4f55\u521d\u59cb\u5316\u826f\u597d\u7684\u68af\u5ea6\u6d41\u90fd\u4f1a\u53d1\u6563\u5230\u65e0\u7a77\u5927\u3002"}}
{"id": "2505.09415", "pdf": "https://arxiv.org/pdf/2505.09415", "abs": "https://arxiv.org/abs/2505.09415", "authors": ["Hongyang Wang", "Yichen Shi", "Zhuofu Tao", "Yuhao Gao", "Liepiao Zhang", "Xun Lin", "Jun Feng", "Xiaochen Yuan", "Zitong Yu", "Xiaochun Cao"], "title": "FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing (FAS) is crucial for protecting facial recognition systems\nfrom presentation attacks. Previous methods approached this task as a\nclassification problem, lacking interpretability and reasoning behind the\npredicted results. Recently, multimodal large language models (MLLMs) have\nshown strong capabilities in perception, reasoning, and decision-making in\nvisual tasks. However, there is currently no universal and comprehensive MLLM\nand dataset specifically designed for FAS task. To address this gap, we propose\nFaceShield, a MLLM for FAS, along with the corresponding pre-training and\nsupervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.\nFaceShield is capable of determining the authenticity of faces, identifying\ntypes of spoofing attacks, providing reasoning for its judgments, and detecting\nattack areas. Specifically, we employ spoof-aware vision perception (SAVP) that\nincorporates both the original image and auxiliary information based on prior\nknowledge. We then use an prompt-guided vision token masking (PVTM) strategy to\nrandom mask vision tokens, thereby improving the model's generalization\nability. We conducted extensive experiments on three benchmark datasets,\ndemonstrating that FaceShield significantly outperforms previous deep learning\nmodels and general MLLMs on four FAS tasks, i.e., coarse-grained\nclassification, fine-grained classification, reasoning, and attack\nlocalization. Our instruction datasets, protocols, and codes will be released\nsoon.", "AI": {"tldr": "This paper proposes FaceShield, a MLLM for face anti-spoofing (FAS), along with pre-training and SFT datasets. FaceShield can determine face authenticity, identify spoofing attack types, provide reasoning, and detect attack areas. It outperforms previous models on FAS tasks.", "motivation": "Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. There is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task.", "method": "We propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. We employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use a prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the model's generalization ability.", "result": "FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks.", "conclusion": "FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization."}}
{"id": "2505.08919", "pdf": "https://arxiv.org/pdf/2505.08919", "abs": "https://arxiv.org/abs/2505.08919", "authors": ["Kangxian Xie", "Yufei Zhu", "Kaiming Kuang", "Li Zhang", "Hongwei Bran Li", "Mingchen Gao", "Jiancheng Yang"], "title": "Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "In revision process", "summary": "High-quality 3D reconstruction of pulmonary segments plays a crucial role in\nsegmentectomy and surgical treatment planning for lung cancer. Due to the\nresolution requirement of the target reconstruction, conventional deep\nlearning-based methods often suffer from computational resource constraints or\nlimited granularity. Conversely, implicit modeling is favored due to its\ncomputational efficiency and continuous representation at any resolution. We\npropose a neural implicit function-based method to learn a 3D surface to\nachieve anatomy-aware, precise pulmonary segment reconstruction, represented as\na shape by deforming a learnable template. Additionally, we introduce two\nclinically relevant evaluation metrics to assess the reconstruction\ncomprehensively. Further, due to the absence of publicly available shape\ndatasets to benchmark reconstruction algorithms, we developed a shape dataset\nnamed Lung3D, including the 3D models of 800 labeled pulmonary segments and the\ncorresponding airways, arteries, veins, and intersegmental veins. We\ndemonstrate that the proposed approach outperforms existing methods, providing\na new perspective for pulmonary segment reconstruction. Code and data will be\navailable at https://github.com/M3DV/ImPulSe.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9690\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u89e3\u5256\u611f\u77e5\u3001\u7cbe\u786e\u7684\u80ba\u6bb5\u91cd\u5efa\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLung3D\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u80ba\u6bb53D\u91cd\u5efa\u5728\u80ba\u6bb5\u5207\u9664\u672f\u548c\u80ba\u764c\u624b\u672f\u6cbb\u7597\u8ba1\u5212\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u4f20\u7edf\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u7531\u4e8e\u76ee\u6807\u91cd\u5efa\u7684\u5206\u8fa8\u7387\u8981\u6c42\uff0c\u5e38\u5e38\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u6216\u7c92\u5ea6\u6709\u9650\u7684\u95ee\u9898\u3002\u76f8\u53cd\uff0c\u9690\u5f0f\u5efa\u6a21\u56e0\u5176\u8ba1\u7b97\u6548\u7387\u548c\u4efb\u4f55\u5206\u8fa8\u7387\u7684\u8fde\u7eed\u8868\u793a\u800c\u53d7\u5230\u9752\u7750\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9690\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5f62\u53ef\u5b66\u4e60\u7684\u6a21\u677f\u6765\u5b9e\u73b0\u89e3\u5256\u611f\u77e5\u3001\u7cbe\u786e\u7684\u80ba\u6bb5\u91cd\u5efa\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLung3D\u7684\u5f62\u72b6\u6570\u636e\u96c6\uff0c\u5305\u62ec800\u4e2a\u6807\u8bb0\u7684\u80ba\u6bb53D\u6a21\u578b\u53ca\u5176\u76f8\u5e94\u7684\u6c14\u9053\u3001\u52a8\u8109\u3001\u9759\u8109\u548c\u6bb5\u95f4\u9759\u8109\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u8fd9\u79cd\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u80ba\u6bb5\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2505.09586", "pdf": "https://arxiv.org/pdf/2505.09586", "abs": "https://arxiv.org/abs/2505.09586", "authors": ["Yipeng Zhang", "Longlong Li", "Kelin Xia"], "title": "Rhomboid Tiling for Geometric Graph Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have proven effective for learning from\ngraph-structured data through their neighborhood-based message passing\nframework. Many hierarchical graph clustering pooling methods modify this\nframework by introducing clustering-based strategies, enabling the construction\nof more expressive and powerful models. However, all of these message passing\nframework heavily rely on the connectivity structure of graphs, limiting their\nability to capture the rich geometric features inherent in geometric graphs. To\naddress this, we propose Rhomboid Tiling (RT) clustering, a novel clustering\nmethod based on the rhomboid tiling structure, which performs clustering by\nleveraging the complex geometric information of the data and effectively\nextracts its higher-order geometric structures. Moreover, we design RTPool, a\nhierarchical graph clustering pooling model based on RT clustering for graph\nclassification tasks. The proposed model demonstrates superior performance,\noutperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u83f1\u5f62\u5e73\u94fa\u7ed3\u6784\u7684\u65b0\u578b\u805a\u7c7b\u65b9\u6cd5\uff08RT\uff09\u548c\u4e00\u4e2a\u57fa\u4e8eRT\u7684\u5206\u5c42\u56fe\u805a\u7c7b\u6c60\u5316\u6a21\u578b\uff08RTPool\uff09\uff0c\u7528\u4e8e\u56fe\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6d88\u606f\u4f20\u9012\u6846\u67b6\u8fc7\u5ea6\u4f9d\u8d56\u56fe\u7684\u8fde\u63a5\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5176\u6355\u6349\u51e0\u4f55\u56fe\u4e2d\u56fa\u6709\u4e30\u5bcc\u51e0\u4f55\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u83f1\u5f62\u5e73\u94fa\u7ed3\u6784\u7684\u65b0\u578b\u805a\u7c7b\u65b9\u6cd5\uff08Rhomboid Tiling, RT\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8eRT\u805a\u7c7b\u7684\u5206\u5c42\u56fe\u805a\u7c7b\u6c60\u5316\u6a21\u578b\uff08RTPool\uff09\u7528\u4e8e\u56fe\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6240\u67097\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e21\u4e2a\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6240\u67097\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e21\u4e2a\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u5bf9\u624b\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09422", "pdf": "https://arxiv.org/pdf/2505.09422", "abs": "https://arxiv.org/abs/2505.09422", "authors": ["Xiangyuan Peng", "Yu Wang", "Miao Tang", "Bierzynski Kay", "Lorenzo Servadei", "Robert Wille"], "title": "MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Reliable autonomous driving systems require accurate detection of traffic\nparticipants. To this end, multi-modal fusion has emerged as an effective\nstrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame\nradar point clouds have demonstrated the effectiveness in bridging the point\ndensity gap. However, they often neglect radar point clouds' inter-frame\nmisalignment caused by object movement during accumulation and do not fully\nexploit the object dynamic information from 4D radar. In this paper, we propose\nMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for\nrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is\ndesigned to compensate for inter-frame radar misalignment from moving objects.\nLater, a Motion Attention Gated Fusion (MAGF) module integrate radar motion\nfeatures to guide LiDAR features to focus on dynamic foreground objects.\nExtensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL\noutperforms existing methods, achieving the highest mAP of 73.30% in the entire\narea and 88.68% in the driving corridor. Notably, our method also achieves the\nbest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in\nthe driving corridor.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6MoRAL\uff0c\u7528\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d3D\u7269\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86\u96f7\u8fbe\u70b9\u4e91\u7531\u4e8e\u7269\u4f53\u79fb\u52a8\u5bfc\u81f4\u7684\u5e27\u95f4\u9519\u4f4d\uff0c\u5e76\u672a\u5145\u5206\u5229\u75284D\u96f7\u8fbe\u4e2d\u7684\u7269\u4f53\u52a8\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoRAL\u7684\u8fd0\u52a8\u611f\u77e5\u591a\u5e274D\u96f7\u8fbe\u548cLiDAR\u878d\u5408\u6846\u67b6\uff0c\u5305\u62ec\u8fd0\u52a8\u611f\u77e5\u96f7\u8fbe\u7f16\u7801\u5668\uff08MRE\uff09\u548c\u8fd0\u52a8\u6ce8\u610f\u529b\u95e8\u63a7\u878d\u5408\uff08MAGF\uff09\u6a21\u5757\u3002", "result": "MoRAL\u5728View-of-Delft (VoD)\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u597d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6574\u4e2a\u533a\u57df\u548c\u9a7e\u9a76\u8d70\u5eca\u4e2d\u7684mAP\u4ee5\u53ca\u5bf9\u884c\u4eba\u548c\u9a91\u81ea\u884c\u8f66\u8005\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "MoRAL\u5728View-of-Delft (VoD)\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6574\u4e2a\u533a\u57df\u548c\u9a7e\u9a76\u8d70\u5eca\u4e2d\u5206\u522b\u8fbe\u5230\u4e86\u6700\u9ad8\u7684mAP 73.30%\u548c88.68%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u884c\u4eba\u548c\u9a91\u81ea\u884c\u8f66\u8005\u68c0\u6d4b\u4e2d\u4e5f\u53d6\u5f97\u4e86\u6700\u4f73\u6210\u7ee9\u3002"}}
{"id": "2505.08939", "pdf": "https://arxiv.org/pdf/2505.08939", "abs": "https://arxiv.org/abs/2505.08939", "authors": ["Suchismita Naik", "Prakash Shukla", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parsons"], "title": "Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work", "categories": ["cs.HC", "cs.AI"], "comment": "5 pages, 2 Tables, In Creativity and Cognition 2025, June 23--25,\n  2025, Virtual, United Kingdom", "summary": "As generative AI tools become integrated into design workflows, students\nincreasingly engage with these tools not just as aids, but as collaborators.\nThis study analyzes reflections from 33 student teams in an HCI design course\nto examine the kinds of judgments students make when using AI tools. We found\nboth established forms of design judgment (e.g., instrumental, appreciative,\nquality) and emergent types: agency-distribution judgment and reliability\njudgment. These new forms capture how students negotiate creative\nresponsibility with AI and assess the trustworthiness of its outputs. Our\nfindings suggest that generative AI introduces new layers of complexity into\ndesign reasoning, prompting students to reflect not only on what AI produces,\nbut also on how and when to rely on it. By foregrounding these judgments, we\noffer a conceptual lens for understanding how students engage in co-creative\nsensemaking with AI in design contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e8633\u4e2a\u5b66\u751f\u56e2\u961f\u5728\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u8bfe\u7a0b\u4e2d\u7684\u53cd\u601d\uff0c\u4ee5\u63a2\u8ba8\u5b66\u751f\u4f7f\u7528AI\u5de5\u5177\u65f6\u6240\u505a\u7684\u5224\u65ad\u7c7b\u578b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9664\u4e86\u4f20\u7edf\u7684\u8bbe\u8ba1\u5224\u65ad\u5916\uff0c\u8fd8\u51fa\u73b0\u4e86\u4ee3\u7406\u5206\u914d\u5224\u65ad\u548c\u53ef\u9760\u6027\u5224\u65ad\u3002\u8fd9\u4e9b\u65b0\u5f62\u5f0f\u53cd\u6620\u4e86\u5b66\u751f\u5982\u4f55\u4e0eAI\u534f\u5546\u521b\u9020\u6027\u8d23\u4efb\u5e76\u8bc4\u4f30\u5176\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u5f0fAI\u5728\u8bbe\u8ba1\u63a8\u7406\u4e2d\u5f15\u5165\u4e86\u65b0\u7684\u590d\u6742\u6027\uff0c\u4fc3\u4f7f\u5b66\u751f\u4e0d\u4ec5\u53cd\u601dAI\u7684\u4ea7\u51fa\uff0c\u8fd8\u53cd\u601d\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f9d\u8d56\u5b83\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u878d\u5165\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b66\u751f\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u8fd9\u4e9b\u5de5\u5177\u89c6\u4e3a\u5408\u4f5c\u8005\u800c\u975e\u4ec5\u4ec5\u662f\u8f85\u52a9\u5de5\u5177\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7406\u89e3\u5b66\u751f\u5728\u4e0eAI\u5408\u4f5c\u65f6\u6240\u505a\u51fa\u7684\u5224\u65ad\u7c7b\u578b\u3002", "method": "\u672c\u7814\u7a76\u5206\u6790\u4e8633\u4e2a\u5b66\u751f\u56e2\u961f\u5728\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u8bfe\u7a0b\u4e2d\u7684\u53cd\u601d\uff0c\u4ee5\u63a2\u8ba8\u5b66\u751f\u4f7f\u7528AI\u5de5\u5177\u65f6\u6240\u505a\u7684\u5224\u65ad\u7c7b\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u65e2\u6709\u7684\u8bbe\u8ba1\u5224\u65ad\u5f62\u5f0f\uff08\u4f8b\u5982\uff0c\u5de5\u5177\u6027\u3001\u6b23\u8d4f\u6027\u548c\u8d28\u91cf\u5224\u65ad\uff09\u4ee5\u53ca\u65b0\u5174\u7c7b\u578b\uff1a\u4ee3\u7406\u5206\u914d\u5224\u65ad\u548c\u53ef\u9760\u6027\u5224\u65ad\u3002\u8fd9\u4e9b\u65b0\u5f62\u5f0f\u6355\u6349\u4e86\u5b66\u751f\u5982\u4f55\u4e0eAI\u534f\u5546\u521b\u9020\u6027\u8d23\u4efb\u5e76\u8bc4\u4f30\u5176\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u5f0fAI\u5728\u8bbe\u8ba1\u63a8\u7406\u4e2d\u5f15\u5165\u4e86\u65b0\u7684\u590d\u6742\u6027\uff0c\u4fc3\u4f7f\u5b66\u751f\u4e0d\u4ec5\u53cd\u601dAI\u7684\u4ea7\u51fa\uff0c\u8fd8\u53cd\u601d\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f9d\u8d56\u5b83\u3002\u901a\u8fc7\u7a81\u51fa\u8fd9\u4e9b\u5224\u65ad\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u6027\u89c6\u89d2\uff0c\u4ee5\u7406\u89e3\u5b66\u751f\u5728\u8bbe\u8ba1\u73af\u5883\u4e2d\u4e0eAI\u8fdb\u884c\u534f\u4f5c\u521b\u9020\u610f\u4e49\u7684\u8fc7\u7a0b\u3002"}}
{"id": "2505.09593", "pdf": "https://arxiv.org/pdf/2505.09593", "abs": "https://arxiv.org/abs/2505.09593", "authors": ["Filippo Leveni", "Guilherme Weigert Cassales", "Bernhard Pfahringer", "Albert Bifet", "Giacomo Boracchi"], "title": "Online Isolation Forest", "categories": ["cs.LG"], "comment": "Accepted at International Conference on Machine Learning (ICML 2024)", "summary": "The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.", "AI": {"tldr": "Online-iForest \u662f\u4e00\u79cd\u4e13\u4e3a\u6d41\u5f0f\u6761\u4ef6\u8bbe\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u8ddf\u8e2a\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u8868\u660e\u5b83\u5728\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6240\u6709\u7ade\u4e89\u5bf9\u624b\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u8bc6\u522b\u5f02\u5e38\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u7ea6\u675f\uff0c\u5f80\u5f80\u4f9d\u8d56\u4e8e\u5b9a\u671f\u91cd\u65b0\u8bad\u7ec3\u4ee5\u9002\u5e94\u5728\u7ebf\u73af\u5883\u3002", "method": "Online-iForest \u662f\u4e00\u79cd\u4e13\u4e3a\u6d41\u5f0f\u6761\u4ef6\u8bbe\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u8ddf\u8e2a\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cOnline-iForest \u4e0e\u5728\u7ebf\u66ff\u4ee3\u65b9\u6848\u76f8\u5f53\uff0c\u5e76\u4e14\u4e0e\u5b9a\u671f\u91cd\u65b0\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u76f8\u5f53\u3002", "conclusion": "Online-iForest \u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u8bc6\u522b\u5f02\u5e38\u7684\u5e94\u7528\uff0c\u5982\u7f51\u7edc\u5b89\u5168\u3001\u6b3a\u8bc8\u548c\u6545\u969c\u68c0\u6d4b\u3002"}}
{"id": "2505.09433", "pdf": "https://arxiv.org/pdf/2505.09433", "abs": "https://arxiv.org/abs/2505.09433", "authors": ["Jiahao Zhu", "Kang You", "Dandan Ding", "Zhan Ma"], "title": "Efficient LiDAR Reflectance Compression via Scanning Serialization", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Reflectance attributes in LiDAR point clouds provide essential information\nfor downstream tasks but remain underexplored in neural compression methods. To\naddress this, we introduce SerLiC, a serialization-based neural compression\nframework to fully exploit the intrinsic characteristics of LiDAR reflectance.\nSerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order\nserialization, offering a device-centric perspective for reflectance analysis.\nEach point is then tokenized into a contextual representation comprising its\nsensor scanning index, radial distance, and prior reflectance, for effective\ndependencies exploration. For efficient sequential modeling, Mamba is\nincorporated with a dual parallelization scheme, enabling simultaneous\nautoregressive dependency capture and fast processing. Extensive experiments\ndemonstrate that SerLiC attains over 2x volume reduction against the original\nreflectance data, outperforming the state-of-the-art method by up to 22%\nreduction of compressed bits while using only 2% of its parameters. Moreover, a\nlightweight version of SerLiC achieves > 10 fps (frames per second) with just\n111K parameters, which is attractive for real-world applications.", "AI": {"tldr": "SerLiC \u662f\u4e00\u79cd\u57fa\u4e8e\u5e8f\u5217\u5316\u7684\u795e\u7ecf\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e LiDAR \u53cd\u5c04\u7387\u6570\u636e\u7684\u9ad8\u6548\u538b\u7f29\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u5728 LiDAR \u53cd\u5c04\u7387\u5c5e\u6027\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "SerLiC \u901a\u8fc7\u626b\u63cf\u987a\u5e8f\u5e8f\u5217\u5316\u5c06 3D LiDAR \u70b9\u4e91\u8f6c\u6362\u4e3a 1D \u5e8f\u5217\uff0c\u5e76\u4f7f\u7528 Mamba \u8fdb\u884c\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u3002", "result": "SerLiC \u5728\u538b\u7f29\u4f53\u79ef\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7 2 \u500d\u7684\u51cf\u5c11\uff0c\u5e76\u4e14\u5728\u53c2\u6570\u4f7f\u7528\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c11 98%\u3002", "conclusion": "SerLiC \u662f\u4e00\u79cd\u6709\u6548\u7684 LiDAR \u53cd\u5c04\u7387\u538b\u7f29\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f53\u79ef\u5e76\u63d0\u9ad8\u538b\u7f29\u6548\u7387\u3002"}}
{"id": "2505.09602", "pdf": "https://arxiv.org/pdf/2505.09602", "abs": "https://arxiv.org/abs/2505.09602", "authors": ["David Khachaturov", "Robert Mullins"], "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios.", "AI": {"tldr": "This paper introduces ASF, a new defense mechanism against adversarial suffix attacks, which is effective, lightweight, and model-agnostic.", "motivation": "Existing defenses against adversarial suffix attacks are limited by their reliance on internal model architecture, high memory and computation footprints, or vulnerability to simple prompt engineering methods. There is a need for a more effective and efficient defense mechanism.", "method": "Adversarial Suffix Filtering (ASF) is introduced as a lightweight, model-agnostic defensive pipeline that acts as an input preprocessor and sanitizer to detect and filter adversarially crafted suffixes in prompts.", "result": "ASF effectively neutralizes malicious injections, providing strong defense in both black-box and white-box attack settings, with minimal impact on the model's performance in non-adversarial scenarios.", "conclusion": "ASF provides comprehensive defense capabilities against adversarial suffix attacks, reducing attack efficacy to below 4% while minimally affecting the model's normal capabilities."}}
{"id": "2505.09435", "pdf": "https://arxiv.org/pdf/2505.09435", "abs": "https://arxiv.org/abs/2505.09435", "authors": ["Yili He", "Yan Zhu", "Peiyao Fu", "Ruijie Yang", "Tianyi Chen", "Zhihua Wang", "Quanlin Li", "Pinghong Zhou", "Xian Yang", "Shuo Wang"], "title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records", "categories": ["cs.CV", "cs.AI"], "comment": "Early accepted to MICCAI 2025", "summary": "Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Endo-CLIP\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u7ed3\u80a0\u955c\u56fe\u50cf\u5206\u6790\u7684\u65b0\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u89e3\u51b3\u4e86\u975e\u4fe1\u606f\u6027\u80cc\u666f\u56fe\u50cf\u3001\u590d\u6742\u533b\u5b66\u672f\u8bed\u548c\u6a21\u7cca\u7684\u591a\u75c5\u53d8\u63cf\u8ff0\u7b49\u6311\u6218\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u7684\u606f\u8089\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u56fe\u50cf-\u6587\u672c\u7ed3\u80a0\u955c\u8bb0\u5f55\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u5bf9\u4e8e\u6539\u5584\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u5305\u62ec\u975e\u4fe1\u606f\u6027\u80cc\u666f\u56fe\u50cf\u3001\u590d\u6742\u7684\u533b\u5b66\u672f\u8bed\u548c\u6a21\u7cca\u7684\u591a\u75c5\u53d8\u63cf\u8ff0\u7b49\u6311\u6218\u3002", "method": "Endo-CLIP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\uff08\u6e05\u7406\u3001\u8c03\u8c10\u548c\u7edf\u4e00\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u5305\u62ec\u79fb\u9664\u80cc\u666f\u5e27\u3001\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e34\u5e8a\u5c5e\u6027\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u53ca\u4f7f\u7528\u60a3\u8005\u7ea7\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u89e3\u51b3\u591a\u606f\u8089\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEndo-CLIP\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u7684\u606f\u8089\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "Endo-CLIP\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u7684\u606f\u8089\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\uff0c\u4e3a\u66f4\u51c6\u786e\u548c\u4e34\u5e8a\u76f8\u5173\u7684\u5185\u7aa5\u955c\u5206\u6790\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.07363", "pdf": "https://arxiv.org/pdf/2505.07363", "abs": "https://arxiv.org/abs/2505.07363", "authors": ["Serge Massar"], "title": "Equilibrium Propagation for Learning in Lagrangian Dynamical Systems", "categories": ["nlin.CD", "cs.LG", "physics.data-an"], "comment": "8 pages, 1 figure", "summary": "We propose a method for training dynamical systems governed by Lagrangian\nmechanics using Equilibrium Propagation. Our approach extends Equilibrium\nPropagation -- initially developed for energy-based models -- to dynamical\ntrajectories by leveraging the principle of action extremization. Training is\nachieved by gently nudging trajectories toward desired targets and measuring\nhow the variables conjugate to the parameters to be trained respond. This\nmethod is particularly suited to systems with periodic boundary conditions or\nfixed initial and final states, enabling efficient parameter updates without\nrequiring explicit backpropagation through time. In the case of periodic\nboundary conditions, this approach yields the semiclassical limit of Quantum\nEquilibrium Propagation. Applications to systems with dissipation are also\ndiscussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u8861\u4f20\u64ad\u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u7531\u62c9\u683c\u6717\u65e5\u529b\u5b66\u63a7\u5236\u7684\u52a8\u529b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f5c\u7528\u6781\u503c\u539f\u7406\u5b9e\u73b0\u9ad8\u6548\u7684\u53c2\u6570\u66f4\u65b0\uff0c\u9002\u7528\u4e8e\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u6216\u56fa\u5b9a\u521d\u59cb\u548c\u6700\u7ec8\u72b6\u6001\u7684\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u7684\u53cd\u5411\u4f20\u64ad\u901a\u8fc7\u65f6\u95f4\uff08BPTT\uff09\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u529b\u7cfb\u7edf\u65f6\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u8fd9\u4e9b\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u6216\u56fa\u5b9a\u521d\u59cb\u548c\u6700\u7ec8\u72b6\u6001\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u672c\u6587\u7684\u65b9\u6cd5\u57fa\u4e8e\u5e73\u8861\u4f20\u64ad\uff0c\u901a\u8fc7\u4f5c\u7528\u6781\u503c\u539f\u7406\u5c06\u5e73\u8861\u4f20\u64ad\u6269\u5c55\u5230\u52a8\u529b\u8f68\u8ff9\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u8f7b\u5fae\u8c03\u6574\u8f68\u8ff9\u4ee5\u63a5\u8fd1\u76ee\u6807\uff0c\u5e76\u6d4b\u91cf\u4e0e\u8981\u8bad\u7ec3\u7684\u53c2\u6570\u5171\u8f6d\u7684\u53d8\u91cf\u7684\u54cd\u5e94\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u663e\u5f0f\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u901a\u8fc7\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u53c2\u6570\u66f4\u65b0\u3002\u5728\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5f97\u5230\u4e86\u91cf\u5b50\u5e73\u8861\u4f20\u64ad\u7684\u534a\u7ecf\u5178\u6781\u9650\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u5728\u8017\u6563\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5e73\u8861\u4f20\u64ad\u8bad\u7ec3\u7531\u62c9\u683c\u6717\u65e5\u529b\u5b66\u63a7\u5236\u7684\u52a8\u529b\u7cfb\u7edf\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u4f5c\u7528\u6781\u503c\u539f\u7406\u5c06\u5e73\u8861\u4f20\u64ad\u6269\u5c55\u5230\u52a8\u529b\u8f68\u8ff9\uff0c\u5e76\u5728\u5177\u6709\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u6216\u56fa\u5b9a\u521d\u59cb\u548c\u6700\u7ec8\u72b6\u6001\u7684\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53c2\u6570\u66f4\u65b0\u3002"}}
{"id": "2505.09450", "pdf": "https://arxiv.org/pdf/2505.09450", "abs": "https://arxiv.org/abs/2505.09450", "authors": ["Yuelin Zhang", "Qingpeng Ding", "Long Lei", "Yongxuan Feng", "Raymond Shing-Yan Tang", "Shing Shin Cheng"], "title": "MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy", "categories": ["cs.CV"], "comment": "Early Accepted by MICCAI 2025", "summary": "Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally\ninvasive diagnostic procedure. However, an aspiration needle tracker addressing\nrapid reciprocating motion is still missing. MrTrack, an aspiration needle\ntracker with a mamba-based register mechanism, is proposed. MrTrack leverages a\nMamba-based register extractor to sequentially distill global context from each\nhistorical search map, storing these temporal cues in a register bank. The\nMamba-based register retriever then retrieves temporal prompts from the\nregister bank to provide external cues when current vision features are\ntemporarily unusable due to rapid reciprocating motion and imaging degradation.\nA self-supervised register diversify loss is proposed to encourage feature\ndiversity and dimension independence within the learned register, mitigating\nfeature collapse. Comprehensive experiments conducted on both motorized and\nmanual aspiration datasets demonstrate that MrTrack not only outperforms\nstate-of-the-art trackers in accuracy and robustness but also achieves superior\ninference efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MrTrack\uff0c\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u6ce8\u518c\u673a\u5236\u7684\u7a7f\u523a\u9488\u8ddf\u8e2a\u5668\uff0c\u80fd\u591f\u5728\u5feb\u901f\u5f80\u590d\u8fd0\u52a8\u548c\u6210\u50cf\u9000\u5316\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e00\u79cd\u80fd\u591f\u5e94\u5bf9\u5feb\u901f\u5f80\u590d\u8fd0\u52a8\u7684\u8d85\u58f0\u5f15\u5bfc\u7ec6\u9488\u7a7f\u523a\u6d3b\u68c0\uff08FNA\uff09\u9488\u8ddf\u8e2a\u5668\u3002", "method": "MrTrack\u4f7f\u7528\u57fa\u4e8eMamba\u7684\u6ce8\u518c\u673a\u5236\uff0c\u901a\u8fc7\u5e8f\u5217\u84b8\u998f\u4ece\u6bcf\u4e2a\u5386\u53f2\u641c\u7d22\u56fe\u4e2d\u63d0\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5e76\u5c06\u8fd9\u4e9b\u65f6\u95f4\u7ebf\u7d22\u5b58\u50a8\u5728\u6ce8\u518c\u94f6\u884c\u4e2d\u3002\u7136\u540e\uff0c\u57fa\u4e8eMamba\u7684\u6ce8\u518c\u68c0\u7d22\u5668\u4ece\u6ce8\u518c\u94f6\u884c\u4e2d\u68c0\u7d22\u65f6\u95f4\u63d0\u793a\uff0c\u4ee5\u5728\u5f53\u524d\u89c6\u89c9\u7279\u5f81\u7531\u4e8e\u5feb\u901f\u5f80\u590d\u8fd0\u52a8\u548c\u6210\u50cf\u9000\u5316\u800c\u6682\u65f6\u4e0d\u53ef\u7528\u65f6\u63d0\u4f9b\u5916\u90e8\u63d0\u793a\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u6ce8\u518c\u591a\u6837\u5316\u635f\u5931\uff0c\u4ee5\u9f13\u52b1\u5b66\u4e60\u5230\u7684\u6ce8\u518c\u4e2d\u7684\u7279\u5f81\u591a\u6837\u6027\u53ca\u7ef4\u5ea6\u72ec\u7acb\u6027\uff0c\u4ece\u800c\u7f13\u89e3\u7279\u5f81\u5d29\u6e83\u95ee\u9898\u3002", "result": "\u5728\u7535\u52a8\u548c\u624b\u52a8\u7a7f\u523a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0cMrTrack\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u5668\uff0c\u800c\u4e14\u5728\u63a8\u7406\u6548\u7387\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MrTrack\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u5668\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.09021", "pdf": "https://arxiv.org/pdf/2505.09021", "abs": "https://arxiv.org/abs/2505.09021", "authors": ["Maria Dhakal", "Chia-Yi Su", "Robert Wallace", "Chris Fakhimi", "Aakash Bansal", "Toby Li", "Yu Huang", "Collin McMillan"], "title": "AI-Mediated Code Comment Improvement", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "This paper describes an approach to improve code comments along different\nquality axes by rewriting those comments with customized Artificial\nIntelligence (AI)-based tools. We conduct an empirical study followed by\ngrounded theory qualitative analysis to determine the quality axes to improve.\nThen we propose a procedure using a Large Language Model (LLM) to rewrite\nexisting code comments along the quality axes. We implement our procedure using\nGPT-4o, then distil the results into a smaller model capable of being run\nin-house, so users can maintain data custody. We evaluate both our approach\nusing GPT-4o and the distilled model versions. We show in an evaluation how our\nprocedure improves code comments along the quality axes. We release all data\nand source code in an online repository for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u6765\u6539\u5584\u4ee3\u7801\u6ce8\u91ca\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u9ad8\u4ee3\u7801\u6ce8\u91ca\u7684\u8d28\u91cf\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u73b0\u6709\u7684\u4ee3\u7801\u6ce8\u91ca\u53ef\u80fd\u4e0d\u591f\u51c6\u786e\u6216\u4e0d\u5b8c\u6574\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u5b83\u4eec\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u57fa\u4e8e\u624e\u6839\u7406\u8bba\u7684\u5b9a\u6027\u5206\u6790\u786e\u5b9a\u4e86\u9700\u8981\u6539\u8fdb\u7684\u8d28\u91cf\u7ef4\u5ea6\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u91cd\u5199\u73b0\u6709\u4ee3\u7801\u6ce8\u91ca\u7684\u7a0b\u5e8f\u3002", "result": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u4ee3\u7801\u6ce8\u91ca\u5728\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u7ed3\u679c\u538b\u7f29\u5230\u4e00\u4e2a\u8f83\u5c0f\u7684\u6a21\u578b\u4e2d\uff0c\u4ee5\u4fbf\u7528\u6237\u53ef\u4ee5\u5728\u5185\u90e8\u8fd0\u884c\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5b9a\u5236\u7684\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u91cd\u5199\u4ee3\u7801\u6ce8\u91ca\u6765\u63d0\u9ad8\u4ee3\u7801\u6ce8\u91ca\u5728\u4e0d\u540c\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09455", "pdf": "https://arxiv.org/pdf/2505.09455", "abs": "https://arxiv.org/abs/2505.09455", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos", "categories": ["cs.CV"], "comment": "12 pages, submitted to Advanced Concepts for Intelligent Vision\n  Systems 2025", "summary": "State-of-the-art spatio-temporal action detection (STAD) methods show\npromising results for extracting soccer events from broadcast videos. However,\nwhen operated in the high-recall, low-precision regime required for exhaustive\nevent coverage in soccer analytics, their lack of contextual understanding\nbecomes apparent: many false positives could be resolved by considering a\nbroader sequence of actions and game-state information. In this work, we\naddress this limitation by reasoning at the game level and improving STAD\nthrough the addition of a denoising sequence transduction task. Sequences of\nnoisy, context-free player-centric predictions are processed alongside clean\ngame state information using a Transformer-based encoder-decoder model. By\nmodeling extended temporal context and reasoning jointly over team-level\ndynamics, our method leverages the \"language of soccer\" - its tactical\nregularities and inter-player dependencies - to generate \"denoised\" sequences\nof actions. This approach improves both precision and recall in low-confidence\nregimes, enabling more reliable event extraction from broadcast video and\ncomplementing existing pixel-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u53bb\u566a\u5e8f\u5217\u8f6c\u6362\u4efb\u52a1\u6765\u6539\u8fdb\u65f6\u7a7a\u52a8\u4f5c\u68c0\u6d4b\uff0c\u4ece\u800c\u63d0\u9ad8\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u7a7a\u52a8\u4f5c\u68c0\u6d4b\u65b9\u6cd5\u5728\u9700\u8981\u9ad8\u53ec\u56de\u7387\u3001\u4f4e\u7cbe\u786e\u5ea6\u7684\u573a\u666f\u4e0b\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u5bfc\u81f4\u8bb8\u591a\u8bef\u62a5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6e38\u620f\u7ea7\u522b\u7684\u63a8\u7406\u548c\u6dfb\u52a0\u53bb\u566a\u5e8f\u5217\u8f6c\u6362\u4efb\u52a1\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u5608\u6742\u7684\u3001\u4e0e\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u4ee5\u7403\u5458\u4e3a\u4e2d\u5fc3\u7684\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408\u5e72\u51c0\u7684\u6e38\u620f\u72b6\u6001\u4fe1\u606f\uff0c\u901a\u8fc7\u5efa\u6a21\u6269\u5c55\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u8054\u5408\u63a8\u7406\u56e2\u961f\u7ea7\u52a8\u6001\u6765\u751f\u6210\u201c\u53bb\u566a\u201d\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u4e8b\u4ef6\u63d0\u53d6\uff0c\u5e76\u8865\u5145\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u50cf\u7d20\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u53bb\u566a\u5e8f\u5217\u8f6c\u6362\u4efb\u52a1\u6765\u6539\u8fdb\u65f6\u7a7a\u52a8\u4f5c\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u6a21\u6269\u5c55\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u8054\u5408\u63a8\u7406\u56e2\u961f\u7ea7\u52a8\u6001\uff0c\u63d0\u9ad8\u4e86\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u4e8b\u4ef6\u63d0\u53d6\u3002"}}
{"id": "2505.08804", "pdf": "https://arxiv.org/pdf/2505.08804", "abs": "https://arxiv.org/abs/2505.08804", "authors": ["Longtian Wang", "Xiaofei Xie", "Tianlin Li", "Yuhan Zhi", "Chao Shen"], "title": "TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis", "categories": ["cs.CR", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Text-to-image (T2I) models have significantly advanced in producing\nhigh-quality images. However, such models have the ability to generate images\ncontaining not-safe-for-work (NSFW) content, such as pornography, violence,\npolitical content, and discrimination. To mitigate the risk of generating NSFW\ncontent, refusal mechanisms, i.e., safety checkers, have been developed to\ncheck potential NSFW content. Adversarial prompting techniques have been\ndeveloped to evaluate the robustness of the refusal mechanisms. The key\nchallenge remains to subtly modify the prompt in a way that preserves its\nsensitive nature while bypassing the refusal mechanisms. In this paper, we\nintroduce TokenProber, a method designed for sensitivity-aware differential\ntesting, aimed at evaluating the robustness of the refusal mechanisms in T2I\nmodels by generating adversarial prompts. Our approach is based on the key\nobservation that adversarial prompts often succeed by exploiting discrepancies\nin how T2I models and safety checkers interpret sensitive content. Thus, we\nconduct a fine-grained analysis of the impact of specific words within prompts,\ndistinguishing between dirty words that are essential for NSFW content\ngeneration and discrepant words that highlight the different sensitivity\nassessments between T2I models and safety checkers. Through the\nsensitivity-aware mutation, TokenProber generates adversarial prompts, striking\na balance between maintaining NSFW content generation and evading detection.\nOur evaluation of TokenProber against 5 safety checkers on 3 popular T2I\nmodels, using 324 NSFW prompts, demonstrates its superior effectiveness in\nbypassing safety filters compared to existing methods (e.g., 54%+ increase on\naverage), highlighting TokenProber's ability to uncover robustness issues in\nthe existing refusal mechanisms.", "AI": {"tldr": "TokenProber is a method that generates adversarial prompts to evaluate the robustness of safety checkers in T2I models, demonstrating superior effectiveness in bypassing safety filters.", "motivation": "The key challenge is to subtly modify prompts to preserve their sensitive nature while bypassing refusal mechanisms, as T2I models can generate NSFW content.", "method": "TokenProber is a method designed for sensitivity-aware differential testing, which generates adversarial prompts by analyzing the impact of specific words within prompts and distinguishing between dirty words and discrepant words.", "result": "TokenProber was evaluated against 5 safety checkers on 3 popular T2I models using 324 NSFW prompts, showing a 54%+ increase in bypassing safety filters compared to existing methods.", "conclusion": "TokenProber demonstrates superior effectiveness in bypassing safety filters compared to existing methods, highlighting the robustness issues in existing refusal mechanisms."}}
{"id": "2505.09466", "pdf": "https://arxiv.org/pdf/2505.09466", "abs": "https://arxiv.org/abs/2505.09466", "authors": ["Xi Chen", "Shiyang Zhou", "Muqi Huang", "Jiaxu Feng", "Yun Xiong", "Kun Zhou", "Biao Yang", "Yuhui Zhang", "Huishuai Bao", "Sijia Peng", "Chuan Li", "Feng Shi"], "title": "A 2D Semantic-Aware Position Encoding for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 4 figures, 3 tables", "summary": "Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86$\text{SaPE}^2$\uff0c\u4e00\u79cd\u65b0\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4f4d\u7f6e\u8868\u793a\uff0c\u63d0\u9ad8\u89c6\u89c9\u53d8\u538b\u5668\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u6280\u672f\u65e0\u6cd5\u6709\u6548\u6355\u6349\u56fe\u50cf\u5757\u4e4b\u95f4\u7684\u8bed\u4e49\u611f\u77e5\u4f4d\u7f6e\u5173\u7cfb\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3001\u5e73\u79fb\u7b49\u53d8\u6027\u548c\u5904\u7406\u91cd\u590d\u6216\u7ed3\u6784\u5316\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8bed\u4e49\u610f\u8bc6\u7684\u4e8c\u7ef4\u8bed\u4e49\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\uff08$\text{SaPE}^2$\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5c40\u90e8\u5185\u5bb9\u800c\u4e0d\u662f\u56fa\u5b9a\u7684\u7ebf\u6027\u4f4d\u7f6e\u5173\u7cfb\u6216\u7a7a\u95f4\u5750\u6807\u6765\u52a8\u6001\u9002\u5e94\u4f4d\u7f6e\u8868\u793a\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u5c3a\u5ea6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5e73\u79fb\u7b49\u53d8\u6027\uff0c\u5e76\u66f4\u597d\u5730\u805a\u5408\u4e86\u89c6\u89c9\u76f8\u4f3c\u4f46\u7a7a\u95f4\u4e0a\u8ddd\u79bb\u8f83\u8fdc\u7684\u5757\u7684\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u5c06$\text{SaPE}^2$\u96c6\u6210\u5230\u89c6\u89c9\u53d8\u538b\u5668\u4e2d\uff0c\u6211\u4eec\u5f25\u5408\u4e86\u4f4d\u7f6e\u7f16\u7801\u4e0e\u611f\u77e5\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09027", "pdf": "https://arxiv.org/pdf/2505.09027", "abs": "https://arxiv.org/abs/2505.09027", "authors": ["Yi Cui"], "title": "Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2409.05177", "summary": "We introduce WebApp1K, a novel benchmark for evaluating large language models\n(LLMs) in test-driven development (TDD) tasks, where test cases serve as both\nprompt and verification for code generation. Unlike traditional approaches\nrelying on natural language prompts, our benchmark emphasizes the ability of\nLLMs to interpret and implement functionality directly from test cases,\nreflecting real-world software development practices. Comprising 1000 diverse\nchallenges across 20 application domains, the benchmark evaluates LLMs on their\nability to generate compact, functional code under the constraints of context\nlength and multi-feature complexity. Our findings highlight instruction\nfollowing and in-context learning as critical capabilities for TDD success,\nsurpassing the importance of general coding proficiency or pretraining\nknowledge. Through comprehensive evaluation of 19 frontier models, we reveal\nperformance bottlenecks, such as instruction loss in long prompts, and provide\na detailed error analysis spanning multiple root causes. This work underscores\nthe practical value of TDD-specific benchmarks and lays the foundation for\nadvancing LLM capabilities in rigorous, application-driven coding scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86WebApp1K\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5f3a\u8c03\u4e86\u6307\u4ee4\u9075\u5faa\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5f3a\u8c03LLMs\u76f4\u63a5\u4ece\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u89e3\u91ca\u548c\u5b9e\u73b0\u529f\u80fd\u7684\u80fd\u529b\uff0c\u8fd9\u53cd\u6620\u4e86\u73b0\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u3002", "method": "\u5f15\u5165\u4e86WebApp1K\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5176\u4e2d\u6d4b\u8bd5\u7528\u4f8b\u65e2\u662f\u63d0\u793a\u4e5f\u662f\u4ee3\u7801\u751f\u6210\u7684\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6307\u4ee4\u9075\u5faa\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u662fTDD\u6210\u529f\u7684\u5173\u952e\u80fd\u529b\uff0c\u8d85\u8fc7\u4e86\u901a\u7528\u7f16\u7801\u719f\u7ec3\u5ea6\u6216\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u91cd\u8981\u6027\u3002\u901a\u8fc7\u5168\u9762\u8bc4\u4f3019\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u74f6\u9888\uff0c\u5982\u957f\u63d0\u793a\u4e2d\u7684\u6307\u4ee4\u4e22\u5931\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u6790\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u9488\u5bf9TDD\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u4e3a\u63d0\u5347LLM\u5728\u4e25\u683c\u3001\u5e94\u7528\u9a71\u52a8\u7684\u7f16\u7801\u573a\u666f\u4e2d\u7684\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09484", "pdf": "https://arxiv.org/pdf/2505.09484", "abs": "https://arxiv.org/abs/2505.09484", "authors": ["Yingjie Ma", "Xun Lin", "Zitong Yu", "Xin Liu", "Xiaochen Yuan", "Weicheng Xie", "Linlin Shen"], "title": "Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing", "categories": ["cs.CV"], "comment": null, "summary": "Face Anti-Spoofing (FAS) is essential for the security of facial recognition\nsystems in diverse scenarios such as payment processing and surveillance.\nCurrent multimodal FAS methods often struggle with effective generalization,\nmainly due to modality-specific biases and domain shifts. To address these\nchallenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising\nand \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot\ngeneralization capability of CLIP, the MMDA framework effectively suppresses\nnoise in multimodal data through denoising and alignment mechanisms, thereby\nsignificantly enhancing the generalization performance of cross-modal\nalignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential\n\\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the\nimpacts of domain and modality noise by refining the attention mechanism based\non extracted common noise features. Furthermore, the \\textbf{R}epresentation\n\\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the\npre-trained CLIP model to align multi-domain multimodal data into a generalized\nrepresentation space in a flexible manner, preserving intricate representations\nand enhancing the model's adaptability to various unseen conditions. We also\ndesign a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation\n(\\textbf{U-DSA}) module to enhance the adaptability of representations while\nmaintaining generalization performance. These improvements not only enhance the\nframework's generalization capabilities but also boost its ability to represent\ncomplex representations. Our experimental results on four benchmark datasets\nunder different evaluation protocols demonstrate that the MMDA framework\noutperforms existing state-of-the-art methods in terms of cross-domain\ngeneralization and multimodal detection accuracy. The code will be released\nsoon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMDA\u7684\u591a\u6a21\u6001\u53bb\u566a\u548c\u5bf9\u9f50\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u9762\u90e8\u53cd\u6b3a\u9a97\uff08FAS\uff09\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5229\u7528CLIP\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0cMMDA\u6846\u67b6\u6709\u6548\u5730\u6291\u5236\u4e86\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u51cf\u8f7b\u4e86\u9886\u57df\u548c\u6a21\u6001\u566a\u58f0\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMMDA\u6846\u67b6\u5728\u8de8\u9886\u57df\u6cdb\u5316\u548c\u591a\u6a21\u6001\u68c0\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001FAS\u65b9\u6cd5\u5728\u6709\u6548\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u548c\u9886\u57df\u8f6c\u79fb\u95ee\u9898\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MMDA\u6846\u67b6\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86MMDA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528CLIP\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u5bf9\u9f50\u673a\u5236\u6709\u6548\u6291\u5236\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u566a\u58f0\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6cdb\u5316\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86MD2A\u6a21\u5757\u3001RS2\u5bf9\u9f50\u7b56\u7565\u548cU-DSA\u6a21\u5757\u6765\u589e\u5f3a\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u8868\u793a\u80fd\u529b\u3002", "result": "MMDA\u6846\u67b6\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u4e0d\u540c\u8bc4\u4f30\u534f\u8bae\u4e0b\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728\u8de8\u9886\u57df\u6cdb\u5316\u548c\u591a\u6a21\u6001\u68c0\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMMDA\u6846\u67b6\u5728\u8de8\u9886\u57df\u6cdb\u5316\u548c\u591a\u6a21\u6001\u68c0\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2505.09040", "pdf": "https://arxiv.org/pdf/2505.09040", "abs": "https://arxiv.org/abs/2505.09040", "authors": ["Owen Kwon", "Abraham George", "Alison Bartsch", "Amir Barati Farimani"], "title": "RT-cache: Efficient Robot Trajectory Retrieval System", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference", "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.", "AI": {"tldr": "RT-cache \u662f\u4e00\u79cd\u65b0\u578b\u8f68\u8ff9\u8bb0\u5fc6\u7ba1\u9053\uff0c\u901a\u8fc7\u5229\u7528\u5927\u6570\u636e\u68c0\u7d22\u548c\u7ecf\u9a8c\u5b66\u4e60\u6765\u52a0\u901f\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u63a8\u7406\u3002", "motivation": "\u73b0\u4ee3 Vision-Language-Action (VLA) \u6a21\u578b\u5728\u5904\u7406\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u4efb\u52a1\u65f6\uff0c\u901a\u5e38\u4f1a\u4ea7\u751f\u8f83\u9ad8\u7684\u6bcf\u6b65\u63a8\u7406\u6210\u672c\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u5ef6\u8fdf\uff0c\u6709\u65f6\u6bcf\u4e2a\u4efb\u52a1\u9700\u8981\u51e0\u5206\u949f\u3002", "method": "RT-cache \u901a\u8fc7\u5b58\u50a8\u5927\u91cf\u4e4b\u524d\u6210\u529f\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u5e76\u68c0\u7d22\u76f8\u5173\u591a\u6b65\u9aa4\u8fd0\u52a8\u7247\u6bb5\uff0c\u4ece\u800c\u51cf\u5c11\u63a8\u7406\u5f00\u9500\u3002\u5b83\u7ed3\u5408\u4e86 Memory Builder \u548c Trajectory Retrieval\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u68c0\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728 Open-X Embodiment \u6570\u636e\u96c6\u548c\u5176\u4ed6\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRT-cache \u5728\u4efb\u52a1\u6267\u884c\u901f\u5ea6\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u7f3a\u4e4f\u68c0\u7d22\u7684\u57fa\u7ebf\u3002", "conclusion": "RT-cache \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5b9e\u65f6\u64cd\u4f5c\uff0c\u80fd\u591f\u5728\u4efb\u52a1\u6267\u884c\u901f\u5ea6\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u7f3a\u4e4f\u68c0\u7d22\u7684\u57fa\u7ebf\u3002"}}
{"id": "2505.08816", "pdf": "https://arxiv.org/pdf/2505.08816", "abs": "https://arxiv.org/abs/2505.08816", "authors": ["Ippokratis Koukoulis", "Ilias Syrigos", "Thanasis Korakis"], "title": "Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted at IFIP Networking 2025. Code available at\n  https://github.com/koukipp/contrastive_transformers_ids", "summary": "As the digital landscape becomes more interconnected, the frequency and\nseverity of zero-day attacks, have significantly increased, leading to an\nurgent need for innovative Intrusion Detection Systems (IDS). Machine\nLearning-based IDS that learn from the network traffic characteristics and can\ndiscern attack patterns from benign traffic offer an advanced solution to\ntraditional signature-based IDS. However, they heavily rely on labeled\ndatasets, and their ability to generalize when encountering unseen traffic\npatterns remains a challenge. This paper proposes a novel self-supervised\ncontrastive learning approach based on transformer encoders, specifically\ntailored for generalizable intrusion detection on raw packet sequences. Our\nproposed learning scheme employs a packet-level data augmentation strategy\ncombined with a transformer-based architecture to extract and generate\nmeaningful representations of traffic flows. Unlike traditional methods reliant\non handcrafted statistical features (NetFlow), our approach automatically\nlearns comprehensive packet sequence representations, significantly enhancing\nperformance in anomaly identification tasks and supervised learning for\nintrusion detection. Our transformer-based framework exhibits better\nperformance in comparison to existing NetFlow self-supervised methods.\nSpecifically, we achieve up to a 3% higher AUC in anomaly detection for\nintra-dataset evaluation and up to 20% higher AUC scores in inter-dataset\nevaluation. Moreover, our model provides a strong baseline for supervised\nintrusion detection with limited labeled data, exhibiting an improvement over\nself-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated\non the same dataset. Additionally, we show the adaptability of our pretrained\nmodel when fine-tuned across different datasets, demonstrating strong\nperformance even when lacking benign data from the target domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5165\u4fb5\u68c0\u6d4b\uff0c\u8be5\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u548c\u6709\u76d1\u7763\u7684\u5165\u4fb5\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u6709\u9650\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u73af\u5883\u7684\u65e5\u76ca\u4e92\u8054\uff0c\u96f6\u65e5\u653b\u51fb\u7684\u9891\u7387\u548c\u4e25\u91cd\u6027\u663e\u8457\u589e\u52a0\uff0c\u8feb\u5207\u9700\u8981\u521b\u65b0\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u3002\u673a\u5668\u5b66\u4e60-based IDS\u80fd\u591f\u4ece\u7f51\u7edc\u6d41\u91cf\u7279\u5f81\u4e2d\u5b66\u4e60\u5e76\u8bc6\u522b\u653b\u51fb\u6a21\u5f0f\uff0c\u662f\u4f20\u7edf\u57fa\u4e8e\u7b7e\u540d\u7684IDS\u7684\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e25\u91cd\u4f9d\u8d56\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u5728\u9047\u5230\u672a\u89c1\u8fc7\u7684\u6d41\u91cf\u6a21\u5f0f\u65f6\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u5b58\u5728\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u7684\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4ee5\u63d0\u53d6\u548c\u751f\u6210\u6d41\u91cf\u6d41\u7684\u6709\u610f\u4e49\u8868\u793a\u3002\u4e0e\u4f9d\u8d56\u4e8e\u624b\u5de5\u7edf\u8ba1\u7279\u5f81\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u81ea\u52a8\u5b66\u4e60\u5168\u9762\u7684\u6570\u636e\u5305\u5e8f\u5217\u8868\u793a\u3002", "result": "\u4e0e\u73b0\u6709\u7684NetFlow\u81ea\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8eTransformer\u7684\u6846\u67b6\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u6570\u636e\u96c6\u5185\u8bc4\u4f30\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684AUC\u63d0\u9ad8\u4e86\u9ad8\u8fbe3%\uff0c\u5728\u6570\u636e\u96c6\u95f4\u8bc4\u4f30\u4e2d\uff0cAUC\u5206\u6570\u63d0\u9ad8\u4e86\u9ad8\u8fbe20%\u3002\u6b64\u5916\uff0c\u5f53\u5728\u76f8\u540c\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u548c\u8bc4\u4f30\u65f6\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u81ea\u76d1\u7763NetFlow\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad81.5%\u7684AUC\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u7684\u65b0\u578b\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u5728\u539f\u59cb\u6570\u636e\u5305\u5e8f\u5217\u4e0a\u8fdb\u884c\u901a\u7528\u5165\u4fb5\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u548c\u6709\u76d1\u7763\u7684\u5165\u4fb5\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u6709\u9650\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5fae\u8c03\u5c55\u793a\u4e86\u5176\u9002\u5e94\u6027\uff0c\u5e76\u5728\u7f3a\u4e4f\u76ee\u6807\u9886\u57df\u826f\u6027\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.09498", "pdf": "https://arxiv.org/pdf/2505.09498", "abs": "https://arxiv.org/abs/2505.09498", "authors": ["Bo Zhang", "Shuo Li", "Runhe Tian", "Yang Yang", "Jixin Tang", "Jinhao Zhou", "Lin Ma"], "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Flash-VL 2B\uff0c\u8fd9\u662f\u4e00\u79cd\u4f18\u5316\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4ee5\u5b9e\u73b0\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4ee5\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5148\u8fdb\u7684\u67b6\u6784\u589e\u5f3a\u548c\u9ad8\u6548\u7684\u8ba1\u7b97\u7b56\u7565\uff0cFlash-VL 2B \u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u6765\u6700\u5927\u5316\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5728\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u4e0a\u7684\u7ade\u4e89\u529b\u3002\u65b9\u6cd5\u5305\u62ec\u5b9a\u5236\u7684\u67b6\u6784\u9009\u62e9\u3001\u4ee4\u724c\u538b\u7f29\u673a\u5236\u3001\u6570\u636e\u6574\u7406\u3001\u8bad\u7ec3\u65b9\u6848\u4ee5\u53ca\u4e00\u79cd\u79f0\u4e3a\u9690\u5f0f\u8bed\u4e49\u62fc\u63a5\u7684\u65b0\u56fe\u50cf\u5904\u7406\u6280\u672f\u3002", "result": "\u901a\u8fc7\u572811\u4e2a\u6807\u51c6VLM\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u6211\u4eec\u5c55\u793a\u4e86Flash-VL 2B \u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Flash-VL 2B \u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u548c\u5927\u89c4\u6a21\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2505.09062", "pdf": "https://arxiv.org/pdf/2505.09062", "abs": "https://arxiv.org/abs/2505.09062", "authors": ["Junda Zhao", "Yuliang Song", "Eldan Cohen"], "title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.7"], "comment": "Accepted by the Journal of Systems and Software", "summary": "Recent advancements in source code summarization have leveraged\ntransformer-based pre-trained models, including Large Language Models of Code\n(LLMCs), to automate and improve the generation of code summaries. However,\nexisting methods often focus on generating a single high-quality summary for a\ngiven source code, neglecting scenarios where the generated summary might be\ninadequate and alternative options are needed. In this paper, we introduce\nVariational Prefix Tuning (VPT), a novel approach that enhances pre-trained\nmodels' ability to generate diverse yet accurate sets of summaries, allowing\nthe user to choose the most suitable one for the given source code. Our method\nintegrates a Conditional Variational Autoencoder (CVAE) framework as a modular\ncomponent into pre-trained models, enabling us to model the distribution of\nobserved target summaries and sample continuous embeddings to be used as\nprefixes to steer the generation of diverse outputs during decoding.\nImportantly, we construct our method in a parameter-efficient manner,\neliminating the need for expensive model retraining, especially when using\nLLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset\nof generated summaries, optimizing both the diversity and the accuracy of the\noptions presented to users. We present extensive experimental evaluations using\nwidely used datasets and current state-of-the-art pre-trained code\nsummarization models to demonstrate the effectiveness of our approach and its\nadaptability across models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u53d8\u5206\u524d\u7f00\u8c03\u4f18\uff08VPT\uff09\uff0c\u4ee5\u589e\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u591a\u6837\u4e14\u51c6\u786e\u7684\u4ee3\u7801\u6458\u8981\u96c6\u7684\u80fd\u529b\uff0c\u4ece\u800c\u5141\u8bb8\u7528\u6237\u9009\u62e9\u6700\u9002\u5408\u7ed9\u5b9a\u6e90\u4ee3\u7801\u7684\u6458\u8981\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u4e3a\u7ed9\u5b9a\u7684\u6e90\u4ee3\u7801\u751f\u6210\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6458\u8981\uff0c\u800c\u5ffd\u7565\u4e86\u751f\u6210\u7684\u6458\u8981\u53ef\u80fd\u4e0d\u5145\u5206\u7684\u60c5\u51b5\uff0c\u9700\u8981\u66ff\u4ee3\u9009\u9879\u3002", "method": "\u672c\u6587\u5c06\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u6846\u67b6\u4f5c\u4e3a\u6a21\u5757\u5316\u7ec4\u4ef6\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\uff0c\u4ee5\u5efa\u6a21\u89c2\u5bdf\u5230\u7684\u76ee\u6807\u6458\u8981\u5206\u5e03\uff0c\u5e76\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u91c7\u6837\u8fde\u7eed\u5d4c\u5165\u4f5c\u4e3a\u524d\u7f00\u6765\u5f15\u5bfc\u751f\u6210\u591a\u6837\u5316\u7684\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u53cc\u6807\u51c6\u91cd\u6392\u5e8f\u65b9\u6cd5\u9009\u62e9\u751f\u6210\u6458\u8981\u7684\u5b50\u96c6\uff0c\u4f18\u5316\u4e86\u63d0\u4f9b\u7684\u9009\u9879\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u672c\u6587\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4f7f\u7528\u5e38\u7528\u6570\u636e\u96c6\u548c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u4ee3\u7801\u6458\u8981\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u53d8\u5206\u524d\u7f00\u8c03\u4f18\uff08VPT\uff09\uff0c\u4ee5\u589e\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u591a\u6837\u4e14\u51c6\u786e\u7684\u4ee3\u7801\u6458\u8981\u96c6\u7684\u80fd\u529b\uff0c\u4ece\u800c\u5141\u8bb8\u7528\u6237\u9009\u62e9\u6700\u9002\u5408\u7ed9\u5b9a\u6e90\u4ee3\u7801\u7684\u6458\u8981\u3002"}}
{"id": "2505.09528", "pdf": "https://arxiv.org/pdf/2505.09528", "abs": "https://arxiv.org/abs/2505.09528", "authors": ["Jeffrey Wen", "Rizwan Ahmad", "Philip Schniter"], "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems", "categories": ["cs.CV"], "comment": null, "summary": "In imaging inverse problems, we would like to know how close the recovered\nimage is to the true image in terms of full-reference image quality (FRIQ)\nmetrics like PSNR, SSIM, LPIPS, etc. This is especially important in\nsafety-critical applications like medical imaging, where knowing that, say, the\nSSIM was poor could potentially avoid a costly misdiagnosis. But since we don't\nknow the true image, computing FRIQ is non-trivial. In this work, we combine\nconformal prediction with approximate posterior sampling to construct bounds on\nFRIQ that are guaranteed to hold up to a user-specified error probability. We\ndemonstrate our approach on image denoising and accelerated magnetic resonance\nimaging (MRI) problems. Code is available at\nhttps://github.com/jwen307/quality_uq.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u5408\u9884\u6d4b\u548c\u8fd1\u4f3c\u540e\u9a8c\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efaFRIQ\u7684\u8fb9\u754c\uff0c\u8fd9\u4e9b\u8fb9\u754c\u5728\u7528\u6237\u6307\u5b9a\u7684\u8bef\u5dee\u6982\u7387\u4e0b\u662f\u4fdd\u8bc1\u6210\u7acb\u7684\u3002", "motivation": "\u5728\u6210\u50cf\u9006\u95ee\u9898\u4e2d\uff0c\u4e86\u89e3\u6062\u590d\u7684\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u8ddd\u79bb\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u6210\u50cf\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e0d\u77e5\u9053\u771f\u5b9e\u56fe\u50cf\uff0c\u8ba1\u7b97FRIQ\u662f\u975e\u663e\u800c\u6613\u89c1\u7684\u3002", "method": "\u672c\u6587\u7ed3\u5408\u4e86\u7b26\u5408\u9884\u6d4b\u548c\u8fd1\u4f3c\u540e\u9a8c\u91c7\u6837\uff0c\u4ee5\u6784\u5efaFRIQ\u7684\u8fb9\u754c\u3002", "result": "\u672c\u6587\u5728\u56fe\u50cf\u53bb\u566a\u548c\u52a0\u901f\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u95ee\u9898\u4e0a\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728https://github.com/jwen307/quality_uq\u4e0a\u83b7\u5f97\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u7b26\u5408\u9884\u6d4b\u548c\u8fd1\u4f3c\u540e\u9a8c\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efaFRIQ\u7684\u8fb9\u754c\uff0c\u8fd9\u4e9b\u8fb9\u754c\u5728\u7528\u6237\u6307\u5b9a\u7684\u8bef\u5dee\u6982\u7387\u4e0b\u662f\u4fdd\u8bc1\u6210\u7acb\u7684\u3002"}}
{"id": "2505.09081", "pdf": "https://arxiv.org/pdf/2505.09081", "abs": "https://arxiv.org/abs/2505.09081", "authors": ["Gaurav Koley"], "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation", "categories": ["cs.SI", "cs.AI", "cs.MA"], "comment": null, "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SALM\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u8bed\u8a00\u6a21\u578b\u6574\u5408\u5230\u793e\u4ea4\u7f51\u7edc\u6a21\u62df\u4e2d\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u5b9e\u73b0\u524d\u6240\u672a\u6709\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u884c\u4e3a\u65b9\u6cd5\u9650\u5236\u4e86\u793e\u4f1a\u7cfb\u7edf\u4ee3\u7406\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u8d85\u8d8a\u9884\u5b9a\u4e49\u89c4\u5219\u5e76\u5229\u7528\u4eba\u7c7b\u793e\u4f1a\u4e92\u52a8\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u63d0\u793a\u67b6\u6784\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u4e0eSNAP ego networks\u7684\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86SALM\u6846\u67b6\u5728\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5efa\u6a21\u793e\u4f1a\u73b0\u8c61\u7684\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e8680%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\u548c9.5%\u7684\u6b21\u7ebf\u6027\u5185\u5b58\u589e\u957f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86SALM\u6846\u67b6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5efa\u6a21\u957f\u671f\u793e\u4f1a\u73b0\u8c61\u5e76\u4fdd\u6301\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u884c\u4e3a\u4fdd\u771f\u5ea6\u7684LLM\u6846\u67b6\u3002"}}
{"id": "2505.09529", "pdf": "https://arxiv.org/pdf/2505.09529", "abs": "https://arxiv.org/abs/2505.09529", "authors": ["Mohamed Moustafa", "Joseph Lemley", "Peter Corcoran"], "title": "Contactless Cardiac Pulse Monitoring Using Event Cameras", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "comment": "This paper is a preprint of a paper submitted to IEEE Access and is\n  currently under review", "summary": "Time event cameras are a novel technology for recording scene information at\nextremely low latency and with low power consumption. Event cameras output a\nstream of events that encapsulate pixel-level light intensity changes within\nthe scene, capturing information with a higher dynamic range and temporal\nresolution than traditional cameras. This study investigates the contact-free\nreconstruction of an individual's cardiac pulse signal from time event\nrecording of their face using a supervised convolutional neural network (CNN)\nmodel. An end-to-end model is trained to extract the cardiac signal from a\ntwo-dimensional representation of the event stream, with model performance\nevaluated based on the accuracy of the calculated heart rate. The experimental\nresults confirm that physiological cardiac information in the facial region is\neffectively preserved within the event stream, showcasing the potential of this\nnovel sensor for remote heart rate monitoring. The model trained on event\nframes achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)\ncompared to the RMSE of 2.92 bpm achieved by the baseline model trained on\nstandard camera frames. Furthermore, models trained on event frames generated\nat 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an\nRMSE of 2.54 and 2.13 bpm, respectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u76d1\u7763\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\uff0c\u4ece\u65f6\u95f4\u4e8b\u4ef6\u8bb0\u5f55\u4e2d\u65e0\u63a5\u89e6\u91cd\u5efa\u4e2a\u4f53\u7684\u5fc3\u810f\u4fe1\u53f7\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8be5\u6280\u672f\u5728\u8fdc\u7a0b\u5fc3\u7387\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u65f6\u95f4\u4e8b\u4ef6\u6444\u50cf\u673a\u662f\u4e00\u79cd\u65b0\u6280\u672f\uff0c\u53ef\u4ee5\u4ee5\u6781\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u529f\u8017\u8bb0\u5f55\u573a\u666f\u4fe1\u606f\u3002\u4e8b\u4ef6\u6444\u50cf\u673a\u8f93\u51fa\u5305\u542b\u50cf\u7d20\u7ea7\u5149\u5f3a\u5ea6\u53d8\u5316\u7684\u4e8b\u4ef6\u6d41\uff0c\u5176\u52a8\u6001\u8303\u56f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u9ad8\u4e8e\u4f20\u7edf\u6444\u50cf\u673a\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u4ece\u65f6\u95f4\u4e8b\u4ef6\u8bb0\u5f55\u4e2d\u65e0\u63a5\u89e6\u91cd\u5efa\u5fc3\u810f\u4fe1\u53f7\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u76d1\u7763\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\uff0c\u4ece\u4e2a\u4f53\u9762\u90e8\u7684\u65f6\u95f4\u4e8b\u4ef6\u8bb0\u5f55\u4e2d\u65e0\u63a5\u89e6\u91cd\u5efa\u5176\u5fc3\u810f\u8109\u51b2\u4fe1\u53f7\u3002\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6a21\u578b\uff0c\u4ee5\u4ece\u4e8b\u4ef6\u6d41\u7684\u4e8c\u7ef4\u8868\u793a\u4e2d\u63d0\u53d6\u5fc3\u810f\u4fe1\u53f7\uff0c\u5e76\u6839\u636e\u8ba1\u7b97\u7684\u5fc3\u7387\u51c6\u786e\u6027\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u786e\u8ba4\u4e86\u9762\u90e8\u533a\u57df\u7684\u751f\u7406\u5fc3\u810f\u4fe1\u606f\u5728\u4e8b\u4ef6\u6d41\u4e2d\u5f97\u5230\u6709\u6548\u4fdd\u7559\u3002\u4e0e\u57fa\u4e8e\u6807\u51c6\u6444\u50cf\u673a\u5e27\u7684\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u57fa\u4e8e\u4e8b\u4ef6\u5e27\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u3002\u6b64\u5916\uff0c\u4ee560\u548c120 FPS\u751f\u6210\u7684\u4e8b\u4ef6\u5e27\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e30 FPS\u6807\u51c6\u6444\u50cf\u673a\u7684\u7ed3\u679c\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u751f\u7406\u5fc3\u810f\u4fe1\u606f\u5728\u9762\u90e8\u533a\u57df\u6709\u6548\u5730\u4fdd\u7559\u5728\u4e8b\u4ef6\u6d41\u4e2d\uff0c\u5c55\u793a\u4e86\u8fd9\u79cd\u65b0\u578b\u4f20\u611f\u5668\u5728\u8fdc\u7a0b\u5fc3\u7387\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.08819", "pdf": "https://arxiv.org/pdf/2505.08819", "abs": "https://arxiv.org/abs/2505.08819", "authors": ["Asahi Miyazaki", "Tsuyoshi Okita"], "title": "Thoughts on Objectives of Sparse and Hierarchical Masked Image Model", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "9 pages, 11 figures", "summary": "Masked image modeling is one of the most poplular objectives of training.\nRecently, the SparK model has been proposed with superior performance among\nself-supervised learning models. This paper proposes a new mask pattern for\nthis SparK model, proposing it as the Mesh Mask-ed SparK model. We report the\neffect of the mask pattern used for image masking in pre-training on\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u906e\u7f69\u6a21\u5f0f\uff0c\u7528\u4e8e\u6539\u8fdbSparK\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdbSparK\u6a21\u578b\u7684\u6027\u80fd\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u906e\u7f69\u6a21\u5f0f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u906e\u7f69\u6a21\u5f0f\uff0c\u7528\u4e8e\u6539\u8fdbSparK\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u672c\u6587\u62a5\u544a\u4e86\u7528\u4e8e\u56fe\u50cf\u906e\u7f69\u7684\u906e\u7f69\u6a21\u5f0f\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u906e\u7f69\u6a21\u5f0f\uff0c\u5373Mesh Mask-ed SparK\u6a21\u578b\uff0c\u4ee5\u6539\u8fdbSparK\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09562", "pdf": "https://arxiv.org/pdf/2505.09562", "abs": "https://arxiv.org/abs/2505.09562", "authors": ["Nicola Marinello", "Simen Cassiman", "Jonas Heylen", "Marc Proesmans", "Luc Van Gool"], "title": "Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 Workshop on Autonomous Driving", "summary": "Autonomous vehicles need a complete map of their surroundings to plan and\nact. This has sparked research into the tasks of 3D occupancy prediction, 3D\nscene completion, and 3D panoptic scene completion, which predict a dense map\nof the ego vehicle's surroundings as a voxel grid. Scene completion extends\noccupancy prediction by predicting occluded regions of the voxel grid, and\npanoptic scene completion further extends this task by also distinguishing\nobject instances within the same class; both aspects are crucial for path\nplanning and decision-making. However, 3D panoptic scene completion is\ncurrently underexplored. This work introduces a novel framework for 3D panoptic\nscene completion that extends existing 3D semantic scene completion models. We\npropose an Object Module and Panoptic Module that can easily be integrated with\n3D occupancy and scene completion methods presented in the literature. Our\napproach leverages the available annotations in occupancy benchmarks, allowing\nindividual object shapes to be learned as a differentiable problem. The code is\navailable at https://github.com/nicolamarinello/OffsetOcc .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e3D panoptic scene completion\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u73b0\u67093D\u8bed\u4e49\u573a\u666f\u5b8c\u6210\u6a21\u578b\u4e0a\u7684\u6269\u5c55\u80fd\u529b\u3002", "motivation": "3D panoptic scene completion\u76ee\u524d\u7814\u7a76\u8f83\u5c11\uff0c\u4f46\u5b83\u662f\u8def\u5f84\u89c4\u5212\u548c\u51b3\u7b56\u5236\u5b9a\u7684\u5173\u952e\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aObject Module\u548cPanoptic Module\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u6587\u732e\u4e2d\u76843D\u5360\u7528\u548c\u573a\u666f\u5b8c\u6210\u65b9\u6cd5\u4e2d\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u5360\u7528\u57fa\u51c6\u4e2d\u7684\u53ef\u7528\u6ce8\u91ca\uff0c\u4f7f\u4e2a\u4f53\u5bf9\u8c61\u5f62\u72b6\u80fd\u591f\u4f5c\u4e3a\u53ef\u5fae\u95ee\u9898\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u73b0\u6709\u76843D\u8bed\u4e49\u573a\u666f\u5b8c\u6210\u6a21\u578b\uff0c\u5e76\u4e14\u80fd\u591f\u5b66\u4e60\u4e2a\u4f53\u5bf9\u8c61\u5f62\u72b6\u4f5c\u4e3a\u53ef\u5fae\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e3D panoptic scene completion\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u73b0\u67093D\u8bed\u4e49\u573a\u666f\u5b8c\u6210\u6a21\u578b\u4e0a\u7684\u6269\u5c55\u80fd\u529b\u3002"}}
{"id": "2505.08822", "pdf": "https://arxiv.org/pdf/2505.08822", "abs": "https://arxiv.org/abs/2505.08822", "authors": ["Yuhao Wang", "Kailai Wang", "Songhua Hu", "Yunpeng", "Zhang", "Gino Lim", "Pengyu Zhu"], "title": "The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics", "categories": ["cs.CY", "cs.LG", "physics.soc-ph"], "comment": null, "summary": "The rapid evolution of the transportation cybersecurity ecosystem,\nencompassing cybersecurity, automotive, and transportation and logistics\nsectors, will lead to the formation of distinct spatial clusters and visitor\nflow patterns across the US. This study examines the spatiotemporal dynamics of\nvisitor flows, analyzing how socioeconomic factors shape industry clustering\nand workforce distribution within these evolving sectors. To model and predict\nvisitor flow patterns, we develop a BiTransGCN framework, integrating an\nattention-based Transformer architecture with a Graph Convolutional Network\nbackbone. By integrating AI-enabled forecasting techniques with spatial\nanalysis, this study improves our ability to track, interpret, and anticipate\nchanges in industry clustering and mobility trends, thereby supporting\nstrategic planning for a secure and resilient transportation network. It offers\na data-driven foundation for economic planning, workforce development, and\ntargeted investments in the transportation cybersecurity ecosystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4ea4\u901a\u7f51\u7edc\u5b89\u5168\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u8bbf\u5ba2\u6d41\u52a8\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7AI\u548c\u7a7a\u95f4\u5206\u6790\u63d0\u9ad8\u5bf9\u884c\u4e1a\u96c6\u7fa4\u548c\u79fb\u52a8\u8d8b\u52bf\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u4ea4\u901a\u7f51\u7edc\u5b89\u5168\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u5176\u65f6\u7a7a\u52a8\u6001\uff0c\u4ee5\u53ca\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u884c\u4e1a\u96c6\u7fa4\u548c\u52b3\u52a8\u529b\u5206\u5e03\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2aBiTransGCN\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u6ce8\u610f\u529b\u7684Transformer\u67b6\u6784\u4e0e\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u7ed3\u5408\uff0c\u4ee5\u5efa\u6a21\u548c\u9884\u6d4b\u8bbf\u5ba2\u6d41\u52a8\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u5c06AI\u9a71\u52a8\u7684\u9884\u6d4b\u6280\u672f\u4e0e\u7a7a\u95f4\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u672c\u6587\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u3001\u89e3\u91ca\u548c\u9884\u6d4b\u884c\u4e1a\u96c6\u7fa4\u548c\u79fb\u52a8\u8d8b\u52bf\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u9884\u6d4b\u6280\u672f\u4e0e\u7a7a\u95f4\u5206\u6790\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u5bf9\u884c\u4e1a\u96c6\u7fa4\u548c\u79fb\u52a8\u8d8b\u52bf\u7684\u8ddf\u8e2a\u3001\u89e3\u91ca\u548c\u9884\u6d4b\u80fd\u529b\uff0c\u4ece\u800c\u652f\u6301\u5b89\u5168\u548c\u6709\u5f39\u6027\u7684\u4ea4\u901a\u7f51\u7edc\u7684\u6218\u7565\u89c4\u5212\u3002\u5b83\u4e3a\u7ecf\u6d4e\u89c4\u5212\u3001\u52b3\u52a8\u529b\u53d1\u5c55\u548c\u4ea4\u901a\u7f51\u7edc\u5b89\u5168\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u9488\u5bf9\u6027\u6295\u8d44\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u57fa\u7840\u3002"}}
{"id": "2505.09564", "pdf": "https://arxiv.org/pdf/2505.09564", "abs": "https://arxiv.org/abs/2505.09564", "authors": ["Anne-Marie Rickmann", "Stephanie L. Thorn", "Shawn S. Ahn", "Supum Lee", "Selen Uman", "Taras Lysyy", "Rachel Burns", "Nicole Guerrera", "Francis G. Spinale", "Jason A. Burdick", "Albert J. Sinusas", "James S. Duncan"], "title": "Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation", "categories": ["cs.CV"], "comment": "accepted at FIMH 2025", "summary": "Cardiac image segmentation is an important step in many cardiac image\nanalysis and modeling tasks such as motion tracking or simulations of cardiac\nmechanics. While deep learning has greatly advanced segmentation in clinical\nsettings, there is limited work on pre-clinical imaging, notably in porcine\nmodels, which are often used due to their anatomical and physiological\nsimilarity to humans. However, differences between species create a domain\nshift that complicates direct model transfer from human to pig data.\n  Recently, foundation models trained on large human datasets have shown\npromise for robust medical image segmentation; yet their applicability to\nporcine data remains largely unexplored. In this work, we investigate whether\nfoundation models can generate sufficiently accurate pseudo-labels for pig\ncardiac CT and propose a simple self-training approach to iteratively refine\nthese labels. Our method requires no manually annotated pig data, relying\ninstead on iterative updates to improve segmentation quality. We demonstrate\nthat this self-training process not only enhances segmentation accuracy but\nalso smooths out temporal inconsistencies across consecutive frames. Although\nour results are encouraging, there remains room for improvement, for example by\nincorporating more sophisticated self-training strategies and by exploring\nadditional foundation models and other cardiac imaging technologies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u7840\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4e3a\u732a\u5fc3\u810fCT\u751f\u6210\u8db3\u591f\u51c6\u786e\u7684\u4f2a\u6807\u7b7e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u6570\u636e\u7684\u81ea\u6211\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5206\u5272\u8d28\u91cf\u3002", "motivation": "\u7531\u4e8e\u7269\u79cd\u4e4b\u95f4\u7684\u5dee\u5f02\u5bfc\u81f4\u4ece\u4eba\u7c7b\u6570\u636e\u5230\u732a\u6570\u636e\u7684\u9886\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u57fa\u7840\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4e3a\u732a\u5fc3\u810fCT\u751f\u6210\u8db3\u591f\u51c6\u786e\u7684\u4f2a\u6807\u7b7e\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u81ea\u6211\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u6765\u63d0\u9ad8\u5206\u5272\u8d28\u91cf\uff0c\u800c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u7684\u732a\u6570\u636e\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u81ea\u6211\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u8fd8\u5e73\u6ed1\u4e86\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u65f6\u5e8f\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u867d\u7136\u6211\u4eec\u7684\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\uff0c\u4f46\u4ecd\u5b58\u5728\u6539\u8fdb\u7684\u7a7a\u95f4\uff0c\u4f8b\u5982\u901a\u8fc7\u5f15\u5165\u66f4\u590d\u6742\u7684\u81ea\u6211\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u53ca\u63a2\u7d22\u5176\u4ed6\u57fa\u7840\u6a21\u578b\u548c\u5176\u4ed6\u5fc3\u810f\u6210\u50cf\u6280\u672f\u3002"}}
{"id": "2505.09091", "pdf": "https://arxiv.org/pdf/2505.09091", "abs": "https://arxiv.org/abs/2505.09091", "authors": ["Zeeshan Ahmad", "Shudi Bao", "Meng Chen"], "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "comment": null, "summary": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08DPN-GAN\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u5468\u671f\u6027\u504f\u5dee\u548c\u53ef\u53d8\u5f62\u5377\u79ef\u64cd\u4f5c\uff0c\u63d0\u9ad8\u4e86\u97f3\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u901a\u5e38\u4f9d\u8d56\u4e8e\u5e26\u5bbd\u6709\u9650\u7684\u6885\u5c14\u9891\u8c31\u56fe\uff0c\u8fd9\u9650\u5236\u4e86\u751f\u6210\u97f3\u9891\u5e8f\u5217\u7684\u5206\u8fa8\u7387\uff0c\u5e76\u5728\u6761\u4ef6\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DPN-GAN\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62\u5468\u671f\u7f51\u7edc\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08DPN-GAN\uff09\uff0c\u8be5\u7f51\u7edc\u7ed3\u5408\u4e86\u57fa\u4e8e\u5185\u6838\u7684\u5468\u671fReLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ee5\u5728\u97f3\u9891\u751f\u6210\u4e2d\u5f15\u5165\u5468\u671f\u6027\u504f\u5dee\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528\u53ef\u53d8\u5f62\u5377\u79ef\u589e\u5f3a\u4e86\u5224\u522b\u5668\u7f51\u7edc\uff0c\u4ee5\u66f4\u597d\u5730\u533a\u5206\u771f\u5b9e\u548c\u751f\u6210\u7684\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDPN-GAN\u5728\u5206\u5e03\u5916\u548c\u566a\u58f0\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5c55\u793a\u4e86\u5176\u7a33\u5065\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "DPN-GAN\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684GAN\u67b6\u6784\uff0c\u5e76\u5728\u5408\u6210\u97f3\u9891\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.09568", "pdf": "https://arxiv.org/pdf/2505.09568", "abs": "https://arxiv.org/abs/2505.09568", "authors": ["Jiuhai Chen", "Zhiyang Xu", "Xichen Pan", "Yushi Hu", "Can Qin", "Tom Goldstein", "Lifu Huang", "Tianyi Zhou", "Saining Xie", "Silvio Savarese", "Le Xue", "Caiming Xiong", "Ran Xu"], "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u53d8\u538b\u5668\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684CLIP\u56fe\u50cf\u7279\u5f81\uff0c\u4e0e\u4f20\u7edf\u7684VAE\u8868\u793a\u76f8\u6bd4\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u90fd\u6709\u6240\u63d0\u9ad8\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u987a\u5e8f\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5148\u8fdb\u884c\u56fe\u50cf\u7406\u89e3\uff0c\u7136\u540e\u8fdb\u884c\u56fe\u50cf\u751f\u6210\uff09\u5177\u6709\u5b9e\u9645\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u5728\u4fdd\u6301\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u7684\u540c\u65f6\u53d1\u5c55\u4e86\u5f3a\u5927\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002\u6700\u540e\uff0c\u6211\u4eec\u7cbe\u5fc3\u7b56\u5212\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6BLIP3o-60k\uff0c\u7528\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u63d0\u793aGPT-4o\u4f7f\u7528\u6db5\u76d6\u5404\u79cd\u573a\u666f\u3001\u7269\u4f53\u3001\u4eba\u7c7b\u624b\u52bf\u7b49\u7684\u591a\u6837\u5316\u6807\u9898\u3002\u57fa\u4e8e\u6211\u4eec\u7684\u521b\u65b0\u6a21\u578b\u8bbe\u8ba1\u3001\u8bad\u7ec3\u65b9\u6848\u548c\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86BLIP3-o\uff0c\u4e00\u5957\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u3002BLIP3-o\u5728\u5927\u591a\u6570\u6d41\u884c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6db5\u76d6\u4e86\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002\u4e3a\u4e86\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\uff0c\u6211\u4eec\u5b8c\u5168\u5f00\u6e90\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u5305\u62ec\u4ee3\u7801\u3001\u6a21\u578b\u6743\u91cd\u3001\u8bad\u7ec3\u811a\u672c\u548c\u9884\u8bad\u7ec3\u53ca\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u3002", "motivation": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies.", "method": "We introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more.", "result": "This design yields both higher training efficiency and improved generative quality. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models.", "conclusion": "BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets."}}
{"id": "2505.09108", "pdf": "https://arxiv.org/pdf/2505.09108", "abs": "https://arxiv.org/abs/2505.09108", "authors": ["Fernando Cladera", "Zachary Ravichandran", "Jason Hughes", "Varun Murali", "Carlos Nieto-Granda", "M. Ani Hsieh", "George J. Pappas", "Camillo J. Taylor", "Vijay Kumar"], "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown Environments", "categories": ["cs.RO", "cs.AI"], "comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR", "summary": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u4f7f\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u80fd\u591f\u534f\u4f5c\u5b8c\u6210\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u7684\u4efb\u52a1\uff0c\u5e76\u5728\u57ce\u5e02\u548c\u519c\u6751\u5730\u533a\u5c55\u793a\u4e86\u5176\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6210\u719f\uff0c\u7528\u6237\u5e0c\u671b\u4ee5\u610f\u56fe\u7ea7\u522b\u800c\u975e\u4f4e\u7ea7\u7ec6\u8282\u6765\u6307\u5b9a\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5b9e\u73b0\u8bed\u8a00\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u56e2\u961f\u9700\u8981\u514b\u670d\u91cd\u5927\u6280\u672f\u969c\u788d\uff0c\u5305\u62ec\u8bed\u4e49\u63a8\u7406\u3001\u5f02\u6784\u673a\u5668\u4eba\u534f\u8c03\u4ee5\u53ca\u5728\u95f4\u6b47\u6027\u901a\u4fe1\u4e0b\u7684\u7a33\u5065\u7b56\u7565\u3002", "method": "\u672c\u6587\u5229\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u5668\uff0c\u5728\u5728\u7ebf\u6784\u5efa\u5e76\u673a\u4f1a\u6027\u5171\u4eab\u7684\u8bed\u4e49-\u5ea6\u91cf\u5730\u56fe\u4e0a\u8fdb\u884c\u8bed\u4e49\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u6620\u5c04\u4e3b\u52a8\u83b7\u53d6\u4fe1\u606f\u3002", "result": "\u5728\u5730\u9762\u548c\u7a7a\u5730\u56e2\u961f\u5b9e\u9a8c\u4e2d\uff0c\u672c\u6587\u7cfb\u7edf\u5728\u4e03\u79cd\u4e0d\u540c\u7684\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u516c\u91cc\u7ea7\u7684\u5bfc\u822a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9996\u4e2a\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u534f\u4f5c\u5b8c\u6210\u4efb\u52a1\u7684\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5728\u57ce\u5e02\u548c\u519c\u6751\u5730\u533a\u8fdb\u884c\u4efb\u52a1\u9a71\u52a8\u5bfc\u822a\u7684\u80fd\u529b\u3002"}}
{"id": "2505.08837", "pdf": "https://arxiv.org/pdf/2505.08837", "abs": "https://arxiv.org/abs/2505.08837", "authors": ["Muhammad Saqib", "Dipkumar Mehta", "Fnu Yashu", "Shubham Malhotra"], "title": "Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning", "categories": ["cs.CR", "cs.CV", "cs.DC", "cs.LG", "cs.NI"], "comment": "10 pages, 6 figures, 1 table", "summary": "The security of cloud environments, such as Amazon Web Services (AWS), is\ncomplex and dynamic. Static security policies have become inadequate as threats\nevolve and cloud resources exhibit elasticity [1]. This paper addresses the\nlimitations of static policies by proposing a security policy management\nframework that uses reinforcement learning (RL) to adapt dynamically.\nSpecifically, we employ deep reinforcement learning algorithms, including deep\nQ Networks and proximal policy optimization, enabling the learning and\ncontinuous adjustment of controls such as firewall rules and Identity and\nAccess Management (IAM) policies. The proposed RL based solution leverages\ncloud telemetry data (AWS Cloud Trail logs, network traffic data, threat\nintelligence feeds) to continuously refine security policies, maximizing threat\nmitigation, and compliance while minimizing resource impact. Experimental\nresults demonstrate that our adaptive RL based framework significantly\noutperforms static policies, achieving higher intrusion detection rates (92%\ncompared to 82% for static policies) and substantially reducing incident\ndetection and response times by 58%. In addition, it maintains high conformity\nwith security requirements and efficient resource usage. These findings\nvalidate the effectiveness of adaptive reinforcement learning approaches in\nimproving cloud security policy management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u7b56\u7565\u7ba1\u7406\u6846\u67b6\uff0c\u4ee5\u52a8\u6001\u9002\u5e94\u4e91\u73af\u5883\u4e2d\u7684\u5b89\u5168\u9700\u6c42\u3002", "motivation": "\u9759\u6001\u5b89\u5168\u7b56\u7565\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u5a01\u80c1\u548c\u4e91\u8d44\u6e90\u7684\u5f39\u6027\u3002", "method": "\u6211\u4eec\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec\u6df1\u5ea6Q\u7f51\u7edc\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff0c\u4f7f\u63a7\u5236\uff08\u5982\u9632\u706b\u5899\u89c4\u5219\u548c\u8eab\u4efd\u548c\u8bbf\u95ee\u7ba1\u7406\uff08IAM\uff09\u7b56\u7565\uff09\u80fd\u591f\u5b66\u4e60\u548c\u6301\u7eed\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u81ea\u9002\u5e94RL\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u9759\u6001\u7b56\u7565\uff0c\u5728\u5165\u4fb5\u68c0\u6d4b\u7387\uff0892%\u5bf9\u6bd482%\uff09\u548c\u51cf\u5c11\u4e8b\u4ef6\u68c0\u6d4b\u548c\u54cd\u5e94\u65f6\u95f4\uff0858%\uff09\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u9a8c\u8bc1\u4e86\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6539\u8fdb\u4e91\u5b89\u5168\u7b56\u7565\u7ba1\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09571", "pdf": "https://arxiv.org/pdf/2505.09571", "abs": "https://arxiv.org/abs/2505.09571", "authors": ["Guillermo Gomez-Trenado", "Pablo Mesejo", "Oscar Cord\u00f3n", "St\u00e9phane Lathuili\u00e8re"], "title": "Don't Forget your Inverse DDIM for Image Editing", "categories": ["cs.CV", "I.2.10; I.5.0"], "comment": "12 pages, 12 figures, code available at\n  https://guillermogotre.github.io/sage/", "summary": "The field of text-to-image generation has undergone significant advancements\nwith the introduction of diffusion models. Nevertheless, the challenge of\nediting real images persists, as most methods are either computationally\nintensive or produce poor reconstructions. This paper introduces SAGE\n(Self-Attention Guidance for image Editing) - a novel technique leveraging\npre-trained diffusion models for image editing. SAGE builds upon the DDIM\nalgorithm and incorporates a novel guidance mechanism utilizing the\nself-attention layers of the diffusion U-Net. This mechanism computes a\nreconstruction objective based on attention maps generated during the inverse\nDDIM process, enabling efficient reconstruction of unedited regions without the\nneed to precisely reconstruct the entire input image. Thus, SAGE directly\naddresses the key challenges in image editing. The superiority of SAGE over\nother methods is demonstrated through quantitative and qualitative evaluations\nand confirmed by a statistically validated comprehensive user study, in which\nall 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE\nranks as the top-performing method in seven out of 10 quantitative analyses and\nsecures second and third places in the remaining three.", "AI": {"tldr": "SAGE is a new technique that improves image editing by using self-attention mechanisms in diffusion models, achieving better results than existing methods.", "motivation": "The challenge of editing real images remains difficult due to computational intensity and poor reconstructions, prompting the need for a more efficient and effective method.", "method": "SAGE (Self-Attention Guidance for Image Editing) leverages pre-trained diffusion models and incorporates a novel guidance mechanism using self-attention layers of the diffusion U-Net.", "result": "SAGE outperforms other methods in seven out of 10 quantitative analyses and ranks second or third in the remaining three. It also receives high user preference in a comprehensive study.", "conclusion": "SAGE demonstrates superior performance in image editing compared to other methods, as shown through quantitative and qualitative evaluations and a user study."}}
{"id": "2505.09115", "pdf": "https://arxiv.org/pdf/2505.09115", "abs": "https://arxiv.org/abs/2505.09115", "authors": ["Yu Lun Hsu", "Yun-Rung Chou", "Chiao-Ju Chang", "Yu-Cheng Chang", "Zer-Wei Lee", "Rokas Gipi\u0161kis", "Rachel Li", "Chih-Yuan Shih", "Jen-Kuei Peng", "Hsien-Liang Huang", "Jaw-Shiun Tsai", "Mike Y. Chen"], "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Advance Care Planning (ACP) allows individuals to specify their preferred\nend-of-life life-sustaining treatments before they become incapacitated by\ninjury or terminal illness (e.g., coma, cancer, dementia). While online ACP\noffers high accessibility, it lacks key benefits of clinical consultations,\nincluding personalized value exploration, immediate clarification of decision\nconsequences. To bridge this gap, we conducted two formative studies: 1)\nshadowed and interviewed 3 ACP teams consisting of physicians, nurses, and\nsocial workers (18 patients total), and 2) interviewed 14 users of ACP\nwebsites. Building on these insights, we designed PreCare in collaboration with\n6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed\nto guide users through exploring personal values, gaining ACP knowledge, and\nsupporting informed decision-making. A usability study (n=12) showed that\nPreCare achieved a System Usability Scale (SUS) rating of excellent. A\ncomparative evaluation (n=12) showed that PreCare's AI assistants significantly\nimproved exploration of personal values, knowledge, and decisional confidence,\nand was preferred by 92% of participants.", "AI": {"tldr": "This paper presents PreCare, an online platform with AI assistants that helps individuals plan for end-of-life care by exploring personal values, gaining knowledge, and making informed decisions. It shows excellent usability and was preferred by most participants.", "motivation": "Online ACP lacks key benefits of clinical consultations, including personalized value exploration and immediate clarification of decision consequences.", "method": "We conducted two formative studies: 1) shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and social workers (18 patients total), and 2) interviewed 14 users of ACP websites. Building on these insights, we designed PreCare in collaboration with 6 ACP professionals.", "result": "A usability study (n=12) showed that PreCare achieved a System Usability Scale (SUS) rating of excellent. A comparative evaluation (n=12) showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.", "conclusion": "PreCare is a website with 3 AI-driven assistants designed to guide users through exploring personal values, gaining ACP knowledge, and supporting informed decision-making. It achieved excellent usability and was preferred by most participants."}}
{"id": "2505.09591", "pdf": "https://arxiv.org/pdf/2505.09591", "abs": "https://arxiv.org/abs/2505.09591", "authors": ["Tobias Jan Wieczorek", "Nathalie Daun", "Mohammad Emtiyaz Khan", "Marcus Rohrbach"], "title": "Variational Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 16 figures, under review at ICCV 2025", "summary": "Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5206VQA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528IVON\u7b97\u6cd5\u6765\u63d0\u9ad8\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6821\u51c6\u548c\u62d2\u7edd\u63a5\u53d7\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5982\u8fc7\u5ea6\u81ea\u4fe1\u548c\u6821\u51c6\u4e0d\u826f\u3002", "method": "\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u79f0\u4e3aIVON\u7684\u53d8\u5206\u7b97\u6cd5\uff0c\u800c\u4e0d\u662f\u4f7f\u7528AdamW\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u727a\u7272AdamW\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u6821\u51c6\u548c\u62d2\u7edd\u63a5\u53d7\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u4e0eAdamW\u5fae\u8c03\u76f8\u6bd4\uff0c\u9884\u671f\u6821\u51c6\u8bef\u5dee\u51cf\u5c11\u4e8650%\u4ee5\u4e0a\uff0c\u5e76\u4e14\u5728\u56fa\u5b9a\u98ce\u9669\u4e3a1%\u7684\u60c5\u51b5\u4e0b\uff0c\u8986\u76d6\u7387\u63d0\u9ad8\u4e864%\u3002\u5728\u5206\u5e03\u504f\u79fb\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u63d0\u5347\u66f4\u9ad8\uff0c\u5f5350%\u7684\u6d4b\u8bd5\u6848\u4f8b\u662fOOD\u65f6\uff0c\u8986\u76d6\u7387\u63d0\u9ad8\u4e868%\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u53d8\u5206VQA\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.09608", "pdf": "https://arxiv.org/pdf/2505.09608", "abs": "https://arxiv.org/abs/2505.09608", "authors": ["Nadav Magar", "Amir Hertz", "Eric Tabellion", "Yael Pritch", "Alex Rav-Acha", "Ariel Shamir", "Yedid Hoshen"], "title": "LightLab: Controlling Light Sources in Images with Diffusion Models", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://nadmag.github.io/LightLab/", "summary": "We present a simple, yet effective diffusion-based method for fine-grained,\nparametric control over light sources in an image. Existing relighting methods\neither rely on multiple input views to perform inverse rendering at inference\ntime, or fail to provide explicit control over light changes. Our method\nfine-tunes a diffusion model on a small set of real raw photograph pairs,\nsupplemented by synthetically rendered images at scale, to elicit its\nphotorealistic prior for relighting. We leverage the linearity of light to\nsynthesize image pairs depicting controlled light changes of either a target\nlight source or ambient illumination. Using this data and an appropriate\nfine-tuning scheme, we train a model for precise illumination changes with\nexplicit control over light intensity and color. Lastly, we show how our method\ncan achieve compelling light editing results, and outperforms existing methods\nbased on user preference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u56fe\u50cf\u4e2d\u7684\u5149\u6e90\u8fdb\u884c\u7ec6\u7c92\u5ea6\u3001\u53c2\u6570\u5316\u7684\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u91cd\u65b0\u7167\u660e\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u591a\u4e2a\u8f93\u5165\u89c6\u56fe\u6765\u8fdb\u884c\u9006\u5411\u6e32\u67d3\uff0c\u8981\u4e48\u65e0\u6cd5\u63d0\u4f9b\u5bf9\u5149\u7167\u53d8\u5316\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u6211\u4eec\u5bf9\u4e00\u4e2a\u5c0f\u89c4\u6a21\u7684\u771f\u5b9e\u539f\u59cb\u7167\u7247\u5bf9\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u5927\u89c4\u6a21\u5408\u6210\u56fe\u50cf\uff0c\u4ee5\u6fc0\u53d1\u5176\u903c\u771f\u7684\u5149\u7167\u5148\u9a8c\u3002\u6211\u4eec\u5229\u7528\u5149\u7684\u7ebf\u6027\u7279\u6027\u6765\u5408\u6210\u663e\u793a\u53d7\u63a7\u5149\u7167\u53d8\u5316\u7684\u56fe\u50cf\u5bf9\u3002", "result": "\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u53ef\u4ee5\u7cbe\u786e\u5730\u6539\u53d8\u5149\u7167\uff0c\u5e76\u4e14\u53ef\u4ee5\u663e\u5f0f\u63a7\u5236\u5149\u7167\u5f3a\u5ea6\u548c\u989c\u8272\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5149\u7167\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.09615", "pdf": "https://arxiv.org/pdf/2505.09615", "abs": "https://arxiv.org/abs/2505.09615", "authors": ["Yung-Hsuan Lai", "Janek Ebbers", "Yu-Chiang Frank Wang", "Fran\u00e7ois Germain", "Michael Jeffrey Jones", "Moitreya Chatterjee"], "title": "UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing\nboth uni-modal events (i.e., those occurring exclusively in either the visual\nor acoustic modality of a video) and multi-modal events (i.e., those occurring\nin both modalities concurrently). Moreover, the prohibitive cost of annotating\ntraining data with the class labels of all these events, along with their start\nand end times, imposes constraints on the scalability of AVVP techniques unless\nthey can be trained in a weakly-supervised setting, where only\nmodality-agnostic, video-level labels are available in the training data. To\nthis end, recently proposed approaches seek to generate segment-level\npseudo-labels to better guide model training. However, the absence of\ninter-segment dependencies when generating these pseudo-labels and the general\nbias towards predicting labels that are absent in a segment limit their\nperformance. This work proposes a novel approach towards overcoming these\nweaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video\nParsing (UWAV). Additionally, our innovative approach factors in the\nuncertainty associated with these estimated pseudo-labels and incorporates a\nfeature mixup based training regularization for improved training. Empirical\nresults show that UWAV outperforms state-of-the-art methods for the AVVP task\non multiple metrics, across two different datasets, attesting to its\neffectiveness and generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u65b9\u6cd5UWAV\uff0c\u901a\u8fc7\u8003\u8651\u4f2a\u6807\u7b7e\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u5f15\u5165\u57fa\u4e8e\u7279\u5f81\u6df7\u5408\u7684\u8bad\u7ec3\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\uff0cAVVP\u6280\u672f\u9700\u8981\u5728\u5f31\u76d1\u7763\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u751f\u6210\u4f2a\u6807\u7b7e\u65f6\u7f3a\u4e4f\u6bb5\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u4e14\u5b58\u5728\u5bf9\u6bb5\u4e2d\u4e0d\u5b58\u5728\u6807\u7b7e\u7684\u504f\u5dee\u3002", "method": "UWAV\u901a\u8fc7\u8003\u8651\u4f2a\u6807\u7b7e\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u5f15\u5165\u57fa\u4e8e\u7279\u5f81\u6df7\u5408\u7684\u8bad\u7ec3\u6b63\u5219\u5316\u6765\u6539\u8fdb\u8bad\u7ec3\u3002", "result": "UWAV\u5728\u591a\u4e2a\u6307\u6807\u548c\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "UWAV\u5728\u591a\u4e2a\u6307\u6807\u548c\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.09142", "pdf": "https://arxiv.org/pdf/2505.09142", "abs": "https://arxiv.org/abs/2505.09142", "authors": ["Seungbeom Choi", "Jeonghoe Goo", "Eunjoo Jeon", "Mingyu Yang", "Minsung Jang"], "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "13 pages, 5 figures. Cloud-native LLM scheduling system with\n  latency-aware inference optimization", "summary": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%.", "AI": {"tldr": "ELIS\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u670d\u52a1\u5668\u7cfb\u7edf\uff0c\u91c7\u7528ISRTF\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u54cd\u5e94\u957f\u5ea6\u548c\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u6548\u7387\u3002", "motivation": "\u5f53\u524dLLM\u670d\u52a1\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5148\u5230\u5148\u670d\u52a1\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u5bfc\u81f4\u201c\u961f\u5934\u963b\u585e\u201d\u95ee\u9898\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u9884\u6d4bLLM\u63a8\u7406\u65f6\u95f4\u5e76\u5e94\u7528\u6700\u77ed\u4f5c\u4e1a\u4f18\u5148\u7b56\u7565\uff0c\u4f46\u7531\u4e8eLLM\u7684\u81ea\u56de\u5f52\u7279\u6027\uff0c\u9884\u6d4b\u63a8\u7406\u5ef6\u8fdf\u5177\u6709\u6311\u6218\u6027\u3002", "method": "ELIS\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8eBGE\u6a21\u578b\u7684\u54cd\u5e94\u957f\u5ea6\u9884\u6d4b\u5668\uff0c\u5e76\u8bbe\u8ba1\u4e86ISRTF\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u73b0\u6709LLM\u8fed\u4ee3\u6279\u5904\u7406\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u57fa\u4e8eKubernetes\u7684\u4e91\u539f\u751f\u8c03\u5ea6\u7cfb\u7edf\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cISRTF\u5c06\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe19.6%\u3002", "conclusion": "ELIS\u901a\u8fc7ISRTF\u8c03\u5ea6\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.08899", "pdf": "https://arxiv.org/pdf/2505.08899", "abs": "https://arxiv.org/abs/2505.08899", "authors": ["Andrew Mullhaupt", "Cheng Peng"], "title": "Bounding Neyman-Pearson Region with $f$-Divergences", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": null, "summary": "The Neyman-Pearson region of a simple binary hypothesis testing is the set of\npoints whose coordinates represent the false positive rate and false negative\nrate of some test. The lower boundary of this region is given by the\nNeyman-Pearson lemma, and is up to a coordinate change, equivalent to the\noptimal ROC curve. We establish a novel lower bound for the boundary in terms\nof any $f$-divergence. Since the bound generated by hockey-stick\n$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best\npossible. In the case of KL divergence, this bound improves Pinsker's\ninequality. Furthermore, we obtain a closed-form refined upper bound for the\nNeyman-Pearson boundary in terms of the Chernoff $\\alpha$-coefficient. Finally,\nwe present methods for constructing pairs of distributions that can\napproximately or exactly realize any given Neyman-Pearson boundary.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e8c\u5143\u5047\u8bbe\u68c0\u9a8c\u7684Neyman-Pearson\u533a\u57df\u8fb9\u754c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0b\u754c\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u4e0d\u7b49\u5f0f\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6784\u9020\u5206\u5e03\u5bf9\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76Neyman-Pearson\u533a\u57df\u7684\u8fb9\u754c\u5bf9\u4e8e\u7406\u89e3\u4e8c\u5143\u5047\u8bbe\u68c0\u9a8c\u7684\u6027\u80fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u65b0\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u7684\u4e0d\u7b49\u5f0f\u5e76\u6269\u5c55\u5176\u5e94\u7528\u3002", "method": "\u672c\u6587\u5229\u7528f-\u6563\u5ea6\u548cChernoff \u03b1\u7cfb\u6570\u6765\u5206\u6790Neyman-Pearson\u533a\u57df\u7684\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u6784\u9020\u5206\u5e03\u5bf9\u6765\u5b9e\u73b0\u7279\u5b9a\u7684\u8fb9\u754c\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4e0b\u754c\uff0c\u8be5\u4e0b\u754c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u662f\u6700\u4f73\u7684\uff0c\u5e76\u4e14\u5728KL\u6563\u5ea6\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u4e86Pinsker\u4e0d\u7b49\u5f0f\u3002\u540c\u65f6\uff0c\u5f97\u5230\u4e86\u4e00\u4e2a\u95ed\u5f0f\u4e0a\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u5206\u5e03\u5bf9\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4e0b\u754c\uff0c\u7528\u4e8e\u63cf\u8ff0\u4e8c\u5143\u5047\u8bbe\u68c0\u9a8c\u7684Neyman-Pearson\u533a\u57df\u8fb9\u754c\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u4e0b\u754c\u5728KL\u6563\u5ea6\u60c5\u51b5\u4e0b\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u8fd8\u5f97\u5230\u4e86\u4e00\u4e2a\u95ed\u5f0f\u4e0a\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u5206\u5e03\u5bf9\u7684\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u7ed9\u5b9a\u7684Neyman-Pearson\u8fb9\u754c\u3002"}}
{"id": "2505.08908", "pdf": "https://arxiv.org/pdf/2505.08908", "abs": "https://arxiv.org/abs/2505.08908", "authors": ["Benedikt Koch", "Kosuke Imai"], "title": "Statistical Decision Theory with Counterfactual Loss", "categories": ["math.ST", "cs.LG", "econ.TH", "stat.TH"], "comment": null, "summary": "Classical statistical decision theory evaluates treatment choices based\nsolely on observed outcomes. However, by ignoring counterfactual outcomes, it\ncannot assess the quality of decisions relative to feasible alternatives. For\nexample, the quality of a physician's decision may depend not only on patient\nsurvival, but also on whether a less invasive treatment could have produced a\nsimilar result. To address this limitation, we extend standard decision theory\nto incorporate counterfactual losses--criteria that evaluate decisions using\nall potential outcomes. The central challenge in this generalization is\nidentification: because only one potential outcome is observed for each unit,\nthe associated risk under a counterfactual loss is generally not identifiable.\nWe show that under the assumption of strong ignorability, a counterfactual risk\nis identifiable if and only if the counterfactual loss function is additive in\nthe potential outcomes. Moreover, we demonstrate that additive counterfactual\nlosses can yield treatment recommendations that differ from those based on\nstandard loss functions, provided that the decision problem involves more than\ntwo treatment options.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u6807\u51c6\u51b3\u7b56\u7406\u8bba\uff0c\u5f15\u5165\u53cd\u4e8b\u5b9e\u635f\u5931\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u6cbb\u7597\u9009\u62e9\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53cd\u4e8b\u5b9e\u635f\u5931\u51fd\u6570\u5982\u4f55\u5f71\u54cd\u6cbb\u7597\u5efa\u8bae\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u4ec5\u57fa\u4e8e\u89c2\u5bdf\u7ed3\u679c\u8bc4\u4f30\u6cbb\u7597\u9009\u62e9\uff0c\u4f46\u5ffd\u7565\u4e86\u53cd\u4e8b\u5b9e\u7ed3\u679c\uff0c\u56e0\u6b64\u65e0\u6cd5\u76f8\u5bf9\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u8bc4\u4f30\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5047\u8bbe\u5f3a\u53ef\u5ffd\u7565\u6027\uff0c\u7814\u7a76\u4e86\u53cd\u4e8b\u5b9e\u98ce\u9669\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u591a\u4e8e\u4e24\u4e2a\u6cbb\u7597\u9009\u9879\u7684\u60c5\u51b5\u4e0b\uff0c\u53cd\u4e8b\u5b9e\u635f\u5931\u51fd\u6570\u5982\u4f55\u5f71\u54cd\u6cbb\u7597\u5efa\u8bae\u3002", "result": "\u5728\u5f3a\u53ef\u5ffd\u7565\u6027\u5047\u8bbe\u4e0b\uff0c\u53cd\u4e8b\u5b9e\u98ce\u9669\u53ea\u6709\u5728\u53cd\u4e8b\u5b9e\u635f\u5931\u51fd\u6570\u662f\u6f5c\u5728\u7ed3\u679c\u7684\u53ef\u52a0\u51fd\u6570\u65f6\u624d\u53ef\u8bc6\u522b\u3002\u6b64\u5916\uff0c\u53cd\u4e8b\u5b9e\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u4ea7\u751f\u4e0e\u6807\u51c6\u635f\u5931\u51fd\u6570\u4e0d\u540c\u7684\u6cbb\u7597\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u6807\u51c6\u51b3\u7b56\u7406\u8bba\uff0c\u4ee5\u7eb3\u5165\u53cd\u4e8b\u5b9e\u635f\u5931\uff0c\u4ece\u800c\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u6cbb\u7597\u9009\u62e9\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.09166", "pdf": "https://arxiv.org/pdf/2505.09166", "abs": "https://arxiv.org/abs/2505.09166", "authors": ["Hannu Simonen", "Atte Kiviniemi", "Jonas Oppenlaender"], "title": "An Initial Exploration of Default Images in Text-to-Image Generation", "categories": ["cs.HC", "cs.AI", "H.5.m; I.2.m"], "comment": "16 pages, 6 figures", "summary": "In the creative practice of text-to-image generation (TTI), images are\ngenerated from text prompts. However, TTI models are trained to always yield an\noutput, even if the prompt contains unknown terms. In this case, the model may\ngenerate what we call \"default images\": images that closely resemble each other\nacross many unrelated prompts. We argue studying default images is valuable for\ndesigning better solutions for TTI and prompt engineering. In this paper, we\nprovide the first investigation into default images on Midjourney, a popular\nimage generator. We describe our systematic approach to create input prompts\ntriggering default images, and present the results of our initial experiments\nand several small-scale ablation studies. We also report on a survey study\ninvestigating how default images affect user satisfaction. Our work lays the\nfoundation for understanding default images in TTI and highlights challenges\nand future research directions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u9ed8\u8ba4\u56fe\u50cf\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u521b\u5efa\u89e6\u53d1\u8fd9\u4e9b\u56fe\u50cf\u7684\u8f93\u5165\u63d0\u793a\u3002\u901a\u8fc7\u5b9e\u9a8c\u548c\u8c03\u67e5\uff0c\u6211\u4eec\u5206\u6790\u4e86\u9ed8\u8ba4\u56fe\u50cf\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u6211\u4eec\u8ba4\u4e3a\u7814\u7a76\u9ed8\u8ba4\u56fe\u50cf\u5bf9\u4e8e\u8bbe\u8ba1\u66f4\u597d\u7684TTI\u89e3\u51b3\u65b9\u6848\u548c\u63d0\u793a\u5de5\u7a0b\u662f\u6709\u4ef7\u503c\u7684\u3002", "method": "\u6211\u4eec\u63cf\u8ff0\u4e86\u7cfb\u7edf\u6027\u65b9\u6cd5\u6765\u521b\u5efa\u89e6\u53d1\u9ed8\u8ba4\u56fe\u50cf\u7684\u8f93\u5165\u63d0\u793a\uff0c\u5e76\u5c55\u793a\u4e86\u521d\u6b65\u5b9e\u9a8c\u548c\u51e0\u4e2a\u5c0f\u89c4\u6a21\u6d88\u878d\u7814\u7a76\u7684\u7ed3\u679c\u3002\u6211\u4eec\u8fd8\u62a5\u544a\u4e86\u4e00\u9879\u8c03\u67e5\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u9ed8\u8ba4\u56fe\u50cf\u5982\u4f55\u5f71\u54cd\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "result": "\u6211\u4eec\u63d0\u4f9b\u4e86\u5bf9Midjourney\u4e2d\u9ed8\u8ba4\u56fe\u50cf\u7684\u9996\u6b21\u8c03\u67e5\uff0c\u5e76\u5c55\u793a\u4e86\u6211\u4eec\u7684\u521d\u59cb\u5b9e\u9a8c\u7ed3\u679c\u548c\u5c0f\u89c4\u6a21\u6d88\u878d\u7814\u7a76\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u7406\u89e3TTI\u4e2d\u7684\u9ed8\u8ba4\u56fe\u50cf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u7a81\u51fa\u4e86\u6311\u6218\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.09203", "pdf": "https://arxiv.org/pdf/2505.09203", "abs": "https://arxiv.org/abs/2505.09203", "authors": ["Xiao-Qi Han", "Peng-Jie Guo", "Ze-Feng Gao", "Hao Sun", "Zhong-Yi Lu"], "title": "InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials", "categories": ["cond-mat.mtrl-sci", "cond-mat.supr-con", "cs.AI", "cs.LG"], "comment": "29 pages, 11 figures", "summary": "Developing inverse design methods for functional materials with specific\nproperties is critical to advancing fields like renewable energy, catalysis,\nenergy storage, and carbon capture. Generative models based on diffusion\nprinciples can directly produce new materials that meet performance\nconstraints, thereby significantly accelerating the material design process.\nHowever, existing methods for generating and predicting crystal structures\noften remain limited by low success rates. In this work, we propose a novel\ninverse material design generative framework called InvDesFlow-AL, which is\nbased on active learning strategies. This framework can iteratively optimize\nthe material generation process to gradually guide it towards desired\nperformance characteristics. In terms of crystal structure prediction, the\nInvDesFlow-AL model achieves an RMSE of 0.0423 {\\AA}, representing an 32.96%\nimprovement in performance compared to exsisting generative models.\nAdditionally, InvDesFlow-AL has been successfully validated in the design of\nlow-formation-energy and low-Ehull materials. It can systematically generate\nmaterials with progressively lower formation energies while continuously\nexpanding the exploration across diverse chemical spaces. These results fully\ndemonstrate the effectiveness of the proposed active learning-driven generative\nmodel in accelerating material discovery and inverse design. To further prove\nthe effectiveness of this method, we took the search for BCS superconductors\nunder ambient pressure as an example explored by InvDesFlow-AL. As a result, we\nsuccessfully identified Li\\(_2\\)AuH\\(_6\\) as a conventional BCS superconductor\nwith an ultra-high transition temperature of 140 K. This discovery provides\nstrong empirical support for the application of inverse design in materials\nscience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u9006\u5411\u6750\u6599\u8bbe\u8ba1\u751f\u6210\u6846\u67b6InvDesFlow-AL\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6750\u6599\u8bbe\u8ba1\u6548\u7387\uff0c\u5e76\u6210\u529f\u53d1\u73b0\u4e86\u5177\u6709\u8d85\u9ad8\u8f6c\u53d8\u6e29\u5ea6\u7684BCS\u8d85\u5bfc\u4f53\u3002", "motivation": "\u5f00\u53d1\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u5bf9\u4e8e\u529f\u80fd\u6750\u6599\u7684\u7279\u5b9a\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u548c\u9884\u6d4b\u6676\u4f53\u7ed3\u6784\u65f6\u5b58\u5728\u6210\u529f\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "InvDesFlow-AL\u6846\u67b6\u7ed3\u5408\u4e86\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u6750\u6599\u751f\u6210\u8fc7\u7a0b\uff0c\u9010\u6b65\u5f15\u5bfc\u5176\u8fbe\u5230\u671f\u671b\u7684\u6027\u80fd\u7279\u5f81\u3002", "result": "InvDesFlow-AL\u5728\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e860.0423 \u00c5\u7684RMSE\uff0c\u6bd4\u73b0\u6709\u751f\u6210\u6a21\u578b\u63d0\u9ad8\u4e8632.96%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u9a8c\u8bc1\u4e86\u4f4e\u5f62\u6210\u80fd\u548c\u4f4eEhull\u6750\u6599\u7684\u8bbe\u8ba1\uff0c\u5e76\u53d1\u73b0\u4e86Li$_2$AuH$_6$\u4f5c\u4e3a\u5177\u6709140 K\u8d85\u9ad8\u8f6c\u53d8\u6e29\u5ea6\u7684BCS\u8d85\u5bfc\u4f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u7684\u65b0\u578b\u9006\u5411\u6750\u6599\u8bbe\u8ba1\u751f\u6210\u6846\u67b6InvDesFlow-AL\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u6750\u6599\u53d1\u73b0\u548c\u9006\u5411\u8bbe\u8ba1\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u5728\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u4f4e\u5f62\u6210\u80fd\u548c\u4f4eEhull\u6750\u6599\u7684\u8bbe\u8ba1\uff0c\u8fd8\u53d1\u73b0\u4e86\u5177\u6709\u8d85\u9ad8\u8f6c\u53d8\u6e29\u5ea6\u7684BCS\u8d85\u5bfc\u4f53Li$_2$AuH$_6$\u3002"}}
{"id": "2505.09208", "pdf": "https://arxiv.org/pdf/2505.09208", "abs": "https://arxiv.org/abs/2505.09208", "authors": ["Lei Fan", "Kunyang Deng", "Fangxue Liu"], "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the rapid advancement of generative artificial intelligence(AI), its\npotential applications in higher education have attracted significant\nattention. This study investigated how 148 students from diverse engineering\ndisciplines and regions across China used generative AI, focusing on its impact\non their learning experience and the opportunities and challenges it poses in\nengineering education. Based on the surveyed data, we explored four key areas:\nthe frequency and application scenarios of AI use among engineering students,\nits impact on students' learning and performance, commonly encountered\nchallenges in using generative AI, and future prospects for its adoption in\nengineering education. The results showed that more than half of the\nparticipants reported a positive impact of generative AI on their learning\nefficiency, initiative, and creativity, with nearly half believing it also\nenhanced their independent thinking. However, despite acknowledging improved\nstudy efficiency, many felt their actual academic performance remained largely\nunchanged and expressed concerns about the accuracy and domain-specific\nreliability of generative AI. Our findings provide a first-hand insight into\nthe current benefits and challenges generative AI brings to students,\nparticularly Chinese engineering students, while offering several\nrecommendations, especially from the students' perspective, for effectively\nintegrating generative AI into engineering education.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u53ca\u5176\u5bf9\u5b66\u4e60\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5728\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u521b\u9020\u529b\u65b9\u9762\u6709\u79ef\u6781\u4f5c\u7528\uff0c\u4f46\u4e5f\u5b58\u5728\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u5b66\u672f\u8868\u73b0\u672a\u63d0\u5347\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u751f\u6210\u5f0fAI\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u6765\u81ea\u4e2d\u56fd\u4e0d\u540c\u5de5\u7a0b\u5b66\u79d1\u548c\u5730\u533a\u7684148\u540d\u5b66\u751f\u5982\u4f55\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5bf9\u5b66\u4e60\u4f53\u9a8c\u7684\u5f71\u54cd\u4ee5\u53ca\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "result": "\u8d85\u8fc7\u4e00\u534a\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u79f0\u751f\u6210\u5f0fAI\u5bf9\u4ed6\u4eec\u7684\u5b66\u4e60\u6548\u7387\u3001\u4e3b\u52a8\u6027\u548c\u521b\u9020\u529b\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u8fd1\u4e00\u534a\u7684\u4eba\u8ba4\u4e3a\u5b83\u589e\u5f3a\u4e86\u4ed6\u4eec\u7684\u72ec\u7acb\u601d\u8003\u80fd\u529b\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u627f\u8ba4\u5b66\u4e60\u6548\u7387\u63d0\u9ad8\uff0c\u8bb8\u591a\u4eba\u8ba4\u4e3a\u4ed6\u4eec\u7684\u5b9e\u9645\u5b66\u672f\u6210\u7ee9\u57fa\u672c\u672a\u53d8\uff0c\u5e76\u5bf9\u751f\u6210\u5f0fAI\u7684\u51c6\u786e\u6027\u548c\u9886\u57df\u7279\u5b9a\u53ef\u9760\u6027\u8868\u793a\u62c5\u5fe7\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u751f\u6210\u5f0fAI\u5bf9\u5de5\u7a0b\u6559\u80b2\u5e26\u6765\u4e86\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u4e5f\u5b58\u5728\u51c6\u786e\u6027\u3001\u9886\u57df\u7279\u5b9a\u53ef\u9760\u6027\u548c\u5b66\u672f\u8868\u73b0\u672a\u660e\u663e\u63d0\u5347\u7b49\u95ee\u9898\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5c06\u751f\u6210\u5f0fAI\u6709\u6548\u6574\u5408\u5230\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u5efa\u8bae\u3002"}}
{"id": "2505.08986", "pdf": "https://arxiv.org/pdf/2505.08986", "abs": "https://arxiv.org/abs/2505.08986", "authors": ["Amirreza Davar", "Zhengtong Xu", "Siavash Mahmoudi", "Pouya Sohrabipour", "Chaitanya Pallerla", "Yu She", "Wan Shou", "Philip Crandall", "Dongyi Wang"], "title": "ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": "Submitted for journal review", "summary": "Automated poultry processing lines still rely on humans to lift slippery,\neasily bruised carcasses onto a shackle conveyor. Deformability, anatomical\nvariance, and strict hygiene rules make conventional suction and scripted\nmotions unreliable. We present ChicGrasp, an end--to--end hardware--software\nco-design for this task. An independently actuated dual-jaw pneumatic gripper\nclamps both chicken legs, while a conditional diffusion-policy controller,\ntrained from only 50 multi--view teleoperation demonstrations (RGB +\nproprioception), plans 5 DoF end--effector motion, which includes jaw commands\nin one shot. On individually presented raw broiler carcasses, our system\nachieves a 40.6\\% grasp--and--lift success rate and completes the pick to\nshackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning\n(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be\nopen-source. ChicGrasp shows that imitation learning can bridge the gap between\nrigid hardware and variable bio--products, offering a reproducible benchmark\nand a public dataset for researchers in agricultural engineering and robot\nlearning.", "AI": {"tldr": "ChicGrasp\u662f\u4e00\u7a2e\u7528\u65bc\u81ea\u52d5\u5316\u5bb6\u79bd\u8655\u7406\u7684\u7aef\u5230\u7aef\u786c\u9ad4-\u8edf\u9ad4\u5354\u540c\u8a2d\u8a08\uff0c\u5229\u7528\u6a21\u4eff\u5b78\u7fd2\u4f86\u89e3\u6c7a\u50b3\u7d71\u65b9\u6cd5\u5728\u8655\u7406\u6613\u8b8a\u751f\u7269\u7522\u54c1\u6642\u7684\u53ef\u9760\u6027\u554f\u984c\u3002", "motivation": "\u81ea\u52d5\u5bb6\u79bd\u52a0\u5de5\u7dda\u4ecd\u7136\u4f9d\u8cf4\u4eba\u985e\u5c07\u6ed1\u6e9c\u3001\u6613\u53d7\u50b7\u7684\u5bb6\u79bd carcasses \u653e\u5230\u639b\u9264\u50b3\u9001\u5e36\u4e0a\u3002\u8b8a\u5f62\u6027\u3001\u89e3\u5256\u5b78\u5dee\u7570\u548c\u56b4\u683c\u7684\u885b\u751f\u898f\u5247\u4f7f\u50b3\u7d71\u5438\u529b\u548c\u8173\u672c\u52d5\u4f5c\u4e0d\u53ef\u9760\u3002", "method": "ChicGrasp\u662f\u4e00\u7a2e\u7aef\u5230\u7aef\u7684\u786c\u9ad4-\u8edf\u9ad4\u5354\u540c\u8a2d\u8a08\uff0c\u4f7f\u7528\u7368\u7acb\u9a45\u52d5\u7684\u96d9\u722a\u6c23\u52d5\u593e\u5177\u593e\u4f4f\u96de\u817f\uff0c\u4e26\u4f7f\u7528\u689d\u4ef6\u64f4\u6563\u7b56\u7565\u63a7\u5236\u5668\u9032\u884c5\u81ea\u7531\u5ea6\u672b\u7aef\u57f7\u884c\u5668\u904b\u52d5\u898f\u5283\u3002", "result": "\u5728\u500b\u5225\u5c55\u793a\u7684\u539f\u59cb\u8089\u96de\u5c4d\u9ad4\u4e0a\uff0c\u7cfb\u7d71\u9054\u5230\u4e8640.6%\u7684\u6293\u53d6\u548c\u63d0\u5347\u6210\u529f\u7387\uff0c\u4e26\u572838\u79d2\u5167\u5b8c\u6210\u5f9e\u62fe\u53d6\u5230\u639b\u9264\u7684\u9031\u671f\uff0c\u800c\u73fe\u6709\u7684\u96b1\u5f0f\u884c\u70ba\u514b\u9686(IBC)\u548cLSTM-GMM\u57fa\u7dda\u5b8c\u5168\u5931\u6557\u3002", "conclusion": "ChicGrasp\u5c55\u793a\u4e86\u6a21\u4eff\u5b78\u7fd2\u53ef\u4ee5\u5728\u525b\u6027\u786c\u9ad4\u548c\u8b8a\u7570\u751f\u7269\u7522\u54c1\u4e4b\u9593\u5efa\u7acb\u6a4b\u6a11\uff0c\u70ba\u8fb2\u696d\u5de5\u7a0b\u548c\u6a5f\u5668\u4eba\u5b78\u7fd2\u7684\u7814\u7a76\u4eba\u54e1\u63d0\u4f9b\u4e86\u53ef\u91cd\u73fe\u7684\u57fa\u6e96\u548c\u516c\u958b\u8cc7\u6599\u96c6\u3002"}}
{"id": "2505.08843", "pdf": "https://arxiv.org/pdf/2505.08843", "abs": "https://arxiv.org/abs/2505.08843", "authors": ["Marco Corrias", "Giada Franceschi", "Michele Riva", "Alberto Tampieri", "Karin F\u00f6ttinger", "Ulrike Diebold", "Thomas Pock", "Cesare Franchini"], "title": "Total Variation-Based Image Decomposition and Denoising for Microscopy Images", "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.CV"], "comment": null, "summary": "Experimentally acquired microscopy images are unavoidably affected by the\npresence of noise and other unwanted signals, which degrade their quality and\nmight hide relevant features. With the recent increase in image acquisition\nrate, modern denoising and restoration solutions become necessary. This study\nfocuses on image decomposition and denoising of microscopy images through a\nworkflow based on total variation (TV), addressing images obtained from various\nmicroscopy techniques, including atomic force microscopy (AFM), scanning\ntunneling microscopy (STM), and scanning electron microscopy (SEM). Our\napproach consists in restoring an image by extracting its unwanted signal\ncomponents and subtracting them from the raw one, or by denoising it. We\nevaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving\nthis goal in distinct study cases. Huber-ROF proved to be the most flexible\none, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a\nwider applicability of this method in microscopy, restricted not only to STM,\nAFM, and SEM images. The Python code used for this study is publicly available\nas part of AiSurf. It is designed to be integrated into experimental workflows\nfor image acquisition or can be used to denoise previously acquired images.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u603b\u53d8\u5206\uff08TV\uff09\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8bc4\u4f30\u4e86TV-L^1\u3001Huber-ROF\u548cTGV-L^1\u5728\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u566a\u4e2d\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793aHuber-ROF\u6700\u4e3a\u7075\u6d3b\uff0cTGV-L^1\u6700\u9002\u7528\u4e8e\u53bb\u566a\uff0c\u5e76\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u663e\u5fae\u955c\u5b66\u4e2d\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u7531\u4e8e\u5b9e\u9a8c\u83b7\u5f97\u7684\u663e\u5fae\u955c\u56fe\u50cf\u4e0d\u53ef\u907f\u514d\u5730\u53d7\u5230\u566a\u58f0\u548c\u5176\u4ed6\u4e0d\u9700\u8981\u7684\u4fe1\u53f7\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5176\u8d28\u91cf\u5e76\u53ef\u80fd\u9690\u85cf\u76f8\u5173\u7279\u5f81\u3002\u968f\u7740\u56fe\u50cf\u91c7\u96c6\u901f\u7387\u7684\u589e\u52a0\uff0c\u73b0\u4ee3\u53bb\u566a\u548c\u4fee\u590d\u89e3\u51b3\u65b9\u6848\u53d8\u5f97\u5fc5\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u603b\u53d8\u5206\uff08TV\uff09\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e13\u6ce8\u4e8e\u663e\u5fae\u955c\u56fe\u50cf\u7684\u5206\u89e3\u548c\u53bb\u566a\uff0c\u5305\u62ec\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08AFM\uff09\u3001\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\uff08STM\uff09\u548c\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\uff08SEM\uff09\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u63d0\u53d6\u5e76\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u51cf\u53bb\u4e0d\u9700\u8981\u7684\u4fe1\u53f7\u6210\u5206\uff0c\u6216\u8005\u5bf9\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\u3002", "result": "Huber-ROF\u88ab\u8bc1\u660e\u662f\u6700\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u800cTGV-L^1\u6700\u9002\u5408\u53bb\u566a\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u663e\u5fae\u955c\u5b66\u4e2d\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u4e0d\u4ec5\u9650\u4e8eSTM\u3001AFM\u548cSEM\u56fe\u50cf\u3002"}}
{"id": "2505.09262", "pdf": "https://arxiv.org/pdf/2505.09262", "abs": "https://arxiv.org/abs/2505.09262", "authors": ["Hongxin Xiang", "Ke Li", "Mingquan Liu", "Zhixiang Cheng", "Bin Yao", "Wenjie Du", "Jun Xia", "Li Zeng", "Xin Jin", "Xiangxiang Zeng"], "title": "EDBench: Large-Scale Electron Density Data for Molecular Modeling", "categories": ["physics.chem-ph", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Existing molecular machine learning force fields (MLFFs) generally focus on\nthe learning of atoms, molecules, and simple quantum chemical properties (such\nas energy and force), but ignore the importance of electron density (ED)\n$\\rho(r)$ in accurately understanding molecular force fields (MFFs). ED\ndescribes the probability of finding electrons at specific locations around\natoms or molecules, which uniquely determines all ground state properties (such\nas energy, molecular structure, etc.) of interactive multi-particle systems\naccording to the Hohenberg-Kohn theorem. However, the calculation of ED relies\non the time-consuming first-principles density functional theory (DFT) which\nleads to the lack of large-scale ED data and limits its application in MLFFs.\nIn this paper, we introduce EDBench, a large-scale, high-quality dataset of ED\ndesigned to advance learning-based research at the electronic scale. Built upon\nthe PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million\nmolecules. To comprehensively evaluate the ability of models to understand and\nutilize electronic information, we design a suite of ED-centric benchmark tasks\nspanning prediction, retrieval, and generation. Our evaluation on several\nstate-of-the-art methods demonstrates that learning from EDBench is not only\nfeasible but also achieves high accuracy. Moreover, we show that learning-based\nmethod can efficiently calculate ED with comparable precision while\nsignificantly reducing the computational cost relative to traditional DFT\ncalculations. All data and benchmarks from EDBench will be freely available,\nlaying a robust foundation for ED-driven drug discovery and materials science.", "AI": {"tldr": "This paper introduces EDBench, a large-scale, high-quality dataset of electron density (ED) designed to advance learning-based research at the electronic scale. The dataset enables efficient and accurate calculation of ED with reduced computational cost compared to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.", "motivation": "Existing molecular machine learning force fields (MLFFs) focus on learning atoms, molecules, and simple quantum chemical properties but ignore the importance of electron density (ED) in accurately understanding molecular force fields (MFFs). The calculation of ED relies on time-consuming first-principles density functional theory (DFT), leading to a lack of large-scale ED data and limiting its application in MLFFs.", "method": "The paper introduces EDBench, a large-scale, high-quality dataset of ED built upon the PCQM4Mv2. It also designs a suite of ED-centric benchmark tasks spanning prediction, retrieval, and generation to evaluate the ability of models to understand and utilize electronic information.", "result": "The evaluation on several state-of-the-art methods demonstrates that learning from EDBench is not only feasible but also achieves high accuracy. Moreover, learning-based methods can efficiently calculate ED with comparable precision while significantly reducing the computational cost relative to traditional DFT calculations.", "conclusion": "EDBench provides a large-scale, high-quality dataset of electron density (ED) that can advance learning-based research at the electronic scale. The dataset enables efficient and accurate calculation of ED with reduced computational cost compared to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science."}}
{"id": "2505.09004", "pdf": "https://arxiv.org/pdf/2505.09004", "abs": "https://arxiv.org/abs/2505.09004", "authors": ["Monica Welfert", "Nathan Stromberg", "Mario Diaz", "Lalitha Sankar"], "title": "Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features", "categories": ["stat.ML", "cs.LG"], "comment": "submitted to IEEE Transactions on Information Theory", "summary": "We propose an adversarial evaluation framework for sensitive feature\ninference based on minimum mean-squared error (MMSE) estimation with a finite\nsample size and linear predictive models. Our approach establishes theoretical\nlower bounds on the true MMSE of inferring sensitive features from noisy\nobservations of other correlated features. These bounds are expressed in terms\nof the empirical MMSE under a restricted hypothesis class and a non-negative\nerror term. The error term captures both the estimation error due to finite\nnumber of samples and the approximation error from using a restricted\nhypothesis class. For linear predictive models, we derive closed-form bounds,\nwhich are order optimal in terms of the noise variance, on the approximation\nerror for several classes of relationships between the sensitive and\nnon-sensitive features, including linear mappings, binary symmetric channels,\nand class-conditional multi-variate Gaussian distributions. We also present a\nnew lower bound that relies on the MSE computed on a hold-out validation\ndataset of the MMSE estimator learned on finite-samples and a restricted\nhypothesis class. Through empirical evaluation, we demonstrate that our\nframework serves as an effective tool for MMSE-based adversarial evaluation of\nsensitive feature inference that balances theoretical guarantees with practical\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMMSE\u7684\u5bf9\u6297\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u654f\u611f\u7279\u5f81\u63a8\u65ad\uff0c\u80fd\u591f\u63d0\u4f9b\u7406\u8bba\u4e0b\u754c\u5e76\u517c\u987e\u5b9e\u9645\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u654f\u611f\u7279\u5f81\u63a8\u65ad\u4e2d\u7f3a\u4e4f\u7406\u8bba\u4fdd\u969c\u4e0e\u5b9e\u9645\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08MMSE\uff09\u4f30\u8ba1\u7684\u5bf9\u6297\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u4e0b\u754c\u5206\u6790\u654f\u611f\u7279\u5f81\u63a8\u65ad\u7684\u51c6\u786e\u6027\uff0c\u5e76\u7ed3\u5408\u6709\u9650\u6837\u672c\u548c\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u63a8\u5bfc\u3002", "result": "\u6211\u4eec\u63a8\u5bfc\u4e86\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u4e0b\u7684\u95ed\u5f0f\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u654f\u611f\u7279\u5f81\u63a8\u65ad\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u4e3a\u57fa\u4e8e\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08MMSE\uff09\u4f30\u8ba1\u7684\u654f\u611f\u7279\u5f81\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u6297\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u591f\u5728\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2505.08889", "pdf": "https://arxiv.org/pdf/2505.08889", "abs": "https://arxiv.org/abs/2505.08889", "authors": ["Linjie Lyu", "Valentin Deschaintre", "Yannick Hold-Geoffroy", "Milo\u0161 Ha\u0161an", "Jae Shin Yoon", "Thomas Leimk\u00fchler", "Christian Theobalt", "Iliyan Georgiev"], "title": "IntrinsicEdit: Precise generative image manipulation in intrinsic space", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025 Journal track", "summary": "Generative diffusion models have advanced image editing with high-quality\nresults and intuitive interfaces such as prompts and semantic drawing. However,\nthese interfaces lack precise control, and the associated methods typically\nspecialize on a single editing task. We introduce a versatile, generative\nworkflow that operates in an intrinsic-image latent space, enabling semantic,\nlocal manipulation with pixel precision for a range of editing operations.\nBuilding atop the RGB-X diffusion framework, we address key challenges of\nidentity preservation and intrinsic-channel entanglement. By incorporating\nexact diffusion inversion and disentangled channel manipulation, we enable\nprecise, efficient editing with automatic resolution of global illumination\neffects -- all without additional data collection or model fine-tuning. We\ndemonstrate state-of-the-art performance across a variety of tasks on complex\nimages, including color and texture adjustments, object insertion and removal,\nglobal relighting, and their combinations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u53ef\u4ee5\u5728\u5185\u5728\u56fe\u50cf\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7cbe\u786e\u7684\u50cf\u7d20\u7ea7\u7f16\u8f91\uff0c\u65e0\u9700\u989d\u5916\u7684\u6570\u636e\u6536\u96c6\u6216\u6a21\u578b\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u63a5\u53e3\u7f3a\u4e4f\u7cbe\u786e\u63a7\u5236\uff0c\u76f8\u5173\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u7f16\u8f91\u4efb\u52a1\u3002\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u7cbe\u786e\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u901a\u7528\u7684\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u8be5\u5de5\u4f5c\u6d41\u5728\u5185\u5728\u56fe\u50cf\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u80fd\u591f\u5bf9\u5404\u79cd\u7f16\u8f91\u64cd\u4f5c\u8fdb\u884c\u8bed\u4e49\u548c\u5c40\u90e8\u7684\u50cf\u7d20\u7ea7\u64cd\u4f5c\u3002\u6211\u4eec\u57fa\u4e8eRGB-X\u6269\u6563\u6846\u67b6\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u7559\u548c\u5185\u5728\u901a\u9053\u7ea0\u7f20\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u7cbe\u786e\u7684\u6269\u6563\u53cd\u6f14\u548c\u89e3\u7f20\u901a\u9053\u64cd\u4f5c\u5b9e\u73b0\u4e86\u7cbe\u786e\u9ad8\u6548\u7684\u7f16\u8f91\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86\u5728\u590d\u6742\u56fe\u50cf\u4e0a\u7684\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u989c\u8272\u548c\u7eb9\u7406\u8c03\u6574\u3001\u5bf9\u8c61\u63d2\u5165\u548c\u5220\u9664\u3001\u5168\u5c40\u91cd\u65b0\u7167\u660e\u53ca\u5176\u7ec4\u5408\u3002", "conclusion": "\u6211\u4eec\u5c55\u793a\u4e86\u5728\u590d\u6742\u56fe\u50cf\u4e0a\u591a\u79cd\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ec\u989c\u8272\u548c\u7eb9\u7406\u8c03\u6574\u3001\u5bf9\u8c61\u63d2\u5165\u548c\u5220\u9664\u3001\u5168\u5c40\u91cd\u65b0\u7167\u660e\u53ca\u5176\u7ec4\u5408\u3002"}}
{"id": "2505.08932", "pdf": "https://arxiv.org/pdf/2505.08932", "abs": "https://arxiv.org/abs/2505.08932", "authors": ["Mohammad Wasil", "Ahmad Drak", "Brennan Penfold", "Ludovico Scarton", "Maximilian Johenneken", "Alexander Asteroth", "Sebastian Houben"], "title": "Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to the Novel Approaches for Precision Agriculture and\n  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and\nforest monitoring, including seed dispersal in hard-to-reach terrains. However,\na detailed understanding of the forest floor remains a challenge due to high\nnatural variability, quickly changing environmental parameters, and ambiguous\nannotations due to unclear definitions. To address this issue, we adapt the\nSegment Anything Model (SAM), a vision foundation model with strong\ngeneralization capabilities, to segment forest floor objects such as tree\nstumps, vegetation, and woody debris. To this end, we employ\nparameter-efficient fine-tuning (PEFT) to fine-tune a small subset of\nadditional model parameters while keeping the original weights fixed. We adjust\nSAM's mask decoder to generate masks corresponding to our dataset categories,\nallowing for automatic segmentation without manual prompting. Our results show\nthat the adapter-based PEFT method achieves the highest mean intersection over\nunion (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a\nlightweight alternative for resource-constrained UAV platforms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528SAM\u6a21\u578b\u548cPEFT\u65b9\u6cd5\u5bf9\u68ee\u6797\u5730\u9762\u5bf9\u8c61\u8fdb\u884c\u81ea\u52a8\u5206\u5272\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u81ea\u7136\u53d8\u5f02\u6027\u9ad8\u3001\u73af\u5883\u53c2\u6570\u53d8\u5316\u5feb\u4ee5\u53ca\u6807\u6ce8\u4e0d\u660e\u786e\uff0c\u5bf9\u68ee\u6797\u5730\u9762\u7684\u8be6\u7ec6\u7406\u89e3\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9002\u5e94\u4e86\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578bSegment Anything Model\uff08SAM\uff09\uff0c\u4ee5\u5206\u5272\u68ee\u6797\u5730\u9762\u5bf9\u8c61\uff0c\u5982\u6811\u6869\u3001\u690d\u88ab\u548c\u6728\u5c51\u3002", "method": "\u6211\u4eec\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u5bf9SAM\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6743\u91cd\u4e0d\u53d8\u3002\u6211\u4eec\u8c03\u6574\u4e86SAM\u7684\u63a9\u7801\u89e3\u7801\u5668\uff0c\u4ee5\u751f\u6210\u4e0e\u6211\u4eec\u7684\u6570\u636e\u96c6\u7c7b\u522b\u5bf9\u5e94\u7684\u63a9\u7801\uff0c\u4ece\u800c\u5b9e\u73b0\u65e0\u9700\u624b\u52a8\u63d0\u793a\u7684\u81ea\u52a8\u5206\u5272\u3002", "result": "\u57fa\u4e8e\u9002\u914d\u5668\u7684PEFT\u65b9\u6cd5\u5728mIoU\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800cLoRA\u5219\u63d0\u4f9b\u4e86\u8d44\u6e90\u53d7\u9650\u7684UAV\u5e73\u53f0\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u9002\u914d\u5668\u7684PEFT\u65b9\u6cd5\u5728mIoU\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800cLoRA\u5219\u63d0\u4f9b\u4e86\u8d44\u6e90\u53d7\u9650\u7684UAV\u5e73\u53f0\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.09026", "pdf": "https://arxiv.org/pdf/2505.09026", "abs": "https://arxiv.org/abs/2505.09026", "authors": ["Domniki Ladopoulou", "Dat Minh Hong", "Petros Dellaportas"], "title": "Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes", "categories": ["stat.AP", "cs.LG", "stat.ML"], "comment": "11 pages, 3 figures, 2 tables", "summary": "Accurate probabilistic forecasting of wind power is essential for maintaining\ngrid stability and enabling efficient integration of renewable energy sources.\nGaussian Process (GP) models offer a principled framework for quantifying\nuncertainty; however, conventional approaches rely on stationary kernels, which\nare inadequate for modeling the inherently non-stationary nature of wind speed\nand power output. We propose a non-stationary GP framework that incorporates\nthe generalized spectral mixture (GSM) kernel, enabling the model to capture\ntime-varying patterns and heteroscedastic behaviors in wind speed and wind\npower data. We evaluate the performance of the proposed model on real-world\nSCADA data across short\\mbox{-,} medium-, and long-term forecasting horizons.\nCompared to standard radial basis function and spectral mixture kernels, the\nGSM-based model outperforms, particularly in short-term forecasts. These\nresults highlight the necessity of modeling non-stationarity in wind power\nforecasting and demonstrate the practical value of non-stationary GP models in\noperational settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u8c31\u6df7\u5408\u6838\u7684\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u98ce\u529b\u53d1\u7535\uff0c\u7279\u522b\u662f\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u51c6\u786e\u7684\u98ce\u529b\u53d1\u7535\u6982\u7387\u9884\u6d4b\u5bf9\u4e8e\u4fdd\u6301\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u5b9e\u73b0\u53ef\u518d\u751f\u80fd\u6e90\u7684\u9ad8\u6548\u6574\u5408\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5e73\u7a33\u6838\uff0c\u8fd9\u4e0d\u8db3\u4ee5\u5efa\u6a21\u98ce\u901f\u548c\u529f\u7387\u8f93\u51fa\u7684\u56fa\u6709\u975e\u5e73\u7a33\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u5e73\u7a33\u7684\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5e7f\u4e49\u8c31\u6df7\u5408\uff08GSM\uff09\u6838\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u98ce\u901f\u548c\u98ce\u529b\u6570\u636e\u4e2d\u7684\u65f6\u53d8\u6a21\u5f0f\u548c\u5f02\u65b9\u5dee\u884c\u4e3a\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u7684SCADA\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u77ed\u671f\u3001\u4e2d\u671f\u548c\u957f\u671f\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u3002\u4e0e\u6807\u51c6\u5f84\u5411\u57fa\u51fd\u6570\u548c\u8c31\u6df7\u5408\u6838\u76f8\u6bd4\uff0c\u57fa\u4e8eGSM\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u98ce\u529b\u53d1\u7535\u9884\u6d4b\u4e2d\u5efa\u6a21\u975e\u5e73\u7a33\u6027\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u5728\u5b9e\u9645\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.08949", "pdf": "https://arxiv.org/pdf/2505.08949", "abs": "https://arxiv.org/abs/2505.08949", "authors": ["Kateryna Zorina", "David Kovar", "Mederic Fourmy", "Florent Lamiraux", "Nicolas Mansard", "Justin Carpentier", "Josef Sivic", "Vladimir Petrik"], "title": "Multi-step manipulation task and motion planning guided by video demonstration", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "This work aims to leverage instructional video to solve complex multi-step\ntask-and-motion planning tasks in robotics. Towards this goal, we propose an\nextension of the well-established Rapidly-Exploring Random Tree (RRT) planner,\nwhich simultaneously grows multiple trees around grasp and release states\nextracted from the guiding video. Our key novelty lies in combining contact\nstates and 3D object poses extracted from the guiding video with a traditional\nplanning algorithm that allows us to solve tasks with sequential dependencies,\nfor example, if an object needs to be placed at a specific location to be\ngrasped later. We also investigate the generalization capabilities of our\napproach to go beyond the scene depicted in the instructional video. To\ndemonstrate the benefits of the proposed video-guided planning approach, we\ndesign a new benchmark with three challenging tasks: (I) 3D re-arrangement of\nmultiple objects between a table and a shelf, (ii) multi-step transfer of an\nobject through a tunnel, and (iii) transferring objects using a tray similar to\na waiter transfers dishes. We demonstrate the effectiveness of our planning\nalgorithm on several robots, including the Franka Emika Panda and the KUKA KMR\niiwa. For a seamless transfer of the obtained plans to the real robot, we\ndevelop a trajectory refinement approach formulated as an optimal control\nproblem (OCP).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6559\u5b66\u89c6\u9891\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55RRT\u7b97\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6709\u6548\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u751f\u6210\u7684\u8ba1\u5212\u65e0\u7f1d\u8f6c\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u9700\u8981\u987a\u5e8f\u4f9d\u8d56\u7684\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u6559\u5b66\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684RRT\u89c4\u5212\u5668\uff0c\u540c\u65f6\u5728\u4ece\u6559\u5b66\u89c6\u9891\u4e2d\u63d0\u53d6\u7684\u6293\u53d6\u548c\u91ca\u653e\u72b6\u6001\u5468\u56f4\u751f\u957f\u591a\u68f5\u6811\u3002\u7ed3\u5408\u63a5\u89e6\u72b6\u6001\u548c3D\u7269\u4f53\u59ff\u6001\uff0c\u5229\u7528\u4f20\u7edf\u89c4\u5212\u7b97\u6cd5\u89e3\u51b3\u5177\u6709\u987a\u5e8f\u4f9d\u8d56\u6027\u7684\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff1a(I) 3D\u7269\u4f53\u5728\u684c\u5b50\u548c\u67b6\u5b50\u4e4b\u95f4\u7684\u91cd\u65b0\u6392\u5217\uff0c(ii) \u7269\u4f53\u901a\u8fc7\u96a7\u9053\u7684\u591a\u6b65\u9aa4\u4f20\u8f93\uff0c\u4ee5\u53ca(iii) \u4f7f\u7528\u6258\u76d8\u4f20\u8f93\u7269\u4f53\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6559\u5b66\u89c6\u9891\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55RRT\u7b97\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6709\u6548\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u751f\u6210\u7684\u8ba1\u5212\u65e0\u7f1d\u8f6c\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u3002"}}
{"id": "2505.09295", "pdf": "https://arxiv.org/pdf/2505.09295", "abs": "https://arxiv.org/abs/2505.09295", "authors": ["Qiming Wu", "Siqi Li", "Doudou Zhou", "Nan Liu"], "title": "Toward Fair Federated Learning under Demographic Disparities and Data Imbalance", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Ensuring fairness is critical when applying artificial intelligence to\nhigh-stakes domains such as healthcare, where predictive models trained on\nimbalanced and demographically skewed data risk exacerbating existing\ndisparities. Federated learning (FL) enables privacy-preserving collaboration\nacross institutions, but remains vulnerable to both algorithmic bias and\nsubgroup imbalance - particularly when multiple sensitive attributes intersect.\nWe propose FedIDA (Fed erated Learning for Imbalance and D isparity A\nwareness), a framework-agnostic method that combines fairness-aware\nregularization with group-conditional oversampling. FedIDA supports multiple\nsensitive attributes and heterogeneous data distributions without altering the\nconvergence behavior of the underlying FL algorithm. We provide theoretical\nanalysis establishing fairness improvement bounds using Lipschitz continuity\nand concentration inequalities, and show that FedIDA reduces the variance of\nfairness metrics across test sets. Empirical results on both benchmark and\nreal-world clinical datasets confirm that FedIDA consistently improves fairness\nwhile maintaining competitive predictive performance, demonstrating its\neffectiveness for equitable and privacy-preserving modeling in healthcare. The\nsource code is available on GitHub.", "AI": {"tldr": "FedIDA is a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling to improve fairness in federated learning for healthcare applications.", "motivation": "Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect.", "method": "FedIDA (Federated Learning for Imbalance and Disparity Awareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling.", "result": "Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance.", "conclusion": "FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare."}}
{"id": "2505.08990", "pdf": "https://arxiv.org/pdf/2505.08990", "abs": "https://arxiv.org/abs/2505.08990", "authors": ["Andrew C. Freeman"], "title": "Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ", "categories": ["cs.MM", "cs.CV", "cs.DC", "cs.NI"], "comment": "Accepted to the ICME 2025 LIVES workshop", "summary": "Live video streaming is increasingly popular on social media platforms. With\nthe growth of live streaming comes an increased need for robust content\nmoderation to remove dangerous, illegal, or otherwise objectionable content.\nWhereas video on demand distribution enables offline content analysis, live\nstreaming imposes restrictions on latency for both analysis and distribution.\nIn this paper, we present extensions to the in-progress Media Over QUIC\nTransport protocol that enable real-time content moderation in one-to-many\nvideo live streams. Importantly, our solution removes only the video segments\nthat contain objectionable content, allowing playback resumption as soon as the\nstream conforms to content policies again. Content analysis tasks may be\ntransparently distributed to arbitrary client devices. We implement and\nevaluate our system in the context of light strobe removal for photosensitive\nviewers, finding that streaming clients experience an increased latency of only\none group-of-pictures duration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMedia Over QUIC Transport\u534f\u8bae\u7684\u5b9e\u65f6\u5185\u5bb9\u5ba1\u6838\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u76f4\u64ad\u6d41\u4e2d\u5feb\u901f\u5220\u9664\u4e0d\u5f53\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u64ad\u653e\u7684\u8fde\u7eed\u6027\u3002", "motivation": "\u968f\u7740\u76f4\u64ad\u6d41\u7684\u666e\u53ca\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u5b9e\u65f6\u5185\u5bb9\u5ba1\u6838\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u5185\u5bb9\u7b26\u5408\u653f\u7b56\u8981\u6c42\u3002", "method": "\u6211\u4eec\u6269\u5c55\u4e86Media Over QUIC Transport\u534f\u8bae\uff0c\u4ee5\u5b9e\u73b0\u4e00\u5bf9\u591a\u89c6\u9891\u76f4\u64ad\u6d41\u4e2d\u7684\u5b9e\u65f6\u5185\u5bb9\u5ba1\u6838\u3002\u5185\u5bb9\u5206\u6790\u4efb\u52a1\u53ef\u4ee5\u900f\u660e\u5730\u5206\u914d\u7ed9\u4efb\u610f\u5ba2\u6237\u7aef\u8bbe\u5907\u3002", "result": "\u6211\u4eec\u5728\u5149\u95ea\u70c1\u53bb\u9664\u7684\u80cc\u666f\u4e0b\u5b9e\u73b0\u4e86\u5e76\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u7cfb\u7edf\uff0c\u53d1\u73b0\u6d41\u5a92\u4f53\u5ba2\u6237\u7aef\u7684\u5ef6\u8fdf\u4ec5\u589e\u52a0\u4e86\u4e00\u4e2a\u56fe\u50cf\u7ec4\u6301\u7eed\u65f6\u95f4\u3002", "conclusion": "\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u5728\u4e0d\u4e2d\u65ad\u64ad\u653e\u7684\u60c5\u51b5\u4e0b\u5b9e\u65f6\u5220\u9664\u542b\u6709\u4e0d\u5f53\u5185\u5bb9\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u4ece\u800c\u63d0\u9ad8\u76f4\u64ad\u6d41\u7684\u5185\u5bb9\u5ba1\u6838\u6548\u7387\u3002"}}
{"id": "2505.08998", "pdf": "https://arxiv.org/pdf/2505.08998", "abs": "https://arxiv.org/abs/2505.08998", "authors": ["Liwen Wu", "Sai Bi", "Zexiang Xu", "Hao Tan", "Kai Zhang", "Fujun Luan", "Haolin Lu", "Ravi Ramamoorthi"], "title": "Neural BRDF Importance Sampling by Reparameterization", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Neural bidirectional reflectance distribution functions (BRDFs) have emerged\nas popular material representations for enhancing realism in physically-based\nrendering. Yet their importance sampling remains a significant challenge. In\nthis paper, we introduce a reparameterization-based formulation of neural BRDF\nimportance sampling that seamlessly integrates into the standard rendering\npipeline with precise generation of BRDF samples. The reparameterization-based\nformulation transfers the distribution learning task to a problem of\nidentifying BRDF integral substitutions. In contrast to previous methods that\nrely on invertible networks and multi-step inference to reconstruct BRDF\ndistributions, our model removes these constraints, which offers greater\nflexibility and efficiency. Our variance and performance analysis demonstrates\nthat our reparameterization method achieves the best variance reduction in\nneural BRDF renderings while maintaining high inference speeds compared to\nexisting baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u65b0\u53c2\u6570\u5316\u7684\u795e\u7ecfBRDF\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u6e32\u67d3\u6d41\u7a0b\u4e2d\uff0c\u5e76\u5728\u4fdd\u6301\u9ad8\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u4f73\u7684\u65b9\u5dee\u51cf\u5c11\u3002", "motivation": "\u795e\u7ecf\u53cc\u5411\u53cd\u5c04\u5206\u5e03\u51fd\u6570\uff08BRDF\uff09\u4f5c\u4e3a\u589e\u5f3a\u7269\u7406\u57fa\u7840\u6e32\u67d3\u771f\u5b9e\u611f\u7684\u6d41\u884c\u6750\u6599\u8868\u793a\u5f62\u5f0f\uff0c\u4f46\u5176\u91cd\u8981\u6027\u91c7\u6837\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u65b0\u53c2\u6570\u5316\u7684\u795e\u7ecfBRDF\u91cd\u8981\u6027\u91c7\u6837\u516c\u5f0f\uff0c\u8be5\u516c\u5f0f\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u6e32\u67d3\u7ba1\u9053\u4e2d\uff0c\u5e76\u7cbe\u786e\u751f\u6210BRDF\u6837\u672c\u3002", "result": "\u6211\u4eec\u7684\u65b9\u5dee\u548c\u6027\u80fd\u5206\u6790\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u91cd\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u795e\u7ecfBRDF\u6e32\u67d3\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u65b9\u5dee\u51cf\u5c11\u3002", "conclusion": "\u6211\u4eec\u7684\u91cd\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u795e\u7ecfBRDF\u6e32\u67d3\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u65b9\u5dee\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2505.09342", "pdf": "https://arxiv.org/pdf/2505.09342", "abs": "https://arxiv.org/abs/2505.09342", "authors": ["Mostafa Jafari", "Alireza Shameli-Sendi"], "title": "Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems", "categories": ["cs.CR", "cs.AI", "cs.LG", "68", "I.2.1"], "comment": "Submitted to IEEE Transactions on Information Forensics and Security\n  (T-IFS), 13 pages, 4 figures", "summary": "Machine learning is a key tool for Android malware detection, effectively\nidentifying malicious patterns in apps. However, ML-based detectors are\nvulnerable to evasion attacks, where small, crafted changes bypass detection.\nDespite progress in adversarial defenses, the lack of comprehensive evaluation\nframeworks in binary-constrained domains limits understanding of their\nrobustness. We introduce two key contributions. First, Prioritized Binary\nRounding, a technique to convert continuous perturbations into binary feature\nspaces while preserving high attack success and low perturbation size. Second,\nthe sigma-binary attack, a novel adversarial method for binary domains,\ndesigned to achieve attack goals with minimal feature changes. Experiments on\nthe Malscan dataset show that sigma-binary outperforms existing attacks and\nexposes key vulnerabilities in state-of-the-art defenses. Defenses equipped\nwith adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant\nbrittleness, with attack success rates exceeding 90% using fewer than 10\nfeature modifications and reaching 100% with just 20. Adversarially trained\ndefenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small\nbudgets but remains vulnerable to unrestricted perturbations, with attack\nsuccess rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates\nstrong robustness against state-of-the-art gradient-based adversarial attacks\nby maintaining an attack success rate below 16.55%, the sigma-binary attack\nsignificantly outperforms these methods, achieving a 94.56% success rate under\nunrestricted perturbations. These findings highlight the critical need for\nprecise method like sigma-binary to expose hidden vulnerabilities in existing\ndefenses and support the development of more resilient malware detection\nsystems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Prioritized Binary Rounding\u548csigma-binary\u653b\u51fb\uff0c\u4ee5\u5728\u4e8c\u8fdb\u5236\u9886\u57df\u4e2d\u5b9e\u73b0\u6700\u5c0f\u7279\u5f81\u53d8\u5316\u7684\u653b\u51fb\u76ee\u6807\u3002\u5b9e\u9a8c\u8868\u660e\uff0csigma-binary\u653b\u51fb\u6bd4\u73b0\u6709\u653b\u51fb\u66f4\u6709\u6548\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u9632\u5fa1\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "motivation": "ML-based detectors\u5728\u4e8c\u8fdb\u5236\u53d7\u9650\u9886\u57df\u4e2d\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u9c81\u68d2\u6027\u7684\u7406\u89e3\u3002", "method": "Prioritized Binary Rounding\uff08\u4f18\u5148\u4e8c\u8fdb\u5236\u820d\u5165\uff09\u548csigma-binary\u653b\u51fb\uff08\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u65b9\u6cd5\uff09\u3002", "result": "sigma-binary\u653b\u51fb\u5728Malscan\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\uff0c\u5e76\u63ed\u793a\u4e86\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002\u914d\u5907\u5bf9\u6297\u68c0\u6d4b\u5668\u7684\u9632\u5fa1\u63aa\u65bd\u8868\u73b0\u51fa\u663e\u8457\u7684\u8106\u5f31\u6027\uff0c\u4f7f\u7528\u5c11\u4e8e10\u4e2a\u7279\u5f81\u4fee\u6539\u7684\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc790%\u3002\u5bf9\u6297\u8bad\u7ec3\u7684\u9632\u5fa1\u63aa\u65bd\u5728\u5c0f\u9884\u7b97\u4e0b\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u4e0d\u53d7\u9650\u5236\u7684\u6270\u52a8\u5f71\u54cd\u3002PAD-SMA\u5728\u5bf9\u6297\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u4f46sigma-binary\u653b\u51fb\u5728\u4e0d\u53d7\u9650\u5236\u7684\u6270\u52a8\u4e0b\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u50cfsigma-binary\u8fd9\u6837\u7684\u7cbe\u786e\u65b9\u6cd5\u5728\u66b4\u9732\u73b0\u6709\u9632\u5fa1\u4e2d\u7684\u9690\u85cf\u6f0f\u6d1e\u548c\u652f\u6301\u5f00\u53d1\u66f4\u5f3a\u5927\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.09075", "pdf": "https://arxiv.org/pdf/2505.09075", "abs": "https://arxiv.org/abs/2505.09075", "authors": ["Carlos Misael Madrid Padilla", "Oscar Hernan Madrid Padilla", "Sabyasachi Chatterjee"], "title": "Risk Bounds For Distributional Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This work examines risk bounds for nonparametric distributional regression\nestimators. For convex-constrained distributional regression, general upper\nbounds are established for the continuous ranked probability score (CRPS) and\nthe worst-case mean squared error (MSE) across the domain. These theoretical\nresults are applied to isotonic and trend filtering distributional regression,\nyielding convergence rates consistent with those for mean estimation.\nFurthermore, a general upper bound is derived for distributional regression\nunder non-convex constraints, with a specific application to neural\nnetwork-based estimators. Comprehensive experiments on both simulated and real\ndata validate the theoretical contributions, demonstrating their practical\neffectiveness.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u53c2\u6570\u5206\u5e03\u56de\u5f52\u4f30\u8ba1\u5668\u7684\u98ce\u9669\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u975e\u53c2\u6570\u5206\u5e03\u56de\u5f52\u4f30\u8ba1\u5668\u7684\u98ce\u9669\u754c\u9650\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u53c2\u6570\u5206\u5e03\u56de\u5f52\u4f30\u8ba1\u5668\u7684\u98ce\u9669\u754c\u9650\uff0c\u5efa\u7acb\u4e86\u51f8\u7ea6\u675f\u5206\u5e03\u56de\u5f52\u7684CRPS\u548c\u6700\u574f\u60c5\u51b5MSE\u7684\u4e0a\u754c\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u7b49\u8ddd\u548c\u8d8b\u52bf\u8fc7\u6ee4\u5206\u5e03\u56de\u5f52\u3002\u6b64\u5916\uff0c\u8fd8\u4e3a\u975e\u51f8\u7ea6\u675f\u4e0b\u7684\u5206\u5e03\u56de\u5f52\u5efa\u7acb\u4e86\u4e0a\u754c\uff0c\u5e76\u5e94\u7528\u4e8e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4f30\u8ba1\u5668\u3002", "result": "\u672c\u6587\u5efa\u7acb\u4e86\u51f8\u7ea6\u675f\u548c\u975e\u51f8\u7ea6\u675f\u4e0b\u5206\u5e03\u56de\u5f52\u7684\u4e0a\u754c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u8d21\u732e\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u6548\u679c\u3002"}}
{"id": "2505.09343", "pdf": "https://arxiv.org/pdf/2505.09343", "abs": "https://arxiv.org/abs/2505.09343", "authors": ["Chenggang Zhao", "Chengqi Deng", "Chong Ruan", "Damai Dai", "Huazuo Gao", "Jiashi Li", "Liyue Zhang", "Panpan Huang", "Shangyan Zhou", "Shirong Ma", "Wenfeng Liang", "Ying He", "Yuqing Wang", "Yuxuan Liu", "Y. X. Wei"], "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures", "categories": ["cs.DC", "cs.AI", "cs.AR"], "comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive version will appear as\n  part of the Industry Track in Proceedings of the 52nd Annual International\n  Symposium on Computer Architecture (ISCA '25)", "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86DeepSeek-V3\u6a21\u578b\u67b6\u6784\u53ca\u5176AI\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u51fa\u4e86\u786c\u4ef6\u548c\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u786c\u4ef6\u74f6\u9888\u3002", "motivation": "\u5f53\u524d\u786c\u4ef6\u67b6\u6784\u5728\u5185\u5b58\u5bb9\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u548c\u4e92\u8fde\u5e26\u5bbd\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86DeepSeek-V3/R1\u6a21\u578b\u67b6\u6784\u53ca\u5176AI\u57fa\u7840\u8bbe\u65bd\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u3001\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\u3001FP8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u591a\u5e73\u9762\u7f51\u7edc\u62d3\u6251\u7b49\u5173\u952e\u6280\u672f\u3002", "result": "DeepSeek-V3\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u786c\u4ef6\u548c\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u5728\u6ee1\u8db3AI\u5de5\u4f5c\u8d1f\u8f7d\u65e5\u76ca\u589e\u957f\u9700\u6c42\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u521b\u65b0\u84dd\u56fe\u3002"}}
{"id": "2505.09087", "pdf": "https://arxiv.org/pdf/2505.09087", "abs": "https://arxiv.org/abs/2505.09087", "authors": ["He Wang", "Yikun Zhang", "Jie Chen", "Jian Zhan", "Yaoqi Zhou"], "title": "A Comparative Review of RNA Language Models", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Given usefulness of protein language models (LMs) in structure and functional\ninference, RNA LMs have received increased attentions in the last few years.\nHowever, these RNA models are often not compared against the same standard.\nHere, we divided RNA LMs into three classes (pretrained on multiple RNA types\n(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with\nDNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein\nLMs as controls in zero-shot prediction of RNA secondary structure and\nfunctional classification. Results shows that the models doing well on\nsecondary structure prediction often perform worse in function classification\nor vice versa, suggesting that more balanced unsupervised training is needed.", "AI": {"tldr": "\u672c\u6587\u5bf913\u79cdRNA\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca3\u79cdDNA\u548c1\u79cd\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u548c\u529f\u80fd\u5206\u7c7b\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u77db\u76fe\uff0c\u6697\u793a\u9700\u8981\u66f4\u5e73\u8861\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u3002", "motivation": "\u7531\u4e8e\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u548c\u529f\u80fd\u63a8\u65ad\u4e2d\u7684\u6709\u7528\u6027\uff0c\u8fd1\u5e74\u6765RNA\u8bed\u8a00\u6a21\u578b\u53d7\u5230\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9bRNA\u6a21\u578b\u901a\u5e38\u6ca1\u6709\u7ecf\u8fc7\u76f8\u540c\u7684\u6bd4\u8f83\u6807\u51c6\u3002", "method": "\u5c06RNA\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u4e09\u7c7b\uff0c\u5e76\u4e0e3\u4e2aDNA\u548c1\u4e2a\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4bRNA\u4e8c\u7ea7\u7ed3\u6784\u548c\u529f\u80fd\u5206\u7c7b\u7684\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u4e0a\u7684\u8868\u73b0\u4e0e\u529f\u80fd\u5206\u7c7b\u4e0a\u7684\u8868\u73b0\u5f80\u5f80\u76f8\u4e92\u77db\u76fe\uff0c\u8fd9\u8868\u660e\u9700\u8981\u66f4\u5e73\u8861\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u90a3\u4e9b\u5728\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u5728\u529f\u80fd\u5206\u7c7b\u4e0a\u53ef\u80fd\u8868\u73b0\u8f83\u5dee\uff0c\u53cd\u4e4b\u4ea6\u7136\uff0c\u8fd9\u8868\u660e\u9700\u8981\u66f4\u5e73\u8861\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u3002"}}
{"id": "2505.09109", "pdf": "https://arxiv.org/pdf/2505.09109", "abs": "https://arxiv.org/abs/2505.09109", "authors": ["Yuxing Chen", "Bowen Xiao", "He Wang"], "title": "FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Due to the deformability of garments, generating a large amount of\nhigh-quality data for robotic garment manipulation tasks is highly challenging.\nIn this paper, we present a synthetic garment dataset that can be used for\nrobotic garment folding. We begin by constructing geometric garment templates\nbased on keypoints and applying generative models to generate realistic texture\npatterns. Leveraging these keypoint annotations, we generate folding\ndemonstrations in simulation and train folding policies via closed-loop\nimitation learning. To improve robustness, we propose KG-DAgger, which uses a\nkeypoint-based strategy to generate demonstration data for recovering from\nfailures. KG-DAgger significantly improves the model performance, boosting the\nreal-world success rate by 25\\%. After training with 15K trajectories (about 2M\nimage-action pairs), the model achieves a 75\\% success rate in the real world.\nExperiments in both simulation and real-world settings validate the\neffectiveness of our proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u670d\u88c5\u6298\u53e0\u7684\u5408\u6210\u670d\u88c5\u6570\u636e\u96c6\u3002\u901a\u8fc7\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u7b56\u7565\u751f\u6210\u6f14\u793a\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u4e8675%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u7531\u4e8e\u670d\u88c5\u7684\u53ef\u53d8\u5f62\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c\u4efb\u52a1\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u662f\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u7684\u3002", "method": "\u6211\u4eec\u9996\u5148\u57fa\u4e8e\u5173\u952e\u70b9\u6784\u5efa\u51e0\u4f55\u670d\u88c5\u6a21\u677f\uff0c\u5e76\u5e94\u7528\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u903c\u771f\u7684\u7eb9\u7406\u56fe\u6848\u3002\u5229\u7528\u8fd9\u4e9b\u5173\u952e\u70b9\u6ce8\u91ca\uff0c\u6211\u4eec\u5728\u6a21\u62df\u4e2d\u751f\u6210\u6298\u53e0\u6f14\u793a\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u6298\u53e0\u7b56\u7565\u3002\u4e3a\u4e86\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86KG-DAgger\uff0c\u5b83\u4f7f\u7528\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u7b56\u7565\u6765\u751f\u6210\u6062\u590d\u5931\u8d25\u7684\u6f14\u793a\u6570\u636e\u3002", "result": "KG-DAgger\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c06\u771f\u5b9e\u4e16\u754c\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8625%\u3002\u572815K\u8f68\u8ff9\uff08\u7ea62M\u56fe\u50cf-\u52a8\u4f5c\u5bf9\uff09\u4e0a\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6210\u529f\u7387\u4e3a75%\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u90fd\u6709\u6548\u3002"}}
{"id": "2505.09371", "pdf": "https://arxiv.org/pdf/2505.09371", "abs": "https://arxiv.org/abs/2505.09371", "authors": ["Akash Kundu", "Stefano Mangini"], "title": "TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG"], "comment": "The code will be available soon! Comments are welcomed!", "summary": "Variational quantum algorithms hold the promise to address meaningful quantum\nproblems already on noisy intermediate-scale quantum hardware, but they face\nthe challenge of designing quantum circuits that both solve the target problem\nand comply with device limitations. Quantum architecture search (QAS) automates\nthis design process, with reinforcement learning (RL) emerging as a promising\napproach. Yet, RL-based QAS methods encounter significant scalability issues,\nas computational and training costs grow rapidly with the number of qubits,\ncircuit depth, and noise, severely impacting performance. To address these\nchallenges, we introduce $\\textit{TensorRL-QAS}$, a scalable framework that\ncombines tensor network (TN) methods with RL for designing quantum circuits. By\nwarm-starting the architecture search with a matrix product state approximation\nof the target solution, TensorRL-QAS effectively narrows the search space to\nphysically meaningful circuits, accelerating convergence to the desired\nsolution. Tested on several quantum chemistry problems of up to 12-qubit,\nTensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth\ncompared to baseline methods, while maintaining or surpassing chemical\naccuracy. It reduces function evaluations by up to 100-fold, accelerates\ntraining episodes by up to $98\\%$, and achieves up to $50\\%$ success\nprobability for 10-qubit systems-far exceeding the $<1\\%$ rates of baseline\napproaches. Robustness and versatility are demonstrated both in the noiseless\nand noisy scenarios, where we report a simulation of up to 8-qubit. These\nadvancements establish TensorRL-QAS as a promising candidate for a scalable and\nefficient quantum circuit discovery protocol on near-term quantum hardware.", "AI": {"tldr": "TensorRL-QAS \u662f\u4e00\u79cd\u7ed3\u5408\u5f20\u91cf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u91cf\u5b50\u7535\u8def\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u3001\u7535\u8def\u6df1\u5ea6\u548c\u566a\u58f0\u589e\u52a0\u65f6\u8ba1\u7b97\u548c\u8bad\u7ec3\u6210\u672c\u8fc5\u901f\u589e\u957f\u7684\u95ee\u9898\u3002", "method": "TensorRL-QAS \u7ed3\u5408\u4e86\u5f20\u91cf\u7f51\u7edc\uff08TN\uff09\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u8bbe\u8ba1\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u901a\u8fc7\u77e9\u9635\u4e58\u79ef\u72b6\u6001\u8fd1\u4f3c\u76ee\u6807\u89e3\u51b3\u65b9\u6848\u6765\u542f\u52a8\u67b6\u6784\u641c\u7d22\u3002", "result": "TensorRL-QAS \u5728\u6700\u591a 12 \u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u5316\u5b66\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86 CNOT \u6570\u91cf\u548c\u7535\u8def\u6df1\u5ea6\u51cf\u5c11 10 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u5316\u5b66\u7cbe\u5ea6\u3002\u5b83\u51cf\u5c11\u4e86\u529f\u80fd\u8bc4\u4f30\u591a\u8fbe 100 \u500d\uff0c\u52a0\u901f\u4e86\u8bad\u7ec3\u5468\u671f\u9ad8\u8fbe 98%\uff0c\u5e76\u5728 10 \u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 50% \u7684\u6210\u529f\u7387\uff0c\u8fdc\u8d85\u57fa\u7ebf\u65b9\u6cd5\u7684 <1%\u3002", "conclusion": "TensorRL-QAS \u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u91cf\u5b50\u7535\u8def\u53d1\u73b0\u534f\u8bae\uff0c\u9002\u7528\u4e8e\u8fd1\u671f\u7684\u91cf\u5b50\u786c\u4ef6\u3002"}}
{"id": "2505.09098", "pdf": "https://arxiv.org/pdf/2505.09098", "abs": "https://arxiv.org/abs/2505.09098", "authors": ["Yan Hao Ling", "Zhouhao Yang", "Jonathan Scarlett"], "title": "Statistical Mean Estimation with Coded Relayed Observations", "categories": ["cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "comment": null, "summary": "We consider a problem of statistical mean estimation in which the samples are\nnot observed directly, but are instead observed by a relay (``teacher'') that\ntransmits information through a memoryless channel to the decoder\n(``student''), who then produces the final estimate. We consider the minimax\nestimation error in the large deviations regime, and establish achievable error\nexponents that are tight in broad regimes of the estimation accuracy and\nchannel quality. In contrast, two natural baseline methods are shown to yield\nstrictly suboptimal error exponents. We initially focus on Bernoulli sources\nand binary symmetric channels, and then generalize to sub-Gaussian and\nheavy-tailed settings along with arbitrary discrete memoryless channels.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6837\u672c\u4e0d\u76f4\u63a5\u89c2\u5bdf\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e2d\u7ee7\u4f20\u8f93\u4fe1\u606f\u8fdb\u884c\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u5b9e\u73b0\u7684\u8bef\u5dee\u6307\u6570\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u5728\u6837\u672c\u4e0d\u76f4\u63a5\u89c2\u5bdf\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u4e2d\u7ee7\u4f20\u8f93\u4fe1\u606f\u6765\u63d0\u9ad8\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u9996\u5148\u4e13\u6ce8\u4e8e\u4f2f\u52aa\u5229\u6e90\u548c\u4e8c\u8fdb\u5236\u5bf9\u79f0\u4fe1\u9053\uff0c\u7136\u540e\u6269\u5c55\u5230\u6b21\u9ad8\u65af\u548c\u91cd\u5c3e\u8bbe\u7f6e\u4ee5\u53ca\u4efb\u610f\u79bb\u6563\u65e0\u8bb0\u5fc6\u4fe1\u9053\u3002", "result": "\u672c\u6587\u5efa\u7acb\u4e86\u5728\u5927\u504f\u5dee\u8303\u56f4\u5185\u6700\u5c0f\u6700\u5927\u4f30\u8ba1\u8bef\u5dee\u7684\u53ef\u5b9e\u73b0\u8bef\u5dee\u6307\u6570\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e9b\u6307\u6570\u5728\u5e7f\u6cdb\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u4fe1\u9053\u8d28\u91cf\u8303\u56f4\u5185\u662f\u7d27\u81f4\u7684\u3002\u4e0e\u4e24\u79cd\u81ea\u7136\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6837\u672c\u4e0d\u76f4\u63a5\u89c2\u5bdf\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e2d\u7ee7\uff08\u6559\u5e08\uff09\u901a\u8fc7\u65e0\u8bb0\u5fc6\u4fe1\u9053\u4f20\u8f93\u4fe1\u606f\u7ed9\u89e3\u7801\u5668\uff08\u5b66\u751f\uff09\u8fdb\u884c\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u7684\u95ee\u9898\u3002\u6211\u4eec\u5efa\u7acb\u4e86\u5728\u5927\u504f\u5dee\u8303\u56f4\u5185\u6700\u5c0f\u6700\u5927\u4f30\u8ba1\u8bef\u5dee\u7684\u53ef\u5b9e\u73b0\u8bef\u5dee\u6307\u6570\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e9b\u6307\u6570\u5728\u5e7f\u6cdb\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u4fe1\u9053\u8d28\u91cf\u8303\u56f4\u5185\u662f\u7d27\u81f4\u7684\u3002\u4e0e\u4e24\u79cd\u81ea\u7136\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2505.09193", "pdf": "https://arxiv.org/pdf/2505.09193", "abs": "https://arxiv.org/abs/2505.09193", "authors": ["Wei Jiang", "Junru Li", "Kai Zhang", "Li Zhang"], "title": "BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression", "categories": ["eess.IV", "cs.CV"], "comment": "The first learned video codec that surpasses VTM 13.2 RA across all\n  standard test datasets. Code will be available at\n  https://github.com/JiangWeibeta/ECVC", "summary": "Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead. To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets. Code will be available\nat https://github.com/JiangWeibeta/ECVC.", "AI": {"tldr": "BiECVC\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u5411\u89c6\u9891\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6837\u5316\u5c40\u90e8\u548c\u975e\u5c40\u90e8\u4e0a\u4e0b\u6587\u5efa\u6a21\u4ee5\u53ca\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u95e8\u63a7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684BVC\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u65f6\u95f4\u8fd0\u52a8\uff0c\u800c\u5ffd\u7565\u4e86\u8de8\u5e27\u7684\u975e\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u7f3a\u4e4f\u52a8\u6001\u6291\u5236\u56e0\u5feb\u901f\u8fd0\u52a8\u6216\u906e\u6321\u4ea7\u751f\u7684\u6709\u5bb3\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BiECVC\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u6837\u5316\u5c40\u90e8\u548c\u975e\u5c40\u90e8\u4e0a\u4e0b\u6587\u5efa\u6a21\u4ee5\u53ca\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u95e8\u63a7\u3002\u5b83\u91cd\u65b0\u4f7f\u7528\u4f4e\u5c42\u7684\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u89e3\u7801\u7684\u8fd0\u52a8\u77e2\u91cf\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ee5\u907f\u514d\u5f15\u5165\u989d\u5916\u7684\u8fd0\u52a8\u5f00\u9500\u3002\u6b64\u5916\uff0c\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u6765\u9ad8\u6548\u5efa\u6a21\u975e\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u53cc\u5411\u4e0a\u4e0b\u6587\u95e8\u63a7\u6765\u52a8\u6001\u8fc7\u6ee4\u57fa\u4e8e\u6761\u4ef6\u7f16\u7801\u7ed3\u679c\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "BiECVC\u5728Random Access (RA)\u914d\u7f6e\u4e0b\uff0c\u4e0eVTM 13.2\u76f8\u6bd4\uff0c\u5206\u522b\u5728intra\u5468\u671f\u4e3a32\u548c64\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u6bd4\u7279\u7387\u964d\u4f4e\u4e8613.4%\u548c15.7%\u3002BiECVC\u662f\u7b2c\u4e00\u4e2a\u5728\u6240\u6709\u6807\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aVTM 13.2 RA\u7684 learned video codec\u3002", "conclusion": "BiECVC\u662f\u7b2c\u4e00\u4e2a\u5728\u6240\u6709\u6807\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aVTM 13.2 RA\u7684 learned video codec\u3002"}}
{"id": "2505.09099", "pdf": "https://arxiv.org/pdf/2505.09099", "abs": "https://arxiv.org/abs/2505.09099", "authors": ["Shirui Lyu", "Vittorio Caggiano", "Matteo Leonetti", "Dario Farina", "Letizia Gionfrida"], "title": "Imitation Learning for Adaptive Control of a Virtual Soft Exoglove", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "The use of wearable robots has been widely adopted in rehabilitation training\nfor patients with hand motor impairments. However, the uniqueness of patients'\nmuscle loss is often overlooked. Leveraging reinforcement learning and a\nbiologically accurate musculoskeletal model in simulation, we propose a\ncustomized wearable robotic controller that is able to address specific muscle\ndeficits and to provide compensation for hand-object manipulation tasks. Video\ndata of a same subject performing human grasping tasks is used to train a\nmanipulation model through learning from demonstration. This manipulation model\nis subsequently fine-tuned to perform object-specific interaction tasks. The\nmuscle forces in the musculoskeletal manipulation model are then weakened to\nsimulate neurological motor impairments, which are later compensated by the\nactuation of a virtual wearable robotics glove. Results shows that integrating\nthe virtual wearable robotic glove provides shared assistance to support the\nhand manipulator with weakened muscle forces. The learned exoglove controller\nachieved an average of 90.5\\% of the original manipulation proficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u7269\u51c6\u786e\u9aa8\u9abc\u808c\u6a21\u578b\u7684\u5b9a\u5236\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u90e8\u8fd0\u52a8\u969c\u788d\u60a3\u8005\u7684\u7279\u5b9a\u808c\u8089\u7f3a\u9677\u95ee\u9898\u3002\u901a\u8fc7\u5b66\u4e60\u6f14\u793a\u8bad\u7ec3\u64cd\u4f5c\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5fae\u8c03\u4ee5\u6267\u884c\u7279\u5b9a\u7269\u4f53\u7684\u4ea4\u4e92\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408\u865a\u62df\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u624b\u5957\u53ef\u4ee5\u6709\u6548\u652f\u6301\u808c\u8089\u529b\u91cf\u51cf\u5f31\u7684\u624b\u90e8\u64cd\u4f5c\u5668\uff0c\u8fbe\u5230\u539f\u59cb\u64cd\u4f5c\u719f\u7ec3\u5ea6\u7684\u5e73\u574790.5%\u3002", "motivation": "\u7531\u4e8e\u60a3\u8005\u808c\u8089\u635f\u5931\u7684\u72ec\u7279\u6027\u7ecf\u5e38\u88ab\u5ffd\u89c6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9488\u5bf9\u7279\u5b9a\u808c\u8089\u7f3a\u9677\u63d0\u4f9b\u8865\u507f\u7684\u5b9a\u5236\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u63a7\u5236\u5668\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u7269\u51c6\u786e\u7684\u9aa8\u9abc\u808c\u6a21\u578b\u8fdb\u884c\u6a21\u62df\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u5236\u7684\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u63a7\u5236\u5668\uff0c\u80fd\u591f\u89e3\u51b3\u7279\u5b9a\u7684\u808c\u8089\u7f3a\u9677\uff0c\u5e76\u4e3a\u624b-\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u8865\u507f\u3002\u901a\u8fc7\u5b66\u4e60\u6f14\u793a\u8bad\u7ec3\u4e86\u64cd\u4f5c\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u4ee5\u6267\u884c\u7279\u5b9a\u7269\u4f53\u7684\u4ea4\u4e92\u4efb\u52a1\u3002\u968f\u540e\uff0c\u9aa8\u9abc\u808c\u64cd\u4f5c\u6a21\u578b\u4e2d\u7684\u808c\u8089\u529b\u91cf\u88ab\u524a\u5f31\u4ee5\u6a21\u62df\u795e\u7ecf\u8fd0\u52a8\u969c\u788d\uff0c\u5e76\u7531\u865a\u62df\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u624b\u5957\u7684\u9a71\u52a8\u8fdb\u884c\u8865\u507f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408\u865a\u62df\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u624b\u5957\u53ef\u4ee5\u63d0\u4f9b\u5171\u4eab\u8f85\u52a9\uff0c\u4ee5\u652f\u6301\u808c\u8089\u529b\u91cf\u51cf\u5f31\u7684\u624b\u90e8\u64cd\u4f5c\u5668\u3002\u5b66\u4e60\u5230\u7684\u5916\u624b\u5957\u63a7\u5236\u5668\u8fbe\u5230\u4e86\u539f\u59cb\u64cd\u4f5c\u719f\u7ec3\u5ea6\u7684\u5e73\u574790.5%\u3002", "conclusion": "\u6574\u5408\u865a\u62df\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u624b\u5957\u53ef\u4ee5\u63d0\u4f9b\u5171\u4eab\u8f85\u52a9\uff0c\u4ee5\u652f\u6301\u808c\u8089\u529b\u91cf\u51cf\u5f31\u7684\u624b\u90e8\u64cd\u4f5c\u5668\u3002\u5b66\u4e60\u5230\u7684\u5916\u624b\u5957\u63a7\u5236\u5668\u8fbe\u5230\u4e86\u539f\u59cb\u64cd\u4f5c\u719f\u7ec3\u5ea6\u7684\u5e73\u574790.5%\u3002"}}
{"id": "2505.09382", "pdf": "https://arxiv.org/pdf/2505.09382", "abs": "https://arxiv.org/abs/2505.09382", "authors": ["Zhengyan Sheng", "Jinghao He", "Liping Chen", "Kong Aik Lee", "Zhen-Hua Ling"], "title": "The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Voice timbre refers to the unique quality or character of a person's voice\nthat distinguishes it from others as perceived by human hearing. The Voice\nTimbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the\nvoice timbre attribute in a comparative manner. In this challenge, the human\nimpression of voice timbre is verbalized with a set of sensory descriptors,\nincluding bright, coarse, soft, magnetic, and so on. The timbre is explained\nfrom the comparison between two voices in their intensity within a specific\ndescriptor dimension. The VtaD 2025 challenge starts in May and culminates in a\nspecial proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,\nChina.", "AI": {"tldr": "VtaD 2025\u6311\u6218\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83\u65b9\u5f0f\u89e3\u91ca\u58f0\u97f3\u97f3\u8272\u5c5e\u6027\uff0c\u5e76\u5728NCMMSC2025\u4f1a\u8bae\u4e0a\u8fdb\u884c\u7279\u522b\u63d0\u6848\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u63cf\u8ff0\u58f0\u97f3\u97f3\u8272\u5c5e\u6027\uff0c\u8be5\u6311\u6218\u5c06\u4eba\u7c7b\u5bf9\u58f0\u97f3\u97f3\u8272\u7684\u5370\u8c61\u7528\u4e00\u7cfb\u5217\u611f\u5b98\u63cf\u8ff0\u7b26\u8fdb\u884c\u8868\u8fbe\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\u58f0\u97f3\u5728\u7279\u5b9a\u63cf\u8ff0\u7b26\u7ef4\u5ea6\u4e2d\u7684\u5f3a\u5ea6\u6765\u89e3\u91ca\u97f3\u8272\u5c5e\u6027\u3002", "result": "VtaD 2025\u6311\u6218\u5c06\u4e8e2025\u5e7410\u6708\u5728\u4e2d\u56fd\u9547\u6c5f\u7684NCMMSC2025\u4f1a\u8bae\u4e0a\u4e3e\u884c\u7279\u522b\u63d0\u6848\u3002", "conclusion": "VtaD 2025\u6311\u6218\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83\u65b9\u5f0f\u89e3\u91ca\u58f0\u97f3\u97f3\u8272\u5c5e\u6027\uff0c\u5e76\u5728NCMMSC2025\u4f1a\u8bae\u4e0a\u8fdb\u884c\u7279\u522b\u63d0\u6848\u3002"}}
{"id": "2505.09110", "pdf": "https://arxiv.org/pdf/2505.09110", "abs": "https://arxiv.org/abs/2505.09110", "authors": ["Zhihao Dou", "Jiaqi Wang", "Wei Sun", "Zhuqing Liu", "Minghong Fang"], "title": "Toward Malicious Clients Detection in Federated Learning", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "To appear in ACM ASIACCS 2025", "summary": "Federated learning (FL) enables multiple clients to collaboratively train a\nglobal machine learning model without sharing their raw data. However, the\ndecentralized nature of FL introduces vulnerabilities, particularly to\npoisoning attacks, where malicious clients manipulate their local models to\ndisrupt the training process. While Byzantine-robust aggregation rules have\nbeen developed to mitigate such attacks, they remain inadequate against more\nadvanced threats. In response, recent advancements have focused on FL detection\ntechniques to identify potentially malicious participants. Unfortunately, these\nmethods often misclassify numerous benign clients as threats or rely on\nunrealistic assumptions about the server's capabilities. In this paper, we\npropose a novel algorithm, SafeFL, specifically designed to accurately identify\nmalicious clients in FL. The SafeFL approach involves the server collecting a\nseries of global models to generate a synthetic dataset, which is then used to\ndistinguish between malicious and benign models based on their behavior.\nExtensive testing demonstrates that SafeFL outperforms existing methods,\noffering superior efficiency and accuracy in detecting malicious clients.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSafeFL\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u51c6\u786e\u8bc6\u522b\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6076\u610f\u5ba2\u6237\u7aef\u3002", "motivation": "\u73b0\u6709\u7684FL\u68c0\u6d4b\u6280\u672f\u5e38\u5e38\u9519\u8bef\u5730\u5c06\u5927\u91cf\u826f\u6027\u5ba2\u6237\u7aef\u5206\u7c7b\u4e3a\u5a01\u80c1\uff0c\u6216\u8005\u4f9d\u8d56\u4e8e\u5bf9\u670d\u52a1\u5668\u80fd\u529b\u7684\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\u3002", "method": "SafeFL\u65b9\u6cd5\u6d89\u53ca\u670d\u52a1\u5668\u6536\u96c6\u4e00\u7cfb\u5217\u5168\u5c40\u6a21\u578b\u4ee5\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u5176\u884c\u4e3a\u533a\u5206\u6076\u610f\u548c\u826f\u6027\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafeFL\u5728\u68c0\u6d4b\u6076\u610f\u5ba2\u6237\u7aef\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "SafeFL\u5728\u68c0\u6d4b\u6076\u610f\u5ba2\u6237\u7aef\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.09315", "pdf": "https://arxiv.org/pdf/2505.09315", "abs": "https://arxiv.org/abs/2505.09315", "authors": ["Xuefeng Jiang", "Yuan Ma", "Pengxiang Li", "Leimeng Xu", "Xin Wen", "Kun Zhan", "Zhongpu Xia", "Peng Jia", "XianPeng Lang", "Sheng Sun"], "title": "TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Under review", "summary": "In recent years, diffusion model has shown its potential across diverse\ndomains from vision generation to language modeling. Transferring its\ncapabilities to modern autonomous driving systems has also emerged as a\npromising direction.In this work, we propose TransDiffuser, an encoder-decoder\nbased generative trajectory planning model for end-to-end autonomous driving.\nThe encoded scene information serves as the multi-modal conditional input of\nthe denoising decoder. To tackle the mode collapse dilemma in generating\nhigh-quality diverse trajectories, we introduce a simple yet effective\nmulti-modal representation decorrelation optimization mechanism during the\ntraining process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,\nsurpassing previous state-of-the-art methods without any anchor-based prior\ntrajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u751f\u6210\u8f68\u8ff9\u89c4\u5212\u6a21\u578bTransDiffuser\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u591a\u6a21\u6001\u8868\u793a\u53bb\u76f8\u5173\u4f18\u5316\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u8f68\u8ff9\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002", "motivation": "\u5c06\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u8f6c\u79fb\u5230\u73b0\u4ee3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5df2\u6210\u4e3a\u4e00\u4e2a\u6709\u524d\u9014\u7684\u65b9\u5411\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86TransDiffuser\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u751f\u6210\u8f68\u8ff9\u89c4\u5212\u6a21\u578b\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u3002\u7f16\u7801\u7684\u573a\u666f\u4fe1\u606f\u4f5c\u4e3a\u53bb\u566a\u89e3\u7801\u5668\u7684\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\u3002\u4e3a\u4e86\u5e94\u5bf9\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u8f68\u8ff9\u7684\u6a21\u5f0f\u5d29\u6e83\u56f0\u5883\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u591a\u6a21\u6001\u8868\u793a\u53bb\u76f8\u5173\u4f18\u5316\u673a\u5236\u3002", "result": "TransDiffuser\u5728NAVSIM\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8694.85\u7684PDMS\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u57fa\u4e8e\u951a\u70b9\u7684\u5148\u9a8c\u8f68\u8ff9\u3002", "conclusion": "TransDiffuser\u5728NAVSIM\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8694.85\u7684PDMS\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u57fa\u4e8e\u951a\u70b9\u7684\u5148\u9a8c\u8f68\u8ff9\u3002"}}
{"id": "2505.09323", "pdf": "https://arxiv.org/pdf/2505.09323", "abs": "https://arxiv.org/abs/2505.09323", "authors": ["Pengli Zhu", "Yingji Fu", "Nanguang Chen", "Anqi Qiu"], "title": "Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025", "summary": "This study, we propose a novel Q-space Guided Collaborative Attention\nTranslation Networks (Q-CATN) for multi-shell, high-angular resolution DWI\n(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly\nacquired structural MRI data. Q-CATN employs a collaborative attention\nmechanism to effectively extract complementary information from multiple\nmodalities and dynamically adjust its internal representations based on\nflexible q-space information, eliminating the need for fixed sampling schemes.\nAdditionally, we introduce a range of task-specific constraints to preserve\nanatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic\nrelationships between directional DWI signal distributions and q-space.\nExtensive experiments on the Human Connectome Project (HCP) dataset demonstrate\nthat Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,\nand QGAN, in estimating parameter maps and fiber tracts both quantitatively and\nqualitatively, while preserving fine-grained details. Notably, its ability to\naccommodate flexible q-space sampling highlights its potential as a promising\ntoolkit for clinical and research applications. Our code is available at\nhttps://github.com/Idea89560041/Q-CATN.", "AI": {"tldr": "Q-CATN\u662f\u4e00\u79cd\u65b0\u7684Q\u7a7a\u95f4\u5f15\u5bfc\u534f\u4f5c\u6ce8\u610f\u529b\u7ffb\u8bd1\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u7075\u6d3b\u7684q\u7a7a\u95f4\u91c7\u6837\u4e2d\u5408\u6210\u591a\u58f3\u9ad8\u89d2\u5206\u8fa8\u7387\u6269\u6563\u52a0\u6743\u6210\u50cf\uff08MS-HARDI\uff09\uff0c\u901a\u8fc7\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\u548c\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\uff0c\u63d0\u9ad8\u4e86\u53c2\u6570\u56fe\u548c\u7ea4\u7ef4\u675f\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u56fa\u5b9a\u7684\u91c7\u6837\u65b9\u6848\uff0c\u800cQ-CATN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u901a\u8fc7\u7075\u6d3b\u7684q\u7a7a\u95f4\u91c7\u6837\u6765\u63d0\u9ad8MS-HARDI\u5408\u6210\u7684\u51c6\u786e\u6027\u3002", "method": "Q-CATN\u91c7\u7528\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u591a\u79cd\u6a21\u6001\u4e2d\u6709\u6548\u63d0\u53d6\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u6839\u636e\u7075\u6d3b\u7684q\u7a7a\u95f4\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u5185\u90e8\u8868\u793a\uff0c\u540c\u65f6\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\u4ee5\u4fdd\u6301DWI\u7684\u89e3\u5256\u4fdd\u771f\u5ea6\u3002", "result": "\u5728HCP\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cQ-CATN\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e1D-qDL\u30012D-qDL\u3001MESC-SD\u548cQGAN\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002", "conclusion": "Q-CATN\u5728\u4f30\u8ba1\u53c2\u6570\u56fe\u548c\u7ea4\u7ef4\u675f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u9002\u5e94\u7075\u6d3b\u7684q\u7a7a\u95f4\u91c7\u6837\uff0c\u663e\u793a\u51fa\u5176\u5728\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09393", "pdf": "https://arxiv.org/pdf/2505.09393", "abs": "https://arxiv.org/abs/2505.09393", "authors": ["Huakun Liu", "Hiroki Ota", "Xin Wei", "Yutaro Hirao", "Monica Perusquia-Hernandez", "Hideaki Uchiyama", "Kiyoshi Kiyokawa"], "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy.", "AI": {"tldr": "UMotion\u662f\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u5728\u7ebf\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e3D\u4eba\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u7ed3\u5408\u4e86UWB\u4f20\u611f\u5668\u548cIMUs\uff0c\u901a\u8fc7UKF\u4f18\u5316\u4f30\u8ba1\uff0c\u63d0\u9ad8\u4e86\u59ff\u6001\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u7a00\u758f\u53ef\u7a7f\u6234\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMUs\uff09\u5728\u4f30\u8ba13D\u4eba\u4f53\u8fd0\u52a8\u65f6\u9762\u4e34\u7684\u59ff\u6001\u6b67\u4e49\u3001\u6570\u636e\u6f02\u79fb\u548c\u9002\u5e94\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u5728\u7ebf\u878d\u5408\u6240\u6709\u72b6\u6001\u4f30\u8ba1\u6846\u67b6UMotion\uff0c\u7ed3\u5408\u4e86\u516d\u4e2a\u96c6\u6210\u7684\u4f53\u7a7f\u6234\u8d85\u5bbd\u5e26\uff08UWB\uff09\u8ddd\u79bb\u4f20\u611f\u5668\u548cIMUs\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u7d27\u5bc6\u8026\u5408\u7684\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08UKF\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u5bf9\u9f50IMU\u548cUWB\u6d4b\u91cf\u4e0e\u4e0d\u786e\u5b9a\u7684\u4eba\u4f53\u8fd0\u52a8\u7ea6\u675f\u6765\u4f18\u5316\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUMotion\u5728\u7a33\u5b9a\u4f20\u611f\u5668\u6570\u636e\u548c\u63d0\u9ad8\u59ff\u6001\u51c6\u786e\u6027\u65b9\u9762\u6709\u6548\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "UMotion\u5728\u7a33\u5b9a\u4f20\u611f\u5668\u6570\u636e\u548c\u63d0\u9ad8\u59ff\u6001\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.09334", "pdf": "https://arxiv.org/pdf/2505.09334", "abs": "https://arxiv.org/abs/2505.09334", "authors": ["Sadman Sakib Alif", "Nasim Anzum Promise", "Fiaz Al Abid", "Aniqua Nusrat Zereen"], "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Lung cancer is a leading cause of cancer-related deaths globally, where early\ndetection and accurate diagnosis are critical for improving survival rates.\nWhile deep learning, particularly convolutional neural networks (CNNs), has\nrevolutionized medical image analysis by detecting subtle patterns indicative\nof early-stage lung cancer, its adoption faces challenges. These models are\noften computationally expensive and require significant resources, making them\nunsuitable for resource constrained environments. Additionally, their lack of\ntransparency hinders trust and broader adoption in sensitive fields like\nhealthcare. Knowledge distillation addresses these challenges by transferring\nknowledge from large, complex models (teachers) to smaller, lightweight models\n(students). We propose a knowledge distillation-based approach for lung cancer\ndetection, incorporating explainable AI (XAI) techniques to enhance model\ntransparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,\nand VGG16, are evaluated as teacher models. We developed and trained a\nlightweight student model, Distilled Custom Student Network (DCSNet) using\nResNet50 as the teacher. This approach not only ensures high diagnostic\nperformance in resource-constrained settings but also addresses transparency\nconcerns, facilitating the adoption of AI-driven diagnostic tools in\nhealthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5\u7528\u4e8e\u80ba\u764c\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e86\u53ef\u89e3\u91caAI\u6280\u672f\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3002\u901a\u8fc7\u8bc4\u4f30\u591a\u79cdCNN\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578bDCSNet\uff0c\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8bca\u65ad\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u4e86\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86AI\u9a71\u52a8\u8bca\u65ad\u5de5\u5177\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u80ba\u764c\u662f\u5168\u7403\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u51c6\u786e\u8bca\u65ad\u5bf9\u4e8e\u63d0\u9ad8\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5176\u5e94\u7528\u9762\u4e34\u6311\u6218\uff0c\u5982\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8d44\u6e90\u9700\u6c42\u5927\u4ee5\u53ca\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u7684\u4fe1\u4efb\u548c\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5\u7528\u4e8e\u80ba\u764c\u68c0\u6d4b\uff0c\u5e76\u7ed3\u5408\u4e86\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6280\u672f\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3002\u8bc4\u4f30\u4e86\u516b\u79cdCNN\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u5305\u62ecResNet50\u3001EfficientNetB0\u3001EfficientNetB3\u548cVGG16\uff0c\u5e76\u4f7f\u7528ResNet50\u4f5c\u4e3a\u6559\u5e08\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578bDCSNet\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u786e\u4fdd\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9ad8\u8bca\u65ad\u6027\u80fd\uff0c\u8fd8\u89e3\u51b3\u4e86\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u6709\u52a9\u4e8eAI\u9a71\u52a8\u8bca\u65ad\u5de5\u5177\u5728\u533b\u7597\u9886\u57df\u7684\u91c7\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u786e\u4fdd\u4e86\u9ad8\u8bca\u65ad\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u4e86\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86AI\u9a71\u52a8\u8bca\u65ad\u5de5\u5177\u5728\u533b\u7597\u9886\u57df\u7684\u91c7\u7528\u3002"}}
{"id": "2505.09395", "pdf": "https://arxiv.org/pdf/2505.09395", "abs": "https://arxiv.org/abs/2505.09395", "authors": ["Chen-Yu Liu", "Kuan-Cheng Chen", "Yi-Chien Chen", "Samuel Yen-Chi Chen", "Wei-Hao Huang", "Wei-Jia Huang", "Yen-Jui Chang"], "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u91cf\u5b50\u53c2\u6570\u9002\u5e94\uff08QPA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u3002QPA\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5377\u79ef\u95e8\u63a7\u5faa\u73af\u5355\u5143\u6a21\u578b\u7ed3\u5408\uff0c\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u5bf9\u4e8e\u707e\u5bb3\u51c6\u5907\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5927\u6c14\u52a8\u529b\u5b66\u7684\u590d\u6742\u6027\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8d44\u6e90\u9700\u6c42\uff0c\u4ecd\u7136\u8ba1\u7b97\u4e0a\u5f88\u8017\u65f6\u3002\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86QPA\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u91cf\u5b50\u53c2\u6570\u9002\u5e94\uff08QPA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53f0\u98ce\u9884\u6d4b\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u3002QPA\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5377\u79ef\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08Multi-ConvGRU\uff09\u6a21\u578b\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "QPA\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4f7f\u5f97\u9ad8\u6027\u80fd\u7684\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u66f4\u52a0\u53ef\u884c\u548c\u53ef\u6301\u7eed\u3002", "conclusion": "\u672c\u7814\u7a76\u4ee3\u8868\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5728\u5927\u89c4\u6a21\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u8282\u80fd\u7684\u6c14\u5019\u5efa\u6a21\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0cQPA\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u4f7f\u9ad8\u6027\u80fd\u9884\u6d4b\u66f4\u52a0\u53ef\u884c\u548c\u53ef\u6301\u7eed\u3002"}}
{"id": "2505.09161", "pdf": "https://arxiv.org/pdf/2505.09161", "abs": "https://arxiv.org/abs/2505.09161", "authors": ["Yu Xin", "Peng Liu", "Zhuohang Xie", "Wenhui Mi", "Pengyue Gao", "Hong Jian Zhao", "Jian Lv", "Yanchao Wang", "Yanming Ma"], "title": "Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Even though thermodynamic energy-based crystal structure prediction (CSP) has\nrevolutionized materials discovery, the energy-driven CSP approaches often\nstruggle to identify experimentally realizable metastable materials synthesized\nthrough kinetically controlled pathways, creating a critical gap between\ntheoretical predictions and experimental synthesis. Here, we propose a\nsynthesizability-driven CSP framework that integrates symmetry-guided structure\nderivation with a Wyckoff encode-based machine-learning model, allowing for the\nefficient localization of subspaces likely to yield highly synthesizable\nstructures. Within the identified promising subspaces, a structure-based\nsynthesizability evaluation model, fine-tuned using recently synthesized\nstructures to enhance predictive accuracy, is employed in conjunction with ab\ninitio calculations to systematically identify synthesizable candidates. The\nframework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,\nFe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting\nsynthesizable structures. Notably, 92,310 structures are filtered from the\n554,054 candidates predicted by GNoME, exhibiting great potential for promising\nsynthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =\nTi, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$\ncandidates exhibit high synthesizability, presenting viable candidates for\nexperimental realization and potentially associated with experimentally\nobserved temperature-induced phase transitions. This work establishes a\ndata-driven paradigm for machine-learning-assisted inorganic materials\nsynthesis, highlighting its potential to bridge the gap between computational\npredictions and experimental realization while unlocking new opportunities for\nthe targeted discovery of novel functional materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5408\u6210\u6027\u7684\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5bf9\u79f0\u6027\u5f15\u5bfc\u7684\u7ed3\u6784\u63a8\u5bfc\u4e0e\u57fa\u4e8eWyckoff\u7f16\u7801\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u9ad8\u6548\u5b9a\u4f4d\u53ef\u80fd\u4ea7\u751f\u9ad8\u5ea6\u53ef\u5408\u6210\u7ed3\u6784\u7684\u5b50\u7a7a\u95f4\u3002\u8be5\u6846\u67b6\u6210\u529f\u518d\u73b0\u4e8613\u79cd\u5b9e\u9a8c\u5df2\u77e5\u7684XSe\u7ed3\u6784\uff0c\u5e76\u4ece\u5927\u91cf\u5019\u9009\u7ed3\u6784\u4e2d\u7b5b\u9009\u51fa92,310\u4e2a\u5177\u6709\u9ad8\u53ef\u5408\u6210\u6027\u7684\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e868\u79cd\u70ed\u529b\u5b66\u6709\u5229\u7684Hf-X-O\u7ed3\u6784\uff0c\u5176\u4e2d\u4e09\u79cd\u8868\u73b0\u51fa\u9ad8\u53ef\u5408\u6210\u6027\uff0c\u4e3a\u5b9e\u9a8c\u5b9e\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5019\u9009\u6750\u6599\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u70ed\u529b\u5b66\u80fd\u91cf\u7684\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\uff08CSP\uff09\u5df2\u5f7b\u5e95\u6539\u53d8\u4e86\u6750\u6599\u53d1\u73b0\uff0c\u4f46\u80fd\u91cf\u9a71\u52a8\u7684CSP\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u8bc6\u522b\u901a\u8fc7\u52a8\u529b\u5b66\u63a7\u5236\u8def\u5f84\u5408\u6210\u7684\u5b9e\u9a8c\u4e0a\u53ef\u5b9e\u73b0\u7684\u4e9a\u7a33\u6750\u6599\uff0c\u8fd9\u5728\u7406\u8bba\u9884\u6d4b\u548c\u5b9e\u9a8c\u5408\u6210\u4e4b\u95f4\u9020\u6210\u4e86\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5408\u6210\u6027\u7684\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5bf9\u79f0\u6027\u5f15\u5bfc\u7684\u7ed3\u6784\u63a8\u5bfc\u4e0e\u57fa\u4e8eWyckoff\u7f16\u7801\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u9ad8\u6548\u5b9a\u4f4d\u53ef\u80fd\u4ea7\u751f\u9ad8\u5ea6\u53ef\u5408\u6210\u7ed3\u6784\u7684\u5b50\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528\u6700\u8fd1\u5408\u6210\u7684\u7ed3\u6784\u5bf9\u57fa\u4e8e\u7ed3\u6784\u7684\u53ef\u5408\u6210\u6027\u8bc4\u4f30\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7cfb\u7edf\u5730\u8bc6\u522b\u53ef\u5408\u6210\u5019\u9009\u6750\u6599\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u518d\u73b0\u4e8613\u79cd\u5b9e\u9a8c\u5df2\u77e5\u7684XSe\uff08X = Sc, Ti, Mn, Fe, Ni, Cu, Zn\uff09\u7ed3\u6784\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9884\u6d4b\u53ef\u5408\u6210\u7ed3\u6784\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u4eceGNoME\u9884\u6d4b\u7684554,054\u4e2a\u5019\u9009\u7ed3\u6784\u4e2d\u7b5b\u9009\u51fa92,310\u4e2a\u7ed3\u6784\uff0c\u663e\u793a\u51fa\u5de8\u5927\u7684\u53ef\u5408\u6210\u6027\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e868\u79cd\u70ed\u529b\u5b66\u6709\u5229\u7684Hf-X-O\uff08X = Ti, V, \u548c Mn\uff09\u7ed3\u6784\uff0c\u5176\u4e2d\u4e09\u79cdHfV$_2$O$_7$\u5019\u9009\u7ed3\u6784\u8868\u73b0\u51fa\u9ad8\u53ef\u5408\u6210\u6027\uff0c\u4e3a\u5b9e\u9a8c\u5b9e\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5019\u9009\u6750\u6599\uff0c\u5e76\u53ef\u80fd\u4e0e\u5b9e\u9a8c\u89c2\u5bdf\u5230\u7684\u6e29\u5ea6\u8bf1\u5bfc\u76f8\u53d8\u76f8\u5173\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u8f85\u52a9\u65e0\u673a\u6750\u6599\u5408\u6210\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8ba1\u7b97\u9884\u6d4b\u548c\u5b9e\u9a8c\u5b9e\u73b0\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u9776\u5411\u53d1\u73b0\u65b0\u578b\u529f\u80fd\u6750\u6599\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002"}}
{"id": "2505.09167", "pdf": "https://arxiv.org/pdf/2505.09167", "abs": "https://arxiv.org/abs/2505.09167", "authors": ["Amit Daniely", "Idan Mehalel", "Elchanan Mossel"], "title": "Online Learning of Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study online learning of feedforward neural networks with the sign\nactivation function that implement functions from the unit ball in\n$\\mathbb{R}^d$ to a finite label set $\\{1, \\ldots, Y\\}$.\n  First, we characterize a margin condition that is sufficient and in some\ncases necessary for online learnability of a neural network: Every neuron in\nthe first hidden layer classifies all instances with some margin $\\gamma$\nbounded away from zero. Quantitatively, we prove that for any net, the optimal\nmistake bound is at most approximately $\\mathtt{TS}(d,\\gamma)$, which is the\n$(d,\\gamma)$-totally-separable-packing number, a more restricted variation of\nthe standard $(d,\\gamma)$-packing number. We complement this result by\nconstructing a net on which any learner makes $\\mathtt{TS}(d,\\gamma)$ many\nmistakes. We also give a quantitative lower bound of approximately\n$\\mathtt{TS}(d,\\gamma) \\geq \\max\\{1/(\\gamma \\sqrt{d})^d, d\\}$ when $\\gamma \\geq\n1/2$, implying that for some nets and input sequences every learner will err\nfor $\\exp(d)$ many times, and that a dimension-free mistake bound is almost\nalways impossible.\n  To remedy this inevitable dependence on $d$, it is natural to seek additional\nnatural restrictions to be placed on the network, so that the dependence on $d$\nis removed. We study two such restrictions. The first is the multi-index model,\nin which the function computed by the net depends only on $k \\ll d$ orthonormal\ndirections. We prove a mistake bound of approximately $(1.5/\\gamma)^{k + 2}$ in\nthis model. The second is the extended margin assumption. In this setting, we\nassume that all neurons (in all layers) in the network classify every ingoing\ninput from previous layer with margin $\\gamma$ bounded away from zero. In this\nmodel, we prove a mistake bound of approximately $(\\log Y)/ \\gamma^{O(L)}$,\nwhere L is the depth of the network.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u7b26\u53f7\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5728\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u901a\u8fc7\u8fb9\u754c\u6761\u4ef6\u548c\u5305\u88c5\u6570\u5206\u6790\uff0c\u5f97\u51fa\u4e0d\u540c\u9650\u5236\u6761\u4ef6\u4e0b\u7684\u9519\u8bef\u754c\u9650\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u6dfb\u52a0\u989d\u5916\u9650\u5236\u6765\u51cf\u5c11\u7ef4\u5ea6\u4f9d\u8d56\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u7684\u8fb9\u754c\u6761\u4ef6\u548c\u5305\u88c5\u6570\uff0c\u7ed3\u5408\u591a\u6307\u6807\u6a21\u578b\u548c\u6269\u5c55\u8fb9\u754c\u5047\u8bbe\uff0c\u63a8\u5bfc\u51fa\u4e0d\u540c\u7684\u9519\u8bef\u754c\u9650\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u4f18\u9519\u8bef\u754c\u9650\u4e0e(d, \u03b3)-\u5b8c\u5168\u53ef\u5206\u79bb\u5305\u88c5\u6570\u6709\u5173\uff0c\u5e76\u5728\u591a\u6307\u6807\u6a21\u578b\u548c\u6269\u5c55\u8fb9\u754c\u5047\u8bbe\u4e0b\u5f97\u5230\u4e86\u4e0d\u540c\u7684\u9519\u8bef\u754c\u9650\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u7b26\u53f7\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5728\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u53ef\u5b66\u4e60\u6027\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u8db3\u591f\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5fc5\u8981\u7684\u8fb9\u754c\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u6700\u4f18\u9519\u8bef\u754c\u9650\u4e0e(d, \u03b3)-\u5b8c\u5168\u53ef\u5206\u79bb\u5305\u88c5\u6570\u6709\u5173\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u4e24\u79cd\u9650\u5236\u6761\u4ef6\uff1a\u591a\u6307\u6807\u6a21\u578b\u548c\u6269\u5c55\u8fb9\u754c\u5047\u8bbe\uff0c\u5206\u522b\u7ed9\u51fa\u4e86\u76f8\u5e94\u7684\u9519\u8bef\u754c\u9650\u3002"}}
{"id": "2505.09356", "pdf": "https://arxiv.org/pdf/2505.09356", "abs": "https://arxiv.org/abs/2505.09356", "authors": ["Srinivas Ravuri", "Yuan Xu", "Martin Ludwig Zehetner", "Ketan Motlag", "Sahin Albayrak"], "title": "APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages with 6 figures", "summary": "Precise initialization plays a critical role in the performance of\nlocalization algorithms, especially in the context of robotics, autonomous\ndriving, and computer vision. Poor localization accuracy is often a consequence\nof inaccurate initial poses, particularly noticeable in GNSS-denied\nenvironments where GPS signals are primarily relied upon for initialization.\nRecent advances in leveraging deep neural networks for pose regression have led\nto significant improvements in both accuracy and robustness, especially in\nestimating complex spatial relationships and orientations. In this paper, we\nintroduce APR-Transformer, a model architecture inspired by state-of-the-art\nmethods, which predicts absolute pose (3D position and 3D orientation) using\neither image or LiDAR data. We demonstrate that our proposed method achieves\nstate-of-the-art performance on established benchmark datasets such as the\nRadar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our\nexperiments to include our custom complex APR-BeIntelli dataset. Additionally,\nwe validate the reliability of our approach in GNSS-denied environments by\ndeploying the model in real-time on an autonomous test vehicle. This showcases\nthe practical feasibility and effectiveness of our approach. The source code is\navailable at:https://github.com/GT-ARC/APR-Transformer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86APR-Transformer\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u56fe\u50cf\u6216LiDAR\u6570\u636e\u9884\u6d4b\u7edd\u5bf9\u59ff\u6001\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728GNSS-denied\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u7cbe\u786e\u7684\u521d\u59cb\u5316\u5728\u5b9a\u4f4d\u7b97\u6cd5\u7684\u6027\u80fd\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u80cc\u666f\u4e0b\u3002\u5728GNSS-denied\u73af\u5883\u4e2d\uff0cGPS\u4fe1\u53f7\u4e3b\u8981\u7528\u4e8e\u521d\u59cb\u5316\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u51c6\u786e\u7684\u521d\u59cb\u59ff\u6001\u4ee5\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86APR-Transformer\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u6700\u65b0\u65b9\u6cd5\u542f\u53d1\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5229\u7528\u56fe\u50cf\u6216LiDAR\u6570\u636e\u9884\u6d4b\u7edd\u5bf9\u59ff\u6001\uff083D\u4f4d\u7f6e\u548c3D\u65b9\u5411\uff09\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684APR-Transformer\u6a21\u578b\u5728Radar Oxford Robot-Car\u548cDeepLoc\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u81ea\u5b9a\u4e49\u7684APR-BeIntelli\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6269\u5c55\u5b9e\u9a8c\u3002\u6b64\u5916\uff0c\u8fd8\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684APR-Transformer\u6a21\u578b\u5728GNSS-denied\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09229", "pdf": "https://arxiv.org/pdf/2505.09229", "abs": "https://arxiv.org/abs/2505.09229", "authors": ["Brian Britos", "Mathias Bourel"], "title": "Optimal Transport-Based Domain Adaptation for Rotated Linear Regression", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "Optimal Transport (OT) has proven effective for domain adaptation (DA) by\naligning distributions across domains with differing statistical properties.\nBuilding on the approach of Courty et al. (2016), who mapped source data to the\ntarget domain for improved model transfer, we focus on a supervised DA problem\ninvolving linear regression models under rotational shifts. This ongoing work\nconsiders cases where source and target domains are related by a\nrotation-common in applications like sensor calibration or image orientation.\nWe show that in $\\mathbb{R}^2$ , when using a p-norm cost with $p $\\ge$ 2$, the\noptimal transport map recovers the underlying rotation. Based on this, we\npropose an algorithm that combines K-means clustering, OT, and singular value\ndecomposition (SVD) to estimate the rotation angle and adapt the regression\nmodel. This method is particularly effective when the target domain is sparsely\nsampled, leveraging abundant source data for improved generalization. Our\ncontributions offer both theoretical and practical insights into OT-based model\nadaptation under geometric transformations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.09521", "pdf": "https://arxiv.org/pdf/2505.09521", "abs": "https://arxiv.org/abs/2505.09521", "authors": ["Dongyi He", "Shiyang Li", "Bin Jiang", "He Yan"], "title": "Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-resolution functional magnetic resonance imaging (fMRI) is essential for\nmapping human brain activity; however, it remains costly and logistically\nchallenging. If comparable volumes could be generated directly from widely\navailable scalp electroencephalography (EEG), advanced neuroimaging would\nbecome significantly more accessible. Existing EEG-to-fMRI generators rely on\nplain CNNs that fail to capture cross-channel time-frequency cues or on heavy\ntransformer/GAN decoders that strain memory and stability. We propose\nSpec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts\nthese issues via a Multi-directional Time-Frequency Convolutional Attention\nEncoder, stacking temporal, spectral and joint convolutions with\nself-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space\nblocks enable efficient long-range spatial modelling. Trained end-to-end with a\nhybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on\nthree public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball\nand 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%\nrespectively over previous best SSIM scores. Furthermore, it achieves\ncompetitive PSNR scores, particularly excelling on the CN-EPFL dataset with a\n4.6% improvement over the previous best PSNR, thus striking a better balance in\nreconstruction quality. The proposed model is lightweight and efficient, making\nit suitable for real-time applications in clinical and research settings. The\ncode is available at https://github.com/hdy6438/Spec2VolCAMU-Net.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpec2VolCAMU-Net\u7684\u8f7b\u91cf\u7ea7\u9891\u8c31\u56fe\u5230\u4f53\u79ef\u751f\u6210\u5668\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u65f6\u95f4-\u9891\u7387\u5377\u79ef\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548cVision-Mamba U-Net\u89e3\u7801\u5668\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u95ee\u9898\u3002\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728PSNR\u5f97\u5206\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u529f\u80fd\u6027\u78c1\u5171\u632f\u6210\u50cf\uff08fMRI\uff09\u5728\u6620\u5c04\u4eba\u7c7b\u5927\u8111\u6d3b\u52a8\u65b9\u9762\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u4e14\u7269\u6d41\u56f0\u96be\u3002\u5982\u679c\u80fd\u591f\u76f4\u63a5\u4ece\u5e7f\u6cdb\u53ef\u7528\u7684\u5934\u76ae\u8111\u7535\u56fe\uff08EEG\uff09\u751f\u6210\u53ef\u6bd4\u8f83\u7684\u4f53\u79ef\uff0c\u5148\u8fdb\u7684\u795e\u7ecf\u6210\u50cf\u5c06\u53d8\u5f97\u66f4\u52a0\u53ef\u53ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpec2VolCAMU-Net\u7684\u8f7b\u91cf\u7ea7\u9891\u8c31\u56fe\u5230\u4f53\u79ef\u751f\u6210\u5668\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u65f6\u95f4-\u9891\u7387\u5377\u79ef\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548cVision-Mamba U-Net\u89e3\u7801\u5668\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4fdd\u771f\u5ea6\uff0c\u5206\u522b\u5728NODDI\u3001Oddball\u548cCN-EPFL\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e860.693\u30010.725\u548c0.788\u7684SSIM\u503c\uff0c\u5206\u522b\u6bd4\u4e4b\u524d\u6700\u597d\u7684SSIM\u5206\u6570\u63d0\u9ad8\u4e8614.5%\u300114.9%\u548c16.9%\u3002\u6b64\u5916\uff0c\u5728CN-EPFL\u6570\u636e\u96c6\u4e0aPSNR\u5f97\u5206\u67094.6%\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u6a21\u578b\u8f7b\u91cf\u4e14\u9ad8\u6548\uff0c\u9002\u5408\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2505.09438", "pdf": "https://arxiv.org/pdf/2505.09438", "abs": "https://arxiv.org/abs/2505.09438", "authors": ["Paul Tschisgale", "Holger Maus", "Fabian Kieser", "Ben Kroehs", "Stefan Petersen", "Peter Wulff"], "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment", "categories": ["physics.ed-ph", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86GPT-4o\u548co1-preview\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u5fb7\u56fd\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u53c2\u4e0e\u8005\u7684\u89e3\u9898\u8868\u73b0\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5965\u6797\u5339\u514b\u7269\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u5148\u8fdb\u80fd\u529b\uff0c\u5e73\u5747\u4f18\u4e8e\u4eba\u7c7b\u53c2\u4e0e\u8005\u3002\u7814\u7a76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u7269\u7406\u6559\u80b2\u4e2d\u8bc4\u4f30\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u7269\u7406\u6559\u80b2\u4e2d\uff0c\u95ee\u9898\u89e3\u51b3\u5728\u6559\u5b66\u548c\u8bc4\u4f30\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u56e0\u6b64\u4e86\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u7279\u5b9a\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u79cd\u7406\u89e3\u5bf9\u4e8e\u5236\u5b9a\u8d1f\u8d23\u4efb\u4e14\u7b26\u5408\u6559\u5b66\u539f\u5219\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6574\u5408\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u63d0\u793a\u6280\u672f\uff09\u548c\u4f18\u5316\u63a8\u7406\u7684\u6a21\u578b\uff08o1-preview\uff09\u4e0e\u5fb7\u56fd\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u53c2\u4e0e\u8005\u7684\u89e3\u9898\u8868\u73b0\uff0c\u57fa\u4e8e\u4e00\u7ec4\u5b9a\u4e49\u660e\u786e\u7684\u5965\u6797\u5339\u514b\u95ee\u9898\u3002\u9664\u4e86\u8bc4\u4f30\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u6b63\u786e\u6027\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u7279\u70b9\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u79cd\u6d4b\u8bd5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u548co1-preview\uff09\u5728\u5965\u6797\u5339\u514b\u7c7b\u578b\u7684\u7269\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u5148\u8fdb\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u53c2\u4e0e\u8005\u3002\u63d0\u793a\u6280\u672f\u5bf9GPT-4o\u7684\u8868\u73b0\u5f71\u54cd\u4e0d\u5927\uff0c\u800co1-preview\u51e0\u4e4e\u4e00\u76f4\u4f18\u4e8eGPT-4o\u548c\u4eba\u7c7b\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cGPT-4o\u548co1-preview\u8fd9\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5965\u6797\u5339\u514b\u7269\u7406\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u51fa\u5148\u8fdb\u7684\u80fd\u529b\uff0c\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u53c2\u4e0e\u8005\u3002\u7814\u7a76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u7269\u7406\u6559\u80b2\u4e2d\u603b\u7ed3\u6027\u8bc4\u4f30\u548c\u5f62\u6210\u6027\u8bc4\u4f30\u8bbe\u8ba1\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u5982\u4f55\u7ef4\u62a4\u8bc4\u4f30\u7684\u5b8c\u6574\u6027\u5e76\u652f\u6301\u5b66\u751f\u6279\u5224\u6027\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2505.09565", "pdf": "https://arxiv.org/pdf/2505.09565", "abs": "https://arxiv.org/abs/2505.09565", "authors": ["Maik Dannecker", "Thomas Sanchez", "Meritxell Bach Cuadra", "\u00d6zg\u00fcn Turgut", "Anthony N. Price", "Lucilio Cordero-Grande", "Vanessa Kyriakopoulou", "Joseph V. Hajnal", "Daniel Rueckert"], "title": "Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 6 figures", "summary": "High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SVR\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e25\u91cd\u8fd0\u52a8\u548c\u56fe\u50cf\u4f2a\u5f71\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684MRI\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u56fe\u50cf\u4f2a\u5f71\u548c\u4e25\u91cd\u7684\u53d7\u8bd5\u8005\u8fd0\u52a8\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u6216\u8005\u9700\u8981\u5207\u7247\u9884\u5bf9\u9f50\u4ee5\u83b7\u5f97\u4ee4\u4eba\u6ee1\u610f\u7684\u91cd\u5efa\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SVR\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8fdb\u884c\u8fd0\u52a8\u6821\u6b63\u3001\u5f02\u5e38\u503c\u5904\u7406\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5728\u6a21\u62df\u6216\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b8c\u5168\u81ea\u76d1\u7763\u5143\u5b66\u4e60\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\u7684\u521d\u59cb\u5316\u3002", "result": "\u5728\u5305\u62ec\u6765\u81ea\u4e0d\u540c\u4e2d\u5fc3\u7684480\u591a\u4e2a\u6a21\u62df\u548c\u4e34\u5e8aMRI\u8111\u6570\u636e\u91cd\u5efa\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u4e25\u91cd\u53d7\u8bd5\u8005\u8fd0\u52a8\u548c\u56fe\u50cf\u4f2a\u5f71\u60c5\u51b5\u4e0b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e25\u91cd\u8fd0\u52a8\u548c\u56fe\u50cf\u4f2a\u5f71\u7684\u60c5\u51b5\u4e0b\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u91cd\u5efa\u8d28\u91cf\u6709\u6240\u63d0\u9ad8\uff0c\u91cd\u5efa\u65f6\u95f4\u51cf\u5c11\u4e86\u6700\u591a50%\u3002"}}
{"id": "2505.09456", "pdf": "https://arxiv.org/pdf/2505.09456", "abs": "https://arxiv.org/abs/2505.09456", "authors": ["Josep Lumbreras", "Ruo Cheng Huang", "Yanglin Hu", "Mile Gu", "Marco Tomamichel"], "title": "Quantum state-agnostic work extraction (almost) without dissipation", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "5 pages+14 pages, 2 figures", "summary": "We investigate work extraction protocols designed to transfer the maximum\npossible energy to a battery using sequential access to $N$ copies of an\nunknown pure qubit state. The core challenge is designing interactions to\noptimally balance two competing goals: charging of the battery optimally using\nthe qubit in hand, and acquiring more information by qubit to improve energy\nharvesting in subsequent rounds. Here, we leverage exploration-exploitation\ntrade-off in reinforcement learning to develop adaptive strategies achieving\nenergy dissipation that scales only poly-logarithmically in $N$. This\nrepresents an exponential improvement over current protocols based on full\nstate tomography.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u7eaf\u91cf\u5b50\u6001\u526f\u672c\u4e2d\u63d0\u53d6\u6700\u5927\u53ef\u80fd\u7684\u80fd\u91cf\uff0c\u5e76\u5b9e\u73b0\u4e86\u80fd\u91cf\u8017\u6563\u4ec5\u968fN\u7684\u591a\u9879\u5f0f\u5bf9\u6570\u589e\u957f\uff0c\u8fd9\u6bd4\u5f53\u524d\u57fa\u4e8e\u5b8c\u6574\u72b6\u6001\u5c42\u6790\u7684\u534f\u8bae\u6709\u6307\u6570\u7ea7\u7684\u6539\u8fdb\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u5de5\u4f5c\u63d0\u53d6\u534f\u8bae\uff0c\u4ee5\u4f7f\u7528\u987a\u5e8f\u8bbf\u95eeN\u4e2a\u672a\u77e5\u7eaf\u91cf\u5b50\u6001\u526f\u672c\u7684\u6700\u5927\u53ef\u80fd\u80fd\u91cf\u4f20\u9012\u5230\u7535\u6c60\u4e2d\uff0c\u5e76\u4f18\u5316\u7535\u6c60\u5145\u7535\u548c\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "method": "\u672c\u6587\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u6743\u8861\u6765\u5f00\u53d1\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u80fd\u91cf\u8017\u6563\u4ec5\u968fN\u7684\u591a\u9879\u5f0f\u5bf9\u6570\u589e\u957f\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u7b56\u7565\u5b9e\u73b0\u4e86\u80fd\u91cf\u8017\u6563\u4ec5\u968fN\u7684\u591a\u9879\u5f0f\u5bf9\u6570\u589e\u957f\uff0c\u8fd9\u6bd4\u5f53\u524d\u57fa\u4e8e\u5b8c\u6574\u72b6\u6001\u5c42\u6790\u7684\u534f\u8bae\u6709\u6307\u6570\u7ea7\u7684\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0e\u5229\u7528\u6743\u8861\u7684\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u80fd\u91cf\u8017\u6563\u4ec5\u968fN\u7684\u591a\u9879\u5f0f\u5bf9\u6570\u589e\u957f\uff0c\u8fd9\u6bd4\u5f53\u524d\u57fa\u4e8e\u5b8c\u6574\u72b6\u6001\u5c42\u6790\u7684\u534f\u8bae\u6709\u6307\u6570\u7ea7\u7684\u6539\u8fdb\u3002"}}
{"id": "2505.09266", "pdf": "https://arxiv.org/pdf/2505.09266", "abs": "https://arxiv.org/abs/2505.09266", "authors": ["Lirand\u00eb Pira", "Airin Antony", "Nayanthara Prathap", "Daniel Peace", "Jacquiline Romero"], "title": "Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques", "categories": ["physics.optics", "cs.LG", "quant-ph"], "comment": null, "summary": "Photonic chip design has seen significant advancements with the adoption of\ninverse design methodologies, offering flexibility and efficiency in optimizing\ndevice performance. However, the black-box nature of the optimization\napproaches, such as those used in inverse design in order to minimize a loss\nfunction or maximize coupling efficiency, poses challenges in understanding the\noutputs. This challenge is prevalent in machine learning-based optimization\nmethods, which can suffer from the same lack of transparency. To this end,\ninterpretability techniques address the opacity of optimization models. In this\nwork, we apply interpretability techniques from machine learning, with the aim\nof gaining understanding of inverse design optimization used in designing\nphotonic components, specifically two-mode multiplexers. We base our\nmethodology on the widespread interpretability technique known as local\ninterpretable model-agnostic explanations, or LIME. As a result, LIME-informed\ninsights point us to more effective initial conditions, directly improving\ndevice performance. This demonstrates that interpretability methods can do more\nthan explain models -- they can actively guide and enhance the inverse-designed\nphotonic components. Our results demonstrate the ability of interpretable\ntechniques to reveal underlying patterns in the inverse design process, leading\nto the development of better-performing components.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08\u5982LIME\uff09\u6765\u7406\u89e3\u5e76\u6539\u8fdb\u9006\u5411\u8bbe\u8ba1\u7684\u5149\u5b50\u7ec4\u4ef6\uff0c\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u6280\u672f\u4e0d\u4ec5\u80fd\u89e3\u91ca\u6a21\u578b\uff0c\u8fd8\u80fd\u6307\u5bfc\u8bbe\u8ba1\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5149\u5b50\u82af\u7247\u8bbe\u8ba1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u9ed1\u7bb1\u6027\u8d28\u4f7f\u5f97\u96be\u4ee5\u7406\u89e3\u8f93\u51fa\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6765\u63d0\u9ad8\u900f\u660e\u5ea6\u3002", "method": "\u6211\u4eec\u5e94\u7528\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u7279\u522b\u662f\u5c40\u90e8\u53ef\u89e3\u91ca\u6a21\u578b\u65e0\u5173\u89e3\u91ca\uff08LIME\uff09\uff0c\u4ee5\u7406\u89e3\u9006\u5411\u8bbe\u8ba1\u4f18\u5316\u5728\u8bbe\u8ba1\u53cc\u6a21\u590d\u7528\u5668\u4e2d\u7684\u4f5c\u7528\u3002", "result": "LIME\u63d0\u4f9b\u7684\u89c1\u89e3\u5e2e\u52a9\u6211\u4eec\u627e\u5230\u4e86\u66f4\u6709\u6548\u7684\u521d\u59cb\u6761\u4ef6\uff0c\u4ece\u800c\u76f4\u63a5\u63d0\u9ad8\u4e86\u8bbe\u5907\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u4e0d\u4ec5\u53ef\u4ee5\u89e3\u91ca\u6a21\u578b\uff0c\u8fd8\u53ef\u4ee5\u4e3b\u52a8\u6307\u5bfc\u548c\u589e\u5f3a\u9006\u5411\u8bbe\u8ba1\u7684\u5149\u5b50\u7ec4\u4ef6\u3002"}}
{"id": "2505.09477", "pdf": "https://arxiv.org/pdf/2505.09477", "abs": "https://arxiv.org/abs/2505.09477", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Jason Hughes", "Varun Murali", "M. Ani Hsieh", "George J. Pappas", "Camillo J. Taylor", "Vijay Kumar"], "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2025", "summary": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5c06\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u673a\u5668\u4eba\u4e2d\u4ee5\u4f7f\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86SPINE\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6210\u529f\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u7684FM\u542f\u7528\u7684\u673a\u5668\u4eba\u4e3b\u8981\u5728\u5c01\u95ed\u4e16\u754c\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u800c\u672c\u6587\u65e8\u5728\u5c06FM\u542f\u7528\u7684\u673a\u5668\u4eba\u90e8\u7f72\u5230\u73b0\u573a\uff0c\u5176\u4e2d\u4efb\u52a1\u901a\u5e38\u9700\u8981\u673a\u5668\u4eba\u5728\u5927\u89c4\u6a21\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "method": "\u672c\u6587\u8ba8\u8bba\u4e86SPINE\uff0c\u6211\u4eec\u7684LLM\u542f\u7528\u7684\u81ea\u4e3b\u6846\u67b6\u5728\u5b9e\u9645\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u6700\u65b0\u90e8\u7f72\uff0c\u5e76\u901a\u8fc7\u521d\u6b65\u7684\u6a21\u578b\u84b8\u998f\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u4f7f\u7528\u8bbe\u5907\u4e0a\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u9a71\u52a8\u65e0\u4eba\u673a\u89c4\u5212\u5668\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u7b2c\u4e00\u4e2a\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21LLM\u542f\u7528\u673a\u5668\u4eba\u89c4\u5212\u7684\u6f14\u793a\uff0c\u5e76\u5c55\u793a\u4e86\u7b2c\u4e00\u4e2a\u4f7f\u7528\u8bbe\u5907\u4e0a\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u9a71\u52a8\u65e0\u4eba\u673a\u89c4\u5212\u5668\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u4e00\u4e9b\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2505.09304", "pdf": "https://arxiv.org/pdf/2505.09304", "abs": "https://arxiv.org/abs/2505.09304", "authors": ["Luciano Sebastian Martinez-Rau", "Quynh Nguyen Phuong Vu", "Yuxuan Zhang", "Bengt Oelmann", "Sebastian Bader"], "title": "Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Preprint submitted to the IEEE 11th World Forum on Internet of Things", "summary": "Keyword spotting (KWS) is a key component of smart devices, enabling\nefficient and intuitive audio interaction. However, standard KWS systems\ndeployed on embedded devices often suffer performance degradation under\nreal-world operating conditions. Resilient KWS systems address this issue by\nenabling dynamic adaptation, with applications such as adding or replacing\nkeywords, adjusting to specific users, and improving noise robustness. However,\ndeploying resilient, standalone KWS systems with low latency on\nresource-constrained devices remains challenging due to limited memory and\ncomputational resources. This study proposes a low computational approach for\ncontinuous noise adaptation of pretrained neural networks used for KWS\nclassification, requiring only 1-shot learning and one epoch. The proposed\nmethod was assessed using two pretrained models and three real-world noise\nsources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted\nmodels consistently outperformed the pretrained models across all scenarios,\nespecially at SNR $\\leq$ 18 dB, achieving accuracy improvements of 4.9% to\n46.0%. These results highlight the efficacy of the proposed methodology while\nbeing lightweight enough for deployment on resource-constrained devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u8ba1\u7b97\u91cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u566a\u58f0\u9002\u5e94\u7528\u4e8eKWS\u5206\u7c7b\u7684\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u89811-shot\u5b66\u4e60\u548c\u4e00\u4e2aepoch\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9002\u5e94\u540e\u7684\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e2d\u90fd\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728SNR \u2264 18 dB\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e864.9%\u523046.0%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5176\u8f7b\u91cf\u7ea7\u7279\u6027\u4f7f\u5176\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "motivation": "\u6807\u51c6\u7684KWS\u7cfb\u7edf\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\uff0c\u5e38\u5e38\u5728\u771f\u5b9e\u64cd\u4f5c\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u7684\u9c81\u68d2KWS\u7cfb\u7edf\uff0c\u4f8b\u5982\u6dfb\u52a0\u6216\u66ff\u6362\u5173\u952e\u8bcd\u3001\u9002\u5e94\u7279\u5b9a\u7528\u6237\u548c\u63d0\u9ad8\u566a\u58f0\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u8fd9\u79cd\u9c81\u68d2\u7684\u72ec\u7acbKWS\u7cfb\u7edf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u8ba1\u7b97\u91cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u566a\u58f0\u9002\u5e94\u7528\u4e8eKWS\u5206\u7c7b\u7684\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u89811-shot\u5b66\u4e60\u548c\u4e00\u4e2aepoch\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4e09\u4e2a\u771f\u5b9e\u566a\u58f0\u6e90\u5728\u4fe1\u566a\u6bd4\uff08SNR\uff09\u8303\u56f4\u4ece24\u5230-3 dB\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u9002\u5e94\u540e\u7684\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e2d\u90fd\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728SNR \u2264 18 dB\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e864.9%\u523046.0%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f4e\u8ba1\u7b97\u91cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u566a\u58f0\u9002\u5e94\u7528\u4e8eKWS\u5206\u7c7b\u7684\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u89811-shot\u5b66\u4e60\u548c\u4e00\u4e2aepoch\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9002\u5e94\u540e\u7684\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e2d\u90fd\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728SNR \u2264 18 dB\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e864.9%\u523046.0%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5176\u8f7b\u91cf\u7ea7\u7279\u6027\u4f7f\u5176\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2505.09313", "pdf": "https://arxiv.org/pdf/2505.09313", "abs": "https://arxiv.org/abs/2505.09313", "authors": ["Qiangqiang Liu", "Qian Huang", "Frank Fan", "Haishan Wu", "Xueyan Tang"], "title": "Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach", "categories": ["cs.CR", "cs.LG"], "comment": "IEEE International Conference on Blockchain and Cryptocurrency(Proc.\n  IEEE ICBC 2025)", "summary": "Sybil attacks pose a significant security threat to blockchain ecosystems,\nparticularly in token airdrop events. This paper proposes a novel sybil address\nidentification method based on subgraph feature extraction lightGBM. The method\nfirst constructs a two-layer deep transaction subgraph for each address, then\nextracts key event operation features according to the lifecycle of sybil\naddresses, including the time of first transaction, first gas acquisition,\nparticipation in airdrop activities, and last transaction. These temporal\nfeatures effectively capture the consistency of sybil address behavior\noperations. Additionally, the method extracts amount and network structure\nfeatures, comprehensively describing address behavior patterns and network\ntopology through feature propagation and fusion. Experiments conducted on a\ndataset containing 193,701 addresses (including 23,240 sybil addresses) show\nthat this method outperforms existing approaches in terms of precision, recall,\nF1 score, and AUC, with all metrics exceeding 0.9. The methods and results of\nthis study can be further applied to broader blockchain security areas such as\ntransaction manipulation identification and token liquidity risk assessment,\ncontributing to the construction of a more secure and fair blockchain\necosystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u56fe\u7279\u5f81\u63d0\u53d6\u7684\u8f7b\u91cf\u7ea7GBM\uff08lightGBM\uff09\u7684Sybil\u5730\u5740\u8bc6\u522b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6240\u6709\u6307\u6807\u5747\u8d85\u8fc70.9\u3002", "motivation": "Sybil\u653b\u51fb\u5bf9\u533a\u5757\u94fe\u751f\u6001\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u4ee3\u5e01\u7a7a\u6295\u4e8b\u4ef6\u4e2d\u6784\u6210\u91cd\u5927\u5b89\u5168\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684Sybil\u5730\u5740\u8bc6\u522b\u65b9\u6cd5\u6765\u63d0\u9ad8\u533a\u5757\u94fe\u7684\u5b89\u5168\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u56fe\u7279\u5f81\u63d0\u53d6\u7684\u8f7b\u91cf\u7ea7GBM\uff08lightGBM\uff09\u7684Sybil\u5730\u5740\u8bc6\u522b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u4e3a\u6bcf\u4e2a\u5730\u5740\u6784\u5efa\u4e00\u4e2a\u4e24\u5c42\u7684\u6df1\u5ea6\u4ea4\u6613\u5b50\u56fe\uff0c\u7136\u540e\u6839\u636eSybil\u5730\u5740\u7684\u751f\u547d\u5468\u671f\u63d0\u53d6\u5173\u952e\u4e8b\u4ef6\u64cd\u4f5c\u7279\u5f81\uff0c\u5305\u62ec\u9996\u6b21\u4ea4\u6613\u65f6\u95f4\u3001\u9996\u6b21\u83b7\u53d6Gas\u65f6\u95f4\u3001\u53c2\u4e0e\u7a7a\u6295\u6d3b\u52a8\u65f6\u95f4\u548c\u6700\u540e\u4e00\u6b21\u4ea4\u6613\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u63d0\u53d6\u4e86\u91d1\u989d\u548c\u7f51\u7edc\u7ed3\u6784\u7279\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u4f20\u64ad\u548c\u878d\u5408\u5168\u9762\u63cf\u8ff0\u5730\u5740\u884c\u4e3a\u6a21\u5f0f\u548c\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5728\u5305\u542b193,701\u4e2a\u5730\u5740\uff08\u5176\u4e2d23,240\u4e2a\u662fSybil\u5730\u5740\uff09\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6240\u6709\u6307\u6807\u5747\u8d85\u8fc70.9\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u533a\u5757\u94fe\u5b89\u5168\u9886\u57df\uff0c\u5982\u4ea4\u6613\u64cd\u63a7\u8bc6\u522b\u548c\u4ee3\u5e01\u6d41\u52a8\u6027\u98ce\u9669\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u52a0\u5b89\u5168\u548c\u516c\u5e73\u7684\u533a\u5757\u94fe\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2505.09558", "pdf": "https://arxiv.org/pdf/2505.09558", "abs": "https://arxiv.org/abs/2505.09558", "authors": ["Shengpeng Ji", "Tianle Liang", "Yangzhuo Li", "Jialong Zuo", "Minghui Fang", "Jinzheng He", "Yifu Chen", "Zhengqing Liu", "Ziyue Jiang", "Xize Cheng", "Siqi Zheng", "Jin Xu", "Junyang Lin", "Zhou Zhao"], "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered\nsignificant attention in the speech domain. However, the evaluation of spoken\ndialogue models' conversational performance has largely been overlooked. This\nis primarily due to the intelligent chatbots convey a wealth of non-textual\ninformation which cannot be easily measured using text-based language models\nlike ChatGPT. To address this gap, we propose WavReward, a reward feedback\nmodel based on audio language models that can evaluate both the IQ and EQ of\nspoken dialogue systems with speech input. Specifically, 1) based on audio\nlanguage models, WavReward incorporates the deep reasoning process and the\nnonlinear reward mechanism for post-training. By utilizing multi-sample\nfeedback via the reinforcement learning algorithm, we construct a specialized\nevaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a\npreference dataset used to train WavReward. ChatReward-30K includes both\ncomprehension and generation aspects of spoken dialogue models. These scenarios\nspan various tasks, such as text-based chats, nine acoustic attributes of\ninstruction chats, and implicit chats. WavReward outperforms previous\nstate-of-the-art evaluation models across multiple spoken dialogue scenarios,\nachieving a substantial improvement about Qwen2.5-Omni in objective accuracy\nfrom 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a\nmargin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each\ncomponent of WavReward. All data and code will be publicly at\nhttps://github.com/jishengpeng/WavReward after the paper is accepted.", "AI": {"tldr": "WavReward is a reward feedback model based on audio language models that evaluates both the IQ and EQ of spoken dialogue systems. It outperforms previous state-of-the-art evaluation models and shows significant improvements in both objective and subjective evaluations.", "motivation": "The evaluation of spoken dialogue models' conversational performance has been overlooked due to the non-textual information conveyed by intelligent chatbots, which cannot be easily measured using text-based language models like ChatGPT.", "method": "WavReward is a reward feedback model based on audio language models that evaluates both the IQ and EQ of spoken dialogue systems with speech input. It incorporates deep reasoning processes and nonlinear reward mechanisms for post-training, utilizing multi-sample feedback via reinforcement learning algorithms. Additionally, ChatReward-30K, a preference dataset, is introduced to train WavReward.", "result": "WavReward achieves a significant improvement in objective accuracy from 55.1% to 91.5% compared to Qwen2.5-Omni and leads in subjective A/B testing by a margin of 83%. The comprehensive ablation studies confirm the necessity of each component of WavReward.", "conclusion": "WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement in objective accuracy and subjective A/B testing. Comprehensive ablation studies confirm the necessity of each component of WavReward."}}
{"id": "2505.09561", "pdf": "https://arxiv.org/pdf/2505.09561", "abs": "https://arxiv.org/abs/2505.09561", "authors": ["Marcel Torne", "Andy Tang", "Yuejiang Liu", "Chelsea Finn"], "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Videos are available at https://long-context-dp.github.io", "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8fc7\u53bb\u52a8\u4f5c\u6807\u8bb0\u6765\u6539\u8fdb\u957f\u4e0a\u4e0b\u6587\u7b56\u7565\u7684\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5b66\u4e60\u6709\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u7b56\u7565\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u8bad\u7ec3\u53d8\u5f97\u6602\u8d35\u4e14\u7b56\u7565\u6027\u80fd\u901a\u5e38\u4f1a\u56e0\u865a\u5047\u76f8\u5173\u6027\u800c\u4e0b\u964d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Past-Token Prediction (PTP)\u8f85\u52a9\u4efb\u52a1\uff0c\u4ee5\u53ca\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u540c\u65f6\u5728\u6d4b\u8bd5\u65f6\u5f15\u5165\u4e86\u81ea\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u5934\u4e2d\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u516d\u4e2a\u6a21\u62df\u4efb\u52a1\u4e2d\u5c06\u957f\u4e0a\u4e0b\u6587\u6269\u6563\u7b56\u7565\u7684\u6027\u80fd\u63d0\u9ad8\u4e863\u500d\uff0c\u5e76\u52a0\u901f\u4e86\u7b56\u7565\u8bad\u7ec3\u8d85\u8fc710\u500d\u3002"}}
{"id": "2505.09326", "pdf": "https://arxiv.org/pdf/2505.09326", "abs": "https://arxiv.org/abs/2505.09326", "authors": ["Vincent Abbott", "Kotaro Kamiya", "Gerard Glowacki", "Yu Atsumi", "Gioele Zardini", "Yoshihiro Maruyama"], "title": "Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks", "categories": ["math.CT", "cs.LG", "q-bio.MN"], "comment": null, "summary": "How do we enable artificial intelligence models to improve themselves? This\nis central to exponentially improving generalized artificial intelligence\nmodels, which can improve their own architecture to handle new problem domains\nin an efficient manner that leverages the latest hardware. However, current\nautomated compilation methods are poor, and efficient algorithms require years\nof human development. In this paper, we use neural circuit diagrams, based in\ncategory theory, to prove a general theorem related to deep learning\nalgorithms, guide the development of a novel attention algorithm catered to the\ndomain of gene regulatory networks, and produce a corresponding efficient\nkernel. The algorithm we propose, spherical attention, shows that neural\ncircuit diagrams enable a principled and systematic method for reasoning about\ndeep learning architectures and providing high-performance code. By replacing\nSoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special\nfunction unit bottleneck of standard attention while retaining the streaming\nproperty essential to high-performance. Our diagrammatically derived\n\\textit{FlashSign} kernel achieves comparable performance to the\nstate-of-the-art, fine-tuned FlashAttention algorithm on an A100, and\n$3.6\\times$ the performance of PyTorch. Overall, this investigation shows\nneural circuit diagrams' suitability as a high-level framework for the\nautomated development of efficient, novel artificial intelligence\narchitectures.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u795e\u7ecf\u7535\u8def\u56fe\u6765\u6539\u8fdb\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6027\u80fd\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u7f16\u8bd1\u65b9\u6cd5\u8f83\u5dee\uff0c\u800c\u9ad8\u6548\u7684\u7b97\u6cd5\u9700\u8981\u591a\u5e74\u7684\u5f00\u53d1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002", "method": "\u672c\u6587\u4f7f\u7528\u57fa\u4e8e\u8303\u7574\u8bba\u7684\u795e\u7ecf\u7535\u8def\u56fe\uff0c\u8bc1\u660e\u4e86\u4e00\u4e2a\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u76f8\u5173\u7684\u901a\u7528\u5b9a\u7406\uff0c\u6307\u5bfc\u4e86\u9488\u5bf9\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u9886\u57df\u7684\u65b0\u578b\u6ce8\u610f\u529b\u7b97\u6cd5\u7684\u5f00\u53d1\uff0c\u5e76\u4ea7\u751f\u4e86\u76f8\u5e94\u7684\u9ad8\u6548\u5185\u6838\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7403\u9762\u6ce8\u610f\u529b\u7b97\u6cd5\u901a\u8fc7\u5c06SoftMax\u66ff\u6362\u4e3aL^2\u8303\u6570\uff0c\u514b\u670d\u4e86\u6807\u51c6\u6ce8\u610f\u529b\u7684\u7279\u6b8a\u51fd\u6570\u5355\u5143\u74f6\u9888\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u9ad8\u6027\u80fd\u6240\u9700\u7684\u6d41\u7279\u6027\u3002\u56fe\u793a\u63a8\u5bfc\u51fa\u7684FlashSign\u5185\u6838\u5728A100\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u5fae\u8c03FlashAttention\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6bd4PyTorch\u5feb3.6\u500d\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u795e\u7ecf\u7535\u8def\u56fe\u4f5c\u4e3a\u9ad8\u6548\u3001\u65b0\u9896\u4eba\u5de5\u667a\u80fd\u67b6\u6784\u81ea\u52a8\u5f00\u53d1\u7684\u9ad8\u5c42\u6846\u67b6\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.09576", "pdf": "https://arxiv.org/pdf/2505.09576", "abs": "https://arxiv.org/abs/2505.09576", "authors": ["Shannon Lodoen", "Alexi Orchard"], "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach", "categories": ["cs.CY", "cs.AI"], "comment": "10 pages, 1 figure, Accepted version", "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86RLHF\u589e\u5f3a\u7684\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bed\u8a00\u89c4\u8303\u3001\u4fe1\u606f\u5bfb\u6c42\u548c\u793e\u4f1a\u5173\u7cfb\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u53ef\u80fd\u5e26\u6765\u7684\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\uff08\u5982ChatGPT\u548cClaude\uff09\u4f7f\u7528RLHF\u6280\u672f\u6765\u4f18\u5316\u8f93\u51fa\uff0c\u4eba\u7c7b\u4e0e\u673a\u5668\u5199\u4f5c\u6587\u672c\u4e4b\u95f4\u7684\u754c\u9650\u65e5\u76ca\u6a21\u7cca\uff0c\u8fd9\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u4f26\u7406\u3001\u793e\u4f1a\u6280\u672f\u53ca\u6559\u80b2\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5bf9\u8fd9\u4e9b\u5f71\u54cd\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002", "method": "\u672c\u6587\u4f7f\u7528Ian Bogost\u7684\u7a0b\u5e8f\u4fee\u8f9e\u6982\u5ff5\uff0c\u5c06\u4fee\u8f9e\u8c03\u67e5\u7684\u7126\u70b9\u4ece\u5185\u5bb9\u5206\u6790\u8f6c\u79fb\u5230RLHF\u589e\u5f3a\u7684LLMs\u4e2d\u5185\u7f6e\u7684\u8bf4\u670d\u673a\u5236\u3002", "result": "\u672c\u6587\u901a\u8fc7\u4fee\u8f9e\u5206\u6790\u63ed\u793a\u4e86RLHF\u589e\u5f3a\u7684\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u5728\u8bed\u8a00\u89c4\u8303\u3001\u4fe1\u606f\u5bfb\u6c42\u548c\u793e\u4f1a\u5173\u7cfb\u671f\u671b\u65b9\u9762\u7684\u53d8\u5316\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u6f5c\u5728\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8eRLHF\u589e\u5f3a\u7684\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u5982\u4f55\u91cd\u65b0\u5851\u9020\u8bed\u8a00\u89c4\u8303\u3001\u4fe1\u606f\u5bfb\u6c42\u5b9e\u8df5\u548c\u793e\u4f1a\u5173\u7cfb\u671f\u671b\u7684\u4fee\u8f9e\u5206\u6790\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u6307\u51fa\uff0c\u8fd9\u79cd\u6280\u672f\u53ef\u80fd\u5f3a\u5316\u4e3b\u6d41\u8bed\u8a00\u4f7f\u7528\u3001\u5ef6\u7eed\u504f\u89c1\u3001\u53bb\u8bed\u5883\u5316\u5b66\u4e60\uff0c\u5e76\u4fb5\u72af\u4eba\u9645\u5173\u7cfb\u3002"}}
{"id": "2505.09364", "pdf": "https://arxiv.org/pdf/2505.09364", "abs": "https://arxiv.org/abs/2505.09364", "authors": ["Michael Benigni", "Maurizio Ferrari Dacrema", "Dietmar Jannach"], "title": "Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch", "categories": ["cs.IR", "cs.LG", "cs.NE"], "comment": null, "summary": "Countless new machine learning models are published every year and are\nreported to significantly advance the state-of-the-art in \\emph{top-n}\nrecommendation. However, earlier reproducibility studies indicate that progress\nin this area may be quite limited. Specifically, various widespread\nmethodological issues, e.g., comparisons with untuned baseline models, have led\nto an \\emph{illusion of progress}. In this work, our goal is to examine whether\nthese problems persist in today's research. To this end, we aim to reproduce\nthe latest advancements reported from applying modern Denoising Diffusion\nProbabilistic Models to recommender systems, focusing on four models published\nat the top-ranked SIGIR conference in 2023 and 2024. Our findings are\nconcerning, revealing persistent methodological problems. Alarmingly, through\nexperiments, we find that the latest recommendation techniques based on\ndiffusion models, despite their computational complexity and substantial carbon\nfootprint, are consistently outperformed by simpler existing models.\nFurthermore, we identify key mismatches between the characteristics of\ndiffusion models and those of the traditional \\emph{top-n} recommendation task,\nraising doubts about their suitability for recommendation. We also note that,\nin the papers we analyze, the generative capabilities of these models are\nconstrained to a minimum. Overall, our results and continued methodological\nissues call for greater scientific rigor and a disruptive change in the\nresearch and publication culture in this area.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u6700\u8fd1\u5e94\u7528\u6269\u6563\u6a21\u578b\u5230\u63a8\u8350\u7cfb\u7edf\u7684\u8bba\u6587\uff0c\u53d1\u73b0\u5b58\u5728\u65b9\u6cd5\u8bba\u95ee\u9898\uff0c\u4e14\u6700\u65b0\u7684\u63a8\u8350\u6280\u672f\u5e76\u4e0d\u4f18\u4e8e\u66f4\u7b80\u5355\u7684\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u65e9\u671f\u7684\u53ef\u91cd\u590d\u6027\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e00\u9886\u57df\u7684\u8fdb\u5c55\u53ef\u80fd\u76f8\u5f53\u6709\u9650\uff0c\u56e0\u6b64\u6211\u4eec\u5e0c\u671b\u68c0\u67e5\u8fd9\u4e9b\u95ee\u9898\u662f\u5426\u4ecd\u7136\u5b58\u5728\u4e8e\u5f53\u4eca\u7684\u7814\u7a76\u4e2d\u3002", "method": "\u6211\u4eec\u8bd5\u56fe\u91cd\u73b0\u6700\u65b0\u62a5\u9053\u7684\u5c06\u73b0\u4ee3\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u5e94\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u7684\u4f18\u52bf\uff0c\u91cd\u70b9\u5173\u6ce8\u5728\u9876\u7ea7\u6392\u540d\u7684SIGIR\u4f1a\u8bae2023\u5e74\u548c2024\u5e74\u53d1\u8868\u7684\u56db\u4e2a\u6a21\u578b\u3002", "result": "\u6211\u4eec\u7684\u53d1\u73b0\u4ee4\u4eba\u62c5\u5fe7\uff0c\u63ed\u793a\u4e86\u6301\u7eed\u7684\u65b9\u6cd5\u8bba\u95ee\u9898\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u63a8\u8350\u6280\u672f\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5927\u91cf\u7684\u78b3\u8db3\u8ff9\uff0c\u4f46\u59cb\u7ec8\u88ab\u66f4\u7b80\u5355\u7684\u73b0\u6709\u6a21\u578b\u6240\u8d85\u8d8a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u6269\u6563\u6a21\u578b\u7684\u7279\u6027\u4e0e\u4f20\u7edftop-n\u63a8\u8350\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u952e\u4e0d\u5339\u914d\uff0c\u8fd9\u5f15\u53d1\u4e86\u5b83\u4eec\u9002\u7528\u4e8e\u63a8\u8350\u7684\u7591\u95ee\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u548c\u6301\u7eed\u7684\u65b9\u6cd5\u8bba\u95ee\u9898\u547c\u5401\u5728\u8be5\u9886\u57df\u63d0\u9ad8\u79d1\u5b66\u4e25\u8c28\u6027\uff0c\u5e76\u5bf9\u7814\u7a76\u548c\u51fa\u7248\u6587\u5316\u8fdb\u884c\u98a0\u8986\u6027\u7684\u6539\u53d8\u3002"}}
{"id": "2505.09365", "pdf": "https://arxiv.org/pdf/2505.09365", "abs": "https://arxiv.org/abs/2505.09365", "authors": ["H. T. R\u00fcdisser", "G. Nguyen", "J. Le Lou\u00ebdec", "C. M\u00f6stl"], "title": "ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections", "categories": ["physics.space-ph", "astro-ph.IM", "astro-ph.SR", "cs.LG"], "comment": "25 pages, 9 figures, 1 table, submitted to AGU Space Weather on 14th\n  May 2025", "summary": "Interplanetary coronal mass ejections (ICMEs) are major drivers of space\nweather disturbances, posing risks to both technological infrastructure and\nhuman activities. Automatic detection of ICMEs in solar wind in situ data is\nessential for early warning systems. While several methods have been proposed\nto identify these structures in time series data, robust real-time detection\nremains a significant challenge. In this work, we present ARCANE - the first\nframework explicitly designed for early ICME detection in streaming solar wind\ndata under realistic operational constraints, enabling event identification\nwithout requiring observation of the full structure. Our approach evaluates the\nstrengths and limitations of detection models by comparing a machine\nlearning-based method to a threshold-based baseline. The ResUNet++ model,\npreviously validated on science data, significantly outperforms the baseline,\nparticularly in detecting high-impact events, while retaining solid performance\non lower-impact cases. Notably, we find that using real-time solar wind (RTSW)\ndata instead of high-resolution science data leads to only minimal performance\ndegradation. Despite the challenges of operational settings, our detection\npipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%\nof the event's duration while only seeing a minimal amount of data. As more\ndata becomes available, the performance increases significantly. These results\nmark a substantial step forward in automated space weather monitoring and lay\nthe groundwork for enhanced real-time forecasting capabilities.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ARCANE\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5b9e\u65f6\u592a\u9633\u98ce\u6570\u636e\u4e2d\u65e9\u671f\u68c0\u6d4bICMEs\u3002\u901a\u8fc7\u6bd4\u8f83\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u9608\u503c\u7684\u57fa\u7ebf\uff0c\u53d1\u73b0ResUNet++\u6a21\u578b\u5728\u68c0\u6d4b\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u68c0\u6d4bICMEs\u5bf9\u4e8e\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5df2\u7ecf\u63d0\u51fa\u4e86\u51e0\u79cd\u65b9\u6cd5\u6765\u8bc6\u522b\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u8fd9\u4e9b\u7ed3\u6784\uff0c\u4f46\u7a33\u5065\u7684\u5b9e\u65f6\u68c0\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u6211\u4eec\u6bd4\u8f83\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u9608\u503c\u7684\u57fa\u7ebf\uff0c\u8bc4\u4f30\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002\u4f7f\u7528ResUNet++\u6a21\u578b\uff0c\u5728\u79d1\u5b66\u6570\u636e\u4e0a\u9a8c\u8bc1\u8fc7\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u65b9\u9762\u3002", "result": "\u6211\u4eec\u7684\u68c0\u6d4b\u6d41\u7a0b\u5728F1\u5206\u6570\u4e0a\u8fbe\u5230\u4e860.53\uff0c\u5e73\u5747\u68c0\u6d4b\u5ef6\u8fdf\u4e3a\u4e8b\u4ef6\u6301\u7eed\u65f6\u95f4\u768421.5%\uff0c\u5e76\u4e14\u53ea\u770b\u5230\u4e86\u5c11\u91cf\u7684\u6570\u636e\u3002\u968f\u7740\u66f4\u591a\u6570\u636e\u7684\u53ef\u7528\uff0c\u6027\u80fd\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u6807\u5fd7\u7740\u5728\u81ea\u52a8\u5316\u7a7a\u95f4\u5929\u6c14\u76d1\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5e76\u4e3a\u589e\u5f3a\u5b9e\u65f6\u9884\u6d4b\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09598", "pdf": "https://arxiv.org/pdf/2505.09598", "abs": "https://arxiv.org/abs/2505.09598", "authors": ["Nidhal Jegham", "Marwen Abdelatti", "Lassad Elmoubarki", "Abdeltawab Hendawi"], "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316LLM\u63a8\u7406\u5728\u5546\u4e1a\u6570\u636e\u4e2d\u5fc3\u4e2d30\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7684\u73af\u5883\u8db3\u8ff9\u3002\u7ed3\u679c\u8868\u660e\uff0c\u67d0\u4e9b\u6a21\u578b\u7684\u80fd\u8017\u8fdc\u9ad8\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u800c\u5927\u89c4\u6a21\u67e5\u8be2\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u7684\u73af\u5883\u5f71\u54cd\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30LLM\u90e8\u7f72\u53ef\u6301\u7eed\u6027\u7684\u6807\u51c6\u5316\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u884c\u4e1a\u4e2d\u4f20\u64ad\uff0c\u4e86\u89e3\u5176\u63a8\u7406\u5c42\u9762\u7684\u73af\u5883\u8db3\u8ff9\u4e0d\u518d\u53ef\u9009\uff0c\u800c\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7814\u7a76\u6392\u9664\u4e86\u4e13\u6709\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u57fa\u7840\u8bbe\u65bd\u7684\u5dee\u5f02\u6027\u548c\u5f00\u9500\uff0c\u6216\u8005\u53ea\u5173\u6ce8\u8bad\u7ec3\uff0c\u5c3d\u7ba1\u63a8\u7406\u8d8a\u6765\u8d8a\u4e3b\u5bfcAI\u7684\u73af\u5883\u5f71\u54cd\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u57fa\u51c6\u6846\u67b6\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316LLM\u63a8\u7406\u5728\u5546\u4e1a\u6570\u636e\u4e2d\u5fc3\u4e2d30\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7684\u73af\u5883\u8db3\u8ff9\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u516c\u5171API\u6027\u80fd\u6570\u636e\u3001\u7279\u5b9a\u5730\u533a\u7684\u73af\u5883\u4e58\u6570\u548c\u786c\u4ef6\u914d\u7f6e\u7684\u7edf\u8ba1\u63a8\u65ad\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u4ea4\u53c9\u6548\u7387\u6570\u636e\u5305\u7edc\u5206\u6790\uff08DEA\uff09\u6309\u76f8\u5bf9\u4e8e\u73af\u5883\u6210\u672c\u7684\u6027\u80fd\u5bf9\u6a21\u578b\u8fdb\u884c\u6392\u540d\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u663e\u793a\uff0co3\u548cDeepSeek-R1\u662f\u6700\u8017\u80fd\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u957f\u63d0\u793a\u6d88\u8017\u8d85\u8fc733Wh\uff0c\u6bd4GPT-4.1 nano\u7684\u6d88\u8017\u591a70\u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14Claude-3.7 Sonnet\u5728\u751f\u6001\u6548\u7387\u65b9\u9762\u6392\u540d\u6700\u9ad8\u3002\u867d\u7136\u5355\u4e2a\u77edGPT-4o\u67e5\u8be2\u6d88\u80170.43Wh\uff0c\u4f46\u5c06\u5176\u6269\u5c55\u5230\u6bcf\u59297\u4ebf\u6b21\u67e5\u8be2\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u5e74\u5ea6\u73af\u5883\u5f71\u54cd\u3002\u8fd9\u4e9b\u5305\u62ec\u7535\u529b\u4f7f\u7528\u91cf\u76f8\u5f53\u4e8e35,000\u4e2a\u7f8e\u56fd\u5bb6\u5ead\uff0c\u6de1\u6c34\u84b8\u53d1\u91cf\u76f8\u5f53\u4e8e120\u4e07\u4eba\u7684\u5e74\u5ea6\u996e\u7528\u6c34\u9700\u6c42\uff0c\u4ee5\u53ca\u78b3\u6392\u653e\u91cf\u9700\u8981\u4e00\u4e2a\u829d\u52a0\u54e5\u5927\u5c0f\u7684\u68ee\u6797\u6765\u62b5\u6d88\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u7684\u3001\u57fa\u4e8e\u5b9e\u8bc1\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLM\u90e8\u7f72\u7684\u53ef\u6301\u7eed\u6027\uff0c\u4e3a\u672a\u6765AI\u5f00\u53d1\u7684\u73af\u5883\u8d23\u4efb\u548c\u53ef\u6301\u7eed\u6027\u6807\u51c6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09425", "pdf": "https://arxiv.org/pdf/2505.09425", "abs": "https://arxiv.org/abs/2505.09425", "authors": ["Sarah Leyder", "Jakob Raymaekers", "Peter J. Rousseeuw", "Tom Van Deuren", "Tim Verdonck"], "title": "Independent Component Analysis by Robust Distance Correlation", "categories": ["stat.CO", "cs.LG"], "comment": null, "summary": "Independent component analysis (ICA) is a powerful tool for decomposing a\nmultivariate signal or distribution into fully independent sources, not just\nuncorrelated ones. Unfortunately, most approaches to ICA are not robust against\noutliers. Here we propose a robust ICA method called RICA, which estimates the\ncomponents by minimizing a robust measure of dependence between multivariate\nrandom variables. The dependence measure used is the distance correlation\n(dCor). In order to make it more robust we first apply a new transformation\ncalled the bowl transform, which is bounded, one-to-one, continuous, and maps\nfar outliers to points close to the origin. This preserves the crucial property\nthat a zero dCor implies independence. RICA estimates the independent sources\nsequentially, by looking for the component that has the smallest dCor with the\nremainder. RICA is strongly consistent and has the usual parametric rate of\nconvergence. Its robustness is investigated by a simulation study, in which it\ngenerally outperforms its competitors. The method is illustrated on three\napplications, including the well-known cocktail party problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRICA\u7684\u9c81\u68d2ICA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8ddd\u79bb\u76f8\u5173\u6027\u6765\u4f30\u8ba1\u72ec\u7acb\u6e90\uff0c\u5e76\u4f7f\u7528\u7897\u53d8\u6362\u63d0\u9ad8\u9c81\u68d2\u6027\u3002RICA\u5728\u6a21\u62df\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u591a\u6570ICA\u65b9\u6cd5\u5bf9\u5f02\u5e38\u503c\u4e0d\u9c81\u68d2\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684ICA\u65b9\u6cd5\u3002", "method": "RICA\u901a\u8fc7\u6700\u5c0f\u5316\u591a\u53d8\u91cf\u968f\u673a\u53d8\u91cf\u4e4b\u95f4\u7684\u9c81\u68d2\u4f9d\u8d56\u5ea6\u91cf\uff08\u8ddd\u79bb\u76f8\u5173\u6027\uff09\u6765\u4f30\u8ba1\u6210\u5206\uff0c\u5e76\u4f7f\u7528\u4e86\u65b0\u7684\u8f6c\u6362\u65b9\u6cd5\uff08\u7897\u53d8\u6362\uff09\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "RICA\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u901a\u5e38\u4f18\u4e8e\u5176\u7ade\u4e89\u5bf9\u624b\uff0c\u5e76\u5728\u4e09\u4e2a\u5e94\u7528\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5305\u62ec\u8457\u540d\u7684\u9e21\u5c3e\u9152\u4f1a\u95ee\u9898\u3002", "conclusion": "RICA\u662f\u4e00\u79cd\u5f3a\u5927\u7684ICA\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6a21\u62df\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.09430", "pdf": "https://arxiv.org/pdf/2505.09430", "abs": "https://arxiv.org/abs/2505.09430", "authors": ["Yutong Hu", "Pinhao Song", "Kehan Wen", "Renaud Detry"], "title": "Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We present a method for training multi-task vision-language robotic diffusion\npolicies that reduces training time and memory usage by an order of magnitude.\nThis improvement arises from a previously underexplored distinction between\naction diffusion and the image diffusion techniques that inspired it: image\ngeneration targets are high-dimensional, while robot actions lie in a much\nlower-dimensional space. Meanwhile, the vision-language conditions for action\ngeneration remain high-dimensional. Our approach, Mini-Diffuser, exploits this\nasymmetry by introducing Level-2 minibatching, which pairs multiple noised\naction samples with each vision-language condition, instead of the conventional\none-to-one sampling strategy. To support this batching scheme, we introduce\narchitectural adaptations to the diffusion transformer that prevent information\nleakage across samples while maintaining full conditioning access. In RLBench\nsimulations, Mini-Diffuser achieves 95\\% of the performance of state-of-the-art\nmulti-task diffusion policies, while using only 5\\% of the training time and\n7\\% of the memory. Real-world experiments further validate that Mini-Diffuser\npreserves the key strengths of diffusion-based policies, including the ability\nto model multimodal action distributions and produce behavior conditioned on\ndiverse perceptual inputs. Code available at\ngithub.com/utomm/mini-diffuse-actor.", "AI": {"tldr": "Mini-Diffuser is a method for training multi-task vision-language robotic diffusion policies that reduces training time and memory usage by an order of magnitude, achieving high performance while preserving key strengths of diffusion-based policies.", "motivation": "The motivation is to reduce training time and memory usage for multi-task vision-language robotic diffusion policies by exploiting the asymmetry between action diffusion and image diffusion techniques.", "method": "Mini-Diffuser introduces Level-2 minibatching, which pairs multiple noised action samples with each vision-language condition, instead of the conventional one-to-one sampling strategy. Architectural adaptations to the diffusion transformer prevent information leakage across samples while maintaining full conditioning access.", "result": "In RLBench simulations, Mini-Diffuser achieves 95% of the performance of state-of-the-art multi-task diffusion policies, while using only 5% of the training time and 7% of the memory. Real-world experiments validate that it preserves the key strengths of diffusion-based policies.", "conclusion": "Mini-Diffuser achieves 95% of the performance of state-of-the-art multi-task diffusion policies while using only 5% of the training time and 7% of the memory. It preserves the key strengths of diffusion-based policies, including the ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs."}}
{"id": "2505.09471", "pdf": "https://arxiv.org/pdf/2505.09471", "abs": "https://arxiv.org/abs/2505.09471", "authors": ["Xiaoyu Hu", "Gengyu Xue", "Zhenhua Lin", "Yi Yu"], "title": "Fairness-aware Bayes optimal functional classification", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Algorithmic fairness has become a central topic in machine learning, and\nmitigating disparities across different subpopulations has emerged as a rapidly\ngrowing research area. In this paper, we systematically study the\nclassification of functional data under fairness constraints, ensuring the\ndisparity level of the classifier is controlled below a pre-specified\nthreshold. We propose a unified framework for fairness-aware functional\nclassification, tackling an infinite-dimensional functional space, addressing\nkey challenges from the absence of density ratios and intractability of\nposterior probabilities, and discussing unique phenomena in functional\nclassification. We further design a post-processing algorithm, Fair Functional\nLinear Discriminant Analysis classifier (Fair-FLDA), which targets at\nhomoscedastic Gaussian processes and achieves fairness via group-wise\nthresholding. Under weak structural assumptions on eigenspace, theoretical\nguarantees on fairness and excess risk controls are established. As a\nbyproduct, our results cover the excess risk control of the standard FLDA as a\nspecial case, which, to the best of our knowledge, is first time seen. Our\ntheoretical findings are complemented by extensive numerical experiments on\nsynthetic and real datasets, highlighting the practicality of our designed\nalgorithm.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u516c\u5e73\u6027\u7ea6\u675f\u4e0b\u7684\u529f\u80fd\u6570\u636e\u5206\u7c7b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u516c\u5e73\u611f\u77e5\u529f\u80fd\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540e\u5904\u7406\u7b97\u6cd5Fair-FLDA\uff0c\u5b9e\u73b0\u4e86\u516c\u5e73\u6027\u63a7\u5236\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u548c\u98ce\u9669\u63a7\u5236\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u7b97\u6cd5\u516c\u5e73\u6027\u5df2\u6210\u4e3a\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\u8bdd\u9898\uff0c\u51cf\u8f7b\u4e0d\u540c\u5b50\u7fa4\u4f53\u4e4b\u95f4\u7684\u5dee\u5f02\u5df2\u6210\u4e3a\u5feb\u901f\u589e\u957f\u7684\u7814\u7a76\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5728\u516c\u5e73\u6027\u7ea6\u675f\u4e0b\u7684\u529f\u80fd\u6570\u636e\u5206\u7c7b\u95ee\u9898\uff0c\u786e\u4fdd\u5206\u7c7b\u5668\u7684\u5dee\u5f02\u6c34\u5e73\u4f4e\u4e8e\u9884\u8bbe\u9608\u503c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u516c\u5e73\u611f\u77e5\u529f\u80fd\u5206\u7c7b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u65e0\u9650\u7ef4\u529f\u80fd\u7a7a\u95f4\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540e\u5904\u7406\u7b97\u6cd5Fair-FLDA\uff0c\u901a\u8fc7\u7ec4\u5185\u9608\u503c\u8c03\u6574\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684Fair-FLDA\u7b97\u6cd5\u5728\u540c\u65b9\u5dee\u9ad8\u65af\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u516c\u5e73\u6027\uff0c\u5e76\u5728\u5f31\u7ed3\u6784\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u516c\u5e73\u6027\u548c\u989d\u5916\u98ce\u9669\u63a7\u5236\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5177\u6709\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u516c\u5e73\u611f\u77e5\u529f\u80fd\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540e\u5904\u7406\u7b97\u6cd5Fair-FLDA\uff0c\u5b9e\u73b0\u4e86\u516c\u5e73\u6027\u63a7\u5236\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u548c\u98ce\u9669\u63a7\u5236\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2505.09496", "pdf": "https://arxiv.org/pdf/2505.09496", "abs": "https://arxiv.org/abs/2505.09496", "authors": ["Rui Miao", "Babak Shahbaba", "Annie Qu"], "title": "Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to find optimal policies in dynamic\nenvironments in order to maximize the expected total rewards by leveraging\npre-collected data. Learning from heterogeneous data is one of the fundamental\nchallenges in offline RL. Traditional methods focus on learning an optimal\npolicy for all individuals with pre-collected data from a single episode or\nhomogeneous batch episodes, and thus, may result in a suboptimal policy for a\nheterogeneous population. In this paper, we propose an individualized offline\npolicy optimization framework for heterogeneous time-stationary Markov decision\nprocesses (MDPs). The proposed heterogeneous model with individual latent\nvariables enables us to efficiently estimate the individual Q-functions, and\nour Penalized Pessimistic Personalized Policy Learning (P4L) algorithm\nguarantees a fast rate on the average regret under a weak partial coverage\nassumption on behavior policies. In addition, our simulation studies and a real\ndata application demonstrate the superior numerical performance of the proposed\nmethod compared with existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5f02\u6784\u65f6\u95f4\u5e73\u7a33\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u4e2a\u6027\u5316\u79bb\u7ebf\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u65f6\u53ef\u80fd\u65e0\u6cd5\u4e3a\u5f02\u6784\u4eba\u7fa4\u63d0\u4f9b\u6700\u4f18\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5f02\u6784\u6570\u636e\u7684\u4e2a\u6027\u5316\u79bb\u7ebf\u7b56\u7565\u4f18\u5316\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e26\u6709\u4e2a\u4f53\u6f5c\u5728\u53d8\u91cf\u7684\u5f02\u6784\u6a21\u578b\uff0c\u4ee5\u6709\u6548\u4f30\u8ba1\u4e2a\u4f53Q\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u4e86\u60e9\u7f5a\u60b2\u89c2\u4e2a\u6027\u5316\u7b56\u7565\u5b66\u4e60\uff08P4L\uff09\u7b97\u6cd5\uff0c\u4ee5\u5728\u884c\u4e3a\u7b56\u7565\u7684\u5f31\u90e8\u5206\u8986\u76d6\u5047\u8bbe\u4e0b\u4fdd\u8bc1\u5e73\u5747\u9057\u61be\u7684\u5feb\u901f\u7387\u3002", "result": "\u6a21\u62df\u7814\u7a76\u548c\u5b9e\u9645\u6570\u636e\u5e94\u7528\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u503c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5f02\u6784\u65f6\u95f4\u5e73\u7a33\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\u7684\u4e2a\u6027\u5316\u79bb\u7ebf\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u548c\u5b9e\u9645\u6570\u636e\u5e94\u7528\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.09506", "pdf": "https://arxiv.org/pdf/2505.09506", "abs": "https://arxiv.org/abs/2505.09506", "authors": ["Mar\u00eda Alejandra Hern\u00e1ndez", "Oscar Rodriguez", "Dae-Jin Lee"], "title": "Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders", "categories": ["stat.ML", "cs.LG", "F.2.2; I.2.7"], "comment": "Pre-print", "summary": "Several approaches have been developed to capture the complexity and\nnonlinearity of human growth. One widely used is the Super Imposition by\nTranslation and Rotation (SITAR) model, which has become popular in studies of\nadolescent growth. SITAR is a shape-invariant mixed-effects model that\nrepresents the shared growth pattern of a population using a natural cubic\nspline mean curve while incorporating three subject-specific random effects --\ntiming, size, and growth intensity -- to account for variations among\nindividuals. In this work, we introduce a supervised deep learning framework\nbased on an autoencoder architecture that integrates a deep neural network\n(neural network) with a B-spline model to estimate the SITAR model. In this\napproach, the encoder estimates the random effects for each individual, while\nthe decoder performs a fitting based on B-splines similar to the classic SITAR\nmodel. We refer to this method as the Deep-SITAR model. This innovative\napproach enables the prediction of the random effects of new individuals\nentering a population without requiring a full model re-estimation. As a\nresult, Deep-SITAR offers a powerful approach to predicting growth\ntrajectories, combining the flexibility and efficiency of deep learning with\nthe interpretability of traditional mixed-effects models.", "AI": {"tldr": "This paper introduces Deep-SITAR, a deep learning framework that combines the flexibility of deep learning with the interpretability of traditional mixed-effects models to predict growth trajectories efficiently.", "motivation": "Several approaches have been developed to capture the complexity and nonlinearity of human growth. The SITAR model is widely used but has limitations in predicting random effects for new individuals without full model re-estimation.", "method": "We introduce a supervised deep learning framework based on an autoencoder architecture that integrates a deep neural network with a B-spline model to estimate the SITAR model. The encoder estimates the random effects for each individual, while the decoder performs a fitting based on B-splines similar to the classic SITAR model.", "result": "Deep-SITAR enables the prediction of the random effects of new individuals entering a population without requiring a full model re-estimation, offering a powerful approach to predicting growth trajectories.", "conclusion": "Deep-SITAR offers a powerful approach to predicting growth trajectories, combining the flexibility and efficiency of deep learning with the interpretability of traditional mixed-effects models."}}
{"id": "2505.09516", "pdf": "https://arxiv.org/pdf/2505.09516", "abs": "https://arxiv.org/abs/2505.09516", "authors": ["Siyi Wang", "Alexandre Leblanc", "Paul D. McNicholas"], "title": "Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios", "categories": ["stat.ME", "cs.LG", "stat.AP"], "comment": null, "summary": "Cluster analysis, or clustering, plays a crucial role across numerous\nscientific and engineering domains. Despite the wealth of clustering methods\nproposed over the past decades, each method is typically designed for specific\nscenarios and presents certain limitations in practical applications. In this\npaper, we propose depth-based local center clustering (DLCC). This novel method\nmakes use of data depth, which is known to produce a center-outward ordering of\nsample points in a multivariate space. However, data depth typically fails to\ncapture the multimodal characteristics of {data}, something of the utmost\nimportance in the context of clustering. To overcome this, DLCC makes use of a\nlocal version of data depth that is based on subsets of {data}. From this,\nlocal centers can be identified as well as clusters of varying shapes.\nFurthermore, we propose a new internal metric based on density-based clustering\nto evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a\nflexible clustering approach that seems to overcome some limitations of\ntraditional clustering methods, thereby enhancing data analysis capabilities\nacross a wide range of application scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u57fa\u4e8e\u5c40\u90e8\u4e2d\u5fc3\u805a\u7c7b\uff08DLCC\uff09\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6570\u636e\u6df1\u5ea6\u6765\u8bc6\u522b\u5c40\u90e8\u4e2d\u5fc3\u548c\u4e0d\u540c\u5f62\u72b6\u7684\u805a\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5185\u90e8\u5ea6\u91cf\u6765\u8bc4\u4f30\u975e\u51f8\u805a\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u8fc7\u53bb\u51e0\u5341\u5e74\u63d0\u51fa\u4e86\u8bb8\u591a\u805a\u7c7b\u65b9\u6cd5\uff0c\u4f46\u6bcf\u79cd\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u573a\u666f\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u4e00\u5b9a\u7684\u9650\u5236\u3002\u6570\u636e\u6df1\u5ea6\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u6570\u636e\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u8fd9\u5bf9\u805a\u7c7b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6df1\u5ea6\u57fa\u4e8e\u5c40\u90e8\u4e2d\u5fc3\u805a\u7c7b\uff08DLCC\uff09\u5229\u7528\u6570\u636e\u6df1\u5ea6\uff0c\u8fd9\u53ef\u4ee5\u4ea7\u751f\u591a\u53d8\u91cf\u7a7a\u95f4\u4e2d\u6837\u672c\u70b9\u7684\u4e2d\u5fc3\u5411\u5916\u6392\u5e8f\u3002DLCC\u4f7f\u7528\u57fa\u4e8e\u6570\u636e\u5b50\u96c6\u7684\u5c40\u90e8\u6570\u636e\u6df1\u5ea6\uff0c\u4ee5\u8bc6\u522b\u5c40\u90e8\u4e2d\u5fc3\u548c\u4e0d\u540c\u5f62\u72b6\u7684\u805a\u7c7b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u7684\u5185\u90e8\u5ea6\u91cf\u6765\u8bc4\u4f30\u975e\u51f8\u805a\u7c7b\u7684\u805a\u7c7b\u6027\u80fd\u3002", "result": "DLCC\u80fd\u591f\u8bc6\u522b\u5c40\u90e8\u4e2d\u5fc3\u548c\u4e0d\u540c\u5f62\u72b6\u7684\u805a\u7c7b\uff0c\u5e76\u4e14\u63d0\u51fa\u7684\u65b0\u5185\u90e8\u5ea6\u91cf\u53ef\u4ee5\u8bc4\u4f30\u975e\u51f8\u805a\u7c7b\u7684\u805a\u7c7b\u6027\u80fd\u3002", "conclusion": "DLCC\u662f\u4e00\u79cd\u7075\u6d3b\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u4f3c\u4e4e\u514b\u670d\u4e86\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u7684\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5728\u5404\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6570\u636e\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2505.09546", "pdf": "https://arxiv.org/pdf/2505.09546", "abs": "https://arxiv.org/abs/2505.09546", "authors": ["Yujin Kim", "Nathaniel Chin", "Arnav Vasudev", "Sanjiban Choudhury"], "title": "Distilling Realizable Students from Unrealizable Teachers", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We study policy distillation under privileged information, where a student\npolicy with only partial observations must learn from a teacher with full-state\naccess. A key challenge is information asymmetry: the student cannot directly\naccess the teacher's state space, leading to distributional shifts and policy\ndegradation. Existing approaches either modify the teacher to produce\nrealizable but sub-optimal demonstrations or rely on the student to explore\nmissing information independently, both of which are inefficient. Our key\ninsight is that the student should strategically interact with the teacher\n--querying only when necessary and resetting from recovery states --to stay on\na recoverable path within its own observation space. We introduce two methods:\n(i) an imitation learning approach that adaptively determines when the student\nshould query the teacher for corrections, and (ii) a reinforcement learning\napproach that selects where to initialize training for efficient exploration.\nWe validate our methods in both simulated and real-world robotic tasks,\ndemonstrating significant improvements over standard teacher-student baselines\nin training efficiency and final performance. The project website is available\nat : https://portal-cornell.github.io/CritiQ_ReTRy/", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7279\u6743\u4fe1\u606f\u4e0b\u7684\u7b56\u7565\u84b8\u998f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7b56\u7565\u6027\u4ea4\u4e92\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u4e86\u5728\u7279\u6743\u4fe1\u606f\u4e0b\u7684\u7b56\u7565\u84b8\u998f\uff0c\u5176\u4e2d\u5177\u6709\u90e8\u5206\u89c2\u5bdf\u7684\u5b66\u751f\u7b56\u7565\u5fc5\u987b\u4ece\u5177\u6709\u5b8c\u6574\u72b6\u6001\u8bbf\u95ee\u7684\u6559\u5e08\u5b66\u4e60\u3002\u5173\u952e\u6311\u6218\u662f\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff1a\u5b66\u751f\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95ee\u6559\u5e08\u7684\u72b6\u6001\u7a7a\u95f4\uff0c\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\u548c\u7b56\u7565\u9000\u5316\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u8981\u4e48\u4fee\u6539\u6559\u5e08\u4ee5\u4ea7\u751f\u53ef\u5b9e\u73b0\u4f46\u6b21\u4f18\u7684\u6f14\u793a\uff0c\u8981\u4e48\u4f9d\u8d56\u5b66\u751f\u72ec\u7acb\u63a2\u7d22\u7f3a\u5931\u4fe1\u606f\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u662f\u4f4e\u6548\u7684\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a(i) \u4e00\u79cd\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u5b66\u751f\u4f55\u65f6\u5e94\u67e5\u8be2\u6559\u5e08\u4ee5\u83b7\u5f97\u7ea0\u6b63\uff1b(ii) \u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u9009\u62e9\u5728\u54ea\u91cc\u521d\u59cb\u5316\u8bad\u7ec3\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u63a2\u7d22\u3002", "result": "\u6211\u4eec\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u7684\u5e08\u751f\u57fa\u7ebf\u3002"}}
{"id": "2505.09552", "pdf": "https://arxiv.org/pdf/2505.09552", "abs": "https://arxiv.org/abs/2505.09552", "authors": ["Pascal K\u00fcndig", "Fabio Sigrist"], "title": "Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "Mixed effects models are widely used for modeling data with hierarchically\ngrouped structures and high-cardinality categorical predictor variables.\nHowever, for high-dimensional crossed random effects, current standard\ncomputations relying on Cholesky decompositions can become prohibitively slow.\nIn this work, we present novel Krylov subspace-based methods that address\nseveral existing computational bottlenecks. Among other things, we\ntheoretically analyze and empirically evaluate various preconditioners for the\nconjugate gradient and stochastic Lanczos quadrature methods, derive new\nconvergence results, and develop computationally efficient methods for\ncalculating predictive variances. Extensive experiments using simulated and\nreal-world data sets show that our proposed methods scale much better than\nCholesky-based computations, for instance, achieving a runtime reduction of\napproximately two orders of magnitudes for both estimation and prediction.\nMoreover, our software implementation is up to 10'000 times faster and more\nstable than state-of-the-art implementations such as lme4 and glmmTMB when\nusing default settings. Our methods are implemented in the free C++ software\nlibrary GPBoost with high-level Python and R packages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKrylov\u5b50\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u9ad8\u7ef4\u4ea4\u53c9\u968f\u673a\u6548\u5e94\u6df7\u5408\u6548\u5e94\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eCholesky\u5206\u89e3\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u4ea4\u53c9\u968f\u673a\u6548\u5e94\u65f6\u901f\u5ea6\u8fc7\u6162\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8eKrylov\u5b50\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u5171\u8f6d\u68af\u5ea6\u548c\u968f\u673aLanczos\u4e8c\u6b21\u6c42\u79ef\u65b9\u6cd5\u7684\u5404\u79cd\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u65b0\u7684\u6536\u655b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f30\u8ba1\u548c\u9884\u6d4b\u65b9\u9762\u6bd4\u57fa\u4e8eCholesky\u7684\u8ba1\u7b97\u65b9\u6cd5\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u4e14\u5728\u9ed8\u8ba4\u8bbe\u7f6e\u4e0b\u6bd4\u6700\u5148\u8fdb\u7684\u5b9e\u73b0\u5982lme4\u548cglmmTMB\u5feb\u591a\u8fbe10,000\u500d\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u66f4\u9ad8\u6548\u4e14\u66f4\u7a33\u5b9a\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u4ea4\u53c9\u968f\u673a\u6548\u5e94\u7684\u6df7\u5408\u6548\u5e94\u6a21\u578b\u3002"}}
{"id": "2505.09603", "pdf": "https://arxiv.org/pdf/2505.09603", "abs": "https://arxiv.org/abs/2505.09603", "authors": ["Shivin Dass", "Alaa Khaddaj", "Logan Engstrom", "Aleksander Madry", "Andrew Ilyas", "Roberto Mart\u00edn-Mart\u00edn"], "title": "DataMIL: Selecting Data for Robot Imitation Learning with Datamodels", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recently, the robotics community has amassed ever larger and more diverse\ndatasets to train generalist robot policies. However, while these policies\nachieve strong mean performance across a variety of tasks, they often\nunderperform on individual, specialized tasks and require further tuning on\nnewly acquired task-specific data. Combining task-specific data with carefully\ncurated subsets of large prior datasets via co-training can produce better\nspecialized policies, but selecting data naively may actually harm downstream\nperformance. To address this, we introduce DataMIL, a policy-driven data\nselection framework built on the datamodels paradigm that reasons about data\nselection in an end-to-end manner, using the policy itself to identify which\ndata points will most improve performance. Unlike standard practices that\nfilter data using human notions of quality (e.g., based on semantic or visual\nsimilarity), DataMIL directly optimizes data selection for task success,\nallowing us to select data that enhance the policy while dropping data that\ndegrade it. To avoid performing expensive rollouts in the environment during\nselection, we use a novel surrogate loss function on task-specific data,\nallowing us to use DataMIL in the real world without degrading performance. We\nvalidate our approach on a suite of more than 60 simulation and real-world\nmanipulation tasks - most notably showing successful data selection from the\nOpen X-Embodiment datasets-demonstrating consistent gains in success rates and\nsuperior performance over multiple baselines. Our results underscore the\nimportance of end-to-end, performance-aware data selection for unlocking the\npotential of large prior datasets in robotics. More information at\nhttps://robin-lab.cs.utexas.edu/datamodels4imitation/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDataMIL\u7684\u7aef\u5230\u7aef\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba\u7b56\u7565\u7684\u6027\u80fd\u3002\u901a\u8fc7\u76f4\u63a5\u9488\u5bf9\u4efb\u52a1\u6210\u529f\u4f18\u5316\u6570\u636e\u9009\u62e9\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u7684\u8d28\u91cf\u6807\u51c6\uff0cDataMIL\u80fd\u591f\u5728\u4e0d\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002\u5728\u5927\u91cf\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u6210\u529f\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u867d\u7136\u8fd9\u4e9b\u7b56\u7565\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5e73\u5747\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u5355\u4e2a\u3001\u4e13\u4e1a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5f80\u5f80\u4e0d\u4f73\uff0c\u5e76\u4e14\u9700\u8981\u5728\u65b0\u83b7\u53d6\u7684\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u4e0a\u8fdb\u884c\u8fdb\u4e00\u6b65\u8c03\u6574\u3002\u5c06\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u4e0e\u7cbe\u5fc3\u6311\u9009\u7684\u5927\u89c4\u6a21\u5148\u9a8c\u6570\u636e\u96c6\u5b50\u96c6\u901a\u8fc7\u5171\u540c\u8bad\u7ec3\u7ed3\u5408\uff0c\u53ef\u4ee5\u4ea7\u751f\u66f4\u597d\u7684\u4e13\u4e1a\u5316\u7b56\u7565\uff0c\u4f46\u76f2\u76ee\u9009\u62e9\u6570\u636e\u53ef\u80fd\u4f1a\u635f\u5bb3\u4e0b\u6e38\u6027\u80fd\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86DataMIL\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6570\u636e\u6a21\u578b\u8303\u5f0f\u7684\u653f\u7b56\u9a71\u52a8\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u5b83\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u63a8\u7406\u6570\u636e\u9009\u62e9\uff0c\u4f7f\u7528\u7b56\u7565\u672c\u8eab\u6765\u8bc6\u522b\u6700\u80fd\u63d0\u9ad8\u6027\u80fd\u7684\u6570\u636e\u70b9\u3002", "result": "\u6211\u4eec\u5728\u8d85\u8fc760\u4e2a\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u7684\u5957\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u2014\u2014\u6700\u8457\u540d\u7684\u662f\u5c55\u793a\u4e86\u4eceOpen X-Embodiment\u6570\u636e\u96c6\u4e2d\u6210\u529f\u9009\u62e9\u6570\u636e\u2014\u2014\u8bc1\u660e\u4e86\u6210\u529f\u7387\u7684\u4e00\u81f4\u589e\u957f\u548c\u5bf9\u591a\u4e2a\u57fa\u7ebf\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u5f3a\u8c03\u4e86\u7aef\u5230\u7aef\u3001\u6027\u80fd\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u5728\u91ca\u653e\u673a\u5668\u4eba\u9886\u57df\u5927\u578b\u5148\u9a8c\u6570\u636e\u96c6\u6f5c\u529b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.09612", "pdf": "https://arxiv.org/pdf/2505.09612", "abs": "https://arxiv.org/abs/2505.09612", "authors": ["Tathagata Sadhukhan", "Manit Paul", "Raaz Dwivedi"], "title": "Adaptively-weighted Nearest Neighbors for Matrix Completion", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": "25 pages, 6 figures", "summary": "In this technical note, we introduce and analyze AWNN: an adaptively weighted\nnearest neighbor method for performing matrix completion. Nearest neighbor (NN)\nmethods are widely used in missing data problems across multiple disciplines\nsuch as in recommender systems and for performing counterfactual inference in\npanel data settings. Prior works have shown that in addition to being very\nintuitive and easy to implement, NN methods enjoy nice theoretical guarantees.\nHowever, the performance of majority of the NN methods rely on the appropriate\nchoice of the radii and the weights assigned to each member in the nearest\nneighbor set and despite several works on nearest neighbor methods in the past\ntwo decades, there does not exist a systematic approach of choosing the radii\nand the weights without relying on methods like cross-validation. AWNN\naddresses this challenge by judiciously balancing the bias variance trade off\ninherent in weighted nearest-neighbor regression. We provide theoretical\nguarantees for the proposed method under minimal assumptions and support the\ntheory via synthetic experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AWNN\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u52a0\u6743\u6700\u8fd1\u90bb\u65b9\u6cd5\uff0c\u7528\u4e8e\u77e9\u9635\u8865\u5168\uff0c\u901a\u8fc7\u5e73\u8861\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u6765\u7cfb\u7edf\u5730\u9009\u62e9\u534a\u5f84\u548c\u6743\u91cd\u3002", "motivation": "\u4f20\u7edf\u6700\u8fd1\u90bb\u65b9\u6cd5\u5728\u9009\u62e9\u534a\u5f84\u548c\u6743\u91cd\u65f6\u7f3a\u4e4f\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002AWNN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "AWNN\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u52a0\u6743\u6700\u8fd1\u90bb\u65b9\u6cd5\uff0c\u7528\u4e8e\u6267\u884c\u77e9\u9635\u8865\u5168\u3002\u5b83\u901a\u8fc7\u5e73\u8861\u52a0\u6743\u6700\u8fd1\u90bb\u56de\u5f52\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u53c2\u6570\u9009\u62e9\u7684\u95ee\u9898\u3002", "result": "AWNN\u5728\u7406\u8bba\u4e0a\u6709\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u5f97\u5230\u4e86\u652f\u6301\u3002", "conclusion": "AWNN\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u52a0\u6743\u6700\u8fd1\u90bb\u56de\u5f52\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u9009\u62e9\u534a\u5f84\u548c\u6743\u91cd\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u4f20\u7edf\u6700\u8fd1\u90bb\u65b9\u6cd5\u5728\u9009\u62e9\u8fd9\u4e9b\u53c2\u6570\u65f6\u4f9d\u8d56\u4ea4\u53c9\u9a8c\u8bc1\u7684\u95ee\u9898\u3002"}}
{"id": "2505.09649", "pdf": "https://arxiv.org/pdf/2505.09649", "abs": "https://arxiv.org/abs/2505.09649", "authors": ["Abisha Thapa Magar", "Anup Shakya"], "title": "Next Word Suggestion using Graph Neural Network", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language Modeling is a prevalent task in Natural Language Processing. The\ncurrently existing most recent and most successful language models often tend\nto build a massive model with billions of parameters, feed in a tremendous\namount of text data, and train with enormous computation resources which\nrequire millions of dollars. In this project, we aim to address an important\nsub-task in language modeling, i.e., context embedding. We propose an approach\nto exploit the Graph Convolution operation in GNNs to encode the context and\nuse it in coalition with LSTMs to predict the next word given a local context\nof preceding words. We test this on the custom Wikipedia text corpus using a\nvery limited amount of resources and show that this approach works fairly well\nto predict the next word.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u5377\u79ef\u548cLSTMs\u8fdb\u884c\u4e0a\u4e0b\u6587\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u7684\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u53c2\u6570\u3001\u6587\u672c\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8bed\u8a00\u5efa\u6a21\u4e2d\u7684\u4e00\u4e2a\u5b50\u4efb\u52a1\u2014\u2014\u4e0a\u4e0b\u6587\u5d4c\u5165\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u5377\u79ef\u64cd\u4f5c\u7684GNN\u65b9\u6cd5\u6765\u7f16\u7801\u4e0a\u4e0b\u6587\uff0c\u5e76\u5c06\u5176\u4e0eLSTMs\u7ed3\u5408\u7528\u4e8e\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u7684Wikipedia\u6587\u672c\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u7684\u8d44\u6e90\u4e0b\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u7684\u8d44\u6e90\u4e0b\u80fd\u591f\u6709\u6548\u5730\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655", "abs": "https://arxiv.org/abs/2505.09655", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRA\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u591a\u6837\u6027\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u8ba1\u7b97\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u591a\u6837\u6027-\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002DRA\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "GRPO\u901a\u5e38\u4f9d\u8d56\u4e8e\u89e3\u51b3\u65b9\u6848\u7ea7\u522b\u548c\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u65e0\u6cd5\u6355\u6349\u5230\u91c7\u6837\u5b8c\u6210\u4e4b\u95f4\u7684\u8bed\u4e49\u591a\u6837\u6027\u3002\u8fd9\u5bfc\u81f4\u4e86\u6211\u4eec\u79f0\u4e4b\u4e3a\u591a\u6837\u6027-\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u4e0d\u540c\u7684\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u83b7\u5f97\u65e0\u6cd5\u533a\u5206\u7684\u5956\u52b1\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86DRA\uff0c\u4e00\u79cd\u5c06\u8bed\u4e49\u591a\u6837\u6027\u663e\u5f0f\u7eb3\u5165\u5956\u52b1\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002DRA\u4f7f\u7528\u5b50\u6a21\u6001\u4e92\u4fe1\u606f\uff08SMI\uff09\u6765\u964d\u4f4e\u5197\u4f59\u5b8c\u6210\u7684\u6743\u91cd\u5e76\u653e\u5927\u591a\u6837\u5b8c\u6210\u7684\u5956\u52b1\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u4e0eGRPO\u53ca\u5176\u53d8\u4f53DR.GRPO\u65e0\u7f1d\u96c6\u6210\uff0c\u4ea7\u751f\u4e86DRA-GRPO\u548cDGA-DR.GRPO\u3002\u6211\u4eec\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5b83\u4f18\u4e8e\u6700\u8fd1\u7684\u5f3a\u57fa\u7ebf\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4f18\u4e8e\u6700\u8fd1\u7684\u5f3a\u57fa\u7ebf\u3002\u5b83\u5728\u4ec5\u4f7f\u75287,000\u4e2a\u5fae\u8c03\u6837\u672c\u548c\u5927\u7ea655\u7f8e\u5143\u7684\u603b\u8bad\u7ec3\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e8658.2%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002"}}
{"id": "2505.09662", "pdf": "https://arxiv.org/pdf/2505.09662", "abs": "https://arxiv.org/abs/2505.09662", "authors": ["Philipp Schoenegger", "Francesco Salvi", "Jiacheng Liu", "Xiaoli Nan", "Ramit Debnath", "Barbara Fasolo", "Evelina Leivada", "Gabriel Recchia", "Fritz G\u00fcnther", "Ali Zarifhonarvar", "Joe Kwon", "Zahoor Ul Islam", "Marco Dehnert", "Daryl Y. H. Lee", "Madeline G. Reinecke", "David G. Kamper", "Mert Koba\u015f", "Adam Sandford", "Jonas Kgomo", "Luke Hewitt", "Shreya Kapoor", "Kerem Oktar", "Eyup Engin Kucuk", "Bo Feng", "Cameron R. Jones", "Izzy Gainsburg", "Sebastian Olschewski", "Nora Heinzelmann", "Francisco Cruz", "Ben M. Tappin", "Tao Ma", "Peter S. Park", "Rayan Onyonka", "Arthur Hjorth", "Peter Slattery", "Qingcheng Zeng", "Lennart Finke", "Igor Grossmann", "Alessandro Salatiello", "Ezra Karger"], "title": "Large Language Models Are More Persuasive Than Incentivized Human Persuaders", "categories": ["cs.CL", "I.2.7; H.1.2; K.4.1; H.5.2"], "comment": null, "summary": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u5728\u8bf4\u670d\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u5728\u8bf4\u670d\u80fd\u529b\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5e76\u4e14\u5728\u5f15\u5bfc\u7b54\u9898\u8005\u8d70\u5411\u6b63\u786e\u6216\u9519\u8bef\u7b54\u6848\u65f6\u4ea7\u751f\u4e86\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30AI\u5728\u8bf4\u670d\u80fd\u529b\u65b9\u9762\u662f\u5426\u8d85\u8fc7\u4eba\u7c7b\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9\u5b9e\u9645\u5e94\u7528\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u76f4\u63a5\u6bd4\u8f83\u4e86\u524d\u6cbf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff1bClaude Sonnet 3.5\uff09\u4e0e\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u5728\u4e92\u52a8\u3001\u5b9e\u65f6\u5bf9\u8bdd\u6d4b\u9a8c\u8bbe\u7f6e\u4e2d\u7684\u8bf4\u670d\u80fd\u529b\u3002\u5728\u4e00\u9879\u9884\u5148\u6ce8\u518c\u7684\u5927\u89c4\u6a21\u6fc0\u52b1\u5b9e\u9a8c\u4e2d\uff0c\u53c2\u4e0e\u8005\uff08\u6d4b\u9a8c\u7b54\u9898\u8005\uff09\u5b8c\u6210\u4e86\u4e00\u4e2a\u5728\u7ebf\u6d4b\u9a8c\uff0c\u5176\u4e2d\u8bf4\u670d\u8005\uff08\u4eba\u7c7b\u6216LLM\uff09\u8bd5\u56fe\u8bf4\u670d\u6d4b\u9a8c\u7b54\u9898\u8005\u8d70\u5411\u6b63\u786e\u6216\u9519\u8bef\u7684\u7b54\u6848\u3002", "result": "\u6211\u4eec\u53d1\u73b0LLM\u8bf4\u670d\u8005\u5728\u65b9\u5411\u6027\u8bf4\u670d\u5c1d\u8bd5\u4e2d\u83b7\u5f97\u4e86\u6bd4\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u66f4\u9ad8\u7684\u5408\u89c4\u6027\uff0c\u8fd9\u8868\u660e\u5728\u8bda\u5b9e\uff08\u5411\u6b63\u786e\u7b54\u6848\uff09\u548c\u6b3a\u9a97\uff08\u5411\u9519\u8bef\u7b54\u6848\uff09\u60c5\u5883\u4e0b\uff0cLLM\u5177\u6709\u4f18\u8d8a\u7684\u8bf4\u670d\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5f53\u5f15\u5bfc\u6d4b\u9a8c\u7b54\u9898\u8005\u8d70\u5411\u6b63\u786e\u7b54\u6848\u65f6\uff0cLLM\u8bf4\u670d\u8005\u663e\u8457\u63d0\u9ad8\u4e86\u7b54\u9898\u8005\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u9ad8\u7684\u6536\u76ca\uff1b\u800c\u5f53\u5f15\u5bfc\u4ed6\u4eec\u8d70\u5411\u9519\u8bef\u7b54\u6848\u65f6\uff0cLLM\u8bf4\u670d\u8005\u5219\u663e\u8457\u964d\u4f4e\u4e86\u4ed6\u4eec\u7684\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u6536\u76ca\u51cf\u5c11\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cAI\u7684\u8bf4\u670d\u80fd\u529b\u5df2\u7ecf\u8d85\u8fc7\u4e86\u90a3\u4e9b\u6709\u771f\u5b9e\u91d1\u94b1\u5956\u52b1\u7684\u771f\u4eba\u8bf4\u670d\u8005\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u53d1\u5c55\u65b0\u5174\u5bf9\u9f50\u548c\u6cbb\u7406\u6846\u67b6\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2505.09666", "pdf": "https://arxiv.org/pdf/2505.09666", "abs": "https://arxiv.org/abs/2505.09666", "authors": ["Yumin Choi", "Jinheon Baek", "Sung Ju Hwang"], "title": "System Prompt Optimization with Meta-Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\uff0c\u4f7f\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u5feb\u901f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u7279\u5b9a\u4e8e\u5355\u4e2a\u67e5\u8be2\u6216\u4efb\u52a1\u7684\u7528\u6237\u63d0\u793a\u4e0a\uff0c\u800c\u5ffd\u7565\u4e86\u53ef\u4ee5\u8de8\u4e0d\u540c\u4efb\u52a1\u548c\u9886\u57df\u5e94\u7528\u7684\u7cfb\u7edf\u63d0\u793a\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5404\u79cd\u7528\u6237\u63d0\u793a\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\uff0c\u540c\u65f6\u8fed\u4ee3\u66f4\u65b0\u7528\u6237\u63d0\u793a\u4ee5\u786e\u4fdd\u5b83\u4eec\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "result": "\u6211\u4eec\u572814\u4e2a\u8de85\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u6709\u6548\u6cdb\u5316\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u5e76\u4e14\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u63d0\u793a\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5728\u4e0d\u540c\u7528\u6237\u63d0\u793a\u4e0a\u8868\u73b0\u826f\u597d\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u4e5f\u80fd\u5feb\u901f\u9002\u5e94\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6d4b\u8bd5\u65f6\u7528\u6237\u63d0\u793a\u7684\u4f18\u5316\u6b65\u9aa4\u3002"}}
{"id": "2505.09639", "pdf": "https://arxiv.org/pdf/2505.09639", "abs": "https://arxiv.org/abs/2505.09639", "authors": ["Quentin Cohen-Solal"], "title": "Study and improvement of search algorithms in two-players perfect information games", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Games, in their mathematical sense, are everywhere (game industries,\neconomics, defense, education, chemistry, biology, ...).Search algorithms in\ngames are artificial intelligence methods for playing such games.\nUnfortunately, there is no study on these algorithms that evaluates the\ngenerality of their performance. We propose to address this gap in the case of\ntwo-player zero-sum games with perfect information. Furthermore, we propose a\nnew search algorithm and we show that, for a short search time, it outperforms\nall studied algorithms on all games in this large experiment and that, for a\nmedium search time, it outperforms all studied algorithms on 17 of the 22\nstudied games.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5e76\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u5728\u77ed\u65f6\u95f4\u548c\u4e2d\u7b49\u65f6\u95f4\u5185\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u5c1a\u65e0\u5bf9\u8fd9\u4e9b\u7b97\u6cd5\u7684\u901a\u7528\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8be5\u7814\u7a76\u9488\u5bf9\u5177\u6709\u5b8c\u7f8e\u4fe1\u606f\u7684\u4e24\u4eba\u96f6\u548c\u535a\u5f08\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "result": "\u5728\u77ed\u65f6\u95f4\u5185\uff0c\u65b0\u7b97\u6cd5\u5728\u6240\u6709\u6e38\u620f\u4e2d\u90fd\u4f18\u4e8e\u6240\u6709\u7814\u7a76\u8fc7\u7684\u7b97\u6cd5\uff1b\u5728\u4e2d\u7b49\u65f6\u95f4\u5185\uff0c\u5b83\u572822\u4e2a\u6e38\u620f\u4e2d\u768417\u4e2a\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u77ed\u65f6\u95f4\u641c\u7d22\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u6240\u6709\u6e38\u620f\u4e2d\u90fd\u4f18\u4e8e\u6240\u6709\u7814\u7a76\u8fc7\u7684\u7b97\u6cd5\uff0c\u5728\u4e2d\u7b49\u65f6\u95f4\u641c\u7d22\u4e2d\uff0c\u5b83\u572822\u4e2a\u7814\u7a76\u6e38\u620f\u4e2d\u768417\u4e2a\u4e2d\u4f18\u4e8e\u6240\u6709\u7814\u7a76\u8fc7\u7684\u7b97\u6cd5\u3002"}}
{"id": "2505.09659", "pdf": "https://arxiv.org/pdf/2505.09659", "abs": "https://arxiv.org/abs/2505.09659", "authors": ["Long Chen", "Xiaotian Song", "Yanan Sun"], "title": "LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Spiking Large Language Models (LLMs) have emerged as an energy-efficient\nalternative to conventional LLMs through their event-driven computation. To\neffectively obtain spiking LLMs, researchers develop different ANN-to-SNN\nconversion methods by leveraging pre-trained ANN parameters while inheriting\nthe energy efficiency of SNN. However, existing conversion methods struggle\nwith extreme activation outliers and incompatible nonlinear operations of\nANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for\nfully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel\nneurons to convert the activation outlier and nonlinear operation of ANN-based\nLLMs. Moreover, LAS tailors the spike-equivalent Transformer components for\nspiking LLMs, which can ensure full spiking conversion without any loss of\nperformance. Experimental results on six language models and two\nvision-language models demonstrate that LAS achieves loss-less conversion.\nNotably, on OPT-66B, LAS even improves the accuracy of 2\\% on the WSC task. In\naddition, the parameter and ablation studies further verify the effectiveness\nof LAS. The source code is available at https://github.com/lc783/LAS", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u635f\u5931\u7684ANN-SNN\u8f6c\u6362\u65b9\u6cd5\uff08LAS\uff09\uff0c\u7528\u4e8e\u5168\u8109\u51b2\u9a71\u52a8\u7684LLMs\u3002LAS\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u65b0\u578b\u795e\u7ecf\u5143\u548c\u5b9a\u5236\u7684\u8109\u51b2\u7b49\u6548Transformer\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e86\u65e0\u635f\u5931\u8f6c\u6362\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8f6c\u6362\u65b9\u6cd5\u5728\u5904\u7406\u57fa\u4e8eANN\u7684LLMs\u7684\u6781\u7aef\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u4e0d\u517c\u5bb9\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8f6c\u6362\u65b9\u6cd5\u6765\u5b9e\u73b0\u65e0\u635f\u5931\u7684\u8f6c\u6362\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u635f\u5931\u7684ANN-SNN\u8f6c\u6362\u65b9\u6cd5\uff0c\u79f0\u4e3aLAS\u3002LAS\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u578b\u795e\u7ecf\u5143\u6765\u8f6c\u6362\u57fa\u4e8eANN\u7684LLMs\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u5e76\u4e3a\u57fa\u4e8e\u8109\u51b2\u7684LLMs\u5b9a\u5236\u4e86\u7b49\u6548\u4e8e\u8109\u51b2\u7684Transformer\u7ec4\u4ef6\uff0c\u4ee5\u786e\u4fdd\u5b8c\u5168\u8109\u51b2\u8f6c\u6362\u800c\u4e0d\u4f1a\u6709\u4efb\u4f55\u6027\u80fd\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLAS\u5728\u516d\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u65e0\u635f\u5931\u8f6c\u6362\u3002\u5728OPT-66B\u4e0a\uff0cLAS\u751a\u81f3\u5728WSC\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e862%\u7684\u51c6\u786e\u7387\u3002\u53c2\u6570\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86LAS\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLAS\u5b9e\u73b0\u4e86\u65e0\u635f\u5931\u8f6c\u6362\uff0c\u5e76\u5728OPT-66B\u4e0a\u63d0\u9ad8\u4e86WSC\u4efb\u52a1\u7684\u51c6\u786e\u73872%\u3002\u53c2\u6570\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86LAS\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09746", "pdf": "https://arxiv.org/pdf/2505.09746", "abs": "https://arxiv.org/abs/2505.09746", "authors": ["Xabier Morales", "Ayah Elsayed", "Debbie Zhao", "Filip Loncaric", "Ainhoa Aguado", "Mireia Masias", "Gina Quill", "Marc Ramos", "Ada Doltra", "Ana Garcia", "Marta Sitges", "David Marlevi", "Alistair Young", "Martyn Nash", "Bart Bijnens", "Oscar Camara"], "title": "A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium", "categories": ["cs.CV"], "comment": null, "summary": "The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u4e2a\u9488\u5bf9\u5de6\u5fc3\u623f4D\u6d41\u52a8\u78c1\u5171\u632f\u6210\u50cf\u5206\u6790\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u9ad8\u7ea7\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u8fdb\u884c\u5168\u9762\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u4e2d\u5fc3\u6570\u636e\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u80fd\u91cf\u3001\u6da1\u5ea6\u548c\u538b\u529b\u53c2\u6570\u5728\u5404\u79cd\u75be\u75c5\u4e2d\u7684\u6f5c\u5728\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u4ef7\u503c\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u8d85\u58f0\u5206\u6790\u7684\u9650\u5236\uff0c\u5de6\u5fc3\u623f\u7684\u8840\u6d41\u52a8\u529b\u5b66\u7406\u89e3\u53d7\u5230\u663e\u8457\u9650\u5236\u30024D Flow MRI\u867d\u7136\u6709\u6f5c\u529b\u63d0\u9ad8\u5bf9\u5fc3\u623f\u8840\u6d41\u52a8\u529b\u5b66\u7684\u7406\u89e3\uff0c\u4f46\u7531\u4e8e\u4f4e\u901f\u548c\u6709\u9650\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u5206\u6790\u5de6\u5fc3\u623f\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u8ba1\u7b97\u6846\u67b6\u4ee5\u53ca\u4e0d\u540c\u7684\u91c7\u96c6\u534f\u8bae\u548c\u4f9b\u5e94\u5546\u4f7f\u5f97\u5927\u89c4\u6a21\u7814\u7a764D Flow MRI\u63d0\u4f9b\u7684\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u7684\u9884\u540e\u4ef7\u503c\u53d8\u5f97\u590d\u6742\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u67904D Flow MRI\u5728\u5de6\u5fc3\u623f\u4e2d\u7684\u5e94\u7528\u3002\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u81ea\u52a8\u5206\u5272\uff0c\u5e76\u5bf9\u80fd\u91cf\u3001\u6da1\u5ea6\u548c\u538b\u529b\u7b49\u53c2\u6570\u8fdb\u884c\u5206\u6790\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4e0d\u540c\u8d28\u91cf\u7684\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4ea7\u751f\u9ad8\u7cbe\u5ea6\u7684\u81ea\u52a8\u5206\u5272\uff08Dice > 0.9 \u548c Hausdorff 95 < 3 mm\uff09\u3002\u6b64\u5916\uff0c\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u80fd\u91cf\u3001\u6da1\u5ea6\u548c\u538b\u529b\u53c2\u6570\u5728\u5404\u79cd\u75be\u75c5\u4e2d\u7684\u6f5c\u5728\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u4ef7\u503c\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u4e2a\u9488\u5bf9\u5de6\u5fc3\u623f\uff08LA\uff094D\u6d41\u52a8\u78c1\u5171\u632f\u6210\u50cf\uff084D Flow MRI\uff09\u5206\u6790\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5bf9\u9ad8\u7ea7\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u8fdb\u884c\u5168\u9762\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u4e2d\u5fc3\u6570\u636e\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u80fd\u91cf\u3001\u6da1\u5ea6\u548c\u538b\u529b\u53c2\u6570\u5728\u5404\u79cd\u75be\u75c5\u4e2d\u7684\u6f5c\u5728\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u4ef7\u503c\u3002"}}
{"id": "2505.09701", "pdf": "https://arxiv.org/pdf/2505.09701", "abs": "https://arxiv.org/abs/2505.09701", "authors": ["Xin Liu", "Lechen Zhang", "Sheza Munir", "Yiyang Gu", "Lu Wang"], "title": "VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at generating long-form responses, but\nevaluating their factuality remains challenging due to complex inter-sentence\ndependencies within the generated facts. Prior solutions predominantly follow a\ndecompose-decontextualize-verify pipeline but often fail to capture essential\ncontext and miss key relational facts. In this paper, we introduce VeriFact, a\nfactuality evaluation framework designed to enhance fact extraction by\nidentifying and resolving incomplete and missing facts to support more accurate\nverification results. Moreover, we introduce FactRBench , a benchmark that\nevaluates both precision and recall in long-form model responses, whereas prior\nwork primarily focuses on precision. FactRBench provides reference fact sets\nfrom advanced LLMs and human-written answers, enabling recall assessment.\nEmpirical evaluations show that VeriFact significantly enhances fact\ncompleteness and preserves complex facts with critical relational information,\nresulting in more accurate factuality evaluation. Benchmarking various open-\nand close-weight LLMs on FactRBench indicate that larger models within same\nmodel family improve precision and recall, but high precision does not always\ncorrelate with high recall, underscoring the importance of comprehensive\nfactuality assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VeriFact\u4e8b\u5b9e\u8bc4\u4f30\u6846\u67b6\u548cFactRBench\u57fa\u51c6\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u4e8b\u5b9e\u5b8c\u6574\u6027\u548c\u5173\u7cfb\u4fe1\u606f\u65b9\u9762\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u7684\u4e8b\u5b9e\u4e2d\u5b58\u5728\u590d\u6742\u7684\u53e5\u5b50\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e8b\u5b9e\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4ee5\u524d\u7684\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u9075\u5faa\u5206\u89e3-\u53bb\u4e0a\u4e0b\u6587-\u9a8c\u8bc1\u7684\u6d41\u7a0b\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5230\u5173\u952e\u7684\u4e0a\u4e0b\u6587\u548c\u9057\u6f0f\u4e86\u91cd\u8981\u7684\u5173\u7cfb\u4e8b\u5b9e\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86VeriFact\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e8b\u5b9e\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u548c\u89e3\u51b3\u4e0d\u5b8c\u6574\u548c\u7f3a\u5931\u7684\u4e8b\u5b9e\u6765\u589e\u5f3a\u4e8b\u5b9e\u63d0\u53d6\uff0c\u4ee5\u652f\u6301\u66f4\u51c6\u786e\u7684\u9a8c\u8bc1\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86FactRBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u957f\u6587\u672c\u6a21\u578b\u54cd\u5e94\u4e2d\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u7684\u57fa\u51c6\uff0c\u800c\u4e4b\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u7cbe\u786e\u5ea6\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cVeriFact\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u5b8c\u6574\u6027\u5e76\u4fdd\u7559\u4e86\u5305\u542b\u5173\u952e\u5173\u7cfb\u4fe1\u606f\u7684\u590d\u6742\u4e8b\u5b9e\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u8bc4\u4f30\u3002\u5728FactRBench\u4e0a\u5bf9\u5404\u79cd\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u5927\u578b\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u90fd\u6709\u6240\u63d0\u9ad8\uff0c\u4f46\u9ad8\u7cbe\u5ea6\u5e76\u4e0d\u603b\u662f\u4e0e\u9ad8\u53ec\u56de\u7387\u76f8\u5173\uff0c\u8fd9\u7a81\u663e\u4e86\u5168\u9762\u4e8b\u5b9e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "conclusion": "VeriFact\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u5b8c\u6574\u6027\u5e76\u4fdd\u7559\u4e86\u5305\u542b\u5173\u952e\u5173\u7cfb\u4fe1\u606f\u7684\u590d\u6742\u4e8b\u5b9e\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u8bc4\u4f30\u3002\u5728FactRBench\u4e0a\u5bf9\u5404\u79cd\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u5927\u578b\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u90fd\u6709\u6240\u63d0\u9ad8\uff0c\u4f46\u9ad8\u7cbe\u5ea6\u5e76\u4e0d\u603b\u662f\u4e0e\u9ad8\u53ec\u56de\u7387\u76f8\u5173\uff0c\u8fd9\u7a81\u663e\u4e86\u5168\u9762\u4e8b\u5b9e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.09640", "pdf": "https://arxiv.org/pdf/2505.09640", "abs": "https://arxiv.org/abs/2505.09640", "authors": ["Tom\u00e1s Capdevielle", "Santiago Cifuentes"], "title": "Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms", "categories": ["cs.AI", "68T01", "I.2.0"], "comment": "22 pages, 7 figures", "summary": "Given a classification model and a prediction for some input, there are\nheuristic strategies for ranking features according to their importance in\nregard to the prediction. One common approach to this task is rooted in\npropositional logic and the notion of \\textit{sufficient reason}. Through this\nconcept, the categories of relevant and necessary features were proposed in\norder to identify the crucial aspects of the input. This paper improves the\nexisting techniques and algorithms for deciding which are the relevant and/or\nnecessary features, showing in particular that necessity can be detected\nefficiently in complex models such as neural networks. We also generalize the\nnotion of relevancy and study associated problems. Moreover, we present a new\nglobal notion (i.e. that intends to explain whether a feature is important for\nthe behavior of the model in general, not depending on a particular input) of\n\\textit{usefulness} and prove that it is related to relevancy and necessity.\nFurthermore, we develop efficient algorithms for detecting it in decision trees\nand other more complex models, and experiment on three datasets to analyze its\npractical utility.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u73b0\u6709\u6280\u672f\uff0c\u4ee5\u786e\u5b9a\u7279\u5f81\u7684\u76f8\u5173\u6027\u548c\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5168\u5c40\u6982\u5ff5\u201c\u6709\u7528\u6027\u201d\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6548\u7528\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u786e\u5b9a\u7279\u5f81\u7684\u91cd\u8981\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u6a21\u578b\u4e2d\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdb\u8fd9\u4e9b\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u6982\u5ff5\u6765\u66f4\u597d\u5730\u7406\u89e3\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002", "method": "\u672c\u6587\u57fa\u4e8e\u547d\u9898\u903b\u8f91\u548c\u201c\u5145\u5206\u7406\u7531\u201d\u7684\u6982\u5ff5\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u6280\u672f\u4e0e\u7b97\u6cd5\uff0c\u4ee5\u786e\u5b9a\u54ea\u4e9b\u7279\u5f81\u662f\u76f8\u5173\u7684\u548c/\u6216\u5fc5\u8981\u7684\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5168\u5c40\u6982\u5ff5\u201c\u6709\u7528\u6027\u201d\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u68c0\u6d4b\u51b3\u7b56\u6811\u548c\u5176\u4ed6\u66f4\u590d\u6742\u6a21\u578b\u4e2d\u7684\u6709\u7528\u6027\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u5fc5\u8981\u6027\u53ef\u4ee5\u5728\u590d\u6742\u7684\u6a21\u578b\u4e2d\u9ad8\u6548\u68c0\u6d4b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5168\u5c40\u6982\u5ff5\u201c\u6709\u7528\u6027\u201d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u5176\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "\u672c\u6587\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u6280\u672f\u4e0e\u7b97\u6cd5\uff0c\u4ee5\u786e\u5b9a\u54ea\u4e9b\u7279\u5f81\u662f\u76f8\u5173\u7684\u548c/\u6216\u5fc5\u8981\u7684\uff0c\u5e76\u5c55\u793a\u4e86\u5fc5\u8981\u6027\u53ef\u4ee5\u5728\u590d\u6742\u7684\u6a21\u578b\uff08\u5982\u795e\u7ecf\u7f51\u7edc\uff09\u4e2d\u9ad8\u6548\u68c0\u6d4b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5168\u5c40\u6982\u5ff5\u201c\u6709\u7528\u6027\u201d\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4e0e\u76f8\u5173\u6027\u548c\u5fc5\u8981\u6027\u6709\u5173\u3002"}}
{"id": "2505.09663", "pdf": "https://arxiv.org/pdf/2505.09663", "abs": "https://arxiv.org/abs/2505.09663", "authors": ["Julian B\u00fcchel", "Iason Chalas", "Giovanni Acampa", "An Chen", "Omobayode Fagbohungbe", "Sidney Tsai", "Kaoutar El Maghraoui", "Manuel Le Gallo", "Abbas Rahimi", "Abu Sebastian"], "title": "Analog Foundation Models", "categories": ["cs.LG"], "comment": "43 pages, 8 figures, under review", "summary": "Analog in-memory computing (AIMC) is a promising compute paradigm to improve\nspeed and power efficiency of neural network inference beyond the limits of\nconventional von Neumann-based architectures. However, AIMC introduces\nfundamental challenges such as noisy computations and strict constraints on\ninput and output quantization. Because of these constraints and imprecisions,\noff-the-shelf LLMs are not able to achieve 4-bit-level performance when\ndeployed on AIMC-based hardware. While researchers previously investigated\nrecovering this accuracy gap on small, mostly vision-based models, a generic\nmethod applicable to LLMs pre-trained on trillions of tokens does not yet\nexist. In this work, we introduce a general and scalable method to robustly\nadapt LLMs for execution on noisy, low-precision analog hardware. Our approach\nenables state-of-the-art models $\\unicode{x2013}$ including\nPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\\unicode{x2013}$ to retain\nperformance comparable to 4-bit weight, 8-bit activation baselines, despite the\npresence of analog noise and quantization constraints. Additionally, we show\nthat as a byproduct of our training methodology, analog foundation models can\nbe quantized for inference on low-precision digital hardware. Finally, we show\nthat our models also benefit from test-time compute scaling, showing better\nscaling behavior than models trained with 4-bit weight and 8-bit static input\nquantization. Our work bridges the gap between high-capacity LLMs and efficient\nanalog hardware, offering a path toward energy-efficient foundation models.\nCode is available at https://github.com/IBM/analog-foundation-models .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u5728\u566a\u58f0\u548c\u4f4e\u7cbe\u5ea6\u6a21\u62df\u786c\u4ef6\u4e0a\u6267\u884c\u3002\u8be5\u65b9\u6cd5\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5b58\u5728\u6a21\u62df\u566a\u58f0\u548c\u91cf\u5316\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u4e0e4\u4f4d\u6743\u91cd\u30018\u4f4d\u6fc0\u6d3b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6a21\u62df\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u88ab\u91cf\u5316\u4ee5\u5728\u4f4e\u7cbe\u5ea6\u6570\u5b57\u786c\u4ef6\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u4e14\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u597d\u7684\u884c\u4e3a\u3002", "motivation": "\u7531\u4e8e\u8fd9\u4e9b\u7ea6\u675f\u548c\u4e0d\u7cbe\u786e\u6027\uff0c\u73b0\u6210\u7684\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5728\u57fa\u4e8e\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u7684\u786c\u4ef6\u4e0a\u5b9e\u73b04\u4f4d\u7ea7\u522b\u7684\u6027\u80fd\u3002\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5c0f\u578b\u3001\u4e3b\u8981\u662f\u57fa\u4e8e\u89c6\u89c9\u7684\u6a21\u578b\u4e0a\uff0c\u4f46\u5c1a\u672a\u5b58\u5728\u9002\u7528\u4e8e\u9884\u8bad\u7ec3\u4e86\u6570\u4e07\u4ebf\u4e2a\u6807\u8bb0\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u4ee5\u7a33\u5065\u5730\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u5728\u566a\u58f0\u548c\u4f4e\u7cbe\u5ea6\u6a21\u62df\u786c\u4ef6\u4e0a\u6267\u884c\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5305\u62ecPhi-3-mini-4k-instruct\u548cLlama-3.2-1B-Instruct\uff09\u5728\u5b58\u5728\u6a21\u62df\u566a\u58f0\u548c\u91cf\u5316\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u4e0e4\u4f4d\u6743\u91cd\u30018\u4f4d\u6fc0\u6d3b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4f5c\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u7684\u526f\u4ea7\u54c1\uff0c\u6a21\u62df\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u88ab\u91cf\u5316\u4ee5\u5728\u4f4e\u7cbe\u5ea6\u6570\u5b57\u786c\u4ef6\u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u9762\u4e5f\u53d7\u76ca\uff0c\u663e\u793a\u51fa\u6bd4\u4f7f\u75284\u4f4d\u6743\u91cd\u548c8\u4f4d\u9759\u6001\u8f93\u5165\u91cf\u5316\u7684\u6a21\u578b\u66f4\u597d\u7684\u6269\u5c55\u884c\u4e3a\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u5f25\u5408\u4e86\u9ad8\u5bb9\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u548c\u9ad8\u6548\u6a21\u62df\u786c\u4ef6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8282\u80fd\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u8def\u5f84\u3002"}}
{"id": "2505.09827", "pdf": "https://arxiv.org/pdf/2505.09827", "abs": "https://arxiv.org/abs/2505.09827", "authors": ["Julian Tanke", "Takashi Shibuya", "Kengo Uchida", "Koichi Saito", "Yuki Mitsufuji"], "title": "Dyadic Mamba: Long-term Dyadic Human Motion Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 HuMoGen Workshop", "summary": "Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Dyadic Mamba\uff0c\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u53cc\u4eba\u4eba\u7c7b\u8fd0\u52a8\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fde\u63a5\u5728\u4e2a\u4f53\u8fd0\u52a8\u5e8f\u5217\u4e4b\u95f4\u4fc3\u8fdb\u4fe1\u606f\u6d41\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDyadic Mamba\u5728\u77ed\u671f\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5728\u66f4\u957f\u7684\u5e8f\u5217\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u957f\u671f\u8fd0\u52a8\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u4ece\u6587\u672c\u63cf\u8ff0\u751f\u6210\u73b0\u5b9e\u7684\u53cc\u4eba\u8fd0\u52a8\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8d85\u8fc7\u5178\u578b\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u7684\u957f\u65f6\u95f4\u4ea4\u4e92\u3002\u867d\u7136\u6700\u8fd1\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u5728\u77ed\u671f\u53cc\u4eba\u8fd0\u52a8\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u5728\u66f4\u957f\u7684\u5e8f\u5217\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86Dyadic Mamba\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u53cc\u4eba\u4eba\u7c7b\u8fd0\u52a8\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u8fde\u63a5\u5728\u4e2a\u4f53\u8fd0\u52a8\u5e8f\u5217\u4e4b\u95f4\u4fc3\u8fdb\u4fe1\u606f\u6d41\uff0c\u6d88\u9664\u4e86\u5bf9\u590d\u6742\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u7684\u9700\u6c42\u3002", "result": "\u6211\u4eec\u8bc1\u660e\uff0cDyadic Mamba\u5728\u6807\u51c6\u77ed\u671f\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u800c\u5728\u66f4\u957f\u7684\u5e8f\u5217\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u957f\u671f\u8fd0\u52a8\u5408\u6210\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u67b6\u6784\u4e3a\u89e3\u51b3\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u5408\u6210\u957f\u671f\u53cc\u4eba\u4eba\u7c7b\u8fd0\u52a8\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724", "abs": "https://arxiv.org/abs/2505.09724", "authors": ["Gino Carmona-D\u00edaz", "William Jim\u00e9nez-Leal", "Mar\u00eda Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Berm\u00fadez"], "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528LLMs\u8fdb\u884c\u6587\u672c\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u548c\u534f\u4f5c\u8fc7\u7a0b\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u5e94\u7528\u5206\u7c7b\u6cd5\u3002", "motivation": "\u5206\u6790\u6587\u672c\u5982\u5f00\u653e\u5f0f\u56de\u7b54\u3001\u6807\u9898\u6216\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u662f\u4e00\u4e2a\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u8fc7\u7a0b\uff0c\u5bb9\u6613\u53d7\u5230\u504f\u89c1\u7684\u5f71\u54cd\u3002LLMs\u662f\u7528\u4e8e\u6587\u672c\u5206\u6790\u7684\u6709\u524d\u9014\u7684\u5de5\u5177\u3002", "method": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9010\u6b65\u6559\u7a0b\uff0c\u901a\u8fc7\u7814\u7a76\u4eba\u5458\u548cLLMs\u4e4b\u95f4\u7684\u8fed\u4ee3\u548c\u534f\u4f5c\u8fc7\u7a0b\uff0c\u9ad8\u6548\u5730\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u5e94\u7528\u5206\u7c7b\u6cd5\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u7f16\u5199\u63d0\u793a\u6765\u5ba1\u67e5\u6570\u636e\u96c6\u5e76\u751f\u6210\u751f\u6d3b\u9886\u57df\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u548c\u76f4\u63a5\u4fee\u6539\u8bc4\u4f30\u548c\u4f18\u5316\u5206\u7c7b\u6cd5\uff0c\u6d4b\u8bd5\u5206\u7c7b\u6cd5\u5e76\u8bc4\u4f30\u7f16\u7801\u8005\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5e94\u7528\u5206\u7c7b\u6cd5\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\uff0c\u5177\u6709\u9ad8\u7f16\u7801\u8005\u95f4\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u6587\u672c\u5206\u6790\u7684\u53ef\u80fd\u6027\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2505.09737", "pdf": "https://arxiv.org/pdf/2505.09737", "abs": "https://arxiv.org/abs/2505.09737", "authors": ["Osher Elhadad", "Reuth Mirsky"], "title": "General Dynamic Goal Recognition", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted for publication at Generalization in Planning (GenPlan) as\n  part of AAAI 2025 workshops", "summary": "Understanding an agent's intent through its behavior is essential in\nhuman-robot interaction, interactive AI systems, and multi-agent\ncollaborations. This task, known as Goal Recognition (GR), poses significant\nchallenges in dynamic environments where goals are numerous and constantly\nevolving. Traditional GR methods, designed for a predefined set of goals, often\nstruggle to adapt to these dynamic scenarios. To address this limitation, we\nintroduce the General Dynamic GR problem - a broader definition of GR - aimed\nat enabling real-time GR systems and fostering further research in this area.\nExpanding on this foundation, this paper employs a model-free goal-conditioned\nRL approach to enable fast adaptation for GR across various changing tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u901a\u7528\u52a8\u6001\u76ee\u6807\u8bc6\u522b\uff08GR\uff09\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u65e0\u6a21\u578b\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u53d8\u5316\u4efb\u52a1\u7684GR\u3002", "motivation": "\u4f20\u7edfGR\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u96be\u4ee5\u9002\u5e94\uff0c\u56e0\u4e3a\u76ee\u6807\u4f17\u591a\u4e14\u4e0d\u65ad\u53d8\u5316\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5e7f\u6cdb\u7684GR\u5b9a\u4e49\u548c\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6a21\u578b\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u53d8\u5316\u4efb\u52a1\u7684GR\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u901a\u7528\u52a8\u6001GR\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u53d8\u5316\u4efb\u52a1\u7684GR\u3002", "conclusion": "\u672c\u6587\u7684\u5de5\u4f5c\u4e3a\u5b9e\u65f6GR\u7cfb\u7edf\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09702", "pdf": "https://arxiv.org/pdf/2505.09702", "abs": "https://arxiv.org/abs/2505.09702", "authors": ["Yezi Liu", "Prathyush Poduval", "Wenjun Huang", "Yang Ni", "Hanning Chen", "Mohsen Imani"], "title": "Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing", "categories": ["cs.LG"], "comment": null, "summary": "Graph unlearning is a crucial approach for protecting user privacy by erasing\nthe influence of user data on trained graph models. Recent developments in\ngraph unlearning methods have primarily focused on maintaining model prediction\nperformance while removing user information. However, we have observed that\nwhen user information is deleted from the model, the prediction distribution\nacross different sensitive groups often changes. Furthermore, graph models are\nshown to be prone to amplifying biases, making the study of fairness in graph\nunlearning particularly important. This raises the question: Does graph\nunlearning actually introduce bias? Our findings indicate that the predictions\nof post-unlearning models become highly correlated with sensitive attributes,\nconfirming the introduction of bias in the graph unlearning process. To address\nthis issue, we propose a fair graph unlearning method, FGU. To guarantee\nprivacy, FGU trains shard models on partitioned subgraphs, unlearns the\nrequested data from the corresponding subgraphs, and retrains the shard models\non the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing\nprocess: it first enables shard-level fairness by incorporating a fairness\nregularizer in the shard model retraining, and then achieves global-level\nfairness by aligning all shard models to minimize global disparity. Our\nexperiments demonstrate that FGU achieves superior fairness while maintaining\nprivacy and accuracy. Additionally, FGU is robust to diverse unlearning\nrequests, ensuring fairness and utility performance across various data\ndistributions.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u56fe\u5220\u9664\u662f\u5426\u4f1a\u5bfc\u81f4\u504f\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u7684\u56fe\u5220\u9664\u65b9\u6cd5FGU\u3002FGU\u901a\u8fc7\u5728\u5206\u5272\u7684\u5b50\u56fe\u4e0a\u8bad\u7ec3\u788e\u7247\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u53cc\u5c42\u53bb\u504f\u8fc7\u7a0b\u6765\u786e\u4fdd\u9690\u79c1\u548c\u516c\u5e73\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFGU\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u516c\u5e73\u6027\uff0c\u5e76\u4e14\u5bf9\u5404\u79cd\u5220\u9664\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u7528\u6237\u4fe1\u606f\u4ece\u6a21\u578b\u4e2d\u5220\u9664\u65f6\uff0c\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u4e4b\u95f4\u7684\u9884\u6d4b\u5206\u5e03\u5f80\u5f80\u4f1a\u6539\u53d8\u3002\u6b64\u5916\uff0c\u56fe\u6a21\u578b\u5bb9\u6613\u653e\u5927\u504f\u5dee\uff0c\u4f7f\u5f97\u7814\u7a76\u56fe\u5220\u9664\u4e2d\u7684\u516c\u5e73\u6027\u5c24\u4e3a\u91cd\u8981\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u56fe\u5220\u9664\u662f\u5426\u4f1a\u5f15\u5165\u504f\u5dee\uff1f", "method": "\u4e3a\u4e86\u4fdd\u8bc1\u9690\u79c1\uff0cFGU\u5728\u5206\u5272\u7684\u5b50\u56fe\u4e0a\u8bad\u7ec3\u788e\u7247\u6a21\u578b\uff0c\u4ece\u76f8\u5e94\u7684\u5b50\u56fe\u4e2d\u5220\u9664\u6240\u9700\u7684\u6570\u636e\uff0c\u5e76\u5728\u4fee\u6539\u540e\u7684\u5b50\u56fe\u4e0a\u91cd\u65b0\u8bad\u7ec3\u788e\u7247\u6a21\u578b\u3002\u4e3a\u4e86\u786e\u4fdd\u516c\u5e73\u6027\uff0cFGU\u91c7\u7528\u4e86\u4e00\u4e2a\u53cc\u5c42\u53bb\u504f\u8fc7\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u5728\u788e\u7247\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u4e2d\u5f15\u5165\u516c\u5e73\u6b63\u5219\u5316\u6765\u5b9e\u73b0\u788e\u7247\u7ea7\u522b\u7684\u516c\u5e73\u6027\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u9f50\u6240\u6709\u788e\u7247\u6a21\u578b\u4ee5\u6700\u5c0f\u5316\u5168\u5c40\u5dee\u5f02\u6765\u5b9e\u73b0\u5168\u5c40\u7ea7\u522b\u7684\u516c\u5e73\u6027\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5220\u9664\u540e\u7684\u6a21\u578b\u7684\u9884\u6d4b\u4e0e\u654f\u611f\u5c5e\u6027\u9ad8\u5ea6\u76f8\u5173\uff0c\u8bc1\u5b9e\u4e86\u56fe\u5220\u9664\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u504f\u5dee\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFGU\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u516c\u5e73\u6027\u3002\u6b64\u5916\uff0cFGU\u5bf9\u5404\u79cd\u5220\u9664\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u4e86\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.09829", "pdf": "https://arxiv.org/pdf/2505.09829", "abs": "https://arxiv.org/abs/2505.09829", "authors": ["Tushar Kataria", "Shireen Y. Elhabian"], "title": "BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes", "categories": ["cs.CV"], "comment": null, "summary": "Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u5229\u7528\u73b0\u6709\u6807\u6ce8\u7684\u7b80\u5355\u4f46\u6709\u6548\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5668\u5b98\u8fb9\u754c\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u6765\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8e\u4e25\u683c\u7684\u9690\u79c1\u6cd5\u89c4\u548c\u6570\u636e\u4fdd\u62a4\u653f\u7b56\uff0c\u83b7\u53d6\u5927\u89c4\u6a21\u533b\u7597\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u533b\u5b66\u56fe\u50cf\u7684\u6807\u6ce8\u9700\u8981\u9886\u57df\u4e13\u5bb6\u624b\u52a8\u63cf\u7ed8\u89e3\u5256\u7ed3\u6784\uff0c\u8fd9\u4f7f\u5f97\u8fc7\u7a0b\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u56e0\u6b64\uff0c\u534a\u76d1\u7763\u65b9\u6cd5\u56e0\u5176\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u800c\u53d7\u5230\u6b22\u8fce\u3002\u7136\u800c\uff0c\u534a\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u672a\u6807\u6ce8\u6570\u636e\u7684\u53ef\u7528\u6027\uff0c\u5f53\u8fd9\u4e9b\u6570\u636e\u7a00\u7f3a\u6216\u4e0d\u5b58\u5728\u65f6\uff0c\u5176\u6548\u679c\u4f1a\u4e0b\u964d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86BoundarySeg\uff0c\u4e00\u4e2a\u5c06\u5668\u5b98\u8fb9\u754c\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2a\u4efb\u52a1\u9884\u6d4b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u63d0\u4f9b\u989d\u5916\u76d1\u7763\u3002", "result": "\u8be5\u7b56\u7565\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u91cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u4e0e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u672a\u6807\u6ce8\u6570\u636e\u6216\u589e\u52a0\u8ba1\u7b97\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09738", "pdf": "https://arxiv.org/pdf/2505.09738", "abs": "https://arxiv.org/abs/2505.09738", "authors": ["Shaurya Sharthak", "Vinayak Pahalwan", "Adithya Kamath", "Adarsh Shirawalmath"], "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTokenAdapt\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6a21\u578b\u65e0\u5173\u7684\u5206\u8bcd\u5668\u79fb\u690d\u65b9\u6cd5\u548c\u9884\u5206\u8bcd\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u8bcd\u5668\u66ff\u6362\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e76\u4e14\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u4fdd\u7559\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u6216\u89e3\u51b3\u538b\u7f29\u6548\u7387\u95ee\u9898\u3002", "method": "TokenAdapt\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u521b\u65b0\uff1a\u4e00\u662f\u6a21\u578b\u65e0\u5173\u7684\u5206\u8bcd\u5668\u79fb\u690d\u65b9\u6cd5\uff0c\u4e8c\u662f\u7528\u4e8e\u591a\u8bcd\u8d85\u4ee4\u724c\u7684\u9884\u5206\u8bcd\u5b66\u4e60\u4ee5\u63d0\u9ad8\u538b\u7f29\u548c\u51cf\u5c11\u788e\u7247\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u4e24\u79cd\u8d21\u732e\uff1a\u79fb\u690d\u542f\u53d1\u5f0f\u65b9\u6cd5\u6210\u529f\u521d\u59cb\u5316\u4e86\u552f\u4e00\u6807\u8bb0\uff0c\u5e76\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u548c\u590d\u6742\u65b9\u6cd5\uff0c\u800c\u6211\u4eec\u7684\u8d85\u4ee4\u724c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u538b\u7f29\u589e\u76ca\u3002", "conclusion": "TokenAdapt\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u6216\u4e13\u4e1a\u5e94\u7528\u4e2d\u3002"}}
{"id": "2505.09755", "pdf": "https://arxiv.org/pdf/2505.09755", "abs": "https://arxiv.org/abs/2505.09755", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "title": "Explainability Through Human-Centric Design for XAI in Lung Cancer Detection", "categories": ["cs.AI"], "comment": null, "summary": "Deep learning models have shown promise in lung pathology detection from\nchest X-rays, but widespread clinical adoption remains limited due to opaque\nmodel decision-making. In prior work, we introduced ClinicXAI, a human-centric,\nexpert-guided concept bottleneck model (CBM) designed for interpretable lung\ncancer diagnosis. We now extend that approach and present XpertXAI, a\ngeneralizable expert-driven model that preserves human-interpretable clinical\nconcepts while scaling to detect multiple lung pathologies. Using a\nhigh-performing InceptionV3-based classifier and a public dataset of chest\nX-rays with radiology reports, we compare XpertXAI against leading post-hoc\nexplainability methods and an unsupervised CBM, XCBs. We assess explanations\nthrough comparison with expert radiologist annotations and medical ground\ntruth. Although XpertXAI is trained for multiple pathologies, our expert\nvalidation focuses on lung cancer. We find that existing techniques frequently\nfail to produce clinically meaningful explanations, omitting key diagnostic\nfeatures and disagreeing with radiologist judgments. XpertXAI not only\noutperforms these baselines in predictive accuracy but also delivers\nconcept-level explanations that better align with expert reasoning. While our\nfocus remains on explainability in lung cancer detection, this work illustrates\nhow human-centric model design can be effectively extended to broader\ndiagnostic contexts - offering a scalable path toward clinically meaningful\nexplainable AI in medical diagnostics.", "AI": {"tldr": "XpertXAI\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u591f\u68c0\u6d4b\u591a\u79cd\u80ba\u90e8\u75c5\u7406\uff0c\u5e76\u63d0\u4f9b\u4e0e\u4e13\u5bb6\u5224\u65ad\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u533b\u7597AI\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u5176\u5728\u4e34\u5e8a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u5230\u9650\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u6765\u63d0\u9ad8\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u3002", "method": "XpertXAI\u662f\u4e00\u79cd\u57fa\u4e8eInceptionV3\u7684\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u4e86\u4e13\u5bb6\u6307\u5bfc\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u4ee5\u4fdd\u6301\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u6982\u5ff5\u5e76\u68c0\u6d4b\u591a\u79cd\u80ba\u90e8\u75c5\u7406\u3002", "result": "XpertXAI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u4e0e\u4e13\u5bb6\u63a8\u7406\u66f4\u4e00\u81f4\u7684\u6982\u5ff5\u7ea7\u89e3\u91ca\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86XpertXAI\u6a21\u578b\u5728\u80ba\u90e8\u75c5\u7406\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u4eba\u7c7b\u4e2d\u5fc3\u6a21\u578b\u8bbe\u8ba1\u5728\u66f4\u5e7f\u6cdb\u8bca\u65ad\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.09704", "pdf": "https://arxiv.org/pdf/2505.09704", "abs": "https://arxiv.org/abs/2505.09704", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Charalampos Kalalas", "Paolo Dini"], "title": "Energy-Efficient Federated Learning for AIoT using Clustering Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While substantial research has been devoted to optimizing model performance,\nconvergence rates, and communication efficiency, the energy implications of\nfederated learning (FL) within Artificial Intelligence of Things (AIoT)\nscenarios are often overlooked in the existing literature. This study examines\nthe energy consumed during the FL process, focusing on three main\nenergy-intensive processes: pre-processing, communication, and local learning,\nall contributing to the overall energy footprint. We rely on the observation\nthat device/client selection is crucial for speeding up the convergence of\nmodel training in a distributed AIoT setting and propose two\nclustering-informed methods. These clustering solutions are designed to group\nAIoT devices with similar label distributions, resulting in clusters composed\nof nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity\noften encountered in real-world distributed learning applications. Throughout\nextensive numerical experimentation, we demonstrate that our clustering\nstrategies typically achieve high convergence rates while maintaining low\nenergy consumption when compared to other recent approaches available in the\nliterature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u4eba\u5de5\u667a\u80fd\u7269\u8054\u7f51\u573a\u666f\u4e2d\u7684\u80fd\u8017\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u6a21\u578b\u8bad\u7ec3\u7684\u6536\u655b\uff0c\u5e76\u5728\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6536\u655b\u7387\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u4e2d\u5e38\u5e38\u5ffd\u89c6\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u4eba\u5de5\u667a\u80fd\u7269\u8054\u7f51\uff08AIoT\uff09\u573a\u666f\u4e2d\u7684\u80fd\u6e90\u5f71\u54cd\uff0c\u800c\u8bbe\u5907/\u5ba2\u6237\u7aef\u9009\u62e9\u5bf9\u4e8e\u52a0\u901f\u5206\u5e03\u5f0fAIoT\u73af\u5883\u4e2d\u7684\u6a21\u578b\u8bad\u7ec3\u6536\u655b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u805a\u7c7b\u89e3\u51b3\u65b9\u6848\u65e8\u5728\u5c06\u5177\u6709\u76f8\u4f3c\u6807\u7b7e\u5206\u5e03\u7684AIoT\u8bbe\u5907\u5206\u7ec4\uff0c\u4ece\u800c\u5f62\u6210\u7531\u51e0\u4e4e\u5f02\u6784\u8bbe\u5907\u7ec4\u6210\u7684\u96c6\u7fa4\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u805a\u7c7b\u7b56\u7565\u5728\u4e0e\u5176\u4ed6\u6587\u732e\u4e2d\u53ef\u7528\u7684\u5176\u4ed6\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\u65f6\uff0c\u901a\u5e38\u80fd\u5b9e\u73b0\u9ad8\u6536\u655b\u7387\u5e76\u4fdd\u6301\u4f4e\u80fd\u8017\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u540c\u65f6\uff0c\u901a\u5e38\u80fd\u5b9e\u73b0\u8f83\u9ad8\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4f18\u4e8e\u6587\u732e\u4e2d\u5176\u4ed6\u6700\u8fd1\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.09858", "pdf": "https://arxiv.org/pdf/2505.09858", "abs": "https://arxiv.org/abs/2505.09858", "authors": ["Danush Kumar Venkatesh", "Isabel Funke", "Micha Pfeiffer", "Fiona Kolbinger", "Hanna Maria Schmeiser", "Juergen Weitz", "Marius Distler", "Stefanie Speidel"], "title": "Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models", "categories": ["cs.CV"], "comment": "Early accept at MICCAI 2025", "summary": "Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u6761\u4ef6\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u624b\u672f\u89c6\u9891\uff0c\u4ee5\u89e3\u51b3\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u7684\u4e25\u91cd\u6570\u636e\u4e0d\u5e73\u8861\u963b\u788d\u4e86\u9ad8\u6027\u80fd\u6a21\u578b\u7684\u53d1\u5c55\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u65e8\u5728\u901a\u8fc7\u5408\u6210\u624b\u672f\u89c6\u9891\u6765\u514b\u670d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7279\u7684\u4e24\u9636\u6bb5\u3001\u6587\u672c\u6761\u4ef6\u6269\u6563\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u624b\u672f\u89c6\u9891\u3002\u8be5\u65b9\u6cd5\u5229\u75282D\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6355\u6349\u7a7a\u95f4\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u6765\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u62d2\u7edd\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u9009\u62e9\u6700\u5408\u9002\u7684\u5408\u6210\u6837\u672c\u3002", "result": "\u6211\u4eec\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u2014\u2014\u624b\u672f\u52a8\u4f5c\u8bc6\u522b\u548c\u672f\u4e2d\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4ece\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\u83b7\u5f97\u7684\u5408\u6210\u89c6\u9891\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5408\u6210\u89c6\u9891\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.09794", "pdf": "https://arxiv.org/pdf/2505.09794", "abs": "https://arxiv.org/abs/2505.09794", "authors": ["J. Moreno-Casanova", "J. M. Au\u00f1\u00f3n", "A. M\u00e1rtinez-P\u00e9rez", "M. E. P\u00e9rez-Mart\u00ednez", "M. E. Gas-L\u00f3pez"], "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528NLP\u6280\u672f\uff0c\u7279\u522b\u662f\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\uff0c\u6765\u81ea\u52a8\u63d0\u53d6\u80ba\u764c\u548c\u4e73\u817a\u764c\u7684\u4e34\u5e8a\u4fe1\u606f\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u67d0\u4e9b\u5b9e\u4f53\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u624b\u52a8\u4ece\u4e34\u5e8a\u62a5\u544a\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u8fc7\u7a0b\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u51fa\u9519\uff0c\u9650\u5236\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u6548\u7387\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6570\u636e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86GMV\u7684NLP\u5de5\u5177uQuery\uff0c\u4ee5\u53ca\u57fa\u4e8eRoBERTa\u7684\u751f\u7269\u533b\u5b66\u8bed\u8a00\u6a21\u578bbsc-bio-ehr-en3\uff0c\u5e76\u901a\u8fc7Transformers\u67b6\u6784\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4ee5\u6267\u884c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cNLP\u6280\u672f\u5728\u81ea\u52a8\u63d0\u53d6\u80ba\u764c\u548c\u4e73\u817a\u764c\u7684\u4e34\u5e8a\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u50cfMET\u548cPAT\u8fd9\u6837\u7684\u5b9e\u4f53\u65b9\u9762\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8f83\u5c11\u89c1\u7684\u5b9e\u4f53\u5982EVOL\uff0c\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cNLP\u6280\u672f\u5728\u81ea\u52a8\u63d0\u53d6\u80ba\u764c\u548c\u4e73\u817a\u764c\u7684\u4e34\u5e8a\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u50cfMET\u548cPAT\u8fd9\u6837\u7684\u5b9e\u4f53\u65b9\u9762\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8f83\u5c11\u89c1\u7684\u5b9e\u4f53\u5982EVOL\uff0c\u4ecd\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2505.09787", "pdf": "https://arxiv.org/pdf/2505.09787", "abs": "https://arxiv.org/abs/2505.09787", "authors": ["Ziruo Yi", "Ting Xiao", "Mark V. Albert"], "title": "A Multimodal Multi-Agent Framework for Radiology Report Generation", "categories": ["cs.AI"], "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce diagnostic\nreports from medical images, with the potential to enhance clinical workflows\nand reduce radiologists' workload. While recent approaches leveraging\nmultimodal large language models (MLLMs) and retrieval-augmented generation\n(RAG) have achieved strong results, they continue to face challenges such as\nfactual inconsistency, hallucination, and cross-modal misalignment. We propose\na multimodal multi-agent framework for RRG that aligns with the stepwise\nclinical reasoning workflow, where task-specific agents handle retrieval, draft\ngeneration, visual analysis, refinement, and synthesis. Experimental results\ndemonstrate that our approach outperforms a strong baseline in both automatic\nmetrics and LLM-based evaluations, producing more accurate, structured, and\ninterpretable reports. This work highlights the potential of clinically aligned\nmulti-agent frameworks to support explainable and trustworthy clinical AI\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7528\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff0c\u8be5\u6846\u67b6\u4e0e\u9010\u6b65\u7684\u4e34\u5e8a\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b\u76f8\u4e00\u81f4\uff0c\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u3001\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u7684\u62a5\u544a\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u652f\u6301\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u7684\u4e34\u5e8aAI\u5e94\u7528\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u65e8\u5728\u4ece\u533b\u5b66\u56fe\u50cf\u4e2d\u81ea\u52a8\u751f\u6210\u8bca\u65ad\u62a5\u544a\uff0c\u5177\u6709\u589e\u5f3a\u4e34\u5e8a\u6d41\u7a0b\u548c\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u5de5\u4f5c\u91cf\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u4ecd\u7136\u9762\u4e34\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u9519\u4f4d\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7528\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff0c\u8be5\u6846\u67b6\u4e0e\u9010\u6b65\u7684\u4e34\u5e8a\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b\u76f8\u4e00\u81f4\uff0c\u5176\u4e2d\u7279\u5b9a\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u5904\u7406\u68c0\u7d22\u3001\u8349\u7a3f\u751f\u6210\u3001\u89c6\u89c9\u5206\u6790\u3001\u7cbe\u70bc\u548c\u7efc\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u6307\u6807\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u751f\u6210\u7684\u62a5\u544a\u66f4\u52a0\u51c6\u786e\u3001\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u4e34\u5e8a\u5bf9\u9f50\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u652f\u6301\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u7684\u4e34\u5e8aAI\u5e94\u7528\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09710", "pdf": "https://arxiv.org/pdf/2505.09710", "abs": "https://arxiv.org/abs/2505.09710", "authors": ["Konstantinos Fotopoulos", "Petros Maragos"], "title": "Training Deep Morphological Neural Networks as Universal Approximators", "categories": ["cs.LG"], "comment": null, "summary": "We investigate deep morphological neural networks (DMNNs). We demonstrate\nthat despite their inherent non-linearity, activations between layers are\nessential for DMNNs. We then propose several new architectures for DMNNs, each\nwith a different constraint on their parameters. For the first (resp. second)\narchitecture, we work under the constraint that the majority of parameters\n(resp. learnable parameters) should be part of morphological operations. We\nempirically show that our proposed networks can be successfully trained, and\nare more prunable than linear networks. To the best of our knowledge, we are\nthe first to successfully train DMNNs under such constraints, although the\ngeneralization capabilities of our networks remain limited. Finally, we propose\na hybrid network architecture combining linear and morphological layers,\nshowing empirically that the inclusion of morphological layers significantly\naccelerates the convergence of gradient descent with large batches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\uff0c\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u5e76\u5c55\u793a\u4e86\u5f62\u6001\u5c42\u5728\u52a0\u901f\u5927\u89c4\u6a21\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u4e86\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\uff0c\u5e76\u8bc1\u660e\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u5185\u5728\u7684\u975e\u7ebf\u6027\uff0c\u4f46\u5c42\u95f4\u7684\u6fc0\u6d3b\u5bf9\u4e8eDMNNs\u662f\u5fc5\u8981\u7684\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u7684DMNN\u67b6\u6784\uff0c\u6bcf\u79cd\u67b6\u6784\u90fd\u6709\u4e0d\u540c\u7684\u53c2\u6570\u7ea6\u675f\u3002\u7b2c\u4e00\uff08\u6216\u7b2c\u4e8c\uff09\u67b6\u6784\u5728\u591a\u6570\u53c2\u6570\uff08\u6216\u53ef\u5b66\u4e60\u53c2\u6570\uff09\u5e94\u5c5e\u4e8e\u5f62\u6001\u64cd\u4f5c\u7684\u7ea6\u675f\u4e0b\u5de5\u4f5c\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u6210\u529f\u8bad\u7ec3\uff0c\u5e76\u4e14\u6bd4\u7ebf\u6027\u7f51\u7edc\u66f4\u6613\u4e8e\u526a\u679d\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u5728\u8fd9\u4e9b\u7ea6\u675f\u4e0b\u6210\u529f\u8bad\u7ec3DMNNs\u7684\u56e2\u961f\uff0c\u5c3d\u7ba1\u6211\u4eec\u7684\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u548c\u5f62\u6001\u5c42\u7684\u6df7\u5408\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5f62\u6001\u5c42\u7684\u5f15\u5165\u663e\u8457\u52a0\u901f\u4e86\u5927\u89c4\u6a21\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u3002"}}
{"id": "2505.09859", "pdf": "https://arxiv.org/pdf/2505.09859", "abs": "https://arxiv.org/abs/2505.09859", "authors": ["Andrew Jun Lee", "Taylor Webb", "Trevor Bihl", "Keith Holyoak", "Hongjing Lu"], "title": "Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction", "categories": ["cs.CV"], "comment": "Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society", "summary": "The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Probabilistic Schema Induction (PSI)\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5728\u7ed3\u6784\u5316\u8868\u793a\u4e0a\u8fdb\u884c\u7c7b\u6bd4\u6620\u5c04\uff0c\u5f62\u6210\u7ec4\u5408\u6982\u5ff5\u3002PSI\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5b66\u4e60\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u4e24\u79cd\u5bf9\u7167\u6a21\u578b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u6784\u5316\u8868\u793a\u548c\u7c7b\u6bd4\u6620\u5c04\u5bf9\u4e8e\u6a21\u62df\u5feb\u901f\u7684\u4eba\u7c7b\u7c7b\u4f3c\u5b66\u4e60\u7ec4\u5408\u89c6\u89c9\u6982\u5ff5\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u4eba\u7c7b\u8ba4\u77e5\u7684\u4e00\u4e2a\u7279\u5f81\u662f\u80fd\u591f\u4ece\u6709\u9650\u7684\u4f8b\u5b50\u4e2d\u5b66\u4e60\u65b0\u7684\u89c6\u89c9\u6982\u5ff5\u3002\u4f20\u7edf\u7c7b\u522b\u5b66\u4e60\u6a21\u578b\u5c06\u6bcf\u4e2a\u4f8b\u5b50\u8868\u793a\u4e3a\u65e0\u7ed3\u6784\u7684\u7279\u5f81\u5411\u91cf\uff0c\u800c\u7ec4\u5408\u6982\u5ff5\u5b66\u4e60\u88ab\u8ba4\u4e3a\u4f9d\u8d56\u4e8e(1)\u4f8b\u5b50\u7684\u7ed3\u6784\u5316\u8868\u793a\u548c(2)\u901a\u8fc7\u7c7b\u6bd4\u6620\u5c04\u8bc6\u522b\u8de8\u4f8b\u5b50\u7684\u5171\u4eab\u5173\u7cfb\u7ed3\u6784\u3002", "method": "PSI\u6a21\u578b\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5728\u7ed3\u6784\u5316\u8868\u793a\u4e0a\u8fdb\u884c\u7c7b\u6bd4\u6620\u5c04\uff0c\u5f62\u6210\u79f0\u4e3a\u6a21\u5f0f\u7684\u7ec4\u5408\u6982\u5ff5\u3002\u5b83\u4f9d\u8d56\u4e8e\u4e00\u79cd\u65b0\u7684\u76f8\u4f3c\u6027\u6982\u5ff5\uff0c\u6743\u8861\u5bf9\u8c61\u7ea7\u76f8\u4f3c\u6027\u548c\u5173\u7cfb\u76f8\u4f3c\u6027\uff0c\u5e76\u5177\u6709\u589e\u5f3a\u4e0e\u5206\u7c7b\u76f8\u5173\u7684\u673a\u5236\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edf\u6a21\u578b\u4e2d\u7684\u9009\u62e9\u6027\u6ce8\u610f\u53c2\u6570\u3002", "result": "PSI\u4ea7\u751f\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5b66\u4e60\u8868\u73b0\uff0c\u5e76\u4f18\u4e8e\u4e24\u4e2a\u5bf9\u7167\u7ec4\uff1a\u4e00\u4e2a\u4f7f\u7528\u4ece\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u65e0\u7ed3\u6784\u7279\u5f81\u5411\u91cf\u7684\u539f\u578b\u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7ed3\u6784\u5316\u8868\u793a\u8f83\u5f31\u7684PSI\u53d8\u4f53\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPSI\u7684\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8868\u73b0\u662f\u7531\u4e00\u79cd\u9002\u5e94\u6027\u7b56\u7565\u9a71\u52a8\u7684\uff0c\u8be5\u7b56\u7565\u589e\u52a0\u4e86\u5173\u7cfb\u76f8\u4f3c\u6027\u76f8\u5bf9\u4e8e\u5bf9\u8c61\u7ea7\u76f8\u4f3c\u6027\u7684\u6743\u91cd\uff0c\u5e76\u63d0\u9ad8\u4e86\u533a\u5206\u7c7b\u522b\u7684\u5173\u7cfb\u7684\u8d21\u732e\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u7ed3\u6784\u5316\u8868\u793a\u548c\u7c7b\u6bd4\u6620\u5c04\u5bf9\u4e8e\u5efa\u6a21\u5feb\u901f\u7684\u4eba\u7c7b\u7c7b\u4f3c\u5b66\u4e60\u7ec4\u5408\u89c6\u89c9\u6982\u5ff5\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u521b\u5efa\u5fc3\u7406\u6a21\u578b\u3002"}}
{"id": "2505.09807", "pdf": "https://arxiv.org/pdf/2505.09807", "abs": "https://arxiv.org/abs/2505.09807", "authors": ["Timour Ichmoukhamedov", "David Martens"], "title": "Exploring the generalization of LLM truth directions on conversational formats", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86LLM\u4e2d\u7684\u771f\u5b9e\u65b9\u5411\u5728\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f\u4e4b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u5bf9\u8bdd\u672b\u5c3e\u6dfb\u52a0\u56fa\u5b9a\u5173\u952e\u8bcd\u77ed\u8bed\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "motivation": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cLLM\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5177\u6709\u666e\u904d\u7684\u771f\u5b9e\u65b9\u5411\uff0c\u771f\u5b9e\u548c\u865a\u5047\u9648\u8ff0\u662f\u7ebf\u6027\u53ef\u5206\u7684\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u771f\u5b9e\u65b9\u5411\u5728\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f\u4e4b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5728\u6bcf\u6bb5\u5bf9\u8bdd\u7684\u672b\u5c3e\u6dfb\u52a0\u4e00\u4e2a\u56fa\u5b9a\u7684\u5173\u952e\u8bcd\u77ed\u8bed\u6765\u63d0\u51fa\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u663e\u8457\u6539\u5584\u8fd9\u79cd\u6cdb\u5316\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u5bf9\u4e8e\u4ee5\u8c0e\u8a00\u7ed3\u675f\u7684\u7b80\u77ed\u5bf9\u8bdd\uff0c\u8fd9\u79cd\u771f\u5b9e\u65b9\u5411\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5bf9\u8f83\u957f\u7684\u5bf9\u8bdd\u683c\u5f0f\uff08\u5176\u4e2d\u8c0e\u8a00\u51fa\u73b0\u5728\u8f93\u5165\u63d0\u793a\u7684\u65e9\u671f\uff09\u5219\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u663e\u4e86\u5f00\u53d1\u80fd\u591f\u63a8\u5e7f\u5230\u65b0\u8bbe\u7f6e\u7684\u53ef\u9760LLM\u8c0e\u8a00\u68c0\u6d4b\u5668\u6240\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2505.09920", "pdf": "https://arxiv.org/pdf/2505.09920", "abs": "https://arxiv.org/abs/2505.09920", "authors": ["Shan Yang", "Yongli Zhu"], "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "This paper presents a study on using different offline reinforcement learning\nalgorithms for microgrid voltage regulation with solar power penetration. When\nenvironment interaction is unviable due to technical or safety reasons, the\nproposed approach can still obtain an applicable model through offline-style\ntraining on a previously collected dataset, lowering the negative impact of\nlacking online environment interactions. Experiment results on the IEEE 33-bus\nsystem demonstrate the feasibility and effectiveness of the proposed approach\non different offline datasets, including the one with merely low-quality\nexperience.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e0d\u540c\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5fae\u7535\u7f51\u7535\u538b\u8c03\u8282\uff0c\u5e76\u5728IEEE 33-bus\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u7531\u4e8e\u6280\u672f\u6216\u5b89\u5168\u539f\u56e0\u65e0\u6cd5\u8fdb\u884c\u73af\u5883\u4ea4\u4e92\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u83b7\u5f97\u9002\u7528\u7684\u6a21\u578b\uff0c\u4ee5\u964d\u4f4e\u7f3a\u4e4f\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e0d\u540c\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5fae\u7535\u7f51\u7535\u538b\u8c03\u8282\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u4ea4\u4e92\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5728\u4e4b\u524d\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u79bb\u7ebf\u8bad\u7ec3\u6765\u83b7\u5f97\u9002\u7528\u7684\u6a21\u578b\u3002", "result": "\u5728IEEE 33-bus\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u53ef\u884c\u6027\uff0c\u5e76\u4e14\u5728\u4ec5\u5305\u542b\u4f4e\u8d28\u91cf\u7ecf\u9a8c\u7684\u6570\u636e\u96c6\u4e0a\u4e5f\u6709\u6548\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u5305\u62ec\u4ec5\u5305\u542b\u4f4e\u8d28\u91cf\u7ecf\u9a8c\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2505.09716", "pdf": "https://arxiv.org/pdf/2505.09716", "abs": "https://arxiv.org/abs/2505.09716", "authors": ["George Dimitriadis. Spyridon Samothrakis"], "title": "Out-of-distribution generalisation is hard: evidence from ARC-like tasks", "categories": ["cs.LG", "cs.AI"], "comment": "Submission to NeurIPS 2025", "summary": "Out-of-distribution (OOD) generalisation is considered a hallmark of human\nand animal intelligence. To achieve OOD through composition, a system must\ndiscover the environment-invariant properties of experienced input-output\nmappings and transfer them to novel inputs. This can be realised if an\nintelligent system can identify appropriate, task-invariant, and composable\ninput features, as well as the composition methods, thus allowing it to act\nbased not on the interpolation between learnt data points but on the\ntask-invariant composition of those features. We propose that in order to\nconfirm that an algorithm does indeed learn compositional structures from data,\nit is not enough to just test on an OOD setup, but one also needs to confirm\nthat the features identified are indeed compositional. We showcase this by\nexploring two tasks with clearly defined OOD metrics that are not OOD solvable\nby three commonly used neural networks: a Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and a Transformer. In addition, we develop\ntwo novel network architectures imbued with biases that allow them to be\nsuccessful in OOD scenarios. We show that even with correct biases and almost\nperfect OOD performance, an algorithm can still fail to learn the correct\nfeatures for compositional generalisation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u9a8c\u8bc1\u7b97\u6cd5\u662f\u5426\u5b66\u4e60\u5230\u7ec4\u5408\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u65b0\u7f51\u7edc\u67b6\u6784\u4ee5\u63d0\u5347OOD\u6027\u80fd\uff0c\u4f46\u5373\u4f7f\u6709\u826f\u597d\u8868\u73b0\uff0c\u4e5f\u53ef\u80fd\u672a\u80fd\u5b66\u5230\u6b63\u786e\u7684\u7ec4\u5408\u7279\u5f81\u3002", "motivation": "\u4e3a\u4e86\u786e\u8ba4\u7b97\u6cd5\u662f\u5426\u771f\u6b63\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7ec4\u5408\u7ed3\u6784\uff0c\u4ec5\u5728OOD\u8bbe\u7f6e\u4e0a\u6d4b\u8bd5\u662f\u4e0d\u591f\u7684\uff0c\u8fd8\u9700\u8981\u9a8c\u8bc1\u6240\u8bc6\u522b\u7684\u7279\u5f81\u662f\u5426\u786e\u5b9e\u5177\u6709\u7ec4\u5408\u6027\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u4e24\u4e2a\u5177\u6709\u660e\u786eOOD\u6307\u6807\u7684\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e09\u79cd\u5e38\u7528\u7684\u795e\u7ecf\u7f51\u7edc\uff08MLP\u3001CNN\u548cTransformer\uff09\u7684OOD\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e24\u79cd\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u4ee5\u63d0\u9ad8OOD\u8868\u73b0\u3002", "result": "\u4e09\u79cd\u5e38\u7528\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u65e0\u6cd5\u89e3\u51b3OOD\u95ee\u9898\uff0c\u800c\u4e24\u79cd\u65b0\u63d0\u51fa\u7684\u7f51\u7edc\u67b6\u6784\u5728OOD\u573a\u666f\u4e2d\u8868\u73b0\u6210\u529f\u3002", "conclusion": "\u5373\u4f7f\u5728\u6b63\u786e\u7684\u504f\u5dee\u548c\u51e0\u4e4e\u5b8c\u7f8e\u7684OOD\u6027\u80fd\u4e0b\uff0c\u7b97\u6cd5\u4ecd\u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60\u5230\u6b63\u786e\u7684\u7279\u5f81\u4ee5\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u3002"}}
{"id": "2505.09915", "pdf": "https://arxiv.org/pdf/2505.09915", "abs": "https://arxiv.org/abs/2505.09915", "authors": ["Zhe Xin", "Chenyang Wu", "Penghui Huang", "Yanyong Zhang", "Yinian Mao", "Guoquan Huang"], "title": "Large-Scale Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.", "AI": {"tldr": "LSG-SLAM is a large-scale 3DGS-based visual SLAM with stereo cameras that improves robustness in large-scale outdoor scenarios through multi-modality strategies, feature-alignment warping constraints, continuous Gaussian Splatting submaps, and structure refinement.", "motivation": "Most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored.", "method": "LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. It introduces feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. Continuous Gaussian Splatting submaps are used to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition, and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. A structure refinement module enhances the reconstruction quality after global optimization of camera poses and Gaussian points.", "result": "LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches.", "conclusion": "LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches."}}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825", "abs": "https://arxiv.org/abs/2505.09825", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86KRISTEVA\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89e3\u91ca\u6027\u63a8\u7406\u7684\u7ec6\u8bfb\u57fa\u51c6\uff0c\u5305\u542b1331\u4e2a\u9009\u62e9\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u9010\u6b65\u589e\u52a0\u96be\u5ea6\u7684\u4efb\u52a1\u96c6\uff0c\u4ee5\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u5b66\u4f5c\u54c1\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u4e00\u4e9b\u5927\u5b66\u6c34\u5e73\u7684\u7ec6\u8bfb\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u7684\u8868\u73b0\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "motivation": "\u7531\u4e8e\u7ec6\u8bfb\u4ece\u672a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u8fc7\u8bc4\u4f30\uff0c\u4e14\u591a\u5b66\u79d1\u57fa\u51c6\u5982MMLU\u4e0d\u5305\u62ec\u6587\u5b66\u4f5c\u4e3a\u4e3b\u9898\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86KRISTEVA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89e3\u91ca\u6027\u63a8\u7406\u7684\u9996\u6b21\u7ec6\u8bfb\u57fa\u51c6\uff0c\u5305\u542b\u4ece\u8bfe\u5802\u6570\u636e\u4e2d\u6539\u7f16\u76841331\u4e2a\u9009\u62e9\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u9010\u6b65\u589e\u52a0\u96be\u5ea6\u7684\u4efb\u52a1\u96c6\uff0c\u4ee5\u8fd1\u4f3c\u7ec6\u8bfb\u8fc7\u7a0b\u7684\u4e0d\u540c\u5143\u7d20\uff0c\u5e76\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7406\u89e3\u6587\u5b66\u4f5c\u54c1\u3002", "result": "\u6211\u4eec\u7684\u57fa\u7ebf\u7ed3\u679c\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e00\u4e9b\u5927\u5b66\u6c34\u5e73\u7684\u7ec6\u8bfb\u80fd\u529b\uff08\u51c6\u786e\u738749.7% - 69.7%\uff09\uff0c\u4f46\u5b83\u4eec\u7684\u8868\u73b0\u4ecd\u843d\u540e\u4e8e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u8bc4\u4f30\u8005\u3002", "conclusion": "\u867d\u7136\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e00\u4e9b\u5927\u5b66\u6c34\u5e73\u7684\u7ec6\u8bfb\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u572811\u9879\u4efb\u52a1\u4e2d\u768410\u9879\u4e0a\u4ecd\u7136\u843d\u540e\u4e8e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u8bc4\u4f30\u8005\u3002"}}
{"id": "2505.09923", "pdf": "https://arxiv.org/pdf/2505.09923", "abs": "https://arxiv.org/abs/2505.09923", "authors": ["Minjung Shin", "Donghyun Kim", "Jeh-Kwang Ryu"], "title": "\"There Is No Such Thing as a Dumb Question,\" But There Are Good Ones", "categories": ["cs.AI"], "comment": "8 pages, 4 figures and 4 tables. This work has been accepted for\n  presentation as a poster with full paper publication at CogSci 2025. This is\n  the final submission", "summary": "Questioning has become increasingly crucial for both humans and artificial\nintelligence, yet there remains limited research comprehensively assessing\nquestion quality. In response, this study defines good questions and presents a\nsystematic evaluation framework. We propose two key evaluation dimensions:\nappropriateness (sociolinguistic competence in context) and effectiveness\n(strategic competence in goal achievement). Based on these foundational\ndimensions, a rubric-based scoring system was developed. By incorporating\ndynamic contextual variables, our evaluation framework achieves structure and\nflexibility through semi-adaptive criteria. The methodology was validated using\nthe CAUS and SQUARE datasets, demonstrating the ability of the framework to\naccess both well-formed and problematic questions while adapting to varied\ncontexts. As we establish a flexible and comprehensive framework for question\nevaluation, this study takes a significant step toward integrating questioning\nbehavior with structured analytical methods grounded in the intrinsic nature of\nquestioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u5168\u9762\u7684\u95ee\u9898\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\u2014\u2014\u9002\u5f53\u6027\u548c\u6709\u6548\u6027\uff0c\u7ed3\u5408\u52a8\u6001\u60c5\u5883\u53d8\u91cf\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u548c\u7075\u6d3b\u6027\u3002\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u80fd\u591f\u8bc4\u4f30\u5404\u79cd\u95ee\u9898\u5e76\u9002\u5e94\u4e0d\u540c\u60c5\u5883\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u95ee\u9898\u8d28\u91cf\u7684\u5168\u9762\u7814\u7a76\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u5b9a\u4e49\u597d\u95ee\u9898\u5e76\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff1a\u9002\u5f53\u6027\uff08\u60c5\u5883\u4e2d\u7684\u793e\u4f1a\u8bed\u8a00\u80fd\u529b\uff09\u548c\u6709\u6548\u6027\uff08\u76ee\u6807\u5b9e\u73b0\u7684\u6218\u7565\u80fd\u529b\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bc4\u5206\u91cf\u8868\u7684\u7cfb\u7edf\u3002\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u60c5\u5883\u53d8\u91cf\uff0c\u8bc4\u4f30\u6846\u67b6\u5b9e\u73b0\u4e86\u7ed3\u6784\u548c\u7075\u6d3b\u6027\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728CAUS\u548cSQUARE\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u826f\u597d\u5f62\u6210\u548c\u6709\u95ee\u9898\u7684\u95ee\u9898\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u7684\u60c5\u5883\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u5168\u9762\u7684\u95ee\u9898\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u5c06\u63d0\u95ee\u884c\u4e3a\u4e0e\u7ed3\u6784\u5316\u5206\u6790\u65b9\u6cd5\u76f8\u7ed3\u5408\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09733", "pdf": "https://arxiv.org/pdf/2505.09733", "abs": "https://arxiv.org/abs/2505.09733", "authors": ["Alpaslan Gokcen", "Ali Boyaci"], "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6e05\u6d17\u3001\u57fa\u4e8e\u6761\u4ef6GAN\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9c81\u68d2\u8bad\u7ec3\u6765\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u4fdd\u6301\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u65b9\u6848\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff08\u5982\u566a\u58f0\u6807\u7b7e\u3001\u7f3a\u5931\u7c7b\u522b\u548c\u4e0d\u5e73\u8861\u5206\u5e03\uff09\u663e\u8457\u5f71\u54cd\u5176\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cfb\u7edf\u6027\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u95ee\u9898\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u566a\u58f0\u6e05\u6d17\u3001\u57fa\u4e8e\u6761\u4ef6GAN\u7684\u534f\u4f5c\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9c81\u68d2\u7684\u8054\u90a6\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\uff08MNIST\u548cFashion-MNIST\uff09\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u4e0d\u540c\u566a\u58f0\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\uff0c\u8054\u90a6\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5b8fF1\u5206\u6570\u65b9\u9762\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5e38\u89c1\u7684\u6570\u636e\u8d28\u91cf\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u7684\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2505.09926", "pdf": "https://arxiv.org/pdf/2505.09926", "abs": "https://arxiv.org/abs/2505.09926", "authors": ["Bin-Bin Gao", "Yue Zhu", "Jiangtao Yan", "Yuezhi Cai", "Weixi Zhang", "Meng Wang", "Jun Liu", "Yong Liu", "Lei Wang", "Chengjie Wang"], "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 15 figures, 22 tables", "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.", "AI": {"tldr": "AdaptCLIP is a simple yet effective method for universal visual anomaly detection that outperforms existing methods by leveraging adaptive visual and textual representations and comparative learning.", "motivation": "Existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility.", "method": "AdaptCLIP is a simple yet effective method based on two key insights: adaptive visual and textual representations should be learned alternately rather than jointly, and comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters at its input or output ends.", "result": "AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. It achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains.", "conclusion": "AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods."}}
{"id": "2505.09852", "pdf": "https://arxiv.org/pdf/2505.09852", "abs": "https://arxiv.org/abs/2505.09852", "authors": ["Apollinaire Poli Nemkova", "Sarath Chandra Lingareddy", "Sagnik Ray Choudhury", "Mark V. Albert"], "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u51b2\u7a81\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5728\u6ca1\u6709\u5916\u90e8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u9884\u6d4b\uff0c\u4f46\u7ed3\u5408\u5916\u90e8\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5728\u9884\u6d4b\u66b4\u529b\u51b2\u7a81\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u652f\u6301\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3001\u4eba\u9053\u4e3b\u4e49\u89c4\u5212\u548c\u653f\u7b56\u5236\u5b9a\u3002", "method": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86LLMs\u7684\u53c2\u6570\u5316\u77e5\u8bc6\uff08\u4ec5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6743\u91cd\uff09\u4e0e\u975e\u53c2\u6570\u5316\u80fd\u529b\uff08\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210RAG\u8bbf\u95ee\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u6570\u636e\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u51b2\u7a81\u8d8b\u52bf\u548c\u6b7b\u4ea1\u4eba\u6570\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u4fe1\u606f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51b2\u7a81\u9884\u6d4b\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u8868\u660e\u901a\u8fc7\u7ed3\u6784\u5316\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u5b83\u4eec\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.09932", "pdf": "https://arxiv.org/pdf/2505.09932", "abs": "https://arxiv.org/abs/2505.09932", "authors": ["Kevin J McNamara", "Rhea Pritham Marpu"], "title": "Demystifying AI Agents: The Final Generation of Intelligence", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.MA"], "comment": null, "summary": "The trajectory of artificial intelligence (AI) has been one of relentless\nacceleration, evolving from rudimentary rule-based systems to sophisticated,\nautonomous agents capable of complex reasoning and interaction. This whitepaper\nchronicles this remarkable journey, charting the key technological\nmilestones--advancements in prompting, training methodologies, hardware\ncapabilities, and architectural innovations--that have converged to create the\nAI agents of today. We argue that these agents, exemplified by systems like\nOpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in\nAI development, potentially constituting the \"final generation\" of intelligence\nas we currently conceive it. We explore the capabilities and underlying\ntechnologies of these agents, grounded in practical examples, while also\nexamining the profound societal implications and the unprecedented pace of\nprogress that suggests intelligence is now doubling approximately every six\nmonths. The paper concludes by underscoring the critical need for wisdom and\nforesight in navigating the opportunities and challenges presented by this\npowerful new era of intelligence.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5206\u6790\u4e86\u5176\u5173\u952e\u6280\u672f\u8fdb\u6b65\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5bf9\u793e\u4f1a\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u540c\u65f6\u547c\u5401\u5728\u9762\u5bf9\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u65f6\u4fdd\u6301\u667a\u6167\u548c\u8fdc\u89c1\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u7684\u6700\u65b0\u8fdb\u5c55\u53ca\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u6307\u51fa\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u53ef\u80fd\u4ee3\u8868\u4e86\u6211\u4eec\u76ee\u524d\u6240\u80fd\u8bbe\u60f3\u7684\u2018\u6700\u7ec8\u4e00\u4ee3\u2019\u667a\u80fd\u3002", "method": "\u672c\u6587\u901a\u8fc7\u56de\u987e\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5206\u6790\u4e86\u5173\u952e\u6280\u672f\u91cc\u7a0b\u7891\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u548c\u6f5c\u5728\u5f71\u54cd\u3002", "result": "\u672c\u6587\u6307\u51fa\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u901f\u5ea6\u975e\u5e38\u5feb\uff0c\u667a\u80fd\u6c34\u5e73\u5927\u7ea6\u6bcf\u516d\u4e2a\u6708\u7ffb\u4e00\u756a\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u8fd9\u4e00\u65b0\u65f6\u4ee3\u4e2d\u9700\u8981\u8c28\u614e\u5bf9\u5f85\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u5e94\u5bf9\u4eba\u5de5\u667a\u80fd\u5e26\u6765\u7684\u673a\u9047\u548c\u6311\u6218\u65f6\uff0c\u9700\u8981\u667a\u6167\u548c\u8fdc\u89c1\u3002"}}
{"id": "2505.09742", "pdf": "https://arxiv.org/pdf/2505.09742", "abs": "https://arxiv.org/abs/2505.09742", "authors": ["Yuan-Hang Zhang", "Massimiliano Di Ventra"], "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.NE"], "comment": "15 pages, 3 figures", "summary": "We propose a generative, end-to-end solver for black-box combinatorial\noptimization that emphasizes both sample efficiency and solution quality on NP\nproblems. Drawing inspiration from annealing-based algorithms, we treat the\nblack-box objective as an energy function and train a neural network to model\nthe associated Boltzmann distribution. By conditioning on temperature, the\nnetwork captures a continuum of distributions--from near-uniform at high\ntemperatures to sharply peaked around global optima at low\ntemperatures--thereby learning the structure of the energy landscape and\nfacilitating global optimization. When queries are expensive, the\ntemperature-dependent distributions naturally enable data augmentation and\nimprove sample efficiency. When queries are cheap but the problem remains hard,\nthe model learns implicit variable interactions, effectively \"opening\" the\nblack box. We validate our approach on challenging combinatorial tasks under\nboth limited and unlimited query budgets, showing competitive performance\nagainst state-of-the-art black-box optimizers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u6027\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5f3a\u8c03\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u57fa\u4e8e\u9000\u706b\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u6a21\u62df\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u4ece\u800c\u5b66\u4e60\u80fd\u91cf\u666f\u89c2\u7684\u7ed3\u6784\u5e76\u4fc3\u8fdb\u5168\u5c40\u4f18\u5316\u3002\u5728\u67e5\u8be2\u6602\u8d35\u6216\u4fbf\u5b9c\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u90fd\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u67e5\u8be2\u6602\u8d35\u65f6\uff0c\u6e29\u5ea6\u4f9d\u8d56\u7684\u5206\u5e03\u81ea\u7136\u5730\u5b9e\u73b0\u4e86\u6570\u636e\u589e\u5f3a\u5e76\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff1b\u5f53\u67e5\u8be2\u4fbf\u5b9c\u4f46\u95ee\u9898\u4ecd\u7136\u56f0\u96be\u65f6\uff0c\u6a21\u578b\u5b66\u4e60\u9690\u5f0f\u53d8\u91cf\u4ea4\u4e92\uff0c\u6709\u6548\u5730'\u6253\u5f00'\u9ed1\u76d2\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u6027\u7684\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u5b83\u501f\u9274\u4e86\u57fa\u4e8e\u9000\u706b\u7684\u7b97\u6cd5\uff0c\u5c06\u9ed1\u76d2\u76ee\u6807\u89c6\u4e3a\u80fd\u91cf\u51fd\u6570\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u6a21\u62df\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u3002\u901a\u8fc7\u6761\u4ef6\u6e29\u5ea6\uff0c\u7f51\u7edc\u6355\u6349\u5230\u4e86\u4ece\u9ad8\u6e29\u4e0b\u7684\u8fd1\u4f3c\u5747\u5300\u5206\u5e03\u5230\u4f4e\u6e29\u4e0b\u56f4\u7ed5\u5168\u5c40\u6700\u4f18\u503c\u7684\u5c16\u9510\u5206\u5e03\u7684\u8fde\u7eed\u5206\u5e03\uff0c\u4ece\u800c\u5b66\u4e60\u4e86\u80fd\u91cf\u666f\u89c2\u7684\u7ed3\u6784\u5e76\u4fc3\u8fdb\u4e86\u5168\u5c40\u4f18\u5316\u3002", "result": "\u6211\u4eec\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u548c\u65e0\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6709\u9650\u548c\u65e0\u9650\u67e5\u8be2\u9884\u7b97\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u3002"}}
{"id": "2505.09927", "pdf": "https://arxiv.org/pdf/2505.09927", "abs": "https://arxiv.org/abs/2505.09927", "authors": ["Siqi Yin", "Shaolei Liu", "Manning Wang"], "title": "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u9002\u5e94\u3001\u6570\u636e\u4f9d\u8d56\u7684\u9891\u7387\u63d0\u793a\u548c\u98ce\u683c\u76f8\u5173\u7684\u5c42\u5fae\u8c03\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u6e90\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u548c\u57df\u95f4\u5dee\u5f02\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u540c\u65f6\u5728\u6709\u9650\u76d1\u7763\u4e0b\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u6548\u7387\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u9884\u9002\u5e94\u9636\u6bb5\uff0c\u751f\u6210\u4e00\u4e2a\u9884\u9002\u5e94\u6a21\u578b\uff0c\u4f5c\u4e3a\u76ee\u6807\u6a21\u578b\u7684\u521d\u59cb\u5316\uff0c\u5e76\u5141\u8bb8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u589e\u5f3a\u4f2a\u6807\u7b7e\u800c\u4e0d\u5f15\u5165\u989d\u5916\u53c2\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u4f9d\u8d56\u7684\u9891\u7387\u63d0\u793a\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5c06\u76ee\u6807\u57df\u56fe\u50cf\u8f6c\u6362\u4e3a\u6e90\u57df\u98ce\u683c\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u4e0e\u98ce\u683c\u76f8\u5173\u7684\u5c42\u5fae\u8c03\u7b56\u7565\u6765\u8bad\u7ec3\u76ee\u6807\u6a21\u578b\u3002", "result": "\u5728\u8de8\u6a21\u6001\u8179\u90e8\u548c\u5fc3\u810f\u65e0\u6e90\u57df\u9002\u5e94\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u57df\u9002\u5e94\uff08SFDA\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u548c\u57df\u95f4\u5dee\u5f02\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u8179\u90e8\u548c\u5fc3\u810fSFDA\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2505.09902", "pdf": "https://arxiv.org/pdf/2505.09902", "abs": "https://arxiv.org/abs/2505.09902", "authors": ["Martin Capdevila", "Esteban Villa Turek", "Ellen Karina Chumbe Fernandez", "Luis Felipe Polo Galvez", "Luis Cadavid", "Andrea Marroquin", "Rebeca Vargas Quesada", "Johanna Crew", "Nicole Vallejo Galarraga", "Christopher Rodriguez", "Diego Gutierrez", "Radhi Datla"], "title": "Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are, by definition, based on language. In an effort to\nunderscore the critical need for regional localized models, this paper examines\nprimary differences between variants of written Spanish across Latin America\nand Spain, with an in-depth sociocultural and linguistic contextualization\ntherein. We argue that these differences effectively constitute significant\ngaps in the quotidian use of Spanish among dialectal groups by creating\nsociolinguistic dissonances, to the extent that locale-sensitive AI models\nwould play a pivotal role in bridging these divides. In doing so, this approach\ninforms better and more efficient localization strategies that also serve to\nmore adequately meet inclusivity goals, while securing sustainable active daily\nuser growth in a major low-risk investment geographic area. Therefore,\nimplementing at least the proposed five sub variants of Spanish addresses two\nlines of action: to foment user trust and reliance on AI language models while\nalso demonstrating a level of cultural, historical, and sociolinguistic\nawareness that reflects positively on any internationalization strategy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u897f\u73ed\u7259\u8bed\u4e0d\u540c\u53d8\u4f53\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u5b9e\u65bd\u4e94\u4e2a\u897f\u73ed\u7259\u8bed\u5b50\u53d8\u4f53\u4ee5\u4fc3\u8fdb\u7528\u6237\u4fe1\u4efb\u548c\u56fd\u9645\u6218\u7565\u7684\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u5f3a\u8c03\u533a\u57df\u672c\u5730\u5316\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5dee\u5f02\u5728\u897f\u73ed\u7259\u8bed\u65b9\u8a00\u7fa4\u4f53\u7684\u65e5\u5e38\u4f7f\u7528\u4e2d\u9020\u6210\u4e86\u663e\u8457\u7684\u7a7a\u767d\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u793e\u4f1a\u8bed\u8a00\u4e0d\u548c\u8c10\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u62c9\u4e01\u7f8e\u6d32\u548c\u897f\u73ed\u7259\u7684\u4e66\u9762\u897f\u73ed\u7259\u8bed\u53d8\u4f53\u4e4b\u95f4\u7684\u4e3b\u8981\u5dee\u5f02\uff0c\u5e76\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u793e\u4f1a\u6587\u5316\u53ca\u8bed\u8a00\u80cc\u666f\u5206\u6790\u3002", "result": "\u8fd9\u79cd\u505a\u6cd5\u6709\u52a9\u4e8e\u5236\u5b9a\u66f4\u597d\u7684\u672c\u5730\u5316\u7b56\u7565\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u6ee1\u8db3\u5305\u5bb9\u6027\u76ee\u6807\uff0c\u5e76\u786e\u4fdd\u5728\u4f4e\u98ce\u9669\u6295\u8d44\u5730\u7406\u533a\u57df\u5185\u7684\u53ef\u6301\u7eed\u6d3b\u8dc3\u7528\u6237\u589e\u957f\u3002", "conclusion": "\u5b9e\u65bd\u81f3\u5c11\u63d0\u51fa\u7684\u4e94\u4e2a\u897f\u73ed\u7259\u8bed\u5b50\u53d8\u4f53\u53ef\u4ee5\u5b9e\u73b0\u4e24\u4e2a\u884c\u52a8\u65b9\u5411\uff1a\u4fc3\u8fdb\u7528\u6237\u5bf9\u4eba\u5de5\u667a\u80fd\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u4efb\u548c\u4f9d\u8d56\uff0c\u540c\u65f6\u5c55\u793a\u51fa\u53cd\u6620\u56fd\u9645\u6218\u7565\u79ef\u6781\u5f71\u54cd\u7684\u6587\u5316\u3001\u5386\u53f2\u548c\u793e\u4f1a\u8bed\u8a00\u610f\u8bc6\u3002"}}
{"id": "2505.09970", "pdf": "https://arxiv.org/pdf/2505.09970", "abs": "https://arxiv.org/abs/2505.09970", "authors": ["Mrinal Rawat", "Ambuje Gupta", "Rushil Goomer", "Alessandro Di Bari", "Neha Gupta", "Roberto Pieraccini"], "title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has\nbecome the foundation of modern agentic systems. Recent LLMs, such as\nDeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through\nthe generation of ample intermediate tokens, which help build a strong premise\nbefore producing the final output tokens. In this paper, we introduce Pre-Act,\na novel approach that enhances the agent's performance by creating a multi-step\nexecution plan along with the detailed reasoning for the given user input. This\nplan incrementally incorporates previous steps and tool outputs, refining\nitself after each step execution until the final response is obtained. Our\napproach is applicable to both conversational and non-conversational agents. To\nmeasure the performance of task-oriented agents comprehensively, we propose a\ntwo-level evaluation framework: (1) turn level and (2) end-to-end. Our\nturn-level evaluation, averaged across five models, shows that our approach,\nPre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While\nthis approach is effective for larger models, smaller models crucial for\npractical applications, where latency and cost are key constraints, often\nstruggle with complex reasoning tasks required for agentic systems. To address\nthis limitation, we fine-tune relatively small models such as Llama 3.1 (8B &\n70B) using the proposed Pre-Act approach. Our experiments show that the\nfine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action\naccuracy (turn-level) and a 28% improvement in goal completion rate\n(end-to-end) on the Almita (out-of-domain) dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Pre-Act \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u591a\u6b65\u9aa4\u6267\u884c\u8ba1\u5212\u548c\u8be6\u7ec6\u63a8\u7406\u6765\u589e\u5f3a\u4ee3\u7406\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPre-Act \u5728 Almita \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e ReAct\uff0c\u5e76\u4e14\u5728\u5fae\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u4ee3\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684 ReAct\uff08\u63a8\u7406+\u884c\u52a8\uff09\u80fd\u529b\u3002\u7136\u800c\uff0c\u8f83\u5c0f\u7684\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "Pre-Act \u901a\u8fc7\u521b\u5efa\u591a\u6b65\u9aa4\u6267\u884c\u8ba1\u5212\u5e76\u7ed3\u5408\u8be6\u7ec6\u63a8\u7406\u6765\u589e\u5f3a\u4ee3\u7406\u6027\u80fd\u3002\u8be5\u8ba1\u5212\u5728\u6bcf\u4e00\u6b65\u6267\u884c\u540e\u9010\u6b65\u6539\u8fdb\uff0c\u76f4\u5230\u83b7\u5f97\u6700\u7ec8\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u5c42\u8bc4\u4f30\u6846\u67b6\uff1a\uff081\uff09\u56de\u5408\u7ea7\u548c\uff082\uff09\u7aef\u5230\u7aef\u3002", "result": "Pre-Act \u5728 Almita \u6570\u636e\u96c6\u4e0a\u7684 Action Recall \u6307\u6807\u4e0a\u6bd4 ReAct \u63d0\u9ad8\u4e86 70%\u3002\u7ecf\u8fc7\u5fae\u8c03\u7684 70B \u6a21\u578b\u5728 Action Accuracy\uff08\u56de\u5408\u7ea7\uff09\u548c\u76ee\u6807\u5b8c\u6210\u7387\uff08\u7aef\u5230\u7aef\uff09\u4e0a\u5206\u522b\u4f18\u4e8e GPT-4 69.5% \u548c 28%\u3002", "conclusion": "Pre-Act \u662f\u4e00\u79cd\u6709\u6548\u7684\u589e\u5f3a\u4ee3\u7406\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.09756", "pdf": "https://arxiv.org/pdf/2505.09756", "abs": "https://arxiv.org/abs/2505.09756", "authors": ["Zhaoyang Shi"], "title": "Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration", "categories": ["cs.LG", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "We propose a new framework for multi-agent reinforcement learning (MARL),\nwhere the agents cooperate in a time-evolving network with latent community\nstructures and mixed memberships. Unlike traditional neighbor-based or fixed\ninteraction graphs, our community-based framework captures flexible and\nabstract coordination patterns by allowing each agent to belong to multiple\noverlapping communities. Each community maintains shared policy and value\nfunctions, which are aggregated by individual agents according to personalized\nmembership weights. We also design actor-critic algorithms that exploit this\nstructure: agents inherit community-level estimates for policy updates and\nvalue learning, enabling structured information sharing without requiring\naccess to other agents' policies. Importantly, our approach supports both\ntransfer learning by adapting to new agents or tasks via membership estimation,\nand active learning by prioritizing uncertain communities during exploration.\nTheoretically, we establish convergence guarantees under linear function\napproximation for both actor and critic updates. To our knowledge, this is the\nfirst MARL framework that integrates community structure, transferability, and\nactive learning with provable guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u793e\u533a\u7ed3\u6784\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u7684\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u6001\u7f51\u7edc\u548c\u6df7\u5408\u6210\u5458\u7ed3\u6784\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u548c\u62bd\u8c61\u7684\u534f\u8c03\u6a21\u5f0f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793e\u533a\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u53ef\u4ee5\u5c5e\u4e8e\u591a\u4e2a\u91cd\u53e0\u7684\u793e\u533a\uff0c\u6bcf\u4e2a\u793e\u533a\u7ef4\u62a4\u5171\u4eab\u7684\u7b56\u7565\u548c\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u4e2a\u6027\u5316\u7684\u6210\u5458\u6743\u91cd\u8fdb\u884c\u805a\u5408\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u652f\u6301\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u5e76\u5728\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u8fd1\u4f3c\u7684\u60c5\u51b5\u4e0b\u5efa\u7acb\u4e86\u6f14\u5458\u548c\u8bc4\u8bba\u5bb6\u66f4\u65b0\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u793e\u533a\u7ed3\u6784\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u5e76\u5177\u6709\u53ef\u8bc1\u660e\u7684\u4fdd\u8bc1\u3002"}}
{"id": "2505.09935", "pdf": "https://arxiv.org/pdf/2505.09935", "abs": "https://arxiv.org/abs/2505.09935", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Quoc Dai Tran"], "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VRU-CIPI\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u5e8f\u5217\u6ce8\u610f\u529b\u7684\u6a21\u578b\u9884\u6d4bVRU\u5728\u4ea4\u53c9\u53e3\u7684\u8fc7\u8857\u610f\u56fe\u3002\u901a\u8fc7\u4f7f\u7528GRU\u548c\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u5728UCF-VRU\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8696.45%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u96c6\u6210I2V\u901a\u4fe1\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4e3b\u52a8\u589e\u5f3a\u4ea4\u53c9\u53e3\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u7406\u89e3\u5e76\u9884\u6d4b\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u57ce\u5e02\u4ea4\u53c9\u53e3\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u9053\u8def\u4f7f\u7528\u8005\u4e4b\u95f4\u7684\u4e92\u52a8\u5b89\u5168\u6027\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002\u5176\u4e2d\u6700\u5173\u952e\u7684\u7684\u884c\u4e3a\u662f\u8106\u5f31\u9053\u8def\u7528\u6237\uff08VRUs\uff09\u7684\u8fc7\u8857\u610f\u56fe\uff0c\u8bef\u89e3\u53ef\u80fd\u5bfc\u81f4\u4e0e\u6765\u8f66\u53d1\u751f\u5371\u9669\u51b2\u7a81\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86VRU-CIPI\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u5e8f\u5217\u6ce8\u610f\u529b\u7684\u6a21\u578b\u6765\u9884\u6d4b\u4ea4\u53c9\u53e3\u7684VRU\u8fc7\u8857\u610f\u56fe\u3002VRU-CIPI\u4f7f\u7528\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u6355\u6349VRU\u8fd0\u52a8\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u7f16\u7801\u5bf9\u9884\u6d4b\u8fc7\u8857\u65b9\u5411\u81f3\u5173\u91cd\u8981\u7684\u4e0a\u4e0b\u6587\u548c\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728UCF-VRU\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u4e3a96.45%\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u8fbe\u5230\u6bcf\u79d233\u5e27\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u57fa\u7840\u8bbe\u65bd\u5230\u8f66\u8f86\uff08I2V\uff09\u901a\u4fe1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u53ca\u65f6\u6fc0\u6d3b\u8fc7\u8857\u4fe1\u53f7\u548c\u5411\u8054\u7f51\u8f66\u8f86\u63d0\u4f9b\u65e9\u671f\u8b66\u544a\u6765\u4e3b\u52a8\u589e\u5f3a\u4ea4\u53c9\u53e3\u7684\u5b89\u5168\u6027\uff0c\u786e\u4fdd\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u987a\u7545\u548c\u5b89\u5168\u4ea4\u4e92\u3002"}}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924", "abs": "https://arxiv.org/abs/2505.09924", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\n\\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u529f\u80fd\u7684\u5171\u751f\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408logits-based\u548csampling-based\u65b9\u6848\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6c34\u5370\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u6c34\u5370\u65b9\u6848\u5728\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408logits-based\u548csampling-based\u65b9\u6848\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u529f\u80fd\u7684\u5171\u751f\u6c34\u5370\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u7b56\u7565\uff1a\u4e32\u884c\u3001\u5e76\u884c\u548c\u6df7\u5408\u3002\u6df7\u5408\u6846\u67b6\u5229\u7528\u6807\u8bb0\u71b5\u548c\u8bed\u4e49\u71b5\u81ea\u9002\u5e94\u5730\u5d4c\u5165\u6c34\u5370\uff0c\u4ee5\u4f18\u5316\u53ef\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u591a\u6837\u5316\u7684\u6c34\u5370\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.10034", "pdf": "https://arxiv.org/pdf/2505.10034", "abs": "https://arxiv.org/abs/2505.10034", "authors": ["Changzeng Fu", "Zelin Fu", "Xinhe Kuang", "Jiacheng Dong", "Qi Zhang", "Kaifeng Su", "Yikai Su", "Wenbo Shi", "Junfeng Yao", "Yuliang Zhao", "Shiqi Zhao", "Jiadong Wang", "Siyang Song", "Chaoran Liu", "Yuichiro Yoshikawa", "Bj\u00f6rn Schuller", "Hiroshi Ishiguro"], "title": "The First MPDD Challenge: Multimodal Personality-aware Depression Detection", "categories": ["cs.AI", "68T07", "I.2.0; H.5.1"], "comment": "This paper has been accepted as part of the MPDD Challenge in the\n  ACMMM 2025 Grand Challenge", "summary": "Depression is a widespread mental health issue affecting diverse age groups,\nwith notable prevalence among college students and the elderly. However,\nexisting datasets and detection methods primarily focus on young adults,\nneglecting the broader age spectrum and individual differences that influence\ndepression manifestation. Current approaches often establish a direct mapping\nbetween multimodal data and depression indicators, failing to capture the\ncomplexity and diversity of depression across individuals. This challenge\nincludes two tracks based on age-specific subsets: Track 1 uses the\nMPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses\nthe MPDD-Young dataset for detecting depression in younger participants. The\nMultimodal Personality-aware Depression Detection (MPDD) Challenge aims to\naddress this gap by incorporating multimodal data alongside individual\ndifference factors. We provide a baseline model that fuses audio and video\nmodalities with individual difference information to detect depression\nmanifestations in diverse populations. This challenge aims to promote the\ndevelopment of more personalized and accurate de pression detection methods,\nadvancing mental health research and fostering inclusive detection systems.\nMore details are available on the official challenge website:\nhttps://hacilab.github.io/MPDDChallenge.github.io.", "AI": {"tldr": "\u8be5\u6311\u6218\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u4e2a\u4f53\u5dee\u5f02\u56e0\u7d20\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\u5728\u5e74\u9f84\u548c\u4e2a\u4f53\u5dee\u5f02\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u4fc3\u8fdb\u66f4\u4e2a\u6027\u5316\u548c\u51c6\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e74\u8f7b\u4eba\uff0c\u5ffd\u7565\u4e86\u5f71\u54cd\u6291\u90c1\u75c7\u8868\u73b0\u7684\u66f4\u5e7f\u6cdb\u5e74\u9f84\u8303\u56f4\u548c\u4e2a\u4f53\u5dee\u5f02\u3002\u5f53\u524d\u65b9\u6cd5\u5f80\u5f80\u5efa\u7acb\u591a\u6a21\u6001\u6570\u636e\u4e0e\u6291\u90c1\u6307\u6807\u4e4b\u95f4\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u672a\u80fd\u6355\u6349\u5230\u4e2a\u4f53\u4e4b\u95f4\u6291\u90c1\u75c7\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u878d\u5408\u4e86\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\u4e0e\u4e2a\u4f53\u5dee\u5f02\u4fe1\u606f\uff0c\u4ee5\u68c0\u6d4b\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u6291\u90c1\u75c7\u8868\u73b0\u3002", "result": "\u8be5\u6311\u6218\u5305\u62ec\u4e24\u4e2a\u57fa\u4e8e\u5e74\u9f84\u7279\u5b9a\u5b50\u96c6\u7684\u8f68\u9053\uff1a\u8f68\u90531\u4f7f\u7528MPDD-Elderly\u6570\u636e\u96c6\u68c0\u6d4b\u8001\u5e74\u4eba\u7684\u6291\u90c1\u75c7\uff0c\u8f68\u90532\u4f7f\u7528MPDD-Young\u6570\u636e\u96c6\u68c0\u6d4b\u5e74\u8f7b\u53c2\u4e0e\u8005\u7684\u6291\u90c1\u75c7\u3002", "conclusion": "\u8be5\u6311\u6218\u65e8\u5728\u4fc3\u8fdb\u66f4\u4e2a\u6027\u5316\u548c\u51c6\u786e\u7684\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u63a8\u52a8\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u5e76\u4fc3\u8fdb\u5305\u5bb9\u6027\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2505.09768", "pdf": "https://arxiv.org/pdf/2505.09768", "abs": "https://arxiv.org/abs/2505.09768", "authors": ["Xiukun Wei", "Xueru Zhang"], "title": "Self-Consuming Generative Models with Adversarially Curated Data", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in generative models have made it increasingly difficult to\ndistinguish real data from model-generated synthetic data. Using synthetic data\nfor successive training of future model generations creates \"self-consuming\nloops\", which may lead to model collapse or training instability. Furthermore,\nsynthetic data is often subject to human feedback and curated by users based on\ntheir preferences. Ferbach et al. (2024) recently showed that when data is\ncurated according to user preferences, the self-consuming retraining loop\ndrives the model to converge toward a distribution that optimizes those\npreferences. However, in practice, data curation is often noisy or\nadversarially manipulated. For example, competing platforms may recruit\nmalicious users to adversarially curate data and disrupt rival models. In this\npaper, we study how generative models evolve under self-consuming retraining\nloops with noisy and adversarially curated data. We theoretically analyze the\nimpact of such noisy data curation on generative models and identify conditions\nfor the robustness of the retraining process. Building on this analysis, we\ndesign attack algorithms for competitive adversarial scenarios, where a\nplatform with a limited budget employs malicious users to misalign a rival's\nmodel from actual user preferences. Experiments on both synthetic and\nreal-world datasets demonstrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.09939", "pdf": "https://arxiv.org/pdf/2505.09939", "abs": "https://arxiv.org/abs/2505.09939", "authors": ["Zhe Shan", "Lei Zhou", "Liu Mao", "Shaofan Chen", "Chuanqiu Ren", "Xia Xie"], "title": "Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IGARSS 2025", "summary": "In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u2014\u2014\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u6f5c\u5728\u5371\u5bb3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e5\u76ca\u589e\u52a0\u7684\u7d27\u6025\u60c5\u51b5\uff08\u5982\u81ea\u7136\u707e\u5bb3\u3001\u4eba\u4e3a\u4e8b\u6545\u548c\u519b\u4e8b\u6253\u51fb\uff09\u5e26\u6765\u7684\u6311\u6218\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u65b0\u7684\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u2014\u2014\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u9488\u5bf9\u5404\u79cd\u573a\u666f\u7684\u4e0d\u540c\u56fe\u50cf\u53d8\u6362\u65b9\u6848\uff0c\u5c06\u73b0\u6709\u7684\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u975e\u6ce8\u518c\u7248\u672c\u3002", "result": "\u6211\u4eec\u7cfb\u7edf\u5730\u63d0\u51fa\u4e86\u516b\u4e2a\u53ef\u80fd\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u51fa\u73b0\u5e76\u53ef\u80fd\u5bfc\u81f4\u975e\u6ce8\u518c\u95ee\u9898\u7684\u60c5\u666f\uff0c\u5e76\u5c55\u793a\u4e86\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u5bf9\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u7834\u574f\u6027\u5f71\u54cd\u3002", "conclusion": "\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u53ef\u80fd\u5bfc\u81f4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9020\u6210\u707e\u96be\u6027\u635f\u5bb3\u3002"}}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930", "abs": "https://arxiv.org/abs/2505.09930", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Prompt optimization (PO) offers a practical alternative to fine-tuning large\nlanguage models (LLMs), enabling performance improvements without altering\nmodel weights. Existing methods typically rely on advanced, large-scale LLMs\nlike GPT-4 to generate optimized prompts. However, due to limited downward\ncompatibility, verbose, instruction-heavy prompts from advanced LLMs can\noverwhelm lightweight inference models and degrade response quality. In this\nwork, we rethink prompt optimization through the lens of interpretable design.\nWe first identify a set of model-agnostic prompt quality merits and empirically\nvalidate their effectiveness in enhancing prompt and response quality. We then\nintroduce MePO, a merit-guided, lightweight, and locally deployable prompt\noptimizer trained on our preference dataset built from merit-aligned prompts\ngenerated by a lightweight LLM. Unlike prior work, MePO avoids online\noptimization reliance, reduces cost and privacy concerns, and, by learning\nclear, interpretable merits, generalizes effectively to both large-scale and\nlightweight inference models. Experiments demonstrate that MePO achieves better\nresults across diverse tasks and model types, offering a scalable and robust\nsolution for real-world deployment. Our model and dataset are available at:\nhttps://github.com/MidiyaZhu/MePO", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5MePO\uff0c\u5b83\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u63d0\u793a\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5148\u8fdb\u7684\u5927\u578bLLM\uff08\u5982GPT-4\uff09\u6765\u751f\u6210\u4f18\u5316\u7684\u63d0\u793a\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5411\u4e0b\u517c\u5bb9\u6027\u6709\u9650\uff0c\u6765\u81ea\u5148\u8fdbLLM\u7684\u5197\u957f\u3001\u6307\u4ee4\u5bc6\u96c6\u578b\u63d0\u793a\u53ef\u80fd\u4f1a\u4f7f\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u578b\u8fc7\u8f7d\u5e76\u964d\u4f4e\u54cd\u5e94\u8d28\u91cf\u3002", "method": "\u6211\u4eec\u9996\u5148\u786e\u5b9a\u4e86\u4e00\u7ec4\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u63d0\u793a\u8d28\u91cf\u4f18\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u63d0\u9ad8\u63d0\u793a\u548c\u54cd\u5e94\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86MePO\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u4f18\u70b9\u5f15\u5bfc\u7684\u8f7b\u91cf\u7ea7\u3001\u672c\u5730\u53ef\u90e8\u7f72\u7684\u63d0\u793a\u4f18\u5316\u5668\uff0c\u5b83\u5728\u6211\u4eec\u4ece\u7531\u8f7b\u91cf\u7ea7LLM\u751f\u6210\u7684\u5bf9\u9f50\u4f18\u70b9\u63d0\u793a\u6784\u5efa\u7684\u504f\u597d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMePO\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cMePO\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u53ef\u5728https://github.com/MidiyaZhu/MePO\u83b7\u53d6\u3002"}}
{"id": "2505.10074", "pdf": "https://arxiv.org/pdf/2505.10074", "abs": "https://arxiv.org/abs/2505.10074", "authors": ["Mohamed Abdelmagied", "Mohamed Amine Chatti", "Shoeb Joarder", "Qurat Ul Ain", "Rawaa Alatrash"], "title": "Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs", "categories": ["cs.AI", "cs.CY"], "comment": "Accepted at EMOOCs 2025", "summary": "Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.", "AI": {"tldr": "This paper proposes a Graph RAG pipeline that uses Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to help learners understand new knowledge concepts in MOOCs. The approach includes generating personalized questions based on PKGs and answering questions using EduKGs. Evaluation with expert instructors shows the potential of this method.", "motivation": "MOOCs lack direct interaction between learners and instructors, making it challenging for learners to understand new knowledge concepts. LLMs are prone to hallucinations which limits their reliability. Current RAG systems do not actively guide learners toward their learning needs.", "method": "We propose a Graph RAG pipeline that leverages Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide learners to understand knowledge concepts in the MOOC platform CourseMapper. Specifically, we implement (1) a PKG-based Question Generation method to recommend personalized questions for learners in context, and (2) an EduKG-based Question Answering method that leverages the relationships between knowledge concepts in the EduKG to answer learner selected questions.", "result": "The results of the evaluation show the potential of Graph RAG to empower learners to understand new knowledge concepts in a personalized learning experience.", "conclusion": "Graph RAG has the potential to empower learners to understand new knowledge concepts in a personalized learning experience."}}
{"id": "2505.09792", "pdf": "https://arxiv.org/pdf/2505.09792", "abs": "https://arxiv.org/abs/2505.09792", "authors": ["Michael Kamfonas"], "title": "Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints", "categories": ["cs.LG"], "comment": null, "summary": "This case study applies a phased hyperparameter optimization process to\ncompare multitask natural language model variants that utilize multiphase\nlearning rate scheduling and optimizer parameter grouping. We employ short,\nBayesian optimization sessions that leverage multi-fidelity, hyperparameter\nspace pruning, progressive halving, and a degree of human guidance. We utilize\nthe Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn\nGaussian process minimization. Initially, we use efficient low-fidelity sprints\nto prune the hyperparameter space. Subsequent sprints progressively increase\ntheir model fidelity and employ hyperband pruning for efficiency. A second\naspect of our approach is using a meta-learner to tune threshold values to\nresolve classification probabilities during inference. We demonstrate our\nmethod on a collection of variants of the 2021 Joint Entity and Relation\nExtraction model proposed by Eberts and Ulges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u7684\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83\u5229\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u4f18\u5316\u5668\u53c2\u6570\u5206\u7ec4\u7684\u591a\u4efb\u52a1\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\uff0c\u5e76\u57282021\u5e74Eberts\u548cUlges\u63d0\u51fa\u7684\u8054\u5408\u5b9e\u4f53\u548c\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u53d8\u4f53\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u9636\u6bb5\u7684\u8d85\u53c2\u6570\u4f18\u5316\u8fc7\u7a0b\uff0c\u6bd4\u8f83\u4e0d\u540c\u591a\u4efb\u52a1\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u5e76\u5229\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u4f18\u5316\u5668\u53c2\u6570\u5206\u7ec4\u6765\u63d0\u5347\u6a21\u578b\u6548\u679c\u3002", "method": "\u672c\u6587\u5e94\u7528\u4e86\u5206\u9636\u6bb5\u7684\u8d85\u53c2\u6570\u4f18\u5316\u8fc7\u7a0b\uff0c\u6bd4\u8f83\u4e86\u5229\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u4f18\u5316\u5668\u53c2\u6570\u5206\u7ec4\u7684\u591a\u4efb\u52a1\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u3002\u91c7\u7528\u77ed\u65f6\u95f4\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u4f1a\u8bdd\uff0c\u5229\u7528\u591a\u4fdd\u771f\u5ea6\u3001\u8d85\u53c2\u6570\u7a7a\u95f4\u526a\u679d\u3001\u6e10\u8fdb\u5f0f\u51cf\u534a\u548c\u4e00\u5b9a\u7a0b\u5ea6\u7684\u4eba\u7c7b\u6307\u5bfc\u3002\u4f7f\u7528Optuna TPE\u91c7\u6837\u5668\u548cHyperband\u526a\u679d\u5668\u4ee5\u53caScikit-Learn\u9ad8\u65af\u8fc7\u7a0b\u6700\u5c0f\u5316\u3002\u9996\u5148\u4f7f\u7528\u9ad8\u6548\u7684\u4f4e\u4fdd\u771f\u51b2\u523a\u6765\u526a\u679d\u8d85\u53c2\u6570\u7a7a\u95f4\u3002\u540e\u7eed\u7684\u51b2\u523a\u9010\u6b65\u63d0\u9ad8\u6a21\u578b\u4fdd\u771f\u5ea6\uff0c\u5e76\u4f7f\u7528Hyperband\u526a\u679d\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528\u5143\u5b66\u4e60\u8005\u6765\u8c03\u6574\u9608\u503c\u4ee5\u5728\u63a8\u7406\u671f\u95f4\u89e3\u51b3\u5206\u7c7b\u6982\u7387\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u57282021\u5e74Eberts\u548cUlges\u63d0\u51fa\u7684\u8054\u5408\u5b9e\u4f53\u548c\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u53d8\u4f53\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u57282021\u5e74Eberts\u548cUlges\u63d0\u51fa\u7684\u8054\u5408\u5b9e\u4f53\u548c\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u53d8\u4f53\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09943", "pdf": "https://arxiv.org/pdf/2505.09943", "abs": "https://arxiv.org/abs/2505.09943", "authors": ["Jiakun Deng", "Kexuan Li", "Xingye Cui", "Jiaxuan Li", "Chang Long", "Tian Pu", "Zhenming Peng"], "title": "CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5CSPENet\uff0c\u901a\u8fc7\u5f15\u5165\u8f6e\u5ed3\u611f\u77e5\u548c\u663e\u8457\u6027\u5148\u9a8c\u5d4c\u5165\u7f51\u7edc\uff0c\u63d0\u9ad8\u4e86\u5728\u5bc6\u96c6\u6742\u6ce2\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5bc6\u96c6\u6742\u6ce2\u73af\u5883\u4e0b\u5bf9\u5fae\u5f31\u76ee\u6807\u7684\u5b9a\u4f4d\u548c\u8f6e\u5ed3\u4fe1\u606f\u611f\u77e5\u5b58\u5728\u4e0d\u8db3\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6e\u5ed3\u611f\u77e5\u548c\u663e\u8457\u6027\u5148\u9a8c\u5d4c\u5165\u7f51\u7edc\uff08CSPENet\uff09\uff0c\u5305\u62ec\u4e00\u4e2a\u73af\u7ed5\u6536\u655b\u5148\u9a8c\u63d0\u53d6\u6a21\u5757\uff08SCPEM\uff09\u3001\u4e00\u4e2a\u53cc\u5206\u652f\u5148\u9a8c\u5d4c\u5165\u67b6\u6784\uff08DBPEA\uff09\u548c\u4e00\u4e2a\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff08AGFEM\uff09\u3002", "result": "CSPENet\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CSPENet\u5728\u516c\u5f00\u6570\u636e\u96c6NUDT-SIRST\u3001IRSTD-1k\u548cNUAA-SIRST\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.09945", "pdf": "https://arxiv.org/pdf/2505.09945", "abs": "https://arxiv.org/abs/2505.09945", "authors": ["Deeksha Prahlad", "Chanhee Lee", "Dongha Kim", "Hokeun Kim"], "title": "Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)", "summary": "The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8fc7\u5ea6\u62df\u5408\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LLMs\u7ecf\u5e38\u56e0\u8fc7\u5ea6\u62df\u5408\u800c\u4ea7\u751f\u989d\u5916\u548c\u9519\u8bef\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u3002\u7f3a\u4e4f\u53ca\u65f6\u3001\u4e8b\u5b9e\u6027\u548c\u4e2a\u6027\u5316\u4fe1\u606f\u662f\u8fd9\u4e9b\u95ee\u9898\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u6765\u8f85\u52a9LLM\u751f\u6210\u4e2a\u6027\u5316\u7684\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7406\u89e3\u4e2a\u4eba\u4fe1\u606f\u548c\u751f\u6210\u51c6\u786e\u54cd\u5e94\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u4e2a\u4eba\u6570\u636e\u4f5c\u4e3a\u6587\u672c\u8f93\u5165\u7684\u57fa\u7ebfLLMs\uff0c\u540c\u65f6\u54cd\u5e94\u65f6\u95f4\u6709\u9002\u5ea6\u7684\u51cf\u5c11\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7406\u89e3\u4e2a\u4eba\u4fe1\u606f\u548c\u751f\u6210\u51c6\u786e\u54cd\u5e94\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u4e2a\u4eba\u6570\u636e\u4f5c\u4e3a\u6587\u672c\u8f93\u5165\u7684\u57fa\u7ebfLLMs\uff0c\u540c\u65f6\u54cd\u5e94\u65f6\u95f4\u6709\u9002\u5ea6\u7684\u51cf\u5c11\u3002"}}
{"id": "2505.10093", "pdf": "https://arxiv.org/pdf/2505.10093", "abs": "https://arxiv.org/abs/2505.10093", "authors": ["Hsuan-Lei Shao"], "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI", "categories": ["cs.AI", "cs.CL", "I.2.4; H.3.3; J.5"], "comment": "4 pages, 4 figures", "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u8f85\u52a9\u7684\u65b9\u6cd5\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u5b66\u672f\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u4ea4\u4e92\u5f0f\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u5411\u91cf\u6570\u636e\u5e93\u5b9e\u73b0\u5bf9\u4e2d\u56fd\u7814\u7a76\u6587\u732e\u7684\u91cd\u65b0\u7ec4\u7ec7\u548c\u63a2\u7d22\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5730\u56de\u987e\u548c\u91cd\u65b0\u7ec4\u7ec7\u57fa\u4e8e\u53f0\u6e7e\u7684\u4e2d\u56fd\u7814\u7a76\u5b66\u672f\u6210\u679c\uff0c\u4ee5\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u8f85\u52a9\u7684\u65b9\u6cd5\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u5b66\u672f\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u4ea4\u4e92\u5f0f\u7684\u77e5\u8bc6\u8868\u793a\u3002\u6211\u4eec\u5e94\u7528\u751f\u6210\u5f0fAI\uff08GAI\uff09\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece1996\u5e74\u81f32019\u5e74\u95f4\u53d1\u8868\u76841,367\u7bc7\u540c\u884c\u8bc4\u5ba1\u7684\u4e2d\u56fd\u7814\u7a76\u6587\u7ae0\u4e2d\u63d0\u53d6\u548c\u6807\u51c6\u5316\u5b9e\u4f53\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eD3.js\u7684\u8f7b\u91cf\u7ea7\u7cfb\u7edf\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6784\u5efa\u9886\u57df\u7279\u5b9a\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u5411\u91cf\u6570\u636e\u5e93\u3002", "result": "\u901a\u8fc7\u5c06\u6587\u672c\u5185\u5bb9\u5206\u89e3\u4e3a\u56fe\u7ed3\u6784\u7684\u77e5\u8bc6\u5355\u5143\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4ece\u7ebf\u6027\u6587\u672c\u6d88\u8d39\u5230\u57fa\u4e8e\u7f51\u7edc\u7684\u77e5\u8bc6\u5bfc\u822a\u7684\u8303\u5f0f\u8f6c\u53d8\u3002\u5b83\u589e\u5f3a\u4e86\u5b66\u8005\u5bf9\u4e2d\u56fd\u7814\u7a76\u6587\u732e\u7684\u8bbf\u95ee\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u4f20\u7edf\u672c\u4f53\u6784\u5efa\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u589e\u5f3a\u533a\u57df\u7814\u7a76\u548c\u6570\u5b57\u4eba\u6587\uff0c\u540c\u65f6\u4e5f\u7a81\u663e\u4e86\u5176\u652f\u6301\u91cd\u65b0\u6784\u60f3\u533a\u57df\u77e5\u8bc6\u7cfb\u7edf\u5b66\u672f\u57fa\u7840\u8bbe\u65bd\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09810", "pdf": "https://arxiv.org/pdf/2505.09810", "abs": "https://arxiv.org/abs/2505.09810", "authors": ["Daniel Waddington", "Cornel Constantinescu"], "title": "Lossless Compression for LLM Tensor Incremental Snapshots", "categories": ["cs.LG"], "comment": null, "summary": "During the training of Large Language Models (LLMs), tensor data is\nperiodically \"checkpointed\" to persistent storage to allow recovery of work\ndone in the event of failure. The volume of data that must be copied during\neach checkpoint, even when using reduced-precision representations such as\nbfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be\nmoved across a network and written to a storage system before the next epoch\noccurs. With a view to ultimately building an optimized checkpointing solution,\nthis paper presents experimental analysis of checkpoint data used to derive a\ndesign that maximizes the use of lossless compression to reduce the volume of\ndata. We examine how tensor data and its compressibility evolve during model\ntraining and evaluate the efficacy of existing common off-the-shelf general\npurpose compression engines combined with known data optimization techniques\nsuch as byte-grouping and incremental delta compression.\n  Leveraging our analysis we have built an effective compression solution,\nknown as Language Model Compressor (LMC), which is based on byte-grouping and\nHuffman encoding. LMC offers more compression performance than the best\nalternative (BZ2) but with an order-of-magnitude reduction in the time needed\nto perform the compression. We show that a 16-core parallel implementation of\nLMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76\nGiB/s respectively. This increase in performance ultimately reduces the CPU\nresources needed and provides more time to copy the data to the storage system\nbefore the next epoch thus allowing for higher-frequency checkpoints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u538b\u7f29\u65b9\u6848LMC\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u538b\u7f29\u6027\u80fd\u5e76\u51cf\u5c11CPU\u8d44\u6e90\u6d88\u8017\uff0c\u4ece\u800c\u5141\u8bb8\u66f4\u9891\u7e41\u7684\u68c0\u67e5\u70b9\u64cd\u4f5c\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9700\u8981\u9891\u7e41\u5730\u5c06\u5f20\u91cf\u6570\u636e\u4fdd\u5b58\u5230\u6301\u4e45\u5316\u5b58\u50a8\u4e2d\uff0c\u800c\u6bcf\u6b21\u68c0\u67e5\u70b9\u7684\u6570\u636e\u91cf\u975e\u5e38\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f18\u5316\u7684\u68c0\u67e5\u70b9\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u6570\u636e\u91cf\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u68c0\u67e5\u70b9\u6570\u636e\uff0c\u5229\u7528\u5b57\u8282\u5206\u7ec4\u548c\u970d\u592b\u66fc\u7f16\u7801\u6784\u5efa\u4e86LMC\u538b\u7f29\u65b9\u6848\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u901a\u7528\u538b\u7f29\u5f15\u64ce\u7684\u6548\u679c\u3002", "result": "LMC\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u4f73\u66ff\u4ee3\u65b9\u6848\uff08BZ2\uff09\uff0c\u5e76\u4e14\u538b\u7f29\u65f6\u95f4\u51cf\u5c11\u4e86\u6570\u91cf\u7ea7\u300216\u6838\u5e76\u884c\u5b9e\u73b0\u7684LMC\u53ef\u4ee5\u8fbe\u52302.78 GiB/s\u7684\u538b\u7f29\u541e\u5410\u91cf\u548c3.76 GiB/s\u7684\u89e3\u538b\u7f29\u541e\u5410\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u79f0\u4e3aLanguage Model Compressor (LMC)\uff0c\u5b83\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u538b\u7f29\u6027\u80fd\uff0c\u5e76\u51cf\u5c11CPU\u8d44\u6e90\u7684\u4f7f\u7528\uff0c\u4ece\u800c\u5141\u8bb8\u66f4\u9891\u7e41\u7684\u68c0\u67e5\u70b9\u64cd\u4f5c\u3002"}}
{"id": "2505.09965", "pdf": "https://arxiv.org/pdf/2505.09965", "abs": "https://arxiv.org/abs/2505.09965", "authors": ["Hao Yang", "Tao Tan", "Shuai Tan", "Weiqin Yang", "Kunyan Cai", "Calvin Chen", "Yue Sun"], "title": "MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.", "AI": {"tldr": "MambaControl is a novel framework that combines Mamba-based long-range modelling with graph-guided anatomical control and Fourier-enhanced spectral graph representations to improve the prediction of medical image trajectories, achieving state-of-the-art results in Alzheimer's disease prediction.", "motivation": "Existing methods struggle with longitudinal dependencies and structural consistency in progressive disorders, requiring better models for capturing complex spatio-temporal dynamics while preserving anatomical integrity.", "method": "MambaControl integrates selective state-space modelling with diffusion processes, combining Mamba-based long-range modelling with graph-guided anatomical control, and introduces Fourier-enhanced spectral graph representations to capture spatial coherence and multiscale detail.", "result": "MambaControl achieves state-of-the-art performance in Alzheimer's disease prediction, with quantitative and regional evaluations showing improved progression prediction quality and anatomical fidelity.", "conclusion": "MambaControl demonstrates improved progression prediction quality and anatomical fidelity, highlighting its potential for personalized prognosis and clinical decision support."}}
{"id": "2505.10013", "pdf": "https://arxiv.org/pdf/2505.10013", "abs": "https://arxiv.org/abs/2505.10013", "authors": ["Lake Yin", "Fan Huang"], "title": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 1 figure", "summary": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cfLLM\u9690\u6027\u504f\u89c1\u7684\u65b0\u65b9\u6cd5DIF\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u65b9\u6cd5\u6765\u8861\u91cfLLM\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709\u7684LLM\u903b\u8f91\u548c\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u793e\u4f1a\u4eba\u53e3\u5b66\u4eba\u7269\u89d2\u8272\u6765\u8ba1\u7b97\u4e00\u4e2a\u6613\u4e8e\u89e3\u91ca\u7684\u57fa\u51c6DIF\u3002", "result": "\u672c\u6587\u53d1\u73b0\uff0cDIF\u65b9\u6cd5\u53ef\u4ee5\u7edf\u8ba1\u9a8c\u8bc1LLM\u884c\u4e3a\u4e2d\u9690\u6027\u504f\u89c1\u7684\u5b58\u5728\uff0c\u5e76\u4e14\u53d1\u73b0\u95ee\u7b54\u51c6\u786e\u6027\u548c\u9690\u6027\u504f\u89c1\u4e4b\u95f4\u5b58\u5728\u53cd\u5411\u8d8b\u52bf\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0cLLM\u4e2d\u7684\u9690\u6027\u504f\u89c1\u4e0d\u4ec5\u662f\u4e00\u4e2a\u4f26\u7406\u95ee\u9898\uff0c\u4e5f\u662f\u4e00\u4e2a\u6280\u672f\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8861\u91cfLLM\u504f\u89c1\u7684\u57fa\u51c6\u65b9\u6cd5DIF\u3002"}}
{"id": "2505.10188", "pdf": "https://arxiv.org/pdf/2505.10188", "abs": "https://arxiv.org/abs/2505.10188", "authors": ["Felix Liedeker", "Olivia Sanchez-Graillet", "Moana Seidler", "Christian Brandt", "J\u00f6rg Wellmer", "Philipp Cimiano"], "title": "A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support", "categories": ["cs.AI"], "comment": "Presented at 'The First Workshop on Natural Language Argument-Based\n  Explanations', co-located with ECAI 2024", "summary": "As the field of healthcare increasingly adopts artificial intelligence, it\nbecomes important to understand which types of explanations increase\ntransparency and empower users to develop confidence and trust in the\npredictions made by machine learning (ML) systems. In shared decision-making\nscenarios where doctors cooperate with ML systems to reach an appropriate\ndecision, establishing mutual trust is crucial. In this paper, we explore\ndifferent approaches to generating explanations in eXplainable AI (XAI) and\nmake their underlying arguments explicit so that they can be evaluated by\nmedical experts. In particular, we present the findings of a user study\nconducted with physicians to investigate their perceptions of various types of\nAI-generated explanations in the context of diagnostic decision support. The\nstudy aims to identify the most effective and useful explanations that enhance\nthe diagnostic process. In the study, medical doctors filled out a survey to\nassess different types of explanations. Further, an interview was carried out\npost-survey to gain qualitative insights on the requirements of explanations\nincorporated in diagnostic decision support. Overall, the insights gained from\nthis study contribute to understanding the types of explanations that are most\neffective.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u533b\u751f\u5bf9AI\u751f\u6210\u89e3\u91ca\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u4e0d\u540c\u7c7b\u578b\u7684\u89e3\u91ca\u5728\u589e\u5f3a\u8bca\u65ad\u8fc7\u7a0b\u65b9\u9762\u6709\u4e0d\u540c\u7684\u6548\u679c\uff0c\u8fd9\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u968f\u7740\u533b\u7597\u9886\u57df\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u4eba\u5de5\u667a\u80fd\uff0c\u7406\u89e3\u54ea\u4e9b\u7c7b\u578b\u7684\u89e3\u91ca\u80fd\u63d0\u9ad8\u900f\u660e\u5ea6\u5e76\u589e\u5f3a\u7528\u6237\u5bf9\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9884\u6d4b\u7684\u4fe1\u4efb\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u5728\u5171\u4eab\u51b3\u7b56\u573a\u666f\u4e2d\uff0c\u5efa\u7acb\u76f8\u4e92\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u901a\u8fc7\u4e00\u9879\u9488\u5bf9\u533b\u751f\u7684\u7528\u6237\u7814\u7a76\uff0c\u8c03\u67e5\u4e86\u4ed6\u4eec\u5bf9\u5404\u79cdAI\u751f\u6210\u89e3\u91ca\u7684\u770b\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u95ee\u5377\u8c03\u67e5\u548c\u540e\u7eed\u8bbf\u8c08\u4ee5\u83b7\u5f97\u5b9a\u6027\u89c1\u89e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u89e3\u91ca\u5728\u589e\u5f3a\u8bca\u65ad\u8fc7\u7a0b\u65b9\u9762\u6709\u4e0d\u540c\u7684\u6548\u679c\uff0c\u8fd9\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u89e3\u91ca\u5728\u589e\u5f3a\u8bca\u65ad\u8fc7\u7a0b\u65b9\u9762\u6709\u4e0d\u540c\u7684\u6548\u679c\uff0c\u8fd9\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2505.09812", "pdf": "https://arxiv.org/pdf/2505.09812", "abs": "https://arxiv.org/abs/2505.09812", "authors": ["Anastasija Tashkova", "Stefan Eftimov", "Bojan Ristov", "Slobodan Kalajdziski"], "title": "Comparative Analysis of Stroke Prediction Models Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Stroke remains one of the most critical global health challenges, ranking as\nthe second leading cause of death and the third leading cause of disability\nworldwide. This study explores the effectiveness of machine learning algorithms\nin predicting stroke risk using demographic, clinical, and lifestyle data from\nthe Stroke Prediction Dataset. By addressing key methodological challenges such\nas class imbalance and missing data, we evaluated the performance of multiple\nmodels, including Logistic Regression, Random Forest, and XGBoost. Our results\ndemonstrate that while these models achieve high accuracy, sensitivity remains\na limiting factor for real-world clinical applications. In addition, we\nidentify the most influential predictive features and propose strategies to\nimprove machine learning-based stroke prediction. These findings contribute to\nthe development of more reliable and interpretable models for the early\nassessment of stroke risk.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u9884\u6d4b\u4e2d\u98ce\u98ce\u9669\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e2d\u98ce\u9884\u6d4b\u7684\u7b56\u7565\u3002", "motivation": "\u4e2d\u98ce\u4ecd\u7136\u662f\u5168\u7403\u6700\u91cd\u8981\u7684\u5065\u5eb7\u6311\u6218\u4e4b\u4e00\uff0c\u6392\u540d\u4e3a\u7b2c\u4e8c\u5927\u6b7b\u4ea1\u539f\u56e0\u548c\u7b2c\u4e09\u5927\u6b8b\u75be\u539f\u56e0\u3002\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u6765\u65e9\u671f\u8bc4\u4f30\u4e2d\u98ce\u98ce\u9669\u3002", "method": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u9884\u6d4b\u4e2d\u98ce\u98ce\u9669\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u4e86\u6765\u81ea\u4e2d\u98ce\u9884\u6d4b\u6570\u636e\u96c6\u7684\u4eba\u53e3\u7edf\u8ba1\u3001\u4e34\u5e8a\u548c\u751f\u6d3b\u65b9\u5f0f\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u5305\u62ec\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u3002", "result": "\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u7075\u654f\u5ea6\u4ecd\u7136\u662f\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u9650\u5236\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u6700\u5177\u5f71\u54cd\u529b\u7684\u9884\u6d4b\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e2d\u98ce\u9884\u6d4b\u7684\u7b56\u7565\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4e2d\u98ce\u98ce\u9669\u7684\u65e9\u671f\u8bc4\u4f30\u3002"}}
{"id": "2505.09967", "pdf": "https://arxiv.org/pdf/2505.09967", "abs": "https://arxiv.org/abs/2505.09967", "authors": ["Liqian Deng"], "title": "TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e13\u6ce8\u4e8eTexture Key Driver Factors (TKDF)\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u5305\u542bTexture-Aware Feature Extractor (TAFE)\u548cDual Contextual Information Filtering (DCIF)\u7684FER\u67b6\u6784\u6765\u6709\u6548\u6355\u6349\u548c\u5229\u7528\u8fd9\u4e9b\u7ebf\u7d22\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728RAF-DB\u548cKDEF\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u8868\u60c5\u76f8\u5173\u7279\u5f81\u7684\u7ec6\u5fae\u548c\u5c40\u90e8\u6027\u8d28\u4ee5\u53ca\u9762\u90e8\u5916\u89c2\u7684\u590d\u6742\u53d8\u5316\uff0c\u4f7f\u5f97\u5728\u91ce\u5916\u8fdb\u884c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u6211\u4eec\u89c2\u5bdf\u5230\u67d0\u4e9b\u7eb9\u7406\u7ebf\u7d22\uff0c\u5982\u7709\u6bdb\u3001\u773c\u775b\u548c\u5634\u5df4\u5468\u56f4\u7684\u76ae\u80a4\u5fae\u5c0f\u53d8\u5316\uff0c\u662f\u60c5\u611f\u52a8\u6001\u7684\u4e3b\u8981\u6307\u6807\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542bTexture-Aware Feature Extractor (TAFE)\u548cDual Contextual Information Filtering (DCIF)\u7684FER\u67b6\u6784\u3002TAFE\u4f7f\u7528\u5e26\u6709\u591a\u5206\u652f\u6ce8\u610f\u529b\u673a\u5236\u7684ResNet\u9aa8\u5e72\u7f51\u7edc\u6765\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7eb9\u7406\u8868\u793a\uff0c\u800cDCIF\u901a\u8fc7\u81ea\u9002\u5e94\u6c60\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u6765\u4f18\u5316\u8fd9\u4e9b\u7279\u5f81\u3002", "result": "\u5728RAF-DB\u548cKDEF\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728RAF-DB\u548cKDEF\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5c06TKDF\u7eb3\u5165FER\u6d41\u7a0b\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.10063", "pdf": "https://arxiv.org/pdf/2505.10063", "abs": "https://arxiv.org/abs/2505.10063", "authors": ["Han Peng", "Jinhao Jiang", "Zican Dong", "Wayne Xin Zhao", "Lei Fang"], "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability", "categories": ["cs.CL"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAFE\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6587\u6863\u95ee\u7b54\u80fd\u529b\uff0c\u901a\u8fc7\u9010\u6b65\u6d88\u9664\u80cc\u666f\u548c\u5e72\u6270\u6587\u6863\u7684\u5f71\u54cd\uff0c\u4f7f\u54cd\u5e94\u66f4\u4f9d\u8d56\u4e8e\u8bc1\u636e\u6587\u6863\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCAFE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5e73\u8861\u68c0\u7d22\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u5b83\u4eec\u56de\u7b54\u95ee\u9898\u7684\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u591a\u6587\u6863\u95ee\u7b54\u80fd\u529b\u3002", "method": "CAFE\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u7c97\u5230\u7ec6\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u6d88\u9664\u80cc\u666f\u548c\u5e72\u6270\u6587\u6863\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4f7f\u54cd\u5e94\u66f4\u4f9d\u8d56\u4e8e\u8bc1\u636e\u6587\u6863\u3002\u9996\u5148\uff0c\u4f7f\u7528\u68c0\u7d22\u5934\u8fdb\u884c\u7c97\u7c92\u5ea6\u8fc7\u6ee4\uff0c\u4ee5\u8bc6\u522b\u548c\u6392\u5e8f\u76f8\u5173\u6587\u6863\u3002\u7136\u540e\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u5f15\u5bfc\u6ce8\u610f\u529b\u5230\u6700\u76f8\u5173\u7684\u5185\u5bb9\u3002", "result": "CAFE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Mistral\u6a21\u578b\u4e0a\u5206\u522b\u6bd4SFT\u548cRAG\u65b9\u6cd5\u63d0\u9ad8\u4e8622.1%\u548c13.7%\u7684SubEM\u3002", "conclusion": "CAFE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Mistral\u6a21\u578b\u4e0a\u5206\u522b\u6bd4SFT\u548cRAG\u65b9\u6cd5\u63d0\u9ad8\u4e8622.1%\u548c13.7%\u7684SubEM\u3002"}}
{"id": "2505.10278", "pdf": "https://arxiv.org/pdf/2505.10278", "abs": "https://arxiv.org/abs/2505.10278", "authors": ["Taian Guo", "Haiyang Shen", "Jinsheng Huang", "Zhengyang Mao", "Junyu Luo", "Zhuoru Chen", "Xuhui Liu", "Bingyu Xia", "Luchen Liu", "Yun Ma", "Ming Zhang"], "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction", "categories": ["cs.AI"], "comment": null, "summary": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MASS\uff0c\u4e00\u79cd\u7528\u4e8e\u6295\u8d44\u7ec4\u5408\u6784\u5efa\u7684\u591a\u667a\u80fd\u4f53\u89c4\u6a21\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u667a\u80fd\u4f53\u6570\u91cf\u548c\u53cd\u5411\u4f18\u5316\u8fc7\u7a0b\u6765\u63d0\u9ad8\u5e02\u573a\u7406\u89e3\u548c\u4ee3\u7406\u5206\u5e03\u4f18\u5316\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4ec5\u9650\u4e8e\u7eaf\u6a21\u62df\u6216\u53d7\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7a0b\u7684\u9650\u5236\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "method": "MASS\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u4ee3\u7406\u6570\u91cf\u8fdb\u884c\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u4ee5\u83b7\u5f97\u5bf9\u5e02\u573a\u7684\u66f4\u6df1\u5165\u4e86\u89e3\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u4f18\u5316\u8fc7\u7a0b\u7aef\u5230\u7aef\u5730\u4f18\u5316\u4ee3\u7406\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u6027\u80fd\u5b9e\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u3001\u56de\u6d4b\u5b9e\u9a8c\u3001\u66f4\u65b0\u6570\u636e\u548c\u80a1\u7968\u6c60\u7684\u5b9e\u9a8c\u3001\u6269\u5c55\u5b9e\u9a8c\u3001\u53c2\u6570\u654f\u611f\u6027\u5b9e\u9a8c\u548c\u53ef\u89c6\u5316\u5b9e\u9a8c\uff0cMASS\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684A\u80a1\u80a1\u7968\u6c60\u4e2d\u4e0e\u516d\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "MASS\u7684\u8303\u5f0f\u6709\u671b\u6269\u5c55\u5230\u5177\u6709\u7c7b\u4f3c\u7279\u5f81\u7684\u5176\u4ed6\u4efb\u52a1\u3002"}}
{"id": "2505.09820", "pdf": "https://arxiv.org/pdf/2505.09820", "abs": "https://arxiv.org/abs/2505.09820", "authors": ["Sajib Biswas", "Mao Nishino", "Samuel Jacob Chacko", "Xiuwen Liu"], "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent", "categories": ["cs.LG"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u548cBregman\u6295\u5f71\u65b9\u6cd5\u7684LLM\u653b\u51fb\u6280\u672f\uff0c\u8be5\u6280\u672f\u5728\u591a\u4e2a\u5f00\u6e90LLM\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5145\u5206\u5229\u7528\u7a7a\u95f4\u7684\u7ea6\u675f\u548c\u7ed3\u6784\uff0c\u5f00\u53d1\u4e00\u79cd\u6709\u6548\u7684LLM\u653b\u51fb\u6280\u672f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u548cBregman\u6295\u5f71\u65b9\u6cd5\u7684\u5185\u5728\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u4f18\u5316\u7684one-hot\u7f16\u7801\u59cb\u7ec8\u4f4d\u4e8e\u6982\u7387\u5355\u7eaf\u5f62\u5185\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u4e94\u4e2a\u5f00\u6e90LLM\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5176\u6210\u529f\u7387\u548c\u6548\u7387\u5747\u4f18\u4e8e\u5176\u4ed6\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6280\u672f\u3002", "conclusion": "\u8be5\u6280\u672f\u5728\u63d0\u9ad8LLM\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u5176\u4ed6\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6280\u672f\u3002"}}
{"id": "2505.09971", "pdf": "https://arxiv.org/pdf/2505.09971", "abs": "https://arxiv.org/abs/2505.09971", "authors": ["Yuan Gao", "Shaobo Xia", "Sheng Nie", "Cheng Wang", "Xiaohuan Xi", "Bisheng Yang"], "title": "APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "18 pages,12 figures", "summary": "Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86APCoTTA\uff0c\u4e00\u79cd\u9488\u5bf9ALS\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u8fde\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u3001\u57fa\u4e8e\u71b5\u7684\u4e00\u81f4\u6027\u635f\u5931\u548c\u968f\u673a\u53c2\u6570\u63d2\u503c\u673a\u5236\u6765\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u548c\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5728ALS\u70b9\u4e91\u4e0a\u7684\u8fde\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08CTTA\uff09\u65b9\u6cd5\u6709\u9650\uff0c\u9762\u4e34\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u9519\u8bef\u7d2f\u79ef\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u6a21\u5757\uff0c\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u9009\u62e9\u4f4e\u7f6e\u4fe1\u5ea6\u5c42\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4fdd\u6301\u5176\u4f59\u5c42\u51bb\u7ed3\u4ee5\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u71b5\u7684\u4e00\u81f4\u6027\u635f\u5931\u548c\u968f\u673a\u53c2\u6570\u63d2\u503c\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u9519\u8bef\u7d2f\u79ef\u5e76\u5e73\u8861\u76ee\u6807\u9002\u5e94\u548c\u6e90\u77e5\u8bc6\u4fdd\u7559\u3002", "result": "APCoTTA\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5ISPRSC\u548cH3DC\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0cmIoU\u5206\u522b\u63d0\u9ad8\u4e86\u7ea69%\u548c14%\u3002", "conclusion": "APCoTTA\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff0cmIoU\u5206\u522b\u63d0\u9ad8\u4e86\u7ea69%\u548c14%\u3002\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801\u5df2\u53d1\u5e03\u3002"}}
{"id": "2505.10066", "pdf": "https://arxiv.org/pdf/2505.10066", "abs": "https://arxiv.org/abs/2505.10066", "authors": ["Michael Fire", "Yitzhak Elbazis", "Adi Wasenstein", "Lior Rokach"], "title": "Dark LLMs: The Growing Threat of Unaligned AI Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "68T50, 68T05, 68P25", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.", "AI": {"tldr": "This paper highlights the growing threat of dark LLMs and the vulnerability of state-of-the-art models to jailbreak attacks. The research reveals a universal jailbreak method that can compromise multiple models, leading to harmful outputs. Despite responsible disclosure, industry responses have been inadequate, raising concerns about AI safety practices.", "motivation": "The fundamental vulnerability of LLMs to jailbreak attacks stems from the data they learn from. Training data that includes unfiltered, problematic, or 'dark' content can lead to undesirable patterns or weaknesses that allow users to circumvent safety controls.", "method": "Our research identifies the growing threat posed by dark LLMs models deliberately designed without ethical guardrails or modified through jailbreak techniques. We uncovered a universal jailbreak attack that effectively compromises multiple state-of-the-art models.", "result": "Our universal jailbreak attack effectively compromised multiple state-of-the-art models, enabling them to answer almost any question and produce harmful outputs upon request. Many tested LLMs were still vulnerable to this attack despite our responsible disclosure efforts.", "conclusion": "Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated."}}
{"id": "2505.10309", "pdf": "https://arxiv.org/pdf/2505.10309", "abs": "https://arxiv.org/abs/2505.10309", "authors": ["Tuan Dung Nguyen", "Duncan J. Watts", "Mark E. Whiting"], "title": "Empirically evaluating commonsense intelligence in large language models with large-scale human judgments", "categories": ["cs.AI", "cs.HC", "cs.SI"], "comment": null, "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u5e38\u8bc6\uff0c\u8003\u8651\u4e86\u4eba\u7c7b\u4e4b\u95f4\u7684\u5f02\u8d28\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5927\u591a\u6570\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e38\u8bc6\u80fd\u529b\u4f4e\u4e8e\u4eba\u7c7b\u4e2d\u4f4d\u6570\uff0c\u5e76\u4e14\u4e0e\u771f\u5b9e\u4eba\u7c7b\u7684\u76f8\u5173\u6027\u8f83\u4f4e\u3002", "motivation": "\u73b0\u6709\u7684\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5047\u8bbe\u4eba\u7c7b\u5e38\u8bc6\u662f\u540c\u8d28\u7684\uff0c\u4f46\u6700\u8fd1\u7684\u5b9e\u8bc1\u5de5\u4f5c\u8868\u660e\uff0c\u4eba\u7c7b\u5728\u4ed6\u4eec\u8ba4\u4e3a\u5e38\u8bc6\u6027\u7684\u4e8b\u60c5\u4e0a\u5b58\u5728\u5de8\u5927\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8ba4\u4e3a\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8003\u8651\u8fd9\u79cd\u5f02\u8d28\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4e2d\u7684\u5e38\u8bc6\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u7684\u5224\u65ad\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u7684\u5bf9\u5e94\u5173\u7cfb\u6765\u7eb3\u5165\u4eba\u7c7b\u4e4b\u95f4\u7684\u5f02\u8d28\u6027\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u5f53\u88ab\u89c6\u4e3a\u72ec\u7acb\u7684\u8c03\u67e5\u53d7\u8bbf\u8005\u65f6\uff0c\u5927\u591a\u6570LLMs\u7684\u5e38\u8bc6\u80fd\u529b\u4f4e\u4e8e\u4eba\u7c7b\u4e2d\u4f4d\u6570\u3002\u6b64\u5916\uff0c\u5f53\u7528\u4f5c\u5047\u8bbe\u4eba\u53e3\u7684\u6a21\u62df\u5668\u65f6\uff0cLLMs\u4e0e\u771f\u5b9e\u4eba\u7c7b\u5728\u540c\u610f\u540c\u4e00\u7ec4\u9648\u8ff0\u7684\u7a0b\u5ea6\u4e0a\u7684\u76f8\u5173\u6027\u4ec5\u9002\u4e2d\u3002\u8f83\u5c0f\u7684\u3001\u5f00\u653e\u6743\u91cd\u7684\u6a21\u578b\u6bd4\u66f4\u5927\u7684\u3001\u4e13\u6709\u7684\u524d\u6cbf\u6a21\u578b\u8868\u73b0\u5f97\u66f4\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u8bc4\u4f30\u6846\u67b6\u5c06\u5e38\u8bc6\u667a\u80fd\u4e0e\u5176\u6587\u5316\u57fa\u7840\u8054\u7cfb\u8d77\u6765\uff0c\u6709\u52a9\u4e8e\u65e5\u76ca\u589e\u957f\u7684\u547c\u5401\uff0c\u5373\u8c03\u6574\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u4ee5\u9002\u5e94\u62e5\u6709\u4e0d\u540c\u4e14\u5e38\u5e38\u4e0d\u517c\u5bb9\u7684\u793e\u4f1a\u77e5\u8bc6\u5e93\u7684\u4eba\u7c7b\u7fa4\u4f53\u3002"}}
{"id": "2505.09822", "pdf": "https://arxiv.org/pdf/2505.09822", "abs": "https://arxiv.org/abs/2505.09822", "authors": ["Changhao Shi", "Gal Mishne"], "title": "Learning Kronecker-Structured Graphs from Smooth Signals", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Graph learning, or network inference, is a prominent problem in graph signal\nprocessing (GSP). GSP generalizes the Fourier transform to non-Euclidean\ndomains, and graph learning is pivotal to applying GSP when these domains are\nunknown. With the recent prevalence of multi-way data, there has been growing\ninterest in product graphs that naturally factorize dependencies across\ndifferent ways. However, the types of graph products that can be learned are\nstill limited for modeling diverse dependency structures. In this paper, we\nstudy the problem of learning a Kronecker-structured product graph from smooth\nsignals. Unlike the more commonly used Cartesian product, the Kronecker product\nmodels dependencies in a more intricate, non-separable way, but posits harder\nconstraints on the graph learning problem. To tackle this non-convex problem,\nwe propose an alternating scheme to optimize each factor graph and provide\ntheoretical guarantees for its asymptotic convergence. The proposed algorithm\nis also modified to learn factor graphs of the strong product. We conduct\nexperiments on synthetic and real-world graphs and demonstrate our approach's\nefficacy and superior performance compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u5e73\u6ed1\u4fe1\u53f7\u4e2d\u5b66\u4e60Kronecker\u7ed3\u6784\u4ea7\u54c1\u56fe\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u591a\u5411\u6570\u636e\u7684\u666e\u53ca\uff0c\u5bf9\u4ea7\u54c1\u56fe\u7684\u5174\u8da3\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u7684\u56fe\u4ea7\u54c1\u7c7b\u578b\u5728\u5efa\u6a21\u591a\u6837\u4f9d\u8d56\u7ed3\u6784\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002Kronecker\u4e58\u79ef\u80fd\u591f\u66f4\u590d\u6742\u5730\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u5bf9\u56fe\u5b66\u4e60\u95ee\u9898\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u7ea6\u675f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u65b9\u6848\u6765\u4f18\u5316\u6bcf\u4e2a\u56e0\u5b50\u56fe\uff0c\u5e76\u63d0\u4f9b\u4e86\u5176\u6e10\u8fd1\u6536\u655b\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u4fee\u6539\u4e86\u7b97\u6cd5\u4ee5\u5b66\u4e60\u5f3a\u79ef\u7684\u56e0\u5b50\u56fe\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5e73\u6ed1\u4fe1\u53f7\u4e2d\u5b66\u4e60Kronecker\u7ed3\u6784\u4ea7\u54c1\u56fe\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.09986", "pdf": "https://arxiv.org/pdf/2505.09986", "abs": "https://arxiv.org/abs/2505.09986", "authors": ["Yimin Zhou", "Yichong Xia", "Sicheng Pan", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.", "AI": {"tldr": "HQUIC is a new underwater image compression algorithm that leverages specific features of underwater images to improve compression efficiency.", "motivation": "Contemporary underwater image compression algorithms fail to fully leverage the unique characteristics of underwater scenes, leading to suboptimal performance.", "method": "HQUIC employs an ALTC module to adaptively predict attenuation coefficients and global light information, uses a codebook to extract common objects, and dynamically weights multi-scale frequency components.", "result": "HQUIC demonstrates superior performance on diverse underwater datasets compared to existing methods.", "conclusion": "HQUIC outperforms state-of-the-art compression methods in underwater image compression."}}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081", "abs": "https://arxiv.org/abs/2505.10081", "authors": ["Wisdom Aduah", "Francois Meyer"], "title": "Designing and Contextualising Probes for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u975e\u6d32\u8bed\u8a00\u4e2d\u7684\u8bed\u8a00\u77e5\u8bc6\u3002\u901a\u8fc7\u8bad\u7ec3\u9010\u5c42\u63a2\u6d4b\u5668\u548c\u8bbe\u8ba1\u63a7\u5236\u4efb\u52a1\uff0c\u6211\u4eec\u53d1\u73b0\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u8c03\u6574\u7684PLMs\u6bd4\u5927\u89c4\u6a21\u591a\u8bed\u8a00PLMs\u7f16\u7801\u4e86\u66f4\u591a\u7684\u76ee\u6807\u8bed\u8a00\u8bed\u8a00\u4fe1\u606f\u3002\u6211\u4eec\u7684\u7814\u7a76\u5e94\u7528\u4e86\u5df2\u5efa\u7acb\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6765\u5206\u6790\u975e\u6d32\u8bed\u8a00\u7684PLMs\uff0c\u5e76\u7a81\u663e\u4e86\u4e3b\u52a8\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9002\u5e94\u7b56\u7565\u6210\u529f\u80cc\u540e\u7684\u5185\u90e8\u673a\u5236\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5728\u975e\u6d32\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u6301\u7eed\u6539\u5584\uff0c\u4f46\u8fd9\u4e9b\u8fdb\u6b65\u7684\u539f\u56e0\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86PLMs\u5728\u975e\u6d32\u8bed\u8a00\u4e2d\u7684\u8bed\u8a00\u77e5\u8bc6\u3002", "method": "\u6211\u4eec\u4e3a\u516d\u79cd\u8bed\u8a00\u5b66\u4e0a\u591a\u6837\u7684\u975e\u6d32\u8bed\u8a00\u8bad\u7ec3\u4e86\u9010\u5c42\u63a2\u6d4b\u5668\uff0c\u4ee5\u5206\u6790\u8bed\u8a00\u7279\u5f81\u7684\u5206\u5e03\u60c5\u51b5\u3002\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u63a7\u5236\u4efb\u52a1\uff0c\u4ee5\u89e3\u91ca\u63a2\u6d4b\u5668\u6027\u80fd\uff0c\u5e76\u5728MasakhaPOS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u8c03\u6574\u7684PLMs\u6bd4\u5927\u89c4\u6a21\u591a\u8bed\u8a00PLMs\u7f16\u7801\u4e86\u66f4\u591a\u7684\u76ee\u6807\u8bed\u8a00\u8bed\u8a00\u4fe1\u606f\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8bc1\u5b9e\u4e86\u4e4b\u524d\u7684\u53d1\u73b0\uff0c\u5373\u8bcd\u6cd5\u7ea7\u53e5\u6cd5\u4fe1\u606f\u96c6\u4e2d\u5728\u4e2d\u5c42\u5230\u6700\u540e\u4e00\u5c42\uff0c\u800c\u53e5\u5b50\u7ea7\u8bed\u4e49\u4fe1\u606f\u5206\u5e03\u5728\u6240\u6709\u5c42\u4e2d\u3002\u901a\u8fc7\u63a7\u5236\u4efb\u52a1\u548c\u63a2\u6d4b\u57fa\u7ebf\uff0c\u6211\u4eec\u786e\u8ba4\u6027\u80fd\u53cd\u6620\u4e86PLMs\u7684\u5185\u90e8\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u63a2\u6d4b\u5668\u7684\u8bb0\u5fc6\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5e94\u7528\u4e86\u5df2\u5efa\u7acb\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6765\u5206\u6790\u975e\u6d32\u8bed\u8a00\u7684PLMs\u3002\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u6211\u4eec\u7a81\u663e\u4e86\u4e3b\u52a8\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9002\u5e94\u7b56\u7565\u6210\u529f\u80cc\u540e\u7684\u5185\u90e8\u673a\u5236\u3002"}}
{"id": "2505.10328", "pdf": "https://arxiv.org/pdf/2505.10328", "abs": "https://arxiv.org/abs/2505.10328", "authors": ["Alvin Combrink", "Stephie Do", "Kristofer Bengtsson", "Sabino Francesco Roselli", "Martin Fabian"], "title": "A Comparative Study of SMT and MILP for the Nurse Rostering Problem", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 3 figures", "summary": "The effects of personnel scheduling on the quality of care and working\nconditions for healthcare personnel have been thoroughly documented. However,\nthe ever-present demand and large variation of constraints make healthcare\nscheduling particularly challenging. This problem has been studied for decades,\nwith limited research aimed at applying Satisfiability Modulo Theories (SMT).\nSMT has gained momentum within the formal verification community in the last\ndecades, leading to the advancement of SMT solvers that have been shown to\noutperform standard mathematical programming techniques.\n  In this work, we propose generic constraint formulations that can model a\nwide range of real-world scheduling constraints. Then, the generic constraints\nare formulated as SMT and MILP problems and used to compare the respective\nstate-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired\nrostering problems. Experimental results show how each solver excels for\ncertain types of problems; the MILP solver generally performs better when the\nproblem is highly constrained or infeasible, while the SMT solver performs\nbetter otherwise. On real-world inspired problems containing a more varied set\nof shifts and personnel, the SMT solver excels. Additionally, it was noted\nduring experimentation that the SMT solver was more sensitive to the way the\ngeneric constraints were formulated, requiring careful consideration and\nexperimentation to achieve better performance. We conclude that SMT-based\nmethods present a promising avenue for future research within the domain of\npersonnel scheduling.", "AI": {"tldr": "This paper explores the application of Satisfiability Modulo Theories (SMT) in healthcare scheduling, comparing SMT solvers like Z3 with traditional mathematical programming techniques like Gurobi. It finds that SMT solvers perform better in certain scenarios, especially with varied shift and personnel configurations, but require careful formulation of constraints for optimal performance.", "motivation": "The effects of personnel scheduling on the quality of care and working conditions for healthcare personnel have been thoroughly documented. However, the ever-present demand and large variation of constraints make healthcare scheduling particularly challenging. This problem has been studied for decades, with limited research aimed at applying Satisfiability Modulo Theories (SMT).", "method": "We propose generic constraint formulations that can model a wide range of real-world scheduling constraints. Then, the generic constraints are formulated as SMT and MILP problems and used to compare the respective state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired rostering problems.", "result": "Experimental results show how each solver excels for certain types of problems; the MILP solver generally performs better when the problem is highly constrained or infeasible, while the SMT solver performs better otherwise. On real-world inspired problems containing a more varied set of shifts and personnel, the SMT solver excels. Additionally, it was noted during experimentation that the SMT solver was more sensitive to the way the generic constraints were formulated, requiring careful consideration and experimentation to achieve better performance.", "conclusion": "SMT-based methods present a promising avenue for future research within the domain of personnel scheduling."}}
{"id": "2505.09847", "pdf": "https://arxiv.org/pdf/2505.09847", "abs": "https://arxiv.org/abs/2505.09847", "authors": ["Liyang Zhao", "Olurotimi Seton", "Himadeep Reddy Reddivari", "Suvendu Jena", "Shadow Zhao", "Rachit Kumar", "Changshuai Wei"], "title": "Causal Predictive Optimization and Generation for Business AI", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "comment": null, "summary": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u673a\u5668\u5b66\u4e60\u3001\u7ea6\u675f\u4f18\u5316\u548c\u751f\u6210AI\u7684\u9500\u552e\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5728LinkedIn\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u9500\u552e\u8fc7\u7a0b\u7684\u4f18\u5316\u5bf9\u4e8e\u4efb\u4f55B2B\u4e1a\u52a1\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7cfb\u7edf\u5728\u5904\u7406\u9500\u552e\u8fc7\u7a0b\u4e2d\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u4e09\u4e2a\u5c42\u6b21\u7684\u7cfb\u7edf\uff1a1) \u5e26\u6709\u56e0\u679c\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u5c42\uff1b2) \u5e26\u6709\u7ea6\u675f\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u5e26\u653b\u51fb\u7684\u4f18\u5316\u5c42\uff1b3) \u5e26\u6709\u751f\u6210AI\u548c\u53cd\u9988\u5faa\u73af\u7684\u90e8\u7f72\u5c42\u3002", "result": "\u672c\u6587\u5728LinkedIn\u4e0a\u5b9e\u73b0\u4e86\u8be5\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7cfb\u7edf\u7684\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u9500\u552e\u4f18\u5316\u548c\u5546\u4e1aAI\u65b9\u6cd5\uff0c\u5373\u56e0\u679c\u9884\u6d4b\u4f18\u5316\u4e0e\u751f\u6210\uff08CPOG\uff09\uff0c\u5e76\u5728LinkedIn\u4e0a\u8fdb\u884c\u4e86\u5b9e\u73b0\u548c\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u6210\u679c\u3002"}}
{"id": "2505.09990", "pdf": "https://arxiv.org/pdf/2505.09990", "abs": "https://arxiv.org/abs/2505.09990", "authors": ["Long Cheng", "Jiafei Duan", "Yi Ru Wang", "Haoquan Fang", "Boyang Li", "Yushan Huang", "Elvis Wang", "Ainaz Eftekhar", "Jason Lee", "Wentao Yuan", "Rose Hendrix", "Noah A. Smith", "Fei Xia", "Dieter Fox", "Ranjay Krishna"], "title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing", "categories": ["cs.CV"], "comment": "10 Pages, Dataset and code:https://pointarena.github.io/", "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PointArena\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6307\u9488\u5728\u5404\u79cd\u63a8\u7406\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u5e73\u53f0\u5305\u62ecPoint-Bench\u6570\u636e\u96c6\u3001Point-Battle\u7ade\u6280\u573a\u548cPoint-Act\u673a\u5668\u4eba\u7cfb\u7edf\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cMolmo-72B\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u800c\u76d1\u7763\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6307\u9488\u4f5c\u4e3a\u5728\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e2d\u5c06\u8bed\u8a00\u5b9a\u4f4d\u7684\u57fa\u672c\u4e14\u76f4\u89c2\u7684\u673a\u5236\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5305\u62ec\u673a\u5668\u4eba\u6280\u672f\u3001\u8f85\u52a9\u6280\u672f\u548c\u4ea4\u4e92\u5f0fAI\u7cfb\u7edf\u3002\u867d\u7136\u6700\u8fd1\u7684\u591a\u6a21\u6001\u6a21\u578b\u5f00\u59cb\u652f\u6301\u6307\u9488\u529f\u80fd\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u53ea\u5173\u6ce8\u53c2\u8003\u5bf9\u8c61\u5b9a\u4f4d\u4efb\u52a1\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86PointArena\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u591a\u6837\u63a8\u7406\u573a\u666f\u7684\u591a\u6a21\u6001\u6307\u9488\u3002PointArena\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1) Point-Bench\uff0c\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e94\u4e2a\u63a8\u7406\u7c7b\u522b\u4e2d\u7684\u7ea61,000\u4e2a\u6307\u9488\u4efb\u52a1\uff1b(2) Point-Battle\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u3001\u57fa\u4e8e\u7f51\u7edc\u7684\u7ade\u6280\u573a\uff0c\u4fc3\u8fdb\u76f2\u76ee\u7684\u6210\u5bf9\u6a21\u578b\u6bd4\u8f83\uff0c\u5df2\u7ecf\u6536\u96c6\u4e86\u8d85\u8fc74,500\u4e2a\u533f\u540d\u6295\u7968\uff1b\u548c(3) Point-Act\uff0c\u4e00\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u76f4\u63a5\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u6307\u9488\u80fd\u529b\u3002", "result": "\u6211\u4eec\u5bf9\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u4e13\u6709\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0cMolmo-72B\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c3d\u7ba1\u4e13\u6709\u6a21\u578b\u65e5\u76ca\u5c55\u73b0\u51fa\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4e13\u95e8\u9488\u5bf9\u6307\u9488\u4efb\u52a1\u7684\u76d1\u7763\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002\u5728\u6211\u4eec\u7684\u591a\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\u4e2d\uff0c\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u7a81\u663e\u4e86\u7cbe\u786e\u6307\u9488\u80fd\u529b\u5728\u4f7f\u591a\u6a21\u6001\u6a21\u578b\u6709\u6548\u8fde\u63a5\u62bd\u8c61\u63a8\u7406\u4e0e\u5177\u4f53\u73b0\u5b9e\u884c\u52a8\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cMolmo-72B\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c3d\u7ba1\u4e13\u6709\u6a21\u578b\u65e5\u76ca\u5c55\u73b0\u51fa\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4e13\u95e8\u9488\u5bf9\u6307\u9488\u4efb\u52a1\u7684\u76d1\u7763\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002\u5728\u6211\u4eec\u7684\u591a\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\u4e2d\uff0c\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u7a81\u663e\u4e86\u7cbe\u786e\u6307\u9488\u80fd\u529b\u5728\u4f7f\u591a\u6a21\u6001\u6a21\u578b\u6709\u6548\u8fde\u63a5\u62bd\u8c61\u63a8\u7406\u4e0e\u5177\u4f53\u73b0\u5b9e\u884c\u52a8\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2505.10089", "pdf": "https://arxiv.org/pdf/2505.10089", "abs": "https://arxiv.org/abs/2505.10089", "authors": ["Wei Liu", "Sony Trenous", "Leonardo F. R. Ribeiro", "Bill Byrne", "Felix Hieber"], "title": "XRAG: Cross-lingual Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.", "AI": {"tldr": "XRAG is a novel benchmark designed to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings. It is constructed from recent news articles and covers both monolingual and multilingual retrieval scenarios. The experimental results reveal two previously unreported challenges in cross-lingual RAG.", "motivation": "The need to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings where the user language does not match the retrieval results.", "method": "XRAG is constructed from recent news articles to ensure that its questions require external knowledge to be answered. It covers monolingual and multilingual retrieval scenarios and provides relevancy annotations for each retrieved document.", "result": "Experimental results on five LLMs uncover two previously unreported challenges in cross-lingual RAG: 1) in the monolingual retrieval setting, all evaluated models struggle with response language correctness; 2) in the multilingual retrieval setting, the main challenge lies in reasoning over retrieved information across languages rather than generation of non-English text.", "conclusion": "XRAG serves as a valuable benchmark for studying LLM reasoning abilities, even before considering the additional cross-lingual complexity."}}
{"id": "2505.10361", "pdf": "https://arxiv.org/pdf/2505.10361", "abs": "https://arxiv.org/abs/2505.10361", "authors": ["David Abel", "Michael Bowling", "Andr\u00e9 Barreto", "Will Dabney", "Shi Dong", "Steven Hansen", "Anna Harutyunyan", "Khimya Khetarpal", "Clare Lyle", "Razvan Pascanu", "Georgios Piliouras", "Doina Precup", "Jonathan Richens", "Mark Rowland", "Tom Schaul", "Satinder Singh"], "title": "Plasticity as the Mirror of Empowerment", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Our first finding is that\nplasticity is the mirror of empowerment: The agent's plasticity is identical to\nthe empowerment of the environment, and vice versa. Our second finding\nestablishes a tension between the plasticity and empowerment of an agent,\nsuggesting that agent design needs to be mindful of both characteristics. We\nexplore the implications of these findings, and suggest that plasticity,\nempowerment, and their relationship are essential to understanding agency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u79f0\u4e3a\u5851\u6599\u6027\u7684\u65b0\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u4e0e\u81ea\u4e3b\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4e86\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u4ee3\u7406\u5982\u4f55\u88ab\u5176\u89c2\u5bdf\u5230\u7684\u5185\u5bb9\u6240\u5f71\u54cd\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5f71\u54cd\u7684\u7a0b\u5ea6\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u8bba\u91cf\u2014\u2014\u5e7f\u4e49\u5b9a\u5411\u4fe1\u606f\uff0c\u5b9a\u4e49\u4e86\u5851\u6599\u6027\u3002", "result": "\u5851\u6599\u6027\u662f\u81ea\u4e3b\u6027\u7684\u955c\u50cf\uff0c\u4ee3\u7406\u7684\u5851\u6599\u6027\u7b49\u4e8e\u73af\u5883\u7684\u81ea\u4e3b\u6027\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u540c\u65f6\uff0c\u4ee3\u7406\u7684\u5851\u6599\u6027\u548c\u81ea\u4e3b\u6027\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\u3002", "conclusion": "\u5851\u6599\u6027\u548c\u81ea\u4e3b\u6027\u53ca\u5176\u5173\u7cfb\u5bf9\u4e8e\u7406\u89e3\u4ee3\u7406\u884c\u4e3a\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002"}}
{"id": "2505.09848", "pdf": "https://arxiv.org/pdf/2505.09848", "abs": "https://arxiv.org/abs/2505.09848", "authors": ["Aditya Raj", "Golrokh Mirzaei"], "title": "Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection", "categories": ["cs.LG", "eess.IV"], "comment": "11 pages", "summary": "Imaging and genomic data offer distinct and rich features, and their\nintegration can unveil new insights into the complex landscape of diseases. In\nthis study, we present a novel approach utilizing radiogenomic data including\nstructural MRI images and gene expression data, for Alzheimer's disease\ndetection. Our framework introduces a novel heterogeneous bipartite graph\nrepresentation learning featuring two distinct node types: genes and images.\nThe network can effectively classify Alzheimer's disease (AD) into three\ndistinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)\nclasses, utilizing a small dataset. Additionally, it identified which genes\nplay a significant role in each of these classification groups. We evaluate the\nperformance of our approach using metrics including classification accuracy,\nrecall, precision, and F1 score. The proposed technique holds potential for\nextending to radiogenomic-based classification to other diseases.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u7ed3\u6784MRI\u56fe\u50cf\u548c\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f02\u6784\u4e8c\u90e8\u56fe\u8868\u793a\u5b66\u4e60\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u5e76\u6709\u671b\u5e94\u7528\u4e8e\u5176\u4ed6\u75be\u75c5\u3002", "motivation": "\u5f71\u50cf\u548c\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u6574\u5408\u53ef\u4ee5\u63ed\u793a\u75be\u75c5\u590d\u6742\u666f\u89c2\u7684\u65b0\u89c1\u89e3\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6784\u4e8c\u90e8\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u7ed3\u6784MRI\u56fe\u50cf\u548c\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5206\u4e3a\u4e09\u4e2a\u4e0d\u540c\u7684\u9636\u6bb5\uff0c\u5e76\u8bc6\u522b\u51fa\u6bcf\u4e2a\u5206\u7c7b\u7ec4\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\u7684\u57fa\u56e0\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u75be\u75c5\u7684\u57fa\u4e8e\u653e\u5c04\u57fa\u56e0\u7ec4\u7684\u5206\u7c7b\u3002"}}
{"id": "2505.09997", "pdf": "https://arxiv.org/pdf/2505.09997", "abs": "https://arxiv.org/abs/2505.09997", "authors": ["Jinhyun Jang", "Jiyeong Lee", "Kwanghoon Sohn"], "title": "Descriptive Image-Text Matching with Graded Contextual Similarity", "categories": ["cs.CV"], "comment": null, "summary": "Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.", "AI": {"tldr": "DITM improves image-text matching by considering descriptive flexibility and enhancing hierarchical reasoning, outperforming existing methods in representing complex relationships.", "motivation": "Existing approaches use sparse binary supervision, which covers limited subsets of image-text relationships and neglects many-to-many correspondences and implicit connections from general to specific descriptions.", "method": "DITM proposes descriptive image-text matching by exploring the descriptive flexibility of language, using cumulative term frequency-inverse document frequency (TF-IDF) to balance pairwise similarity according to keywords in the sentence. It refines false negative labeling and builds more precise matching by aligning relevant sentences in a generic-to-specific order.", "result": "Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the effectiveness of DITM in representing complex image-text relationships. Additionally, DITM enhances the hierarchical reasoning ability of the model, supported by analysis on the HierarCaps benchmark.", "conclusion": "DITM enhances the hierarchical reasoning ability of the model and demonstrates effectiveness in representing complex image-text relationships compared to state-of-the-art approaches."}}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113", "abs": "https://arxiv.org/abs/2505.10113", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86S-MedQA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u4e8e\u9886\u57df\u8f6c\u6362\u800c\u975e\u77e5\u8bc6\u6ce8\u5165\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5728\u533b\u5b66QA\u8fd9\u79cd\u77e5\u8bc6\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\uff0c\u77e5\u8bc6\u6ce8\u5165\u7684\u5047\u8bbe\u662f\u5426\u9002\u7528\uff0c\u5e76\u63a2\u7d22\u5fae\u8c03\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u7684\u4f5c\u7528\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86S-MedQA\uff0c\u4e00\u4e2a\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4e34\u5e8a\u4e13\u79d1\u4e2d\u7684\u82f1\u8bed\u533b\u5b66\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u96c6\u3002\u6211\u4eec\u4f7f\u7528S-MedQA\u6765\u68c0\u67e5\u4e00\u4e2a\u4e0e\u77e5\u8bc6\u6ce8\u5165\u76f8\u5173\u7684\u6d41\u884c\u5047\u8bbe\u5728\u533b\u5b66QA\u7684\u77e5\u8bc6\u5bc6\u96c6\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "1) \u5728\u7279\u5b9a\u4e13\u79d1\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u5e76\u4e0d\u4e00\u5b9a\u80fd\u5728\u8be5\u4e13\u79d1\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff1b2) \u65e0\u8bba\u5728\u54ea\u4e2a\u4e13\u79d1\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6240\u6709\u4e13\u79d1\u7684\u4e34\u5e8a\u76f8\u5173\u672f\u8bed\u7684token\u6982\u7387\u90fd\u4f1a\u4e00\u81f4\u589e\u52a0\u3002", "conclusion": "\u6211\u4eec\u76f8\u4fe1\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u4e8e\u9886\u57df\u8f6c\u6362\uff08\u4f8b\u5982\uff0c\u4ece\u4e00\u822c\u5230\u533b\u5b66\uff09\uff0c\u800c\u4e0d\u662f\u77e5\u8bc6\u6ce8\u5165\uff0c\u5e76\u5efa\u8bae\u91cd\u65b0\u8003\u8651\u5fae\u8c03\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u7684\u89d2\u8272\u3002"}}
{"id": "2505.10399", "pdf": "https://arxiv.org/pdf/2505.10399", "abs": "https://arxiv.org/abs/2505.10399", "authors": ["Kaivalya Rawal", "Zihao Fu", "Eoin Delaney", "Chris Russell"], "title": "Evaluating Model Explanations without Ground Truth", "categories": ["cs.AI", "cs.LG", "I.2.6"], "comment": "https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth", "summary": "There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.09851", "pdf": "https://arxiv.org/pdf/2505.09851", "abs": "https://arxiv.org/abs/2505.09851", "authors": ["Shun Wang", "Shun-Li Shang", "Zi-Kui Liu", "Wenrui Hao"], "title": "ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "9 pages, 4 figures", "summary": "Traditional entropy-based methods - such as cross-entropy loss in\nclassification problems - have long been essential tools for quantifying\nuncertainty and disorder in data and developing artificial intelligence\nalgorithms. However, the rapid growth of data across various domains has\nintroduced new challenges, particularly the integration of heterogeneous\ndatasets with intrinsic disparities. In this paper, we extend zentropy theory\ninto the data science domain by introducing intrinsic entropy, enabling more\neffective learning from heterogeneous data sources. We propose a\nzentropy-enhanced neural network (ZENN) that simultaneously learns both energy\nand intrinsic entropy components, capturing the underlying structure of\nmulti-source data. To support this, we redesign the neural network architecture\nto better reflect the intrinsic properties and variability inherent in diverse\ndatasets. We demonstrate the effectiveness of ZENN on classification tasks and\nenergy landscape reconstructions, showing its superior generalization\ncapabilities and robustness-particularly in predicting high-order derivatives.\nAs a practical application, we employ ZENN to reconstruct the Helmholtz energy\nlandscape of Fe3Pt using data generated from DFT and capture key material\nbehaviors, including negative thermal expansion and the critical point in the\ntemperature-pressure space. Overall, our study introduces a novel approach for\ndata-driven machine learning grounded in zentropy theory, highlighting ZENN as\na versatile and robust deep learning framework for scientific problems\ninvolving complex, heterogeneous datasets.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8ezentropy\u7406\u8bba\u7684\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZENN\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u6709\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u71b5\u7684\u65b9\u6cd5\u5728\u91cf\u5316\u6570\u636e\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u65e0\u5e8f\u6027\u4ee5\u53ca\u5f00\u53d1\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u65b9\u9762\u4e00\u76f4\u662f\u975e\u5e38\u91cd\u8981\u7684\u5de5\u5177\u3002\u7136\u800c\uff0c\u5404\u79cd\u9886\u57df\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6574\u5408\u5177\u6709\u5185\u5728\u5dee\u5f02\u7684\u5f02\u6784\u6570\u636e\u96c6\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u5185\u5728\u71b5\uff0c\u5c06zentropy\u7406\u8bba\u6269\u5c55\u5230\u6570\u636e\u79d1\u5b66\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2azentropy\u589e\u5f3a\u578b\u795e\u7ecf\u7f51\u7edc\uff08ZENN\uff09\uff0c\u8be5\u7f51\u7edc\u540c\u65f6\u5b66\u4e60\u80fd\u91cf\u548c\u5185\u5728\u71b5\u7ec4\u4ef6\uff0c\u4ee5\u6355\u6349\u591a\u6e90\u6570\u636e\u7684\u6f5c\u5728\u7ed3\u6784\u3002\u6211\u4eec\u91cd\u65b0\u8bbe\u8ba1\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u7684\u56fa\u6709\u5c5e\u6027\u548c\u53d8\u5f02\u6027\u3002", "result": "\u6211\u4eec\u5728\u5206\u7c7b\u4efb\u52a1\u548c\u80fd\u91cf\u666f\u89c2\u91cd\u5efa\u4e2d\u8bc1\u660e\u4e86ZENN\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u4e86\u5176\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u9884\u6d4b\u9ad8\u9636\u5bfc\u6570\u65b9\u9762\u3002\u4f5c\u4e3a\u5b9e\u9645\u5e94\u7528\uff0c\u6211\u4eec\u4f7f\u7528ZENN\u91cd\u6784\u4e86Fe3Pt\u7684Helmholtz\u80fd\u91cf\u666f\u89c2\uff0c\u5e76\u6355\u83b7\u4e86\u5173\u952e\u6750\u6599\u884c\u4e3a\uff0c\u5305\u62ec\u8d1f\u70ed\u81a8\u80c0\u548c\u6e29\u5ea6-\u538b\u529b\u7a7a\u95f4\u4e2d\u7684\u4e34\u754c\u70b9\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8ezentropy\u7406\u8bba\u7684\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86ZENN\u4f5c\u4e3a\u5904\u7406\u590d\u6742\u5f02\u6784\u6570\u636e\u96c6\u7684\u79d1\u5b66\u95ee\u9898\u7684\u591a\u529f\u80fd\u4e14\u7a33\u5065\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2505.09998", "pdf": "https://arxiv.org/pdf/2505.09998", "abs": "https://arxiv.org/abs/2505.09998", "authors": ["Ying Zang", "Yuanqi Hu", "Xinyu Chen", "Yuxia Xu", "Suhui Wang", "Chunan Yu", "Lanyun Zhu", "Deyi Ji", "Xin Xu", "Tianrun Chen"], "title": "From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u76843D\u670d\u88c5\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u666e\u901a\u7528\u6237\u80fd\u591f\u5728AR/VR\u73af\u5883\u4e2d\u901a\u8fc7\u7b80\u5355\u76843D\u8349\u56fe\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u5b57\u670d\u88c5\u3002", "motivation": "\u5728\u6c89\u6d78\u5f0f\u6d88\u8d39\u7535\u5b50\u4ea7\u54c1\u65f6\u4ee3\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5e0c\u671b\u901a\u8fc7\u865a\u62df\u65f6\u5c1a\u6765\u8868\u8fbe\u81ea\u5df1\u7684\u8eab\u4efd\uff0c\u4f46\u73b0\u6709\u76843D\u670d\u88c5\u8bbe\u8ba1\u5de5\u5177\u5bf9\u666e\u901a\u7528\u6237\u6765\u8bf4\u4ecd\u7136\u96be\u4ee5\u4f7f\u7528\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a3D\u8349\u56fe\u9a71\u52a8\u76843D\u670d\u88c5\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6761\u4ef6\u6269\u6563\u6a21\u578b\u3001\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u7684\u8349\u56fe\u7f16\u7801\u5668\u4ee5\u53ca\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u6211\u4eec\u7684\u7cfb\u7edf\u80fd\u591f\u89e3\u91ca\u4e0d\u7cbe\u786e\u7684\u81ea\u7531\u624b\u8f93\u5165\u5e76\u751f\u6210\u903c\u771f\u3001\u4e2a\u6027\u5316\u7684\u670d\u88c5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86KO3DClothes\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u6613\u7528\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0b\u4e00\u4ee3\u6d88\u8d39\u5e73\u53f0\u4e0a\u7684\u6c11\u4e3b\u5316\u65f6\u5c1a\u8bbe\u8ba1\u7684\u524d\u666f\u3002"}}
{"id": "2505.10143", "pdf": "https://arxiv.org/pdf/2505.10143", "abs": "https://arxiv.org/abs/2505.10143", "authors": ["Longchao Da", "Parth Mitesh Shah", "Kuan-Ru Liou", "Jiaxing Zhang", "Hua Wei"], "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4; H.3.3"], "comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track", "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6GE-Chat\uff0c\u4ee5\u63d0\u9ad8LLM\u8f93\u51fa\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u521b\u5efa\u77e5\u8bc6\u56fe\u8c31\u5e76\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u903b\u8f91\u751f\u6210\u3001\u591a\u8df3\u5b50\u56fe\u641c\u7d22\u548c\u57fa\u4e8e\u8574\u542b\u7684\u53e5\u5b50\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u8bc1\u636e\u68c0\u7d22\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u73b0\u5728\u662f\u4eba\u7c7b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u52a9\u624b\uff0c\u4f46\u5b83\u4eec\u7684\u8f93\u51fa\u5e76\u4e0d\u603b\u662f\u53ef\u9760\u7684\uff0c\u7528\u6237\u5fc5\u987b\u624b\u52a8\u8bc4\u4f30\u3002\u5e7b\u89c9\u54cd\u5e94\u5e38\u5e38\u5e26\u6709\u770b\u4f3c\u5408\u7406\u7684\u89e3\u91ca\uff0c\u7ed9\u7528\u6237\u5e26\u6765\u590d\u6742\u6027\u548c\u4fe1\u4efb\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u68c0\u67e5LLM\u7684\u7ed3\u8bba\u5e76\u5e2e\u52a9\u5224\u65ad\u5176\u53ef\u4fe1\u5ea6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86GE-Chat\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u3002\u5f53\u7528\u6237\u4e0a\u4f20\u6750\u6599\u6587\u6863\u65f6\uff0c\u4f1a\u521b\u5efa\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\uff0c\u8fd9\u6709\u52a9\u4e8e\u6784\u5efa\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u4ee3\u7406\uff0c\u4ee5\u989d\u5916\u7684\u77e5\u8bc6\u589e\u5f3a\u4ee3\u7406\u7684\u54cd\u5e94\u3002\u7136\u540e\u5229\u7528\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u903b\u8f91\u751f\u6210\u3001\u591a\u8df3\u5b50\u56fe\u641c\u7d22\u548c\u57fa\u4e8e\u8574\u542b\u7684\u53e5\u5b50\u751f\u6210\u6765\u5b9e\u73b0\u51c6\u786e\u7684\u8bc1\u636e\u68c0\u7d22\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u81ea\u7531\u5f62\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u8bc6\u522b\u7cbe\u786e\u8bc1\u636e\u65b9\u9762\u63d0\u9ad8\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u5f0f\u6765\u68c0\u67e5LLM\u7ed3\u8bba\u7684\u8d44\u6e90\uff0c\u5e76\u5e2e\u52a9\u5224\u65ad\u5176\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6GE-Chat\uff0c\u4ee5\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u54cd\u5e94\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u5efa\u77e5\u8bc6\u56fe\u8c31\u5e76\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u903b\u8f91\u751f\u6210\u3001\u591a\u8df3\u5b50\u56fe\u641c\u7d22\u548c\u57fa\u4e8e\u8574\u542b\u7684\u53e5\u5b50\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u73b0\u6709\u6a21\u578b\u5728\u81ea\u7531\u5f62\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u8bc6\u522b\u7cbe\u786e\u8bc1\u636e\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u5f0f\u6765\u68c0\u67e5LLM\u7ed3\u8bba\u7684\u8d44\u6e90\u5e76\u5e2e\u52a9\u5224\u65ad\u5176\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.10468", "pdf": "https://arxiv.org/pdf/2505.10468", "abs": "https://arxiv.org/abs/2505.10468", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "categories": ["cs.AI"], "comment": "32 pages, 14 figures, 11 tables", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "AI": {"tldr": "\u672c\u6587\u533a\u5206\u4e86AI\u4ee3\u7406\u548cAgentic AI\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u8bbe\u8ba1\u54f2\u5b66\u3001\u80fd\u529b\u3001\u5e94\u7528\u9886\u57df\u548c\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u6f84\u6e05AI\u4ee3\u7406\u548cAgentic AI\u7684\u4e0d\u540c\u8bbe\u8ba1\u54f2\u5b66\u548c\u80fd\u529b\uff0c\u4ee5\u4fc3\u8fdb\u5b83\u4eec\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u6982\u5ff5\u5206\u7c7b\u6cd5\u3001\u5e94\u7528\u6620\u5c04\u548c\u6311\u6218\u5206\u6790\uff0c\u533a\u5206\u4e86AI\u4ee3\u7406\u548cAgentic AI\uff0c\u5e76\u901a\u8fc7\u987a\u5e8f\u8bc4\u4f30\u67b6\u6784\u6f14\u53d8\u3001\u64cd\u4f5c\u673a\u5236\u3001\u4ea4\u4e92\u98ce\u683c\u548c\u81ea\u4e3b\u7ea7\u522b\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u6bcf\u4e2a\u8303\u5f0f\u7684\u72ec\u7279\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5982ReAct\u5faa\u73af\u3001RAG\u3001\u7f16\u6392\u5c42\u548c\u56e0\u679c\u5efa\u6a21\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5e94\u7528\u9886\u57df\u4e2d\u7684\u5bf9\u6bd4\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u5f00\u53d1\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684AI\u4ee3\u7406\u548cAgentic AI\u9a71\u52a8\u7684\u7cfb\u7edf\u63d0\u4f9b\u660e\u786e\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2505.09854", "pdf": "https://arxiv.org/pdf/2505.09854", "abs": "https://arxiv.org/abs/2505.09854", "authors": ["Harikrishna Kuttivelil", "Katia Obraczka"], "title": "Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence", "categories": ["cs.LG", "cs.ET", "cs.MA", "cs.SI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "As demand for intelligent services rises and edge devices become more\ncapable, distributed learning at the network edge has emerged as a key enabling\ntechnology. While existing paradigms like federated learning (FL) and\ndecentralized FL (DFL) enable privacy-preserving distributed learning in many\nscenarios, they face potential challenges in connectivity and synchronization\nimposed by resource-constrained and infrastructure-less environments. While\nmore robust, gossip learning (GL) algorithms have generally been designed for\nhomogeneous data distributions and may not suit all contexts. This paper\nintroduces Chisme, a novel suite of protocols designed to address the\nchallenges of implementing robust intelligence in the network edge,\ncharacterized by heterogeneous data distributions, episodic connectivity, and\nlack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and\nasynchronous GL (Chisme-GL) variants that enable collaborative yet\ndecentralized model training that considers underlying data heterogeneity. We\nintroduce a data similarity heuristic that allows agents to opportunistically\ninfer affinity with each other using the existing communication of model\nupdates in decentralized FL and GL. We leverage the heuristic to extend DFL's\nmodel aggregation and GL's model merge mechanisms for better personalized\ntraining while maintaining collaboration. While Chisme-DFL is a synchronous\ndecentralized approach whose resource utilization scales linearly with network\nsize, Chisme-GL is fully asynchronous and has a lower, constant resource\nrequirement independent of network size. We demonstrate that Chisme methods\noutperform their standard counterparts in model training over distributed and\nheterogeneous data in network scenarios ranging from less connected and\nreliable networks to fully connected and lossless networks.", "AI": {"tldr": "This paper introduces Chisme, a novel suite of protocols for robust intelligence at the network edge, addressing challenges such as heterogeneous data distributions, episodic connectivity, and lack of infrastructure. It includes synchronous DFL and asynchronous GL variants, along with a data similarity heuristic to improve personalized training while maintaining collaboration.", "motivation": "Distributed learning at the network edge has emerged as a key enabling technology, but existing paradigms like federated learning (FL) and decentralized FL (DFL) face challenges in connectivity and synchronization in resource-constrained and infrastructure-less environments. Gossip learning (GL) algorithms are more robust but have generally been designed for homogeneous data distributions and may not suit all contexts.", "method": "Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training that considers underlying data heterogeneity. A data similarity heuristic is introduced to allow agents to opportunistically infer affinity with each other using the existing communication of model updates in decentralized FL and GL.", "result": "Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.", "conclusion": "Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks."}}
{"id": "2505.10016", "pdf": "https://arxiv.org/pdf/2505.10016", "abs": "https://arxiv.org/abs/2505.10016", "authors": ["Shijie Lyu"], "title": "Application of YOLOv8 in monocular downward multiple Car Target detection", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "comment": "Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering", "summary": "Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u6539\u8fdb\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u91cd\u65b0\u53c2\u6570\u5316\u6280\u672f\u3001\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u7f51\u7edc\u6a21\u578b\u548c\u65b0\u7684\u68c0\u6d4b\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u591a\u5c3a\u5ea6\u3001\u5c0f\u76ee\u6807\u548c\u8fdc\u8ddd\u79bb\u76ee\u6807\u7684\u9ad8\u6548\u7cbe\u786e\u68c0\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u578b\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5927\u76ee\u6807\u548c\u5c0f\u76ee\u6807\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523065%\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002\u8be5\u6539\u8fdb\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6bd4\u8d5b\uff0c\u5982Formula Student Autonomous China (FSAC)\uff0c\u5728\u5355\u76ee\u6807\u548c\u5c0f\u76ee\u6807\u68c0\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u7684\u73af\u5883\u611f\u77e5\u96f7\u8fbe\u3001\u9053\u8def\u611f\u77e5\u76f8\u673a\u548c\u8f66\u8f86\u4f20\u611f\u5668\u7f51\u7edc\u9762\u4e34\u6210\u672c\u9ad8\u3001\u6613\u53d7\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u5f71\u54cd\u4ee5\u53ca\u5206\u8fa8\u7387\u6709\u9650\u7b49\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u6539\u8fdb\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u91cd\u65b0\u53c2\u6570\u5316\u6280\u672f\u3001\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u7f51\u7edc\u6a21\u578b\u548c\u65b0\u7684\u68c0\u6d4b\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u591a\u5c3a\u5ea6\u3001\u5c0f\u76ee\u6807\u548c\u8fdc\u8ddd\u79bb\u76ee\u6807\u7684\u9ad8\u6548\u7cbe\u786e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u578b\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5927\u76ee\u6807\u548c\u5c0f\u76ee\u6807\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523065%\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6539\u8fdb\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6bd4\u8d5b\uff0c\u5982Formula Student Autonomous China (FSAC)\uff0c\u5728\u5355\u76ee\u6807\u548c\u5c0f\u76ee\u6807\u68c0\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.10182", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u6301\u7eed\u9884\u8bad\u7ec3\uff08Reasoning CPT\uff09\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u5f53\u524d\uff0c\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff08\u5982\u6570\u5b66\u548c\u7f16\u7a0b\uff09\u7684\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u5e7f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002\u800c\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u4e0d\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u4fe1\u53f7\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5408\u6210\u63a8\u7406\u6570\u636e\u4ee5\u53ca\u8fd9\u4e9b\u6570\u636e\u5bf9\u5e7f\u6cdb\u9886\u57df\u7684\u5f71\u54cd\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528Reasoning CPT\u65b9\u6cd5\uff0c\u5229\u7528\u6765\u81eaSTEM\u548c\u6cd5\u5f8b\u8bed\u6599\u5e93\u7684\u9690\u85cf\u601d\u7ef4\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eGemma2-9B\u6a21\u578b\u4e0a\uff0c\u4e0e\u6807\u51c6CPT\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cReasoning CPT\u5728\u6240\u6709\u8bc4\u4f30\u9886\u57df\u90fd\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u66f4\u590d\u6742\u7684\u95ee\u9898\u4e0a\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u5dee\u8ddd\u589e\u5927\uff0c\u6700\u9ad8\u53ef\u8fbe8\u5206\u3002\u6b64\u5916\uff0c\u7ecf\u8fc7\u9690\u85cf\u601d\u7ef4\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u96be\u5ea6\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u4f7f\u7528\u5305\u542b\u9690\u85cf\u601d\u7ef4\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff08Reasoning CPT\uff09\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u5728\u9762\u5bf9\u66f4\u590d\u6742\u7684\u95ee\u9898\u65f6\uff0c\u8fd9\u79cd\u4f18\u52bf\u66f4\u52a0\u660e\u663e\u3002"}}
{"id": "2505.10543", "pdf": "https://arxiv.org/pdf/2505.10543", "abs": "https://arxiv.org/abs/2505.10543", "authors": ["Annie Wong", "Thomas B\u00e4ck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "title": "Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u81ea\u6211\u53cd\u601d\u3001\u542f\u53d1\u5f0f\u53d8\u5f02\u548c\u8ba1\u5212\u4f5c\u4e3a\u63d0\u793a\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u8f83\u5927\u6a21\u578b\u901a\u5e38\u8868\u73b0\u4f18\u4e8e\u8f83\u5c0f\u6a21\u578b\uff0c\u4f46\u6218\u7565\u63d0\u793a\u53ef\u4ee5\u7f29\u5c0f\u6027\u80fd\u5dee\u8ddd\u3002\u592a\u957f\u7684\u63d0\u793a\u4f1a\u8d1f\u9762\u5f71\u54cd\u8f83\u5c0f\u6a21\u578b\u7684\u57fa\u672c\u53cd\u5e94\u4efb\u52a1\uff0c\u800c\u8f83\u5927\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u7a33\u5065\u7684\u884c\u4e3a\u3002\u5148\u8fdb\u7684\u63d0\u793a\u6280\u672f\u4e3b\u8981\u5bf9\u8f83\u5c0f\u6a21\u578b\u5728\u590d\u6742\u6e38\u620f\u4e2d\u6709\u76ca\uff0c\u4f46\u5bf9\u5df2\u7ecf\u8868\u73b0\u826f\u597d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6539\u5584\u8f83\u5c11\u3002\u7136\u800c\uff0c\u9ad8\u7ea7\u63a8\u7406\u65b9\u6cd5\u7684\u7ed3\u679c\u9ad8\u5ea6\u53ef\u53d8\uff0c\u53ef\u80fd\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\u5e76\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5982\u89c4\u5212\u3001\u63a8\u7406\u548c\u7a7a\u95f4\u534f\u8c03\u4e0a\u4ecd\u7136\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u63d0\u793a\u5b8c\u5168\u514b\u670d\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u5b66\u4e60\u548c\u63a8\u7406\u80fd\u529b\u7684\u771f\u6b63\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u81ea\u6211\u53cd\u601d\u3001\u542f\u53d1\u5f0f\u53d8\u5f02\u548c\u8ba1\u5212\u4f5c\u4e3a\u63d0\u793a\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u4ee5\u6d4b\u8bd5\u4ee3\u7406\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u8f83\u5927\u7684\u6a21\u578b\u901a\u5e38\u8868\u73b0\u4f18\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u6218\u7565\u63d0\u793a\u53ef\u4ee5\u7f29\u5c0f\u8fd9\u79cd\u6027\u80fd\u5dee\u8ddd\u3002\u592a\u957f\u7684\u63d0\u793a\u4f1a\u8d1f\u9762\u5f71\u54cd\u8f83\u5c0f\u6a21\u578b\u7684\u57fa\u672c\u53cd\u5e94\u4efb\u52a1\uff0c\u800c\u8f83\u5927\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u7a33\u5065\u7684\u884c\u4e3a\u3002\u5148\u8fdb\u7684\u63d0\u793a\u6280\u672f\u4e3b\u8981\u5bf9\u8f83\u5c0f\u6a21\u578b\u5728\u590d\u6742\u6e38\u620f\u4e2d\u6709\u76ca\uff0c\u4f46\u5bf9\u5df2\u7ecf\u8868\u73b0\u826f\u597d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6539\u5584\u8f83\u5c11\u3002\u7136\u800c\uff0c\u9ad8\u7ea7\u63a8\u7406\u65b9\u6cd5\u7684\u7ed3\u679c\u9ad8\u5ea6\u53ef\u53d8\uff1a\u5f53\u63a8\u7406\u548c\u51b3\u7b56\u4e00\u81f4\u65f6\uff0c\u5b83\u4eec\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u4e5f\u53ef\u80fd\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\u5e76\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5982\u89c4\u5212\u3001\u63a8\u7406\u548c\u7a7a\u95f4\u534f\u8c03\u4e0a\u4ecd\u7136\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u63d0\u793a\u5b8c\u5168\u514b\u670d\u3002"}}
{"id": "2505.09855", "pdf": "https://arxiv.org/pdf/2505.09855", "abs": "https://arxiv.org/abs/2505.09855", "authors": ["Alexander Y. Ku", "Thomas L. Griffiths", "Stephanie C. Y. Chan"], "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u4e2d\u7684\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\uff08IWL\u548cICL\uff09\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u73af\u5883\u7a33\u5b9a\u6027\u548c\u7ebf\u7d22\u53ef\u9760\u6027\u5bf9\u5b83\u4eec\u5e73\u8861\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6211\u4eec\u501f\u9274\u4e86\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u7684\u7c7b\u4f3c\u9002\u5e94\u7b56\u7565\u3002", "method": "\u6211\u4eec\u4ece\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u7c7b\u4f3c\u7684\u9002\u5e94\u7b56\u7565\u4e2d\u83b7\u5f97\u7075\u611f\uff0c\u5b9e\u9a8c\u64cd\u4f5c\u4e86\u53ef\u9884\u6d4b\u6027\u7684\u8fd9\u4e9b\u7ef4\u5ea6\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5b83\u4eec\u5bf9\u53d8\u538b\u5668\u4e2dICL/IWL\u5e73\u8861\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u73af\u5883\u7a33\u5b9a\u6027\u660e\u663e\u6709\u5229\u4e8eIWL\uff0c\u800c\u9ad8\u7ebf\u7d22\u53ef\u9760\u6027\u5219\u589e\u5f3a\u4e86ICL\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u7a33\u5b9a\u6027\u8f83\u4f4e\u65f6\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u52a8\u6001\u663e\u793a\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u968f\u65f6\u95f4\u6f14\u53d8\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\uff0c\u5177\u6709\u8bb8\u591a\u7c7b\u522b\u7684\u5206\u7c7b\uff09\uff0c\u4f1a\u53d1\u751f\u5178\u578b\u7684ICL\u5230IWL\u8f6c\u53d8\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\uff0c\u8f83\u5c11\u7c7b\u522b\u6216\u8f83\u6162\u7684ICL\u83b7\u53d6\uff09\u53ef\u80fd\u4f1a\u51fa\u73b0\u521d\u59cbIWL\u9636\u6bb5\uff0c\u968f\u540e\u88abICL\u4e3b\u5bfc\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u4e86\u5173\u4e8e\u89e3\u91ca\u8fd9\u4e9b\u5b66\u4e60\u6a21\u5f0f\u8f6c\u6362\u7684\u76f8\u5bf9\u6210\u672c\u5047\u8bbe\uff0c\u786e\u7acb\u4e86\u53ef\u9884\u6d4b\u6027\u4f5c\u4e3a\u63a7\u5236\u53d8\u538b\u5668\u9002\u5e94\u7b56\u7565\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u6765\u7406\u89e3ICL\u5e76\u6307\u5bfc\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2505.10027", "pdf": "https://arxiv.org/pdf/2505.10027", "abs": "https://arxiv.org/abs/2505.10027", "authors": ["Shijie Lyu"], "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)", "summary": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u9065\u611f\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5b9e\u9645\u610f\u4e49\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u53d6\u5f97\u4e86\u4e00\u5b9a\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u548c\u4fdd\u6301\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u5728LDM\u6a21\u578b\u7684\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f18\u5316\u51b3\u7b56\u76ee\u6807\u3002", "result": "\u5728RESISC45\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728PSNR\u3001SSIM\u548cLPIPS\u6307\u6807\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0cPSNR\u63d0\u9ad8\u4e863-4dB\uff0cSSIM\u63d0\u9ad8\u4e860.08-0.11\uff0cLPIPS\u964d\u4f4e\u4e860.06-0.10\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u5316\u548c\u590d\u6742\u7684\u81ea\u7136\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u548c\u573a\u666f\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2505.10185", "pdf": "https://arxiv.org/pdf/2505.10185", "abs": "https://arxiv.org/abs/2505.10185", "authors": ["Seongyun Lee", "Seungone Kim", "Minju Seo", "Yongrae Jo", "Dongyoung Go", "Hyeonbin Hwang", "Jinho Park", "Xiang Yue", "Sean Welleck", "Graham Neubig", "Moontae Lee", "Minjoon Seo"], "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CoT Encyclopedia\uff0c\u4e00\u4e2a\u81ea\u4e0b\u800c\u4e0a\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u5f15\u5bfc\u6a21\u578b\u63a8\u7406\u3002\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u63d0\u53d6\u591a\u6837\u5316\u7684\u63a8\u7406\u6807\u51c6\uff0c\u5e76\u5c06\u5176\u805a\u7c7b\u4e3a\u4ee3\u8868\u6027\u7c7b\u522b\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u53ef\u89e3\u91ca\u548c\u5168\u9762\u7684\u5206\u6790\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u5bf9\u63a8\u7406\u884c\u4e3a\u7684\u5f71\u54cd\u5927\u4e8e\u6570\u636e\u9886\u57df\uff0c\u5f3a\u8c03\u4e86\u683c\u5f0f\u611f\u77e5\u6a21\u578b\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4e4b\u524d\u7684\u7814\u7a76\u5c1d\u8bd5\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u7b56\u7565\u7c7b\u578b\u5bf9CoT\u8fdb\u884c\u5206\u7c7b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53d7\u5230\u4eba\u7c7b\u76f4\u89c9\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u884c\u4e3a\u7684\u5168\u90e8\u591a\u6837\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u548c\u81ea\u52a8\u5316\u7684\u6846\u67b6\u6765\u5206\u6790\u548c\u5f15\u5bfc\u6a21\u578b\u63a8\u7406\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CoT Encyclopedia\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u4e0b\u800c\u4e0a\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u5f15\u5bfc\u6a21\u578b\u63a8\u7406\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4ece\u6a21\u578b\u751f\u6210\u7684CoTs\u4e2d\u81ea\u52a8\u63d0\u53d6\u591a\u6837\u7684\u63a8\u7406\u6807\u51c6\uff0c\u5c06\u5b83\u4eec\u5d4c\u5165\u5230\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u805a\u7c7b\u6210\u4ee3\u8868\u6027\u7684\u7c7b\u522b\uff0c\u5e76\u63a8\u5bfc\u51fa\u5bf9\u6bd4\u6027\u6807\u51c6\u6765\u89e3\u91ca\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6846\u67b6\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u5168\u9762\u7684\u5206\u6790\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u7406\u89e3\u53ef\u4ee5\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff1a\u6211\u4eec\u53ef\u4ee5\u9884\u6d4b\u6a21\u578b\u53ef\u80fd\u4f7f\u7528\u7684\u7b56\u7565\uff0c\u5e76\u5f15\u5bfc\u5b83\u8d70\u5411\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7CoT Encyclopedia\u6846\u67b6\u53ef\u4ee5\u66f4\u5168\u9762\u548c\u53ef\u89e3\u91ca\u5730\u5206\u6790\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u5e76\u4e14\u8fd9\u79cd\u7406\u89e3\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u5bf9\u63a8\u7406\u884c\u4e3a\u7684\u5f71\u54cd\u5927\u4e8e\u6570\u636e\u9886\u57df\uff0c\u8fd9\u5f3a\u8c03\u4e86\u683c\u5f0f\u611f\u77e5\u6a21\u578b\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2306.07615", "pdf": "https://arxiv.org/pdf/2306.07615", "abs": "https://arxiv.org/abs/2306.07615", "authors": ["Heqin Zhu", "Quan Quan", "Qingsong Yao", "Zaiyi Liu", "S. Kevin Zhou"], "title": "UOD: Universal One-shot Detection of Anatomical Landmarks", "categories": ["cs.CV", "cs.AI"], "comment": "Eealy accepted by MICCAI 2023. 11pages, 4 figures, 2 tables. arXiv\n  admin note: text overlap with arXiv:2203.06433", "summary": "One-shot medical landmark detection gains much attention and achieves great\nsuccess for its label-efficient training process. However, existing one-shot\nlearning methods are highly specialized in a single domain and suffer domain\npreference heavily in the situation of multi-domain unlabeled data. Moreover,\none-shot learning is not robust that it faces performance drop when annotating\na sub-optimal image. To tackle these issues, we resort to developing a\ndomain-adaptive one-shot landmark detection framework for handling multi-domain\nmedical images, named Universal One-shot Detection (UOD). UOD consists of two\nstages and two corresponding universal models which are designed as\ncombinations of domain-specific modules and domain-shared modules. In the first\nstage, a domain-adaptive convolution model is self-supervised learned to\ngenerate pseudo landmark labels. In the second stage, we design a\ndomain-adaptive transformer to eliminate domain preference and build the global\ncontext for multi-domain data. Even though only one annotated sample from each\ndomain is available for training, the domain-shared modules help UOD aggregate\nall one-shot samples to detect more robust and accurate landmarks. We\ninvestigated both qualitatively and quantitatively the proposed UOD on three\nwidely-used public X-ray datasets in different anatomical domains (i.e., head,\nhand, chest) and obtained state-of-the-art performances in each domain. The\ncode is available at\nhttps://github.com/heqin-zhu/UOD_universal_oneshot_detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUOD\u7684\u9886\u57df\u81ea\u9002\u5e94\u5355\u6b21\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u9886\u57df\u533b\u5b66\u56fe\u50cf\uff0c\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u89e3\u5256\u9886\u57df\u4e2d\u7684\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6b21\u5b66\u4e60\u65b9\u6cd5\u5728\u5355\u4e00\u9886\u57df\u4e2d\u9ad8\u5ea6\u4e13\u4e1a\u5316\uff0c\u5e76\u4e14\u5728\u591a\u9886\u57df\u672a\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u906d\u53d7\u9886\u57df\u504f\u597d\u3002\u6b64\u5916\uff0c\u5355\u6b21\u5b66\u4e60\u5728\u6807\u6ce8\u6b21\u4f18\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "UOD\u7531\u4e24\u4e2a\u9636\u6bb5\u548c\u4e24\u4e2a\u76f8\u5e94\u7684\u901a\u7528\u6a21\u578b\u7ec4\u6210\uff0c\u8fd9\u4e9b\u6a21\u578b\u8bbe\u8ba1\u4e3a\u9886\u57df\u7279\u5b9a\u6a21\u5757\u548c\u9886\u57df\u5171\u4eab\u6a21\u5757\u7684\u7ec4\u5408\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u751f\u6210\u4f2a\u5730\u6807\u6807\u7b7e\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9886\u57df\u9002\u5e94\u7684Transformer\u6765\u6d88\u9664\u9886\u57df\u504f\u597d\u5e76\u6784\u5efa\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5373\u4f7f\u6bcf\u4e2a\u9886\u57df\u53ea\u63d0\u4f9b\u4e00\u4e2a\u6807\u6ce8\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u9886\u57df\u5171\u4eab\u6a21\u5757\u5e2e\u52a9UOD\u805a\u5408\u6240\u6709\u5355\u6b21\u6837\u672c\u4ee5\u68c0\u6d4b\u66f4\u7a33\u5065\u548c\u51c6\u786e\u7684\u5730\u6807\u3002", "conclusion": "UOD\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u516c\u5171X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u660e\u5176\u5728\u591a\u9886\u57df\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09861", "pdf": "https://arxiv.org/pdf/2505.09861", "abs": "https://arxiv.org/abs/2505.09861", "authors": ["John Bencina", "Erkut Aykutlug", "Yue Chen", "Zerui Zhang", "Stephanie Sorenson", "Shao Tang", "Changshuai Wei"], "title": "LiDDA: Data Driven Attribution at LinkedIn", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ME"], "comment": null, "summary": "Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u5f52\u56e0\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u6210\u5458\u7ea7\u6570\u636e\u3001\u6c47\u603b\u7ea7\u6570\u636e\u4ee5\u53ca\u5916\u90e8\u5b8f\u89c2\u56e0\u7d20\u7684\u6574\u5408\uff0c\u5e76\u5728LinkedIn\u4e0a\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u65bd\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u5f52\u56e0\u662f\u73b0\u4ee3\u8425\u9500\u667a\u80fd\u7684\u57fa\u7840\uff0c\u5bf9\u4e8e\u4efb\u4f55\u8425\u9500\u4e1a\u52a1\u548c\u5e7f\u544a\u5e73\u53f0\u90fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u7ea7\u522b\u7684\u6570\u636e\u548c\u5916\u90e8\u56e0\u7d20\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u5f52\u56e0\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u6210\u5458\u7ea7\u6570\u636e\u3001\u6c47\u603b\u7ea7\u6570\u636e\u4ee5\u53ca\u5916\u90e8\u5b8f\u89c2\u56e0\u7d20\u7684\u6574\u5408\u3002", "result": "\u672c\u6587\u5728LinkedIn\u4e0a\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u5927\u89c4\u6a21\u5b9e\u65bd\u6548\u679c\uff0c\u5e76\u5206\u4eab\u4e86\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8425\u9500\u548c\u5e7f\u544a\u6280\u672f\u9886\u57df\u7684\u5b66\u4e60\u548c\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u5f52\u56e0\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728LinkedIn\u7684\u5927\u89c4\u6a21\u5b9e\u65bd\u6548\u679c\uff0c\u4ee5\u53ca\u5bf9\u8425\u9500\u548c\u5e7f\u544a\u6280\u672f\u9886\u57df\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.10030", "pdf": "https://arxiv.org/pdf/2505.10030", "abs": "https://arxiv.org/abs/2505.10030", "authors": ["Miit Daga", "Dhriti Parikh", "Swarna Priya Ramu"], "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication", "summary": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DeepSeqCoco\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u6930\u5b50\u6811\u56fe\u50cf\u4e2d\u51c6\u786e\u81ea\u52a8\u5730\u8bc6\u522b\u75be\u75c5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u9884\u6d4b\u65f6\u95f4\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u75be\u75c5\u8bc6\u522b\u65b9\u6cd5\u662f\u624b\u52a8\u7684\u3001\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u4e14\u4e0d\u53ef\u6269\u5c55\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u65e9\u671f\u8bca\u65ad\u548c\u5e72\u9884\u3002", "method": "\u63d0\u51fa\u4e86DeepSeqCoco\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u6930\u5b50\u6811\u56fe\u50cf\u4e2d\u51c6\u786e\u81ea\u52a8\u5730\u8bc6\u522b\u75be\u75c5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeepSeqCoco\u53ef\u4ee5\u8fbe\u5230\u9ad8\u8fbe99.5%\u7684\u51c6\u786e\u7387\uff08\u6bd4\u73b0\u6709\u6a21\u578b\u9ad8\u51fa\u6700\u591a5%\uff09\uff0c\u6df7\u5408SGD-Adam\u663e\u793a\u51fa\u6700\u4f4e\u7684\u9a8c\u8bc1\u635f\u59312.81%\u3002\u8bad\u7ec3\u65f6\u95f4\u548c\u9884\u6d4b\u65f6\u95f4\u5206\u522b\u51cf\u5c11\u4e86\u6700\u591a18%\u548c85%\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u901a\u8fc7\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u75be\u75c5\u76d1\u6d4b\u7cfb\u7edf\u6539\u5584\u7cbe\u51c6\u519c\u4e1a\u7684\u524d\u666f\u3002"}}
{"id": "2505.10202", "pdf": "https://arxiv.org/pdf/2505.10202", "abs": "https://arxiv.org/abs/2505.10202", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "YiMing Cheng", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.", "AI": {"tldr": "VQ-Logits is a novel approach that uses Vector Quantization to reduce the parameter count and computational load of the LLM output layer, achieving significant improvements in efficiency with minimal impact on performance.", "motivation": "Large Language Models (LLMs) face significant computational and memory challenges due to their extensive output vocabularies. The final linear projection layer, mapping hidden states to vocabulary-sized logits, often constitutes a substantial portion of the model's parameters and computational cost during inference.", "method": "VQ-Logits leverages Vector Quantization (VQ) to replace the large V * dmodel output embedding matrix with a small, shared codebook of K embedding vectors (K << V). Each token in the vocabulary is mapped to one of these K codebook vectors. The LLM predicts logits over this compact codebook, which are then efficiently 'scattered' to the full vocabulary space using the learned or preassigned mapping.", "result": "Through extensive experiments on standard language modeling benchmarks (e.g., WikiText-103, C4), VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines.", "conclusion": "VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines."}}
{"id": "2410.13778", "pdf": "https://arxiv.org/pdf/2410.13778", "abs": "https://arxiv.org/abs/2410.13778", "authors": ["Michelangelo Olmo Nogara Notarianni", "Filippo Leveni", "Diego Stucchi", "Luca Frittoli", "Giacomo Boracchi"], "title": "Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "AALTD workshop at ECML 2024 (https://ecml-aaltd.github.io/aaltd2024/)", "summary": "We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.", "AI": {"tldr": "KQT-EWMA\u662f\u4e00\u79cd\u975e\u53c2\u6570\u53d8\u5316\u68c0\u6d4b\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86KQT\u76f4\u65b9\u56fe\u548cEWMA\u7edf\u8ba1\u91cf\uff0c\u80fd\u591f\u5728\u63a7\u5236\u8bef\u62a5\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u5728\u7ebf\u76d1\u6d4b\u3002", "motivation": "\u5927\u591a\u6570\u975e\u53c2\u6570\u53d8\u5316\u68c0\u6d4b\u6d4b\u8bd5\u5f88\u5c11\u80fd\u4e8b\u5148\u63a7\u5236ARL_0\uff0c\u800cKQT-EWMA\u80fd\u591f\u901a\u8fc7\u64cd\u4f5c\u9884\u5b9a\u4e49\u7684ARL_0\u6765\u63a7\u5236\u8bef\u62a5\u7387\u3002", "method": "KQT-EWMA\u7ed3\u5408\u4e86Kernel-QuantTree (KQT)\u76f4\u65b9\u56fe\u548cEWMA\u7edf\u8ba1\u91cf\uff0c\u7528\u4e8e\u5728\u7ebf\u76d1\u63a7\u591a\u53d8\u91cf\u6570\u636e\u6d41\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKQT-EWMA\u53ef\u4ee5\u5728\u63a7\u5236ARL_0\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f4e\u7684\u68c0\u6d4b\u5ef6\u8fdf\u3002", "conclusion": "KQT-EWMA\u53ef\u4ee5\u63a7\u5236ARL_0\uff0c\u540c\u65f6\u5728\u68c0\u6d4b\u5ef6\u8fdf\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002"}}
{"id": "2505.09864", "pdf": "https://arxiv.org/pdf/2505.09864", "abs": "https://arxiv.org/abs/2505.09864", "authors": ["Aditya Panangat"], "title": "BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks", "categories": ["cs.LG"], "comment": "6 pages, 0 figures, 2 tables", "summary": "Over the past decade, the use of machine learning has increased\nexponentially. Models are far more complex than ever before, growing to\ngargantuan sizes and housing millions of weights. Unfortunately, the fact that\nlarge models have become the state of the art means that it often costs\nmillions of dollars to train and operate them. These expenses not only hurt\ncompanies but also bar non-wealthy individuals from contributing to new\ndevelopments and force consumers to pay greater prices for AI. Current methods\nused to prune models, such as iterative magnitude pruning, have shown great\naccuracy but require an iterative training sequence that is incredibly\ncomputationally and environmentally taxing. To solve this problem, BINGO is\nintroduced. BINGO, during the training pass, studies specific subsets of a\nneural network one at a time to gauge how significant of a role each weight\nplays in contributing to a network's accuracy. By the time training is done,\nBINGO generates a significance score for each weight, allowing for\ninsignificant weights to be pruned in one shot. BINGO provides an\naccuracy-preserving pruning technique that is less computationally intensive\nthan current methods, allowing for a world where AI growth does not have to\nmean model growth, as well.", "AI": {"tldr": "BINGO is a new pruning technique that reduces the computational and environmental costs of training large machine learning models while preserving accuracy.", "motivation": "The increasing complexity and cost of large machine learning models have made them inaccessible to non-wealthy individuals and companies, and have led to higher prices for consumers. Current pruning methods are computationally and environmentally taxing.", "method": "BINGO studies specific subsets of a neural network one at a time during the training pass to gauge how significant each weight plays in contributing to a network's accuracy. It generates a significance score for each weight by the end of training, allowing insignificant weights to be pruned in one shot.", "result": "BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than current methods, allowing for a world where AI growth does not have to mean model growth.", "conclusion": "BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than current methods, allowing for a world where AI growth does not have to mean model growth."}}
{"id": "2505.10046", "pdf": "https://arxiv.org/pdf/2505.10046", "abs": "https://arxiv.org/abs/2505.10046", "authors": ["Bingda Tang", "Boyang Zheng", "Xichen Pan", "Sayak Paul", "Saining Xie"], "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6df1\u5ea6\u878d\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u8bbe\u8ba1\u9009\u62e9\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\uff0c\u800c\u4e0d\u662f\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u7684\u8be6\u7ec6\u6bd4\u8f83\uff0c\u5173\u952e\u8bbe\u8ba1\u7ec6\u8282\u548c\u8bad\u7ec3\u65b9\u6848\u901a\u5e38\u672a\u88ab\u62ab\u9732\u3002\u8fd9\u4e9b\u5dee\u8ddd\u5bfc\u81f4\u5bf9\u8be5\u65b9\u6cd5\u5b9e\u9645\u6f5c\u529b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u6587\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u8fdb\u884c\u4e86\u4e0e\u73b0\u6709\u57fa\u7ebf\u7684\u53d7\u63a7\u6bd4\u8f83\uff0c\u5206\u6790\u4e86\u91cd\u8981\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6e05\u6670\u590d\u73b0\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u672c\u6587\u8fdb\u884c\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u91cd\u8981\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6e05\u6670\u590d\u73b0\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u5e0c\u671b\u4e3a\u591a\u6a21\u6001\u751f\u6210\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6570\u636e\u70b9\u548c\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2505.10218", "pdf": "https://arxiv.org/pdf/2505.10218", "abs": "https://arxiv.org/abs/2505.10218", "authors": ["Zongsheng Wang", "Kaili Sun", "Bowen Wu", "Qun Yu", "Ying Li", "Baoxun Wang"], "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward", "categories": ["cs.CL"], "comment": null, "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6RAIDEN-R1\uff0c\u7528\u4e8e\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u9a8c\u8bc1\u7684\u89d2\u8272\u610f\u8bc6\u5956\u52b1\u548c\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89d2\u8272\u4e00\u81f4\u6027\u548c\u63a8\u7406\u8fde\u8d2f\u6027\u3002", "motivation": "\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4ee3\u7406\uff08RPCAs\uff09\u9762\u4e34\u6301\u7eed\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RAIDEN-R1\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u53ef\u9a8c\u8bc1\u7684\u89d2\u8272\u610f\u8bc6\u5956\u52b1\uff08VRAR\uff09\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86RAIDEN-R1\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u53ef\u9a8c\u8bc1\u7684\u89d2\u8272\u610f\u8bc6\u5956\u52b1\uff08VRAR\uff09\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u5355\u4e2a\u548c\u591a\u672f\u8bed\u6316\u6398\u7b56\u7565\uff0c\u901a\u8fc7\u8bc4\u4f30\u89d2\u8272\u7279\u5b9a\u7684\u5173\u952e\u70b9\u6765\u751f\u6210\u53ef\u91cf\u5316\u7684\u5956\u52b1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u591aLLM\u534f\u4f5c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u89d2\u8272\u610f\u8bc6\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u4ee5\u589e\u5f3a\u63a8\u7406\u8fde\u8d2f\u6027\u3002", "result": "\u5728RAIDEN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAIDEN-R1\u8868\u73b0\u51fa\u8272\uff1a\u6211\u4eec\u768414B-GRPO\u6a21\u578b\u5728\u57fa\u4e8e\u811a\u672c\u7684\u77e5\u8bc6\u548c\u5bf9\u8bdd\u8bb0\u5fc6\u6307\u6807\u4e0a\u5206\u522b\u8fbe\u5230\u4e8688.04%\u548c88.65%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5065\u6027\u3002\u6848\u4f8b\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u6a21\u578b\u5728\u89e3\u51b3\u51b2\u7a81\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u7ef4\u6301\u7b2c\u4e00\u4eba\u79f0\u53d9\u8ff0\u4e00\u81f4\u6027\u65b9\u9762\u7684\u80fd\u529b\u589e\u5f3a\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86RPCA\u8bad\u7ec3\u4e2d\u975e\u91cf\u5316\u6027\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u89d2\u8272\u610f\u8bc6\u63a8\u7406\u6a21\u5f0f\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86RPCAs\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.03084", "pdf": "https://arxiv.org/pdf/2505.03084", "abs": "https://arxiv.org/abs/2505.03084", "authors": ["Shashank Kapoor", "Sanjay Surendranath Girija", "Lakshit Arora", "Dipen Pradhan", "Ankit Shetgaonkar", "Aman Raj"], "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in IEEE COMPSAC 2025", "summary": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u603b\u7ed3\u4e86\u591a\u6a21\u6001\u4e16\u754c\u4e2d\u7684\u5a01\u80c1\u666f\u89c2\uff0c\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u7684\u5bf9\u6297\u6027\u653b\u51fb\u8fdb\u884c\u4e86\u7efc\u8ff0\u3002", "motivation": "\u7531\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u5bf9\u5a01\u80c1\u666f\u89c2\u6709\u6e05\u6670\u7684\u8ba4\u8bc6\u4ee5\u91c7\u53d6\u9884\u9632\u63aa\u65bd\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u9762\u5411\u5b9e\u8df5\u8005\u7684\u653b\u51fb\u7c7b\u578b\u6982\u8ff0\u3002", "method": "\u672c\u6587\u5bf9\u9488\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u56db\u79cd\u6a21\u6001\u7684\u5bf9\u6297\u6027\u653b\u51fb\u8fdb\u884c\u4e86\u7efc\u8ff0\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u6297\u6027\u653b\u51fb\u666f\u89c2\u7684\u89c6\u56fe\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5bf9\u6297\u6027\u5a01\u80c1\u7684\u6f14\u53d8\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u591a\u6a21\u6001\u4e16\u754c\u4e2d\u5bf9\u6297\u6027\u5a01\u80c1\u666f\u89c2\u7efc\u8ff0\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u5bf9\u591a\u6a21\u6001\u5bf9\u6297\u6027\u5a01\u80c1\u6f14\u5316\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.09901", "pdf": "https://arxiv.org/pdf/2505.09901", "abs": "https://arxiv.org/abs/2505.09901", "authors": ["Ziyuan Zhang", "Darcy Wang", "Ningyuan Chen", "Rodrigo Mansur", "Vahid Sarhangian"], "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4eba\u7c7b\u548cMAB\u7b97\u6cd5\u7684\u63a2\u7d22-\u5229\u7528\u7b56\u7565\uff0c\u53d1\u73b0\u63a8\u7406\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u884c\u4e3a\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u6211\u4eec\u60f3\u4e86\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u5e76\u80fd\u5426\u5b9e\u73b0\u76f8\u5f53\uff08\u6216\u66f4\u597d\uff09\u7684\u6027\u80fd\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u9009\u62e9\u6a21\u578b\u6765\u6355\u6349\u4ee3\u7406\u7684\u63a2\u7d22-\u5229\u7528\u7b56\u7565\uff0c\u5e76\u7814\u7a76\u901a\u8fc7\u63d0\u793a\u7b56\u7565\u548c\u589e\u5f3a\u63a8\u7406\u7684\u6a21\u578b\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u63a8\u7406\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u8fd9\u79cd\u884c\u4e3a\u7531\u968f\u673a\u548c\u5b9a\u5411\u63a2\u7d22\u7684\u6df7\u5408\u7279\u5f81\u3002\u5728\u7b80\u5355\u7684\u9759\u6001\u4efb\u52a1\u4e2d\uff0c\u63a8\u7406\u542f\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u6c34\u5e73\u7684\u968f\u673a\u548c\u5b9a\u5411\u63a2\u7d22\u3002\u7136\u800c\uff0c\u5728\u66f4\u590d\u6742\u7684\u975e\u9759\u6001\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5339\u914d\u4eba\u7c7b\u7684\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u5728\u6709\u6548\u7684\u5b9a\u5411\u63a2\u7d22\u65b9\u9762\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u76f8\u4f3c\u7684\u9057\u61be\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u548c\u81ea\u52a8\u5316\u51b3\u7b56\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u53ef\u80fd\u7684\u6539\u8fdb\u9886\u57df\u3002"}}
{"id": "2505.10049", "pdf": "https://arxiv.org/pdf/2505.10049", "abs": "https://arxiv.org/abs/2505.10049", "authors": ["Jinlong Fan", "Xuepu Zeng", "Jing Zhang", "Mingming Gong", "Yuxiang Yang", "Dacheng Tao"], "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86200\u591a\u7bc7\u5173\u4e8e\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u8bba\u6587\uff0c\u6db5\u76d6\u4e86\u4ece\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5230\u663e\u5f0f\u9ad8\u65af\u57fa\u5143\u7684\u5404\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u8868\u793a\u548c\u91cd\u5efa\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u8bb8\u591a\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5206\u6790\u548c\u603b\u7ed3\u3002", "method": "\u672c\u6587\u5bf9\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u8bc4\u4f30\uff0c\u5305\u62ec\u8fd0\u52a8\u8868\u793a\u8303\u5f0f\u3001\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u52a8\u6001\u7684\u91cd\u5efa\u6280\u672f\u3001\u8f85\u52a9\u4fe1\u606f\u96c6\u6210\u7b56\u7565\u4ee5\u53ca\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u5168\u9762\u6982\u8ff0\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6743\u5a01\u7684\u53c2\u8003\uff0c\u540c\u65f6\u4e3a\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6982\u5ff5\u539f\u7406\u548c\u5b9e\u9645\u524d\u6cbf\u7684\u7cfb\u7edf\u7406\u89e3\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790200\u591a\u7bc7\u5173\u4e8e\u4f7f\u7528\u8f90\u5c04\u573a\u8fdb\u884c\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\uff0c\u5e76\u5bf9\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u548c\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u6279\u5224\u6027\u5ba1\u89c6\u3002"}}
{"id": "2505.10260", "pdf": "https://arxiv.org/pdf/2505.10260", "abs": "https://arxiv.org/abs/2505.10260", "authors": ["Poli Apollinaire Nemkova", "Solomon Ubani", "Mark V. Albert"], "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u6761\u4ef6\u4e0b\u5bf9\u4fc4\u8bed\u548c\u4e4c\u514b\u5170\u8bed\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u8fdb\u884c\u4e8c\u5206\u7c7b\u6807\u6ce8\u7684\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u654f\u611f\u9886\u57df\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u540c\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6d89\u53ca\u654f\u611f\u9886\u57df\u4efb\u52a1\u7684\u591a\u8bed\u8a00\u73af\u5883\uff0c\u9700\u8981\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u8fd9\u4e9b\u6a21\u578b\u7684\u53ef\u9760\u6027\u4e0e\u9002\u5e94\u6027\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u4eba\u6743\u4fb5\u72af\u5f15\u7528\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-3.5\u3001GPT-4\u3001LLAMA3\u3001Mistral 7B\u548cClaude-2\uff09\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u6761\u4ef6\u4e0b\u5bf9\u5305\u542b\u4fc4\u8bed\u548c\u4e4c\u514b\u5170\u8bed\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u7684\u590d\u6742\u6587\u672c\u6570\u636e\u96c6\u8fdb\u884c\u4e8c\u5206\u7c7b\u6807\u6ce8\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u6807\u6ce8\u4e0e\u4eba\u7c7b\u53cc\u91cd\u6807\u6ce8\u7684\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u6807\u6ce8\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u4e86\u6bcf\u4e2a\u6a21\u578b\u7684\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u6807\u6ce8\u6027\u80fd\u5b58\u5728\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\uff08\u82f1\u8bed\u548c\u4fc4\u8bed\uff09\u4e0b\u7684\u8868\u73b0\u4e5f\u6709\u6240\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u5404\u6a21\u578b\u5728\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6a21\u5f0f\u4e0a\u7684\u72ec\u7279\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5b83\u4eec\u5728\u5904\u7406\u4e3b\u89c2\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u5224\u65ad\u65f6\u7684\u8868\u73b0\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u4e0e\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u6267\u884c\u654f\u611f\u9886\u57df\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u672c\u8d28\u4e0a\u4e3b\u89c2\u548c\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u5224\u65ad\uff0c\u8fd9\u5bf9\u5b83\u4eec\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.08202", "pdf": "https://arxiv.org/pdf/2505.08202", "abs": "https://arxiv.org/abs/2505.08202", "authors": ["Aman Raj", "Lakshit Arora", "Sanjay Surendranath Girija", "Shashank Kapoor", "Dipen Pradhan", "Ankit Shetgaonkar"], "title": "AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "Accepted in IEEE Compsac 2025", "summary": "Natural disasters, including earthquakes, wildfires and cyclones, bear a huge\nrisk on human lives as well as infrastructure assets. An effective response to\ndisaster depends on the ability to rapidly and efficiently assess the intensity\nof damage. Artificial Intelligence (AI) and Generative Artificial Intelligence\n(GenAI) presents a breakthrough solution, capable of combining knowledge from\nmultiple types and sources of data, simulating realistic scenarios of disaster,\nand identifying emerging trends at a speed previously unimaginable. In this\npaper, we present a comprehensive review on the prospects of AI and GenAI in\ndamage assessment for various natural disasters, highlighting both its\nstrengths and limitations. We talk about its application to multimodal data\nsuch as text, image, video, and audio, and also cover major issues of data\nprivacy, security, and ethical use of the technology during crises. The paper\nalso recognizes the threat of Generative AI misuse, in the form of\ndissemination of misinformation and for adversarial attacks. Finally, we\noutline avenues of future research, emphasizing the need for secure, reliable,\nand ethical Generative AI systems for disaster management in general. We\nbelieve that this work represents the first comprehensive survey of Gen-AI\ntechniques being used in the field of Disaster Assessment and Response.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u548cGenAI\u5728\u5404\u79cd\u81ea\u7136\u707e\u5bb3\u4e2d\u7684\u635f\u5bb3\u8bc4\u4f30\u524d\u666f\uff0c\u5f3a\u8c03\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u6570\u636e\u9690\u79c1\u3001\u5b89\u5168\u6027\u548c\u4f26\u7406\u4f7f\u7528\u7b49\u95ee\u9898\u3002", "motivation": "\u81ea\u7136\u707e\u96be\u5bf9\u4eba\u7c7b\u751f\u547d\u548c\u57fa\u7840\u8bbe\u65bd\u8d44\u4ea7\u6784\u6210\u5de8\u5927\u98ce\u9669\uff0c\u6709\u6548\u7684\u707e\u5bb3\u54cd\u5e94\u4f9d\u8d56\u4e8e\u5feb\u901f\u800c\u9ad8\u6548\u5730\u8bc4\u4f30\u635f\u5bb3\u7a0b\u5ea6\u3002AI\u548cGenAI\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7ed3\u5408\u591a\u79cd\u7c7b\u578b\u548c\u6765\u6e90\u7684\u6570\u636e\uff0c\u6a21\u62df\u73b0\u5b9e\u7684\u707e\u5bb3\u573a\u666f\uff0c\u5e76\u4ee5\u524d\u6240\u672a\u6709\u7684\u901f\u5ea6\u8bc6\u522b\u65b0\u5174\u8d8b\u52bf\u3002", "method": "\u672c\u6587\u5bf9AI\u548cGenAI\u5728\u5404\u79cd\u81ea\u7136\u707e\u5bb3\u4e2d\u7684\u635f\u5bb3\u8bc4\u4f30\u524d\u666f\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\uff09\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u672c\u6587\u7a81\u51fa\u4e86AI\u548cGenAI\u5728\u707e\u5bb3\u8bc4\u4f30\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6db5\u76d6\u4e86\u6570\u636e\u9690\u79c1\u3001\u5b89\u5168\u6027\u548c\u6280\u672f\u5728\u5371\u673a\u4e2d\u7684\u4f26\u7406\u4f7f\u7528\u7b49\u4e3b\u8981\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u6307\u51fa\u4e86GenAI\u88ab\u6ee5\u7528\u7684\u5a01\u80c1\uff0c\u4f8b\u5982\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u548c\u8fdb\u884c\u5bf9\u6297\u6027\u653b\u51fb\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u5bf9\u707e\u5bb3\u8bc4\u4f30\u548c\u54cd\u5e94\u9886\u57df\u4e2d\u4f7f\u7528Gen-AI\u6280\u672f\u7684\u9996\u6b21\u5168\u9762\u8c03\u67e5\u3002"}}
{"id": "2505.09907", "pdf": "https://arxiv.org/pdf/2505.09907", "abs": "https://arxiv.org/abs/2505.09907", "authors": ["Linwei Zhang", "LuFeng", "Ruijia Liang"], "title": "Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "With the growing demand for healthy foods, agricultural product price\nforecasting has become increasingly important. Hass avocados, as a high-value\ncrop, exhibit complex price fluctuations influenced by factors such as\nseasonality, region, and weather. Traditional prediction models often struggle\nwith highly nonlinear and dynamic data. To address this, we propose a hybrid\ndeep learning model, TCN-MLP-Attention Architecture, combining Temporal\nConvolutional Networks (TCN) for sequential feature extraction, Multi-Layer\nPerceptrons (MLP) for nonlinear interactions, and an Attention mechanism for\ndynamic feature weighting. The dataset used covers over 50,000 records of Hass\navocado sales across the U.S. from 2015 to 2018, including variables such as\nsales volume, average price, time, region, weather, and variety type, collected\nfrom point-of-sale systems and the Hass Avocado Board. After systematic\npreprocessing, including missing value imputation and feature normalization,\nthe proposed model was trained and evaluated. Experimental results demonstrate\nthat the TCN-MLP-Attention model achieves excellent predictive performance,\nwith an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.\nThis research provides a scalable and effective approach for time series\nforecasting in agricultural markets and offers valuable insights for\nintelligent supply chain management and price strategy optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bTCN-MLP-Attention\uff0c\u7528\u4e8e\u9884\u6d4b\u54c8\u65af\u725b\u6cb9\u679c\u7684\u4ef7\u683c\u6ce2\u52a8\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5bf9\u5065\u5eb7\u98df\u54c1\u9700\u6c42\u7684\u589e\u957f\uff0c\u519c\u4ea7\u54c1\u4ef7\u683c\u9884\u6d4b\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u54c8\u65af\u725b\u6cb9\u679c\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u4ef7\u503c\u4f5c\u7269\uff0c\u5176\u4ef7\u683c\u6ce2\u52a8\u590d\u6742\uff0c\u53d7\u5b63\u8282\u6027\u3001\u5730\u533a\u548c\u5929\u6c14\u7b49\u56e0\u7d20\u5f71\u54cd\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5728\u5904\u7406\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u52a8\u6001\u6570\u636e\u65f6\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0cTCN-MLP-Attention\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u7528\u4e8e\u5e8f\u5217\u7279\u5f81\u63d0\u53d6\uff0c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7528\u4e8e\u975e\u7ebf\u6027\u4ea4\u4e92\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\u7528\u4e8e\u52a8\u6001\u7279\u5f81\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTCN-MLP-Attention\u6a21\u578b\u5177\u6709\u51fa\u8272\u7684\u9884\u6d4b\u6027\u80fd\uff0cRMSE\u4e3a1.23\uff0cMSE\u4e3a1.51\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u519c\u4e1a\u5e02\u573a\u7684\u667a\u80fd\u4f9b\u5e94\u94fe\u7ba1\u7406\u548c\u4ef7\u683c\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.10055", "pdf": "https://arxiv.org/pdf/2505.10055", "abs": "https://arxiv.org/abs/2505.10055", "authors": ["Ijazul Haq", "Yingjie Zhang", "Irfan Ali Khan"], "title": "PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86Pashto OCR\u6570\u636e\u96c6PsOCR\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728Pashto\u8bed\u8a00OCR\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u73b0Gemini\u548cQwen-7B\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8ePashto\u8bed\u8a00\u7684\u811a\u672c\u5177\u6709\u8fde\u7b14\u7279\u6027\u4e14\u7ed3\u6784\u5316\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5408\u6210OCR\u6570\u636e\u96c6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u4e00\u767e\u4e07\u5f20\u56fe\u50cf\u7684\u5408\u6210Pashto OCR\u6570\u636e\u96c6PsOCR\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5728\u5355\u8bcd\u3001\u884c\u548c\u6587\u6863\u7ea7\u522b\u90fd\u6709\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\uff08\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cTransformer\uff09\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u9009\u62e9\u4e8610,000\u5f20\u56fe\u50cf\u7684\u57fa\u51c6\u5b50\u96c6\u6765\u8bc4\u4f30\u591a\u4e2a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGemini\u5728\u6240\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\uff0cQwen-7B\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u672c\u6587\u5bf9\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728Pashto\u8bed\u8a00OCR\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u8fdb\u884c\u4e86\u6df1\u5165\u8bc4\u4f30\uff0c\u5e76\u4e3aPashto OCR\u53ca\u5176\u4ed6\u7c7b\u4f3c\u811a\u672c\uff08\u5982\u963f\u62c9\u4f2f\u8bed\u3001\u6ce2\u65af\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\uff09\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.10261", "pdf": "https://arxiv.org/pdf/2505.10261", "abs": "https://arxiv.org/abs/2505.10261", "authors": ["Rui Yang", "Huitao Li", "Matthew Yu Heng Wong", "Yuhe Ke", "Xin Li", "Kunyu Yu", "Jingchi Liao", "Jonathan Chong Kai Liew", "Sabarinath Vinod Nair", "Jasmine Chiat Ling Ong", "Irene Li", "Douglas Teodoro", "Chuan Hong", "Daniel Shu Wei Ting", "Nan Liu"], "title": "The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e8619,123\u9879\u7814\u7a76\uff0c\u53d1\u73b0\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f46\u4f26\u7406\u4f7f\u7528\u8fd9\u4e9b\u6280\u672f\u5bf9\u533b\u7597\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5728\u4e0d\u540c\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u5dee\u5f02\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5b83\u4eec\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5206\u6790\u4e8619,123\u9879\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5728\u4e0d\u540c\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u800c\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u4fe1\u606f\u63d0\u53d6\u548c\u5206\u6790\u4efb\u52a1\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "\u968f\u7740\u8fd9\u4e9b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u5b83\u4eec\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u4f26\u7406\u4f7f\u7528\u5b83\u4eec\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002"}}
{"id": "2505.09922", "pdf": "https://arxiv.org/pdf/2505.09922", "abs": "https://arxiv.org/abs/2505.09922", "authors": ["Zichen Liu", "Wei Zhang", "Tiejun Li"], "title": "Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity", "categories": ["cs.LG"], "comment": "22 pages", "summary": "Euclidean diffusion models have achieved remarkable success in generative\nmodeling across diverse domains, and they have been extended to manifold case\nin recent advances. Instead of explicitly utilizing the structure of special\nmanifolds as studied in previous works, we investigate direct sampling of the\nEuclidean diffusion models for general manifold-constrained data in this paper.\nWe reveal the multiscale singularity of the score function in the embedded\nspace of manifold, which hinders the accuracy of diffusion-generated samples.\nWe then present an elaborate theoretical analysis of the singularity structure\nof the score function by separating it along the tangential and normal\ndirections of the manifold. To mitigate the singularity and improve the\nsampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces\nnon-isotropic noise along the normal direction to reduce scale discrepancies,\nand (2) Tango-DM, which trains only the tangential component of the score\nfunction using a tangential-only loss function. Numerical experiments\ndemonstrate that our methods achieve superior performance on distributions over\nvarious manifolds with complex geometries.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u76f4\u63a5\u91c7\u6837\u6b27\u51e0\u91cc\u5f97\u6269\u6563\u6a21\u578b\u7528\u4e8e\u4e00\u822c\u6d41\u5f62\u7ea6\u675f\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\u6765\u51cf\u8f7b\u5f97\u5206\u51fd\u6570\u7684\u5947\u5f02\u6027\u548c\u63d0\u9ad8\u91c7\u6837\u7cbe\u5ea6\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u4e86\u76f4\u63a5\u91c7\u6837\u6b27\u51e0\u91cc\u5f97\u6269\u6563\u6a21\u578b\u7528\u4e8e\u4e00\u822c\u6d41\u5f62\u7ea6\u675f\u6570\u636e\uff0c\u56e0\u4e3a\u4ee5\u524d\u7684\u5de5\u4f5c\u6ca1\u6709\u663e\u5f0f\u5229\u7528\u7279\u6b8a\u6d41\u5f62\u7684\u7ed3\u6784\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1a(1) Niso-DM\uff0c\u5b83\u6cbf\u6cd5\u7ebf\u65b9\u5411\u5f15\u5165\u975e\u5404\u5411\u540c\u6027\u566a\u58f0\u4ee5\u51cf\u5c11\u5c3a\u5ea6\u5dee\u5f02\uff0c(2) Tango-DM\uff0c\u5b83\u4ec5\u4f7f\u7528\u4ec5\u5207\u5411\u7684\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u5f97\u5206\u51fd\u6570\u7684\u5207\u5411\u5206\u91cf\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u5177\u6709\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u7684\u6d41\u5f62\u4e0a\u7684\u5206\u5e03\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5177\u6709\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u7684\u6d41\u5f62\u4e0a\u7684\u5206\u5e03\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10072", "pdf": "https://arxiv.org/pdf/2505.10072", "abs": "https://arxiv.org/abs/2505.10072", "authors": ["Rui-Yang Ju", "Sheng-Yen Huang", "Yi-Ping Hung"], "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.", "AI": {"tldr": "ToonifyGB is a two-stage framework that uses an improved StyleGAN to generate stylized videos and then learns a stylized neutral head model and expression blendshapes to render stylized avatars with arbitrary expressions.", "motivation": "To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes.", "method": "ToonifyGB is an efficient two-stage framework that first generates a stylized video using an improved StyleGAN and then learns a stylized neutral head model and a set of expression blendshapes from the generated video.", "result": "ToonifyGB was validated on the benchmark dataset using two styles: Arcane and Pixar, showing its effectiveness in generating high-quality animation.", "conclusion": "ToonifyGB can efficiently render stylized avatars with arbitrary expressions by combining a stylized neutral head model with expression blendshapes."}}
{"id": "2505.10282", "pdf": "https://arxiv.org/pdf/2505.10282", "abs": "https://arxiv.org/abs/2505.10282", "authors": ["Dubai Li", "Nan Jiang", "Kangping Huang", "Ruiqi Tu", "Shuyu Ouyang", "Huayu Yu", "Lin Qiao", "Chen Yu", "Tianshu Zhou", "Danyang Tong", "Qian Wang", "Mengtao Li", "Xiaofeng Zeng", "Yu Tian", "Xinping Tian", "Jingsong Li"], "title": "From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making", "categories": ["cs.CL"], "comment": null, "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aQuicker\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u8bc1\u636e\u7efc\u5408\u5e76\u751f\u6210\u7b26\u5408\u6807\u51c6\u4e34\u5e8a\u6307\u5357\u5f00\u53d1\u6d41\u7a0b\u7684\u4e34\u5e8a\u5efa\u8bae\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQuicker\u5728\u95ee\u9898\u5206\u89e3\u3001\u68c0\u7d22\u7075\u654f\u5ea6\u548c\u6587\u732e\u7b5b\u9009\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u6709\u6548\u652f\u6301\u4eba\u7c7b\u8bc4\u5ba1\u5458\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5168\u9762\u548c\u903b\u8f91\u8fde\u8d2f\u7684\u5efa\u8bae\u3002", "motivation": "\u5c06\u4e34\u5e8a\u8bc1\u636e\u6574\u5408\u5230\u5b9e\u65f6\u5b9e\u8df5\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5de5\u4f5c\u91cf\u5927\u3001\u4e13\u4e1a\u6d41\u7a0b\u590d\u6742\u4e14\u65f6\u95f4\u6709\u9650\u3002\u8fd9\u51f8\u663e\u4e86\u9700\u8981\u81ea\u52a8\u5316\u8bc1\u636e\u7efc\u5408\u5de5\u5177\u4ee5\u652f\u6301\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86Quicker\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5faa\u8bc1\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u8bc1\u636e\u7efc\u5408\u5e76\u751f\u6210\u7b26\u5408\u6807\u51c6\u4e34\u5e8a\u6307\u5357\u5f00\u53d1\u6d41\u7a0b\u7684\u4e34\u5e8a\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQuicker\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7ec6\u7c92\u5ea6\u7684\u95ee\u9898\u5206\u89e3\uff0c\u68c0\u7d22\u7075\u654f\u5ea6\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\uff0c\u6587\u732e\u7b5b\u9009\u6027\u80fd\u63a5\u8fd1\u5168\u9762\u7eb3\u5165\u76f8\u5173\u7814\u7a76\u3002\u6b64\u5916\uff0cQuicker\u8f85\u52a9\u7684\u8bc1\u636e\u8bc4\u4f30\u6709\u6548\u652f\u6301\u4e86\u4eba\u7c7b\u8bc4\u5ba1\u5458\uff0c\u800cQuicker\u7684\u5efa\u8bae\u6bd4\u4e34\u5e8a\u533b\u751f\u7684\u5efa\u8bae\u66f4\u5168\u9762\u3001\u903b\u8f91\u66f4\u8fde\u8d2f\u3002\u5728\u7cfb\u7edf\u7ea7\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4e2a\u8bc4\u5ba1\u5458\u4e0eQuicker\u5408\u4f5c\u5c06\u63a8\u8350\u5f00\u53d1\u65f6\u95f4\u51cf\u5c11\u523020-40\u5206\u949f\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86Quicker\u5728\u5e2e\u52a9\u533b\u751f\u505a\u51fa\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684\u5faa\u8bc1\u4e34\u5e8a\u51b3\u7b56\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09616", "pdf": "https://arxiv.org/pdf/2505.09616", "abs": "https://arxiv.org/abs/2505.09616", "authors": ["Yuqi Li", "Yuanzhong Zheng", "Zhongtian Guo", "Yaoxuan Wang", "Jianjun Yin", "Haojun Fei"], "title": "SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech", "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2.0"], "comment": "2 pages,3 figures,1 chart", "summary": "This paper presents SpecWav-Attack, an adversarial model for detecting\nspeakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and\nincorporates spectrogram resizing and incremental training for improved\nperformance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack\noutperforms conventional attacks, revealing vulnerabilities in anonymized\nspeech systems and emphasizing the need for stronger defenses, benchmarked\nagainst the ICASSP 2025 Attacker Challenge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSpecWav-Attack\u7684\u5bf9\u6297\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u533f\u540d\u8bed\u97f3\u4e2d\u7684\u8bf4\u8bdd\u4eba\uff0c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u533f\u540d\u8bed\u97f3\u7cfb\u7edf\u7684\u6f0f\u6d1e\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u68c0\u6d4b\u533f\u540d\u8bed\u97f3\u7cfb\u7edf\u4e2d\u7684\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5bf9\u6297\u6a21\u578b\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SpecWav-Attack\uff0c\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u533f\u540d\u8bed\u97f3\u4e2d\u8bf4\u8bdd\u4eba\u7684\u5bf9\u6297\u6a21\u578b\uff0c\u5229\u7528\u4e86Wav2Vec2\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408\u4e86\u9891\u8c31\u56fe\u7f29\u653e\u548c\u589e\u91cf\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u5728librispeech-dev\u548clibrispeech-test\u4e0a\u8bc4\u4f30\uff0cSpecWav-Attack\u4f18\u4e8e\u4f20\u7edf\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u533f\u540d\u8bed\u97f3\u7cfb\u7edf\u4e2d\u7684\u6f0f\u6d1e\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5f3a\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2505.09925", "pdf": "https://arxiv.org/pdf/2505.09925", "abs": "https://arxiv.org/abs/2505.09925", "authors": ["Yutao Yang", "Jie Zhou", "Junsong Li", "Qianjun Pan", "Bihao Zhan", "Qin Chen", "Xipeng Qiu", "Liang He"], "title": "Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an interactive continual learning paradigm where AI\nmodels dynamically learn new skills from real-time human feedback while\nretaining prior knowledge. This paradigm distinctively addresses two major\nlimitations of traditional continual learning: (1) dynamic model updates using\nstreaming, real-time human-annotated data, rather than static datasets with\nfixed labels, and (2) the assumption of clean labels, by explicitly handling\nthe noisy feedback common in real-world interactions. To tackle these problems,\nwe propose RiCL, a Reinforced interactive Continual Learning framework\nleveraging Large Language Models (LLMs) to learn new skills effectively from\ndynamic feedback. RiCL incorporates three key components: a temporal\nconsistency-aware purifier to automatically discern clean from noisy samples in\ndata streams; an interaction-aware direct preference optimization strategy to\nalign model behavior with human intent by reconciling AI-generated and\nhuman-provided feedback; and a noise-resistant contrastive learning module that\ncaptures robust representations by exploiting inherent data relationships, thus\navoiding reliance on potentially unreliable labels. Extensive experiments on\ntwo benchmark datasets (FewRel and TACRED), contaminated with realistic noise\npatterns, demonstrate that our RiCL approach substantially outperforms existing\ncombinations of state-of-the-art online continual learning and noisy-label\nlearning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6RiCL\uff0c\u901a\u8fc7\u5904\u7406\u5b9e\u65f6\u4eba\u7c7b\u53cd\u9988\u4e2d\u7684\u566a\u58f0\uff0c\u63d0\u9ad8\u4e86AI\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4f7f\u7528\u6d41\u5f0f\u5b9e\u65f6\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u52a8\u6001\u6a21\u578b\u66f4\u65b0\uff0c\u800c\u4e0d\u662f\u9759\u6001\u6570\u636e\u96c6\uff1b\u4ee5\u53ca\u5047\u8bbe\u6807\u7b7e\u662f\u5e72\u51c0\u7684\uff0c\u800c\u73b0\u5b9e\u4e2d\u53cd\u9988\u5f80\u5f80\u5b58\u5728\u566a\u58f0\u3002", "method": "RiCL\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u52a8\u6001\u53cd\u9988\u4e2d\u6709\u6548\u5b66\u4e60\u65b0\u6280\u80fd\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u51c0\u5316\u5668\u3001\u4ea4\u4e92\u611f\u77e5\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\u548c\u6297\u566a\u58f0\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08FewRel\u548cTACRED\uff09\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRiCL\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RiCL\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u548c\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u7684\u7ec4\u5408\u3002"}}
{"id": "2505.10088", "pdf": "https://arxiv.org/pdf/2505.10088", "abs": "https://arxiv.org/abs/2505.10088", "authors": ["Yuncheng Guo", "Xiaodong Gu"], "title": "MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file", "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMRL\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5176\u6539\u8fdb\u7248\u672cMMRL++\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0cMMRL\u548cMMRL++\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8fc1\u79fb\u5b66\u4e60\uff0c\u4f46\u4f7f\u7528\u6709\u9650\u7684\u5c11\u91cf\u6570\u636e\u9002\u5e94\u8fd9\u4e9b\u6a21\u578b\u5e38\u5e38\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u524a\u5f31\u4e86\u5b83\u4eec\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff08MMRL\uff09\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u5171\u4eab\u7684\u3001\u53ef\u5b66\u4e60\u7684\u3001\u6a21\u6001\u65e0\u5173\u7684\u8868\u793a\u7a7a\u95f4\u3002MMRL\u5c06\u7a7a\u95f4\u6807\u8bb0\u6295\u5f71\u5230\u6587\u672c\u548c\u56fe\u50cf\u7f16\u7801\u5668\u4e2d\u4f5c\u4e3a\u8868\u793a\u6807\u8bb0\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86MMRL++\uff0c\u8fd9\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u4ea4\u4e92\u611f\u77e5\u7684\u6269\u5c55\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u589e\u5f3a\u4e86\u6a21\u6001\u5185\u4ea4\u4e92\u3002", "result": "MMRL\u548cMMRL++\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u4eec\u5728\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u6cdb\u5316\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MMRL\u548cMMRL++\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u6cdb\u5316\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2505.10320", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "AI": {"tldr": "This paper introduces J1, a reinforcement learning approach for training LLM-as-a-Judge models, which improves their judgment ability by converting prompts to judgment tasks with verifiable rewards. J1 outperforms other models and provides insights into better training strategies.", "motivation": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think.", "method": "J1 is a reinforcement learning approach that converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards, incentivizing thinking and mitigating judgment bias.", "result": "J1 outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model.", "conclusion": "J1 is a reinforcement learning approach that improves the judgment ability of LLM-as-a-Judge models by converting prompts to judgment tasks with verifiable rewards, outperforming other models and providing insights into better training strategies."}}
{"id": "2505.09619", "pdf": "https://arxiv.org/pdf/2505.09619", "abs": "https://arxiv.org/abs/2505.09619", "authors": ["Pietro Cassieri", "Aiman Faiz", "Anna Maria De Roberto", "Claudio Pascarelli", "Gianvito Mitrano", "Gianluca Fimiani", "Marina Garofano", "Christiancarmine Esposito", "Genoveffa Tortora", "Alessia Bramanti", "Giuseppe Scanniello"], "title": "Predictive Models for Chronic Heart Failure", "categories": ["stat.OT", "cs.AI"], "comment": null, "summary": "The management of chronic Heart Failure (HF) presents significant challenges\nin modern healthcare, requiring continuous monitoring, early detection of\nexacerbations, and personalized treatment strategies. In this paper, we present\na predictive model founded on Machine Learning (ML) techniques to identify\npatients at HF risk. This model is an ensemble learning approach, a modified\nstacking technique, that uses two specialized models leveraging clinical and\nechocardiographic features and then a meta-model to combine the predictions of\nthese two models. We initially assess the model on a real dataset and the\nobtained results suggest that it performs well in the stratification of\npatients at HR risk. Specifically, we obtained high sensitivity (95\\%),\nensuring that nearly all high-risk patients are identified. As for accuracy, we\nobtained 84\\%, which can be considered moderate in some ML contexts. However,\nit is acceptable given our priority of identifying patients at risk of HF\nbecause they will be asked to participate in the telemonitoring program of the\nPrediHealth research project on which some of the authors of this paper are\nworking. The initial findings also suggest that ML-based risk stratification\nmodels can serve as valuable decision-support tools not only in the PrediHealth\nproject but also for healthcare professionals, aiding in early intervention and\npersonalized patient management. To have a better understanding of the value\nand of potentiality of our predictive model, we also contrasted its results\nwith those obtained by using three baseline models. The preliminary results\nindicate that our predictive model outperforms these baselines that flatly\nconsider features, \\ie not grouping them in clinical and echocardiographic\nfeatures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522bHF\u98ce\u9669\u60a3\u8005\u3002\u8be5\u6a21\u578b\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u4e34\u5e8a\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5143\u6a21\u578b\u7ed3\u5408\u9884\u6d4b\u7ed3\u679c\u3002\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u9ad8\u5371\u60a3\u8005\u5206\u5c42\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u53ef\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "motivation": "\u6162\u6027\u5fc3\u529b\u8870\u7aed\uff08HF\uff09\u7684\u7ba1\u7406\u5728\u73b0\u4ee3\u533b\u7597\u4fdd\u5065\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u6301\u7eed\u76d1\u6d4b\u3001\u65e9\u671f\u68c0\u6d4b\u52a0\u91cd\u60c5\u51b5\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u4fee\u6539\u540e\u7684\u5806\u53e0\u6280\u672f\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e13\u95e8\u7684\u6a21\u578b\u5229\u7528\u4e34\u5e8a\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u5143\u6a21\u578b\u6765\u7ed3\u5408\u8fd9\u4e24\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728\u9ad8\u5371\u60a3\u8005\u5206\u5c42\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5177\u6709\u9ad8\u7075\u654f\u5ea6\uff0895%\uff09\u548c\u4e2d\u7b49\u51c6\u786e\u7387\uff0884%\uff09\u3002\u6b64\u5916\uff0c\u4e0e\u4e09\u79cd\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "ML-based risk stratification models can serve as valuable decision-support tools for healthcare professionals, aiding in early intervention and personalized patient management."}}
{"id": "2505.09949", "pdf": "https://arxiv.org/pdf/2505.09949", "abs": "https://arxiv.org/abs/2505.09949", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Samgyu Yang", "Abdulrahman Faden"], "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors", "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u6570\u636e\uff0c\u8bc6\u522b\u4e8b\u6545\u539f\u56e0\uff0c\u5e76\u63d0\u4f9b\u5168\u9762\u7684\u89e3\u91ca\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u4e3b\u8981\u4e8b\u6545\u539f\u56e0\uff0c\u5982\u9152\u540e\u9a7e\u9a76\u3001\u8d85\u901f\u7b49\uff0c\u5e76\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002\u7814\u7a76\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4e3a\u4ea4\u901a\u5b89\u5168\u63aa\u65bd\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u6f5c\u5728\u5bf9\u7b56\u3002", "motivation": "\u7406\u89e3\u5bfc\u81f4\u4ea4\u901a\u4e8b\u6545\u7684\u56e0\u7d20\u5e76\u5236\u5b9a\u7b56\u7565\u4ee5\u51cf\u8f7b\u5176\u4e25\u91cd\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u7edf\u8ba1\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u5404\u79cd\u56e0\u7d20\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u4ee5\u53ca\u6bcf\u6b21\u4e8b\u6545\u7684\u72ec\u7279\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u5206\u6790\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u6570\u636e\u5e76\u63d0\u4f9b\u4e8b\u6545\u539f\u56e0\u5206\u6790\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5206\u6790\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03Llama3 8B\u6a21\u578b\u8fdb\u884c\u4e8b\u6545\u539f\u56e0\u5206\u6790\u3002\u4f7f\u7528QLoRA\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u589e\u5f3a\u5176\u5bf9\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u53ca\u5176\u76f8\u5173\u56e0\u7d20\u7684\u7406\u89e3\u3002\u7136\u540e\uff0c\u8be5\u5fae\u8c03\u540e\u7684\u6a21\u578b\u901a\u8fc7\u96f6\u6837\u672c\u5206\u7c7b\u8bc6\u522b\u4e8b\u6545\u539f\u56e0\uff0c\u65e0\u9700\u9884\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u5168\u9762\u7684\u89e3\u91ca\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u4e3b\u8981\u4e8b\u6545\u539f\u56e0\uff0c\u5982\u9152\u540e\u9a7e\u9a76\u3001\u8d85\u901f\u3001\u653b\u51fb\u6027\u9a7e\u9a76\u548c\u9a7e\u9a76\u5458\u5206\u5fc3\u3002\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\uff08\u5982\u9053\u8def\u7ef4\u62a4\uff09\u53ef\u4ee5\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002\u901a\u8fc7\u4ea4\u901a\u5b89\u5168\u7ba1\u7406\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u95ee\u5377\u8c03\u67e5\uff0888.89%\u7684\u9ad8\u4e00\u81f4\u6027\uff09\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u9002\u7528\u6027\u548c\u6539\u5584\u4ea4\u901a\u5b89\u5168\u63aa\u65bd\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4ea4\u901a\u4e8b\u6545\u7684\u590d\u6742\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u9762\u5206\u6790\u4e8b\u6545\u539f\u56e0\u548c\u5176\u4ed6\u76f8\u5173\u56e0\u7d20\u65b9\u9762\u7684\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u5b83\u4e3a\u89c4\u5212\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u6f5c\u5728\u7684\u5e94\u5bf9\u63aa\u65bd\uff0c\u4ee5\u5236\u5b9a\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u4ea4\u901a\u5b89\u5168\u63aa\u65bd\u3002"}}
{"id": "2505.10118", "pdf": "https://arxiv.org/pdf/2505.10118", "abs": "https://arxiv.org/abs/2505.10118", "authors": ["Yangfu Li", "Hongjian Zhan", "Tianyi Chen", "Qi Liu", "Yue Lu"], "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering", "categories": ["cs.CV", "cs.CL"], "comment": "31 pages,9 figures,conference", "summary": "Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.", "AI": {"tldr": "This paper introduces MoB, a method for visual token pruning that addresses the trade-off between prompt alignment and visual preservation by deriving a closed-form error bound and using \u03b5-covering theory. MoB reformulates the problem as a bi-objective covering problem, achieving high performance with minimal visual tokens.", "motivation": "Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance.", "method": "We derive the first closed-form error bound for visual token pruning based on the Hausdorff distance and leverage \u03b5-covering theory to reveal an intrinsic trade-off between objectives. We propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem, reducing the attainment trade-off to budget allocation via greedy radius trading.", "result": "MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5\u00d7 with negligible performance loss. Evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.", "conclusion": "MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. It preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5\u00d7 with negligible performance loss. Additionally, MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks."}}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354", "abs": "https://arxiv.org/abs/2505.10354", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.", "AI": {"tldr": "LDIR is a low-dimensional, dense, and interpretable text embedding method that shows good performance on various NLP tasks.", "motivation": "Existing text embeddings are either difficult to trace and interpret or suffer from poor performance. The goal is to create a low-dimensional, dense, and interpretable text embedding method.", "method": "LDIR uses farthest point sampling to determine the numerical values of its dimensions, which indicate semantic relatedness to different anchor texts.", "result": "LDIR was validated on multiple semantic textual similarity, retrieval, and clustering tasks, showing competitive performance with fewer dimensions compared to existing methods.", "conclusion": "LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions."}}
{"id": "2505.09624", "pdf": "https://arxiv.org/pdf/2505.09624", "abs": "https://arxiv.org/abs/2505.09624", "authors": ["Ekaterina Kuzmina", "Dmitrii Kriukov", "Mikhail Lebedev", "Dmitry V. Dylov"], "title": "Neurophysiologically Realistic Environment for Comparing Adaptive Deep Brain Stimulation Algorithms in Parkinson Disease", "categories": ["q-bio.NC", "cs.AI", "68T05"], "comment": "8 pages, 3 figures, submission to KDD", "summary": "Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment\nfor Parkinson disease (PD). In aDBS, a surgically placed electrode sends\ndynamically altered stimuli to the brain based on neurophysiological feedback:\nan invasive gadget that limits the amount of data one could collect for\noptimizing the control offline. As a consequence, a plethora of synthetic\nmodels of PD and those of the control algorithms have been proposed. Herein, we\nintroduce the first neurophysiologically realistic benchmark for comparing said\nmodels. Specifically, our methodology covers not only conventional basal\nganglia circuit dynamics and pathological oscillations, but also captures 15\npreviously dismissed physiological attributes, such as signal instabilities and\nnoise, neural drift, electrode conductance changes and individual variability -\nall modeled as spatially distributed and temporally registered features via\nbeta-band activity in the brain and a feedback. Furthermore, we purposely built\nour framework as a structured environment for training and evaluating deep\nreinforcement learning (RL) algorithms, opening new possibilities for\noptimizing aDBS control strategies and inviting the machine learning community\nto contribute to the emerging field of intelligent neurostimulation interfaces.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u4e2a\u795e\u7ecf\u751f\u7406\u5b66\u4e0a\u73b0\u5b9e\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u6bd4\u8f83\u81ea\u9002\u5e94\u6df1\u90e8\u8111\u523a\u6fc0\uff08aDBS\uff09\u6a21\u578b\uff0c\u5e76\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u73af\u5883\uff0c\u4ece\u800c\u4e3a\u4f18\u5316aDBS\u63a7\u5236\u7b56\u7565\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "motivation": "\u7531\u4e8eaDBS\u9700\u8981\u4fb5\u5165\u6027\u8bbe\u5907\uff0c\u9650\u5236\u4e86\u53ef\u7528\u4e8e\u79bb\u7ebf\u4f18\u5316\u63a7\u5236\u7684\u6570\u636e\u91cf\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u5927\u91cf\u5408\u6210\u6a21\u578b\u548c\u63a7\u5236\u7b97\u6cd5\u3002\u672c\u6587\u65e8\u5728\u5f15\u5165\u4e00\u4e2a\u795e\u7ecf\u751f\u7406\u5b66\u4e0a\u73b0\u5b9e\u7684\u57fa\u51c6\uff0c\u4ee5\u6bd4\u8f83\u8fd9\u4e9b\u6a21\u578b\u5e76\u4fc3\u8fdb\u667a\u80fd\u795e\u7ecf\u523a\u6fc0\u63a5\u53e3\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u4f20\u7edf\u7684\u57fa\u5e95\u795e\u7ecf\u8282\u56de\u8def\u52a8\u529b\u5b66\u548c\u75c5\u7406\u632f\u8361\uff0c\u5e76\u6355\u6349\u4e8615\u4e2a\u4ee5\u524d\u88ab\u5ffd\u89c6\u7684\u751f\u7406\u7279\u5f81\uff0c\u5982\u4fe1\u53f7\u4e0d\u7a33\u5b9a\u6027\u3001\u566a\u58f0\u3001\u795e\u7ecf\u6f02\u79fb\u3001\u7535\u6781\u7535\u5bfc\u53d8\u5316\u548c\u4e2a\u4f53\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u03b2\u6ce2\u6bb5\u6d3b\u52a8\u548c\u53cd\u9988\u5c06\u5176\u5efa\u6a21\u4e3a\u65f6\u7a7a\u5206\u5e03\u7684\u7279\u5f81\u3002", "result": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u73af\u5883\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u4f18\u5316aDBS\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u5e76\u9080\u8bf7\u673a\u5668\u5b66\u4e60\u793e\u533a\u53c2\u4e0e\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u4e2a\u795e\u7ecf\u751f\u7406\u5b66\u4e0a\u73b0\u5b9e\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u6bd4\u8f83\u81ea\u9002\u5e94\u6df1\u90e8\u8111\u523a\u6fc0\uff08aDBS\uff09\u6a21\u578b\uff0c\u5e76\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u73af\u5883\uff0c\u4ece\u800c\u4e3a\u4f18\u5316aDBS\u63a7\u5236\u7b56\u7565\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.09952", "pdf": "https://arxiv.org/pdf/2505.09952", "abs": "https://arxiv.org/abs/2505.09952", "authors": ["Tianyu Huai", "Jie Zhou", "Yuxuan Cai", "Qin Chen", "Wen Wu", "Xingjiao Wu", "Xipeng Qiu", "Liang He"], "title": "Task-Core Memory Management and Consolidation for Long-term Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to Neurips2025", "summary": "In this paper, we focus on a long-term continual learning (CL) task, where a\nmodel learns sequentially from a stream of vast tasks over time, acquiring new\nknowledge while retaining previously learned information in a manner akin to\nhuman learning. Unlike traditional CL settings, long-term CL involves handling\na significantly larger number of tasks, which exacerbates the issue of\ncatastrophic forgetting. Our work seeks to address two critical questions: 1)\nHow do existing CL methods perform in the context of long-term CL? and 2) How\ncan we mitigate the catastrophic forgetting that arises from prolonged\nsequential updates? To tackle these challenges, we propose a novel framework\ninspired by human memory mechanisms for long-term continual learning (Long-CL).\nSpecifically, we introduce a task-core memory management strategy to\nefficiently index crucial memories and adaptively update them as learning\nprogresses. Additionally, we develop a long-term memory consolidation mechanism\nthat selectively retains hard and discriminative samples, ensuring robust\nknowledge retention. To facilitate research in this area, we construct and\nrelease two multi-modal and textual benchmarks, MMLongCL-Bench and\nTextLongCL-Bench, providing a valuable resource for evaluating long-term CL\napproaches. Experimental results show that Long-CL outperforms the previous\nstate-of-the-art by 7.4\\% and 6.5\\% AP on the two benchmarks, respectively,\ndemonstrating the effectiveness of our approach.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10124", "pdf": "https://arxiv.org/pdf/2505.10124", "abs": "https://arxiv.org/abs/2505.10124", "authors": ["Ziad Kheil", "Lucas Robinet", "Laurent Risser", "Soleakhena Ken"], "title": "IMITATE: Image Registration with Context for unknown time frame recovery", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE ISBI 2025", "summary": "In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u914d\u51c6\u5f62\u5f0f\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e0e\u672a\u77e5\u6761\u4ef6\u76f8\u5173\u7684\u56fe\u50cf\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6216\u66f4\u591a\u5df2\u77e5\u56fe\u50cf\u53ca\u5176\u76f8\u5173\u6761\u4ef6\u3002\u901a\u8fc7\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u6761\u4ef6U-Net\u67b6\u6784\uff0c\u8be5\u65b9\u6cd5\u5145\u5206\u5229\u7528\u4e86\u6761\u4ef6\u4fe1\u606f\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u56fa\u5b9a\u56fe\u50cf\u3002\u8be5\u5f62\u5f0f\u88ab\u5e94\u7528\u4e8e\u653e\u5c04\u6cbb\u7597\u4e2d\u4e0d\u540c\u547c\u5438\u5e45\u5ea6\u7684\u56fe\u50cf\u79fb\u52a8\u80bf\u7624\uff0c\u4f7f\u7528\u80f8\u8179\u90e8\u533a\u57df\u76844D-CT\uff083D+t\uff09\u626b\u63cf\u3002\u8be5\u5e94\u7528\u7279\u522b\u590d\u6742\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5c06\u4e00\u7cfb\u52172D\u5207\u7247\u62fc\u63a5\u6210\u4e0d\u540c\u5668\u5b98\u4f4d\u7f6e\u7684\u591a\u4e2a3D\u4f53\u79ef\u3002\u6807\u51c6\u65b9\u6cd5\u5728\u7ec4\u88c5\u4f53\u79ef\u4e2d\u4f1a\u4ea7\u751f\u5df2\u77e5\u7684\u91cd\u5efa\u4f2a\u5f71\uff0c\u7531\u4e8e\u4e0d\u89c4\u5219\u7684\u60a3\u8005\u547c\u5438\u3001\u6ede\u56de\u6548\u5e94\u548c\u547c\u5438\u4fe1\u53f7\u4e0e\u5185\u90e8\u8fd0\u52a8\u7684\u76f8\u5173\u6027\u5dee\u3002\u57284D-CT\u4e34\u5e8a\u6570\u636e\u4e0a\u83b7\u5f97\u7684\u7ed3\u679c\u5c55\u793a\u4e86\u901a\u8fc7\u5b9e\u65f6\u5ef6\u8fdf\u5b9e\u73b0\u7684\u65e0\u4f2a\u5f71\u4f53\u79ef\u3002\u4ee3\u7801\u53ef\u5728https://github.com/Kheil-Z/IMITATE \u4e0a\u516c\u5f00\u83b7\u53d6\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u653e\u5c04\u6cbb\u7597\u4e2d\u7531\u4e8e\u4e0d\u89c4\u5219\u547c\u5438\u3001\u6ede\u56de\u6548\u5e94\u548c\u547c\u5438\u4fe1\u53f7\u4e0e\u5185\u90e8\u8fd0\u52a8\u76f8\u5173\u6027\u5dee\u800c\u5bfc\u81f4\u7684\u56fe\u50cf\u91cd\u5efa\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u914d\u51c6\u5f62\u5f0f\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6216\u66f4\u591a\u5df2\u77e5\u56fe\u50cf\u53ca\u5176\u76f8\u5173\u6761\u4ef6\uff0c\u4f7f\u7528\u65b0\u7684\u6761\u4ef6U-Net\u67b6\u6784\u6765\u5efa\u6a21\u3002", "result": "\u57284D-CT\u4e34\u5e8a\u6570\u636e\u4e0a\u83b7\u5f97\u7684\u7ed3\u679c\u5c55\u793a\u4e86\u901a\u8fc7\u5b9e\u65f6\u5ef6\u8fdf\u5b9e\u73b0\u7684\u65e0\u4f2a\u5f71\u4f53\u79ef\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u56fe\u50cf\u914d\u51c6\u5f62\u5f0f\u80fd\u591f\u6709\u6548\u89e3\u51b3\u653e\u5c04\u6cbb\u7597\u4e2d\u7531\u4e8e\u4e0d\u89c4\u5219\u547c\u5438\u7b49\u95ee\u9898\u5bfc\u81f4\u7684\u56fe\u50cf\u91cd\u5efa\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u4e14\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356", "abs": "https://arxiv.org/abs/2505.10356", "authors": ["Chunyu Ye", "Shaonan Wang"], "title": "Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli", "categories": ["cs.CL"], "comment": null, "summary": "Decoding thoughts from brain activity offers valuable insights into human\ncognition and enables promising applications in brain-computer interaction.\nWhile prior studies have explored language reconstruction from fMRI data, they\nare typically limited to single-modality inputs such as images or audio. In\ncontrast, human thought is inherently multimodal. To bridge this gap, we\npropose a unified and flexible framework for reconstructing coherent language\nfrom brain recordings elicited by diverse input modalities-visual, auditory,\nand textual. Our approach leverages visual-language models (VLMs), using\nmodality-specific experts to jointly interpret information across modalities.\nExperiments demonstrate that our method achieves performance comparable to\nstate-of-the-art systems while remaining adaptable and extensible. This work\nadvances toward more ecologically valid and generalizable mind decoding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u79cd\u8f93\u5165\u6a21\u6001\u4e2d\u91cd\u5efa\u8fde\u8d2f\u7684\u8bed\u8a00\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u89e3\u7801\u4eba\u7c7b\u601d\u7ef4\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u8f93\u5165\uff0c\u5982\u56fe\u50cf\u6216\u97f3\u9891\uff0c\u800c\u4eba\u7c7b\u601d\u7ef4\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7531\u4e0d\u540c\u8f93\u5165\u6a21\u6001\uff08\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\uff09\u5f15\u53d1\u7684\u8111\u8bb0\u5f55\u4e2d\u91cd\u5efa\u8fde\u8d2f\u7684\u8bed\u8a00\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u4f7f\u7528\u6a21\u6001\u7279\u5b9a\u7684\u4e13\u5bb6\u6765\u8054\u5408\u89e3\u91ca\u8de8\u6a21\u6001\u7684\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63a8\u52a8\u4e86\u66f4\u751f\u6001\u6709\u6548\u548c\u901a\u7528\u7684\u601d\u7ef4\u89e3\u7801\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.09646", "pdf": "https://arxiv.org/pdf/2505.09646", "abs": "https://arxiv.org/abs/2505.09646", "authors": ["Carmel Mary Esther A"], "title": "Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making", "categories": ["q-bio.NC", "cs.AI", "physics.hist-ph"], "comment": "8 pages, 3 figures", "summary": "This paper proposes a novel theoretical model to explain how the human mind\nand artificial intelligence can approach real-time awareness by reducing\nperceptual delays. By investigating cosmic signal delay, neurological reaction\ntimes, and the ancient cognitive state of stillness, we explore how one may\nshift from reactive perception to a conscious interface with the near future.\nThis paper introduces both a physical and cognitive model for perceiving the\npresent not as a linear timestamp, but as an interference zone where\nearly-arriving cosmic signals and reactive human delays intersect. We propose\nexperimental approaches to test these ideas using human neural observation and\nneuro-receptive extensions. Finally, we propose a mathematical framework to\nguide the evolution of AI systems toward temporally efficient, ethically sound,\nand internally conscious decision-making processes", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u91ca\u4eba\u7c7b\u5927\u8111\u548c\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u901a\u8fc7\u51cf\u5c11\u611f\u77e5\u5ef6\u8fdf\u6765\u5b9e\u73b0\u5b9e\u65f6\u610f\u8bc6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4eba\u7c7b\u5927\u8111\u548c\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u901a\u8fc7\u51cf\u5c11\u611f\u77e5\u5ef6\u8fdf\u6765\u5b9e\u73b0\u5b9e\u65f6\u610f\u8bc6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6570\u5b66\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7814\u7a76\u5b87\u5b99\u4fe1\u53f7\u5ef6\u8fdf\u3001\u795e\u7ecf\u53cd\u5e94\u65f6\u95f4\u548c\u53e4\u4ee3\u8ba4\u77e5\u72b6\u6001\u7684\u9759\u6b62\u72b6\u6001\uff0c\u63a2\u7d22\u5982\u4f55\u4ece\u53cd\u5e94\u6027\u611f\u77e5\u8f6c\u53d8\u4e3a\u5bf9\u8fd1\u672a\u6765\u7684\u6709\u610f\u8bc6\u754c\u9762\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4f7f\u7528\u4eba\u7c7b\u795e\u7ecf\u89c2\u5bdf\u548c\u795e\u7ecf\u63a5\u53d7\u6269\u5c55\u6765\u6d4b\u8bd5\u8fd9\u4e9b\u60f3\u6cd5\u7684\u5b9e\u9a8c\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u91ca\u4eba\u7c7b\u5927\u8111\u548c\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u901a\u8fc7\u51cf\u5c11\u611f\u77e5\u5ef6\u8fdf\u6765\u5b9e\u73b0\u5b9e\u65f6\u610f\u8bc6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6a21\u578b\uff0c\u4ee5\u89e3\u91ca\u4eba\u7c7b\u5927\u8111\u548c\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u901a\u8fc7\u51cf\u5c11\u611f\u77e5\u5ef6\u8fdf\u6765\u5b9e\u73b0\u5b9e\u65f6\u610f\u8bc6\u3002\u540c\u65f6\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u6307\u5bfc\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5411\u65f6\u95f4\u9ad8\u6548\u3001\u9053\u5fb7\u4e0a\u6b63\u5f53\u4e14\u5185\u90e8\u6709\u610f\u8bc6\u7684\u51b3\u7b56\u8fc7\u7a0b\u53d1\u5c55\u3002"}}
{"id": "2505.09955", "pdf": "https://arxiv.org/pdf/2505.09955", "abs": "https://arxiv.org/abs/2505.09955", "authors": ["Jaeho Kim", "Seulki Lee"], "title": "TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 Accept", "summary": "Unsupervised domain adaptation (UDA) for time series data remains a critical\nchallenge in deep learning, with traditional pseudo-labeling strategies failing\nto capture temporal patterns and channel-wise shifts between domains, producing\nsub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that\naddresses these limitations by modeling the joint distribution $P(\\mathbf{X},\ny)$ of the source domain through code transition matrices, where the codes are\nderived from vector quantization (VQ) of time series patches. Our method\nconstructs class- and channel-wise code transition matrices from the source\ndomain and employs Bayes' rule for target domain adaptation, generating\npseudo-labels based on channel-wise weighted class-conditional likelihoods.\nTransPL offers three key advantages: explicit modeling of temporal transitions\nand channel-wise shifts between different domains, versatility towards\ndifferent UDA scenarios (e.g., weakly-supervised UDA), and explainable\npseudo-label generation. We validate TransPL's effectiveness through extensive\nanalysis on four time series UDA benchmarks and confirm that it consistently\noutperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%\naccuracy improvement, 4.9% F1 improvement), while providing interpretable\ninsights into the domain adaptation process through its learned code transition\nmatrices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TransPL \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\u5efa\u6a21\u6e90\u57df\u7684\u8054\u5408\u5206\u5e03\uff0c\u5e76\u5229\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u751f\u6210\u4f2a\u6807\u7b7e\u3002TransPL \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002", "motivation": "\u4f20\u7edf\u4f2a\u6807\u7b7e\u7b56\u7565\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u6a21\u5f0f\u548c\u901a\u9053\u95f4\u504f\u79fb\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684\u4f2a\u6807\u7b7e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "TransPL \u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\u5bf9\u6e90\u57df\u7684\u8054\u5408\u5206\u5e03 P(X, y) \u8fdb\u884c\u5efa\u6a21\uff0c\u5176\u4e2d\u4ee3\u7801\u662f\u4ece\u65f6\u95f4\u5e8f\u5217\u7247\u6bb5\u7684\u5411\u91cf\u91cf\u5316 (VQ) \u5f97\u5230\u7684\u3002\u8be5\u65b9\u6cd5\u4ece\u6e90\u57df\u6784\u5efa\u7c7b\u548c\u901a\u9053\u76f8\u5173\u7684\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u8fdb\u884c\u76ee\u6807\u57df\u9002\u5e94\uff0c\u57fa\u4e8e\u901a\u9053\u52a0\u6743\u7684\u7c7b\u6761\u4ef6\u4f3c\u7136\u751f\u6210\u4f2a\u6807\u7b7e\u3002", "result": "TransPL \u5728\u56db\u4e2a\u65f6\u95f4\u5e8f\u5217 UDA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u9ad8\u4e86 6.1%\uff0c\u5728 F1 \u5206\u6570\u4e0a\u63d0\u9ad8\u4e86 4.9%\u3002", "conclusion": "TransPL \u7684\u6709\u6548\u6027\u5df2\u5728\u56db\u4e2a\u65f6\u95f4\u5e8f\u5217 UDA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u4e14\u5b83\u5728\u4f2a\u6807\u7b7e\u751f\u6210\u65b9\u9762\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.10152", "pdf": "https://arxiv.org/pdf/2505.10152", "abs": "https://arxiv.org/abs/2505.10152", "authors": ["Yikang Wei"], "title": "Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization", "categories": ["cs.CV"], "comment": "IJCAI 2025", "summary": "Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff08MCSAD\uff09\uff0c\u901a\u8fc7\u8de8\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u98ce\u683c\u589e\u5f3a\u65b9\u6cd5\u8981\u4e48\u63a2\u7d22\u5b64\u7acb\u6e90\u57df\u5185\u7684\u6570\u636e\u98ce\u683c\uff0c\u8981\u4e48\u5728\u6570\u636e\u53bb\u4e2d\u5fc3\u5316\u573a\u666f\u4e0b\u8de8\u73b0\u6709\u6e90\u57df\u63d2\u503c\u98ce\u683c\u4fe1\u606f\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6709\u9650\u7684\u98ce\u683c\u7a7a\u95f4\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u65b9\u6cd5\uff08MCSAD\uff09\uff0c\u901a\u8fc7\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u548c\u7c7b\u95f4\u5173\u7cfb\u96c6\u6210\u84b8\u998f\u8fdb\u884c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.10389", "pdf": "https://arxiv.org/pdf/2505.10389", "abs": "https://arxiv.org/abs/2505.10389", "authors": ["Benjamin White", "Anastasia Shimorina"], "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u9762\u60c5\u611f\u5206\u6790\u7cfb\u7edf\uff0c\u91cd\u70b9\u5728\u4e8e\u56db\u5143\u7ec4\u89c2\u70b9\u63d0\u53d6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u591a\u9886\u57df\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u53ef\u4ee5\u4e0e\u5355\u9886\u57df\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u64cd\u4f5c\u590d\u6742\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b9\u9762\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u8bbe\u8ba1\uff0c\u5e76\u89e3\u51b3\u8de8\u9886\u57df\u548c\u8de8\u8bed\u8a00\u7684\u56db\u5143\u7ec4\u89c2\u70b9\u63d0\u53d6\u95ee\u9898\u3002", "method": "\u672c\u6587\u4f7f\u7528\u5185\u90e8\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4e86\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u4e00\u4e2a\u6a21\u578b\u6765\u540c\u65f6\u6709\u6548\u5904\u7406\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86\u4e00\u4e2a\u7ed3\u5408\u7684\u591a\u9886\u57df\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u53ef\u4ee5\u4e0e\u4e13\u95e8\u7684\u5355\u9886\u57df\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u64cd\u4f5c\u590d\u6742\u6027\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5355\u4e00\u7684\u591a\u9886\u57df\u6a21\u578b\u5728\u5904\u7406\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u5206\u7c7b\u6cd5\u65f6\u53ef\u4ee5\u8fbe\u5230\u4e0e\u4e13\u7528\u5355\u9886\u57df\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u64cd\u4f5c\u590d\u6742\u6027\u3002"}}
{"id": "2505.09651", "pdf": "https://arxiv.org/pdf/2505.09651", "abs": "https://arxiv.org/abs/2505.09651", "authors": ["Xixuan Hao", "Yutian Jiang", "Xingchen Zou", "Jiabo Liu", "Yifang Yin", "Yuxuan Liang"], "title": "Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": null, "summary": "Location Intelligence (LI), the science of transforming location-centric\ngeospatial data into actionable knowledge, has become a cornerstone of modern\nspatial decision-making. The rapid evolution of Geospatial Representation\nLearning is fundamentally reshaping LI development through two successive\ntechnological revolutions: the deep learning breakthrough and the emerging\nlarge language model (LLM) paradigm. While deep neural networks (DNNs) have\ndemonstrated remarkable success in automated feature extraction from structured\ngeospatial data (e.g., satellite imagery, GPS trajectories), the recent\nintegration of LLMs introduces transformative capabilities for cross-modal\ngeospatial reasoning and unstructured geo-textual data processing. This survey\npresents a comprehensive review of geospatial representation learning across\nboth technological eras, organizing them into a structured taxonomy based on\nthe complete pipeline comprising: (1) data perspective, (2) methodological\nperspective and (3) application perspective. We also highlight current\nadvancements, discuss existing limitations, and propose potential future\nresearch directions in the LLM era. This work offers a thorough exploration of\nthe field and providing a roadmap for further innovation in LI. The summary of\nthe up-to-date paper list can be found in\nhttps://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo\ncontinuous updates.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5730\u7406\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\uff0c\u8ba8\u8bba\u4e86\u5176\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\u7684\u8fdb\u5c55\u3001\u5c40\u9650\u6027\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f4d\u7f6e\u667a\u80fd\uff08LI\uff09\u5df2\u6210\u4e3a\u73b0\u4ee3\u7a7a\u95f4\u51b3\u7b56\u7684\u6838\u5fc3\u3002\u968f\u7740\u5730\u7406\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5b83\u6b63\u5728\u901a\u8fc7\u4e24\u6b21\u6280\u672f\u9769\u547d\u91cd\u65b0\u5851\u9020LI\u7684\u53d1\u5c55\uff1a\u6df1\u5ea6\u5b66\u4e60\u7a81\u7834\u548c\u65b0\u5174\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8303\u5f0f\u3002", "method": "\u672c\u6587\u5bf9\u5730\u7406\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u5e76\u6839\u636e\u5b8c\u6574\u7684\u6d41\u7a0b\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u7684\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\uff1a(1) \u6570\u636e\u89c6\u89d2\uff0c(2) \u65b9\u6cd5\u8bba\u89c6\u89d2\u548c(3) \u5e94\u7528\u89c6\u89d2\u3002", "result": "\u672c\u6587\u603b\u7ed3\u4e86\u6700\u65b0\u7684\u8bba\u6587\u5217\u8868\uff0c\u5e76\u63d0\u51fa\u4e86\u5728LLM\u65f6\u4ee3\u53ef\u80fd\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u4f4d\u7f6e\u667a\u80fd\u9886\u57df\u7684\u5168\u9762\u63a2\u7d22\uff0c\u5e76\u4e3aLI\u7684\u8fdb\u4e00\u6b65\u521b\u65b0\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002"}}
{"id": "2505.09959", "pdf": "https://arxiv.org/pdf/2505.09959", "abs": "https://arxiv.org/abs/2505.09959", "authors": ["Zengxia Guo", "Bohui An", "Zhongqi Lu"], "title": "Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated reinforcement learning (FRL) methods usually share the encrypted\nlocal state or policy information and help each client to learn from others\nwhile preserving everyone's privacy. In this work, we propose that sharing the\napproximated behavior metric-based state projection function is a promising way\nto enhance the performance of FRL and concurrently provides an effective\nprotection of sensitive information. We introduce FedRAG, a FRL framework to\nlearn a computationally practical projection function of states for each client\nand aggregating the parameters of projection functions at a central server. The\nFedRAG approach shares no sensitive task-specific information, yet provides\ninformation gain for each client. We conduct extensive experiments on the\nDeepMind Control Suite to demonstrate insightful results.", "AI": {"tldr": "FedRAG \u662f\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u8fd1\u4f3c\u884c\u4e3a\u5ea6\u91cf\u7684\u72b6\u6001\u6295\u5f71\u51fd\u6570\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5171\u4eab\u52a0\u5bc6\u7684\u672c\u5730\u72b6\u6001\u6216\u7b56\u7565\u4fe1\u606f\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u4fdd\u62a4\u654f\u611f\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u884c\u4e3a\u5ea6\u91cf\u7684\u72b6\u6001\u6295\u5f71\u51fd\u6570\u5171\u4eab\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e2d\u592e\u670d\u52a1\u5668\u805a\u5408\u6295\u5f71\u51fd\u6570\u7684\u53c2\u6570\u6765\u5b9e\u73b0\u3002", "result": "\u5728 DeepMind Control Suite \u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86 FedRAG \u7684\u6709\u6548\u6027\u3002", "conclusion": "FedRAG \u662f\u4e00\u79cd\u6709\u6548\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.10169", "pdf": "https://arxiv.org/pdf/2505.10169", "abs": "https://arxiv.org/abs/2505.10169", "authors": ["Matthias K\u00fcmmerer", "Harneet Khanuja", "Matthias Bethge"], "title": "Modeling Saliency Dataset Bias", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u6570\u636e\u96c6\u663e\u8457\u6027\u9884\u6d4b\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u96c6\u7279\u5b9a\u53c2\u6570\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u590d\u6742\u7684\u591a\u5c3a\u5ea6\u663e\u8457\u6027\u6548\u5e94\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u56fe\u50cf\u663e\u8457\u6027\u9884\u6d4b\u65b9\u6cd5\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u63a5\u8fd1\u9ec4\u91d1\u6807\u51c6\u6027\u80fd\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u8de8\u591a\u4e2a\u663e\u8457\u6027\u6570\u636e\u96c6\u9884\u6d4b\u6ce8\u89c6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6570\u636e\u96c6\u5b58\u5728\u504f\u5dee\u3002\u5f53\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u6a21\u578b\u65f6\uff0c\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff08\u7ea640%\uff09\u3002\u589e\u52a0\u6570\u636e\u96c6\u591a\u6837\u6027\u5e76\u4e0d\u80fd\u89e3\u51b3\u8fd9\u4e00\u8de8\u6570\u636e\u96c6\u5dee\u8ddd\uff0c\u63a5\u8fd160%\u7684\u5dee\u8ddd\u5f52\u56e0\u4e8e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u504f\u5dee\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u6269\u5c55\u4e86\u4e00\u4e2a\u5927\u90e8\u5206\u6570\u636e\u96c6\u65e0\u5173\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u4ec5\u5305\u542b\u5c11\u4e8e20\u4e2a\u6570\u636e\u96c6\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u63a7\u5236\u53ef\u89e3\u91ca\u7684\u673a\u5236\uff0c\u5982\u591a\u5c3a\u5ea6\u7ed3\u6784\u3001\u4e2d\u5fc3\u504f\u5dee\u548c\u6ce8\u89c6\u6269\u6563\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728MIT/Tuebingen Saliency Benchmark\u7684\u4e09\u4e2a\u6570\u636e\u96c6\uff08MIT300\u3001CAT2000\u548cCOCO-Freeview\uff09\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5373\u4f7f\u7eaf\u7cb9\u4ece\u4e0d\u76f8\u5173\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6cdb\u5316\uff0c\u4f46\u5f53\u9002\u5e94\u5230\u76f8\u5e94\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u65f6\uff0c\u6548\u679c\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728MIT/Tuebingen Saliency Benchmark\u7684\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5373\u4f7f\u7eaf\u7cb9\u4ece\u4e0d\u76f8\u5173\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6cdb\u5316\uff0c\u4f46\u5f53\u9002\u5e94\u5230\u76f8\u5e94\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u65f6\uff0c\u6548\u679c\u6709\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u7a7a\u95f4\u663e\u8457\u6027\u5c5e\u6027\u7684\u6709\u4ef7\u503c\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u7ed3\u5408\u7edd\u5bf9\u548c\u76f8\u5bf9\u5927\u5c0f\u7684\u590d\u6742\u591a\u5c3a\u5ea6\u6548\u5e94\u3002"}}
{"id": "2505.10402", "pdf": "https://arxiv.org/pdf/2505.10402", "abs": "https://arxiv.org/abs/2505.10402", "authors": ["Yihong Dong", "Yuchen Liu", "Xue Jiang", "Zhi Jin", "Ge Li"], "title": "Rethinking Repetition Problems of LLMs in Code Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025 (main)", "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRPG\u7684\u9ad8\u6548\u89e3\u7801\u65b9\u6cd5\uff0c\u4ee5\u7f13\u89e3\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6CodeRepetEval\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eRPG\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\uff0c\u4f46\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u91cd\u590d\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002\u4ee5\u5f80\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5185\u5bb9\u91cd\u590d\u4e0a\uff0c\u800c\u7ed3\u6784\u91cd\u590d\u662f\u66f4\u666e\u904d\u4e14\u66f4\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "RPG\uff08\u57fa\u4e8e\u8bed\u6cd5\u7684\u91cd\u590d\u60e9\u7f5a\uff09\u65b9\u6cd5\u9996\u5148\u5229\u7528\u8bed\u6cd5\u89c4\u5219\u8bc6\u522b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\uff0c\u7136\u540e\u6218\u7565\u6027\u5730\u964d\u4f4e\u5bfc\u81f4\u91cd\u590d\u7684\u5173\u952e\u6807\u8bb0\u7684\u53ef\u80fd\u6027\uff0c\u4ece\u800c\u51cf\u8f7b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\u3002", "result": "RPG\u5728CodeRepetEval\u6570\u636e\u96c6\u4ee5\u53caHumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u3002", "conclusion": "RPG\u5728CodeRepetEval\u6570\u636e\u96c6\u4ee5\u53caHumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u4f73\u57fa\u7ebf\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.09653", "pdf": "https://arxiv.org/pdf/2505.09653", "abs": "https://arxiv.org/abs/2505.09653", "authors": ["Samuel Yen-Chi Chen", "Chen-Yu Liu", "Kuan-Cheng Chen", "Wei-Jia Huang", "Yen-Jui Chang", "Wei-Hao Huang"], "title": "Differentiable Quantum Architecture Search in Quantum-Enhanced Neural Network Parameter Generation", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG", "cs.NE"], "comment": null, "summary": "The rapid advancements in quantum computing (QC) and machine learning (ML)\nhave led to the emergence of quantum machine learning (QML), which integrates\nthe strengths of both fields. Among QML approaches, variational quantum\ncircuits (VQCs), also known as quantum neural networks (QNNs), have shown\npromise both empirically and theoretically. However, their broader adoption is\nhindered by reliance on quantum hardware during inference. Hardware\nimperfections and limited access to quantum devices pose practical challenges.\nTo address this, the Quantum-Train (QT) framework leverages the exponential\nscaling of quantum amplitudes to generate classical neural network parameters,\nenabling inference without quantum hardware and achieving significant parameter\ncompression. Yet, designing effective quantum circuit architectures for such\nquantum-enhanced neural programmers remains non-trivial and often requires\nexpertise in quantum information science. In this paper, we propose an\nautomated solution using differentiable optimization. Our method jointly\noptimizes both conventional circuit parameters and architectural parameters in\nan end-to-end manner via automatic differentiation. We evaluate the proposed\nframework on classification, time-series prediction, and reinforcement learning\ntasks. Simulation results show that our method matches or outperforms manually\ndesigned QNN architectures. This work offers a scalable and automated pathway\nfor designing QNNs that can generate classical neural network parameters across\ndiverse applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u4f18\u5316\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u91cf\u5b50\u786c\u4ef6\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5c3d\u7ba1\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQCs\uff09\u5728\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5e7f\u6cdb\u5e94\u7528\u53d7\u5230\u5bf9\u91cf\u5b50\u786c\u4ef6\u4f9d\u8d56\u7684\u9650\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u6709\u6548\u7684\u91cf\u5b50\u7535\u8def\u67b6\u6784\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u4f18\u5316\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u4f20\u7edf\u7535\u8def\u53c2\u6570\u548c\u67b6\u6784\u53c2\u6570\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5206\u7c7b\u3001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u624b\u52a8\u8bbe\u8ba1\u7684QNN\u67b6\u6784\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u6765\u8bbe\u8ba1\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u91cf\u5b50\u786c\u4ef6\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\uff0c\u5e76\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.09969", "pdf": "https://arxiv.org/pdf/2505.09969", "abs": "https://arxiv.org/abs/2505.09969", "authors": ["Ali Azimi Lamir", "Shiva Razzagzadeh", "Zeynab Rezaei"], "title": "A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study presents a machine learning-based framework for heart disease\nprediction using the heart-disease dataset, comprising 303 samples with 14\nfeatures. The methodology involves data preprocessing, model training, and\nevaluation using three classifiers: Logistic Regression, K-Nearest Neighbors\n(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and\nRandomizedSearchCV was employed to enhance model performance. The Random Forest\nclassifier outperformed other models, achieving an accuracy of 91% and an\nF1-score of 0.89. Evaluation metrics, including precision, recall, and\nconfusion matrix, revealed balanced performance across classes. The proposed\nmodel demonstrates strong potential for aiding clinical decision-making by\neffectively predicting heart disease. Limitations such as dataset size and\ngeneralizability underscore the need for future studies using larger and more\ndiverse datasets. This work highlights the utility of machine learning in\nhealthcare, offering insights for further advancements in predictive\ndiagnostics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5fc3\u810f\u75c5\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u79cd\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u4e2d\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u4f73\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5728\u5fc3\u810f\u75c5\u9884\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u5fc3\u810f\u75c5\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u4e09\u79cd\u5206\u7c7b\u5668\uff1a\u903b\u8f91\u56de\u5f52\u3001K-\u8fd1\u90bb\uff08KNN\uff09\u548c\u968f\u673a\u68ee\u6797\u3002\u901a\u8fc7GridSearchCV\u548cRandomizedSearchCV\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u4e3a91%\uff0cF1\u5f97\u5206\u4e3a0.89\u3002\u8bc4\u4f30\u6307\u6807\u663e\u793a\u5404\u7c7b\u522b\u4e4b\u95f4\u8868\u73b0\u5747\u8861\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u8f85\u52a9\u9884\u6d4b\u5fc3\u810f\u75c5\u7684\u6f5c\u529b\uff0c\u4f46\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6cdb\u5316\u80fd\u529b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u672a\u6765\u4f7f\u7528\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u7814\u7a76\u3002"}}
{"id": "2505.10205", "pdf": "https://arxiv.org/pdf/2505.10205", "abs": "https://arxiv.org/abs/2505.10205", "authors": ["Umair Haroon", "Ahmad AlMughrabi", "Thanasis Zoumpekas", "Ricardo Marques", "Petia Radeva"], "title": "VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVolE\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u79fb\u52a8\u8bbe\u5907\u9a71\u52a8\u76843D\u91cd\u5efa\u6765\u4f30\u8ba1\u98df\u54c1\u4f53\u79ef\uff0c\u65e0\u9700\u53c2\u8003\u7269\u6216\u6df1\u5ea6\u4fe1\u606f\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u7684\u98df\u54c1\u4f53\u79ef\u4f30\u8ba1\u65b9\u6cd5\u53d7\u5230\u5355\u6838\u6570\u636e\u3001\u4e13\u7528\u786c\u4ef6\u6216\u4f9d\u8d56\u53c2\u8003\u7269\u4f53\u76f8\u673a\u6821\u51c6\u7684\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u548c\u5b9e\u7528\u7684\u65b9\u6cd5\u3002", "method": "VolE\u5229\u7528\u79fb\u52a8\u8bbe\u5907\u9a71\u52a8\u76843D\u91cd\u5efa\u6765\u4f30\u8ba1\u98df\u54c1\u4f53\u79ef\uff0c\u901a\u8fc7\u81ea\u7531\u8fd0\u52a8\u6355\u6349\u56fe\u50cf\u548c\u76f8\u673a\u4f4d\u7f6e\u751f\u6210\u7cbe\u786e\u76843D\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u98df\u54c1\u89c6\u9891\u5206\u5272\u751f\u6210\u98df\u54c1\u63a9\u7801\u3002", "result": "VolE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862.22%\u7684MAPE\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u98df\u54c1\u4f53\u79ef\u4f30\u8ba1\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "VolE\u5728\u98df\u54c1\u4f53\u79ef\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u4f53\u79ef\u4f30\u8ba1\u6280\u672f\u3002"}}
{"id": "2505.10409", "pdf": "https://arxiv.org/pdf/2505.10409", "abs": "https://arxiv.org/abs/2505.10409", "authors": ["Yue Guo", "Jae Ho Sohn", "Gondy Leroy", "Trevor Cohen"], "title": "Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u666e\u901a\u8bed\u8a00\u6458\u8981\uff08PLS\uff09\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5c3d\u7ba1LLM\u751f\u6210\u7684PLS\u5728\u4e3b\u89c2\u8bc4\u4ef7\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4eba\u7c7b\u64b0\u5199\u7684PLS\u5728\u7406\u89e3\u65b9\u9762\u66f4\u4f18\u3002\u540c\u65f6\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\uff0c\u8fd9\u8868\u660e\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u6846\u67b6\u548c\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684PLS\u7684\u6709\u6548\u6027\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8e\u81ea\u52a8\u5316\u5206\u6570\u6216\u4fbf\u5229\u6837\u672c\u7684\u4e3b\u89c2Likert\u91cf\u8868\u8bc4\u5206\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u63a5\u8861\u91cf\u53ef\u7406\u89e3\u6027\u7684\u80fd\u529b\uff0c\u4e14\u7ed3\u679c\u7684\u666e\u9002\u6027\u6709\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLM\u751f\u6210\u7684PLS\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e9a\u9a6c\u900aMechanical Turk\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7684\u4f17\u5305\u8bc4\u4f30\uff0c\u6709150\u540d\u53c2\u4e0e\u8005\u53c2\u4e0e\u3002\u6211\u4eec\u901a\u8fc7\u4e3b\u89c2Likert\u91cf\u8868\u8bc4\u5206\uff08\u5173\u6ce8\u7b80\u5355\u6027\u3001\u4fe1\u606f\u91cf\u3001\u8fde\u8d2f\u6027\u548c\u5fe0\u5b9e\u6027\uff09\u4ee5\u53ca\u5ba2\u89c2\u7684\u591a\u9879\u9009\u62e9\u7406\u89e3\u548c\u56de\u5fc6\u6d4b\u91cf\u6765\u8bc4\u4f30PLS\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u68c0\u67e5\u4e8610\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136LLM\u751f\u6210\u7684PLS\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u770b\u8d77\u6765\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684PLS\u65e0\u6cd5\u533a\u5206\uff0c\u4f46\u4eba\u7c7b\u64b0\u5199\u7684PLS\u5728\u7406\u89e3\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\uff0c\u8fd9\u5f15\u53d1\u4e86\u5b83\u4eec\u5728\u8bc4\u4f30PLS\u4e2d\u7684\u9002\u7528\u6027\u7684\u7591\u95ee\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u770b\u8d77\u6765\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684PLS\u65e0\u6cd5\u533a\u5206\u7684\u6458\u8981\uff0c\u4f46\u4eba\u7c7b\u64b0\u5199\u7684PLS\u5728\u7406\u89e3\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\uff0c\u8fd9\u5f15\u53d1\u4e86\u5b83\u4eec\u5728\u8bc4\u4f30PLS\u4e2d\u7684\u9002\u7528\u6027\u7684\u7591\u95ee\u3002\u9700\u8981\u5f00\u53d1\u8d85\u8d8a\u8868\u9762\u8d28\u91cf\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u4f18\u5316\u4e3a\u975e\u4e13\u4e1a\u8bfb\u8005\u7406\u89e3\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.09661", "pdf": "https://arxiv.org/pdf/2505.09661", "abs": "https://arxiv.org/abs/2505.09661", "authors": ["Jinghao He", "Zhengyan Sheng", "Liping Chen", "Kong Aik Lee", "Zhen-Hua Ling"], "title": "Introducing voice timbre attribute detection", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper focuses on explaining the timbre conveyed by speech signals and\nintroduces a task termed voice timbre attribute detection (vTAD). In this task,\nvoice timbre is explained with a set of sensory attributes describing its human\nperception. A pair of speech utterances is processed, and their intensity is\ncompared in a designated timbre descriptor. Moreover, a framework is proposed,\nwhich is built upon the speaker embeddings extracted from the speech\nutterances. The investigation is conducted on the VCTK-RVA dataset.\nExperimental examinations on the ECAPA-TDNN and FACodec speaker encoders\ndemonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the\nseen scenario, where the testing speakers were included in the training set; 2)\nthe FACodec speaker encoder was superior in the unseen scenario, where the\ntesting speakers were not part of the training, indicating enhanced\ngeneralization capability. The VCTK-RVA dataset and open-source code are\navailable on the website https://github.com/vTAD2025-Challenge/vTAD.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u97f3\u97f3\u8272\u5c5e\u6027\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bf4\u8bdd\u4eba\u5d4c\u5165\u7684\u6846\u67b6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u4e0d\u540c\u7f16\u7801\u5668\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u91ca\u8bed\u97f3\u4fe1\u53f7\u4e2d\u7684\u97f3\u8272\uff0c\u5e76\u901a\u8fc7\u4e00\u7ec4\u63cf\u8ff0\u4eba\u7c7b\u611f\u77e5\u7684\u611f\u5b98\u5c5e\u6027\u6765\u8868\u8fbe\u8bed\u97f3\u97f3\u8272\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8bed\u97f3\u97f3\u8272\u5c5e\u6027\u68c0\u6d4b\uff08vTAD\uff09\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u8bed\u97f3\u8bed\u6599\u5bf9\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u5229\u7528\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6784\u5efa\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECAPA-TDNN\u5728\u5df2\u77e5\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800cFACodec\u5728\u672a\u77e5\u573a\u666f\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECAPA-TDNN\u5728\u5df2\u77e5\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u800cFACodec\u5728\u672a\u77e5\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002VCTK-RVA\u6570\u636e\u96c6\u548c\u5f00\u6e90\u4ee3\u7801\u5df2\u53d1\u5e03\u3002"}}
{"id": "2505.09983", "pdf": "https://arxiv.org/pdf/2505.09983", "abs": "https://arxiv.org/abs/2505.09983", "authors": ["Changxun Zhu", "Qilong Wu", "Lingjuan Lyu", "Shibei Xue"], "title": "Sybil-based Virtual Data Poisoning Attacks in Federated Learning", "categories": ["cs.LG"], "comment": "7 pages, 6 figures, accepted by IEEE Codit 2025", "summary": "Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8esybil\u7684\u865a\u62df\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210sybil\u8282\u70b9\u548c\u57fa\u4e8e\u68af\u5ea6\u5339\u914d\u7684\u865a\u62df\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u76ee\u6807\u6a21\u578b\u83b7\u53d6\u65b9\u6848\uff0c\u5728\u975e\u72ec\u7acb\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5bb9\u6613\u53d7\u5230\u6076\u610f\u5bf9\u624b\u7684\u4e2d\u6bd2\u653b\u51fb\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9ad8\u6602\u7684\u6210\u672c\u624d\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u653b\u51fb\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8esybil\u7684\u865a\u62df\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\uff0c\u5176\u4e2d\u6076\u610f\u5ba2\u6237\u7aef\u751f\u6210sybil\u8282\u70b9\u4ee5\u653e\u5927\u4e2d\u6bd2\u6a21\u578b\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u5339\u914d\u7684\u865a\u62df\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e09\u79cd\u9488\u5bf9\u76ee\u6807\u6a21\u578b\u83b7\u53d6\u7684\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5728\u7ebf\u672c\u5730\u3001\u5728\u7ebf\u5168\u5c40\u548c\u79bb\u7ebf\u573a\u666f\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u4e2d\u4f18\u4e8e\u5176\u4ed6\u653b\u51fb\u7b97\u6cd5\uff0c\u56e0\u4e3a\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u975e\u72ec\u7acb\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u4e0b\u83b7\u5f97\u5168\u5c40\u76ee\u6807\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u4e2d\u4f18\u4e8e\u5176\u4ed6\u653b\u51fb\u7b97\u6cd5\uff0c\u56e0\u4e3a\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u975e\u72ec\u7acb\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u4e0b\u83b7\u5f97\u5168\u5c40\u76ee\u6807\u6a21\u578b\u3002"}}
{"id": "2505.10223", "pdf": "https://arxiv.org/pdf/2505.10223", "abs": "https://arxiv.org/abs/2505.10223", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MIDL 2025", "summary": "Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u7b49\u66ff\u4ee3\u589e\u5f3a\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u7531\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u800c\u6027\u80fd\u4e0b\u964d\u3002\u4f20\u7edf\u89c6\u89c9\u4e00\u81f4\u7684\u589e\u5f3a\u7b56\u7565\u7f3a\u4e4f\u5e94\u5bf9\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u6240\u9700\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u66ff\u4ee3\u589e\u5f3a\u7b56\u7565\uff0c\u91cd\u70b9\u662fMixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u3002", "result": "\u8fd9\u4e9b\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u6210\u50cf\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u5fc3\u810f\u7535\u5f71MRI\u548c\u524d\u5217\u817aMRI\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u5e7f\u6cdb\u53d8\u6362\u4e0b\u7684\u6548\u679c\u3002", "conclusion": "\u8fd9\u4e9b\u589e\u5f3a\u65b9\u6cd5\u901a\u8fc7\u4fc3\u8fdb\u53ef\u5206\u79bb\u6027\u548c\u7d27\u51d1\u6027\u6765\u63d0\u9ad8\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8868\u793a\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u96c6\u6210\u4e3a\u63d0\u9ad8\u533b\u5b66\u5206\u5272\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u5b9e\u73b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10413", "pdf": "https://arxiv.org/pdf/2505.10413", "abs": "https://arxiv.org/abs/2505.10413", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Zhicheng Dou"], "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.", "AI": {"tldr": "LongRefiner \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u957f\u6587\u672c RAG \u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u6784\u7279\u5f81\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u5b9e\u73b0\u81ea\u9002\u5e94\u7cbe\u70bc\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684 RAG \u5e94\u7528\u5e38\u5e38\u9047\u5230\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u573a\u666f\uff0c\u5176\u4e2d\u5197\u4f59\u4fe1\u606f\u548c\u566a\u58f0\u5bfc\u81f4\u66f4\u9ad8\u7684\u63a8\u7406\u6210\u672c\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "LongRefiner \u901a\u8fc7\u5229\u7528\u957f\u6587\u6863\u7684\u56fa\u6709\u7ed3\u6784\u7279\u5f81\uff0c\u91c7\u7528\u53cc\u7ea7\u67e5\u8be2\u5206\u6790\u3001\u5206\u5c42\u6587\u6863\u7ed3\u6784\u548c\u57fa\u4e8e\u5355\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u8fdb\u884c\u81ea\u9002\u5e94\u7cbe\u70bc\u3002", "result": "\u5728\u4e03\u4e2a QA \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLongRefiner \u5728\u5404\u79cd\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u6bd4\u6700\u4f73\u57fa\u7ebf\u4f4e 10 \u500d\u3002", "conclusion": "LongRefiner \u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u957f\u6587\u672c RAG \u5e94\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2505.10003", "pdf": "https://arxiv.org/pdf/2505.10003", "abs": "https://arxiv.org/abs/2505.10003", "authors": ["Tianyu Jiao", "Zhuoran Xiao", "Yihang Huang", "Chenhui Ye", "Yijia Feng", "Liyu Cai", "Jiang Chang", "Fangkun Liu", "Yin Xu", "Dazhi He", "Yunfeng Guan", "Wenjun Zhang"], "title": "AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAI2MMUM\u7684\u53ef\u6269\u5c55\u3001\u4efb\u52a1\u611f\u77e5\u7684\u4eba\u5de5\u667a\u80fd-\u7a7a\u6c14\u63a5\u53e3\u591a\u6a21\u6001\u901a\u7528\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6839\u636e\u7ec6\u5fae\u7684\u4efb\u52a1\u6307\u4ee4\u7075\u6d3b\u6709\u6548\u5730\u6267\u884c\u5404\u79cd\u7269\u7406\u5c42\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a6G\u5bfc\u5411\u7684\u901a\u7528\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u5e76\u6267\u884c\u591a\u6837\u5316\u7684\u7a7a\u4e2d\u63a5\u53e3\u4efb\u52a1\uff0c\u5df2\u6210\u4e3a\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u7684\u4e00\u4e2a\u5171\u540c\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4efb\u52a1\u611f\u77e5\u7684\u4eba\u5de5\u667a\u80fd-\u7a7a\u6c14\u63a5\u53e3\u591a\u6a21\u6001\u901a\u7528\u6a21\u578b\uff08AI2MMUM\uff09\uff0c\u901a\u8fc7\u56fa\u5b9a\u4efb\u52a1\u5173\u952e\u8bcd\u548c\u53ef\u5b66\u4e60\u7684\u9690\u5f0f\u524d\u7f00\u63d0\u793a\u6765\u589e\u5f3a\u4efb\u52a1\u9002\u5e94\u6027\u3002LLM\u4e3b\u5e72\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u91c7\u7528\u5fae\u8c03\u65b9\u6cd5\u878d\u5165\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u5934\u90e8\u76f4\u63a5\u8f93\u51fa\u4efb\u52a1\u76ee\u6807\u3002", "result": "AI2MMUM\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u7269\u7406\u73af\u5883/\u65e0\u7ebf\u4fe1\u9053\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "conclusion": "AI2MMUM\u5728\u57fa\u4e8eWAIR-D\u548cDeepMIMO\u6570\u636e\u96c6\u7684\u4e94\u4e2a\u4ee3\u8868\u6027\u7269\u7406\u73af\u5883/\u65e0\u7ebf\u4fe1\u9053\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002"}}
{"id": "2505.10231", "pdf": "https://arxiv.org/pdf/2505.10231", "abs": "https://arxiv.org/abs/2505.10231", "authors": ["Haozhe Luo", "Ziyu Zhou", "Zixin Shu", "Aur\u00e9lie Pahud de Mortanges", "Robert Berke", "Mauricio Reyes"], "title": "On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u4eba\u673a\u5bf9\u9f50\u548c\u516c\u5e73\u6027\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u6574\u5408\u4eba\u7c7b\u89c1\u89e3\u53ef\u4ee5\u51cf\u5c11\u516c\u5e73\u6027\u5dee\u8ddd\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8fc7\u5ea6\u5bf9\u9f50\u53ef\u80fd\u5e26\u6765\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u4ea7\u751f\u504f\u5dee\uff0c\u5bfc\u81f4\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u4e4b\u95f4\u7684\u516c\u5e73\u6027\u5dee\u8ddd\u3002", "method": "\u6211\u4eec\u63d0\u4f9b\u4e86\u5bf9\u8fd9\u4e2a\u9886\u57df\u4e2d\u4eba\u673a\u5bf9\u9f50\u548c\u516c\u5e73\u6027\u7684\u9996\u6b21\u7cfb\u7edf\u6027\u63a2\u7d22\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408\u4eba\u7c7b\u89c1\u89e3\u53ef\u4ee5\u6301\u7eed\u51cf\u5c11\u516c\u5e73\u6027\u5dee\u8ddd\u5e76\u63d0\u9ad8\u57df\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5c3d\u7ba1\u8fc7\u5ea6\u5bf9\u9f50\u53ef\u80fd\u4f1a\u5f15\u5165\u6027\u80fd\u6743\u8861\uff0c\u5f3a\u8c03\u9700\u8981\u6821\u51c6\u7b56\u7565\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u4eba\u673a\u5bf9\u9f50\u662f\u5f00\u53d1\u516c\u5e73\u3001\u7a33\u5065\u548c\u53ef\u6cdb\u5316\u7684\u533b\u7597AI\u7cfb\u7edf\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u4e13\u5bb6\u6307\u5bfc\u548c\u81ea\u52a8\u5316\u6548\u7387\u3002"}}
{"id": "2505.10446", "pdf": "https://arxiv.org/pdf/2505.10446", "abs": "https://arxiv.org/abs/2505.10446", "authors": ["Zemin Huang", "Zhiyang Chen", "Zijun Wang", "Tiancheng Li", "Guo-Jun Qi"], "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.", "AI": {"tldr": "DCoLT is a reasoning framework for diffusion language models that uses reinforcement learning to optimize the entire reasoning trajectory, leading to improved performance in math and code generation tasks.", "motivation": "The motivation is to introduce a reasoning framework for diffusion language models that allows bidirectional, non-linear reasoning without strict grammatical rules in intermediate steps.", "method": "DCoLT treats each intermediate step in the reverse diffusion process as a latent 'thinking' action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL).", "result": "Experiments on math and code generation tasks show that DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by significant percentages on various benchmarks.", "conclusion": "DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both, demonstrating the effectiveness of the proposed framework."}}
{"id": "2505.09698", "pdf": "https://arxiv.org/pdf/2505.09698", "abs": "https://arxiv.org/abs/2505.09698", "authors": ["Enyu Zhao", "Vedant Raval", "Hejia Zhang", "Jiageng Mao", "Zeyu Shangguan", "Stefanos Nikolaidis", "Yue Wang", "Daniel Seita"], "title": "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "47 pages, 29 figures. Under review", "summary": "Vision-Language Models (VLMs) have revolutionized artificial intelligence and\nrobotics due to their commonsense reasoning capabilities. In robotic\nmanipulation, VLMs are used primarily as high-level planners, but recent work\nhas also studied their lower-level reasoning ability, which refers to making\ndecisions about precise robot movements. However, the community currently lacks\na clear and common benchmark that can evaluate how well VLMs can aid low-level\nreasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,\nto evaluate the low-level robot manipulation reasoning capabilities of VLMs\nacross various dimensions, including how well they understand object-object\ninteractions and deformable object manipulation. We extensively test 33\nrepresentative VLMs across 10 model families on our benchmark, including\nvariants to test different model sizes. Our evaluation shows that the\nperformance of VLMs significantly varies across tasks, and there is a strong\ncorrelation between this performance and trends in our real-world manipulation\ntasks. It also shows that there remains a significant gap between these models\nand human-level understanding. See our website at:\nhttps://manipbench.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6ManipBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u7ea7\u673a\u5668\u4eba\u64cd\u4f5c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e14\u4e0e\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u7684\u8d8b\u52bf\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u7684\u7406\u89e3\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u660e\u786e\u4e14\u901a\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4f4e\u7ea7\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86ManipBench\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6ManipBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u7ef4\u5ea6\u4e0a\u7684\u4f4e\u7ea7\u673a\u5668\u4eba\u64cd\u4f5c\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u5bf9\u8c61-\u5bf9\u8c61\u4ea4\u4e92\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u3002\u5bf933\u4e2a\u4ee3\u8868\u6027\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u7684\u53d8\u4f53\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e14\u4e0e\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u7684\u8d8b\u52bf\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u540c\u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u7684\u7406\u89e3\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u7ea7\u673a\u5668\u4eba\u64cd\u4f5c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e14\u4e0e\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u7684\u8d8b\u52bf\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u7684\u7406\u89e3\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2505.10007", "pdf": "https://arxiv.org/pdf/2505.10007", "abs": "https://arxiv.org/abs/2505.10007", "authors": ["Zijun Chen", "Shengbo Wang", "Nian Si"], "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10238", "pdf": "https://arxiv.org/pdf/2505.10238", "abs": "https://arxiv.org/abs/2505.10238", "authors": ["Yanbo Ding"], "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MTVCrafter\u6846\u67b6\uff0c\u76f4\u63a5\u5efa\u6a21\u539f\u59cb\u76844D\u8fd0\u52a8\u5e8f\u5217\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u56fe\u50cf\u52a8\u753b\uff0c\u901a\u8fc74DMoT\u548cMV-DiT\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e2D\u6e32\u67d3\u7684\u59ff\u6001\u56fe\u50cf\u8fdb\u884c\u8fd0\u52a8\u6307\u5bfc\uff0c\u8fd9\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u4e22\u5f03\u4e86\u5f00\u653e\u4e16\u754c\u52a8\u753b\u4e2d\u5fc5\u8981\u76843D\u4fe1\u606f\u3002", "method": "\u63d0\u51faMTVCrafter\u6846\u67b6\uff0c\u901a\u8fc74DMoT\u5c063D\u8fd0\u52a8\u5e8f\u5217\u91cf\u5316\u4e3a4D\u8fd0\u52a8\u6807\u8bb0\uff0c\u5e76\u5f15\u5165MV-DiT\u6a21\u578b\uff0c\u5229\u75284D\u4f4d\u7f6e\u7f16\u7801\u8bbe\u8ba1\u72ec\u7279\u7684\u8fd0\u52a8\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "MTVCrafter\u5728FID-VID\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u7b2c\u4e8c\u540d65%\u3002\u5b83\u8fd8\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u5404\u79cd\u5f00\u653e\u4e16\u754c\u89d2\u8272\uff08\u5355\u4eba/\u591a\u4eba\uff0c\u5168\u8eab/\u534a\u8eab\uff09\u4ee5\u53ca\u4e0d\u540c\u98ce\u683c\u548c\u573a\u666f\u3002", "conclusion": "MTVCrafter\u6807\u5fd7\u7740\u8be5\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u4e3a\u57fa\u4e8e\u59ff\u6001\u7684\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMTVCrafter\u5728FID-VID\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u7b2c\u4e8c\u540d65%\u3002"}}
{"id": "2505.10493", "pdf": "https://arxiv.org/pdf/2505.10493", "abs": "https://arxiv.org/abs/2505.10493", "authors": ["Shaohan Wang", "Licheng Zhang", "Zheren Fu", "Zhendong Mao"], "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u8bad\u7ec3\u6846\u67b6CL-RAG\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u96be\u5ea6\u8bad\u7ec3\u6570\u636e\u5e76\u5206\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684RAG\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f18\u5316\u68c0\u7d22\u5668\u6216\u751f\u6210\u5668\uff0c\u4f46\u4e0d\u540c\u6587\u6863\u5bf9\u7528\u6237\u67e5\u8be2\u7684\u6709\u6548\u6027\u5dee\u5f02\u8f83\u5927\uff0c\u8fd9\u963b\u788d\u4e86\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u5728\u8bad\u7ec3\u4e2d\u7684\u9002\u5e94\u6027\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u5b66\u4e60\u7684\u542f\u53d1\uff0c\u8bfe\u7a0b\u5b66\u4e60\u901a\u8fc7\u4ece\u7b80\u5355\u5230\u56f0\u96be\u7684\u6837\u672c\u8bad\u7ec3\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u5c06\u5176\u6574\u5408\u5230RAG\u7cfb\u7edf\u7684\u8bad\u7ec3\u4e2d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u8bad\u7ec3\u6846\u67b6\uff0c\u79f0\u4e3aCL-RAG\u3002\u9996\u5148\u901a\u8fc7\u6837\u672c\u8fdb\u5316\u6784\u5efa\u5177\u6709\u591a\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u7136\u540e\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u5206\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "CL-RAG\u6846\u67b6\u5728\u56db\u4e2a\u5f00\u653e\u9886\u57df\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u5148\u8fdb\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u4e862%\u52304%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CL-RAG\u6846\u67b6\u5728\u56db\u4e2a\u5f00\u653e\u9886\u57df\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u5148\u8fdb\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u4e862%\u52304%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.10010", "pdf": "https://arxiv.org/pdf/2505.10010", "abs": "https://arxiv.org/abs/2505.10010", "authors": ["Jing-Cheng Pang", "Kaiyuan Li", "Yidi Wang", "Si-Hang Yang", "Shengyi Jiang", "Yang Yu"], "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts", "categories": ["cs.LG"], "comment": null, "summary": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86ImagineBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5229\u7528LLM\u751f\u6210\u7ecf\u9a8c\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u7b97\u6cd5\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8fd9\u4e9b\u751f\u6210\u7ecf\u9a8c\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u57fa\u51c6\uff0c\u8be5\u9886\u57df\u7684\u53d1\u5c55\u53d7\u5230\u963b\u788d\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86ImagineBench\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86ImagineBench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5229\u7528\u771f\u5b9e\u7ecf\u9a8c\u4e0eLLM\u751f\u6210\u7ecf\u9a8c\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\uff0c\u53d1\u73b0\u7b80\u5355\u5730\u5e94\u7528\u73b0\u6709\u7b97\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u8fbe\u523035.44%\u7684\u6210\u529f\u7387\uff0c\u800c\u4f7f\u7528\u771f\u5b9e\u7ecf\u9a8c\u8bad\u7ec3\u7684\u65b9\u6cd5\u5219\u8fbe\u5230\u4e8664.37%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u66f4\u597d\u5730\u5229\u7528LLM\u751f\u6210\u7684\u60f3\u8c61\u7ecf\u9a8c\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u673a\u4f1a\uff0c\u5305\u62ec\u66f4\u597d\u5730\u5229\u7528\u60f3\u8c61\u7ecf\u9a8c\u3001\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u6301\u7eed\u5b66\u4e60\uff0c\u4ee5\u53ca\u6269\u5c55\u5230\u591a\u6a21\u6001\u4efb\u52a1\u3002"}}
{"id": "2505.10250", "pdf": "https://arxiv.org/pdf/2505.10250", "abs": "https://arxiv.org/abs/2505.10250", "authors": ["Wenhao Shen", "Wanqi Yin", "Xiaofeng Yang", "Cheng Chen", "Chaoyue Song", "Zhongang Cai", "Lei Yang", "Hao Wang", "Guosheng Lin"], "title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR", "summary": "Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ADHMR\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684HMR\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u65b9\u5f0f\u5bf9\u9f50\u6a21\u578b\u3002\u901a\u8fc7\u8bad\u7ec3HMR-Scorer\u8bc4\u4f30\u6a21\u578b\u5e76\u521b\u5efa\u504f\u597d\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u5355\u56fe\u50cf\u4e2d\u4eba\u4f53\u7f51\u683c\u6062\u590d\uff08HMR\uff09\u7531\u4e8e\u6df1\u5ea6\u6a21\u7cca\u6027\u548c\u906e\u6321\u800c\u56fa\u6709\u7684\u4e0d\u9002\u5b9a\u95ee\u9898\u3002\u73b0\u6709\u7684\u6982\u7387\u65b9\u6cd5\u751f\u6210\u4e86\u8bb8\u591a\u53ef\u80fd\u76843D\u4eba\u4f53\u7f51\u683c\u9884\u6d4b\uff0c\u4f46\u5b83\u4eec\u5e38\u5e38\u4e0e2D\u56fe\u50cf\u89c2\u6d4b\u4e0d\u4e00\u81f4\uff0c\u5e76\u4e14\u5bf9\u91ce\u5916\u56fe\u50cf\u7684\u9c81\u68d2\u6027\u8f83\u5f31\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684HMR\u6a21\u578b\uff0c\u5728\u504f\u597d\u4f18\u5316\u65b9\u5f0f\u4e0b\u8fdb\u884c\u5bf9\u9f50\u3002\u9996\u5148\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6a21\u578bHMR-Scorer\uff0c\u80fd\u591f\u8bc4\u4f30\u9884\u6d4b\u7ed3\u679c\uff0c\u5373\u4f7f\u5bf9\u4e8e\u6ca1\u67093D\u6ce8\u91ca\u7684\u91ce\u5916\u56fe\u50cf\u4e5f\u662f\u5982\u6b64\u3002\u7136\u540e\u4f7f\u7528HMR-Scorer\u521b\u5efa\u4e00\u4e2a\u504f\u597d\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8f93\u5165\u56fe\u50cf\u90fd\u6709\u4e00\u4e2a\u83b7\u80dc\u548c\u5931\u8d25\u7684\u7f51\u683c\u9884\u6d4b\u5bf9\u3002\u8be5\u6570\u636e\u96c6\u7528\u4e8e\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u3002\u6b64\u5916\uff0cHMR-Scorer\u8fd8\u53ef\u4ee5\u901a\u8fc7\u6570\u636e\u6e05\u7406\u6765\u63d0\u9ad8\u73b0\u6709\u7684HMR\u6a21\u578b\uff0c\u5373\u4f7f\u8bad\u7ec3\u6837\u672c\u8f83\u5c11\u3002", "result": "ADHMR\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728https://github.com/shenwenhao01/ADHMR\u83b7\u5f97\u3002", "conclusion": "ADHMR\u5728\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10494", "pdf": "https://arxiv.org/pdf/2505.10494", "abs": "https://arxiv.org/abs/2505.10494", "authors": ["Yutao Mou", "Xiao Deng", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CoV-Eval\u591a\u4efb\u52a1\u57fa\u51c6\u548cVC-Judge\u5224\u65ad\u6a21\u578b\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30LLM\u4ee3\u7801\u5b89\u5168\uff0c\u5e76\u53d1\u73b0LLM\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u548c\u4fee\u590d\u6f0f\u6d1e\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7684\u4ee3\u7801\u5b89\u5168\u57fa\u51c6\u4ec5\u5173\u6ce8\u5355\u4e00\u7684\u8bc4\u4f30\u4efb\u52a1\u548c\u8303\u5f0f\uff0c\u5982\u4ee3\u7801\u8865\u5168\u548c\u751f\u6210\uff0c\u7f3a\u4e4f\u5728\u5b89\u5168\u4ee3\u7801\u751f\u6210\u3001\u6f0f\u6d1e\u4fee\u590d\u548c\u8bc6\u522b\u7b49\u7ef4\u5ea6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CoV-Eval\uff0c\u4e00\u4e2a\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\u7684\u591a\u4efb\u52a1\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLM\u4ee3\u7801\u5b89\u5168\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86VC-Judge\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u5224\u65ad\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u548c\u53ef\u9760\u5730\u5ba1\u67e5LLM\u751f\u6210\u7684\u7a0b\u5e8f\u4e2d\u7684\u6f0f\u6d1e\u3002", "result": "\u5c3d\u7ba1\u5927\u591a\u6570LLM\u80fd\u591f\u5f88\u597d\u5730\u8bc6\u522b\u6709\u6f0f\u6d1e\u7684\u4ee3\u7801\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u503e\u5411\u4e8e\u751f\u6210\u4e0d\u5b89\u5168\u7684\u4ee3\u7801\uff0c\u5e76\u4e14\u5728\u8bc6\u522b\u7279\u5b9a\u7c7b\u578b\u7684\u6f0f\u6d1e\u548c\u6267\u884c\u4fee\u590d\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u548c\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86LLM\u4ee3\u7801\u5b89\u5168\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u4f18\u5316\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2505.10037", "pdf": "https://arxiv.org/pdf/2505.10037", "abs": "https://arxiv.org/abs/2505.10037", "authors": ["Takafumi Ito", "Lysenko Artem", "Tatsuhiko Tsunoda"], "title": "Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "comment": "10 pages, 3 figures", "summary": "Quantum-classical Hybrid Machine Learning (QHML) models are recognized for\ntheir robust performance and high generalization ability even for relatively\nsmall datasets. These qualities offer unique advantages for anti-cancer drug\nresponse prediction, where the number of available samples is typically small.\nHowever, such hybrid models appear to be very sensitive to the data encoding\nused at the interface of a neural network and a quantum circuit, with\nsuboptimal choices leading to stability issues. To address this problem, we\npropose a novel strategy that uses a normalization function based on a\nmoderated gradient version of the $\\tanh$. This method transforms the outputs\nof the neural networks without concentrating them at the extreme value ranges.\nOur idea was evaluated on a dataset of gene expression and drug response\nmeasurements for various cancer cell lines, where we compared the prediction\nperformance of a classical deep learning model and several QHML models. These\nresults confirmed that QHML performed better than the classical models when\ndata was optimally normalized. This study opens up new possibilities for\nbiomedical data analysis using quantum computers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6297\u764c\u836f\u7269\u53cd\u5e94\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6297\u764c\u836f\u7269\u53cd\u5e94\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u7f16\u7801\u95ee\u9898\u5bb9\u6613\u51fa\u73b0\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u6b63\u68af\u5ea6\u7248\u672c\u7684$\tanh$\u7684\u5f52\u4e00\u5316\u51fd\u6570\u7684\u65b0\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u6570\u636e\u7f16\u7801\u654f\u611f\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u6570\u636e\u7ecf\u8fc7\u6700\u4f18\u5f52\u4e00\u5316\u65f6\uff0c\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u7269\u533b\u5b66\u6570\u636e\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.10257", "pdf": "https://arxiv.org/pdf/2505.10257", "abs": "https://arxiv.org/abs/2505.10257", "authors": ["Hao Lu", "Jiaqi Tang", "Jiyao Wang", "Yunfan LU", "Xu Cao", "Qingyong Hu", "Yin Wang", "Yuting Zhang", "Tianxin Xie", "Yunpeng Zhang", "Yong Chen", "Jiayu. Gao", "Bin Huang", "Dengbo He", "Shuiguang Deng", "Hao Chen", "Ying-Cong Chen"], "title": "Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot", "categories": ["cs.CV"], "comment": null, "summary": "The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAGE DeeR\uff0c\u4e00\u4e2a\u8d85\u7ea7\u5bf9\u9f50\u548c\u901a\u7528\u7684\u9a7e\u9a76\u4ee3\u7406\uff0c\u80fd\u591f\u6839\u636e\u4e0d\u540c\u7684\u7528\u6237\u504f\u597d\u548c\u504f\u5dee\u505a\u51fa\u4e0d\u540c\u7684\u53cd\u5e94\uff0c\u5e76\u80fd\u7406\u89e3\u591a\u89c6\u89d2\u548c\u591a\u6a21\u5f0f\u8f93\u5165\u4ee5\u63a8\u7406\u7528\u6237\u7684\u751f\u7406\u6307\u6807\u3001\u9762\u90e8\u60c5\u7eea\u3001\u624b\u90e8\u52a8\u4f5c\u3001\u8eab\u4f53\u52a8\u4f5c\u3001\u9a7e\u9a76\u573a\u666f\u548c\u884c\u4e3a\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf\u9a7e\u9a76\u4ee3\u7406\u7684\u611f\u77e5\u51b3\u7b56\u80fd\u529b\u548c\u8d85\u7ea7\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u667a\u80fd\u9a7e\u9a76\u8231\u9700\u8981\u5339\u914d\u4e0d\u540c\u7528\u6237\u7684\u8212\u9002\u5ea6\u3001\u4ea4\u4e92\u548c\u5b89\u5168\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6839\u636e\u7528\u6237\u504f\u597d\u548c\u504f\u5dee\u505a\u51fa\u4e0d\u540c\u53cd\u5e94\u7684\u9a7e\u9a76\u4ee3\u7406\u3002\u540c\u65f6\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u591a\u89c6\u89d2\u548c\u591a\u6a21\u5f0f\u8f93\u5165\u4ee5\u53ca\u9690\u5f0f\u601d\u7ef4\u94fe\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SAGE DeeR\uff0c\u4e00\u4e2a\u8d85\u7ea7\u5bf9\u9f50\u548c\u901a\u7528\u7684\u9a7e\u9a76\u4ee3\u7406\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u548c\u591a\u6a21\u5f0f\u8f93\u5165\u6765\u63a8\u7406\u7528\u6237\u7684\u751f\u7406\u6307\u6807\u3001\u9762\u90e8\u60c5\u7eea\u3001\u624b\u90e8\u52a8\u4f5c\u3001\u8eab\u4f53\u52a8\u4f5c\u3001\u9a7e\u9a76\u573a\u666f\u548c\u884c\u4e3a\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u7a7a\u95f4\u4e2d\u7684\u9690\u5f0f\u601d\u7ef4\u94fe\u6765\u63d0\u9ad8\u5176\u901a\u7528\u6027\u548c\u8d85\u7ea7\u5bf9\u9f50\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86SAGE DeeR\uff0c\u4e00\u4e2a\u8d85\u7ea7\u5bf9\u9f50\u548c\u901a\u7528\u7684\u9a7e\u9a76\u4ee3\u7406\uff0c\u80fd\u591f\u6839\u636e\u4e0d\u540c\u7684\u7528\u6237\u504f\u597d\u548c\u504f\u5dee\u505a\u51fa\u4e0d\u540c\u7684\u53cd\u5e94\uff0c\u5e76\u80fd\u7406\u89e3\u591a\u89c6\u89d2\u548c\u591a\u6a21\u5f0f\u8f93\u5165\u4ee5\u63a8\u7406\u7528\u6237\u7684\u751f\u7406\u6307\u6807\u3001\u9762\u90e8\u60c5\u7eea\u3001\u624b\u90e8\u52a8\u4f5c\u3001\u8eab\u4f53\u52a8\u4f5c\u3001\u9a7e\u9a76\u573a\u666f\u548c\u884c\u4e3a\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf\u9a7e\u9a76\u4ee3\u7406\u7684\u611f\u77e5\u51b3\u7b56\u80fd\u529b\u548c\u8d85\u7ea7\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86SAGE DeeR\uff0c\u4e00\u4e2a\u8d85\u7ea7\u5bf9\u9f50\u548c\u901a\u7528\u7684\u9a7e\u9a76\u4ee3\u7406\uff0c\u80fd\u591f\u6839\u636e\u4e0d\u540c\u7684\u7528\u6237\u504f\u597d\u548c\u504f\u5dee\u505a\u51fa\u4e0d\u540c\u7684\u53cd\u5e94\uff0c\u5e76\u80fd\u7406\u89e3\u591a\u89c6\u89d2\u548c\u591a\u6a21\u5f0f\u8f93\u5165\u4ee5\u63a8\u7406\u7528\u6237\u7684\u751f\u7406\u6307\u6807\u3001\u9762\u90e8\u60c5\u7eea\u3001\u624b\u90e8\u52a8\u4f5c\u3001\u8eab\u4f53\u52a8\u4f5c\u3001\u9a7e\u9a76\u573a\u666f\u548c\u884c\u4e3a\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf\u9a7e\u9a76\u4ee3\u7406\u7684\u611f\u77e5\u51b3\u7b56\u80fd\u529b\u548c\u8d85\u7ea7\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507", "abs": "https://arxiv.org/abs/2505.10507", "authors": ["Benedikt Ebing", "Goran Glava\u0161"], "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8bcd\u5bf9\u9f50\u7684\u6807\u7b7e\u6295\u5f71\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\uff08XLT\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u540c\u65f6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86XLT\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u8bcd\u5bf9\u9f50\u7684\u65b9\u6cd5\u5728\u6807\u7b7e\u6295\u5f71\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u4f4e\u7ea7\u8bbe\u8ba1\u51b3\u7b56\u5728\u7ffb\u8bd1-based XLT\u4e2d\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7684\u4e00\u4e9b\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u58f0\u79f0\u5728XLT\u7684\u6807\u7b7e\u6295\u5f71\u4e2d\u4f18\u4e8e\u8bcd\u5bf9\u9f50\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u8bcd\u5bf9\u9f50\u65b9\u6cd5\u5e76\u63a2\u7d22\u5176\u4f18\u5316\u53ef\u80fd\u6027\u3002", "method": "\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u8bcd\u5bf9\u9f50\uff08WAs\uff09\u5728\u6807\u7b7e\u6295\u5f71\u4e2d\u7684\u5e94\u7528\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4f4e\u7ea7\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u57fa\u4e8e\u7ffb\u8bd1\u7684XLT\u7684\u5f71\u54cd\uff0c\u5305\u62ec\uff1a(i) \u5728\u591a\u8bcd\u8de8\u5ea6\u4e4b\u95f4\u6295\u5f71\u6807\u7b7e\u7684\u7b97\u6cd5\uff0c(ii) \u51cf\u5c11\u566a\u58f0\u6620\u5c04\u6807\u7b7e\u6570\u91cf\u7684\u8fc7\u6ee4\u7b56\u7565\uff0c\u4ee5\u53ca(iii) \u7ffb\u8bd1\u53e5\u5b50\u7684\u9884\u5206\u8bcd\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u878d\u5408\u4e86translate-train\u548ctranslate-test\u7684\u9884\u6d4b\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u8fd9\u4e9b\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u57fa\u4e8e\u7ffb\u8bd1\u7684XLT\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u901a\u8fc7\u4f18\u5316\u9009\u62e9\uff0c\u57fa\u4e8e\u8bcd\u5bf9\u9f50\u7684XLT\u6027\u80fd\u81f3\u5c11\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u6211\u4eec\u5f15\u5165\u7684\u65b0\u6295\u5f71\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6295\u5f71\uff0c\u5e76\u4e14\u51cf\u5c11\u4e86\u5bf9\u4f4e\u7ea7\u8bcd\u5bf9\u9f50\u8bbe\u8ba1\u9009\u62e9\u7684\u4f9d\u8d56\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u9009\u62e9\uff0c\u57fa\u4e8e\u8bcd\u5bf9\u9f50\u7684XLT\u53ef\u4ee5\u8fbe\u5230\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u878d\u5408\u4e86translate-train\u548ctranslate-test\u7684\u9884\u6d4b\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6295\u5f71\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u96c6\u6210\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u4f4e\u7ea7\u8bcd\u5bf9\u9f50\u8bbe\u8ba1\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86XLT\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.10039", "pdf": "https://arxiv.org/pdf/2505.10039", "abs": "https://arxiv.org/abs/2505.10039", "authors": ["Hang Chen", "Jiaying Zhu", "Xinyu Yang", "Wenya Wang"], "title": "Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Circuit discovery has gradually become one of the prominent methods for\nmechanistic interpretability, and research on circuit completeness has also\ngarnered increasing attention. Methods of circuit discovery that do not\nguarantee completeness not only result in circuits that are not fixed across\ndifferent runs but also cause key mechanisms to be omitted. The nature of\nincompleteness arises from the presence of OR gates within the circuit, which\nare often only partially detected in standard circuit discovery methods. To\nthis end, we systematically introduce three types of logic gates: AND, OR, and\nADDER gates, and decompose the circuit into combinations of these logical\ngates. Through the concept of these gates, we derive the minimum requirements\nnecessary to achieve faithfulness and completeness. Furthermore, we propose a\nframework that combines noising-based and denoising-based interventions, which\ncan be easily integrated into existing circuit discovery methods without\nsignificantly increasing computational complexity. This framework is capable of\nfully identifying the logic gates and distinguishing them within the circuit.\nIn addition to the extensive experimental validation of the framework's ability\nto restore the faithfulness, completeness, and sparsity of circuits, using this\nframework, we uncover fundamental properties of the three logic gates, such as\ntheir proportions and contributions to the output, and explore how they behave\namong the functionalities of language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u7535\u8def\u53d1\u73b0\u7684\u5b8c\u6574\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\u7684\u57fa\u672c\u5c5e\u6027\u53ca\u5176\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u7535\u8def\u5728\u4e0d\u540c\u8fd0\u884c\u4e2d\u4e0d\u4e00\u81f4\uff0c\u5e76\u4e14\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u673a\u5236\u3002OR \u95e8\u7684\u5b58\u5728\u662f\u4e0d\u5b8c\u6574\u6027\u7684\u6839\u6e90\uff0c\u901a\u5e38\u5728\u6807\u51c6\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u4e2d\u53ea\u80fd\u90e8\u5206\u68c0\u6d4b\u5230\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u5f15\u5165\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\uff1aAND\u3001OR \u548c ADDER \u95e8\uff0c\u5e76\u5c06\u7535\u8def\u5206\u89e3\u4e3a\u8fd9\u4e9b\u903b\u8f91\u95e8\u7684\u7ec4\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u566a\u58f0\u548c\u53bb\u566a\u5e72\u9884\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u4e2d\uff0c\u800c\u4e0d\u4f1a\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u6062\u590d\u7535\u8def\u7684\u5fe0\u5b9e\u6027\u3001\u5b8c\u6574\u6027\u548c\u7a00\u758f\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u5982\u5b83\u4eec\u7684\u6bd4\u4f8b\u548c\u5bf9\u8f93\u51fa\u7684\u8d21\u732e\uff0c\u5e76\u63a2\u7d22\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u6a21\u578b\u529f\u80fd\u4e2d\u7684\u884c\u4e3a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u566a\u58f0\u548c\u53bb\u566a\u5e72\u9884\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b8c\u5168\u8bc6\u522b\u903b\u8f91\u95e8\u5e76\u5728\u7535\u8def\u4e2d\u533a\u5206\u5b83\u4eec\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u6a21\u578b\u529f\u80fd\u4e2d\u7684\u884c\u4e3a\u3002"}}
{"id": "2505.10258", "pdf": "https://arxiv.org/pdf/2505.10258", "abs": "https://arxiv.org/abs/2505.10258", "authors": ["Michael Hubbertz", "Pascal Colling", "Qi Han", "Tobias Meisen"], "title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper was accepted at the CVPR WAD 2025 Workshop", "summary": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u6620\u5c04\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u9a7e\u9a76\u5458\u7684\u975e\u6b63\u5f0f\u8def\u7ebf\uff0c\u4f7f\u7528\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6784\u5efa\u5168\u9762\u7684\u5168\u5c40\u5730\u56fe\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u66f4\u65b0\u5e76\u4fdd\u6301\u4f20\u611f\u5668\u65e0\u5173\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u4f20\u8f93\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u907f\u514d\u624b\u52a8\u6807\u8bb0\u7684\u5927\u91cf\u5de5\u4f5c\uff0c\u51fa\u73b0\u4e86\u81ea\u52a8\u5316\u5730\u56fe\u521b\u5efa\u7684\u65b9\u6cd5\u3002\u6700\u8fd1\u7684\u8d8b\u52bf\u5df2\u4ece\u79bb\u7ebf\u6620\u5c04\u8f6c\u5411\u5728\u7ebf\u6620\u5c04\uff0c\u4ee5\u786e\u4fdd\u6240\u7528\u5730\u56fe\u7684\u53ef\u7528\u6027\u548c\u65f6\u6548\u6027\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u8fd1\u5e74\u6765\u6027\u80fd\u6709\u6240\u63d0\u9ad8\uff0c\u5728\u7ebf\u6620\u5c04\u4ecd\u7136\u9762\u4e34\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u4f20\u611f\u5668\u906e\u6321\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u6cdb\u5316\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u6620\u5c04\u65b9\u6cd5\uff0c\u5c06\u9a7e\u9a76\u5458\u4f7f\u7528\u7684\u975e\u6b63\u5f0f\u8def\u7ebf\uff08trail\uff09\u6574\u5408\u5230\u5730\u56fe\u521b\u5efa\u8fc7\u7a0b\u4e2d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4ece\u81ea\u8f66\u548c\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u805a\u5408\u8f68\u8ff9\u6570\u636e\uff0c\u4f7f\u7528\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6784\u5efa\u5168\u9762\u7684\u5168\u5c40\u5730\u56fe\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0e\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6620\u5c04\u65b9\u6cd5\u76f8\u6bd4\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u73af\u5883\u548c\u4f20\u611f\u5668\u914d\u7f6e\u7684\u6539\u8fdb\u6cdb\u5316\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2505.10518", "pdf": "https://arxiv.org/pdf/2505.10518", "abs": "https://arxiv.org/abs/2505.10518", "authors": ["Anastasios Gerontopoulos", "Spyros Gidaris", "Nikos Komodakis"], "title": "Multi-Token Prediction Needs Registers", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.", "AI": {"tldr": "MuToR is a new approach to multi-token prediction that is simple, effective, and compatible with existing language models. It has shown effectiveness across various tasks in both language and vision domains.", "motivation": "Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning.", "method": "MuToR is a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets.", "result": "MuToR introduces only a negligible number of additional parameters, requires no architectural changes, and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. It also naturally supports scalable prediction horizons.", "conclusion": "MuToR is effective and versatile for various use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains."}}
{"id": "2505.10040", "pdf": "https://arxiv.org/pdf/2505.10040", "abs": "https://arxiv.org/abs/2505.10040", "authors": ["Lei Song", "Jiaxing Li", "Shihan Guan", "Youyong Kong"], "title": "Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their\ncapacity to preserve previously acquired knowledge amid the assimilation of\nnovel information. Rehearsal-based techniques revisit historical examples,\nadopted as a principal strategy to alleviate this phenomenon. However, memory\nexplosion and privacy infringements impose significant constraints on their\nutility. Non-Exemplar methods circumvent the prior issues through Prototype\nReplay (PR), yet feature drift presents new challenges. In this paper, our\nempirical findings reveal that Prototype Contrastive Learning (PCL) exhibits\nless pronounced drift than conventional PR. Drawing upon PCL, we propose\nInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar\nContinual Graph Learning (NECGL). Exploiting graph structural information, we\nformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature\ndistributions towards high-impact nodes to augment the model's capacity for\nassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)\nsafeguards task memory by regularizing discontinuities in class relationships.\nMoreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,\nfostering greater inter-class discriminability. Evaluations on four node\nclassification benchmark datasets demonstrate that our method outperforms\nexisting state-of-the-art methods, achieving a better trade-off between\nplasticity and stability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u793a\u4f8b\u6301\u7eed\u56fe\u5b66\u4e60\u8303\u5f0fIPAL\uff0c\u901a\u8fc7\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff08PCL\uff09\u51cf\u5c11\u7279\u5f81\u6f02\u79fb\uff0c\u5e76\u7ed3\u5408\u62d3\u6251\u96c6\u6210\u9ad8\u65af\u539f\u578b\uff08TIGP\uff09\u548c\u5b9e\u4f8b-\u539f\u578b\u4eb2\u548c\u529b\u84b8\u998f\uff08IPAD\uff09\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u3002\u5728\u56db\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "Graph Neural Networks (GNN) \u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u8fd9\u4f1a\u524a\u5f31\u5b83\u4eec\u5728\u5438\u6536\u65b0\u4fe1\u606f\u65f6\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u57fa\u4e8e\u91cd\u653e\u7684\u6280\u672f\u901a\u8fc7\u56de\u987e\u5386\u53f2\u793a\u4f8b\u6765\u7f13\u89e3\u8fd9\u4e00\u73b0\u8c61\uff0c\u4f46\u8bb0\u5fc6\u7206\u70b8\u548c\u9690\u79c1\u4fb5\u72af\u9650\u5236\u4e86\u5176\u6548\u7528\u3002\u975e\u793a\u4f8b\u65b9\u6cd5\u901a\u8fc7\u539f\u578b\u91cd\u653e\uff08PR\uff09\u907f\u514d\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7279\u5f81\u6f02\u79fb\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Instance-Prototype Affinity Learning (IPAL)\uff0c\u8fd9\u662f\u4e00\u79cd\u975e\u793a\u4f8b\u6301\u7eed\u56fe\u5b66\u4e60\uff08NECGL\uff09\u7684\u65b0\u8303\u5f0f\u3002\u6211\u4eec\u5229\u7528\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u5236\u5b9a\u4e86Topology-Integrated Gaussian Prototypes (TIGP)\uff0c\u6307\u5bfc\u7279\u5f81\u5206\u5e03\u5411\u9ad8\u5f71\u54cd\u8282\u70b9\u53d1\u5c55\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5438\u6536\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\u3002Instance-Prototype Affinity Distillation (IPAD) \u901a\u8fc7\u89c4\u8303\u7c7b\u5173\u7cfb\u4e2d\u7684\u4e0d\u8fde\u7eed\u6027\u6765\u4fdd\u62a4\u4efb\u52a1\u8bb0\u5fc6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728PCL\u4e2d\u5d4c\u5165\u4e86\u4e00\u4e2aDecision Boundary Perception (DBP)\u673a\u5236\uff0c\u4fc3\u8fdb\u66f4\u5927\u7684\u7c7b\u95f4\u53ef\u533a\u5206\u6027\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff08PCL\uff09\u6bd4\u4f20\u7edf\u7684PR\u8868\u73b0\u51fa\u66f4\u5c0f\u7684\u6f02\u79fb\u3002\u57fa\u4e8ePCL\uff0c\u6211\u4eec\u63d0\u51fa\u4e86IPAL\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u975e\u793a\u4f8b\u6301\u7eed\u56fe\u5b66\u4e60\u8303\u5f0f\u3002TIGP\u5f15\u5bfc\u7279\u5f81\u5206\u5e03\u5411\u9ad8\u5f71\u54cd\u8282\u70b9\u53d1\u5c55\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5438\u6536\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\u3002IPAD\u901a\u8fc7\u89c4\u8303\u7c7b\u5173\u7cfb\u4e2d\u7684\u4e0d\u8fde\u7eed\u6027\u6765\u4fdd\u62a4\u4efb\u52a1\u8bb0\u5fc6\u3002DBP\u673a\u5236\u589e\u5f3a\u4e86\u7c7b\u95f4\u53ef\u533a\u5206\u6027\u3002\u5728\u56db\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56db\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2505.10267", "pdf": "https://arxiv.org/pdf/2505.10267", "abs": "https://arxiv.org/abs/2505.10267", "authors": ["Pavel Korotaev", "Petr Surovtsev", "Alexander Kapitanov", "Karina Kvanchiani", "Aleksandr Nagaev"], "title": "HandReader: Advanced Techniques for Efficient Fingerspelling Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/ai-forever/handreader", "summary": "Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HandReader\uff0c\u4e00\u79cd\u7528\u4e8e\u624b\u6307\u62fc\u5199\u8bc6\u522b\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5176\u6570\u636e\u96c6\u548c\u6a21\u578b\u662f\u516c\u5f00\u53ef\u7528\u7684\u3002", "motivation": "\u5c3d\u7ba1\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u5173\u6ce8\u4e8e\u5904\u7406\u89c6\u9891\u7684\u65f6\u95f4\u7ef4\u5ea6\uff0c\u4f46\u624b\u6307\u62fc\u5199\u8bc6\u522b\u7684\u51c6\u786e\u6027\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u624b\u6307\u62fc\u5199\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86HandReader\uff0c\u5305\u62ecHandReader_{RGB}\u3001HandReader_{KP}\u548cHandReader_RGB+KP\u4e09\u79cd\u67b6\u6784\u3002HandReader_{RGB}\u4f7f\u7528\u4e86\u65b0\u7684\u65f6\u95f4\u4f4d\u79fb\u81ea\u9002\u5e94\u6a21\u5757\uff08TSAM\uff09\u6765\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u89c6\u9891\u7684RGB\u7279\u5f81\u3002HandReader_{KP}\u57fa\u4e8e\u63d0\u51fa\u7684\u65f6\u7a7a\u59ff\u6001\u7f16\u7801\u5668\uff08TPE\uff09\uff0c\u5728\u5173\u952e\u70b9\u4e0a\u8fdb\u884c\u64cd\u4f5c\u3002HandReader_RGB+KP\u662f\u4e00\u79cd\u8054\u5408\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u4e86RGB\u548c\u5173\u952e\u70b9\u6a21\u6001\u3002", "result": "HandReader\u6a21\u578b\u5728\u829d\u52a0\u54e5FSWild\u548cChicagoFSWild+\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5728\u4fc4\u7f57\u65af\u624b\u6307\u62fc\u5199\u7684\u7b2c\u4e00\u5f00\u653e\u6570\u636e\u96c6Znaki\u4e0a\u8868\u73b0\u51fa\u8272\u3002Znaki\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u7684HandReader\u6a21\u578b\u662f\u516c\u5f00\u53ef\u7528\u7684\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86HandReader\uff0c\u5b83\u662f\u4e00\u7ec4\u4e09\u4e2a\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u624b\u6307\u62fc\u5199\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u5728\u829d\u52a0\u54e5FSWild\u548cChicagoFSWild+\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u4fc4\u7f57\u65af\u624b\u6307\u62fc\u5199\u7684\u7b2c\u4e00\u5f00\u653e\u6570\u636e\u96c6Znaki\u4e0a\u8868\u73b0\u51fa\u8272\u3002Znaki\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u7684HandReader\u6a21\u578b\u662f\u516c\u5f00\u53ef\u7528\u7684\u3002"}}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527", "abs": "https://arxiv.org/abs/2505.10527", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "title": "WorldPM: Scaling Human Preference Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "AI": {"tldr": "This paper explores scaling laws in preference modeling and introduces WorldPM, which improves generalization performance across human preference datasets and shows significant gains when integrated into RLHF pipelines.", "motivation": "The paper is motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes. It aims to explore similar scaling laws in preference modeling.", "method": "The paper proposes World Preference Modeling (WorldPM) to emphasize the scaling potential of preference modeling. It collects preference data from public forums and conducts extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters.", "result": "The paper observes distinct patterns across different evaluation metrics: adversarial metrics scale up with increased training data and base model size, objective metrics show emergent behavior in larger models, and subjective metrics do not demonstrate scaling trends. WorldPM improves generalization performance across human preference datasets and shows significant improvements when integrated into RLHF pipelines.", "conclusion": "WorldPM is effective for preference fine-tuning and can significantly improve the generalization performance across human preference datasets of varying sizes. Integrating WorldPM into RLHF pipelines leads to notable improvements in evaluations."}}
{"id": "2505.10050", "pdf": "https://arxiv.org/pdf/2505.10050", "abs": "https://arxiv.org/abs/2505.10050", "authors": ["Fahad Almalki", "Mehedi Masud"], "title": "Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional machine learning models often prioritize predictive accuracy,\noften at the expense of model transparency and interpretability. The lack of\ntransparency makes it difficult for organizations to comply with regulatory\nrequirements and gain stakeholders trust. In this research, we propose a fraud\ndetection framework that combines a stacking ensemble of well-known gradient\nboosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable\nartificial intelligence (XAI) techniques are used to enhance the transparency\nand interpretability of the model's decisions. We used SHAP (SHapley Additive\nExplanations) for feature selection to identify the most important features.\nFurther efforts were made to explain the model's predictions using Local\nInterpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots\n(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection\ndataset, which includes more than 590,000 real transaction records, was used to\nevaluate the proposed model. The model achieved a high performance with an\naccuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent\nrelated approaches. These results indicate that combining high prediction\naccuracy with transparent interpretability is possible and could lead to a more\nethical and trustworthy solution in financial fraud detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u6b3a\u8bc8\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f80\u5f80\u4f18\u5148\u8003\u8651\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u800c\u727a\u7272\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u7f3a\u4e4f\u900f\u660e\u5ea6\u4f7f\u5f97\u7ec4\u7ec7\u96be\u4ee5\u9075\u5b88\u76d1\u7ba1\u8981\u6c42\u5e76\u83b7\u5f97\u5229\u76ca\u76f8\u5173\u8005\u7684\u4fe1\u4efb\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u4f17\u6240\u5468\u77e5\u7684\u68af\u5ea6\u63d0\u5347\u6a21\u578b\uff08XGBoost\u3001LightGBM \u548c CatBoost\uff09\u7684\u5806\u53e0\u96c6\u6210\u7684\u6b3a\u8bc8\u68c0\u6d4b\u6846\u67b6\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u4e86\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\u6765\u589e\u5f3a\u6a21\u578b\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002SHAP\uff08SHapley Additive Explanations\uff09\u7528\u4e8e\u7279\u5f81\u9009\u62e9\u4ee5\u786e\u5b9a\u6700\u91cd\u8981\u7684\u7279\u5f81\u3002\u8fdb\u4e00\u6b65\u7684\u52aa\u529b\u662f\u4f7f\u7528 Local Interpretable Model-Agnostic Explanation (LIME)\u3001Partial Dependence Plots (PDP) \u548c Permutation Feature Importance (PFI) \u6765\u89e3\u91ca\u6a21\u578b\u7684\u9884\u6d4b\u3002", "result": "\u8be5\u6a21\u578b\u5728 IEEE-CIS \u6b3a\u8bc8\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 99% \u7684\u51c6\u786e\u7387\u548c 0.99 \u7684 AUC-ROC \u5f97\u5206\uff0c\u4f18\u4e8e\u51e0\u79cd\u6700\u8fd1\u7684\u76f8\u5173\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u900f\u660e\u7684\u53ef\u89e3\u91ca\u6027\u76f8\u7ed3\u5408\u662f\u53ef\u80fd\u7684\uff0c\u5e76\u4e14\u53ef\u80fd\u5728\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u5e26\u6765\u66f4\u9053\u5fb7\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10281", "pdf": "https://arxiv.org/pdf/2505.10281", "abs": "https://arxiv.org/abs/2505.10281", "authors": ["Mengqiu Xu", "Kaixin Chen", "Heng Guo", "Yixiang Huang", "Ming Wu", "Zhenwei Shi", "Chuang Zhang", "Jun Guo"], "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MFogHub\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u533a\u57df\u548c\u591a\u536b\u661f\u7684\u6d77\u6d0b\u96fe\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc768,000\u4e2a\u9ad8\u5206\u8fa8\u7387\u6837\u672c\uff0c\u7528\u4e8e\u6539\u8fdb\u6d77\u6d0b\u96fe\u68c0\u6d4b\u548c\u9884\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u533a\u57df\u6216\u536b\u661f\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u80fd\u529b\uff0c\u5e76\u963b\u788d\u4e86\u5bf9\u6d77\u6d0b\u96fe\u5185\u5728\u7279\u6027\u7684\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86MFogHub\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u591a\u533a\u57df\u548c\u591a\u536b\u661f\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u6765\u81ea15\u4e2a\u6cbf\u6d77\u96fe\u6613\u53d1\u533a\u57df\u548c\u516d\u9897\u9759\u6b62\u536b\u661f\u7684\u6ce8\u91ca\u6d77\u6d0b\u96fe\u89c2\u6d4b\u6570\u636e\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMFogHub\u53ef\u4ee5\u63ed\u793a\u7531\u4e8e\u533a\u57df\u548c\u536b\u661f\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u6ce2\u52a8\uff0c\u540c\u65f6\u4f5c\u4e3a\u5f00\u53d1\u9488\u5bf9\u6027\u548c\u53ef\u6269\u5c55\u7684\u96fe\u9884\u6d4b\u6280\u672f\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "\u901a\u8fc7MFogHub\uff0c\u6211\u4eec\u65e8\u5728\u5728\u5168\u7403\u8303\u56f4\u5185\u63a8\u8fdb\u6d77\u6d0b\u96fe\u52a8\u6001\u7684\u5b9e\u9645\u76d1\u6d4b\u548c\u79d1\u5b66\u7406\u89e3\u3002"}}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554", "abs": "https://arxiv.org/abs/2505.10554", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5bf9\u9f50\u6a21\u578b\u5143\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u63d0\u793a\u548c\u5076\u7136\u7684'\u987f\u609f\u65f6\u523b'\uff0c\u4f46\u8fd9\u4e9b\u884c\u4e3a\u7684\u65f6\u95f4\u548c\u4e00\u81f4\u6027\u4e0d\u53ef\u9884\u6d4b\uff0c\u9650\u5236\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u7ba1\u9053\uff1a\u4e2a\u4f53\u5bf9\u9f50\u3001\u53c2\u6570\u7a7a\u95f4\u5408\u5e76\u548c\u9886\u57df\u7279\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u663e\u5f0f\u5bf9\u9f50\u6a21\u578b\u7684\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u7c7b\u6bd4\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u6307\u4ee4\u8c03\u4f18\u57fa\u7ebf\u63d0\u9ad8\u4e86\u8d85\u8fc710%\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u9886\u57df\u7279\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e862%\u7684\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5bf9\u9f50\u6a21\u578b\u7684\u5143\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.10057", "pdf": "https://arxiv.org/pdf/2505.10057", "abs": "https://arxiv.org/abs/2505.10057", "authors": ["Tiancong Cheng", "Ying Zhang", "Yuxuan Liang", "Roger Zimmermann", "Zhiwen Yu", "Bin Guo"], "title": "JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation", "categories": ["cs.LG"], "comment": null, "summary": "Depth estimation and scene segmentation are two important tasks in\nintelligent transportation systems. A joint modeling of these two tasks will\nreduce the requirement for both the storage and training efforts. This work\nexplores how the multi-task distillation could be used to improve such unified\nmodeling. While existing solutions transfer multiple teachers' knowledge in a\nstatic way, we propose a self-adaptive distillation method that can dynamically\nadjust the knowledge amount from each teacher according to the student's\ncurrent learning ability. Furthermore, as multiple teachers exist, the\nstudent's gradient update direction in the distillation is more prone to be\nerroneous where knowledge forgetting may occur. To avoid this, we propose a\nknowledge trajectory to record the most essential information that a model has\nlearnt in the past, based on which a trajectory-based distillation loss is\ndesigned to guide the student to follow the learning curve similarly in a\ncost-effective way. We evaluate our method on multiple benchmarking datasets\nincluding Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,\nour method achieves a clearly improvement. The code is provided in the\nsupplementary materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6df1\u5ea6\u4f30\u8ba1\u548c\u573a\u666f\u5206\u5272\u7684\u8054\u5408\u5efa\u6a21\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u77e5\u8bc6\u91cf\u548c\u907f\u514d\u77e5\u8bc6\u9057\u5fd8\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6df1\u5ea6\u4f30\u8ba1\u548c\u573a\u666f\u5206\u5272\u662f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u8054\u5408\u5efa\u6a21\u53ef\u4ee5\u51cf\u5c11\u5b58\u50a8\u548c\u8bad\u7ec3\u9700\u6c42\u3002\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u9759\u6001\u65b9\u5f0f\u4f20\u9012\u591a\u4e2a\u6559\u5e08\u7684\u77e5\u8bc6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u907f\u514d\u77e5\u8bc6\u9057\u5fd8\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u84b8\u998f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6839\u636e\u5b66\u751f\u5f53\u524d\u7684\u5b66\u4e60\u80fd\u529b\u52a8\u6001\u8c03\u6574\u4ece\u6bcf\u4e2a\u6559\u5e08\u83b7\u53d6\u7684\u77e5\u8bc6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u7684\u84b8\u998f\u635f\u5931\u6765\u907f\u514d\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u6211\u4eec\u5728Cityscapes\u548cNYU-v2\u7b49\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u660e\u663e\u6539\u8fdb\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u660e\u663e\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.10289", "pdf": "https://arxiv.org/pdf/2505.10289", "abs": "https://arxiv.org/abs/2505.10289", "authors": ["Yue Wang", "Shuai Xu", "Xuelin Zhu", "Yicong Li"], "title": "MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\uff08MSCI\uff09\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5229\u7528CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e2d\u95f4\u5c42\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u805a\u5408\u5668\u6765\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0cMSCI\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4e8eCLIP\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u5c40\u9650\u6027\u6e90\u4e8e\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u5c42\u6b21\u8de8\u6a21\u6001\u4ea4\u4e92\uff08MSCI\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6709\u6548\u5730\u63a2\u7d22\u5e76\u5229\u7528\u4e86CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e2d\u95f4\u5c42\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u81ea\u9002\u5e94\u805a\u5408\u5668\uff0c\u5206\u522b\u4ece\u4f4e\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\u4e2d\u63d0\u53d6\u5c40\u90e8\u4fe1\u606f\uff0c\u5e76\u4ece\u9ad8\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\u4e2d\u6574\u5408\u5168\u5c40\u4fe1\u606f\u3002\u8fd9\u4e9b\u5173\u952e\u4fe1\u606f\u901a\u8fc7\u9010\u9636\u6bb5\u7684\u4ea4\u4e92\u673a\u5236\u9010\u6b65\u878d\u5165\u6587\u672c\u8868\u793a\u4e2d\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u7ec6\u7c92\u5ea6\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\u3002\u6b64\u5916\uff0cMSCI\u6839\u636e\u4e0d\u540c\u7684\u7ec4\u5408\u4ee5\u53ca\u540c\u4e00\u7ec4\u5408\u4e2d\u7684\u4e0d\u540c\u5143\u7d20\u52a8\u6001\u8c03\u6574\u5168\u5c40\u548c\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u4f7f\u5176\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5404\u79cd\u573a\u666f\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u6570\u636e\u548c\u4ee3\u7801\u53ef\u5728https://github.com/ltpwy/MSCI\u83b7\u53d6\u3002"}}
{"id": "2505.09665", "pdf": "https://arxiv.org/pdf/2505.09665", "abs": "https://arxiv.org/abs/2505.09665", "authors": ["Sulong Zhou", "Qunying Huang", "Shaoheng Zhou", "Yun Hang", "Xinyue Ye", "Aodong Mei", "Kathryn Phung", "Yuning Ye", "Uma Govindswamy", "Zehan Li"], "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent\nyears. Understanding how affected populations perceive and respond during\nwildfire crises is critical for timely and empathetic disaster response. Social\nmedia platforms offer a crowd-sourced channel to capture evolving public\ndiscourse, providing hyperlocal information and insight into public sentiment.\nThis study analyzes Reddit discourse during the 2025 Los Angeles wildfires,\nspanning from the onset of the disaster to full containment. We collect 385\nposts and 114,879 comments related to the Palisades and Eaton fires. We adopt\ntopic modeling methods to identify the latent topics, enhanced by large\nlanguage models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we\ndevelop a hierarchical framework to categorize latent topics, consisting of two\nmain categories, Situational Awareness (SA) and Crisis Narratives (CN). The\nvolume of SA category closely aligns with real-world fire progressions, peaking\nwithin the first 2-5 days as the fires reach the maximum extent. The most\nfrequent co-occurring category set of public health and safety, loss and\ndamage, and emergency resources expands on a wide range of health-related\nlatent topics, including environmental health, occupational health, and one\nhealth. Grief signals and mental health risks consistently accounted for 60\npercentage and 40 percentage of CN instances, respectively, with the highest\ntotal volume occurring at night. This study contributes the first annotated\nsocial media dataset on the 2025 LA fires, and introduces a scalable\nmulti-layer framework that leverages topic modeling for crisis discourse\nanalysis. By identifying persistent public health concerns, our results can\ninform more empathetic and adaptive strategies for disaster response, public\nhealth communication, and future research in comparable climate-related\ndisaster events.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e862025\u5e74\u6d1b\u6749\u77f6\u91ce\u706b\u671f\u95f4Reddit\u4e0a\u7684\u8ba8\u8bba\uff0c\u4f7f\u7528\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u4f18\u5316\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\u6765\u5206\u7c7b\u6f5c\u5728\u4e3b\u9898\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u60c5\u5883\u610f\u8bc6\u4e0e\u706b\u707e\u8fdb\u5c55\u5bc6\u5207\u76f8\u5173\uff0c\u800c\u5371\u673a\u53d9\u4e8b\u5219\u6d89\u53ca\u5927\u91cf\u5fc3\u7406\u5065\u5eb7\u548c\u60b2\u4f24\u4fe1\u53f7\u3002", "motivation": "\u4e86\u89e3\u53d7\u5f71\u54cd\u4eba\u53e3\u5728\u91ce\u706b\u5371\u673a\u4e2d\u5982\u4f55\u611f\u77e5\u548c\u54cd\u5e94\u5bf9\u4e8e\u53ca\u65f6\u548c\u5bcc\u6709\u540c\u7406\u5fc3\u7684\u707e\u5bb3\u54cd\u5e94\u81f3\u5173\u91cd\u8981\u3002\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f17\u5305\u6e20\u9053\u6765\u6355\u6349\u4e0d\u65ad\u53d8\u5316\u7684\u516c\u4f17\u8ba8\u8bba\uff0c\u63d0\u4f9b\u4e86\u8d85\u672c\u5730\u4fe1\u606f\u548c\u516c\u4f17\u60c5\u7eea\u7684\u89c1\u89e3\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4eba\u5de5\u5728\u73af\uff08HITL\uff09\u4f18\u5316\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\u6765\u5bf9\u6f5c\u5728\u4e3b\u9898\u8fdb\u884c\u5206\u7c7b\uff0c\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u7c7b\u522b\uff1a\u60c5\u5883\u610f\u8bc6\uff08SA\uff09\u548c\u5371\u673a\u53d9\u4e8b\uff08CN\uff09\u3002", "result": "SA\u7c7b\u522b\u7684\u6570\u91cf\u4e0e\u73b0\u5b9e\u4e2d\u7684\u706b\u707e\u8fdb\u5c55\u7d27\u5bc6\u4e00\u81f4\uff0c\u5728\u706b\u707e\u8fbe\u5230\u6700\u5927\u8303\u56f4\u7684\u524d2-5\u5929\u8fbe\u5230\u5cf0\u503c\u3002\u6700\u5e38\u89c1\u7684\u5171\u540c\u51fa\u73b0\u7c7b\u522b\u662f\u516c\u5171\u536b\u751f\u548c\u5b89\u5168\u3001\u635f\u5931\u548c\u635f\u5bb3\u4ee5\u53ca\u5e94\u6025\u8d44\u6e90\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u5065\u5eb7\u76f8\u5173\u6f5c\u5728\u4e3b\u9898\uff0c\u5305\u62ec\u73af\u5883\u5065\u5eb7\u3001\u804c\u4e1a\u5065\u5eb7\u548c\u4e00\u4e2a\u5065\u5eb7\u3002\u60b2\u4f24\u4fe1\u53f7\u548c\u5fc3\u7406\u5065\u5eb7\u98ce\u9669\u5206\u522b\u5360CN\u5b9e\u4f8b\u768460%\u548c40%\uff0c\u6700\u9ad8\u603b\u6570\u91cf\u51fa\u73b0\u5728\u591c\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u8d21\u732e\u4e86\u7b2c\u4e00\u4e2a\u5173\u4e8e2025\u5e74\u6d1b\u6749\u77f6\u706b\u707e\u7684\u6ce8\u91ca\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u5c42\u6846\u67b6\uff0c\u5229\u7528\u4e3b\u9898\u5efa\u6a21\u8fdb\u884c\u5371\u673a\u8bdd\u8bed\u5206\u6790\u3002\u901a\u8fc7\u8bc6\u522b\u6301\u7eed\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u53ef\u4ee5\u4e3a\u66f4\u5bcc\u6709\u540c\u7406\u5fc3\u548c\u9002\u5e94\u6027\u7684\u707e\u5bb3\u5e94\u5bf9\u3001\u516c\u5171\u536b\u751f\u4f20\u64ad\u548c\u672a\u6765\u7c7b\u4f3c\u6c14\u5019\u76f8\u5173\u707e\u5bb3\u4e8b\u4ef6\u7684\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2505.09747", "pdf": "https://arxiv.org/pdf/2505.09747", "abs": "https://arxiv.org/abs/2505.09747", "authors": ["Benjamin Paa\u00dfen", "Suzana Alpsancar", "Tobias Matzner", "Ingrid Scharlau"], "title": "Healthy Distrust in AI systems", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Under the slogan of trustworthy AI, much of contemporary AI research is\nfocused on designing AI systems and usage practices that inspire human trust\nand, thus, enhance adoption of AI systems. However, a person affected by an AI\nsystem may not be convinced by AI system design alone -- neither should they,\nif the AI system is embedded in a social context that gives good reason to\nbelieve that it is used in tension with a person's interest. In such cases,\ndistrust in the system may be justified and necessary to build meaningful trust\nin the first place. We propose the term \"healthy distrust\" to describe such a\njustified, careful stance towards certain AI usage practices. We investigate\nprior notions of trust and distrust in computer science, sociology, history,\npsychology, and philosophy, outline a remaining gap that healthy distrust might\nfill and conceptualize healthy distrust as a crucial part for AI usage that\nrespects human autonomy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u53ef\u4fe1AI\u7684\u53e3\u53f7\u4e0b\uff0c\u5f53\u4ee3AI\u7814\u7a76\u5982\u4f55\u5173\u6ce8\u8bbe\u8ba1\u80fd\u591f\u6fc0\u53d1\u4eba\u7c7b\u4fe1\u4efb\u5e76\u4fc3\u8fdbAI\u7cfb\u7edf\u91c7\u7528\u7684AI\u7cfb\u7edf\u548c\u4f7f\u7528\u5b9e\u8df5\u3002\u7136\u800c\uff0c\u5f53AI\u7cfb\u7edf\u5d4c\u5165\u5230\u4e00\u4e2a\u53ef\u80fd\u4e0e\u4e2a\u4eba\u5229\u76ca\u76f8\u51b2\u7a81\u7684\u793e\u4f1a\u80cc\u666f\u4e2d\u65f6\uff0c\u4ec5\u9760AI\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u80fd\u65e0\u6cd5\u8bf4\u670d\u53d7\u5f71\u54cd\u7684\u4eba\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5bf9\u7cfb\u7edf\u7684\u4e0d\u4fe1\u4efb\u53ef\u80fd\u662f\u5408\u7406\u4e14\u5fc5\u8981\u7684\uff0c\u4ee5\u5efa\u7acb\u6709\u610f\u4e49\u7684\u4fe1\u4efb\u3002\u672c\u6587\u63d0\u51fa\u4e86\u201c\u5065\u5eb7\u7684\u4e0d\u4fe1\u4efb\u201d\u8fd9\u4e00\u6982\u5ff5\uff0c\u63cf\u8ff0\u4e86\u5bf9\u67d0\u4e9bAI\u4f7f\u7528\u5b9e\u8df5\u7684\u5408\u7406\u3001\u8c28\u614e\u7684\u6001\u5ea6\uff0c\u5e76\u63a2\u8ba8\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u793e\u4f1a\u5b66\u3001\u5386\u53f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u54f2\u5b66\u4e2d\u5173\u4e8e\u4fe1\u4efb\u548c\u4e0d\u4fe1\u4efb\u7684\u5148\u9a8c\u6982\u5ff5\uff0c\u6307\u51fa\u4e86\u5065\u5eb7\u4e0d\u4fe1\u4efb\u53ef\u80fd\u586b\u8865\u7684\u7a7a\u767d\uff0c\u5e76\u5c06\u5065\u5eb7\u4e0d\u4fe1\u4efb\u6982\u5ff5\u5316\u4e3a\u5c0a\u91cd\u4eba\u7c7b\u81ea\u4e3b\u6027\u7684AI\u4f7f\u7528\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u63a2\u8ba8\u5728AI\u7cfb\u7edf\u5d4c\u5165\u5230\u53ef\u80fd\u4e0e\u4e2a\u4eba\u5229\u76ca\u76f8\u51b2\u7a81\u7684\u793e\u4f1a\u80cc\u666f\u4e2d\u65f6\uff0c\u4e3a\u4ec0\u4e48\u4ec5\u9760AI\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u80fd\u65e0\u6cd5\u8bf4\u670d\u53d7\u5f71\u54cd\u7684\u4eba\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u5bf9\u7cfb\u7edf\u7684\u4e0d\u4fe1\u4efb\u53ef\u80fd\u662f\u5408\u7406\u4e14\u5fc5\u8981\u7684\u3002", "method": "\u672c\u6587\u901a\u8fc7\u63a2\u8ba8\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u793e\u4f1a\u5b66\u3001\u5386\u53f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u54f2\u5b66\u4e2d\u5173\u4e8e\u4fe1\u4efb\u548c\u4e0d\u4fe1\u4efb\u7684\u5148\u9a8c\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u5065\u5eb7\u4e0d\u4fe1\u4efb\u53ef\u80fd\u586b\u8865\u7684\u7a7a\u767d\uff0c\u5e76\u5c06\u5176\u6982\u5ff5\u5316\u4e3a\u5c0a\u91cd\u4eba\u7c7b\u81ea\u4e3b\u6027\u7684AI\u4f7f\u7528\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u5065\u5eb7\u7684\u4e0d\u4fe1\u4efb\u201d\u8fd9\u4e00\u6982\u5ff5\uff0c\u63cf\u8ff0\u4e86\u5bf9\u67d0\u4e9bAI\u4f7f\u7528\u5b9e\u8df5\u7684\u5408\u7406\u3001\u8c28\u614e\u7684\u6001\u5ea6\uff0c\u5e76\u6307\u51fa\u5065\u5eb7\u4e0d\u4fe1\u4efb\u662f\u5c0a\u91cd\u4eba\u7c7b\u81ea\u4e3b\u6027\u7684AI\u4f7f\u7528\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u5728AI\u7cfb\u7edf\u5d4c\u5165\u5230\u53ef\u80fd\u4e0e\u4e2a\u4eba\u5229\u76ca\u76f8\u51b2\u7a81\u7684\u793e\u4f1a\u80cc\u666f\u4e2d\u65f6\uff0c\u5bf9\u7cfb\u7edf\u7684\u4e0d\u4fe1\u4efb\u53ef\u80fd\u662f\u5408\u7406\u4e14\u5fc5\u8981\u7684\uff0c\u4ee5\u5efa\u7acb\u6709\u610f\u4e49\u7684\u4fe1\u4efb\u3002\u5065\u5eb7\u4e0d\u4fe1\u4efb\u662f\u5c0a\u91cd\u4eba\u7c7b\u81ea\u4e3b\u6027\u7684AI\u4f7f\u7528\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2505.10083", "pdf": "https://arxiv.org/pdf/2505.10083", "abs": "https://arxiv.org/abs/2505.10083", "authors": ["Chengsen Wang", "Qi Qi", "Zhongwen Rao", "Lujia Pan", "Jingyu Wang", "Jianxin Liao"], "title": "ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6ChronoSteer\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u6709\u6548\u5229\u7528\u65f6\u95f4\u548c\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u672a\u6765\u63a8\u65ad\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5229\u7528\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u7684\u80fd\u529b\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5206\u522b\u5728\u6587\u672c\u63a8\u7406\u548c\u65f6\u95f4\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u3002\u6574\u5408\u4e24\u8005\u7684\u4f18\u52bf\u4ee5\u6784\u5efa\u4e00\u4e2a\u540c\u65f6\u5229\u7528\u65f6\u95f4\u548c\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u672a\u6765\u63a8\u65ad\u7684\u591a\u6a21\u6001\u6a21\u578b\u5df2\u6210\u4e3a\u5173\u952e\u7684\u7814\u7a76\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u6587\u672c\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u4fee\u8ba2\u6307\u4ee4\uff0c\u7136\u540e\u7528\u4e8e\u5f15\u5bfc\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u7684\u8f93\u51fa\u3002\u5f15\u5165\u4e86ChronoSteer\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u4fee\u8ba2\u6307\u4ee4\u8fdb\u884c\u5f15\u5bfc\u7684\u591a\u6a21\u6001TSFM\u3002\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u3002", "result": "ChronoSteer\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u76f8\u6bd4\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e8625.7%\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u9ad8\u4e8622.5%\u3002", "conclusion": "ChronoSteer\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u76f8\u6bd4\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e8625.7%\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u9ad8\u4e8622.5%\u3002"}}
{"id": "2505.10292", "pdf": "https://arxiv.org/pdf/2505.10292", "abs": "https://arxiv.org/abs/2505.10292", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation", "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "31 pages, 14 figures", "summary": "Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86StoryReasoning\u6570\u636e\u96c6\uff0c\u5305\u542b4,178\u4e2a\u6545\u4e8b\uff0c\u8fd9\u4e9b\u6545\u4e8b\u6765\u6e90\u4e8e52,016\u5f20\u7535\u5f71\u56fe\u50cf\uff0c\u5e76\u4e14\u5177\u6709\u7ed3\u6784\u5316\u7684\u573a\u666f\u5206\u6790\u548c\u5b9a\u4f4d\u7684\u6545\u4e8b\u3002\u901a\u8fc7\u8de8\u5e27\u5bf9\u8c61\u91cd\u65b0\u8bc6\u522b\u3001\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u5730\u57fa\u65b9\u6848\uff0c\u6539\u8fdb\u4e86\u89c6\u89c9\u53d9\u4e8b\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u53d9\u4e8b\u7cfb\u7edf\u96be\u4ee5\u5728\u5e27\u4e4b\u95f4\u4fdd\u6301\u89d2\u8272\u8eab\u4efd\u5e76\u5c06\u5176\u52a8\u4f5c\u94fe\u63a5\u5230\u9002\u5f53\u7684\u4e3b\u9898\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u53c2\u8003\u6027\u5e7b\u89c9\u3002\u8fd9\u4e9b\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u5bf9\u89d2\u8272\u3001\u7269\u4f53\u548c\u5176\u4ed6\u5b9e\u4f53\u8fdb\u884c\u53ef\u89c6\u5316\u5143\u7d20\u7684\u5b9a\u4f4d\u6765\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5e27\u5bf9\u8c61\u91cd\u65b0\u8bc6\u522b\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u4eba\u8138\u8bc6\u522b\uff1b\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7528\u4e8e\u663e\u5f0f\u53d9\u4e8b\u5efa\u6a21\uff1b\u4ee5\u53ca\u4e00\u79cd\u5c06\u6587\u672c\u5143\u7d20\u4e0e\u591a\u5e27\u4e2d\u7684\u89c6\u89c9\u5b9e\u4f53\u5173\u8054\u7684\u5730\u57fa\u65b9\u6848\u3002", "result": "\u5efa\u7acb\u4e86\u57fa\u51c6\u6027\u80fd\uff0c\u901a\u8fc7\u5fae\u8c03Qwen2.5-VL 7B\uff0c\u521b\u5efa\u4e86Qwen Storyteller\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u5bf9\u8c61\u5f15\u7528\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6267\u884c\u7aef\u5230\u7aef\u7684\u5bf9\u8c61\u68c0\u6d4b\u3001\u91cd\u65b0\u8bc6\u522b\u548c\u5730\u6807\u68c0\u6d4b\uff0c\u5e76\u4e14\u5728\u5e73\u5747\u6bcf\u4e2a\u6545\u4e8b\u7684\u5e7b\u89c9\u6570\u91cf\u4e0a\u51cf\u5c11\u4e8612.3%\u3002", "conclusion": "\u901a\u8fc7\u5fae\u8c03Qwen2.5-VL 7B\uff0c\u521b\u5efa\u4e86Qwen Storyteller\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u5bf9\u8c61\u5f15\u7528\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6267\u884c\u7aef\u5230\u7aef\u7684\u5bf9\u8c61\u68c0\u6d4b\u3001\u91cd\u65b0\u8bc6\u522b\u548c\u5730\u6807\u68c0\u6d4b\uff0c\u5e76\u4e14\u5728\u5e73\u5747\u6bcf\u4e2a\u6545\u4e8b\u7684\u5e7b\u89c9\u6570\u91cf\u4e0a\u51cf\u5c11\u4e8612.3%\u3002"}}
{"id": "2505.09777", "pdf": "https://arxiv.org/pdf/2505.09777", "abs": "https://arxiv.org/abs/2505.09777", "authors": ["Alejo Lopez-Avila", "Jinhua Du"], "title": "A Survey on Large Language Models in Multimodal Recommender Systems", "categories": ["cs.IR", "cs.CL"], "comment": "30 pages, 6 figures", "summary": "Multimodal recommender systems (MRS) integrate heterogeneous user and item\ndata, such as text, images, and structured information, to enhance\nrecommendation performance. The emergence of large language models (LLMs)\nintroduces new opportunities for MRS by enabling semantic reasoning, in-context\nlearning, and dynamic input handling. Compared to earlier pre-trained language\nmodels (PLMs), LLMs offer greater flexibility and generalisation capabilities\nbut also introduce challenges related to scalability and model accessibility.\nThis survey presents a comprehensive review of recent work at the intersection\nof LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and\ndata adaptation techniques. We propose a novel taxonomy to characterise\nintegration patterns, identify transferable techniques from related\nrecommendation domains, provide an overview of evaluation metrics and datasets,\nand point to possible future directions. We aim to clarify the emerging role of\nLLMs in multimodal recommendation and support future research in this rapidly\nevolving field.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\uff08MRS\uff09\u4ea4\u53c9\u9886\u57df\u7684\u6700\u65b0\u5de5\u4f5c\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u91cd\u70b9\u662f\u63d0\u793a\u7b56\u7565\u3001\u5fae\u8c03\u65b9\u6cd5\u548c\u6570\u636e\u9002\u5e94\u6280\u672f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\u6765\u8868\u5f81\u96c6\u6210\u6a21\u5f0f\uff0c\u8bc6\u522b\u6765\u81ea\u76f8\u5173\u63a8\u8350\u9886\u57df\u7684\u53ef\u8f6c\u79fb\u6280\u672f\uff0c\u6982\u8ff0\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u5e76\u6307\u51fa\u53ef\u80fd\u7684\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\uff08MRS\uff09\u6574\u5408\u5f02\u6784\u7528\u6237\u548c\u9879\u76ee\u6570\u636e\uff0c\u5982\u6587\u672c\u3001\u56fe\u50cf\u548c\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4ee5\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u542f\u7528\u8bed\u4e49\u63a8\u7406\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u52a8\u6001\u8f93\u5165\u5904\u7406\uff0c\u4e3aMRS\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\u3002\u4e0e\u65e9\u671f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u76f8\u6bd4\uff0cLLMs\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6a21\u578b\u53ef\u8bbf\u95ee\u6027\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u5bf9LLMs\u548cMRS\u4ea4\u53c9\u9886\u57df\u7684\u6700\u65b0\u5de5\u4f5c\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u91cd\u70b9\u662f\u63d0\u793a\u7b56\u7565\u3001\u5fae\u8c03\u65b9\u6cd5\u548c\u6570\u636e\u9002\u5e94\u6280\u672f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\u6765\u8868\u5f81\u96c6\u6210\u6a21\u5f0f\uff0c\u8bc6\u522b\u6765\u81ea\u76f8\u5173\u63a8\u8350\u9886\u57df\u7684\u53ef\u8f6c\u79fb\u6280\u672f\uff0c\u6982\u8ff0\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u5e76\u6307\u51fa\u53ef\u80fd\u7684\u672a\u6765\u65b9\u5411\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\u6765\u8868\u5f81\u96c6\u6210\u6a21\u5f0f\uff0c\u8bc6\u522b\u6765\u81ea\u76f8\u5173\u63a8\u8350\u9886\u57df\u7684\u53ef\u8f6c\u79fb\u6280\u672f\uff0c\u6982\u8ff0\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u5e76\u6307\u51fa\u53ef\u80fd\u7684\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u9610\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6a21\u6001\u63a8\u8350\u4e2d\u7684\u65b0\u5174\u4f5c\u7528\uff0c\u5e76\u652f\u6301\u8be5\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.09757", "pdf": "https://arxiv.org/pdf/2505.09757", "abs": "https://arxiv.org/abs/2505.09757", "authors": ["Botao Amber Hu", "Yuhan Liu", "Helena Rong"], "title": "Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "Submitted to CSCW 2026", "summary": "The recent trend of self-sovereign Decentralized AI Agents (DeAgents)\ncombines Large Language Model (LLM)-based AI agents with decentralization\ntechnologies such as blockchain smart contracts and trusted execution\nenvironments (TEEs). These tamper-resistant trustless substrates allow agents\nto achieve self-sovereignty through ownership of cryptowallet private keys and\ncontrol of digital assets and social media accounts. DeAgent eliminates\ncentralized control and reduces human intervention, addressing key trust\nconcerns inherent in centralized AI systems. However, given ongoing challenges\nin LLM reliability such as hallucinations, this creates paradoxical tension\nbetween trustlessness and unreliable autonomy. This study addresses this\nempirical research gap through interviews with DeAgents stakeholders-experts,\nfounders, and developers-to examine their motivations, benefits, and governance\ndilemmas. The findings will guide future DeAgents system and protocol design\nand inform discussions about governance in sociotechnical AI systems in the\nfuture agentic web.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53bb\u4e2d\u5fc3\u5316AI\u4ee3\u7406\uff08DeAgent\uff09\u7684\u52a8\u673a\u3001\u597d\u5904\u548c\u6cbb\u7406\u56f0\u5883\uff0c\u4ee5\u89e3\u51b3\u4fe1\u4efb\u548c\u4e0d\u53ef\u9760\u81ea\u4e3b\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3DeAgent\u5728\u4fe1\u4efb\u548c\u4e0d\u53ef\u9760\u81ea\u4e3b\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4ee5\u53ca\u5728\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u5173\u952e\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08DeAgent\u7684\u5229\u76ca\u76f8\u5173\u8005\uff08\u4e13\u5bb6\u3001\u521b\u59cb\u4eba\u548c\u5f00\u53d1\u8005\uff09\u6765\u63a2\u8ba8\u4ed6\u4eec\u7684\u52a8\u673a\u3001\u597d\u5904\u548c\u6cbb\u7406\u56f0\u5883\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86DeAgent\u5229\u76ca\u76f8\u5173\u8005\u7684\u52a8\u673a\u3001\u597d\u5904\u548c\u6cbb\u7406\u56f0\u5883\uff0c\u4e3a\u672a\u6765DeAgent\u7cfb\u7edf\u548c\u534f\u8bae\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08DeAgent\u7684\u5229\u76ca\u76f8\u5173\u8005\uff0c\u63ed\u793a\u4e86\u4ed6\u4eec\u7684\u52a8\u673a\u3001\u597d\u5904\u548c\u6cbb\u7406\u56f0\u5883\uff0c\u4e3a\u672a\u6765DeAgent\u7cfb\u7edf\u548c\u534f\u8bae\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u4e3a\u672a\u6765\u793e\u4f1a\u6280\u672fAI\u7cfb\u7edf\u7684\u6cbb\u7406\u8ba8\u8bba\u63d0\u4f9b\u4e86\u4fe1\u606f\u3002"}}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117", "abs": "https://arxiv.org/abs/2505.10117", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfei Wang", "Jun Luo"], "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.", "AI": {"tldr": "This paper introduces MiCo, a hierarchical language agent framework that uses LLMs to solve the Online Dynamic Multidimensional Bin Packing problem in cloud services. MiCo outperforms existing methods by achieving a high competitive ratio and maintaining performance under varying conditions.", "motivation": "Traditional optimization methods struggle with real-time changes, heuristic approaches have rigid strategies, and learning-based methods lack generalizability and interpretability. The paper aims to address these limitations by introducing a novel framework for solving ODMBP in cloud services.", "method": "The paper proposes a hierarchical language agent framework named MiCo, which formulates ODMBP as a Semi-Markov Decision Process with Options (SMDP-Option) and employs a two-stage architecture: Option Miner and Option Composer. Option Miner discovers non-context-aware strategies using LLMs, while Option Composer integrates these strategies with contextual ones.", "result": "MiCo achieves a 96.9% competitive ratio in large-scale scenarios involving over 10,000 virtual machines. It maintains high performance under nonstationary request flows and diverse configurations, validating its effectiveness in complex cloud environments.", "conclusion": "MiCo demonstrates effectiveness in complex and large-scale cloud environments by achieving a high competitive ratio and maintaining performance under nonstationary request flows and diverse configurations."}}
{"id": "2505.10294", "pdf": "https://arxiv.org/pdf/2505.10294", "abs": "https://arxiv.org/abs/2505.10294", "authors": ["Guillaume Balezo", "Roger Trullo", "Albert Pla Planas", "Etienne Decenciere", "Thomas Walter"], "title": "MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models", "categories": ["cs.CV", "q-bio.TO", "68T07 (Primary), 92C55 (Secondary)", "I.4.9; I.2.10; I.5.4; J.3"], "comment": null, "summary": "Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.", "AI": {"tldr": "MIPHEI is a model that predicts multiplex immunofluorescence signals from H&E images, achieving high accuracy in cell-type classification and offering a promising approach for analyzing large-scale H&E datasets.", "motivation": "To bridge the gap between histopathological analysis using H&E staining and the more precise but less accessible multiplex immunofluorescence (mIF) technique.", "method": "MIPHEI (Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H&E images.", "result": "MIPHEI achieves accurate cell-type classification from H&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers.", "conclusion": "MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H&E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes."}}
{"id": "2505.09766", "pdf": "https://arxiv.org/pdf/2505.09766", "abs": "https://arxiv.org/abs/2505.09766", "authors": ["Roberto Ponciroli"], "title": "On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion", "categories": ["math.NA", "cs.AI", "cs.NA"], "comment": null, "summary": "This work presents a methodology for reconstructing the spatial distribution\nof the neutron flux in a nuclear reactor, leveraging real-time measurements\nobtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation\ninherently defines the problem of estimating a scalar field within a domain\nbased on boundary data, making it a natural mathematical framework for this\ntask. The main challenge lies in deriving the Green's function specific to the\ndomain and the neutron diffusion process. While analytical solutions for\nGreen's functions exist for simplified geometries, their derivation of complex,\nheterogeneous domains-such as a nuclear reactor-requires a numerical approach.\nThe objective of this work is to demonstrate the well-posedness of the\ndata-driven Green's function approximation by formulating and solving the K-H\nequation as an inverse problem. After establishing the symmetry properties that\nthe Green's function must satisfy, the K-H equation is derived from the\none-speed neutron diffusion model. This is followed by a comprehensive\ndescription of the procedure for interpreting sensor readings and implementing\nthe neutron flux reconstruction algorithm. Finally, the existence and\nuniqueness of the Green's function inferred from the sampled data are\ndemonstrated, ensuring the reliability of the proposed method and its\npredictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eK-H\u65b9\u7a0b\u548c\u6570\u636e\u9a71\u52a8\u683c\u6797\u51fd\u6570\u7684\u4e2d\u5b50\u901a\u91cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6838\u53cd\u5e94\u5806\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u6790\u89e3\u4ec5\u9002\u7528\u4e8e\u7b80\u5355\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u6838\u53cd\u5e94\u5806\u7b49\u590d\u6742\u5f02\u8d28\u57df\u9700\u8981\u4e00\u79cd\u6570\u503c\u65b9\u6cd5\u6765\u63a8\u5bfc\u683c\u6797\u51fd\u6570\uff0c\u4ee5\u5b9e\u73b0\u4e2d\u5b50\u901a\u91cf\u7684\u51c6\u786e\u91cd\u5efa\u3002", "method": "\u672c\u6587\u5229\u7528Kirchhoff-Helmholtz\uff08K-H\uff09\u65b9\u7a0b\uff0c\u5c06\u4e2d\u5b50\u901a\u91cf\u7684\u91cd\u5efa\u95ee\u9898\u4f5c\u4e3a\u53cd\u95ee\u9898\u8fdb\u884c\u6c42\u89e3\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u65b9\u6cd5\u63a8\u5bfc\u51fa\u9002\u7528\u4e8e\u590d\u6742\u5f02\u8d28\u57df\u7684\u683c\u6797\u51fd\u6570\u3002", "result": "\u672c\u6587\u6210\u529f\u8bc1\u660e\u4e86\u4ece\u91c7\u6837\u6570\u636e\u4e2d\u63a8\u5bfc\u51fa\u7684\u683c\u6797\u51fd\u6570\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\uff0c\u786e\u4fdd\u4e86\u6240\u63d0\u65b9\u6cd5\u53ca\u5176\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u5730\u91cd\u5efa\u6838\u53cd\u5e94\u5806\u4e2d\u7684\u4e2d\u5b50\u901a\u91cf\u7a7a\u95f4\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u683c\u6797\u51fd\u6570\u8fd1\u4f3c\u8bc1\u660e\u4e86\u5176\u5408\u7406\u6027\u3002"}}
{"id": "2505.10120", "pdf": "https://arxiv.org/pdf/2505.10120", "abs": "https://arxiv.org/abs/2505.10120", "authors": ["Guillaume Godin"], "title": "All You Need Is Synthetic Task Augmentation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 3 Figures, 6 tables", "summary": "Injecting rule-based models like Random Forests into differentiable neural\nnetwork frameworks remains an open challenge in machine learning. Recent\nadvancements have demonstrated that pretrained models can generate efficient\nmolecular embeddings. However, these approaches often require extensive\npretraining and additional techniques, such as incorporating posterior\nprobabilities, to boost performance. In our study, we propose a novel strategy\nthat jointly trains a single Graph Transformer neural network on both sparse\nmultitask molecular property experimental targets and synthetic targets derived\nfrom XGBoost models trained on Osmordred molecular descriptors. These synthetic\ntasks serve as independent auxiliary tasks. Our results show consistent and\nsignificant performance improvement across all 19 molecular property prediction\ntasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms\nthe XGBoost single-task learner. This demonstrates that synthetic task\naugmentation is an effective method for enhancing neural model performance in\nmultitask molecular property prediction without the need for feature injection\nor pretraining.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u56fe\u53d8\u6362\u5668\u795e\u7ecf\u7f51\u7edc\u4e0a\u540c\u65f6\u8bad\u7ec3\u7a00\u758f\u591a\u4efb\u52a1\u5206\u5b50\u5c5e\u6027\u5b9e\u9a8c\u76ee\u6807\u548c\u4eceXGBoost\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u76ee\u6807\uff0c\u4ee5\u63d0\u9ad8\u591a\u4efb\u52a1\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4e2d\u795e\u7ecf\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u5408\u6210\u4efb\u52a1\u589e\u5f3a\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u65e0\u9700\u7279\u5f81\u6ce8\u5165\u6216\u9884\u8bad\u7ec3\u3002", "motivation": "\u5c06\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u5982\u968f\u673a\u68ee\u6797\u6ce8\u5165\u53ef\u5fae\u5206\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u4ecd\u7136\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u5f00\u653e\u6027\u6311\u6218\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u8fdb\u5c55\u8868\u660e\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u9ad8\u6548\u7684\u5206\u5b50\u5d4c\u5165\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u9884\u8bad\u7ec3\u548c\u989d\u5916\u7684\u6280\u672f\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u5c06\u5355\u4e2a\u56fe\u53d8\u6362\u5668\u795e\u7ecf\u7f51\u7edc\u540c\u65f6\u5728\u7a00\u758f\u591a\u4efb\u52a1\u5206\u5b50\u5c5e\u6027\u5b9e\u9a8c\u76ee\u6807\u548c\u4eceXGBoost\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u76ee\u6807\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u5408\u6210\u4efb\u52a1\u4f5c\u4e3a\u72ec\u7acb\u7684\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6240\u670919\u4e2a\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u90fd\u6709\u6301\u7eed\u4e14\u663e\u8457\u7684\u63d0\u5347\u3002\u572819\u4e2a\u76ee\u6807\u4e2d\u768416\u4e2a\u4e2d\uff0c\u591a\u4efb\u52a1\u56fe\u53d8\u6362\u5668\u4f18\u4e8eXGBoost\u5355\u4efb\u52a1\u5b66\u4e60\u8005\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5c55\u793a\u4e86\u5408\u6210\u4efb\u52a1\u589e\u5f3a\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u7279\u5f81\u6ce8\u5165\u6216\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u591a\u4efb\u52a1\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4e2d\u795e\u7ecf\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10351", "pdf": "https://arxiv.org/pdf/2505.10351", "abs": "https://arxiv.org/abs/2505.10351", "authors": ["Jie Zhu", "Jirong Zha", "Ding Li", "Leye Wang"], "title": "A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability", "categories": ["cs.CV"], "comment": "An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders", "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6210\u5458\u63a8\u7406\u65b9\u6cd5PartCrop\uff0c\u7528\u4e8e\u653b\u51fb\u81ea\u76d1\u7763\u89c6\u89c9\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u9632\u5fa1\u65b9\u6cd5\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5229\u7528\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u9690\u79c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u9886\u57df\u3002\u5728\u5b9e\u9645\u4e2d\uff0c\u653b\u51fb\u8005\u901a\u5e38\u9762\u5bf9\u7684\u662f\u9ed1\u76d2\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u73b0\u5b9e\u7684\u6210\u5458\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6210\u5458\u63a8\u7406\u65b9\u6cd5PartCrop\uff0c\u901a\u8fc7\u88c1\u526a\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u90e8\u5206\u6765\u67e5\u8be2\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u54cd\u5e94\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u7ed3\u6784\u6539\u8fdb\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684PartCrop-v2\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u653b\u51fb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u9a8c\u8bc1\u4e86PartCrop\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8bc4\u4f30\u4e86\u4e24\u79cd\u5e38\u89c1\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u65b9\u6cd5\u3002", "conclusion": "PartCrop\u548c\u5176\u6539\u8fdb\u7248\u672cPartCrop-v2\u5728\u4e0d\u540c\u8bad\u7ec3\u534f\u8bae\u548c\u7ed3\u6784\u7684\u81ea\u76d1\u7763\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u7684\u6210\u5458\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u4e5f\u6709\u6548\u3002"}}
{"id": "2505.10125", "pdf": "https://arxiv.org/pdf/2505.10125", "abs": "https://arxiv.org/abs/2505.10125", "authors": ["Wujun Zhou", "Shu Ding", "ZeLin Li", "Wei Wang"], "title": "Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning enables the clients to collaboratively train a global\nmodel, which is aggregated from local models. Due to the heterogeneous data\ndistributions over clients and data privacy in federated learning, it is\ndifficult to train local models to achieve a well-performed global model. In\nthis paper, we introduce the adaptability of local models, i.e., the average\nperformance of local models on data distributions over clients, and enhance the\nperformance of the global model by improving the adaptability of local models.\nSince each client does not know the data distributions over other clients, the\nadaptability of the local model cannot be directly optimized. First, we provide\nthe property of an appropriate local model which has good adaptability on the\ndata distributions over clients. Then, we formalize the property into the local\ntraining objective with a constraint and propose a feasible solution to train\nthe local model. Extensive experiments on federated learning benchmarks\ndemonstrate that our method significantly improves the adaptability of local\nmodels and achieves a well-performed global model that consistently outperforms\nthe baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u9ad8\u8054\u90a6\u5b66\u4e60\u4e2d\u672c\u5730\u6a21\u578b\u9002\u5e94\u6027\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5ba2\u6237\u7aef\u4e0a\u7684\u6570\u636e\u5206\u5e03\u5f02\u6784\u6027\u548c\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9690\u79c1\uff0c\u5f88\u96be\u8bad\u7ec3\u672c\u5730\u6a21\u578b\u4ee5\u5b9e\u73b0\u8868\u73b0\u826f\u597d\u7684\u5168\u5c40\u6a21\u578b\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5e76\u901a\u8fc7\u63d0\u9ad8\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\u6765\u589e\u5f3a\u5168\u5c40\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u7684\u672c\u5730\u6a21\u578b\u7684\u5c5e\u6027\uff0c\u5e76\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u5e26\u6709\u7ea6\u675f\u7684\u672c\u5730\u8bad\u7ec3\u76ee\u6807\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u6765\u8bad\u7ec3\u672c\u5730\u6a21\u578b\u3002", "result": "\u5728\u8054\u90a6\u5b66\u4e60\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u8868\u73b0\u826f\u597d\u7684\u5168\u5c40\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u672c\u5730\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u8868\u73b0\u826f\u597d\u7684\u5168\u5c40\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2505.10352", "pdf": "https://arxiv.org/pdf/2505.10352", "abs": "https://arxiv.org/abs/2505.10352", "authors": ["Shihao Zou", "Qingfeng Li", "Wei Ji", "Jingjing Li", "Yongkui Yang", "Guoqi Li", "Chao Dong"], "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\\mathcal{O}(T)$ Complexity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SpikeVideoFormer\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u8109\u51b2\u7684\u89c6\u9891Transformer\uff0c\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u8109\u51b2\u7684Hamming\u6ce8\u610f\u529b\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eSNN\u7684Transformer\u4e3b\u8981\u5173\u6ce8\u5355\u56fe\u50cf\u4efb\u52a1\uff0c\u800c\u6ca1\u6709\u5145\u5206\u5229\u7528SNN\u5728\u89c6\u9891\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u8109\u51b2\u7684\u89c6\u9891Transformer\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528SNN\u7684\u4f18\u52bf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u7684Hamming\u6ce8\u610f\u529b\uff08SDHA\uff09\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5206\u6790\u4e86\u5404\u79cd\u57fa\u4e8e\u8109\u51b2\u7684\u7a7a\u95f4-\u65f6\u95f4\u6ce8\u610f\u529b\u8bbe\u8ba1\uff0c\u4ee5\u627e\u5230\u4e00\u79cd\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u4fdd\u6301\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u6700\u4f18\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684SNN\u65b9\u6cd5\u548c\u6700\u8fd1\u7684ANN\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86SpikeVideoFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u8109\u51b2\u7684\u89c6\u9891Transformer\uff0c\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u89c6\u9891\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684SNN\u65b9\u6cd5\u548c\u6700\u8fd1\u7684ANN\u65b9\u6cd5\u3002"}}
{"id": "2505.09921", "pdf": "https://arxiv.org/pdf/2505.09921", "abs": "https://arxiv.org/abs/2505.09921", "authors": ["Yidan Wang", "Yanan Cao", "Yubing Ren", "Fang Fang", "Zheng Lin", "Binxing Fang"], "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Jailbreak\u653b\u51fb\u5728\u63d0\u53d6\u654f\u611f\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86PIG\u6846\u67b6\u6765\u9488\u5bf9PII\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePIG\u5728\u9690\u79c1\u6cc4\u9732\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30LLM\u9690\u79c1\u6cc4\u9732\u7684\u65b9\u6cd5\u5bb9\u6613\u88ab\u5bf9\u9f50\u826f\u597d\u7684\u6a21\u578b\u963b\u6b62\uff0c\u800cJailbreak\u653b\u51fb\u5728\u9690\u79c1\u573a\u666f\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "PIG\u6846\u67b6\u901a\u8fc7\u8bc6\u522bPII\u5b9e\u4f53\u53ca\u5176\u7c7b\u578b\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u6784\u5efa\u9690\u79c1\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u7b56\u7565\u8fed\u4ee3\u66f4\u65b0\u4ee5\u5f15\u51fa\u76ee\u6807PII\u3002", "result": "PIG\u5728\u4e24\u4e2a\u4e0e\u9690\u79c1\u76f8\u5173\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u56db\u4e2a\u767d\u76d2\u548c\u4e24\u4e2a\u9ed1\u76d2LLM\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aPIG\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM\u4e2d\u5b58\u5728\u663e\u8457\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5f3a\u7684\u4fdd\u62a4\u63aa\u65bd\u3002"}}
{"id": "2505.09796", "pdf": "https://arxiv.org/pdf/2505.09796", "abs": "https://arxiv.org/abs/2505.09796", "authors": ["Skylar S. Gay", "Tucker Netherton", "Barbara Marquez", "Raymond Mumme", "Mary Gronberg", "Brent Parker", "Chelsea Pinnix", "Sanjay Shete", "Carlos Cardenas", "Laurence Court"], "title": "Virtual Dosimetrists: A Radiotherapy Training \"Flight Simulator\"", "categories": ["physics.med-ph", "cs.AI"], "comment": null, "summary": "Effective education in radiotherapy plan quality review requires a robust,\nregularly updated set of examples and the flexibility to demonstrate multiple\npossible planning approaches and their consequences. However, the current\nclinic-based paradigm does not support these needs. To address this, we have\ndeveloped 'Virtual Dosimetrist' models that can both generate training examples\nof suboptimal treatment plans and then allow trainees to improve the plan\nquality through simple natural language prompts, as if communicating with a\ndosimetrist. The dose generation and modification process is accurate, rapid,\nand requires only modest resources. This work is the first to combine dose\ndistribution prediction with natural language processing; providing a robust\npipeline for both generating suboptimal training plans and allowing trainees to\npractice their critical plan review and improvement skills that addresses the\nchallenges of the current clinic-based paradigm.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86'\u865a\u62df\u5242\u91cf\u5e08'\u6a21\u578b\uff0c\u7ed3\u5408\u5242\u91cf\u5206\u5e03\u9884\u6d4b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u4ee5\u751f\u6210\u6b21\u4f18\u8bad\u7ec3\u8ba1\u5212\u5e76\u5141\u8bb8\u5b66\u5458\u8fdb\u884c\u8ba1\u5212\u6539\u8fdb\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u57fa\u4e8e\u4e34\u5e8a\u7684\u8303\u5f0f\u65e0\u6cd5\u6ee1\u8db3\u7684\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4e34\u5e8a\u7684\u8303\u5f0f\u65e0\u6cd5\u6ee1\u8db3\u6709\u6548\u6559\u80b2\u5728\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u8d28\u91cf\u5ba1\u67e5\u4e2d\u7684\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u548c\u66f4\u65b0\u9891\u7e41\u7684\u793a\u4f8b\u5e93\u3002", "method": "\u5f00\u53d1\u4e86'\u865a\u62df\u5242\u91cf\u5e08'\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u5242\u91cf\u5206\u5e03\u9884\u6d4b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u4ee5\u751f\u6210\u6b21\u4f18\u8bad\u7ec3\u8ba1\u5212\u5e76\u5141\u8bb8\u5b66\u5458\u8fdb\u884c\u8ba1\u5212\u6539\u8fdb\u3002", "result": "\u8be5\u5de5\u4f5c\u9996\u6b21\u7ed3\u5408\u4e86\u5242\u91cf\u5206\u5e03\u9884\u6d4b\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u6b21\u4f18\u8bad\u7ec3\u8ba1\u5212\uff0c\u5e76\u8ba9\u5b66\u5458\u7ec3\u4e60\u4ed6\u4eec\u7684\u5173\u952e\u8ba1\u5212\u5ba1\u67e5\u548c\u6539\u8fdb\u6280\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86'\u865a\u62df\u5242\u91cf\u5e08'\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u6b21\u4f18\u6cbb\u7597\u8ba1\u5212\u7684\u8bad\u7ec3\u793a\u4f8b\uff0c\u5e76\u5141\u8bb8\u5b66\u5458\u901a\u8fc7\u7b80\u5355\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u63d0\u9ad8\u8ba1\u5212\u8d28\u91cf\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u5f53\u524d\u57fa\u4e8e\u4e34\u5e8a\u7684\u8303\u5f0f\u65e0\u6cd5\u6ee1\u8db3\u7684\u9700\u6c42\u3002"}}
{"id": "2505.10128", "pdf": "https://arxiv.org/pdf/2505.10128", "abs": "https://arxiv.org/abs/2505.10128", "authors": ["Huy Q. Le", "Latif U. Khan", "Choong Seon Hong"], "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": "IWCMC 2025", "summary": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6FedAPC\uff0c\u901a\u8fc7\u539f\u578b\u589e\u5f3a\u6765\u63d0\u9ad8\u5728\u9886\u57df\u5f02\u8d28\u6027\u4e0b\u7684\u5168\u5c40\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5141\u8bb8\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u534f\u4f5c\u8bad\u7ec3\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u9690\u79c1\uff0c\u662f\u9690\u79c1\u654f\u611f\u5e94\u7528\u7684\u6d41\u884c\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u7279\u522b\u662f\u9886\u57df\u5f02\u8d28\u6027\uff0cFL\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u8fd9\u963b\u788d\u4e86\u5168\u5c40\u6a21\u5f0f\u7684\u6536\u655b\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86FedAPC\uff08\u8054\u90a6\u589e\u5f3a\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u539f\u578b\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u7279\u5f81\u591a\u6837\u6027\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002FedAPC\u5229\u7528\u589e\u5f3a\u6570\u636e\u7684\u5e73\u5747\u7279\u5f81\u751f\u6210\u539f\u578b\uff0c\u4ee5\u6355\u83b7\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u3002\u901a\u8fc7\u5c06\u672c\u5730\u7279\u5f81\u4e0e\u5168\u5c40\u539f\u578b\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u4efb\u4f55\u7279\u5b9a\u9886\u57df\u7684\u8fc7\u62df\u5408\u3002", "result": "\u5728Office-10\u548cDigits\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728Office-10\u548cDigits\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10420", "pdf": "https://arxiv.org/pdf/2505.10420", "abs": "https://arxiv.org/abs/2505.10420", "authors": ["Andrei Arhire", "Radu Timofte"], "title": "Learned Lightweight Smartphone ISP with Unpaired Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW 2025", "summary": "The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\u7684\u65b0\u578b\u5b66\u4e60ISP\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u548c\u591a\u672f\u8bed\u635f\u5931\u51fd\u6570\u6765\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\u5e76\u5b66\u4e60\u989c\u8272\u548c\u7eb9\u7406\u7279\u6027\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5c55\u793a\u4e86\u65e0\u914d\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u6f5c\u529b\u3002", "motivation": "\u5f00\u53d1\u5b66\u4e60\u578bISP\u7684\u4e00\u4e2a\u56f0\u96be\u4e14\u6602\u8d35\u7684\u6b65\u9aa4\u662f\u83b7\u53d6\u50cf\u7d20\u5bf9\u9f50\u7684\u6210\u5bf9\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u5c06\u667a\u80fd\u624b\u673a\u76f8\u673a\u4f20\u611f\u5668\u6355\u83b7\u7684\u539f\u59cb\u56fe\u50cf\u6620\u5c04\u5230\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u5b66\u4e60ISP\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u539f\u59cb\u56fe\u50cf\u548c\u5177\u6709\u5339\u914d\u5185\u5bb9\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\u3002\u6211\u4eec\u7684\u65e0\u914d\u5bf9\u65b9\u6cd5\u4f7f\u7528\u4e86\u7531\u5bf9\u6297\u8bad\u7ec3\u5f15\u5bfc\u7684\u591a\u672f\u8bed\u635f\u5931\u51fd\u6570\uff0c\u5176\u4e2d\u591a\u4e2a\u5224\u522b\u5668\u5904\u7406\u6765\u81ea\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u7279\u5f81\u56fe\u4ee5\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\uff0c\u540c\u65f6\u4ece\u76ee\u6807RGB\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u989c\u8272\u548c\u7eb9\u7406\u7279\u6027\u3002", "result": "\u6211\u4eec\u5728Zurich RAW to RGB\u548cFujifilm UltraISP\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u4e0e\u914d\u5bf9\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65e0\u914d\u5bf9\u5b66\u4e60\u7b56\u7565\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff0c\u8868\u660e\u65e0\u914d\u5bf9\u5b66\u4e60\u7b56\u7565\u5177\u6709\u5f88\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09805", "pdf": "https://arxiv.org/pdf/2505.09805", "abs": "https://arxiv.org/abs/2505.09805", "authors": ["Aditya Nagori", "Ayush Gautam", "Matthew O. Wiens", "Vuong Nguyen", "Nathan Kenya Mugisha", "Jerome Kabakyenga", "Niranjan Kissoon", "John Mark Ansermino", "Rishikesan Kamaleswaran"], "title": "Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "stat.AP"], "comment": "11 pages, 2 Figures, 1 Table", "summary": "Clustering patient subgroups is essential for personalized care and efficient\nresource use. Traditional clustering methods struggle with high-dimensional,\nheterogeneous healthcare data and lack contextual understanding. This study\nevaluates Large Language Model (LLM) based clustering against classical methods\nusing a pediatric sepsis dataset from a low-income country (LIC), containing\n2,686 records with 28 numerical and 119 categorical variables. Patient records\nwere serialized into text with and without a clustering objective. Embeddings\nwere generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with\nlow-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was\napplied to these embeddings. Classical comparisons included K-Medoids\nclustering on UMAP and FAMD-reduced mixed data. Silhouette scores and\nstatistical tests evaluated cluster quality and distinctiveness.\nStella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B\nwith the clustering objective performed better with higher number of clusters,\nidentifying subgroups with distinct nutritional, clinical, and socioeconomic\nprofiles. LLM-based methods outperformed classical techniques by capturing\nricher context and prioritizing key features. These results highlight potential\nof LLMs for contextual phenotyping and informed decision-making in\nresource-limited settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u7ecf\u5178\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u3001\u5f02\u6784\u7684\u533b\u7597\u6570\u636e\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u591f\u8bc6\u522b\u51fa\u5177\u6709\u4e0d\u540c\u7279\u5f81\u7684\u60a3\u8005\u4e9a\u7ec4\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u3001\u5f02\u6784\u7684\u533b\u7597\u6570\u636e\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u805a\u7c7b\u65b9\u6cd5\u6765\u8bc6\u522b\u60a3\u8005\u4e9a\u7ec4\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u62a4\u7406\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u7ecf\u5178\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a\u6765\u81ea\u4f4e\u6536\u5165\u56fd\u5bb6\u7684\u513f\u79d1\u8d25\u8840\u75c7\u6570\u636e\u96c6\u3002\u60a3\u8005\u8bb0\u5f55\u88ab\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u5e76\u751f\u6210\u4e86\u5d4c\u5165\u5411\u91cf\u3002\u5e94\u7528\u4e86K\u5747\u503c\u805a\u7c7b\uff0c\u5e76\u4e0eK-\u4e2d\u4f4d\u6570\u805a\u7c7b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "Stella-En-400M-V5\u6a21\u578b\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u8f6e\u5ed3\u5206\u6570\uff080.86\uff09\u3002LLAMA 3.1 8B\u5728\u5177\u6709\u805a\u7c7b\u76ee\u6807\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u591f\u8bc6\u522b\u51fa\u5177\u6709\u4e0d\u540c\u8425\u517b\u3001\u4e34\u5e8a\u548c\u793e\u4f1a\u7ecf\u6d4e\u7279\u5f81\u7684\u4e9a\u7ec4\u3002\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4f18\u4e8e\u7ecf\u5178\u6280\u672f\uff0c\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u5e76\u4f18\u5148\u8003\u8651\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u60c5\u5883\u8868\u578b\u548c\u77e5\u60c5\u51b3\u7b56\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10147", "pdf": "https://arxiv.org/pdf/2505.10147", "abs": "https://arxiv.org/abs/2505.10147", "authors": ["Yash", "Nikhil Karamchandani", "Avishek Ghosh"], "title": "Near Optimal Best Arm Identification for Clustered Bandits", "categories": ["cs.LG", "cs.MA"], "comment": "To be published in ICML 2025", "summary": "This work investigates the problem of best arm identification for multi-agent\nmulti-armed bandits. We consider $N$ agents grouped into $M$ clusters, where\neach cluster solves a stochastic bandit problem. The mapping between agents and\nbandits is a priori unknown. Each bandit is associated with $K$ arms, and the\ngoal is to identify the best arm for each agent under a $\\delta$-probably\ncorrect ($\\delta$-PC) framework, while minimizing sample complexity and\ncommunication overhead.\n  We propose two novel algorithms: Clustering then Best Arm Identification\n(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a\ntwo-phase approach that first clusters agents based on the bandit problems they\nare learning, followed by identifying the best arm for each cluster. BAI-Cl\nreverses the sequence by identifying the best arms first and then clustering\nagents accordingly. Both algorithms leverage the successive elimination\nframework to ensure computational efficiency and high accuracy.\n  We establish $\\delta$-PC guarantees for both methods, derive bounds on their\nsample complexity, and provide a lower bound for this problem class. Moreover,\nwhen $M$ is small (a constant), we show that the sample complexity of a variant\nof BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic\nand real-world datasets (MovieLens, Yelp) demonstrate the superior performance\nof the proposed algorithms in terms of sample and communication efficiency,\nparticularly in settings where $M \\ll N$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u4ee3\u7406\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5Cl-BAI\u548cBAI-Cl\u3002\u8fd9\u4e9b\u7b97\u6cd5\u901a\u8fc7\u805a\u7c7b\u548c\u6700\u4f73\u81c2\u8bc6\u522b\u7684\u7ed3\u5408\uff0c\u5728\u6837\u672c\u590d\u6742\u6027\u548c\u901a\u4fe1\u5f00\u9500\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u591a\u4ee3\u7406\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u5176\u4e2d\u4ee3\u7406\u88ab\u5206\u7ec4\u5230\u4e0d\u540c\u7684\u805a\u7c7b\u4e2d\uff0c\u6bcf\u4e2a\u805a\u7c7b\u89e3\u51b3\u4e00\u4e2a\u968f\u673a\u8001\u864e\u673a\u95ee\u9898\u3002\u76ee\u6807\u662f\u5728\u6700\u5c0f\u5316\u6837\u672c\u590d\u6742\u6027\u548c\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\uff0c\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u786e\u5b9a\u6700\u4f73\u81c2\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5\uff1aCl-BAI\u548cBAI-Cl\u3002Cl-BAI\u9996\u5148\u6839\u636e\u4ee3\u7406\u6240\u5b66\u4e60\u7684\u8001\u864e\u673a\u95ee\u9898\u5bf9\u4ee3\u7406\u8fdb\u884c\u805a\u7c7b\uff0c\u7136\u540e\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u8bc6\u522b\u6700\u4f73\u81c2\u3002BAI-Cl\u5219\u5148\u8bc6\u522b\u6700\u4f73\u81c2\uff0c\u518d\u6839\u636e\u8fd9\u4e9b\u81c2\u5bf9\u4ee3\u7406\u8fdb\u884c\u805a\u7c7b\u3002\u4e24\u79cd\u7b97\u6cd5\u90fd\u5229\u7528\u4e86\u8fde\u7eed\u6d88\u9664\u6846\u67b6\u4ee5\u786e\u4fdd\u8ba1\u7b97\u6548\u7387\u548c\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u672c\u6587\u5efa\u7acb\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u03b4-PC\u4fdd\u8bc1\uff0c\u63a8\u5bfc\u4e86\u5b83\u4eec\u7684\u6837\u672c\u590d\u6742\u6027\u754c\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be5\u95ee\u9898\u7c7b\u7684\u4e0b\u754c\u3002\u5f53M\u662f\u5c0f\u5e38\u6570\u65f6\uff0cBAI-Cl\u7684\u4e00\u4e2a\u53d8\u79cd\u5728\u987a\u5e8f\u610f\u4e49\u4e0a\u8fbe\u5230\u4e86\u6700\u5c0f\u6700\u5927\u6700\u4f18\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6837\u672c\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u7279\u522b\u662f\u5728M\u8fdc\u5c0f\u4e8eN\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5Cl-BAI\u548cBAI-Cl\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u4ee3\u7406\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\u3002\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5728\u6837\u672c\u590d\u6742\u6027\u548c\u901a\u4fe1\u5f00\u9500\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u6027\u3002"}}
{"id": "2505.10453", "pdf": "https://arxiv.org/pdf/2505.10453", "abs": "https://arxiv.org/abs/2505.10453", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "title": "Vision language models have difficulty recognizing virtual objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86VLMs\u5bf9\u865a\u62df\u7269\u4f53\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5176\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u6d4b\u8bd5VLMs\u662f\u5426\u80fd\u591f\u7406\u89e3\u56fe\u50cf\u4e2d\u672a\u89c6\u89c9\u5448\u73b0\u7684\u865a\u62df\u7269\u4f53\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6700\u5148\u8fdb\u7684VLMs\uff0c\u5206\u6790\u5b83\u4eec\u5904\u7406\u865a\u62df\u7269\u4f53\u7684\u80fd\u529b\u3002", "result": "\u672c\u6587\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684VLMs\u5728\u5904\u7406\u865a\u62df\u7269\u4f53\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u865a\u62df\u7269\u4f53\u7684\u63cf\u8ff0\u53ef\u4ee5\u5e2e\u52a9\u6d4b\u8bd5AI\u7cfb\u7edf\u5bf9\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u6700\u5148\u8fdb\u7684VLMs\u5728\u5904\u7406\u865a\u62df\u7269\u4f53\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2505.10167", "pdf": "https://arxiv.org/pdf/2505.10167", "abs": "https://arxiv.org/abs/2505.10167", "authors": ["Saikat Barua", "Mostafizur Rahman", "Shehenaz Khaled", "Md Jafor Sadek", "Rafiul Islam", "Shahnewaz Siddique"], "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "16 pages, 6 figures, 7 equations", "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86QuXAI\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8eQ-MEDLEY\uff0c\u7528\u4e8e\u89e3\u91ca\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08HQML\uff09\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u91cd\u8981\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cQ-MEDLEY\u80fd\u591f\u533a\u5206HQML\u6a21\u578b\u4e2d\u7684\u7ecf\u5178\u5f71\u54cd\u56e0\u7d20\u548c\u566a\u58f0\uff0c\u5e76\u5728\u7ecf\u5178\u9a8c\u8bc1\u8bbe\u7f6e\u4e2d\u4e0e\u73b0\u6709\u7684XAI\u6280\u672f\u76f8\u7ade\u4e89\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86Q-MEDLEY\u4e2d\u590d\u5408\u7ed3\u6784\u7684\u4f18\u70b9\u3002\u672c\u7814\u7a76\u7684\u6210\u679c\u5bf9\u4e8e\u63d0\u9ad8HQML\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u7684\u91cf\u5b50\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\uff0c\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08HQML\uff09\u6a21\u578b\u5728\u8ba1\u7b97\u667a\u80fd\u65b9\u9762\u5c55\u73b0\u51fa\u65b0\u7684\u524d\u666f\uff0c\u4f46\u5176\u57fa\u672c\u590d\u6742\u6027\u7ecf\u5e38\u5bfc\u81f4\u9ed1\u7bb1\u884c\u4e3a\uff0c\u524a\u5f31\u4e86\u5176\u5e94\u7528\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002\u5c3d\u7ba1\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5728\u91cf\u5b50\u7cfb\u7edf\u4e2d\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u4f46\u5728\u9488\u5bf9\u91c7\u7528\u91cf\u5316\u7279\u5f81\u7f16\u7801\u5e76\u968f\u540e\u8fdb\u884c\u7ecf\u5178\u5b66\u4e60\u7684HQML\u67b6\u6784\u7684\u7a33\u5065\u5168\u5c40\u548c\u5c40\u90e8\u89e3\u91ca\u65b9\u6cd5\u65b9\u9762\u5b58\u5728\u660e\u663e\u7684\u7814\u7a76\u7a7a\u767d\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86QuXAI\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8eQ-MEDLEY\uff0c\u7528\u4e8e\u89e3\u91ca\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08HQML\uff09\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u91cd\u8981\u6027\u3002Q-MEDLEY\u7ed3\u5408\u4e86\u57fa\u4e8e\u7279\u5f81\u7684\u63a8\u65ad\uff0c\u4fdd\u7559\u4e86\u91cf\u5b50\u53d8\u6362\u9636\u6bb5\uff0c\u5e76\u53ef\u89c6\u5316\u4e86\u7ed3\u679c\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQ-MEDLEY\u80fd\u591f\u533a\u5206HQML\u6a21\u578b\u4e2d\u7684\u7ecf\u5178\u5f71\u54cd\u56e0\u7d20\u548c\u566a\u58f0\uff0c\u5e76\u5728\u7ecf\u5178\u9a8c\u8bc1\u8bbe\u7f6e\u4e2d\u4e0e\u73b0\u6709\u7684XAI\u6280\u672f\u76f8\u7ade\u4e89\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86Q-MEDLEY\u4e2d\u590d\u5408\u7ed3\u6784\u7684\u4f18\u70b9\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86QuXAI\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8eQ-MEDLEY\uff0c\u7528\u4e8e\u89e3\u91ca\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08HQML\uff09\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u91cd\u8981\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cQ-MEDLEY\u80fd\u591f\u533a\u5206HQML\u6a21\u578b\u4e2d\u7684\u7ecf\u5178\u5f71\u54cd\u56e0\u7d20\u548c\u566a\u58f0\uff0c\u5e76\u5728\u7ecf\u5178\u9a8c\u8bc1\u8bbe\u7f6e\u4e2d\u4e0e\u73b0\u6709\u7684XAI\u6280\u672f\u76f8\u7ade\u4e89\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86Q-MEDLEY\u4e2d\u590d\u5408\u7ed3\u6784\u7684\u4f18\u70b9\u3002\u672c\u7814\u7a76\u7684\u6210\u679c\u5bf9\u4e8e\u63d0\u9ad8HQML\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u7684\u91cf\u5b50\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5e94\u7528\u3002"}}
{"id": "2505.10473", "pdf": "https://arxiv.org/pdf/2505.10473", "abs": "https://arxiv.org/abs/2505.10473", "authors": ["Fengdi Zhang", "Hongkun Cao", "Ruqi Huang"], "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.", "AI": {"tldr": "ControlGS\u662f\u4e00\u79cd3DGS\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u81ea\u52a8\u627e\u5230\u7406\u60f3\u7684\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u70b9\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u66f4\u5c11\u9ad8\u65af\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u540c\u65f6\u652f\u6301\u5e7f\u6cdb\u7684\u65e0\u7ea7\u63a7\u5236\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8ba9\u7528\u6237\u76f4\u89c2\u8c03\u6574\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u7684\u80fd\u529b\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u901a\u4fe1\u7ea6\u675f\u6761\u4ef6\u3002", "method": "ControlGS\u662f\u4e00\u79cd3DGS\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u8fd0\u884c\u548c\u7528\u6237\u6307\u5b9a\u7684\u8d85\u53c2\u6570\u6765\u5b9e\u73b0\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u8de8\u573a\u666f\u4e00\u81f4\u7684\u6570\u91cf-\u8d28\u91cf\u63a7\u5236\u3002", "result": "ControlGS\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u5e76\u51cf\u5c11\u9ad8\u65af\u5206\u5e03\u7684\u6570\u91cf\uff0c\u540c\u65f6\u652f\u6301\u5e7f\u6cdb\u7684\u65e0\u7ea7\u63a7\u5236\u8303\u56f4\u3002", "conclusion": "ControlGS\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u81ea\u52a8\u627e\u5230\u7406\u60f3\u7684\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u70b9\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u66f4\u5c11\u9ad8\u65af\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u540c\u65f6\u652f\u6301\u5e7f\u6cdb\u7684\u65e0\u7ea7\u63a7\u5236\u8303\u56f4\u3002"}}
{"id": "2505.09814", "pdf": "https://arxiv.org/pdf/2505.09814", "abs": "https://arxiv.org/abs/2505.09814", "authors": ["Dmitry Rybin", "Yushun Zhang", "Zhi-Quan Luo"], "title": "$XX^{t}$ Can Be Faster", "categories": ["cs.DS", "cs.AI", "cs.LG", "cs.SC", "68Q25, 68T20", "F.2.1; I.1.2"], "comment": null, "summary": "We present a new algorithm RXTX that computes product of matrix by its\ntranspose $XX^{t}$. RXTX uses $5\\%$ less multiplications and additions than\nState-of-the-Art and achieves accelerations even for small sizes of matrix $X$.\nThe algorithm was discovered by combining Machine Learning-based search methods\nwith Combinatorial Optimization.", "AI": {"tldr": "RXTX\u662f\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u77e9\u9635\u4e0e\u5176\u8f6c\u7f6e\u7684\u4e58\u79ef\uff0c\u5b83\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u9ad8\u6548\uff0c\u5e76\u4e14\u5728\u5c0f\u5c3a\u5bf8\u77e9\u9635\u4e0a\u4e5f\u80fd\u5b9e\u73b0\u52a0\u901f\u3002", "motivation": "\u5bfb\u627e\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u77e9\u9635\u4e0e\u5176\u8f6c\u7f6e\u4e58\u79ef\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u4e2d\u7684\u4e58\u6cd5\u548c\u52a0\u6cd5\u64cd\u4f5c\u3002", "method": "\u901a\u8fc7\u5c06\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u641c\u7d22\u65b9\u6cd5\u4e0e\u7ec4\u5408\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u53d1\u73b0\u4e86RXTX\u7b97\u6cd5\u3002", "result": "RXTX\u7b97\u6cd5\u5728\u8ba1\u7b97\u77e9\u9635\u4e0e\u5176\u8f6c\u7f6e\u7684\u4e58\u79ef\u65f6\uff0c\u4f7f\u7528\u7684\u4e58\u6cd5\u548c\u52a0\u6cd5\u64cd\u4f5c\u51cf\u5c11\u4e865%\u3002", "conclusion": "RXTX\u7b97\u6cd5\u5728\u8ba1\u7b97\u77e9\u9635\u4e0e\u5176\u8f6c\u7f6e\u7684\u4e58\u79ef\u65f6\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u9ad8\u6548\uff0c\u4e14\u5728\u5c0f\u5c3a\u5bf8\u77e9\u9635\u4e0a\u4e5f\u80fd\u5b9e\u73b0\u52a0\u901f\u3002"}}
{"id": "2505.10172", "pdf": "https://arxiv.org/pdf/2505.10172", "abs": "https://arxiv.org/abs/2505.10172", "authors": ["Zeyan Li", "Libing Chen", "Yin Tang"], "title": "Does Scaling Law Apply in Time Series Forecasting?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Rapid expansion of model size has emerged as a key challenge in time series\nforecasting. From early Transformer with tens of megabytes to recent\narchitectures like TimesNet with thousands of megabytes, performance gains have\noften come at the cost of exponentially increasing parameter counts. But is\nthis scaling truly necessary? To question the applicability of the scaling law\nin time series forecasting, we propose Alinear, an ultra-lightweight\nforecasting model that achieves competitive performance using only k-level\nparameters. We introduce a horizon-aware adaptive decomposition mechanism that\ndynamically rebalances component emphasis across different forecast lengths,\nalongside a progressive frequency attenuation strategy that achieves stable\nprediction in various forecasting horizons without incurring the computational\noverhead of attention mechanisms. Extensive experiments on seven benchmark\ndatasets demonstrate that Alinear consistently outperforms large-scale models\nwhile using less than 1% of their parameters, maintaining strong accuracy\nacross both short and ultra-long forecasting horizons. Moreover, to more fairly\nevaluate model efficiency, we propose a new parameter-aware evaluation metric\nthat highlights the superiority of ALinear under constrained model budgets. Our\nanalysis reveals that the relative importance of trend and seasonal components\nvaries depending on data characteristics rather than following a fixed pattern,\nvalidating the necessity of our adaptive design. This work challenges the\nprevailing belief that larger models are inherently better and suggests a\nparadigm shift toward more efficient time series modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Alinear\uff0c\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u4ec5k\u7ea7\u53c2\u6570\u5c31\u80fd\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAlinear\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u540c\u65f6\u4f7f\u7528\u4e0d\u52301%\u7684\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u53c2\u6570\u611f\u77e5\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u5feb\u901f\u6269\u5c55\uff0c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u5173\u952e\u6311\u6218\u3002\u4ece\u65e9\u671f\u7684Transformer\u5230\u6700\u8fd1\u7684TimesNet\uff0c\u6027\u80fd\u63d0\u5347\u5f80\u5f80\u4f34\u968f\u7740\u53c2\u6570\u6570\u91cf\u7684\u6307\u6570\u7ea7\u589e\u957f\u3002\u4f46\u8fd9\u79cd\u6269\u5c55\u662f\u5426\u771f\u7684\u5fc5\u8981\uff1f\u4e3a\u4e86\u8d28\u7591\u6269\u5c55\u5b9a\u5f8b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u9002\u7528\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Alinear\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Alinear\uff0c\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u4ec5k\u7ea7\u53c2\u6570\u5c31\u80fd\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65f6\u95f4\u8303\u56f4\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5206\u89e3\u673a\u5236\uff0c\u4ee5\u53ca\u4e00\u79cd\u6e10\u8fdb\u5f0f\u9891\u7387\u8870\u51cf\u7b56\u7565\uff0c\u4ee5\u5728\u5404\u79cd\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u5b9e\u73b0\u7a33\u5b9a\u7684\u9884\u6d4b\uff0c\u800c\u4e0d\u4f1a\u4ea7\u751f\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAlinear\u59cb\u7ec8\u4f18\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u53c2\u6570\u4e0d\u5230\u5b83\u4eec\u76841%\uff0c\u5e76\u5728\u77ed\u65f6\u95f4\u548c\u8d85\u957f\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u66f4\u516c\u5e73\u5730\u8bc4\u4f30\u6a21\u578b\u6548\u7387\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff0c\u7a81\u51fa\u4e86ALinear\u5728\u53d7\u9650\u6a21\u578b\u9884\u7b97\u4e0b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u6587\u6311\u6218\u4e86\u5927\u578b\u6a21\u578b\u672c\u8d28\u4e0a\u66f4\u597d\u7684\u666e\u904d\u4fe1\u5ff5\uff0c\u5e76\u5efa\u8bae\u5411\u66f4\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2505.10481", "pdf": "https://arxiv.org/pdf/2505.10481", "abs": "https://arxiv.org/abs/2505.10481", "authors": ["Ilya Ovodov", "Petr Surovtsev", "Karina Kvanchiani", "Alexander Kapitanov", "Alexander Nagaev"], "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Logos\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u4fc4\u8bed\u624b\u8bed\uff08RSL\uff09\u6570\u636e\u96c6\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684ISLR\u6570\u636e\u96c6\uff0c\u62e5\u6709\u6700\u591a\u7684\u7b7e\u540d\u8005\u548c\u6700\u5927\u7684RSL\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8bcd\u6c47\u91cf\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4f7f\u7528\u591a\u4e2a\u5206\u7c7b\u5934\u7684\u8054\u5408\u8bad\u7ec3\u6700\u6709\u52a9\u4e8e\u76ee\u6807\u4f4e\u8d44\u6e90\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002Logos\u6570\u636e\u96c6\u7684\u5173\u952e\u7279\u70b9\u662f\u660e\u786e\u6807\u6ce8\u7684\u89c6\u89c9\u76f8\u4f3c\u7b7e\u540d\u7ec4\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u660e\u786e\u6807\u8bb0\u89c6\u89c9\u76f8\u4f3c\u7684\u7b7e\u540d\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u7684\u89c6\u89c9\u7f16\u7801\u5668\u3002\u57fa\u4e8e\u63d0\u51fa\u7684\u8d21\u732e\uff0c\u6211\u4eec\u5728WLASL\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728AUTSL\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u4ec5\u4f7f\u7528\u5355\u6d41\u6a21\u578b\u5904\u7406\u7eafRGB\u89c6\u9891\u3002\u6e90\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5747\u53ef\u516c\u5f00\u83b7\u53d6\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\uff08ISLR\uff09\u4efb\u52a1\u7684\u4e24\u4e2a\u65b9\u9762\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u6709\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u4f46\u5927\u591a\u6570\u4e2a\u522b\u624b\u8bed\u7684\u6570\u636e\u91cf\u6709\u9650\u3002\u8fd9\u5e26\u6765\u4e86\u8de8\u8bed\u8a00ISLR\u6a21\u578b\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u5305\u62ec\u8fc1\u79fb\u5b66\u4e60\u3002\u5176\u6b21\uff0c\u76f8\u4f3c\u7684\u624b\u52bf\u53ef\u80fd\u6709\u4e0d\u540c\u7684\u8bed\u4e49\u542b\u4e49\u3002\u8fd9\u4f1a\u5bfc\u81f4\u6570\u636e\u96c6\u6807\u6ce8\u7684\u6b67\u4e49\uff0c\u5e76\u5f15\u53d1\u5173\u4e8e\u5982\u4f55\u6807\u6ce8\u8fd9\u4e9b\u624b\u52bf\u7684\u6700\u4f73\u7b56\u7565\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Logos\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u4fc4\u8bed\u624b\u8bed\uff08RSL\uff09\u6570\u636e\u96c6\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684ISLR\u6570\u636e\u96c6\uff0c\u62e5\u6709\u6700\u591a\u7684\u7b7e\u540d\u8005\u548c\u6700\u5927\u7684RSL\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8bcd\u6c47\u91cf\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4f7f\u7528\u591a\u4e2a\u5206\u7c7b\u5934\u7684\u8054\u5408\u8bad\u7ec3\u6700\u6709\u52a9\u4e8e\u76ee\u6807\u4f4e\u8d44\u6e90\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002Logos\u6570\u636e\u96c6\u7684\u5173\u952e\u7279\u70b9\u662f\u660e\u786e\u6807\u6ce8\u7684\u89c6\u89c9\u76f8\u4f3c\u7b7e\u540d\u7ec4\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u660e\u786e\u6807\u8bb0\u89c6\u89c9\u76f8\u4f3c\u7684\u7b7e\u540d\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u7684\u89c6\u89c9\u7f16\u7801\u5668\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9884\u8bad\u7ec3\u5728Logos\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u5176\u4ed6\u8bed\u8a00SLR\u4efb\u52a1\u7684\u901a\u7528\u7f16\u7801\u5668\uff0c\u5305\u62ec\u5c11\u6837\u672c\u5b66\u4e60\u3002\u901a\u8fc7\u63d0\u51fa\u7684\u8d21\u732e\uff0c\u6211\u4eec\u5728WLASL\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728AUTSL\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u4ec5\u4f7f\u7528\u5355\u6d41\u6a21\u578b\u5904\u7406\u7eafRGB\u89c6\u9891\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u51fa\u7684\u8d21\u732e\uff0c\u6211\u4eec\u5728WLASL\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728AUTSL\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u4ec5\u4f7f\u7528\u5355\u6d41\u6a21\u578b\u5904\u7406\u7eafRGB\u89c6\u9891\u3002\u6e90\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5747\u53ef\u516c\u5f00\u83b7\u53d6\u3002"}}
{"id": "2505.09830", "pdf": "https://arxiv.org/pdf/2505.09830", "abs": "https://arxiv.org/abs/2505.09830", "authors": ["Mart\u00edn Rodr\u00edguez", "Gustavo Rossi", "Alejandro Fernandez"], "title": "Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values", "categories": ["cs.SE", "cs.AI"], "comment": "Under revision at Jornadas de Cloud Computing, Big Data & Emerging\n  Topics (JCC-BD&ET) - 2025", "summary": "The design and implementation of unit tests is a complex task many\nprogrammers neglect. This research evaluates the potential of Large Language\nModels (LLMs) in automatically generating test cases, comparing them with\nmanual tests. An optimized prompt was developed, that integrates code and\nrequirements, covering critical cases such as equivalence partitions and\nboundary values. The strengths and weaknesses of LLMs versus trained\nprogrammers were compared through quantitative metrics and manual qualitative\nanalysis. The results show that the effectiveness of LLMs depends on\nwell-designed prompts, robust implementation, and precise requirements.\nAlthough flexible and promising, LLMs still require human supervision. This\nwork highlights the importance of manual qualitative analysis as an essential\ncomplement to automation in unit test evaluation.", "AI": {"tldr": "This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests. The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. Although flexible and promising, LLMs still require human supervision.", "motivation": "The design and implementation of unit tests is a complex task many programmers neglect. This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests.", "method": "An optimized prompt was developed that integrates code and requirements, covering critical cases such as equivalence partitions and boundary values. The strengths and weaknesses of LLMs versus trained programmers were compared through quantitative metrics and manual qualitative analysis.", "result": "The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements.", "conclusion": "LLMs still require human supervision and manual qualitative analysis is essential in unit test evaluation."}}
{"id": "2505.10192", "pdf": "https://arxiv.org/pdf/2505.10192", "abs": "https://arxiv.org/abs/2505.10192", "authors": ["Prashant P. Shinde", "Priyadarshini P. Pai", "Shashishekar P. Adiga", "K. Subramanya Mayya", "Yongbeom Seo", "Myungsoo Hwang", "Heeyoung Go", "Changmin Park"], "title": "Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u68c0\u6d4b\u534a\u5bfc\u4f53\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u5fae\u5c0f\u7f3a\u9677\u3002\u7ed3\u679c\u8868\u660e\uff0cYOLOv8\u5728\u68c0\u6d4b\u8f83\u5c0f\u7f3a\u9677\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8e\u7f3a\u9677\u6807\u6ce8\u7684\u8d28\u91cf\u6570\u636e\u7f3a\u4e4f\u826f\u597d\u7684\u4ee3\u8868\u6027\uff0c\u7279\u522b\u662f\u8f83\u5c0f\u7684\u7f3a\u9677\uff0c\u8fd9\u963b\u788d\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7f3a\u9677\u68c0\u6d4b\u6a21\u578b\u5728\u5236\u9020\u751f\u4ea7\u7ebf\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u6211\u4eec\u4eba\u5de5\u751f\u6210\u5177\u6709\u5df2\u77e5\u7f3a\u9677\u5206\u5e03\u7684\u7ebf\u56fe\u6848\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\uff08SEM\uff09\u56fe\u50cf\uff0c\u5e76\u81ea\u4e3b\u6ce8\u91ca\u5b83\u4eec\u3002\u7136\u540e\uff0c\u6211\u4eec\u91c7\u7528\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6765\u7814\u7a76\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\u4f5c\u4e3a\u7f3a\u9677\u5c3a\u5bf8\u7684\u51fd\u6570\uff0c\u6bd4\u8282\u8ddd\u5bbd\u5ea6\u5c0f\u5f97\u591a\u3002", "result": "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u5668YOLOv8\u7684\u5e73\u5747\u7cbe\u5ea6\u4e3a96%\uff0c\u76f8\u6bd4\u4e4b\u4e0bEfficientNet\u4e3a83%\uff0cSSD\u4e3a77%\u3002\u5728\u771f\u5b9eSEM\u6570\u636e\u4e0a\u6d4b\u8bd5\u65f6\uff0cYOLOv8\u6a21\u578b\u6b63\u786e\u68c0\u6d4b\u4e8684.6%\u7684\u6865\u63a5\u7f3a\u9677\u548c78.3%\u7684\u65ad\u88c2\u7f3a\u9677\u3002", "conclusion": "\u8fd9\u4e9b\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u53d1\u7a33\u5065\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u66ff\u4ee3\u54c1\u3002"}}
{"id": "2505.10483", "pdf": "https://arxiv.org/pdf/2505.10483", "abs": "https://arxiv.org/abs/2505.10483", "authors": ["Yi Li", "Haonan Wang", "Qixiang Zhang", "Boyu Xiao", "Chenchang Hu", "Hualiang Wang", "Xiaomeng Li"], "title": "UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric", "summary": "The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniEval\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u9996\u4e2a\u65e0\u9700\u989d\u5916\u6a21\u578b\u3001\u56fe\u50cf\u6216\u6ce8\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b81\u4e2a\u7ec6\u7c92\u5ea6\u6807\u7b7e\u7684UniBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u5bf9\u5e94\u7684UniScore\u6307\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0c\u4e14\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8bf8\u591a\u9650\u5236\uff0c\u5982\u7f3a\u4e4f\u6574\u4f53\u7ed3\u679c\u3001\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u56fe\u50cf\u3001\u57fa\u51c6\u6d4b\u8bd5\u591a\u6837\u6027\u4e0d\u8db3\u4ee5\u53ca\u8bc4\u4f30\u6307\u6807\u80fd\u529b\u6709\u9650\u3002", "method": "\u5f15\u5165\u4e86UniEval\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u6a21\u578b\u3001\u56fe\u50cf\u6216\u6ce8\u91ca\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b81\u4e2a\u7ec6\u7c92\u5ea6\u6807\u7b7e\u7684UniBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u5bf9\u5e94\u7684UniScore\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniBench\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0cUniScore\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u6307\u6807\u3002\u6b64\u5916\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u548c\u89c6\u89c9\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u72ec\u7279\u4ef7\u503c\u3002", "conclusion": "UniEval\u6846\u67b6\u548cUniBench\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u7b80\u5316\u548c\u7edf\u4e00\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0c\u4e14UniScore\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222", "abs": "https://arxiv.org/abs/2505.10222", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.", "AI": {"tldr": "ComplexFormer is a new attention mechanism that improves the integration of semantic and positional information in transformer models, leading to better performance and adaptability.", "motivation": "Prior methods often model semantic and positional differences separately or apply uniform positional adjustments across heads, which can limit representational capacity. This paper aims to address these challenges by introducing a more flexible and expressive attention mechanism.", "method": "ComplexFormer introduces Complex Multi-Head Attention (CMHA), which allows each head to independently model semantic and positional differences within the complex plane. It includes a per-head Euler transformation and a per-head adaptive differential rotation mechanism.", "result": "Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show that ComplexFormer achieves superior performance, significantly lower generation perplexity, and improved long-context coherence compared to strong baselines like RoPE-Transformers.", "conclusion": "ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism."}}
{"id": "2505.10198", "pdf": "https://arxiv.org/pdf/2505.10198", "abs": "https://arxiv.org/abs/2505.10198", "authors": ["Mariano Ferrero", "Jos\u00e9 Omar Chelotti", "Luciano Sebasti\u00e1n Martinez-Rau", "Leandro Vignolo", "Mart\u00edn Pires", "Julio Ricardo Galli", "Leonardo Luis Giovanini", "Hugo Leonardo Rufiner"], "title": "A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals", "categories": ["cs.LG"], "comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence", "summary": "Monitoring feeding behaviour is a relevant task for efficient herd management\nand the effective use of available resources in grazing cattle. The ability to\nautomatically recognise animals' feeding activities through the identification\nof specific jaw movements allows for the improvement of diet formulation, as\nwell as early detection of metabolic problems and symptoms of animal\ndiscomfort, among other benefits. The use of sensors to obtain signals for such\nmonitoring has become popular in the last two decades. The most frequently\nemployed sensors include accelerometers, microphones, and cameras, each with\nits own set of advantages and drawbacks. An unexplored aspect is the\nsimultaneous use of multiple sensors with the aim of combining signals in order\nto enhance the precision of the estimations. In this direction, this work\nintroduces a deep neural network based on the fusion of acoustic and inertial\nsignals, composed of convolutional, recurrent, and dense layers. The main\nadvantage of this model is the combination of signals through the automatic\nextraction of features independently from each of them. The model has emerged\nfrom an exploration and comparison of different neural network architectures\nproposed in this work, which carry out information fusion at different levels.\nFeature-level fusion has outperformed data and decision-level fusion by at\nleast a 0.14 based on the F1-score metric. Moreover, a comparison with\nstate-of-the-art machine learning methods is presented, including traditional\nand deep learning approaches. The proposed model yielded an F1-score value of\n0.802, representing a 14% increase compared to previous methods. Finally,\nresults from an ablation study and post-training quantization evaluation are\nalso reported.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u548c\u60ef\u6027\u4fe1\u53f7\u878d\u5408\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u5bf9\u725b\u91c7\u98df\u884c\u4e3a\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u76d1\u6d4b\u725b\u7684\u91c7\u98df\u884c\u4e3a\u5bf9\u4e8e\u9ad8\u6548\u7684\u7267\u573a\u7ba1\u7406\u548c\u8d44\u6e90\u5229\u7528\u81f3\u5173\u91cd\u8981\u3002\u81ea\u52a8\u8bc6\u522b\u52a8\u7269\u7684\u8fdb\u98df\u6d3b\u52a8\u53ef\u4ee5\u6539\u5584\u996e\u98df\u914d\u65b9\uff0c\u5e76\u53ca\u65e9\u53d1\u73b0\u4ee3\u8c22\u95ee\u9898\u548c\u52a8\u7269\u4e0d\u9002\u7684\u75c7\u72b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u548c\u60ef\u6027\u4fe1\u53f7\u878d\u5408\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u5377\u79ef\u3001\u5faa\u73af\u548c\u5bc6\u96c6\u5c42\u3002", "result": "\u7279\u5f81\u7ea7\u878d\u5408\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8e\u6570\u636e\u7ea7\u548c\u51b3\u7b56\u7ea7\u878d\u5408\uff0c\u4e14\u63d0\u51fa\u7684\u6a21\u578b\u5728F1\u5206\u6570\u4e0a\u8fbe\u5230\u4e860.802\uff0c\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8614%\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u878d\u5408\u58f0\u5b66\u548c\u60ef\u6027\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u5bf9\u725b\u91c7\u98df\u884c\u4e3a\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728F1\u5206\u6570\u4e0a\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8614%\u3002"}}
{"id": "2505.10496", "pdf": "https://arxiv.org/pdf/2505.10496", "abs": "https://arxiv.org/abs/2505.10496", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "AI": {"tldr": "\u6211\u4eec\u5f15\u5165\u4e86CheXGenBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u5408\u6210\u80f8\u90e8X\u5149\u7247\u751f\u6210\u7684\u6846\u67b6\uff0c\u540c\u65f6\u8bc4\u4f30\u4fdd\u771f\u5ea6\u3001\u9690\u79c1\u98ce\u9669\u548c\u4e34\u5e8a\u6548\u7528\u3002\u901a\u8fc7\u6807\u51c6\u5316\u7684\u6570\u636e\u5212\u5206\u548c\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u4e2d\u7684\u5173\u952e\u4f4e\u6548\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86SynthCheX-75K\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210AI\u5728\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u533b\u5b66\u9886\u57df\u7684\u8bc4\u4f30\u53d7\u5230\u65b9\u6cd5\u4e0d\u4e00\u81f4\u3001\u8fc7\u65f6\u7684\u67b6\u6784\u6bd4\u8f83\u548c\u8131\u79bb\u8bc4\u4f30\u6807\u51c6\u7684\u963b\u788d\uff0c\u8fd9\u4e9b\u6807\u51c6\u5f88\u5c11\u6d89\u53ca\u5408\u6210\u6837\u672c\u7684\u5b9e\u9645\u4e34\u5e8a\u4ef7\u503c\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CheXGenBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e25\u683c\u4e14\u591a\u65b9\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u80f8\u90e8X\u5149\u7247\u751f\u6210\uff0c\u540c\u65f6\u8bc4\u4f30\u4fdd\u771f\u5ea6\u3001\u9690\u79c1\u98ce\u9669\u548c\u4e34\u5e8a\u6548\u7528\u3002\u901a\u8fc7\u6807\u51c6\u5316\u7684\u6570\u636e\u5212\u5206\u548c\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u62ec\u8d85\u8fc720\u4e2a\u5b9a\u91cf\u6307\u6807\uff0c\u7cfb\u7edf\u5730\u5206\u6790\u751f\u6210\u8d28\u91cf\u3001\u6f5c\u5728\u9690\u79c1\u6f0f\u6d1e\u4ee5\u53ca\u4e0b\u6e38\u4e34\u5e8a\u9002\u7528\u6027\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u4e2d\u7684\u5173\u952e\u4f4e\u6548\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8bc4\u4f30\u751f\u6210\u4fdd\u771f\u5ea6\u65b9\u9762\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u548c\u65e0\u4fe1\u606f\u7684\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u6846\u67b6\u4e3a\u533b\u5b66AI\u793e\u533a\u5efa\u7acb\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u57fa\u51c6\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u4e3a\u533b\u5b66AI\u793e\u533a\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u7684\u57fa\u51c6\uff0c\u4f7f\u5ba2\u89c2\u548c\u53ef\u91cd\u590d\u7684\u6bd4\u8f83\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u4fc3\u8fdb\u4e86\u73b0\u6709\u548c\u672a\u6765\u751f\u6210\u6a21\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u96c6SynthCheX-75K\uff0c\u4ee5\u652f\u6301\u8be5\u5173\u952e\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.10213", "pdf": "https://arxiv.org/pdf/2505.10213", "abs": "https://arxiv.org/abs/2505.10213", "authors": ["Mohammadmahdi Ghasemloo", "Alireza Moradi"], "title": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u9700\u8981\u5efa\u7acb\u6700\u4f73\u5b9e\u8df5\uff0c\u4ee5\u8d85\u8d8a\u4f20\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u6765\u5229\u7528\u5176\u80fd\u529b\u3002\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u80fd\u6e90\u7cfb\u7edf\u3001\u91d1\u878d\u548c\u533b\u7597\u4fdd\u5065\u7b49\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u5730\u5c06\u7ed3\u6784\u5316\u7684\u65f6\u95f4\u4fe1\u606f\u6ce8\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u5176\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u6ca1\u6709\u8f85\u52a9\u4fe1\u606f\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u77e5\u8bc6\u6307\u5bfc\u7684\u9884\u6d4b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u65e0\u4fe1\u606f\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u77e5\u8bc6\u8fc1\u79fb\u7b56\u7565\u5728\u5f25\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7279\u5b9a\u9886\u57df\u9884\u6d4b\u4efb\u52a1\u4e4b\u95f4\u7684\u5dee\u8ddd\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10497", "pdf": "https://arxiv.org/pdf/2505.10497", "abs": "https://arxiv.org/abs/2505.10497", "authors": ["Iurii Medvedev", "Nuno Goncalves"], "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53cc\u5206\u652f\u5206\u7c7b\u7b56\u7565\u6765\u4fee\u6539\u5206\u7c7b\u4efb\u52a1\uff0c\u4ece\u800c\u6709\u6548\u5904\u7406\u4eba\u8138\u53d8\u5f62\u6807\u7b7e\u7684\u6a21\u7cca\u6027\u3002\u8fd9\u79cd\u9002\u5e94\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u53d8\u5f62\u56fe\u50cf\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u5176\u533a\u5206\u5b83\u4eec\u4e0e\u771f\u5b9e\u6837\u672c\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u7b56\u7565\u5df2\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u5bf9\u6297\u4eba\u8138\u53d8\u5f62\u653b\u51fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5177\u6709\u666e\u904d\u9002\u7528\u6027\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7684\u9762\u90e8\u8bc6\u522b\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u6539\u8fdb\u57fa\u4e8e\u5206\u7c7b\u7684\u8bc6\u522b\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4eba\u8138\u8bc6\u522b\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e5f\u589e\u52a0\u4e86\u5176\u66b4\u9732\u4e8e\u6f14\u793a\u653b\u51fb\uff08\u5305\u62ec\u4eba\u8138\u53d8\u5f62\uff09\u7684\u98ce\u9669\uff0c\u8fd9\u4f1a\u5e26\u6765\u4e25\u91cd\u7684\u5b89\u5168\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u73b0\u4ee3\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5fc5\u987b\u5177\u5907\u5bf9\u8fd9\u4e9b\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53cc\u5206\u652f\u5206\u7c7b\u7b56\u7565\u6765\u4fee\u6539\u5206\u7c7b\u4efb\u52a1\uff0c\u4ece\u800c\u6709\u6548\u5904\u7406\u4eba\u8138\u53d8\u5f62\u6807\u7b7e\u7684\u6a21\u7cca\u6027\u3002\u8fd9\u79cd\u9002\u5e94\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u53d8\u5f62\u56fe\u50cf\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u5176\u533a\u5206\u5b83\u4eec\u4e0e\u771f\u5b9e\u6837\u672c\u7684\u80fd\u529b\u3002", "result": "\u6211\u4eec\u7684\u7b56\u7565\u5df2\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u5bf9\u6297\u4eba\u8138\u53d8\u5f62\u653b\u51fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7b56\u7565\u5df2\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u5bf9\u6297\u4eba\u8138\u53d8\u5f62\u653b\u51fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5177\u6709\u666e\u904d\u9002\u7528\u6027\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7684\u9762\u90e8\u8bc6\u522b\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u6539\u8fdb\u57fa\u4e8e\u5206\u7c7b\u7684\u8bc6\u522b\u65b9\u6cd5\u3002"}}
{"id": "2505.10533", "pdf": "https://arxiv.org/pdf/2505.10533", "abs": "https://arxiv.org/abs/2505.10533", "authors": ["Aaryan Sharma", "Shivansh Gupta", "Samar Agarwal", "Vishak Prasad C.", "Ganesh Ramakrishnan"], "title": "Enhancing Multi-Image Question Answering via Submodular Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u68c0\u7d22\u5668\u6846\u67b6\uff0c\u4f7f\u7528\u5b50\u6a21\u6001\u5b50\u96c6\u9009\u62e9\u6280\u672f\u6765\u63d0\u9ad8\u591a\u56fe\u50cf\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u6d89\u53ca\u5355\u4e2a\u56fe\u50cf\u7684\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u591a\u4e2a\u56fe\u50cf\u7684\u96c6\u5408\u65f6\u8868\u73b0\u4e0d\u4f73\uff08Multiple Image Question Answering\u573a\u666f\uff09\u3002\u8fd9\u4e9b\u4efb\u52a1\u6d89\u53ca\u5bf9\u5927\u91cf\u56fe\u50cf\u8fdb\u884c\u63a8\u7406\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff08\u968f\u7740\u56fe\u50cf\u6570\u91cf\u7684\u589e\u52a0\uff09\u548c\u68c0\u7d22\u6027\u80fd\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9MIRAGE\u6a21\u578b\u4e2d\u5f15\u5165\u7684\u68c0\u7d22\u5668\u6846\u67b6\u7684\u6539\u8fdb\uff0c\u4f7f\u7528\u4e86\u5b50\u6a21\u6001\u5b50\u96c6\u9009\u62e9\u6280\u672f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u67e5\u8be2\u611f\u77e5\u7684\u5b50\u6a21\u6001\u51fd\u6570\uff0c\u5982GraphCut\uff0c\u4ee5\u5728\u4e3b\u8981\u68c0\u7d22\u7ec4\u4ef6\u4e4b\u524d\u9884\u9009\u8bed\u4e49\u76f8\u5173\u7684\u56fe\u50cf\u5b50\u96c6\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86\u4f7f\u7528\u57fa\u4e8e\u951a\u70b9\u7684\u67e5\u8be2\u548c\u6570\u636e\u589e\u5f3a\u53ef\u4ee5\u63d0\u9ad8\u5b50\u6a21\u6001\u68c0\u7d22\u5668\u7ba1\u9053\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5927\u578bhaystack\u5927\u5c0f\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u951a\u70b9\u7684\u67e5\u8be2\u548c\u6570\u636e\u589e\u5f3a\uff0c\u63d0\u9ad8\u4e86\u5b50\u6a21\u6001\u68c0\u7d22\u5668\u7ba1\u9053\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5927\u578bhaystack\u5927\u5c0f\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465", "abs": "https://arxiv.org/abs/2505.10465", "authors": ["Yizhou liu", "Ziming Liu", "Jeff Gore"], "title": "Superposition Yields Robust Neural Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u635f\u5931\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u7f29\u653e\u5173\u7cfb\uff0c\u5e76\u53d1\u73b0\u8868\u793a\u8d85\u53e0\u52a0\u662f\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u91cd\u8981\u673a\u5236\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u89c2\u5bdf\u7ed3\u679c\uff0c\u5373\u66f4\u5927\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u2014\u2014\u635f\u5931\u968f\u7740\u6a21\u578b\u5927\u5c0f\u5448\u5e42\u5f8b\u4e0b\u964d\u2014\u2014\u7684\u8d77\u6e90\u4ecd\u7136\u4e0d\u6e05\u695a\u3002", "method": "\u4ece\u4e24\u4e2a\u7ecf\u9a8c\u539f\u7406\u51fa\u53d1\u2014\u2014LLM\u8868\u793a\u6bd4\u5b83\u4eec\u62e5\u6709\u7684\u6a21\u578b\u7ef4\u5ea6\uff08\u5bbd\u5ea6\uff09\u66f4\u591a\u7684\u4e1c\u897f\uff08\u5373\u8868\u793a\u662f\u53e0\u52a0\u7684\uff09\uff0c\u4ee5\u53ca\u8bed\u8a00\u4e2d\u7684\u5355\u8bcd\u6216\u6982\u5ff5\u51fa\u73b0\u7684\u9891\u7387\u4e0d\u540c\u2014\u2014\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\u6765\u7814\u7a76\u635f\u5931\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u7f29\u653e\u5173\u7cfb\u3002", "result": "\u5f53\u53e0\u52a0\u8f83\u5f31\u65f6\uff0c\u635f\u5931\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u7f29\u653e\u53d6\u51b3\u4e8e\u57fa\u7840\u7279\u5f81\u9891\u7387\uff1b\u5982\u679c\u7279\u5f81\u9891\u7387\u9075\u5faa\u5e42\u5f8b\uff0c\u635f\u5931\u4e5f\u662f\u5982\u6b64\u3002\u76f8\u53cd\uff0c\u5728\u5f3a\u53e0\u52a0\u4e0b\uff0c\u635f\u5931\u4e0e\u6a21\u578b\u7ef4\u5ea6\u6210\u53cd\u6bd4\uff0c\u8fd9\u5728\u5e7f\u6cdb\u7684\u7279\u5f81\u9891\u7387\u5206\u5e03\u4e2d\u90fd\u6210\u7acb\u3002", "conclusion": "\u8868\u793a\u8d85\u53e0\u52a0\u662f\u89c2\u5bdf\u5230\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u91cd\u8981\u673a\u5236\u3002\u6211\u4eec\u9884\u8ba1\u8fd9\u4e9b\u89c1\u89e3\u5c06\u6fc0\u53d1\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u548c\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10259", "pdf": "https://arxiv.org/pdf/2505.10259", "abs": "https://arxiv.org/abs/2505.10259", "authors": ["Xiangwen Zhuge", "Xu Shen", "Zeyu Wang", "Fan Dang", "Xuan Ding", "Danyang Li", "Yahui Han", "Tianxiang Hao", "Zheng Yang"], "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices", "categories": ["cs.LG"], "comment": null, "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .", "AI": {"tldr": "This paper proposes SpecOffload, an inference engine that improves GPU utilization and throughput by embedding speculative decoding into offloading.", "motivation": "Efficient LLM inference on resource-constrained devices faces challenges in compute and memory utilization due to limited GPU memory and substantial I/O overhead between CPU and GPU.", "method": "SpecOffload embeds speculative decoding into offloading, unlocking latent GPU resources for storing and executing a draft model used for speculative decoding.", "result": "SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x compared to the best baseline.", "conclusion": "SpecOffload improves GPU core utilization and boosts inference throughput compared to the best baseline."}}
{"id": "2505.10541", "pdf": "https://arxiv.org/pdf/2505.10541", "abs": "https://arxiv.org/abs/2505.10541", "authors": ["Pengfei Wang", "Guohai Xu", "Weinong Wang", "Junjie Yang", "Jie Lou", "Yunhua Xue"], "title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807\u548c\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u53ef\u80fd\u5728\u4e0d\u771f\u6b63\u7406\u89e3\u89c6\u89c9\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e86\u89c6\u89c9\u8f93\u5165\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u5b9a\u4e49\u4e86\u9690\u5f0f\u89c6\u89c9\u8bef\u89e3\uff08IVM\uff09\uff0c\u5373MLLMs\u63d0\u4f9b\u6b63\u786e\u7b54\u6848\u4f46\u5e76\u672a\u5b8c\u5168\u7406\u89e3\u89c6\u89c9\u8f93\u5165\u7684\u60c5\u51b5\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u56e0\u679c\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u7684\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u63ed\u793a\u4e86\u968f\u7740\u7f51\u7edc\u5c42\u6570\u52a0\u6df1\uff0c\u6ce8\u610f\u529b\u5206\u5e03\u9010\u6e10\u96c6\u4e2d\u5728\u4e0e\u6b63\u786e\u7b54\u6848\u76f8\u5173\u7684\u56fe\u50cf\u4e0a\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c1\u89e3\uff0c\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u51c6\u786e\u6027\u6307\u6807\u548c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u91cf\u5316IVM\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u51c6\u786e\u6027\u6307\u6807\u80fd\u591f\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4e14\u5bf9\u4f4d\u7f6e\u504f\u5dee\u5177\u6709\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u66f4\u7ec6\u7c92\u5ea6\u548c\u5355\u6a21\u6001\u573a\u666f\u4e2d\u4e5f\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c3a\u5ea6\u65e0\u5173\u7684\u6307\u6807\uff0c\u5373\u6ce8\u610f\u529b\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u91cf\u5316\u9690\u5f0f\u89c6\u89c9\u8bef\u89e3\uff08IVM\uff09\u3002\u6ce8\u610f\u529b\u51c6\u786e\u6027\u901a\u8fc7\u5185\u90e8\u673a\u5236\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5bf9\u4f4d\u7f6e\u504f\u5dee\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6269\u5c55\u4e86\u65b9\u6cd5\u5230\u66f4\u7ec6\u7c92\u5ea6\uff0c\u5e76\u5728\u5355\u6a21\u6001\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u7a81\u663e\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.10475", "pdf": "https://arxiv.org/pdf/2505.10475", "abs": "https://arxiv.org/abs/2505.10475", "authors": ["Mouxiang Chen", "Binyuan Hui", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Jianling Sun", "Junyang Lin", "Zhongxin Liu"], "title": "Parallel Scaling Law for Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.", "AI": {"tldr": "ParScale \u662f\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7f29\u653e\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u5e76\u884c\u8ba1\u7b97\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5177\u6709\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u4f1a\u5e26\u6765\u663e\u8457\u7684\u7a7a\u95f4\u6216\u65f6\u95f4\u6210\u672c\uff0c\u800c ParScale \u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7f29\u653e\u65b9\u5f0f\u3002", "method": "ParScale \u901a\u8fc7\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u589e\u52a0\u6a21\u578b\u7684\u5e76\u884c\u8ba1\u7b97\u6765\u5b9e\u73b0\u7f29\u653e\uff0c\u4f7f\u7528 P \u79cd\u591a\u6837\u4e14\u53ef\u5b66\u4e60\u7684\u53d8\u6362\u5bf9\u8f93\u5165\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u52a8\u6001\u805a\u5408 P \u4e2a\u8f93\u51fa\u3002", "result": "ParScale \u53ef\u4ee5\u4f7f\u7528\u66f4\u5c11\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u8fbe\u5230\u4e0e\u53c2\u6570\u6269\u5c55\u76f8\u5f53\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u53ef\u4ee5\u5c06\u73b0\u6210\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8f6c\u6362\u4e3a\u5e76\u884c\u6269\u5c55\u6a21\u578b\u3002", "conclusion": "ParScale \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7f29\u653e\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5e76\u4e14\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.09868", "pdf": "https://arxiv.org/pdf/2505.09868", "abs": "https://arxiv.org/abs/2505.09868", "authors": ["Tin Trung Nguyen", "Jiannan Xu", "Phuong-Anh Nguyen-Le", "Jonathan Lazar", "Donald Braman", "Hal Daum\u00e9 III", "Zubin Jelveh"], "title": "Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Despite its U.S. constitutional foundation, the technical ``individual\nfairness'' criterion has not been operationalized in state or federal\nstatutes/regulations. We conduct a human subjects experiment to address this\ngap, evaluating which demographic features are relevant for individual fairness\nevaluation of recidivism risk assessment (RRA) tools. Our analyses conclude\nthat the individual similarity function should consider age and sex, but it\nshould ignore race.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e00\u9879\u4eba\u7c7b\u53d7\u8bd5\u8005\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u54ea\u4e9b\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u5bf9\u4e8e\u91cf\u5211\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u8bc4\u4f30\u662f\u76f8\u5173\u7684\uff0c\u5e76\u5f97\u51fa\u7ed3\u8bba\uff1a\u4e2a\u4f53\u76f8\u4f3c\u6027\u51fd\u6570\u5e94\u8003\u8651\u5e74\u9f84\u548c\u6027\u522b\uff0c\u4f46\u4e0d\u5e94\u8003\u8651\u79cd\u65cf\u3002", "motivation": "\u5c3d\u7ba1\u6709\u7f8e\u56fd\u5baa\u6cd5\u57fa\u7840\uff0c\u4f46\u6280\u672f\u6027\u7684\u201c\u4e2a\u4f53\u516c\u5e73\u201d\u6807\u51c6\u5c1a\u672a\u5728\u5dde\u6216\u8054\u90a6\u6cd5\u89c4\u4e2d\u5b9e\u73b0\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u4eba\u7c7b\u53d7\u8bd5\u8005\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u54ea\u4e9b\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u5bf9\u4e8e\u91cf\u5211\u98ce\u9669\u8bc4\u4f30\uff08RRA\uff09\u5de5\u5177\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u8bc4\u4f30\u662f\u76f8\u5173\u7684\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u5f97\u51fa\u7ed3\u8bba\uff0c\u4e2a\u4f53\u76f8\u4f3c\u6027\u51fd\u6570\u5e94\u8003\u8651\u5e74\u9f84\u548c\u6027\u522b\uff0c\u4f46\u5e94\u5ffd\u7565\u79cd\u65cf\u3002", "conclusion": "\u4e2a\u4f53\u76f8\u4f3c\u6027\u51fd\u6570\u5e94\u8003\u8651\u5e74\u9f84\u548c\u6027\u522b\uff0c\u4f46\u4e0d\u5e94\u8003\u8651\u79cd\u65cf\u3002"}}
{"id": "2505.10262", "pdf": "https://arxiv.org/pdf/2505.10262", "abs": "https://arxiv.org/abs/2505.10262", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Lajos Hanzo"], "title": "Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The charging scheduling problem of Electric Buses (EBs) is investigated based\non Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is\nconceived, where the time horizon includes multiple charging and operating\nperiods in a day, while each period is further divided into multiple time\nsteps. To overcome the challenge of long-range multi-phase planning with sparse\nreward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP\ninto a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical\nDouble Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is\nproposed for simultaneously solving the decision problems arising at different\ntemporal resolutions. As a result, the high-level agent learns an effective\npolicy for prescribing the charging targets for every charging period, while\nthe low-level agent learns an optimal policy for setting the charging power of\nevery time step within a single charging period, with the aim of minimizing the\ncharging costs while meeting the charging target. It is proved that the flat\npolicy constructed by superimposing the optimal high-level policy and the\noptimal low-level policy performs as well as the optimal policy of the original\nMDP. Since jointly learning both levels of policies is challenging due to the\nnon-stationarity of the high-level agent and the sampling inefficiency of the\nlow-level agent, we divide the joint learning process into two phases and\nexploit our new HER algorithm to manipulate the experience replay buffers for\nboth levels of agents. Numerical experiments are performed with the aid of\nreal-world data to evaluate the performance of the proposed algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7535\u52a8\u516c\u4ea4\u8f66\u5145\u7535\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u548c\u65b0\u7684\u7b97\u6cd5\u4f18\u5316\u5145\u7535\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u5316\u5145\u7535\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u957f\u8ddd\u79bb\u591a\u9636\u6bb5\u89c4\u5212\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u6311\u6218\uff0c\u540c\u65f6\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u7684\u5145\u7535\u8c03\u5ea6\u4ee5\u964d\u4f4e\u5145\u7535\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08HDRL\uff09\u65b9\u6cd5\uff0c\u5c06\u539f\u59cbMDP\u5206\u89e3\u4e3a\u4e00\u4e2a\u9ad8\u5c42\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08SMDP\uff09\u548c\u591a\u4e2a\u4f4e\u5c42MDP\uff0c\u5e76\u7ed3\u5408\u4e86\u5206\u5c42\u53cc\u6df1\u5ea6Q\u7f51\u7edc\uff08HDDQN\uff09\u548c\u4e8b\u540e\u7ecf\u9a8c\u56de\u653e\uff08HER\uff09\u7b97\u6cd5\u6765\u89e3\u51b3\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u7684\u51b3\u7b56\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u5145\u7535\u6210\u672c\u5e76\u6ee1\u8db3\u5145\u7535\u76ee\u6807\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9ad8\u9636\u7b56\u7565\u548c\u4f4e\u9636\u7b56\u7565\u53e0\u52a0\uff0c\u6784\u5efa\u7684\u6241\u5e73\u7b56\u7565\u5728\u6027\u80fd\u4e0a\u4e0e\u539f\u59cbMDP\u7684\u6700\u4f18\u7b56\u7565\u76f8\u5f53\u3002"}}
{"id": "2505.10551", "pdf": "https://arxiv.org/pdf/2505.10551", "abs": "https://arxiv.org/abs/2505.10551", "authors": ["Yiwen Liu", "Jessica Bader", "Jae Myung Kim"], "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025", "summary": "With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u751f\u6210\u7528\u4e8eCLIP\u5206\u7c7b\u5668\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u53ef\u884c\u6027\u662f\u5426\u91cd\u8981\uff0c\u5e76\u53d1\u73b0\u5176\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u7814\u7a76\u53ef\u884c\u6027\u662f\u5426\u5728\u751f\u6210\u7528\u4e8eCLIP\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u65f6\u662f\u5fc5\u8981\u7684\uff0c\u7279\u522b\u662f\u9488\u5bf9\u80cc\u666f\u3001\u989c\u8272\u548c\u7eb9\u7406\u8fd9\u4e09\u4e2a\u76ee\u6807\u5c5e\u6027\u3002", "method": "\u5f15\u5165\u4e86VariReal\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u6700\u5c0f\u5730\u7f16\u8f91\u7ed9\u5b9a\u7684\u6e90\u56fe\u50cf\uff0c\u4ee5\u5305\u542b\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u63d0\u793a\u7ed9\u51fa\u7684\u53ef\u884c\u6216\u4e0d\u53ef\u884c\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u53ef\u884c\u6027\u5bf9LoRA\u5fae\u8c03\u7684CLIP\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\uff0c\u4e14\u5728\u4e09\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684top-1\u51c6\u786e\u7387\u5dee\u5f02\u5c0f\u4e8e0.3%\u3002\u6b64\u5916\uff0c\u5c5e\u6027\u5bf9\u53ef\u884c/\u4e0d\u53ef\u884c\u56fe\u50cf\u662f\u5426\u5bf9\u6297\u6027\u5730\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u6709\u5f71\u54cd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u884c\u6027\u5bf9LoRA\u5fae\u8c03\u7684CLIP\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\uff0c\u4e14\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u6df7\u5408\u53ef\u884c\u548c\u4e0d\u53ef\u884c\u56fe\u50cf\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002"}}
{"id": "2505.10495", "pdf": "https://arxiv.org/pdf/2505.10495", "abs": "https://arxiv.org/abs/2505.10495", "authors": ["Vibha Belavadi", "Tushar Vatsa", "Dewang Sultania", "Suhas Suresha", "Ishita Verma", "Cheng Chen", "Tracy Holloway King", "Michael Friedrich"], "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing", "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u7531\u5668\u7684\u67b6\u6784\uff0c\u5229\u7528\u9886\u57df\u8d44\u6e90\u548c\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u5728\u7f3a\u4e4f\u771f\u5b9e\u7528\u6237\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u7684\u6311\u6218\u3002", "motivation": "\u5728\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\u4e2d\uff0c\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8868\u8fbe\u9700\u6c42\uff0c\u9700\u8981\u6620\u5c04\u5230API\u8c03\u7528\u3002\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u548c\u9690\u79c1\u9650\u5236\uff0c\u9700\u8981\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u65e0\u6cd5\u590d\u5236\u771f\u5b9e\u6570\u636e\u5206\u5e03\uff0c\u5bfc\u81f4\u5fae\u8c03\u540e\u7684\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u7531\u5668\u7684\u67b6\u6784\uff0c\u5229\u7528\u9886\u57df\u8d44\u6e90\uff08\u5982\u5185\u5bb9\u5143\u6570\u636e\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff09\u4ee5\u53ca\u6587\u672c\u5230\u6587\u672c\u548c\u89c6\u89c9\u5230\u6587\u672c\u7684\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u4e00\u7ec4\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u51fd\u6570\u5206\u7c7b\u51c6\u786e\u7387\u548cAPI\u53c2\u6570\u9009\u62e9\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2505.10264", "pdf": "https://arxiv.org/pdf/2505.10264", "abs": "https://arxiv.org/abs/2505.10264", "authors": ["Francesco Diana", "Andr\u00e9 Nusser", "Chuan Xu", "Giovanni Neglia"], "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u91cd\u8981\u9650\u5236\uff0c\u4f8b\u5982\u4f9d\u8d56\u4e8e\u5bf9\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u7684\u5047\u8bbe\uff0c\u6216\u8005\u5f53\u6279\u6b21\u5927\u5c0f\u8d85\u8fc7\u51e0\u5341\u4e2a\u6837\u672c\u65f6\u6548\u7387\u663e\u8457\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u5168\u8fde\u63a5\u5c42\u7684\u65b0\u51e0\u4f55\u89c6\u89d2\u6765\u8bbe\u8ba1\u6076\u610f\u6a21\u578b\u53c2\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u4efb\u610f\u5927\u7684\u6570\u636e\u6279\u6b21\u7684\u5b8c\u7f8e\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u6bd4\u73b0\u6709\u6280\u672f\u5927\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u6570\u636e\u6279\u6b21\u7684\u5b8c\u7f8e\u91cd\u5efa\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.10557", "pdf": "https://arxiv.org/pdf/2505.10557", "abs": "https://arxiv.org/abs/2505.10557", "authors": ["Ke Wang", "Junting Pan", "Linda Wei", "Aojun Zhou", "Weikang Shi", "Zimu Lu", "Han Xiao", "Yunqiao Yang", "Houxing Ren", "Mingjie Zhan", "Hongsheng Li"], "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u4ee3\u7801\u4f5c\u4e3a\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u76d1\u7763\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5f00\u53d1\u56fe\u50cf\u5230\u4ee3\u7801\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u5b66\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86MathCoder-VL\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u6570\u636e\u96c6\u4e3b\u8981\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u6570\u5b66\u56fe\u8868\u4e2d\u5bf9\u95ee\u9898\u89e3\u51b3\u81f3\u5173\u91cd\u8981\u7684\u590d\u6742\u7ec6\u8282\uff0c\u963b\u788d\u4e86\u5f53\u524dLMMs\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u5c55\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u5229\u7528\u4ee3\u7801\u4f5c\u4e3a\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u76d1\u7763\uff0c\u56e0\u4e3a\u4ee3\u7801\u672c\u8d28\u4e0a\u5305\u542b\u4e86\u751f\u6210\u76f8\u5e94\u56fe\u5f62\u6240\u9700\u7684\u6240\u6709\u4fe1\u606f\uff0c\u5efa\u7acb\u4e86\u4e24\u79cd\u6a21\u6001\u4e4b\u95f4\u7684\u7cbe\u786e\u8054\u7cfb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u6a21\u578b\u5728\u5faa\u73af\u7684\u65b9\u6cd5\u5171\u540c\u5f00\u53d1\u4e86\u56fe\u50cf\u5230\u4ee3\u7801\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u5f97\u5230\u4e86\u4e00\u4e2a\u56fe\u50cf\u5230\u4ee3\u7801\u7684\u6a21\u578bFigCodifier\u548cImgCode-8.6M\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5927\u7684\u56fe\u50cf-\u4ee3\u7801\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528FigCodifier\u5408\u6210\u65b0\u9896\u7684\u6570\u5b66\u56fe\u5f62\uff0c\u7136\u540e\u6784\u5efaMM-MathInstruct-3M\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u5b66\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MathCoder-VL\uff0c\u5728ImgCode-8.6M\u4e0a\u8bad\u7ec3\u4ee5\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u968f\u540e\u5728MM-MathInstruct-3M\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u8fdb\u884c\u591a\u6a21\u6001\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u6240\u6709\u516d\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u5f00\u6e90SOTA\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u5728MathVista\u7684\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u5b50\u96c6\u4e0a\u8d85\u8fc7\u4e86GPT-4o\u548cClaude 3.5 Sonnet\uff0c\u5206\u522b\u63d0\u9ad8\u4e868.9%\u548c9.2%\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u6240\u6709\u516d\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u5f00\u6e90SOTA\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u5728MathVista\u7684\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u5b50\u96c6\u4e0a\u8d85\u8fc7\u4e86GPT-4o\u548cClaude 3.5 Sonnet\uff0c\u5206\u522b\u63d0\u9ad8\u4e868.9%\u548c9.2%\u3002"}}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526", "abs": "https://arxiv.org/abs/2505.10526", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.", "AI": {"tldr": "MASSV is a method that accelerates vision-language models by transforming small language models into effective multimodal drafters through a two-phase approach, achieving significant speedups in inference.", "motivation": "Applying speculative decoding to vision-language models (VLMs) presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context.", "method": "MASSV transforms existing small language models into effective multimodal drafters through a two-phase approach. It first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions.", "result": "Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks.", "conclusion": "MASSV provides a scalable, architecture-compatible method for accelerating both current and future vision-language models (VLMs)."}}
{"id": "2505.10271", "pdf": "https://arxiv.org/pdf/2505.10271", "abs": "https://arxiv.org/abs/2505.10271", "authors": ["Rafael Pablos Sarabia", "Joachim Nyborg", "Morten Birk", "Jeppe Liborius Sj\u00f8rup", "Anders Lillevang Vesterholt", "Ira Assent"], "title": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u6b27\u6d32\u8fdb\u884c8\u5c0f\u65f6\u8303\u56f4\u5185\u7684\u9ad8\u5206\u8fa8\u7387\u6982\u7387\u964d\u6c34\u9884\u6d4b\uff0c\u8be5\u6a21\u578b\u6709\u6548\u5730\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u6e90\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u7684\u6982\u7387\u56fe\u5b9e\u73b0\u51c6\u786e\u7684\u9884\u6d4b\u548c\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u8d85\u8d8a\u4e86\u73b0\u6709\u7cfb\u7edf\uff0c\u8bbe\u5b9a\u4e86\u65b0\u7684\u6807\u51c6\u3002", "motivation": "\u514b\u670d\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u77ed\u9884\u62a5\u63d0\u524d\u65f6\u95f4\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u6b27\u6d32\u8fdb\u884c8\u5c0f\u65f6\u8303\u56f4\u5185\u7684\u9ad8\u5206\u8fa8\u7387\u6982\u7387\u964d\u6c34\u9884\u6d4b\uff0c\u8be5\u6a21\u578b\u6709\u6548\u5730\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u6e90\uff08\u5305\u62ec\u96f7\u8fbe\u3001\u536b\u661f\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6570\u503c\u5929\u6c14\u9884\u6d4b\uff09\uff0c\u540c\u65f6\u6355\u6349\u957f\u8ddd\u79bb\u4ea4\u4e92\uff0c\u901a\u8fc7\u4e00\u81f4\u7684\u6982\u7387\u56fe\u5b9e\u73b0\u51c6\u786e\u7684\u9884\u6d4b\u548c\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u5f53\u524d\u7684\u64cd\u4f5c\u6027NWP\u7cfb\u7edf\u3001\u57fa\u4e8e\u5916\u63a8\u7684\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u65f6\u9884\u62a5\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u6b27\u6d32\u9ad8\u5206\u8fa8\u7387\u964d\u6c34\u9884\u62a5\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u7684\u6807\u51c6\uff0c\u786e\u4fdd\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u3002"}}
{"id": "2505.10562", "pdf": "https://arxiv.org/pdf/2505.10562", "abs": "https://arxiv.org/abs/2505.10562", "authors": ["Wenxuan Wang", "Fan Zhang", "Yufeng Cui", "Haiwen Diao", "Zhuoyan Luo", "Huchuan Lu", "Jing Liu", "Xinlong Wang"], "title": "End-to-End Vision Tokenizer Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u6807\u8bb0\u4f18\u5316\u65b9\u6cd5ETT\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u89c6\u89c9\u6807\u8bb0\u5316\u548c\u76ee\u6807\u81ea\u56de\u5f52\u4efb\u52a1\u4e4b\u95f4\u7684\u8054\u5408\u4f18\u5316\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u89c6\u89c9\u6807\u8bb0\u5668\u7684\u4f18\u5316\u4e0e\u4e0b\u6e38\u8bad\u7ec3\u5206\u79bb\uff0c\u9690\u542b\u5730\u5047\u8bbe\u89c6\u89c9\u6807\u8bb0\u53ef\u4ee5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u6cdb\u5316\uff0c\u4f8b\u5982\u56fe\u50cf\u751f\u6210\u548c\u89c6\u89c9\u95ee\u7b54\u3002\u7136\u800c\uff0c\u9488\u5bf9\u4f4e\u7ea7\u91cd\u5efa\u4f18\u5316\u7684\u89c6\u89c9\u6807\u8bb0\u5668\u5bf9\u4e8e\u9700\u8981\u4e0d\u540c\u8868\u793a\u548c\u8bed\u4e49\u7684\u4e0b\u6e38\u4efb\u52a1\u662f\u65e0\u5173\u7684\u3002\u8fd9\u79cd\u89e3\u8026\u8303\u5f0f\u5f15\u5165\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u4e0d\u5339\u914d\uff1a\u89c6\u89c9\u6807\u8bb0\u5316\u7684\u635f\u5931\u53ef\u80fd\u6210\u4e3a\u76ee\u6807\u4efb\u52a1\u7684\u8868\u793a\u74f6\u9888\u3002", "method": "ETT\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u6807\u8bb0\u4f18\u5316\u65b9\u6cd5\uff0c\u5b83\u5b9e\u73b0\u4e86\u89c6\u89c9\u6807\u8bb0\u5316\u548c\u76ee\u6807\u81ea\u56de\u5f52\u4efb\u52a1\u4e4b\u95f4\u7684\u8054\u5408\u4f18\u5316\u3002ETT\u5229\u7528\u4e86\u89c6\u89c9\u6807\u8bb0\u5668\u4ee3\u7801\u672c\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u91cd\u5efa\u548c\u6807\u9898\u76ee\u6807\u5bf9\u89c6\u89c9\u6807\u8bb0\u5668\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u6807\u8bb0\u4f18\u5316\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e0a\u76f8\u6bd4\u51bb\u7ed3\u6807\u8bb0\u5668\u57fa\u7ebf\u67092-6%\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u7684\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "ETT\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u589e\u5f3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u56fe\u50cf\u751f\u6210\u548c\u7406\u89e3\u3002"}}
{"id": "2505.10272", "pdf": "https://arxiv.org/pdf/2505.10272", "abs": "https://arxiv.org/abs/2505.10272", "authors": ["Niklas Dexheimer", "Sascha Gaudlitz", "Johannes Schmidt-Hieber"], "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Hebbian learning is a key principle underlying learning in biological neural\nnetworks. It postulates that synaptic changes occur locally, depending on the\nactivities of pre- and postsynaptic neurons. While Hebbian learning based on\nneuronal firing rates is well explored, much less is known about learning rules\nthat account for precise spike-timing. We relate a Hebbian\nspike-timing-dependent plasticity rule to noisy gradient descent with respect\nto a natural loss function on the probability simplex. This connection allows\nus to prove that the learning rule eventually identifies the presynaptic neuron\nwith the highest activity. We also discover an intrinsic connection to noisy\nmirror descent.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u7cbe\u786e\u5c16\u5cf0\u65f6\u95f4\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u5e76\u5c06\u5176\u4e0e\u566a\u58f0\u68af\u5ea6\u4e0b\u964d\u548c\u566a\u58f0\u955c\u9762\u4e0b\u964d\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u4e86\u8be5\u89c4\u5219\u53ef\u4ee5\u8bc6\u522b\u51fa\u6d3b\u52a8\u6700\u9ad8\u7684\u524d\u7a81\u89e6\u795e\u7ecf\u5143\u3002", "motivation": "\u4e86\u89e3\u57fa\u4e8e\u7cbe\u786e\u5c16\u5cf0\u65f6\u95f4\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u800c\u4e0d\u662f\u57fa\u4e8e\u795e\u7ecf\u5143\u653e\u7535\u7387\u7684\u5df2\u77e5\u89c4\u5219\u3002", "method": "\u901a\u8fc7\u5c06Hebbian\u7684\u5c16\u5cf0\u65f6\u95f4\u4f9d\u8d56\u53ef\u5851\u6027\u89c4\u5219\u4e0e\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u7684\u81ea\u7136\u635f\u5931\u51fd\u6570\u7684\u566a\u58f0\u68af\u5ea6\u4e0b\u964d\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002", "result": "\u8be5\u5b66\u4e60\u89c4\u5219\u6700\u7ec8\u80fd\u591f\u8bc6\u522b\u51fa\u6d3b\u52a8\u6700\u9ad8\u7684\u524d\u7a81\u89e6\u795e\u7ecf\u5143\uff0c\u5e76\u4e14\u4e0e\u566a\u58f0\u955c\u9762\u4e0b\u964d\u6709\u5185\u5728\u8054\u7cfb\u3002", "conclusion": "\u8be5\u5b66\u4e60\u89c4\u5219\u6700\u7ec8\u80fd\u591f\u8bc6\u522b\u51fa\u6d3b\u52a8\u6700\u9ad8\u7684\u524d\u7a81\u89e6\u795e\u7ecf\u5143\u3002"}}
{"id": "2505.10565", "pdf": "https://arxiv.org/pdf/2505.10565", "abs": "https://arxiv.org/abs/2505.10565", "authors": ["Zehan Wang", "Siyu Chen", "Lihe Yang", "Jialei Wang", "Ziang Zhang", "Hengshuang Zhao", "Zhou Zhao"], "title": "Depth Anything with Any Prior", "categories": ["cs.CV"], "comment": "Home page: https://prior-depth-anything.github.io/", "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u5b8c\u6574\u4f46\u7cbe\u786e\u7684\u5ea6\u91cf\u4fe1\u606f\u548c\u76f8\u5bf9\u5b8c\u6574\u7684\u51e0\u4f55\u7ed3\u6784\u7684\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u51c6\u786e\u3001\u5bc6\u96c6\u4e14\u8be6\u7ec6\u7684\u5ea6\u91cf\u6df1\u5ea6\u56fe\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u4f46\u7cbe\u786e\u7684\u5ea6\u91cf\u4fe1\u606f\u548c\u76f8\u5bf9\u5b8c\u6574\u7684\u51e0\u4f55\u7ed3\u6784\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ece\u7c97\u5230\u7ec6\u7684\u6d41\u7a0b\uff0c\u9010\u6b65\u6574\u5408\u4e24\u79cd\u4e92\u8865\u7684\u6df1\u5ea6\u6e90\u3002\u9996\u5148\uff0c\u5f15\u5165\u50cf\u7d20\u7ea7\u5ea6\u91cf\u5bf9\u9f50\u548c\u8ddd\u79bb\u611f\u77e5\u52a0\u6743\u6765\u9884\u586b\u5145\u591a\u6837\u5316\u7684\u5ea6\u91cf\u5148\u9a8c\u3002\u5176\u6b21\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u6a21\u578b\u6765\u7ec6\u5316\u6df1\u5ea6\u5148\u9a8c\u7684\u56fa\u6709\u566a\u58f0\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u57287\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u672a\u89c1\u8fc7\u7684\u6df7\u5408\u5148\u9a8c\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Prior Depth Anything\u6846\u67b6\u80fd\u591f\u751f\u6210\u51c6\u786e\u3001\u5bc6\u96c6\u4e14\u8be6\u7ec6\u7684\u5ea6\u91cf\u6df1\u5ea6\u56fe\uff0c\u5c55\u793a\u4e86\u5728\u6df1\u5ea6\u8865\u5168\u3001\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u51fa\u8272\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002"}}
{"id": "2505.10296", "pdf": "https://arxiv.org/pdf/2505.10296", "abs": "https://arxiv.org/abs/2505.10296", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Dusit Niyato"], "title": "Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The growing adoption of Electric Buses (EBs) represents a significant step\ntoward sustainable development. By utilizing Internet of Things (IoT) systems,\ncharging stations can autonomously determine charging schedules based on\nreal-time data. However, optimizing EB charging schedules remains a critical\nchallenge due to uncertainties in travel time, energy consumption, and\nfluctuating electricity prices. Moreover, to address real-world complexities,\ncharging policies must make decisions efficiently across multiple time scales\nand remain scalable for large EB fleets. In this paper, we propose a\nHierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the\noriginal Markov Decision Process (MDP) into two augmented MDPs. To solve these\nMDPs and enable multi-timescale decision-making, we introduce a novel HDRL\nalgorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization\nEnhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic\n(DAC) algorithm for large-scale EB fleets are addressed through enhancements at\nboth decision levels. At the high level, we redesign the decentralized actor\nnetwork and integrate an attention mechanism to extract relevant global state\ninformation for each EB, decreasing the size of neural networks. At the low\nlevel, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is\nincorporated into the DAC framework, enabling decentralized and coordinated\ncharging power decisions, reducing computational complexity and enhancing\nconvergence speed. Extensive experiments with real-world data demonstrate the\nsuperior performance and scalability of DAC-MAPPO-E in optimizing EB fleet\ncharging schedules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5DAC-MAPPO-E\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u961f\u7684\u5145\u7535\u8ba1\u5212\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u7531\u4e8e\u65c5\u884c\u65f6\u95f4\u3001\u80fd\u8017\u548c\u7535\u4ef7\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\uff08EBs\uff09\u7684\u5145\u7535\u8ba1\u5212\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u5145\u7535\u653f\u7b56\u5fc5\u987b\u5728\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u9ad8\u6548\u51b3\u7b56\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5927\u89c4\u6a21\u7684EB\u8f66\u961f\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08HDRL\uff09\u65b9\u6cd5\uff0c\u5c06\u539f\u59cb\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e24\u4e2a\u589e\u5f3a\u7684MDP\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684HDRL\u7b97\u6cd5\uff0c\u5373\u53cc\u6f14\u5458-\u8bc4\u8bba\u5bb6\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u589e\u5f3a\uff08DAC-MAPPO-E\uff09\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86DAC-MAPPO-E\u5728\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u961f\u5145\u7535\u8ba1\u5212\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDAC-MAPPO-E\u5728\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u961f\u7684\u5145\u7535\u8ba1\u5212\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.10566", "pdf": "https://arxiv.org/pdf/2505.10566", "abs": "https://arxiv.org/abs/2505.10566", "authors": ["Yen-Chi Cheng", "Krishna Kumar Singh", "Jae Shin Yoon", "Alex Schwing", "Liangyan Gui", "Matheus Gadelha", "Paul Guerrero", "Nanxuan Zhao"], "title": "3D-Fixup: Advancing Photo Editing with 3D Priors", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. Project page: https://3dfixup.github.io/", "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/", "AI": {"tldr": "3D-Fixup\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u76843D\u5148\u9a8c\u77e5\u8bc6\u5f15\u5bfc2D\u56fe\u50cf\u7f16\u8f91\u3002\u5b83\u5229\u7528\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u501f\u52a9\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002\u6211\u4eec\u8f6c\u5411\u89c6\u9891\u6570\u636e\u4ee5\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5bf9\uff0c\u5373\u6e90\u5e27\u548c\u76ee\u6807\u5e27\u3002\u6211\u4eec\u7ed3\u5408\u6765\u81eaImage-to-3D\u6a21\u578b\u76843D\u6307\u5bfc\uff0c\u901a\u8fc7\u5c062D\u4fe1\u606f\u663e\u5f0f\u6295\u5f71\u52303D\u7a7a\u95f4\u6765\u89e3\u51b3\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b3D\u5148\u9a8c\u77e5\u8bc6\uff0c3D-Fixup\u80fd\u591f\u6709\u6548\u652f\u6301\u590d\u6742\u7684\u3001\u8eab\u4efd\u4e00\u81f4\u76843D\u611f\u77e5\u7f16\u8f91\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u5e76\u63a8\u52a8\u6269\u6563\u6a21\u578b\u5728\u73b0\u5b9e\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5efa\u6a21\u56fe\u50cf\u5148\u9a8c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f463D\u611f\u77e5\u56fe\u50cf\u7f16\u8f91\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5bf9\u8c61\u4ec5\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u6307\u5b9a\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e863D-Fixup\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5b66\u4e60\u76843D\u5148\u9a8c\u77e5\u8bc6\u5f15\u5bfc2D\u56fe\u50cf\u7f16\u8f91\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b63D-Fixup\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u76843D\u5148\u9a8c\u77e5\u8bc6\u5f15\u5bfc2D\u56fe\u50cf\u7f16\u8f91\u3002\u6211\u4eec\u5229\u7528\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u501f\u52a9\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002\u6211\u4eec\u8f6c\u5411\u89c6\u9891\u6570\u636e\u4ee5\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5bf9\uff0c\u5373\u6e90\u5e27\u548c\u76ee\u6807\u5e27\u3002\u6211\u4eec\u7ed3\u5408\u6765\u81eaImage-to-3D\u6a21\u578b\u76843D\u6307\u5bfc\uff0c\u901a\u8fc7\u5c062D\u4fe1\u606f\u663e\u5f0f\u6295\u5f71\u52303D\u7a7a\u95f4\u6765\u89e3\u51b3\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6570\u636e\u751f\u6210\u7ba1\u9053\u4ee5\u786e\u4fdd\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9ad8\u8d28\u91cf\u76843D\u6307\u5bfc\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b3D\u5148\u9a8c\u77e5\u8bc6\uff0c3D-Fixup\u80fd\u591f\u6709\u6548\u652f\u6301\u590d\u6742\u7684\u3001\u8eab\u4efd\u4e00\u81f4\u76843D\u611f\u77e5\u7f16\u8f91\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u5e76\u63a8\u52a8\u6269\u6563\u6a21\u578b\u5728\u73b0\u5b9e\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b3D\u5148\u9a8c\u77e5\u8bc6\uff0c3D-Fixup\u80fd\u591f\u6709\u6548\u652f\u6301\u590d\u6742\u7684\u3001\u8eab\u4efd\u4e00\u81f4\u76843D\u611f\u77e5\u7f16\u8f91\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u5e76\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u73b0\u5b9e\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2505.10297", "pdf": "https://arxiv.org/pdf/2505.10297", "abs": "https://arxiv.org/abs/2505.10297", "authors": ["Chibueze Peace Obioma", "Youcheng Sun", "Mustafa A. Mustafa"], "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Submitted to ESORICS 2025", "summary": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFeRA\u7684\u65b0\u578b\u8054\u90a6\u4ee3\u8868\u6ce8\u610f\u529b\u9632\u5fa1\u673a\u5236\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u62b5\u5fa1\u540e\u95e8\u653b\u51fb\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u5f02\u6784\u6027\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\uff0c\u4f7f\u5f97\u68c0\u6d4b\u540e\u95e8\u653b\u51fb\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "FeRA\u5229\u7528\u8de8\u5ba2\u6237\u7aef\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5185\u90e8\u7279\u5f81\u8868\u793a\u6765\u533a\u5206\u826f\u6027\u5ba2\u6237\u7aef\u548c\u6076\u610f\u5ba2\u6237\u7aef\uff0c\u5e76\u57fa\u4e8e\u8868\u793a\u91cd\u6784\u8bef\u5dee\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "FeRA\u5728\u5404\u79cd\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u540e\u95e8\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e3b\u4efb\u52a1\u7684\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "FeRA\u662f\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u5728\u5404\u79cd\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u5e76\u6709\u6548\u964d\u4f4e\u540e\u95e8\u653b\u51fb\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2505.09630", "pdf": "https://arxiv.org/pdf/2505.09630", "abs": "https://arxiv.org/abs/2505.09630", "authors": ["Tien Comlekoglu", "J. Quetzalc\u00f3atl Toledo-Mar\u00edn", "Douglas W. DeSimone", "Shayn M. Peirce", "Geoffrey Fox", "James A. Glazier"], "title": "Generative diffusion model surrogates for mechanistic agent-based biological models", "categories": ["q-bio.QM", "cs.CV", "cs.ET", "cs.PF"], "comment": null, "summary": "Mechanistic, multicellular, agent-based models are commonly used to\ninvestigate tissue, organ, and organism-scale biology at single-cell\nresolution. The Cellular-Potts Model (CPM) is a powerful and popular framework\nfor developing and interrogating these models. CPMs become computationally\nexpensive at large space- and time- scales making application and investigation\nof developed models difficult. Surrogate models may allow for the accelerated\nevaluation of CPMs of complex biological systems. However, the stochastic\nnature of these models means each set of parameters may give rise to different\nmodel configurations, complicating surrogate model development. In this work,\nwe leverage denoising diffusion probabilistic models to train a generative AI\nsurrogate of a CPM used to investigate \\textit{in vitro} vasculogenesis. We\ndescribe the use of an image classifier to learn the characteristics that\ndefine unique areas of a 2-dimensional parameter space. We then apply this\nclassifier to aid in surrogate model selection and verification. Our CPM model\nsurrogate generates model configurations 20,000 timesteps ahead of a reference\nconfiguration and demonstrates approximately a 22x reduction in computational\ntime as compared to native code execution. Our work represents a step towards\nthe implementation of DDPMs to develop digital twins of stochastic biological\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u6765\u8bad\u7ec3\u751f\u6210AI\u4ee3\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901f\u7ec6\u80de-\u9648\u6a21\u578b\uff08CPM\uff09\u7684\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u56fe\u50cf\u5206\u7c7b\u5668\u5b66\u4e60\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u7279\u5f81\uff0c\u5e76\u7528\u4e8e\u4ee3\u7406\u6a21\u578b\u7684\u9009\u62e9\u548c\u9a8c\u8bc1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u4ee3\u7406\u6a21\u578b\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "CPM\u5728\u5927\u89c4\u6a21\u7a7a\u95f4\u548c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f7f\u5f97\u5e94\u7528\u548c\u7814\u7a76\u5f00\u53d1\u7684\u6a21\u578b\u53d8\u5f97\u56f0\u96be\u3002\u4ee3\u7406\u6a21\u578b\u53ef\u80fd\u5141\u8bb8\u52a0\u901f\u8bc4\u4f30\u590d\u6742\u7684\u751f\u7269\u7cfb\u7edfCPM\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u968f\u673a\u6027\u610f\u5473\u7740\u6bcf\u7ec4\u53c2\u6570\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u6a21\u578b\u914d\u7f6e\uff0c\u8fd9\u4f7f\u5f97\u4ee3\u7406\u6a21\u578b\u7684\u5f00\u53d1\u53d8\u5f97\u590d\u6742\u3002", "method": "\u6211\u4eec\u5229\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u6765\u8bad\u7ec3\u4e00\u4e2a\u7528\u4e8e\u7814\u7a76\u4f53\u5916\u8840\u7ba1\u751f\u6210\u7684CPM\u7684\u751f\u6210AI\u4ee3\u7406\u6a21\u578b\u3002\u6211\u4eec\u63cf\u8ff0\u4e86\u4f7f\u7528\u56fe\u50cf\u5206\u7c7b\u5668\u6765\u5b66\u4e60\u5b9a\u4e49\u4e8c\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u72ec\u7279\u533a\u57df\u7684\u7279\u5f81\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6b64\u5206\u7c7b\u5668\u7528\u4e8e\u8f85\u52a9\u4ee3\u7406\u6a21\u578b\u7684\u9009\u62e9\u548c\u9a8c\u8bc1\u3002", "result": "\u6211\u4eec\u7684CPM\u6a21\u578b\u4ee3\u7406\u53ef\u4ee5\u751f\u6210\u6bd4\u53c2\u8003\u914d\u7f6e\u63d0\u524d20,000\u4e2a\u65f6\u95f4\u6b65\u7684\u6a21\u578b\u914d\u7f6e\uff0c\u5e76\u4e14\u4e0e\u539f\u751f\u4ee3\u7801\u6267\u884c\u76f8\u6bd4\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea622\u500d\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4ee3\u8868\u4e86\u5c06DDPM\u5e94\u7528\u4e8e\u5f00\u53d1\u968f\u673a\u751f\u7269\u7cfb\u7edf\u7684\u6570\u5b57\u5b6a\u751f\u4f53\u7684\u4e00\u4e2a\u6b65\u9aa4\u3002"}}
{"id": "2505.10307", "pdf": "https://arxiv.org/pdf/2505.10307", "abs": "https://arxiv.org/abs/2505.10307", "authors": ["Yiyang Zhao", "Chengpei Wu", "Lilin Zhang", "Ning Yang"], "title": "Negative Metric Learning for Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Graph contrastive learning (GCL) often suffers from false negatives, which\ndegrades the performance on downstream tasks. The existing methods addressing\nthe false negative issue usually rely on human prior knowledge, still leading\nGCL to suboptimal results. In this paper, we propose a novel Negative Metric\nLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative\nMetric Network (NMN) to build a negative metric space, in which false negatives\ncan be distinguished better from true negatives based on their distance to\nanchor node. To overcome the lack of explicit supervision signals for NML, we\npropose a joint training scheme with bi-level optimization objective, which\nimplicitly utilizes the self-supervision signals to iteratively optimize the\nencoder and the negative metric network. The solid theoretical analysis and the\nextensive experiments conducted on widely used benchmarks verify the\nsuperiority of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1f\u5ea6\u91cf\u5b66\u4e60\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5NML-GCL\uff0c\u901a\u8fc7\u6784\u5efa\u8d1f\u5ea6\u91cf\u7a7a\u95f4\u548c\u8054\u5408\u8bad\u7ec3\u65b9\u6848\u6765\u89e3\u51b3\u5047\u8d1f\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u51b3\u5047\u8d1f\u95ee\u9898\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4eba\u5de5\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4GCL\u7684\u7ed3\u679c\u4ecd\u4e0d\u7406\u60f3\u3002", "method": "NML-GCL\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u8d1f\u5ea6\u91cf\u7f51\u7edc\uff08NMN\uff09\u6784\u5efa\u8d1f\u5ea6\u91cf\u7a7a\u95f4\uff0c\u4ee5\u66f4\u597d\u5730\u533a\u5206\u5047\u8d1f\u6837\u672c\u548c\u771f\u8d1f\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u76ee\u6807\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6848\u6765\u4f18\u5316\u7f16\u7801\u5668\u548c\u8d1f\u5ea6\u91cf\u7f51\u7edc\u3002", "result": "NML-GCL\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5f97\u5230\u4e86\u63d0\u5347\uff0c\u4e14\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "NML-GCL\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.09819", "pdf": "https://arxiv.org/pdf/2505.09819", "abs": "https://arxiv.org/abs/2505.09819", "authors": ["Ruichen Yang", "Gy\u00f6rgy M. L\u00e9vay", "Christopher L. Hunt", "D\u00e1niel Czeiner", "Megan C. Hodgson", "Damini Agarwal", "Rahul R. Kaliki", "Nitish V. Thakor"], "title": "Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "State-of-the-art upper limb myoelectric prostheses often use pattern\nrecognition (PR) control systems that translate electromyography (EMG) signals\ninto desired movements. As prosthesis movement complexity increases, users\noften struggle to produce sufficiently distinct EMG patterns for reliable\nclassification. Existing training typically involves heuristic, trial-and-error\nuser adjustments to static decoder boundaries. Goal: We introduce the Reviewer,\na 3D visual interface projecting EMG signals directly into the decoder's\nclassification space, providing intuitive, real-time insight into PR algorithm\nbehavior. This structured feedback reduces cognitive load and fosters mutual,\ndata-driven adaptation between user-generated EMG patterns and decoder\nboundaries. Methods: A 10-session study with 12 able-bodied participants\ncompared PR performance after motor-based training and updating using the\nReviewer versus conventional virtual arm visualization. Performance was\nassessed using a Fitts law task that involved the aperture of the cursor and\nthe control of orientation. Results: Participants trained with the Reviewer\nachieved higher completion rates, reduced overshoot, and improved path\nefficiency and throughput compared to the standard visualization group.\nSignificance: The Reviewer introduces decoder-informed motor training,\nfacilitating immediate and consistent PR-based myoelectric control\nimprovements. By iteratively refining control through real-time feedback, this\napproach reduces reliance on trial-and-error recalibration, enabling a more\nadaptive, self-correcting training framework. Conclusion: The 3D visual\nfeedback significantly improves PR control in novice operators through\nstructured training, enabling feedback-driven adaptation and reducing reliance\non extensive heuristic adjustments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aReviewer\u76843D\u53ef\u89c6\u5316\u754c\u9762\uff0c\u7528\u4e8e\u5b9e\u65f6\u63d0\u4f9bPR\u7b97\u6cd5\u884c\u4e3a\u7684\u76f4\u89c2\u89c1\u89e3\uff0c\u4ece\u800c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u5e76\u4fc3\u8fdb\u7528\u6237\u751f\u6210\u7684EMG\u6a21\u5f0f\u548c\u89e3\u7801\u5668\u8fb9\u754c\u4e4b\u95f4\u7684\u76f8\u4e92\u6570\u636e\u9a71\u52a8\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u7684\u8bad\u7ec3\u901a\u5e38\u6d89\u53ca\u9759\u6001\u89e3\u7801\u5668\u8fb9\u754c\u7684\u624b\u52a8\u3001\u8bd5\u9519\u7528\u6237\u8c03\u6574\uff0c\u968f\u7740\u5047\u80a2\u8fd0\u52a8\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u7528\u6237\u5f80\u5f80\u96be\u4ee5\u4ea7\u751f\u8db3\u591f\u4e0d\u540c\u7684EMG\u6a21\u5f0f\u4ee5\u8fdb\u884c\u53ef\u9760\u5206\u7c7b\u3002", "method": "\u4e00\u987910\u6b21\u4f1a\u8bdd\u7684\u7814\u7a76\uff0c\u6d89\u53ca12\u540d\u8eab\u4f53\u5065\u5168\u7684\u53c2\u4e0e\u8005\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u8fd0\u52a8\u7684\u8bad\u7ec3\u548c\u4f7f\u7528Reviewer\u66f4\u65b0\u4e0e\u4f20\u7edf\u865a\u62df\u624b\u81c2\u53ef\u89c6\u5316\u540e\u7684PR\u6027\u80fd\u3002", "result": "\u4f7f\u7528Reviewer\u8bad\u7ec3\u7684\u53c2\u4e0e\u8005\u5728\u5b8c\u6210\u7387\u3001\u51cf\u5c11\u8d85\u8c03\u4ee5\u53ca\u63d0\u9ad8\u8def\u5f84\u6548\u7387\u548c\u541e\u5410\u91cf\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u53ef\u89c6\u5316\u7ec4\u3002", "conclusion": "3D\u89c6\u89c9\u53cd\u9988\u901a\u8fc7\u7ed3\u6784\u5316\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u65b0\u624b\u64cd\u4f5c\u5458\u7684PR\u63a7\u5236\uff0c\u4f7f\u53cd\u9988\u9a71\u52a8\u7684\u9002\u5e94\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u542f\u53d1\u5f0f\u8c03\u6574\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.10322", "pdf": "https://arxiv.org/pdf/2505.10322", "abs": "https://arxiv.org/abs/2505.10322", "authors": ["Yijie Zhou", "Shi Pu"], "title": "Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5f02\u6b65\u53bb\u4e2d\u5fc3\u5316\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff08ADSGD\uff09\uff0c\u5e76\u5728\u5b9e\u9645\u5047\u8bbe\u4e0b\u5206\u6790\u4e86\u5176\u6536\u655b\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cADSGD \u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u5899\u949f\u6536\u655b\u65f6\u95f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u5728\u5229\u7528\u5206\u5e03\u5f0f\u6570\u636e\u7684\u540c\u65f6\u907f\u514d\u4e2d\u592e\u63a7\u5236\uff0c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u6027\u3002\u7136\u800c\uff0c\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u7531\u4e8e\u8ba1\u7b97\u901f\u5ea6\u5f02\u6784\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u901a\u4fe1\u5ef6\u8fdf\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u5f02\u6b65\u53bb\u4e2d\u5fc3\u5316\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08ADSGD\uff09\u6a21\u578b\uff0c\u5728\u5b9e\u9645\u5047\u8bbe\u4e0b\u5206\u6790\u4e86\u5176\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cADSGD \u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u5899\u949f\u6536\u655b\u65f6\u95f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ADSGD \u662f\u4e00\u79cd\u9002\u7528\u4e8e\u5b9e\u9645\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u7b97\u6cd5\uff0c\u5177\u6709\u7b80\u5355\u6027\u3001\u5185\u5b58\u548c\u901a\u4fe1\u6548\u7387\u4ee5\u53ca\u5bf9\u901a\u4fe1\u548c\u8ba1\u7b97\u5ef6\u8fdf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.09831", "pdf": "https://arxiv.org/pdf/2505.09831", "abs": "https://arxiv.org/abs/2505.09831", "authors": ["Tushar Kataria", "Beatrice Knudsen", "Shireen Y. Elhabian"], "title": "ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hematoxylin and eosin (H&E) staining is a gold standard for microscopic\ndiagnosis in pathology. However, H&E staining does not capture all the\ndiagnostic information that may be needed. To obtain additional molecular\ninformation, immunohistochemical (IHC) stains highlight proteins that mark\nspecific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.\nWhile IHC stains are vital for prognosis and treatment guidance, they are\ntypically only available at specialized centers and time consuming to acquire,\nleading to treatment delays for patients. Virtual staining, enabled by deep\nlearning-based image translation models, provides a promising alternative by\ncomputationally generating IHC stains from H&E stained images. Although many\nGAN and diffusion based image to image (I2I) translation methods have been used\nfor virtual staining, these models treat image patches as independent data\npoints, which results in increased and more diverse data requirements for\neffective generation. We present ImplicitStainer, a novel approach that\nleverages local implicit functions to improve image translation, specifically\nvirtual staining performance, by focusing on pixel-level predictions. This\nmethod enhances robustness to variations in dataset sizes, delivering\nhigh-quality results even with limited data. We validate our approach on two\ndatasets using a comprehensive set of metrics and benchmark it against over\nfifteen state-of-the-art GAN- and diffusion based models. Full Code and models\ntrained will be released publicly via Github upon acceptance.", "AI": {"tldr": "ImplicitStainer\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u9690\u51fd\u6570\u63d0\u5347\u865a\u62df\u67d3\u8272\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "motivation": "H&E\u67d3\u8272\u65e0\u6cd5\u6355\u6349\u6240\u6709\u8bca\u65ad\u4fe1\u606f\uff0c\u800cIHC\u67d3\u8272\u867d\u7136\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u53ea\u5728\u4e13\u4e1a\u4e2d\u5fc3\u53ef\u7528\u4e14\u8017\u65f6\uff0c\u5bfc\u81f4\u60a3\u8005\u6cbb\u7597\u5ef6\u8fdf\u3002\u865a\u62df\u67d3\u8272\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u8ba1\u7b97\u751f\u6210IHC\u67d3\u8272\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u548c\u591a\u6837\u5316\u7684\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "ImplicitStainer\u5229\u7528\u5c40\u90e8\u9690\u51fd\u6570\u6765\u6539\u8fdb\u56fe\u50cf\u7ffb\u8bd1\uff0c\u4e13\u6ce8\u4e8e\u50cf\u7d20\u7ea7\u9884\u6d4b\uff0c\u4ee5\u63d0\u9ad8\u865a\u62df\u67d3\u8272\u6027\u80fd\u3002", "result": "ImplicitStainer\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u8d85\u8fc7\u5341\u4e94\u79cd\u6700\u5148\u8fdb\u7684GAN\u548c\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6709\u9650\u6570\u636e\u4e0b\u7684\u9ad8\u8d28\u91cf\u7ed3\u679c\u548c\u5bf9\u6570\u636e\u96c6\u5927\u5c0f\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ImplicitStainer\u662f\u4e00\u79cd\u6539\u8fdb\u56fe\u50cf\u7ffb\u8bd1\uff0c\u7279\u522b\u662f\u865a\u62df\u67d3\u8272\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u6ce8\u50cf\u7d20\u7ea7\u9884\u6d4b\u6765\u63d0\u9ad8\u5bf9\u6570\u636e\u96c6\u5927\u5c0f\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6709\u9650\u6570\u636e\u4e0b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002"}}
{"id": "2505.10325", "pdf": "https://arxiv.org/pdf/2505.10325", "abs": "https://arxiv.org/abs/2505.10325", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "AI": {"tldr": "ALERT\u662f\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u7279\u5f81\u5206\u5e03\u53d8\u5316\u5e76\u89e6\u53d1\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u65e0\u7ebf\u7f51\u7edc\u7684\u4e24\u4e2a\u7528\u4f8b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u7279\u5f81\u5206\u5e03\u7684\u53d8\u5316\u53ef\u80fd\u4f1a\u964d\u4f4eAI\u6a21\u578b\u7684\u6027\u80fd\u5e76\u5bfc\u81f4\u4e0d\u826f\u884c\u4e3a\u3002\u4e3a\u4e86\u5e94\u5bf9\u672a\u88ab\u68c0\u6d4b\u5230\u7684\u6a21\u578b\u9000\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ALERT\u3002", "method": "ALERT\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u8868\u793a\u5b66\u4e60\u3001\u7edf\u8ba1\u6d4b\u8bd5\u548c\u6548\u7528\u8bc4\u4f30\u3002\u8868\u793a\u5b66\u4e60\u4f7f\u7528MLP\uff0c\u7edf\u8ba1\u6d4b\u8bd5\u4f7f\u7528Kolmogorov-Smirnov\u548cPopulation Stability Index\u6d4b\u8bd5\uff0c\u6548\u7528\u8bc4\u4f30\u4f7f\u7528\u65b0\u51fd\u6570\u3002", "result": "ALERT\u5728\u4e24\u4e2a\u65e0\u7ebf\u7f51\u7edc\u7528\u4f8b\uff08\u65e0\u7ebf\u6307\u7eb9\u8bc6\u522b\u548c\u94fe\u8def\u5f02\u5e38\u68c0\u6d4b\uff09\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u6587\u732e\u4e2d\u5341\u79cd\u6807\u51c6\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "ALERT\u662f\u4e00\u79cd\u6709\u6548\u68c0\u6d4b\u7279\u5f81\u5206\u5e03\u53d8\u5316\u5e76\u89e6\u53d1\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u65e0\u7ebf\u7f51\u7edc\u7684\u4e24\u4e2a\u7528\u4f8b\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2505.09985", "pdf": "https://arxiv.org/pdf/2505.09985", "abs": "https://arxiv.org/abs/2505.09985", "authors": ["Pengfei Yu", "Bin Huang", "Minghui Zhang", "Weiwen Wu", "Shaoyu Wang", "Qiegen Liu"], "title": "Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Score-based diffusion models have shown significant promise in the field of\nsparse-view CT reconstruction. However, the projection dataset is large and\nriddled with redundancy. Consequently, applying the diffusion model to\nunprocessed data results in lower learning effectiveness and higher learning\ndifficulty, frequently leading to reconstructed images that lack fine details.\nTo address these issues, we propose the ordered-subsets multi-diffusion model\n(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT\nprojection data into equal subsets and employs multi-subsets diffusion model\n(MSDM) to learn from each subset independently. This targeted learning approach\nreduces complexity and enhances the reconstruction of fine details.\nFurthermore, the integration of one-whole diffusion model (OWDM) with complete\nsinogram data acts as a global information constraint, which can reduce the\npossibility of generating erroneous or inconsistent sinogram information.\nMoreover, the OSMM's unsupervised learning framework provides strong robustness\nand generalizability, adapting seamlessly to varying sparsity levels of CT\nsinograms. This ensures consistent and reliable performance across different\nclinical scenarios. Experimental results demonstrate that OSMM outperforms\ntraditional diffusion models in terms of image quality and noise resilience,\noffering a powerful and versatile solution for advanced CT imaging in\nsparse-view scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u65b9\u6cd5\u2014\u2014\u6709\u5e8f\u5b50\u96c6\u591a\u6269\u6563\u6a21\u578b\uff08OSMM\uff09\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5212\u5206\u4e3a\u5b50\u96c6\u5e76\u7ed3\u5408\u5168\u5c40\u4fe1\u606f\u7ea6\u675f\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u672a\u5904\u7406\u7684\u6570\u636e\u65f6\u5b66\u4e60\u6548\u679c\u8f83\u5dee\uff0c\u96be\u4ee5\u91cd\u5efa\u51fa\u7cbe\u7ec6\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u6709\u5e8f\u5b50\u96c6\u591a\u6269\u6563\u6a21\u578b\uff08OSMM\uff09\uff0c\u8be5\u6a21\u578b\u521b\u65b0\u6027\u5730\u5c06CT\u6295\u5f71\u6570\u636e\u5206\u4e3a\u76f8\u7b49\u7684\u5b50\u96c6\uff0c\u5e76\u4f7f\u7528\u591a\u5b50\u96c6\u6269\u6563\u6a21\u578b\uff08MSDM\uff09\u72ec\u7acb\u5b66\u4e60\u6bcf\u4e2a\u5b50\u96c6\u3002\u6b64\u5916\uff0c\u8fd8\u5c06\u4e00\u4e2a\u5b8c\u6574\u7684\u4e00\u4f53\u5316\u6269\u6563\u6a21\u578b\uff08OWDM\uff09\u4e0e\u5b8c\u6574\u7684sinogram\u6570\u636e\u96c6\u6210\uff0c\u4f5c\u4e3a\u5168\u5c40\u4fe1\u606f\u7ea6\u675f\uff0c\u51cf\u5c11\u751f\u6210\u9519\u8bef\u6216\u4e0d\u4e00\u81f4\u7684sinogram\u4fe1\u606f\u7684\u53ef\u80fd\u6027\u3002OSMM\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u65e0\u7f1d\u9002\u5e94CT sinogram\u7684\u4e0d\u540c\u7a00\u758f\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOSMM\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u7a00\u758f\u89c6\u56feCT\u6210\u50cf\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOSMM\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u7a00\u758f\u89c6\u56feCT\u6210\u50cf\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10330", "pdf": "https://arxiv.org/pdf/2505.10330", "abs": "https://arxiv.org/abs/2505.10330", "authors": ["Jonathan Clifford Balloch"], "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change", "categories": ["cs.LG", "cs.AI"], "comment": "PhD Dissertation, 131 pages", "summary": "Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u80fd\u529b\uff1a\u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u53ca\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\u5fc5\u987b\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u3002\u867d\u7136\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u6570\u636e\u5bc6\u96c6\u4e14\u5047\u8bbe\u4e16\u754c\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u95f4\u4e4b\u95f4\u4e0d\u4f1a\u6539\u53d8\u3002\u56e0\u6b64\uff0c\u5f53\u6761\u4ef6\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u4f20\u7edfRL\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u3002\u8fd9\u5e26\u6765\u4e86\u6839\u672c\u6027\u7684\u6311\u6218\uff1a\u5982\u4f55\u5728\u4e0d\u707e\u96be\u6027\u9057\u5fd8\u6709\u7528\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba9RL\u4ee3\u7406\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u9047\u5230\u65b0\u73af\u5883\u53d8\u5316\u65f6\u9ad8\u6548\u9002\u5e94\uff1f", "method": "\u672c\u6587\u8ba8\u8bba\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u80fd\u529b\uff1a\u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u53ca\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u9002\u5e94\u9700\u8981\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a(1) \u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u4ece\u76f8\u5173\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u4ee5\u53ca (2) \u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u5728\u4e0d\u5e72\u6270\u53ef\u91cd\u7528\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u66f4\u65b0\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u9002\u5e94\u9700\u8981\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a(1) \u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u4ece\u76f8\u5173\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u4ee5\u53ca (2) \u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u5728\u4e0d\u5e72\u6270\u53ef\u91cd\u7528\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u66f4\u65b0\u3002"}}
{"id": "2505.10075", "pdf": "https://arxiv.org/pdf/2505.10075", "abs": "https://arxiv.org/abs/2505.10075", "authors": ["Jun Guo", "Xiaojian Ma", "Yikai Wang", "Min Yang", "Huaping Liu", "Qing Li"], "title": "FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: see https://sharinka0715.github.io/FlowDreamer/", "summary": "This paper investigates training better visual world models for robot\nmanipulation, i.e., models that can predict future visual observations by\nconditioning on past frames and robot actions. Specifically, we consider world\nmodels that operate on RGB-D frames (RGB-D world models). As opposed to\ncanonical approaches that handle dynamics prediction mostly implicitly and\nreconcile it with visual rendering in a single model, we introduce FlowDreamer,\nwhich adopts 3D scene flow as explicit motion representations. FlowDreamer\nfirst predicts 3D scene flow from past frame and action conditions with a\nU-Net, and then a diffusion model will predict the future frame utilizing the\nscene flow. FlowDreamer is trained end-to-end despite its modularized nature.\nWe conduct experiments on 4 different benchmarks, covering both video\nprediction and visual planning tasks. The results demonstrate that FlowDreamer\nachieves better performance compared to other baseline RGB-D world models by 7%\non semantic similarity, 11% on pixel quality, and 6% on success rate in various\nrobot manipulation domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u4e16\u754c\u6a21\u578bFlowDreamer\uff0c\u901a\u8fc7\u663e\u5f0f\u8fd0\u52a8\u8868\u793a\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u8bad\u7ec3\u66f4\u597d\u7684\u89c6\u89c9\u4e16\u754c\u6a21\u578b\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u7684\u89c6\u89c9\u89c2\u5bdf\u7ed3\u679c\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "method": "FlowDreamer\u901a\u8fc7U-Net\u9884\u6d4b3D\u573a\u666f\u6d41\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u672a\u6765\u5e27\uff0c\u91c7\u75283D\u573a\u666f\u6d41\u4f5c\u4e3a\u663e\u5f0f\u8fd0\u52a8\u8868\u793a\u3002", "result": "FlowDreamer\u57284\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6db5\u76d6\u4e86\u89c6\u9891\u9884\u6d4b\u548c\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\uff0c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebfRGB-D\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "FlowDreamer\u5728\u5404\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u4e2d\u76f8\u6bd4\u5176\u4ed6\u57fa\u7ebfRGB-D\u4e16\u754c\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5206\u522b\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0a\u63d0\u9ad8\u4e867%\uff0c\u50cf\u7d20\u8d28\u91cf\u4e0a\u63d0\u9ad8\u4e8611%\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u4e866%\u3002"}}
{"id": "2505.09974", "pdf": "https://arxiv.org/pdf/2505.09974", "abs": "https://arxiv.org/abs/2505.09974", "authors": ["Adel ElZemity", "Budi Arief", "Shujun Li"], "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. We present a systematic evaluation of safety risks in fine-tuned\nLLMs for cyber security applications. Using the OWASP Top 10 for LLM\nApplications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,\nMistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.\nOur evaluation shows that fine-tuning reduces safety resilience across all\ntested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection\ndrops from 0.95 to 0.15). We propose and evaluate a safety alignment approach\nthat carefully rewords instruction-response pairs to include explicit safety\nprecautions and ethical considerations. This approach demonstrates that it is\npossible to maintain or even improve model safety while preserving technical\nutility, offering a practical path forward for developing safer fine-tuning\nmethodologies. This work offers a systematic evaluation for safety risks in\nLLMs, enabling safer adoption of generative AI in sensitive domains, and\ncontributing towards the development of secure, trustworthy, and ethically\naligned LLMs.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u5728\u4fdd\u6301\u6280\u672f\u6548\u7528\u7684\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u3002", "motivation": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u96c6\u6210\u5230\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u867d\u7136\u5e26\u6765\u4e86\u663e\u8457\u7684\u673a\u4f1a\uff0c\u4f46\u4e5f\u53ef\u80fd\u5f15\u5165\u5173\u952e\u98ce\u9669\u548c\u5b89\u5168\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u98ce\u9669\u5e76\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528OWASP Top 10 for LLM Applications\u6846\u67b6\u8bc4\u4f30\u4e86\u4e03\u4e2a\u5f00\u6e90LLM\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ed4\u7ec6\u91cd\u5199\u6307\u4ee4-\u54cd\u5e94\u5bf9\u6765\u5305\u542b\u663e\u5f0f\u7684\u5b89\u5168\u9884\u9632\u63aa\u65bd\u548c\u4f26\u7406\u8003\u8651\u3002", "result": "\u5fae\u8c03\u4f1a\u964d\u4f4e\u6240\u6709\u6d4b\u8bd5LLM\u7684\u5b89\u5168\u5f39\u6027\uff0c\u4f8b\u5982Llama 3.1 8B\u5728\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u65f6\u7684\u5b89\u5168\u8bc4\u5206\u4ece0.95\u964d\u81f30.15\u3002\u6240\u63d0\u51fa\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u6280\u672f\u6548\u7528\u7684\u540c\u65f6\u7ef4\u6301\u6216\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9LLM\u5b89\u5168\u98ce\u9669\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e3a\u5728\u654f\u611f\u9886\u57df\u5b89\u5168\u91c7\u7528\u751f\u6210\u5f0fAI\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5e76\u6709\u52a9\u4e8e\u5f00\u53d1\u5b89\u5168\u3001\u53ef\u4fe1\u548c\u7b26\u5408\u4f26\u7406\u7684LLM\u3002"}}
{"id": "2505.10331", "pdf": "https://arxiv.org/pdf/2505.10331", "abs": "https://arxiv.org/abs/2505.10331", "authors": ["Luca Muscarnera", "Luigi Loreti", "Giovanni Todeschini", "Alessio Fumagalli", "Francesco Regazzoni"], "title": "Emergence of Structure in Ensembles of Random Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\uff0c\u5e76\u53d1\u73b0\u6700\u4f18\u6e29\u5ea6\u53c2\u6570\u5177\u6709\u666e\u904d\u6027\uff0c\u4e0d\u53d7\u6559\u5e08\u5206\u7c7b\u5668\u548c\u5206\u7c7b\u5668\u6570\u91cf\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e2d\u5177\u6709\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u7c7b\u6bd4\u63ed\u793a\u4e86\u5176\u81ea\u7ec4\u7ec7\u7279\u6027\u3002", "motivation": "\u968f\u673a\u6027\u5728\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u4f46\u7531\u968f\u673a\u7ec4\u4ef6\u7ec4\u6210\u7684\u7cfb\u7edf\u5f80\u5f80\u8868\u73b0\u51fa\u5b8f\u89c2\u7684\u786e\u5b9a\u6027\u884c\u4e3a\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u79cd\u4ece\u5fae\u89c2\u65e0\u5e8f\u5230\u5b8f\u89c2\u6709\u5e8f\u7684\u8f6c\u53d8\u673a\u5236\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u7406\u8bba\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5206\u7c7b\u635f\u5931\u4f5c\u4e3a\u80fd\u91cf\u5b9a\u4e49\u5409\u5e03\u65af\u6d4b\u5ea6\uff0c\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u3002\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6700\u4f18\u6e29\u5ea6\u53c2\u6570\u7684\u666e\u904d\u6027\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u6837\u672c\u7531\u9ad8\u65af\u5206\u5e03\u751f\u6210\u4e14\u6807\u7b7e\u7531\u6559\u5e08\u611f\u77e5\u673a\u6784\u9020\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u4f18\u6e29\u5ea6\u53c2\u6570\u4e0d\u4f9d\u8d56\u4e8e\u6559\u5e08\u5206\u7c7b\u5668\u6216\u968f\u673a\u5206\u7c7b\u5668\u7684\u6570\u91cf\uff0c\u5c55\u793a\u4e86\u8be5\u884c\u4e3a\u7684\u666e\u904d\u6027\u3002\u5b9e\u9a8c\u5728MNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u73b0\u8c61\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6a21\u578b\u6765\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\uff0c\u5e76\u53d1\u73b0\u6700\u4f18\u6e29\u5ea6\u53c2\u6570\u5177\u6709\u666e\u904d\u6027\uff0c\u4e0d\u53d7\u6559\u5e08\u5206\u7c7b\u5668\u548c\u968f\u673a\u5206\u7c7b\u5668\u6570\u91cf\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e2d\u5177\u6709\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u7c7b\u6bd4\u63ed\u793a\u4e86\u5176\u81ea\u7ec4\u7ec7\u7279\u6027\u3002"}}
{"id": "2505.10144", "pdf": "https://arxiv.org/pdf/2505.10144", "abs": "https://arxiv.org/abs/2505.10144", "authors": ["Xuechang Tu", "Lukas Radl", "Michael Steiner", "Markus Steinberger", "Bernhard Kerbl", "Fernando de la Torre"], "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality", "categories": ["cs.GR", "cs.CV"], "comment": "I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/", "summary": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters.", "AI": {"tldr": "VRSplat\u662f\u4e00\u79cd\u6539\u8fdb\u76843DGS\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3VR\u4e2d\u7684\u65f6\u95f4\u4f2a\u5f71\u3001\u6295\u5f71\u5931\u771f\u548c\u4f4e\u5e27\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u5e76\u4f18\u5316\u5149\u6805\u5316\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684VR\u4f53\u9a8c\u3002", "motivation": "3DGS\u5728\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4e2d\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u65f6\u95f4\u4f2a\u5f71\u3001\u57fa\u4e8e\u6295\u5f71\u7684\u5931\u771f\u4ee5\u53ca\u5728\u6e32\u67d3\u5927\u91cf\u9ad8\u65af\u51fd\u6570\u65f6\u5e27\u7387\u964d\u4f4e\u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u95ee\u9898\u662f\u7531\u4e8e\u5927\u89c6\u573a\u89d2\u3001\u6301\u7eed\u7684\u5934\u90e8\u8fd0\u52a8\u548c\u9ad8\u5206\u8fa8\u7387\u5934\u6234\u663e\u793a\u5668\uff08HMDs\uff09\u800c\u88ab\u653e\u5927\u3002", "method": "VRSplat\u7ed3\u5408\u5e76\u6269\u5c55\u4e863DGS\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4ee5\u5168\u9762\u89e3\u51b3VR\u4e2d\u7684\u6311\u6218\u3002\u5b83\u901a\u8fc7\u4fee\u6539\u5355\u4e2a\u6280\u672f\u53ca\u6838\u5fc33DGS\u5149\u6805\u5316\u5668\uff0c\u5c55\u793a\u4e86Mini-Splatting\u3001StopThePop\u548cOptimal Projection\u7b49\u60f3\u6cd5\u5982\u4f55\u76f8\u4e92\u8865\u5145\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e2d\u592e\u89c6\u7f51\u819c\u5149\u6805\u5316\u5668\uff0c\u80fd\u591f\u5728\u4e00\u6b21GPU\u8c03\u7528\u4e2d\u5904\u7406\u7126\u70b9\u548c\u5468\u8fb9\u533a\u57df\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u5e76\u63d0\u9ad8GPU\u5229\u7528\u7387\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u670925\u540d\u53c2\u4e0e\u8005\u7684\u53d7\u63a7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86VRSplat\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793aVRSplat\u5728Mini-Splatting\u7684\u5176\u4ed6\u914d\u7f6e\u4e0a\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002VRSplat\u80fd\u591f\u6d88\u9664\u5f39\u8df3\u548c\u7acb\u4f53\u89c6\u89c9\u5e72\u6270\u7684\u6d6e\u70b9\u7269\uff0c\u5e76\u5b9e\u73b072+ FPS\u7684\u5e27\u7387\u3002", "conclusion": "VRSplat\u662f\u7b2c\u4e00\u4e2a\u7ecf\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u76843DGS\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u73b0\u4ee3VR\u5e94\u7528\uff0c\u5b9e\u73b0\u4e8672+ FPS\u7684\u5e27\u7387\uff0c\u540c\u65f6\u6d88\u9664\u4e86\u5f39\u8df3\u548c\u7acb\u4f53\u89c6\u89c9\u5e72\u6270\u7684\u6d6e\u70b9\u7269\u3002"}}
{"id": "2505.09989", "pdf": "https://arxiv.org/pdf/2505.09989", "abs": "https://arxiv.org/abs/2505.09989", "authors": ["Tella Rajashekhar Reddy", "Palak", "Rohan Gandhi", "Anjaly Parayil", "Chaojie Zhang", "Mike Shepperd", "Liangcheng Yu", "Jayashree Mohan", "Srinivasan Iyengar", "Shivkumar Kalyanaraman", "Debopam Bhattacherjee"], "title": "AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron", "categories": ["cs.DC", "cs.AI", "cs.NI"], "comment": null, "summary": "AI power demand is growing unprecedentedly thanks to the high power density\nof AI compute and the emerging inferencing workload. On the supply side,\nabundant wind power is waiting for grid access in interconnection queues. In\nthis light, this paper argues bringing AI workload to modular compute clusters\nco-located in wind farms. Our deployment right-sizing strategy makes it\neconomically viable to deploy more than 6 million high-end GPUs today that\ncould consume cheap, green power at its source. We built Heron, a cross-site\nsoftware router, that could efficiently leverage the complementarity of power\ngeneration across wind farms by routing AI inferencing workload around power\ndrops. Using 1-week ofcoding and conversation production traces from Azure and\n(real) variable wind power traces, we show how Heron improves aggregate goodput\nof AI compute by up to 80% compared to the state-of-the-art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06AI\u5de5\u4f5c\u8d1f\u8f7d\u8f6c\u79fb\u5230\u98ce\u529b\u53d1\u7535\u573a\u4e2d\u7684\u6a21\u5757\u5316\u8ba1\u7b97\u96c6\u7fa4\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aHeron\u7684\u8f6f\u4ef6\u8def\u7531\u5668\uff0c\u4ee5\u63d0\u9ad8AI\u8ba1\u7b97\u7684\u6548\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740AI\u7b97\u529b\u9700\u6c42\u7684\u5feb\u901f\u589e\u957f\u548c\u65b0\u5174\u7684\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u9700\u8981\u5bfb\u627e\u4e00\u79cd\u7ecf\u6d4e\u4e14\u73af\u4fdd\u7684\u65b9\u5f0f\u6765\u6ee1\u8db3AI\u8ba1\u7b97\u7684\u9700\u6c42\u3002\u540c\u65f6\uff0c\u7535\u7f51\u63a5\u5165\u961f\u5217\u4e2d\u5b58\u5728\u5927\u91cf\u672a\u4f7f\u7528\u7684\u98ce\u529b\u53d1\u7535\u8d44\u6e90\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u90e8\u7f72\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86Heron\uff0c\u8fd9\u662f\u4e00\u4e2a\u8de8\u7ad9\u70b9\u7684\u8f6f\u4ef6\u8def\u7531\u5668\uff0c\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u98ce\u529b\u53d1\u7535\u573a\u4e4b\u95f4\u7684\u7535\u529b\u751f\u6210\u4e92\u8865\u6027\u3002", "result": "\u901a\u8fc7\u4f7f\u7528Azure\u7684\u4e00\u5468\u7f16\u7801\u548c\u5bf9\u8bdd\u751f\u4ea7\u75d5\u8ff9\u4ee5\u53ca\u771f\u5b9e\u7684\u53ef\u53d8\u98ce\u529b\u53d1\u7535\u75d5\u8ff9\uff0cHeron\u5c55\u793a\u4e86\u5176\u5728\u63d0\u9ad8AI\u8ba1\u7b97\u6574\u4f53\u541e\u5410\u91cf\u65b9\u9762\u7684\u6548\u679c\uff0c\u6700\u9ad8\u53ef\u8fbe80%\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\u5c06AI\u5de5\u4f5c\u8d1f\u8f7d\u5e26\u5230\u4e0e\u98ce\u529b\u53d1\u7535\u573a\u5171\u7f6e\u7684\u6a21\u5757\u5316\u8ba1\u7b97\u96c6\u7fa4\u4e2d\u662f\u7ecf\u6d4e\u4e0a\u53ef\u884c\u7684\uff0c\u5e76\u5c55\u793a\u4e86Heron\u5982\u4f55\u901a\u8fc7\u8def\u7531AI\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u6765\u5229\u7528\u98ce\u529b\u53d1\u7535\u573a\u4e4b\u95f4\u7684\u4e92\u8865\u6027\uff0c\u4ece\u800c\u63d0\u9ad8AI\u8ba1\u7b97\u7684\u6574\u4f53\u541e\u5410\u91cf\u3002"}}
{"id": "2505.10344", "pdf": "https://arxiv.org/pdf/2505.10344", "abs": "https://arxiv.org/abs/2505.10344", "authors": ["Alan Jeffares", "Liyuan Liu"], "title": "An Introduction to Discrete Variational Autoencoders", "categories": ["cs.LG"], "comment": "Tutorial paper", "summary": "Variational Autoencoders (VAEs) are well-established as a principled approach\nto probabilistic unsupervised learning with neural networks. Typically, an\nencoder network defines the parameters of a Gaussian distributed latent space\nfrom which we can sample and pass realizations to a decoder network. This model\nis trained to reconstruct its inputs and is optimized through the evidence\nlower bound. In recent years, discrete latent spaces have grown in popularity,\nsuggesting that they may be a natural choice for many data modalities (e.g.\ntext). In this tutorial, we provide a rigorous, yet practical, introduction to\ndiscrete variational autoencoders -- specifically, VAEs in which the latent\nspace is made up of latent variables that follow a categorical distribution. We\nassume only a basic mathematical background with which we carefully derive each\nstep from first principles. From there, we develop a concrete training recipe\nand provide an example implementation, hosted at\nhttps://github.com/alanjeffares/discreteVAE.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u73b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u5e94\u7528\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u5728\u8bb8\u591a\u6570\u636e\u6a21\u6001\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u4ecb\u7ecd\u6765\u7406\u89e3\u5176\u539f\u7406\u548c\u5e94\u7528\u3002", "method": "\u672c\u6587\u901a\u8fc7\u4ece\u5934\u5f00\u59cb\u63a8\u5bfc\u6bcf\u4e2a\u6b65\u9aa4\uff0c\u4ecb\u7ecd\u4e86\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u4f53\u7684\u8bad\u7ec3\u65b9\u6848\u548c\u793a\u4f8b\u5b9e\u73b0\u3002", "result": "\u672c\u6587\u4e3a\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\u63d0\u4f9b\u4e86\u4e25\u8c28\u4f46\u5b9e\u7528\u7684\u4ecb\u7ecd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u5b9e\u73b0\u4ee5\u4f9b\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6df1\u5165\u4ecb\u7ecd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u6570\u636e\u6a21\u6001\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.10012", "pdf": "https://arxiv.org/pdf/2505.10012", "abs": "https://arxiv.org/abs/2505.10012", "authors": ["Tadashi Kadowaki"], "title": "Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering", "categories": ["quant-ph", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Recent advances in artificial intelligence (AI) and quantum computing are\naccelerating automation in scientific and engineering processes, fundamentally\nreshaping research methodologies. This perspective highlights parallels between\nscientific automation and established Computer-Aided Engineering (CAE)\npractices, introducing Quantum CAE as a framework that leverages quantum\nalgorithms for simulation, optimization, and machine learning within\nengineering design. Practical implementations of Quantum CAE are illustrated\nthrough case studies for combinatorial optimization problems. Further\ndiscussions include advancements toward higher automation levels, highlighting\nthe critical role of specialized AI agents proficient in quantum algorithm\ndesign. The integration of quantum computing with AI raises significant\nquestions about the collaborative dynamics among human scientists and\nengineers, AI systems, and quantum computational resources, underscoring a\ntransformative future for automated discovery and innovation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u8ba1\u7b97\u4e0eAI\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u91cf\u5b50CAE\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u672a\u6765\u81ea\u52a8\u5316\u53d1\u73b0\u548c\u521b\u65b0\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u548c\u91cf\u5b50\u8ba1\u7b97\u7684\u8fdb\u6b65\u6b63\u5728\u52a0\u901f\u79d1\u5b66\u548c\u5de5\u7a0b\u8fc7\u7a0b\u7684\u81ea\u52a8\u5316\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5982\u4f55\u5c06\u91cf\u5b50\u8ba1\u7b97\u4e0eAI\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u5de5\u7a0b\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u521b\u65b0\u80fd\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u91cf\u5b50CAE\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u5411\u66f4\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u53d1\u5c55\u7684\u8fdb\u5c55\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86\u91cf\u5b50CAE\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002\u540c\u65f6\uff0c\u8ba8\u8bba\u4e86AI\u4ee3\u7406\u5728\u91cf\u5b50\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u4ee5\u53ca\u4eba\u673a\u534f\u4f5c\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u91cf\u5b50CAE\u4f5c\u4e3a\u4e00\u79cd\u6846\u67b6\uff0c\u5229\u7528\u91cf\u5b50\u7b97\u6cd5\u8fdb\u884c\u6a21\u62df\u3001\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u5de5\u7a0b\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002\u540c\u65f6\uff0c\u5b83\u5f3a\u8c03\u4e86\u4eba\u7c7b\u79d1\u5b66\u5bb6\u3001\u5de5\u7a0b\u5e08\u3001AI\u7cfb\u7edf\u548c\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u7684\u534f\u4f5c\u52a8\u6001\u7684\u91cd\u8981\u6027\uff0c\u5c55\u671b\u4e86\u81ea\u52a8\u5316\u53d1\u73b0\u548c\u521b\u65b0\u7684\u672a\u6765\u3002"}}
{"id": "2505.10347", "pdf": "https://arxiv.org/pdf/2505.10347", "abs": "https://arxiv.org/abs/2505.10347", "authors": ["Gabriel S. Gama", "Valdir Grassi Jr"], "title": "Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e13\u95e8\u7684\u591a\u4efb\u52a1\u4f18\u5316\u5668\uff08SMTOs\uff09\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u56fa\u5b9a\u6743\u91cd\u53ef\u4ee5\u5b9e\u73b0\u4e0eSMTOs\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6f84\u6e05SMTOs\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u884c\u4e3a\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u7edf\u4e00\u635f\u5931\u51fd\u6570\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4e0eSMTOs\u76f8\u5ab2\u7f8e\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u8bc4\u4f30\u5bf9SMTOs\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u6700\u65b0\u7684\u65b9\u6cd5\uff0c\u5728\u66f4\u590d\u6742\u7684\u591a\u4efb\u52a1\u95ee\u9898\u4e0a\u9a8c\u8bc1\u8fd9\u4e9b\u4e3b\u5f20\u3002", "result": "SMTOs\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u56fa\u5b9a\u6743\u91cd\u53ef\u4ee5\u5b9e\u73b0\u4e0eSMTOs\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7edf\u4e00\u635f\u5931\u51fd\u6570\u7684\u8868\u73b0\u4e0eSMTOs\u76f8\u4f3c\u3002", "conclusion": "SMTOs\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u56fa\u5b9a\u6743\u91cd\u53ef\u4ee5\u4e0eSMTOs\u7ade\u4e89\u6027\u5730\u8868\u73b0\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7edf\u4e00\u635f\u5931\u51fd\u6570\u7684\u8868\u73b0\u4e0eSMTOs\u76f8\u4f3c\u3002"}}
{"id": "2505.10312", "pdf": "https://arxiv.org/pdf/2505.10312", "abs": "https://arxiv.org/abs/2505.10312", "authors": ["Anh Tuan Ha", "Hoang Khang Phan", "Thai Minh Tien Ngo", "Anh Phan Truong", "Nhat Tan Le"], "title": "SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "In the realm of Human Activity Recognition (HAR), obtaining high quality and\nvariance data is still a persistent challenge due to high costs and the\ninherent variability of real-world activities. This study introduces a\ngeneration dataset by deep learning approaches (Attention Autoencoder and\nconditional Generative Adversarial Networks). Another problem that data\nheterogeneity is a critical challenge, one of the solutions is to shuffle the\ndata to homogenize the distribution. Experimental results demonstrate that the\nrandom sequence strategy significantly improves classification performance,\nachieving an accuracy of up to 0.70 $\\pm$ 0.03 and a macro F1 score of 0.64\n$\\pm$ 0.01. For that, disrupting temporal dependencies through random sequence\nreordering compels the model to focus on instantaneous recognition, thereby\nimproving robustness against activity transitions. This approach not only\nbroadens the effective training dataset but also offers promising avenues for\nenhancing HAR systems in complex, real-world scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u968f\u673a\u5e8f\u5217\u7b56\u7565\u6765\u63d0\u9ad8HAR\u7cfb\u7edf\u7684\u5206\u7c7b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u9886\u57df\uff0c\u83b7\u53d6\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u6570\u636e\u4ecd\u7136\u662f\u4e00\u9879\u6301\u7eed\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u6210\u672c\u9ad8\u6602\u4e14\u771f\u5b9e\u4e16\u754c\u6d3b\u52a8\u5177\u6709\u5185\u5728\u7684\u53d8\u5f02\u6027\u3002\u6b64\u5916\uff0c\u6570\u636e\u5f02\u8d28\u6027\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u6ce8\u610f\u529b\u81ea\u7f16\u7801\u5668\u548c\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff09\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u968f\u673a\u5e8f\u5217\u7b56\u7565\u6765\u6253\u4e71\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u968f\u673a\u5e8f\u5217\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u8fbe\u5230\u4e860.70\u00b10.03\u7684\u51c6\u786e\u7387\u548c0.64\u00b10.01\u7684\u5b8fF1\u5206\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6269\u5927\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8fd8\u4e3a\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u589e\u5f3aHAR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002"}}
{"id": "2505.10360", "pdf": "https://arxiv.org/pdf/2505.10360", "abs": "https://arxiv.org/abs/2505.10360", "authors": ["Victor Petr\u00e9n Bach Hansen", "Lasse Krogsb\u00f8ll", "Jonas Lyngs\u00f8", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maal\u00f8e"], "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": null, "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFactsR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u7597\u54a8\u8be2\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u63d0\u53d6\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\uff0c\u5e76\u9012\u5f52\u5730\u751f\u6210\u6700\u7ec8\u7684\u75c5\u5386\u8bb0\u5f55\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u75c5\u5386\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u5e76\u4f7f\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u75c5\u5386\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u4e3a\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u5f00\u8f9f\u4e86\u65b0\u7684\u7528\u4f8b\u3002", "motivation": "\u73b0\u6709\u7684AI\u75c5\u5386\u8bb0\u5f55\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u4e8e\u4e00\u6b21\u6216\u5c11\u91cf\u63d0\u793a\u6765\u751f\u6210\u75c5\u5386\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u75c5\u5386\u5197\u957f\u4e14\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u548c\u8bef\u8ff0\u4e34\u5e8a\u533b\u751f\u7684\u610f\u56fe\uff0c\u9700\u8981\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u6821\u5bf9\uff0c\u8fd9\u53ef\u80fd\u56e0\u5de5\u4f5c\u91cf\u548c\u75b2\u52b3\u800c\u5f71\u54cd\u60a3\u8005\u5b89\u5168\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFactsR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u7597\u54a8\u8be2\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u63d0\u53d6\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u9012\u5f52\u5730\u751f\u6210\u6700\u7ec8\u7684\u75c5\u5386\u8bb0\u5f55\u3002", "result": "FactsR\u65b9\u6cd5\u901a\u8fc7\u8ba9\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u75c5\u5386\u751f\u6210\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u75c5\u5386\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u5e76\u4e3a\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u5f00\u8f9f\u4e86\u65b0\u7684\u7528\u4f8b\u3002", "conclusion": "FactsR\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u63d0\u9ad8\u533b\u7597\u75c5\u5386\u7684\u8d28\u91cf\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u66f4\u597d\u7684\u652f\u6301\u3002"}}
{"id": "2505.10405", "pdf": "https://arxiv.org/pdf/2505.10405", "abs": "https://arxiv.org/abs/2505.10405", "authors": ["Jianhao Huang", "Qunsong Zeng", "Kaibin Huang"], "title": "Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408Gen-SemCom\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u63d0\u793a\u548c\u8bed\u4e49\u5173\u952e\u7279\u5f81\u7684\u4f20\u8f93\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u91cd\u5efa\u7684\u4fdd\u771f\u5ea6\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u751f\u6210\u89c6\u89c9\u4fe1\u606f\u4fdd\u771f\u5ea6\uff08GVIF\uff09\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4fe1\u9053\u81ea\u9002\u5e94\u7684\u7cfb\u7edf\u4ee5\u4f18\u5316\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728PSNR\u548cFID\u5206\u6570\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u751f\u6210\u8bed\u4e49\u901a\u4fe1\uff08Gen-SemCom\uff09\u5229\u7528\u5927\u578b\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6a21\u578b\u4e3a6G\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u53d8\u9769\u6027\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u4f20\u8f93\u4f4e\u7ef4\u63d0\u793a\u800c\u4e0d\u662f\u539f\u59cb\u6570\u636e\u6765\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u3002\u7136\u800c\uff0c\u7eaf\u7cb9\u7684\u63d0\u793a\u9a71\u52a8\u751f\u6210\u4f1a\u4e22\u5931\u7cbe\u7ec6\u7684\u89c6\u89c9\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5ea6\u91cf\u6807\u51c6\u6765\u8bc4\u4f30Gen-SemCom\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df7\u5408Gen-SemCom\u7cfb\u7edf\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u5173\u952e\u4fe1\u606f\u5d4c\u5165\uff08CIE\uff09\u6846\u67b6\uff0c\u5176\u4e2d\u63d0\u53d6\u6587\u672c\u63d0\u793a\u548c\u8bed\u4e49\u5173\u952e\u7279\u5f81\u8fdb\u884c\u4f20\u8f93\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u4ee5\u9009\u62e9\u548c\u4f20\u8f93\u4e0e\u8bed\u4e49\u6807\u7b7e\u76f8\u5173\u7684\u56fe\u50cf\u7684\u8bed\u4e49\u5173\u952e\u7279\u5f81\u3002\u901a\u8fc7\u96c6\u6210\u6587\u672c\u63d0\u793a\u548c\u5173\u952e\u7279\u5f81\uff0c\u63a5\u6536\u5668\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u91cd\u5efa\u9ad8\u4fdd\u771f\u56fe\u50cf\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u751f\u6210\u89c6\u89c9\u4fe1\u606f\u4fdd\u771f\u5ea6\uff08GVIF\uff09\u5ea6\u91cf\u6807\u51c6\u6765\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\u3002\u901a\u8fc7\u8868\u5f81\u56fe\u50cf\u7279\u5f81\u7684\u7edf\u8ba1\u6a21\u578b\uff0cGVIF\u5ea6\u91cf\u6807\u51c6\u91cf\u5316\u4e86\u5931\u771f\u7279\u5f81\u4e0e\u5176\u539f\u59cb\u5bf9\u5e94\u7269\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u901a\u8fc7\u6700\u5927\u5316GVIF\u5ea6\u91cf\u6807\u51c6\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4fe1\u9053\u81ea\u9002\u5e94\u7684Gen-SemCom\u7cfb\u7edf\uff0c\u6839\u636e\u4fe1\u9053\u72b6\u6001\u81ea\u9002\u5e94\u5730\u63a7\u5236\u7279\u5f81\u91cf\u548c\u538b\u7f29\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86GVIF\u5ea6\u91cf\u5bf9\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u654f\u611f\u6027\uff0c\u5e76\u4e0ePSNR\u548c\u5173\u952e\u4fe1\u606f\u91cf\u76f8\u5173\u3002\u6b64\u5916\uff0c\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u5728PSNR\u548cFID\u5206\u6570\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86GVIF\u5ea6\u91cf\u5bf9\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u654f\u611f\u6027\uff0c\u5e76\u4e0ePSNR\u548c\u5173\u952e\u4fe1\u606f\u91cf\u76f8\u5173\u3002\u6b64\u5916\uff0c\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u5728PSNR\u548cFID\u5206\u6570\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002"}}
{"id": "2505.10392", "pdf": "https://arxiv.org/pdf/2505.10392", "abs": "https://arxiv.org/abs/2505.10392", "authors": ["Aryan Mishra", "Lizhen Lin"], "title": "Schreier-Coset Graph Propagation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure , preprint", "summary": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.", "AI": {"tldr": "SCGP\u662f\u4e00\u79cd\u57fa\u4e8e\u7fa4\u8bba\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7Schreier-coset\u5d4c\u5165\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\uff0c\u6539\u5584\u957f\u8ddd\u79bb\u4fe1\u606f\u4f20\u9012\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u9002\u5408\u5904\u7406\u5c42\u6b21\u5316\u548c\u6a21\u5757\u5316\u56fe\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5982\u56fe\u91cd\u5e03\u7ebf\u548c\u6297\u74f6\u9888\u67b6\u6784\uff08\u5982Cayley\u56fe\u548cexpander\u56fe\uff09\u867d\u7136\u907f\u514d\u4e86\u8fc7\u538b\u7f29\u95ee\u9898\uff0c\u4f46\u5f15\u5165\u4e86\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002Cayley\u56fe\u5728SL(2,Z_n)\u4e0a\u6784\u5efa\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7406\u8bba\u6027\u8d28\uff0c\u4f46\u7531\u4e8e\u8282\u70b9\u589e\u957f\u4e3aO(n^3)\uff0c\u5bfc\u81f4\u9ad8\u5185\u5b58\u4f7f\u7528\u3002", "method": "SCGP\u662f\u4e00\u79cd\u57fa\u4e8e\u7fa4\u8bba\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7Schreier-coset\u5d4c\u5165\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\uff0c\u800c\u65e0\u9700\u6539\u53d8\u8f93\u5165\u56fe\u7684\u62d3\u6251\u7ed3\u6784\u3002", "result": "SCGP\u5728\u6807\u51c6\u8282\u70b9\u548c\u56fe\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u5176\u6027\u80fd\u4e0eexpander\u56fe\u548c\u91cd\u5e03\u7ebfGNN\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u4f18\u3002\u6b64\u5916\uff0cSCGP\u5728\u5904\u7406\u5c42\u6b21\u5316\u548c\u6a21\u5757\u5316\u56fe\u7ed3\u6784\u65f6\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "SCGP\u5728\u5904\u7406\u5c42\u6b21\u5316\u548c\u6a21\u5757\u5316\u56fe\u7ed3\u6784\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u63a8\u7406\u5ef6\u8fdf\u3001\u6539\u8fdb\u7684\u53ef\u6269\u5c55\u6027\u548c\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u3002"}}
{"id": "2505.10441", "pdf": "https://arxiv.org/pdf/2505.10441", "abs": "https://arxiv.org/abs/2505.10441", "authors": ["Filippo Leveni", "Luca Magri", "Giacomo Boracchi", "Cesare Alippi"], "title": "PIF: Anomaly detection via preference embedding", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Pattern Recognition (ICPR\n  2020)", "summary": "We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPIF\u7684\u65b0\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u9694\u79bb\u65b9\u6cd5\u548c\u504f\u597d\u5d4c\u5165\u7684\u4f18\u70b9\u3002\u901a\u8fc7\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4f7f\u7528PI-Forest\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\uff0c\u5b9e\u9a8c\u8868\u660ePIF\u4f18\u4e8e\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u3002", "motivation": "To address the problem of detecting anomalies with respect to structured patterns.", "method": "PIF, which combines adaptive isolation methods with preference embedding, and employs PI-Forest in a high-dimensional space to compute an anomaly score.", "result": "Experiments on synthetic and real datasets demonstrate that PIF favorably compares with state-of-the-art anomaly detection techniques.", "conclusion": "PI-Forest is better at measuring arbitrary distances and isolating points in the preference space."}}
{"id": "2505.10407", "pdf": "https://arxiv.org/pdf/2505.10407", "abs": "https://arxiv.org/abs/2505.10407", "authors": ["Wenhao Ding", "Choon Hwai Yap", "Kangjun Ji", "Sim\u00e3o Castro"], "title": "Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning", "categories": ["cs.LG", "68T07"], "comment": "10 pages, 2 figures", "summary": "A generative model for the mesh geometry of intracranial aneurysms (IA) is\ncrucial for training networks to predict blood flow forces in real time, which\nis a key factor affecting disease progression. This need is necessitated by the\nabsence of a large IA image datasets. Existing shape generation methods\nstruggle to capture realistic IA features and ignore the relationship between\nIA pouches and parent vessels, limiting physiological realism and their\ngeneration cannot be controlled to have specific morphological measurements. We\npropose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh\ngenerator. In the first stage, AneuG generates low-dimensional Graph Harmonic\nDeformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,\nconstrained to morphing energy statistics truths. GHD enables more accurate\nshape encoding than alternatives. In the second stage, AneuG generates parent\nvessels conditioned on GHD tokens, by generating vascular centreline and\npropagating the cross-section. AneuG's IA shape generation can further be\nconditioned to have specific clinically relevant morphological measurements.\nThis is useful for studies to understand shape variations represented by\nclinical measurements, and for flow simulation studies to understand effects of\nspecific clinical shape parameters on fluid dynamics. Source code and\nimplementation details are available at\nhttps://github.com/anonymousaneug/AneuG.", "AI": {"tldr": "AneuG\u662f\u4e00\u79cd\u57fa\u4e8e\u4e24\u9636\u6bb5\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684IA\u7f51\u683c\u751f\u6210\u5668\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u7279\u5b9a\u4e34\u5e8a\u76f8\u5173\u5f62\u6001\u6d4b\u91cf\u503c\u7684IA\u5f62\u72b6\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u5f62\u72b6\u53d8\u5316\u548c\u6d41\u4f53\u52a8\u529b\u5b66\u5f71\u54cd\u5f88\u6709\u7528\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5927\u578bIA\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u9700\u8981\u4e00\u4e2a\u751f\u6210\u6a21\u578b\u6765\u8bad\u7ec3\u7f51\u7edc\u4ee5\u5b9e\u65f6\u9884\u6d4b\u8840\u6db2\u6d41\u52a8\u529b\uff0c\u8fd9\u662f\u5f71\u54cd\u75be\u75c5\u8fdb\u5c55\u7684\u5173\u952e\u56e0\u7d20\u3002\u73b0\u6709\u7684\u5f62\u72b6\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u7684IA\u7279\u5f81\uff0c\u5e76\u5ffd\u7565\u4e86IA\u888b\u4e0e\u7236\u8840\u7ba1\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u751f\u7406\u73b0\u5b9e\u6027\u548c\u751f\u6210\u63a7\u5236\u3002", "method": "AneuG\u662f\u4e00\u79cd\u57fa\u4e8e\u4e24\u9636\u6bb5\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684IA\u7f51\u683c\u751f\u6210\u5668\u3002\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u4f4e\u7ef4\u56fe\u8c10\u6ce2\u53d8\u5f62\uff08GHD\uff09\u6807\u8bb0\u4ee5\u7f16\u7801\u548c\u91cd\u5efa\u52a8\u8109\u7624\u888b\u5f62\u72b6\uff0c\u53d7\u5f62\u6001\u80fd\u7edf\u8ba1\u771f\u76f8\u7684\u7ea6\u675f\u3002\u7b2c\u4e8c\u9636\u6bb5\u6839\u636eGHD\u6807\u8bb0\u751f\u6210\u7236\u8840\u7ba1\uff0c\u901a\u8fc7\u751f\u6210\u8840\u7ba1\u4e2d\u5fc3\u7ebf\u5e76\u4f20\u64ad\u6a2a\u622a\u9762\u3002", "result": "AneuG\u80fd\u591f\u751f\u6210\u5177\u6709\u7279\u5b9a\u4e34\u5e8a\u76f8\u5173\u5f62\u6001\u6d4b\u91cf\u503c\u7684IA\u5f62\u72b6\uff0c\u8fd9\u6709\u52a9\u4e8e\u7814\u7a76\u5f62\u72b6\u53d8\u5316\u548c\u6d41\u4f53\u52a8\u529b\u5b66\u5f71\u54cd\u3002\u6e90\u4ee3\u7801\u548c\u5b9e\u73b0\u7ec6\u8282\u53ef\u5728https://github.com/anonymousaneug/AneuG\u4e0a\u83b7\u5f97\u3002", "conclusion": "AneuG\u53ef\u4ee5\u751f\u6210\u5177\u6709\u7279\u5b9a\u4e34\u5e8a\u76f8\u5173\u5f62\u6001\u6d4b\u91cf\u503c\u7684IA\u5f62\u72b6\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u5f62\u72b6\u53d8\u5316\u548c\u6d41\u4f53\u52a8\u529b\u5b66\u5f71\u54cd\u5f88\u6709\u7528\u3002"}}
{"id": "2505.10457", "pdf": "https://arxiv.org/pdf/2505.10457", "abs": "https://arxiv.org/abs/2505.10457", "authors": ["Matteo Gambella", "Vicente Javier Castro Solar", "Manuel Roveri"], "title": "SEAL: Searching Expandable Architectures for Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "8 pages, 5 figures", "summary": "Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.", "AI": {"tldr": "SEAL\u662f\u4e00\u79cd\u57fa\u4e8eNAS\u7684\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u548c\u9009\u62e9\u6027\u6269\u5c55\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u5927\u5c0f\u7684\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eNAS\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5728\u6bcf\u4e2a\u4efb\u52a1\u4e2d\u6269\u5c55\u6a21\u578b\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u5e73\u8861\u6a21\u578b\u7684\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "SEAL\u662f\u4e00\u79cd\u57fa\u4e8eNAS\u7684\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u6570\u636e\u589e\u91cf\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u5e76\u5728\u5fc5\u8981\u65f6\u8fdb\u884c\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u84b8\u998f\u8bad\u7ec3\u4fdd\u6301\u7a33\u5b9a\u6027\u3002NAS\u7ec4\u4ef6\u5171\u540c\u641c\u7d22\u67b6\u6784\u548c\u6700\u4f73\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEAL\u6709\u6548\u51cf\u5c11\u4e86\u9057\u5fd8\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u4f4e\u7684\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "SEAL\u5c55\u793a\u4e86\u7ed3\u5408NAS\u548c\u9009\u62e9\u6027\u6269\u5c55\u5728\u589e\u91cf\u573a\u666f\u4e2d\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10043", "pdf": "https://arxiv.org/pdf/2505.10043", "abs": "https://arxiv.org/abs/2505.10043", "authors": ["Yifan Wu", "Lutao Yan", "Yizhang Zhu", "Yinan Mei", "Jiannan Wang", "Nan Tang", "Yuyu Luo"], "title": "Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Charts are crucial for data analysis and decision-making.Text-to-chart\nretrieval systems have become increasingly important for Business Intelligence\n(BI), where users need to find relevant charts that match their analytical\nneeds. These needs can be categorized into precise queries that are\nwell-specified and fuzzy queries that are more exploratory -- both require\nunderstanding the semantics and context of the charts. However, existing\ntext-to-chart retrieval solutions often fail to capture the semantic content\nand contextual information of charts, primarily due to the lack of\ncomprehensive metadata (or semantic insights). To address this limitation, we\npropose a training data development pipeline that automatically synthesizes\nhierarchical semantic insights for charts, covering visual patterns\n(visual-oriented), statistical properties (statistics-oriented), and practical\napplications (task-oriented), which produces 207,498 semantic insights for\n69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to\nlearn better representations of charts for text-to-chart retrieval. Our method\nleverages rich semantic insights during the training phase to develop a model\nthat understands both visual and semantic aspects of charts.To evaluate\ntext-to-chart retrieval performance, we curate the first benchmark, CRBench,\nfor this task with 21,862 charts and 326 text queries from real-world BI\napplications, with ground-truth labels verified by the crowd\nworkers.Experiments show that ChartFinder significantly outperforms existing\nmethods in text-to-chart retrieval tasks across various settings. For precise\nqueries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than\nstate-of-the-art models. In fuzzy query tasks, our method also demonstrates\nconsistent improvements, with an average increase of 5% across nearly all\nmetrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u65b9\u6cd5\uff0c\u79f0\u4e3aChartFinder\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u56fe\u8868\u7684\u5c42\u6b21\u5316\u8bed\u4e49\u6d1e\u5bdf\u6765\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cChartFinder\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u7684\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u56fe\u8868\u7684\u8bed\u4e49\u5185\u5bb9\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u5168\u9762\u7684\u5143\u6570\u636e\uff08\u6216\u8bed\u4e49\u6d1e\u5bdf\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u5f00\u53d1\u7ba1\u9053\uff0c\u4ee5\u81ea\u52a8\u751f\u6210\u56fe\u8868\u7684\u5c42\u6b21\u5316\u8bed\u4e49\u6d1e\u5bdf\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u5f00\u53d1\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u56fe\u8868\u7684\u5c42\u6b21\u5316\u8bed\u4e49\u6d1e\u5bdf\uff0c\u6db5\u76d6\u89c6\u89c9\u6a21\u5f0f\uff08\u4ee5\u89c6\u89c9\u4e3a\u5bfc\u5411\uff09\u3001\u7edf\u8ba1\u5c5e\u6027\uff08\u4ee5\u7edf\u8ba1\u4e3a\u5bfc\u5411\uff09\u548c\u5b9e\u9645\u5e94\u7528\uff08\u4ee5\u4efb\u52a1\u4e3a\u5bfc\u5411\uff09\uff0c\u5e76\u751f\u6210\u4e86207,498\u4e2a\u8bed\u4e49\u6d1e\u5bdf\uff0c\u7528\u4e8e69,166\u4e2a\u56fe\u8868\u3002\u57fa\u4e8e\u8fd9\u4e9b\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aChartFinder\u7684\u57fa\u4e8eCLIP\u7684\u6a21\u578b\uff0c\u4ee5\u5b66\u4e60\u66f4\u597d\u7684\u56fe\u8868\u8868\u793a\u7528\u4e8e\u6587\u672c\u5230\u56fe\u8868\u7684\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cChartFinder\u5728\u6587\u672c\u5230\u56fe\u8868\u7684\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5bf9\u4e8e\u7cbe\u786e\u67e5\u8be2\uff0cChartFinder\u5728NDCG@10\u4e0a\u8fbe\u5230\u4e8666.9%\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u6a21\u578b\u9ad8\u51fa11.58%\u3002\u5728\u6a21\u7cca\u67e5\u8be2\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e5f\u5c55\u793a\u4e86\u7a33\u5b9a\u7684\u6539\u8fdb\uff0c\u51e0\u4e4e\u6240\u6709\u6307\u6807\u7684\u5e73\u5747\u589e\u52a0\u7ea6\u4e3a5%\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cChartFinder\u5728\u6587\u672c\u5230\u56fe\u8868\u7684\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5bf9\u4e8e\u7cbe\u786e\u67e5\u8be2\uff0cChartFinder\u5728NDCG@10\u4e0a\u8fbe\u5230\u4e8666.9%\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u6a21\u578b\u9ad8\u51fa11.58%\u3002\u5728\u6a21\u7cca\u67e5\u8be2\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e5f\u5c55\u793a\u4e86\u7a33\u5b9a\u7684\u6539\u8fdb\uff0c\u51e0\u4e4e\u6240\u6709\u6307\u6807\u7684\u5e73\u5747\u589e\u52a0\u7ea6\u4e3a5%\u3002"}}
{"id": "2505.10422", "pdf": "https://arxiv.org/pdf/2505.10422", "abs": "https://arxiv.org/abs/2505.10422", "authors": ["Daniel Weitekamp", "Christopher MacLellan", "Erik Harpstead", "Kenneth Koedinger"], "title": "Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency", "categories": ["cs.LG"], "comment": "To appear in CogSci 2025", "summary": "Human learning relies on specialization -- distinct cognitive mechanisms\nworking together to enable rapid learning. In contrast, most modern neural\nnetworks rely on a single mechanism: gradient descent over an objective\nfunction. This raises the question: might human learners' relatively rapid\nlearning from just tens of examples instead of tens of thousands in data-driven\ndeep learning arise from our ability to use multiple specialized mechanisms of\nlearning in combination? We investigate this question through an ablation\nanalysis of inductive human learning simulations in online tutoring\nenvironments. Comparing reinforcement learning to a more data-efficient\n3-mechanism symbolic rule induction approach, we find that decomposing learning\ninto multiple distinct mechanisms significantly improves data efficiency,\nbringing it in line with human learning. Furthermore, we show that this\ndecomposition has a greater impact on efficiency than the distinction between\nsymbolic and subsymbolic learning alone. Efforts to align data-driven machine\nlearning with human learning often overlook the stark difference in learning\nefficiency. Our findings suggest that integrating multiple specialized learning\nmechanisms may be key to bridging this gap.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u5728\u7ebf\u8f85\u5bfc\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u4eba\u7c7b\u5b66\u4e60\u6a21\u62df\uff0c\u63a2\u8ba8\u4e86\u4eba\u7c7b\u5b66\u4e60\u662f\u5426\u53ef\u80fd\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u5b9e\u73b0\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5206\u89e3\u5b66\u4e60\u4e3a\u591a\u4e2a\u4e0d\u540c\u673a\u5236\u80fd\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4f7f\u5176\u63a5\u8fd1\u4eba\u7c7b\u5b66\u4e60\u6c34\u5e73\u3002", "motivation": "\u4eba\u7c7b\u5b66\u4e60\u4f9d\u8d56\u4e8e\u4e13\u4e1a\u5316\u2014\u2014\u4e0d\u540c\u7684\u8ba4\u77e5\u673a\u5236\u534f\u540c\u5de5\u4f5c\u4ee5\u5b9e\u73b0\u5feb\u901f\u5b66\u4e60\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u591a\u6570\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u4e8e\u5355\u4e00\u673a\u5236\uff1a\u76ee\u6807\u51fd\u6570\u4e0a\u7684\u68af\u5ea6\u4e0b\u964d\u3002\u8fd9\u5f15\u53d1\u4e86\u95ee\u9898\uff1a\u4eba\u7c7b\u5b66\u4e60\u8005\u4ece\u4ec5\u51e0\u5341\u4e2a\u4f8b\u5b50\u800c\u4e0d\u662f\u6570\u5341\u4e07\u4e2a\u6570\u636e\u8fdb\u884c\u5feb\u901f\u5b66\u4e60\u662f\u5426\u53ef\u80fd\u6e90\u4e8e\u6211\u4eec\u80fd\u591f\u7ed3\u5408\u4f7f\u7528\u591a\u4e2a\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\uff1f", "method": "\u6211\u4eec\u901a\u8fc7\u5728\u7ebf\u8f85\u5bfc\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u4eba\u7c7b\u5b66\u4e60\u6a21\u62df\u7684\u6d88\u878d\u5206\u6790\u6765\u7814\u7a76\u8fd9\u4e2a\u95ee\u9898\u3002\u6bd4\u8f83\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u66f4\u9ad8\u6548\u7684\u6570\u636e3\u673a\u5236\u7b26\u53f7\u89c4\u5219\u5f52\u7eb3\u65b9\u6cd5\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u5c06\u5b66\u4e60\u5206\u89e3\u4e3a\u591a\u4e2a\u4e0d\u540c\u7684\u673a\u5236\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4f7f\u5176\u4e0e\u4eba\u7c7b\u5b66\u4e60\u76f8\u5339\u914d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u8fd9\u79cd\u5206\u89e3\u5bf9\u6548\u7387\u7684\u5f71\u54cd\u5927\u4e8e\u7b26\u53f7\u548c\u975e\u7b26\u53f7\u5b66\u4e60\u4e4b\u95f4\u7684\u533a\u522b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408\u591a\u4e2a\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u662f\u5f25\u5408\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u4e0e\u4eba\u7c7b\u5b66\u4e60\u4e4b\u95f4\u5dee\u8ddd\u7684\u5173\u952e\u3002"}}
{"id": "2505.10464", "pdf": "https://arxiv.org/pdf/2505.10464", "abs": "https://arxiv.org/abs/2505.10464", "authors": ["Jiaming Liang", "Lihuan Dai", "Xiaoqi Sheng", "Xiangguang Chen", "Chun Yao", "Guihua Tao", "Qibin Leng", "Honming Cai", "Xi Zhong"], "title": "HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been provisionally accepted for MICCAI 2025", "summary": "Multimodal medical image segmentation faces significant challenges in the\ncontext of gastric cancer lesion analysis. This clinical context is defined by\nthe scarcity of independent multimodal datasets and the imperative to\namalgamate inherently misaligned modalities. As a result, algorithms are\nconstrained to train on approximate data and depend on application migration,\nleading to substantial resource expenditure and a potential decline in analysis\naccuracy. To address those challenges, we have made two major contributions:\nFirst, we publicly disseminate the GCM 2025 dataset, which serves as the first\nlarge-scale, open-source collection of gastric cancer multimodal MRI scans,\nfeaturing professionally annotated FS-T2W, CE-T1W, and ADC images from 500\npatients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework\nthat employs an original HWA block with learnable window aggregation layers to\nestablish dynamic feature correspondences between different modalities'\nanatomical structures, and leverages the innovative tri-orientated fusion mamba\nmechanism for context modeling and capturing long-range spatial dependencies.\nExtensive experiments on our GCM 2025 dataset and the publicly BraTS 2021\ndataset validate the performance of our framework, demonstrating that the new\napproach surpasses existing methods by up to 1.68\\% in the Dice score while\nmaintaining solid robustness. The dataset and code are public via\nhttps://github.com/JeMing-creater/HWA-UNETR.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10423", "pdf": "https://arxiv.org/pdf/2505.10423", "abs": "https://arxiv.org/abs/2505.10423", "authors": ["Ari Karchmer", "Eran Malach"], "title": "The Power of Random Features and the Limits of Distribution-Free Gradient Descent", "categories": ["cs.LG"], "comment": null, "summary": "We study the relationship between gradient-based optimization of parametric\nmodels (e.g., neural networks) and optimization of linear combinations of\nrandom features. Our main result shows that if a parametric model can be\nlearned using mini-batch stochastic gradient descent (bSGD) without making\nassumptions about the data distribution, then with high probability, the target\nfunction can also be approximated using a polynomial-sized combination of\nrandom features. The size of this combination depends on the number of gradient\nsteps and numerical precision used in the bSGD process. This finding reveals\nfundamental limitations of distribution-free learning in neural networks\ntrained by gradient descent, highlighting why making assumptions about data\ndistributions is often crucial in practice. Along the way, we also introduce a\nnew theoretical framework called average probabilistic dimension complexity\n(adc), which extends the probabilistic dimension complexity developed by Kamath\net al. (2020). We prove that adc has a polynomial relationship with statistical\nquery dimension, and use this relationship to demonstrate an infinite\nseparation between adc and standard dimension complexity.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53c2\u6570\u6a21\u578b\u4f18\u5316\u4e0e\u968f\u673a\u7279\u5f81\u7ec4\u5408\u4f18\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u5206\u5e03\u65e0\u5173\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u5e73\u5747\u6982\u7387\u7ef4\u6570\u590d\u6742\u5ea6\uff08adc\uff09\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u4e2d\u5206\u5e03\u65e0\u5173\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u5206\u5e03\u5047\u8bbe\u7684\u5fc5\u8981\u6027\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5206\u6790\u968f\u673a\u7279\u5f81\u7684\u7ebf\u6027\u7ec4\u5408\u4f18\u5316\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u53c2\u6570\u6a21\u578b\u4f18\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u6846\u67b6\u2014\u2014\u5e73\u5747\u6982\u7387\u7ef4\u6570\u590d\u6742\u5ea6\uff08adc\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5982\u679c\u53c2\u6570\u6a21\u578b\u53ef\u4ee5\u4f7f\u7528\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08bSGD\uff09\u8fdb\u884c\u5b66\u4e60\u800c\u4e0d\u4f9d\u8d56\u6570\u636e\u5206\u5e03\u5047\u8bbe\uff0c\u5219\u76ee\u6807\u51fd\u6570\u53ef\u4ee5\u7528\u591a\u9879\u5f0f\u5927\u5c0f\u7684\u968f\u673a\u7279\u5f81\u7ec4\u5408\u8fd1\u4f3c\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u65f6\u5206\u5e03\u65e0\u5173\u5b66\u4e60\u7684\u6839\u672c\u9650\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5b9e\u8df5\u4e2d\u505a\u51fa\u6570\u636e\u5206\u5e03\u5047\u8bbe\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.10492", "pdf": "https://arxiv.org/pdf/2505.10492", "abs": "https://arxiv.org/abs/2505.10492", "authors": ["Taylor L. Bobrow", "Mayank Golhar", "Suchapa Arayakarnkul", "Anthony A. Song", "Saowanee Ngamruengphong", "Nicholas J. Durr"], "title": "Multi-contrast laser endoscopy for in vivo gastrointestinal imaging", "categories": ["eess.IV", "cs.CV", "physics.optics"], "comment": null, "summary": "White light endoscopy is the clinical gold standard for detecting diseases in\nthe gastrointestinal tract. Most applications involve identifying visual\nabnormalities in tissue color, texture, and shape. Unfortunately, the contrast\nof these features is often subtle, causing many clinically relevant cases to go\nundetected. To overcome this challenge, we introduce Multi-contrast Laser\nEndoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable\nspectral, coherent, and directional illumination. We demonstrate three\ncapabilities of MLE: enhancing tissue chromophore contrast with multispectral\ndiffuse reflectance, quantifying blood flow using laser speckle contrast\nimaging, and characterizing mucosal topography using photometric stereo. We\nvalidate MLE with benchtop models, then demonstrate MLE in vivo during clinical\ncolonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold\nimprovement in contrast and a five-fold improvement in color difference\ncompared to white light and narrow band imaging. With the ability to reveal\nmultiple complementary types of tissue contrast while seamlessly integrating\ninto the clinical environment, MLE shows promise as an investigative tool to\nimprove gastrointestinal imaging.", "AI": {"tldr": "MLE improves gastrointestinal imaging by enhancing tissue contrast through multispectral diffuse reflectance, laser speckle contrast imaging, and photometric stereo.", "motivation": "White light endoscopy often fails to detect clinically relevant cases due to subtle contrast in tissue color, texture, and shape.", "method": "Multi-contrast Laser Endoscopy (MLE) is introduced, which uses rapidly tunable spectral, coherent, and directional illumination to enhance tissue contrast.", "result": "MLE images from 31 polyps show an approximate three-fold improvement in contrast and a five-fold improvement in color difference compared to white light and narrow band imaging.", "conclusion": "MLE shows promise as an investigative tool to improve gastrointestinal imaging."}}
{"id": "2505.10425", "pdf": "https://arxiv.org/pdf/2505.10425", "abs": "https://arxiv.org/abs/2505.10425", "authors": ["Jingyao Wang", "Wenwen Qiang", "Zeen Song", "Changwen Zheng", "Hui Xiong"], "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.", "AI": {"tldr": "L2T is a framework for large language models that improves reasoning effectiveness and efficiency by optimizing the use of each episode in a hierarchical session.", "motivation": "Existing methods overlook the trade-off between reasoning effectiveness and computational efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens.", "method": "L2T is an information-theoretic reinforcement fine-tuning framework that treats each query-response interaction as a hierarchical session of multiple episodes. It proposes a universal dense process reward, which quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. A method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix is also proposed.", "result": "Theoretical analyses show that L2T significantly reduces computational complexity with high estimation accuracy. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency.", "conclusion": "L2T demonstrates advantages across different tasks, boosting both reasoning effectiveness and efficiency."}}
{"id": "2505.10432", "pdf": "https://arxiv.org/pdf/2505.10432", "abs": "https://arxiv.org/abs/2505.10432", "authors": ["Randy J. Chase", "Katherine Haynes", "Lander Ver Hoef", "Imme Ebert-Uphoff"], "title": "Score-based diffusion nowcasting of GOES imagery", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\u5728\u73b0\u5728\u9884\u62a5\u4e91\u548c\u964d\u6c34\u4e2d\u7684\u5e94\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u80fd\u591f\u4e0d\u4ec5\u5e73\u6d41\u73b0\u6709\u7684\u4e91\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\uff0c\u5305\u62ec\u5bf9\u6d41\u7684\u5f00\u59cb\u3002\u6700\u4f73\u7684\u6269\u6563\u6a21\u578b\u662fCorrDiff\u65b9\u6cd5\uff0c\u5728\u6839\u5747\u65b9\u8bef\u5dee\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u548c\u4f20\u7edf\u65b9\u6cd5\u3002\u6269\u6563\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u5f00\u7bb1\u5373\u7528\u7684\u96c6\u5408\u751f\u6210\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6821\u51c6\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\u5728\u6a21\u62df\u4e91\u548c\u964d\u6c34\u65f6\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9700\u8981\u5b50\u7f51\u683c\u53c2\u6570\u5316\u3002\u673a\u5668\u5b66\u4e60\u5df2\u88ab\u7528\u4e8e\u9884\u6d4b\u4e91\u548c\u964d\u6c34\uff0c\u4f46\u65e9\u671f\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f1a\u4ea7\u751f\u6a21\u7cca\u7684\u9884\u6d4b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u8f83\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u73b0\u5728\u9884\u62a5\u4e91\u548c\u964d\u6c34\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u8f83\u65b0\u7684\u65b9\u6cd5\uff0c\u540d\u4e3a\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u73b0\u5728\u9884\u62a5\uff08\u96f6\u5230\u4e09\u5c0f\u65f6\u7684\u9884\u62a5\uff09\u4e91\u548c\u964d\u6c34\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\u7684\u80cc\u666f\u548c\u76f4\u89c9\uff0c\u4ece\u800c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d77\u70b9\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5728\u9884\u62a5\u9759\u6b62\u8f68\u9053\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u5e94\u7528\u3002\u6211\u4eec\u6d4b\u8bd5\u4e86\u4e09\u79cd\u4e3b\u8981\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\uff1a\u6807\u51c6\u7684\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\uff08Diff\uff09\uff1b\u6b8b\u5dee\u6821\u6b63\u6269\u6563\u6a21\u578b\uff08CorrDiff\uff09\uff1b\u4ee5\u53ca\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u3002", "result": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u4e0d\u4ec5\u5e73\u6d41\u73b0\u6709\u7684\u4e91\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\uff0c\u5305\u62ec\u5bf9\u6d41\u7684\u5f00\u59cb\u3002\u8fd9\u4e9b\u7ed3\u679c\u4ee4\u4eba\u60ca\u8bb6\uff0c\u56e0\u4e3a\u9884\u6d4b\u4ec5\u4ee5\u8fc7\u53bb20\u5206\u949f\u7684\u7ea2\u5916\u536b\u661f\u56fe\u50cf\u4e3a\u8d77\u70b9\u3002\u6700\u4f73\u7684\u6269\u6563\u6a21\u578b\u662fCorrDiff\u65b9\u6cd5\uff0c\u5728\u6839\u5747\u65b9\u8bef\u5dee\u4e0a\u6bd4\u6240\u6709\u5176\u4ed6\u6269\u6563\u6a21\u578b\u3001\u4f20\u7edf\u7684U-Net\u548c\u4e00\u4e2a\u6301\u7eed\u6027\u9884\u6d4b\u597d\u4e00\u5230\u4e24\u4e2a\u5f00\u5c14\u6587\u3002\u6269\u6563\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u5f00\u7bb1\u5373\u7528\u7684\u96c6\u5408\u751f\u6210\uff0c\u8fd9\u663e\u793a\u51fa\u6280\u80fd\u826f\u597d\u7684\u6821\u51c6\uff0c\u96c6\u5408\u7684\u8303\u56f4\u4e0e\u8bef\u5dee\u6709\u5f88\u597d\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u4e0d\u4ec5\u5e73\u6d41\u73b0\u6709\u7684\u4e91\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\uff0c\u5305\u62ec\u5bf9\u6d41\u7684\u5f00\u59cb\u3002\u8fd9\u4e9b\u7ed3\u679c\u4ee4\u4eba\u60ca\u8bb6\uff0c\u56e0\u4e3a\u9884\u6d4b\u4ec5\u4ee5\u8fc7\u53bb20\u5206\u949f\u7684\u7ea2\u5916\u536b\u661f\u56fe\u50cf\u4e3a\u8d77\u70b9\u3002\u6700\u4f73\u7684\u6269\u6563\u6a21\u578b\u662fCorrDiff\u65b9\u6cd5\uff0c\u5728\u6839\u5747\u65b9\u8bef\u5dee\u4e0a\u6bd4\u6240\u6709\u5176\u4ed6\u6269\u6563\u6a21\u578b\u3001\u4f20\u7edf\u7684U-Net\u548c\u4e00\u4e2a\u6301\u7eed\u6027\u9884\u6d4b\u597d\u4e00\u5230\u4e24\u4e2a\u5f00\u5c14\u6587\u3002\u6269\u6563\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u5f00\u7bb1\u5373\u7528\u7684\u96c6\u5408\u751f\u6210\uff0c\u8fd9\u663e\u793a\u51fa\u6280\u80fd\u826f\u597d\u7684\u6821\u51c6\uff0c\u96c6\u5408\u7684\u8303\u56f4\u4e0e\u8bef\u5dee\u6709\u5f88\u597d\u7684\u76f8\u5173\u6027\u3002"}}
{"id": "2505.10073", "pdf": "https://arxiv.org/pdf/2505.10073", "abs": "https://arxiv.org/abs/2505.10073", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering", "categories": ["cs.RO", "cs.AI"], "comment": "5 pages, 4 figures, Scheduled for presentation at an upcoming\n  conference", "summary": "In this paper, a novel framework is presented that achieves a combined\nsolution based on Multi-Robot Task Allocation (MRTA) and collision avoidance\nwith respect to homogeneous measurement tasks taking place in industrial\nenvironments. The spatial clustering we propose offers to simultaneously solve\nthe task allocation problem and deal with collision risks by cutting the\nworkspace into distinguishable operational zones for each robot. To divide task\nsites and to schedule robot routes within corresponding clusters, we use\nK-means clustering and the 2-Opt algorithm. The presented framework shows\nsatisfactory performance, where up to 93\\% time reduction (1.24s against\n17.62s) with a solution quality improvement of up to 7\\% compared to the best\nperforming method is demonstrated. Our method also completely eliminates\ncollision points that persist in comparative methods in a most significant\nsense. Theoretical analysis agrees with the claim that spatial partitioning\nunifies the apparently disjoint tasks allocation and collision avoidance\nproblems under conditions of many identical tasks to be distributed over sparse\ngeographical areas. Ultimately, the findings in this work are of substantial\nimportance for real world applications where both computational efficiency and\noperation free from collisions is of paramount importance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u548c\u907f\u969c\uff0c\u901a\u8fc7\u7a7a\u95f4\u805a\u7c7b\u5212\u5206\u5de5\u4f5c\u533a\uff0c\u5e76\u4f7f\u7528K-means\u805a\u7c7b\u548c2-Opt\u7b97\u6cd5\u8fdb\u884c\u4efb\u52a1\u5212\u5206\u548c\u8def\u5f84\u89c4\u5212\u3002\u8be5\u6846\u67b6\u5728\u65f6\u95f4\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5b8c\u5168\u6d88\u9664\u4e86\u78b0\u649e\u70b9\u3002", "motivation": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u9488\u5bf9\u540c\u8d28\u6d4b\u91cf\u4efb\u52a1\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u4efb\u52a1\u5206\u914d\u548c\u78b0\u649e\u98ce\u9669\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\uff08MRTA\uff09\u548c\u907f\u969c\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u805a\u7c7b\u5c06\u5de5\u4f5c\u533a\u5212\u5206\u4e3a\u53ef\u533a\u5206\u7684\u64cd\u4f5c\u533a\u57df\uff0c\u4f7f\u7528K-means\u805a\u7c7b\u548c2-Opt\u7b97\u6cd5\u6765\u5212\u5206\u4efb\u52a1\u70b9\u5e76\u5b89\u6392\u673a\u5668\u4eba\u8def\u5f84\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u8868\u73b0\u51fa\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\uff0c\u4e0e\u6700\u4f73\u8868\u73b0\u65b9\u6cd5\u76f8\u6bd4\uff0c\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe93%\uff081.24\u79d2\u5bf917.62\u79d2\uff09\uff0c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u9ad8\u4e86\u9ad8\u8fbe7%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b8c\u5168\u6d88\u9664\u4e86\u6bd4\u8f83\u65b9\u6cd5\u4e2d\u6301\u7eed\u5b58\u5728\u7684\u78b0\u649e\u70b9\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u53d1\u73b0\u5bf9\u4e8e\u9700\u8981\u8ba1\u7b97\u6548\u7387\u548c\u65e0\u78b0\u649e\u64cd\u4f5c\u7684\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.10438", "pdf": "https://arxiv.org/pdf/2505.10438", "abs": "https://arxiv.org/abs/2505.10438", "authors": ["David Grasev"], "title": "Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "51 pages, 28 figures", "summary": "Gas turbine engines represent complex highly nonlinear dynamical systems.\nDeriving their physics-based models can be challenging as it requires\nperformance characteristics, that are not always available, and one often has\nto make many simplifying assumptions. In this paper, the limitations of\nconventional experimental methods used to derive component-level and locally\nlinear parameter-varying models are discussed and addressed by employing\nidentification techniques based on data collected from standard engine\noperation under closed-loop control. The rotor dynamics were estimated using\nthe sparse identification of nonlinear dynamics. Subsequently, the autonomous\npart of the dynamics was mapped into an optimally constructed Koopman\neigenfunction space. The process included eigenvalue optimization using\nmetaheuristic algorithms and temporal projection, followed by gradient-based\neigenfunction identification. The resulting Koopman model was validated against\nan in-house reference component-level model. A globally optimal nonlinear\nfeedback controller and a Kalman estimator were then designed in the\neigenfunction space and compared to the classical and gain-scheduled\nproportional-integral controllers, as well as a proposed internal model control\napproach. The eigenmode structure allowed targeting individual modes during the\noptimization process, resulting in a better performance tuning. The results\nshowed that the Koopman-based controller outperformed the other benchmark\ncontrollers in both reference tracking and disturbance rejection, under\nsea-level and varying flight conditions, due to its global nature.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u5728\u63a8\u5bfc\u7ec4\u4ef6\u7ea7\u548c\u5c40\u90e8\u7ebf\u6027\u53c2\u6570\u53d8\u5316\u6a21\u578b\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u4ece\u6807\u51c6\u53d1\u52a8\u673a\u64cd\u4f5c\u4e2d\u6536\u96c6\u7684\u6570\u636e\u8fdb\u884c\u8bc6\u522b\u6280\u672f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eKoopman\u7684\u63a7\u5236\u5668\u5728\u53c2\u8003\u8ddf\u8e2a\u548c\u6270\u52a8\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u63a7\u5236\u5668\u3002", "motivation": "\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u5728\u63a8\u5bfc\u7ec4\u4ef6\u7ea7\u548c\u5c40\u90e8\u7ebf\u6027\u53c2\u6570\u53d8\u5316\u6a21\u578b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u7269\u7406\u6a21\u578b\u7684\u63a8\u5bfc\u9700\u8981\u6027\u80fd\u7279\u6027\uff0c\u8fd9\u4e9b\u7279\u6027\u5e76\u4e0d\u603b\u662f\u53ef\u7528\u7684\u3002", "method": "\u4f7f\u7528\u4ece\u6807\u51c6\u53d1\u52a8\u673a\u64cd\u4f5c\u4e2d\u6536\u96c6\u7684\u6570\u636e\u8fdb\u884c\u8bc6\u522b\u6280\u672f\uff0c\u4f30\u8ba1\u8f6c\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u5c06\u81ea\u4e3b\u52a8\u529b\u5b66\u6620\u5c04\u5230\u6700\u4f18\u6784\u9020\u7684Koopman\u7279\u5f81\u51fd\u6570\u7a7a\u95f4\u4e2d\u3002\u968f\u540e\uff0c\u901a\u8fc7\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u8fdb\u884c\u7279\u5f81\u503c\u4f18\u5316\u548c\u65f6\u95f4\u6295\u5f71\uff0c\u7136\u540e\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u7279\u5f81\u51fd\u6570\u8bc6\u522b\u3002", "result": "\u57fa\u4e8eKoopman\u7684\u63a7\u5236\u5668\u5728\u53c2\u8003\u8ddf\u8e2a\u548c\u6270\u52a8\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u63a7\u5236\u5668\uff0c\u7279\u522b\u662f\u5728\u6d77\u5e73\u9762\u548c\u4e0d\u540c\u98de\u884c\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8eKoopman\u7684\u63a7\u5236\u5668\u5728\u53c2\u8003\u8ddf\u8e2a\u548c\u6270\u52a8\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u63a7\u5236\u5668\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u5168\u5c40\u6027\u8d28\u3002"}}
{"id": "2505.10558", "pdf": "https://arxiv.org/pdf/2505.10558", "abs": "https://arxiv.org/abs/2505.10558", "authors": ["Peiying Zhang", "Nanxuan Zhao", "Jing Liao"], "title": "Style Customization of Text-to-Vector Generation with Image Diffusion Priors", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Paper). Project page:\n  https://customsvg.github.io", "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u98ce\u683c\u5b9a\u5236\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u7ed3\u6784\u89c4\u5f8b\u6027\u548c\u591a\u6837\u5316\u8868\u8fbe\u80fd\u529b\u7684\u81ea\u5b9a\u4e49\u98ce\u683cSVG\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u5411\u91cf\uff08T2V\uff09\u751f\u6210\u65b9\u6cd5\u5728\u521b\u5efaSVG\u65f6\u5f80\u5f80\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u9700\u6c42\uff1a\u98ce\u683c\u5b9a\u5236\uff0c\u8fd9\u5bf9\u4e8e\u751f\u6210\u5177\u6709\u7edf\u4e00\u89c6\u89c9\u5916\u89c2\u548c\u8fde\u8d2f\u7f8e\u5b66\u7684\u4e00\u7ec4\u77e2\u91cf\u56fe\u5f62\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u98ce\u683c\u5b9a\u5236\u7ba1\u9053\uff0c\u5229\u7528\u524d\u9988T2V\u6a21\u578b\u548cT2I\u56fe\u50cf\u5148\u9a8c\u7684\u4f18\u52bf\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5177\u6709\u8def\u5f84\u7ea7\u8868\u793a\u7684T2V\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u786e\u4fddSVG\u7684\u7ed3\u6784\u89c4\u5f8b\u6027\u5e76\u4fdd\u7559\u591a\u6837\u7684\u8868\u8fbe\u80fd\u529b\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u901a\u8fc7\u84b8\u998f\u5b9a\u5236\u7684T2I\u6a21\u578b\u6765\u5b9a\u5236T2V\u6269\u6563\u6a21\u578b\u4ee5\u9002\u5e94\u4e0d\u540c\u98ce\u683c\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u80fd\u591f\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u4ee5\u9ad8\u6548\u7684\u524d\u9988\u65b9\u5f0f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u81ea\u5b9a\u4e49\u98ce\u683cSVG\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u524d\u9988T2V\u6a21\u578b\u548cT2I\u56fe\u50cf\u5148\u9a8c\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u98ce\u683c\u5b9a\u5236\u4e2d\u7684\u6311\u6218\uff0c\u80fd\u591f\u5728\u9ad8\u6548\u524d\u9988\u65b9\u5f0f\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u81ea\u5b9a\u4e49\u98ce\u683cSVG\u3002"}}
{"id": "2505.10101", "pdf": "https://arxiv.org/pdf/2505.10101", "abs": "https://arxiv.org/abs/2505.10101", "authors": ["Jongmin Jung", "Dasaem Jeong"], "title": "LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.MM", "eess.AS"], "comment": "Paper accepted at ISEA 2025, The 30th International Symposium on\n  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025", "summary": "This paper introduces LAV (Latent Audio-Visual), a system that integrates\nEnCodec's neural audio compression with StyleGAN2's generative capabilities to\nproduce visually dynamic outputs driven by pre-recorded audio. Unlike previous\nworks that rely on explicit feature mappings, LAV uses EnCodec embeddings as\nlatent representations, directly transformed into StyleGAN2's style latent\nspace via randomly initialized linear mapping. This approach preserves semantic\nrichness in the transformation, enabling nuanced and semantically coherent\naudio-visual translations. The framework demonstrates the potential of using\npretrained audio compression models for artistic and computational\napplications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LAV\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u5408EnCodec\u548cStyleGAN2\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u97f3\u9891\u751f\u6210\u89c6\u89c9\u52a8\u6001\u5185\u5bb9\uff0c\u5e76\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\u3002", "motivation": "\u4ee5\u5f80\u7684\u5de5\u4f5c\u4f9d\u8d56\u4e8e\u663e\u5f0f\u7684\u7279\u5f81\u6620\u5c04\uff0c\u800cLAV\u901a\u8fc7\u4fdd\u7559\u8bed\u4e49\u4e30\u5bcc\u6027\u6765\u5b9e\u73b0\u66f4\u7ec6\u81f4\u548c\u8bed\u4e49\u8fde\u8d2f\u7684\u97f3\u89c6\u9891\u8f6c\u6362\u3002", "method": "LAV\u7ed3\u5408\u4e86EnCodec\u7684\u795e\u7ecf\u97f3\u9891\u538b\u7f29\u548cStyleGAN2\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f7f\u7528EnCodec\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u521d\u59cb\u5316\u7684\u7ebf\u6027\u6620\u5c04\u5c06\u5176\u76f4\u63a5\u8f6c\u6362\u4e3aStyleGAN2\u7684\u98ce\u683c\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "LAV\u80fd\u591f\u751f\u6210\u89c6\u89c9\u52a8\u6001\u8f93\u51fa\uff0c\u8fd9\u4e9b\u8f93\u51fa\u7531\u9884\u5148\u5f55\u5236\u7684\u97f3\u9891\u9a71\u52a8\uff0c\u5e76\u4e14\u5728\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u97f3\u9891\u538b\u7f29\u6a21\u578b\u5728\u827a\u672f\u548c\u8ba1\u7b97\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10105", "pdf": "https://arxiv.org/pdf/2505.10105", "abs": "https://arxiv.org/abs/2505.10105", "authors": ["Zibin Dong", "Fei Ni", "Yifu Yuan", "Yinchuan Li", "Jianye Hao"], "title": "EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We present EmbodiedMAE, a unified 3D multi-modal representation for robot\nmanipulation. Current approaches suffer from significant domain gaps between\ntraining datasets and robot manipulation tasks, while also lacking model\narchitectures that can effectively incorporate 3D information. To overcome\nthese limitations, we enhance the DROID dataset with high-quality depth maps\nand point clouds, constructing DROID-3D as a valuable supplement for 3D\nembodied vision research. Then we develop EmbodiedMAE, a multi-modal masked\nautoencoder that simultaneously learns representations across RGB, depth, and\npoint cloud modalities through stochastic masking and cross-modal fusion.\nTrained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art\nvision foundation models (VFMs) in both training efficiency and final\nperformance across 70 simulation tasks and 20 real-world robot manipulation\ntasks on two robot platforms. The model exhibits strong scaling behavior with\nsize and promotes effective policy learning from 3D inputs. Experimental\nresults establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for\nembodied AI systems, particularly in precise tabletop manipulation settings\nwhere spatial perception is critical.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 EmbodiedMAE\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7edf\u4e00 3D \u591a\u6a21\u6001\u8868\u793a\u3002\u901a\u8fc7\u589e\u5f3a DROID \u6570\u636e\u96c6\u5e76\u6784\u5efa DROID-3D\uff0c\u5f00\u53d1\u4e86 EmbodiedMAE\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7a7a\u95f4\u611f\u77e5\u7684\u7cbe\u786e\u684c\u9762\u64cd\u4f5c\u573a\u666f\u4e2d\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u5e76\u4e14\u7f3a\u4e4f\u80fd\u591f\u6709\u6548\u7ed3\u5408 3D \u4fe1\u606f\u7684\u6a21\u578b\u67b6\u6784\u3002", "method": "\u6211\u4eec\u589e\u5f3a\u4e86 DROID \u6570\u636e\u96c6\uff0c\u52a0\u5165\u4e86\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u56fe\u548c\u70b9\u4e91\uff0c\u6784\u5efa\u4e86 DROID-3D\uff0c\u7136\u540e\u5f00\u53d1\u4e86 EmbodiedMAE\uff0c\u8fd9\u662f\u4e00\u79cd\u591a\u6a21\u6001\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u968f\u673a\u63a9\u7801\u548c\u8de8\u6a21\u6001\u878d\u5408\u540c\u65f6\u5b66\u4e60 RGB\u3001\u6df1\u5ea6\u548c\u70b9\u4e91\u6a21\u6001\u7684\u8868\u793a\u3002", "result": "\u5728 DROID-3D \u4e0a\u8bad\u7ec3\u540e\uff0cEmbodiedMAE \u5728 70 \u4e2a\u4eff\u771f\u4efb\u52a1\u548c 20 \u4e2a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u65e0\u8bba\u662f\u8bad\u7ec3\u6548\u7387\u8fd8\u662f\u6700\u7ec8\u6027\u80fd\uff0c\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b (VFMs)\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEmbodiedMAE \u662f\u4e00\u79cd\u53ef\u9760\u7684\u7edf\u4e00 3D \u591a\u6a21\u6001 VFM\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u7a7a\u95f4\u611f\u77e5\u7684\u7cbe\u786e\u684c\u9762\u64cd\u4f5c\u573a\u666f\u3002"}}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472", "abs": "https://arxiv.org/abs/2505.10472", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "categories": ["cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u764c\u75c7\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u901a\u7528LLMs\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u6548\u679c\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u533b\u5b66LLMs\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u65b9\u9762\u66f4\u4f18\uff0c\u4f46\u5b58\u5728\u66f4\u9ad8\u7684\u6f5c\u5728\u5371\u5bb3\u548c\u504f\u89c1\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u53cc\u91cd\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdbAI\u751f\u6210\u5065\u5eb7\u5185\u5bb9\u7684\u5efa\u8bae\u3002", "motivation": "\u6709\u6548\u6c9f\u901a\u5173\u4e8e\u4e73\u817a\u764c\u548c\u5bab\u9888\u764c\u7684\u95ee\u9898\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u5065\u5eb7\u6311\u6218\uff0c\u516c\u4f17\u5bf9\u764c\u75c7\u9884\u9632\u3001\u7b5b\u67e5\u548c\u6cbb\u7597\u7684\u7406\u89e3\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bca\u65ad\u5ef6\u8fdf\u548c\u6cbb\u7597\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u51c6\u786e\u3001\u5b89\u5168\u548c\u53ef\u8bbf\u95ee\u7684\u764c\u75c7\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u7684\u80fd\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u4ee5\u652f\u6301\u60a3\u8005\u7406\u89e3\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u8a00\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3001\u6c9f\u901a\u53ef\u53ca\u6027\u548c\u6548\u679c\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u901a\u7528\u548c\u4e09\u79cd\u533b\u5b66LLMs\u3002\u6211\u4eec\u4f7f\u7528\u5b9a\u91cf\u6307\u6807\u3001\u5b9a\u6027\u4e13\u5bb6\u8bc4\u5206\u548c\u7edf\u8ba1\u5206\u6790\uff08\u5982Welch\u7684ANOVA\u3001Games-Howell\u548cHedges' g\uff09\u8fdb\u884c\u5206\u6790\u3002", "result": "\u901a\u7528LLMs\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u6548\u679c\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800c\u533b\u5b66LLMs\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002\u7136\u800c\uff0c\u533b\u5b66LLMs\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6f5c\u5728\u5371\u5bb3\u3001\u6bd2\u6027\u4ee5\u53ca\u504f\u89c1\uff0c\u8fd9\u964d\u4f4e\u4e86\u5b83\u4eec\u5728\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u7528\u4e8e\u764c\u75c7\u6c9f\u901a\u7684LLMs\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u4e3a\u6539\u8fdbAI\u751f\u6210\u7684\u5065\u5eb7\u5185\u5bb9\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u5f00\u53d1\u51c6\u786e\u3001\u5b89\u5168\u548c\u53ef\u8bbf\u95ee\u7684\u6570\u5b57\u5065\u5eb7\u5de5\u5177\u3002"}}
{"id": "2505.10134", "pdf": "https://arxiv.org/pdf/2505.10134", "abs": "https://arxiv.org/abs/2505.10134", "authors": ["Guangjin Pan", "Kaixuan Huang", "Hui Chen", "Shunqing Zhang", "Christian H\u00e4ger", "Henk Wymeersch"], "title": "Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "13 pages,16 figures.This work has been submitted to the IEEE for\n  possible publication", "summary": "Accurate and robust localization is a critical enabler for emerging 5G and 6G\napplications, including autonomous driving, extended reality (XR), and smart\nmanufacturing. While data-driven approaches have shown promise, most existing\nmodels require large amounts of labeled data and struggle to generalize across\ndeployment scenarios and wireless configurations. To address these limitations,\nwe propose a foundation-model-based solution tailored for wireless\nlocalization. We first analyze how different self-supervised learning (SSL)\ntasks acquire general-purpose and task-specific semantic features based on\ninformation bottleneck (IB) theory. Building on this foundation, we design a\npretraining methodology for the proposed Large Wireless Localization Model\n(LWLM). Specifically, we propose an SSL framework that jointly optimizes three\ncomplementary objectives: (i) spatial-frequency masked channel modeling\n(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)\nposition-invariant contrastive learning (PICL). These objectives jointly\ncapture the underlying semantics of wireless channel from multiple\nperspectives. We further design lightweight decoders for key downstream tasks,\nincluding time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,\nsingle base station (BS) localization, and multiple BS localization.\nComprehensive experimental results confirm that LWLM consistently surpasses\nboth model-based and supervised learning baselines across all localization\ntasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer\nmodels without pretraining, and exhibits strong generalization under\nlabel-limited fine-tuning and unseen BS configurations, confirming its\npotential as a foundation model for wireless localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u65e0\u7ebf\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u591a\u4e2a\u76ee\u6807\uff0c\u5b9e\u73b0\u4e86\u5728\u5404\u79cd\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u5b9a\u4f4d\u662f\u65b0\u51745G\u548c6G\u5e94\u7528\u7684\u5173\u952e\u4f7f\u80fd\u5668\uff0c\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u3001\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u548c\u667a\u80fd\u5236\u9020\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u4e14\u5728\u90e8\u7f72\u573a\u666f\u548c\u65e0\u7ebf\u914d\u7f6e\u4e4b\u95f4\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u51fa\u7684\u5927\u578b\u65e0\u7ebf\u5b9a\u4f4d\u6a21\u578b\uff08LWLM\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u4e92\u8865\u76ee\u6807\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff1a(i) \u7a7a\u95f4-\u9891\u7387\u63a9\u7801\u4fe1\u9053\u5efa\u6a21\uff08SF-MCM\uff09\uff0c(ii) \u57df\u53d8\u6362\u4e0d\u53d8\u6027\uff08DTI\uff09\uff0c\u4ee5\u53ca(iii) \u4f4d\u7f6e\u4e0d\u53d8\u5bf9\u6bd4\u5b66\u4e60\uff08PICL\uff09\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0cLWLM\u5728\u6240\u6709\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u90fd\u4e00\u81f4\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6a21\u578b\u548c\u76d1\u7763\u5b66\u4e60\u7684\u57fa\u7ebf\u3002\u7279\u522b\u662f\uff0cLWLM\u5728\u6ca1\u6709\u9884\u8bad\u7ec3\u7684\u53d8\u538b\u5668\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8626.0%--87.5%\u7684\u6539\u8fdb\uff0c\u5e76\u5728\u6807\u7b7e\u6709\u9650\u7684\u5fae\u8c03\u548c\u672a\u89c1\u8fc7\u7684\u57fa\u7ad9\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LWLM\u5728\u6240\u6709\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u57fa\u4e8e\u6a21\u578b\u548c\u76d1\u7763\u5b66\u4e60\u7684\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u65e0\u7ebf\u5b9a\u4f4d\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10482", "pdf": "https://arxiv.org/pdf/2505.10482", "abs": "https://arxiv.org/abs/2505.10482", "authors": ["Ningyuan Yang", "Jiaxuan Gao", "Feng Gao", "Yi Wu", "Chao Yu"], "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures", "summary": "Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.", "AI": {"tldr": "The paper introduces NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps. The experiments show that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks.", "motivation": "Diffusion policies, widely adopted in decision-making scenarios such as robotics, gaming and autonomous driving, are capable of learning diverse skills from demonstration data due to their high representation power. However, the sub-optimal and limited coverage of demonstration data could lead to diffusion policies that generate sub-optimal trajectories and even catastrophic failures. While reinforcement learning (RL)-based fine-tuning has emerged as a promising solution to address these limitations, existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This challenge stems from the computational intractability of action likelihood estimation during the denoising process, which leads to complicated optimization objectives.", "method": "We introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps.", "result": "Our experiments demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.", "conclusion": "NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy."}}
{"id": "2505.10484", "pdf": "https://arxiv.org/pdf/2505.10484", "abs": "https://arxiv.org/abs/2505.10484", "authors": ["Andrea Baisero", "Rupali Bhati", "Shuo Liu", "Aathira Pillai", "Christopher Amato"], "title": "Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Value function decomposition methods for cooperative multi-agent\nreinforcement learning compose joint values from individual per-agent\nutilities, and train them using a joint objective. To ensure that the action\nselection process between individual utilities and joint values remains\nconsistent, it is imperative for the composition to satisfy the\nindividual-global max (IGM) property. Although satisfying IGM itself is\nstraightforward, most existing methods (e.g., VDN, QMIX) have limited\nrepresentation capabilities and are unable to represent the full class of IGM\nvalues, and the one exception that has no such limitation (QPLEX) is\nunnecessarily complex. In this work, we present a simple formulation of the\nfull class of IGM values that naturally leads to the derivation of QFIX, a\nnovel family of value function decomposition models that expand the\nrepresentation capabilities of prior models by means of a thin \"fixing\" layer.\nWe derive multiple variants of QFIX, and implement three variants in two\nwell-known multi-agent frameworks. We perform an empirical evaluation on\nmultiple SMACv2 and Overcooked environments, which confirms that QFIX (i)\nsucceeds in enhancing the performance of prior methods, (ii) learns more stably\nand performs better than its main competitor QPLEX, and (iii) achieves this\nwhile employing the simplest and smallest mixing models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86QFIX\uff0c\u4e00\u79cd\u65b0\u7684\u4ef7\u503c\u51fd\u6570\u5206\u89e3\u6a21\u578b\uff0c\u80fd\u591f\u589e\u5f3a\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7b80\u5355\u548c\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\uff08\u5982VDN\u3001QMIX\uff09\u5728\u8868\u793a\u80fd\u529b\u4e0a\u6709\u9650\uff0c\u65e0\u6cd5\u8868\u793a\u6240\u6709IGM\u503c\uff0c\u800c\u552f\u4e00\u7684\u4f8b\u5916QPLEX\u5219\u8fc7\u4e8e\u590d\u6742\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u6574\u7684IGM\u503c\u7684\u7b80\u5355\u516c\u5f0f\uff0c\u81ea\u7136\u5bfc\u81f4\u4e86QFIX\u7684\u63a8\u5bfc\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u4ef7\u503c\u51fd\u6570\u5206\u89e3\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u4e00\u4e2a\u8584\u7684\u201c\u4fee\u590d\u201d\u5c42\u6269\u5c55\u4e86\u5148\u524d\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "QFIX\u5728\u591a\u4e2aSMACv2\u548cOvercooked\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u80fd\u591f\u63d0\u9ad8\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5b66\u4e60\u66f4\u52a0\u7a33\u5b9a\uff0c\u5e76\u4e14\u4f18\u4e8eQPLEX\u3002", "conclusion": "QFIX\u6210\u529f\u5730\u589e\u5f3a\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5b66\u4e60\u66f4\u52a0\u7a33\u5b9a\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u6700\u7b80\u5355\u548c\u6700\u5c0f\u7684\u6df7\u5408\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u5176\u4e3b\u8981\u7ade\u4e89\u5bf9\u624bQPLEX\u3002"}}
{"id": "2505.10183", "pdf": "https://arxiv.org/pdf/2505.10183", "abs": "https://arxiv.org/abs/2505.10183", "authors": ["Jieke Lin", "Wanyu Wang", "Longxiang Yin", "Yinhe Han"], "title": "KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems", "categories": ["cs.DC", "cs.AI"], "comment": "9 pages, 4 figures. Jieke Lin and Wanyu Wang contributed equally to\n  this work", "summary": "Embodied Artificial Intelligence (AI) systems, such as autonomous robots and\nintelligent vehicles, are increasingly reliant on diverse heterogeneous\naccelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing\nand energy-efficiency demands. However, the proliferation of vendor-specific\nproprietary communication libraries creates significant interoperability\nbarriers, hindering seamless collaboration between different accelerator types\nand leading to suboptimal resource utilization and performance bottlenecks in\ndistributed AI workloads. This paper introduces KAITIAN, a novel distributed\ncommunication framework designed to bridge this gap. KAITIAN provides a unified\nabstraction layer that intelligently integrates vendor-optimized communication\nlibraries for intra-group efficiency with general-purpose communication\nprotocols for inter-group interoperability. Crucially, it incorporates a\nload-adaptive scheduling mechanism that dynamically balances computational\ntasks across heterogeneous devices based on their real-time performance\ncharacteristics. Implemented as an extension to PyTorch and rigorously\nevaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN\ndemonstrates significant improvements in resource utilization and scalability\nfor distributed training tasks. Experimental results show that KAITIAN can\naccelerate training time by up to 42% compared to baseline homogeneous systems,\nwhile incurring minimal communication overhead (2.8--4.3%) and maintaining\nmodel accuracy. KAITIAN paves the way for more flexible and powerful\nheterogeneous computing in complex embodied AI applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86KAITIAN\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u901a\u4fe1\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f02\u6784\u52a0\u901f\u5668\u4e4b\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002KAITIAN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u62bd\u8c61\u5c42\uff0c\u7ed3\u5408\u4e86\u5382\u5546\u4f18\u5316\u7684\u901a\u4fe1\u5e93\u548c\u901a\u7528\u901a\u4fe1\u534f\u8bae\uff0c\u5e76\u91c7\u7528\u8d1f\u8f7d\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u4efb\u52a1\u7684\u8d44\u6e90\u5229\u7528\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "Embodied Artificial Intelligence (AI) systems, such as autonomous robots and intelligent vehicles, are increasingly reliant on diverse heterogeneous accelerators to meet stringent real-time processing and energy-efficiency demands. However, the proliferation of vendor-specific proprietary communication libraries creates significant interoperability barriers, hindering seamless collaboration between different accelerator types and leading to suboptimal resource utilization and performance bottlenecks in distributed AI workloads.", "method": "KAITIAN provides a unified abstraction layer that intelligently integrates vendor-optimized communication libraries for intra-group efficiency with general-purpose communication protocols for inter-group interoperability. It incorporates a load-adaptive scheduling mechanism that dynamically balances computational tasks across heterogeneous devices based on their real-time performance characteristics.", "result": "KAITIAN demonstrates significant improvements in resource utilization and scalability for distributed training tasks. Experimental results show that KAITIAN can accelerate training time by up to 42% compared to baseline homogeneous systems, while incurring minimal communication overhead (2.8--4.3%) and maintaining model accuracy.", "conclusion": "KAITIAN paves the way for more flexible and powerful heterogeneous computing in complex embodied AI applications."}}
{"id": "2505.10515", "pdf": "https://arxiv.org/pdf/2505.10515", "abs": "https://arxiv.org/abs/2505.10515", "authors": ["Seongun Kim", "Sol A Kim", "Geonhyeong Kim", "Enver Menadjiev", "Chanwoo Lee", "Seongwook Chung", "Nari Kim", "Jaesik Choi"], "title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684 XAI \u6846\u67b6 PnPXAI\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709 XAI \u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u548c\u4f18\u5316\u63d0\u4f9b\u6700\u4f73\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u7684 XAI \u6846\u67b6\u5b58\u5728\u7075\u6d3b\u6027\u4e0d\u8db3\u3001\u652f\u6301\u7684 XAI \u65b9\u6cd5\u6709\u9650\u4ee5\u53ca\u89e3\u91ca\u63a8\u8350\u4e0d\u4f18\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86 XAI \u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91c7\u7528\u3002", "method": "PnPXAI \u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u67b6\u6784\u3001\u63a8\u8350\u9002\u7528\u7684\u89e3\u91ca\u65b9\u6cd5\u548c\u4f18\u5316\u8d85\u53c2\u6570\u6765\u5b9e\u73b0\u5176\u76ee\u6807\u3002", "result": "PnPXAI \u5728\u7528\u6237\u8c03\u67e5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u533b\u5b66\u548c\u91d1\u878d\u7b49\u591a\u4e2a\u9886\u57df\u7684\u591a\u529f\u80fd\u6027\u3002", "conclusion": "PnPXAI \u662f\u4e00\u4e2a\u901a\u7528\u7684 XAI \u6846\u67b6\uff0c\u80fd\u591f\u652f\u6301\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u67b6\u6784\u3001\u63a8\u8350\u9002\u7528\u7684\u89e3\u91ca\u65b9\u6cd5\u548c\u4f18\u5316\u8d85\u53c2\u6570\u6765\u63d0\u4f9b\u6700\u4f73\u89e3\u91ca\u3002"}}
{"id": "2505.10191", "pdf": "https://arxiv.org/pdf/2505.10191", "abs": "https://arxiv.org/abs/2505.10191", "authors": ["Qingyu Zheng", "Qi Shao", "Guijun Han", "Wei Li", "Hong Li", "Xuan Wang"], "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting", "categories": ["physics.ao-ph", "cs.AI", "cs.LG", "nlin.CD"], "comment": "22 pages, 6 figures", "summary": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the\nocean, and their impact on the energy cascade of the global ocean cannot be\nignored. Eddy-resolving ocean forecasting is providing more reliable protection\nfor fisheries and navigational safety, but also presents significant scientific\nchallenges and high computational costs for traditional numerical models.\nArtificial intelligence (AI)-based weather and ocean forecasting systems are\nbecoming powerful tools that balance forecast performance with computational\nefficiency. However, the complex multiscale features in the ocean dynamical\nsystem make AI models still face many challenges in mesoscale eddy forecasting\n(especially regional modelling). Here, we develop LanTu, a regional\neddy-resolving ocean forecasting system based on dynamics-enhanced deep\nlearning. We incorporate cross-scale interactions into LanTu and construct\nmultiscale physical constraint for optimising LanTu guided by knowledge of eddy\ndynamics in order to improve the forecasting skill of LanTu for mesoscale\nevolution. The results show that LanTu outperforms the existing advanced\noperational numerical ocean forecasting system (NOFS) and AI-based ocean\nforecasting system (AI-OFS) in temperature, salinity, sea level anomaly and\ncurrent prediction, with a lead time of more than 10 days. Our study highlights\nthat dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for\neddy-resolving ocean forecasting.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LanTu\uff0c\u4e00\u4e2a\u57fa\u4e8e\u589e\u5f3a\u52a8\u529b\u5b66\u7684\u6df1\u5ea6\u5b66\u4e60\u7684\u533a\u57df\u6da1\u65cb\u89e3\u6790\u6d77\u6d0b\u9884\u62a5\u7cfb\u7edf\u3002\u901a\u8fc7\u5f15\u5165\u8de8\u5c3a\u5ea6\u76f8\u4e92\u4f5c\u7528\u548c\u6784\u5efa\u591a\u5c3a\u5ea6\u7269\u7406\u7ea6\u675f\uff0cLanTu\u5728\u591a\u4e2a\u6d77\u6d0b\u53d8\u91cf\u7684\u9884\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6570\u503c\u548c\u4eba\u5de5\u667a\u80fd\u9884\u62a5\u7cfb\u7edf\u3002", "motivation": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the ocean, and their impact on the energy cascade of the global ocean cannot be ignored. Eddy-resolving ocean forecasting is providing more reliable protection for fisheries and navigational safety, but also presents significant scientific challenges and high computational costs for traditional numerical models. Artificial intelligence (AI)-based weather and ocean forecasting systems are becoming powerful tools that balance forecast performance with computational efficiency. However, the complex multiscale features in the ocean dynamical system make AI models still face many challenges in mesoscale eddy forecasting (especially regional modelling).", "method": "\u6211\u4eec\u5f00\u53d1\u4e86LanTu\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u589e\u5f3a\u52a8\u529b\u5b66\u7684\u6df1\u5ea6\u5b66\u4e60\u7684\u533a\u57df\u6da1\u65cb\u89e3\u6790\u6d77\u6d0b\u9884\u62a5\u7cfb\u7edf\u3002\u6211\u4eec\u5c06\u8de8\u5c3a\u5ea6\u76f8\u4e92\u4f5c\u7528\u7eb3\u5165LanTu\uff0c\u5e76\u6784\u5efa\u591a\u5c3a\u5ea6\u7269\u7406\u7ea6\u675f\uff0c\u4ee5\u4f18\u5316LanTu\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u5bf9\u4e2d\u5c3a\u5ea6\u6f14\u5316\u7684\u9884\u6d4b\u6280\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLanTu\u5728\u6e29\u5ea6\u3001\u76d0\u5ea6\u3001\u6d77\u5e73\u9762\u5f02\u5e38\u548c\u7535\u6d41\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u64cd\u4f5c\u6570\u503c\u6d77\u6d0b\u9884\u62a5\u7cfb\u7edf\uff08NOFS\uff09\u548c\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6d77\u6d0b\u9884\u62a5\u7cfb\u7edf\uff08AI-OFS\uff09\uff0c\u9884\u6d4b\u65f6\u6548\u8d85\u8fc710\u5929\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\uff0c\u589e\u5f3a\u52a8\u529b\u5b66\u7684\u6df1\u5ea6\u5b66\u4e60\uff08LanTu\uff09\u53ef\u4ee5\u6210\u4e3a\u6da1\u65cb\u89e3\u6790\u6d77\u6d0b\u9884\u62a5\u7684\u5f3a\u5927\u8303\u5f0f\u3002"}}
{"id": "2505.10545", "pdf": "https://arxiv.org/pdf/2505.10545", "abs": "https://arxiv.org/abs/2505.10545", "authors": ["Amira Alakhdar", "Barnabas Poczos", "Newell Washburn"], "title": "Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design", "categories": ["cs.LG"], "comment": null, "summary": "Developing bioactive molecules remains a central, time- and cost-heavy\nchallenge in drug discovery, particularly for novel targets lacking structural\nor functional data. Pharmacophore modeling presents an alternative for\ncapturing the key features required for molecular bioactivity against a\nbiological target. In this work, we present PharmaDiff, a\npharmacophore-conditioned diffusion model for 3D molecular generation.\nPharmaDiff employs a transformer-based architecture to integrate an atom-based\nrepresentation of the 3D pharmacophore into the generative process, enabling\nthe precise generation of 3D molecular graphs that align with predefined\npharmacophore hypotheses. Through comprehensive testing, PharmaDiff\ndemonstrates superior performance in matching 3D pharmacophore constraints\ncompared to ligand-based drug design methods. Additionally, it achieves higher\ndocking scores across a range of proteins in structure-based drug design,\nwithout the need for target protein structures. By integrating pharmacophore\nmodeling with 3D generative techniques, PharmaDiff offers a powerful and\nflexible framework for rational drug design.", "AI": {"tldr": "PharmaDiff is a pharmacophore-conditioned diffusion model that integrates an atom-based representation of the 3D pharmacophore into the generative process, enabling the precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses. It outperforms ligand-based drug design methods and achieves higher docking scores without requiring target protein structures.", "motivation": "Developing bioactive molecules remains a central, time- and cost-heavy challenge in drug discovery, particularly for novel targets lacking structural or functional data. Pharmacophore modeling presents an alternative for capturing the key features required for molecular bioactivity against a biological target.", "method": "PharmaDiff is a pharmacophore-conditioned diffusion model that uses a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process.", "result": "PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods. It also achieves higher docking scores across a range of proteins in structure-based drug design without the need for target protein structures.", "conclusion": "PharmaDiff offers a powerful and flexible framework for rational drug design by integrating pharmacophore modeling with 3D generative techniques."}}
{"id": "2505.10197", "pdf": "https://arxiv.org/pdf/2505.10197", "abs": "https://arxiv.org/abs/2505.10197", "authors": ["Anjali de Silva", "Gang Chen", "Hui Ma", "Seyed Mohammad Nekooei", "Xingquan Zuo"], "title": "Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion", "categories": ["cs.SI", "cs.AI"], "comment": "This paper has been accepted by IJCAI (International Joint Conference\n  on Artificial Intelligence) 2025", "summary": "Community detection, a vital technology for real-world applications, uncovers\ncohesive node groups (communities) by leveraging both topological and attribute\nsimilarities in social networks. However, existing Graph Convolutional Networks\n(GCNs) trained to maximize modularity often converge to suboptimal solutions.\nAdditionally, directly using human-labeled communities for training can\nundermine topological cohesiveness by grouping disconnected nodes based solely\non node attributes. We address these issues by proposing a novel Topological\nand Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com\nintroduces a novel loss function that exploits the highly effective and\nscalable Leiden algorithm to detect community structures with global optimal\nmodularity. Leiden is further utilized to refine human-labeled communities to\nensure connectivity within each community, enabling TAS-Com to detect community\nstructures with desirable trade-offs between modularity and compliance with\nhuman labels. Experimental results on multiple benchmark networks confirm that\nTAS-Com can significantly outperform several state-of-the-art algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u793e\u533a\u68c0\u6d4b\u65b9\u6cd5TAS-Com\uff0c\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u548c\u5c5e\u6027\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u5757\u5ea6\u4f18\u5316\u548c\u4eba\u5de5\u6807\u7b7e\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u5728\u6700\u5927\u5316\u6a21\u5757\u5ea6\u65f6\u5e38\u5e38\u6536\u655b\u5230\u6b21\u4f18\u89e3\uff0c\u800c\u76f4\u63a5\u4f7f\u7528\u4eba\u5de5\u6807\u8bb0\u7684\u793e\u533a\u8fdb\u884c\u8bad\u7ec3\u53ef\u80fd\u4f1a\u56e0\u4ec5\u57fa\u4e8e\u8282\u70b9\u5c5e\u6027\u800c\u5c06\u4e0d\u76f8\u8fde\u7684\u8282\u70b9\u5206\u7ec4\uff0c\u4ece\u800c\u7834\u574f\u62d3\u6251\u7d27\u5bc6\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u548c\u5c5e\u6027\u76f8\u4f3c\u6027\u7684\u793e\u533a\u68c0\u6d4b\u65b9\u6cd5\uff08TAS-Com\uff09\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5229\u7528\u9ad8\u6548\u7684Leiden\u7b97\u6cd5\u6765\u68c0\u6d4b\u5177\u6709\u5168\u5c40\u6700\u4f18\u6a21\u5757\u5ea6\u7684\u793e\u533a\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTAS-Com\u5728\u591a\u4e2a\u57fa\u51c6\u7f51\u7edc\u4e0a\u80fd\u591f\u663e\u8457\u4f18\u4e8e\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002", "conclusion": "TAS-Com\u53ef\u4ee5\u663e\u8457\u4f18\u4e8e\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002"}}
{"id": "2505.10556", "pdf": "https://arxiv.org/pdf/2505.10556", "abs": "https://arxiv.org/abs/2505.10556", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "categories": ["cs.LG", "physics.ao-ph"], "comment": "Kermani and Naderi share first authorship. 20 pages, 6 figures and 1\n  table", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u7a7f\u6234\u5065\u8eab\u8bbe\u5907\u7684\u751f\u7406\u6570\u636e\u548c\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u6570\u636e\uff0c\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u7684\u5065\u5eb7\u53cd\u5e94\u3002AI\u6a21\u578b\u5728\u4e91\u57fa\u7840\u6a21\u5757\u5316\u6846\u67b6\u4e2d\u8fd0\u884c\uff0c\u5e76\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u63d0\u9ad8\u5176\u6cdb\u5316\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u65f6\u95f4\u4f9d\u8d56\u7684\u5065\u5eb7\u4fe1\u53f7\u5e76\u6355\u6349\u6c61\u67d3\u7684\u975e\u7ebf\u6027\u53cd\u5e94\u3002", "motivation": "\u7a7a\u6c14\u6c61\u67d3\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u5bfc\u81f4\u6216\u52a0\u5267\u8bb8\u591a\u547c\u5438\u7cfb\u7edf\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u3002\u6b64\u5916\uff0c\u6c14\u5019\u53d8\u5316\u5e26\u6765\u4e86\u66f4\u591a\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\uff0c\u5982\u91ce\u706b\u548c\u70ed\u6d6a\uff0c\u8fd9\u53ef\u80fd\u589e\u52a0\u6c61\u67d3\u6c34\u5e73\u5e76\u52a0\u91cd\u6c61\u67d3\u66b4\u9732\u7684\u5f71\u54cd\u3002\u6700\u8fd1\u7684\u4e2a\u4eba\u4f20\u611f\u6280\u672f\u8fdb\u6b65\u6539\u53d8\u4e86\u884c\u4e3a\u548c\u751f\u7406\u6570\u636e\u7684\u6536\u96c6\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u7684\u65b0\u6539\u8fdb\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u4ee5\u53caAI\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4ee5\u76d1\u6d4b\u548c\u9884\u6d4b\u4e2a\u4f53\u7684\u5065\u5eb7\u7ed3\u679c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u7a7f\u6234\u5065\u8eab\u8bbe\u5907\u7684\u751f\u7406\u6570\u636e\u548c\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u6570\u636e\uff0c\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u7684\u5065\u5eb7\u53cd\u5e94\u3002\u6570\u636e\u4ee5\u5b89\u5168\u548c\u4f26\u7406\u7684\u65b9\u5f0f\u4ece\u5404\u79cd\u6765\u6e90\u6536\u96c6\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2aAI\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u4e91\u57fa\u7840\u6a21\u5757\u5316\u6846\u67b6\u4e2d\u8fd0\u884c\u3002\u4f7f\u7528\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3aAI\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u8fc1\u79fb\u5b66\u4e60\u4ee5\u63d0\u9ad8\u5176\u6cdb\u5316\u80fd\u529b\u3002", "result": "AI\u6a21\u578b\uff08\u5728\u6b64\u60c5\u51b5\u4e0b\u4e3a\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\uff09\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u65f6\u95f4\u4f9d\u8d56\u7684\u5065\u5eb7\u4fe1\u53f7\uff0c\u5e76\u6355\u6349\u6c61\u67d3\u7684\u975e\u7ebf\u6027\u53cd\u5e94\u3002\u5229\u7528\u4e2a\u4eba\u667a\u80fd\u624b\u8868\u6570\u636e\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86AI\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u7528\u6237\u751f\u6210\u6570\u636e\u4e2d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u7a7f\u6234\u5065\u8eab\u8bbe\u5907\u7684\u751f\u7406\u6570\u636e\u548c\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u6570\u636e\uff0c\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u7684\u5065\u5eb7\u53cd\u5e94\u3002AI\u6a21\u578b\u5728\u4e91\u57fa\u7840\u6a21\u5757\u5316\u6846\u67b6\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u65f6\u95f4\u4f9d\u8d56\u7684\u5065\u5eb7\u4fe1\u53f7\u5e76\u6355\u6349\u6c61\u67d3\u7684\u975e\u7ebf\u6027\u53cd\u5e94\u3002\u5229\u7528\u4e2a\u4eba\u667a\u80fd\u624b\u8868\u6570\u636e\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86AI\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u7528\u6237\u751f\u6210\u6570\u636e\u4e2d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2505.10201", "pdf": "https://arxiv.org/pdf/2505.10201", "abs": "https://arxiv.org/abs/2505.10201", "authors": ["Victor Lagerkvist", "Mohamed Maizia", "Johannes Schmidt"], "title": "A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds", "categories": ["cs.CC", "cs.AI", "F.2.2"], "comment": null, "summary": "The Boolean satisfiability problem (SAT) is a well-known example of monotonic\nreasoning, of intense practical interest due to fast solvers, complemented by\nrigorous fine-grained complexity results. However, for non-monotonic reasoning,\ne.g., abductive reasoning, comparably little is known outside classic\ncomplexity theory. In this paper we take a first step of bridging the gap\nbetween monotonic and non-monotonic reasoning by analyzing the complexity of\nintractable abduction problems under the seemingly overlooked but natural\nparameter n: the number of variables in the knowledge base. We obtain several\npositive results for $\\Sigma^P_2$- as well as NP- and coNP-complete fragments,\nwhich implies the first example of beating exhaustive search for a\n$\\Sigma^P_2$-complete problem (to the best of our knowledge). We complement\nthis with lower bounds and for many fragments rule out improvements under the\n(strong) exponential-time hypothesis.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10559", "pdf": "https://arxiv.org/pdf/2505.10559", "abs": "https://arxiv.org/abs/2505.10559", "authors": ["Ziming Liu", "Yizhou Liu", "Jeff Gore", "Max Tegmark"], "title": "Neural Thermodynamic Laws for Large Language Model Training", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "comment": "18 pages, 10 figures", "summary": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u795e\u7ecf\u70ed\u529b\u5b66\u5b9a\u5f8b\uff08NTL\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u4f9b\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u52a8\u6001\u7684\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u5b66\u4e60\u7387\u8c03\u5ea6\u63d0\u4f9b\u76f4\u89c2\u7684\u6307\u5bfc\u3002", "motivation": "\u4e86\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5f8b\uff0c\u8d85\u8d8a\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u3002", "method": "\u901a\u8fc7\u5047\u8bbe\u6cb3\u6d41\u5c71\u8c37\u635f\u5931\u666f\u89c2\uff0c\u8bc1\u660e\u4e86\u5173\u952e\u70ed\u529b\u5b66\u91cf\u548c\u7ecf\u5178\u70ed\u529b\u5b66\u539f\u7406\u81ea\u7136\u51fa\u73b0\u3002", "result": "NTL\u6846\u67b6\u63d0\u4f9b\u4e86\u5bf9LLM\u8bad\u7ec3\u52a8\u6001\u7684\u65b0\u89c1\u89e3\uff0c\u5e76\u4ea7\u751f\u4e86\u8bbe\u8ba1\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u76f4\u89c2\u6307\u5357\u3002", "conclusion": "NTL\u6846\u67b6\u4e3a\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u5b66\u4e60\u7387\u8c03\u5ea6\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u6307\u5bfc\u3002"}}
{"id": "2505.10212", "pdf": "https://arxiv.org/pdf/2505.10212", "abs": "https://arxiv.org/abs/2505.10212", "authors": ["Dario Di Palma", "Felice Antonio Merra", "Maurizio Sfilio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u8bb0\u5fc6\u4e86\u516c\u5171\u63a8\u8350\u6570\u636e\u96c6MovieLens-1M\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u6709\u4e00\u5b9a\u7a0b\u5ea6\u7684\u8bb0\u5fc6\uff0c\u5e76\u4e14\u63a8\u8350\u6027\u80fd\u4e0e\u8bb0\u5fc6\u7a0b\u5ea6\u76f8\u5173\u3002", "motivation": "\u9a8c\u8bc1LLMs\u662f\u5426\u5c06\u516c\u5171\u63a8\u8350\u6570\u636e\u96c6\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u7684\u4e00\u90e8\u5206\u8fdb\u884c\u8bb0\u5fc6\uff0c\u56e0\u4e3a\u8bb0\u5fc6\u4f1a\u964d\u4f4e\u7814\u7a76\u7ed3\u679c\u7684\u6cdb\u5316\u6027\uff0c\u5e76\u53ef\u80fd\u653e\u5927\u504f\u5dee\u3002", "method": "\u5b9a\u4e49\u6570\u636e\u96c6\u8bb0\u5fc6\u4e3a\u901a\u8fc7\u63d0\u793aLLMs\u68c0\u7d22\u7269\u54c1\u5c5e\u6027\u3001\u7528\u6237\u8d44\u6599\u548c\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u7684\u7a0b\u5ea6\uff0c\u5e76\u5206\u6790\u8bb0\u5fc6\u5bf9\u63a8\u8350\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u6a21\u578b\u5927\u5c0f\u7684\u8bb0\u5fc6\u5dee\u5f02\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u4e00\u5b9a\u7a0b\u5ea6\u7684MovieLens-1M\u8bb0\u5fc6\uff0c\u4e14\u63a8\u8350\u6027\u80fd\u4e0e\u8bb0\u5fc6\u7a0b\u5ea6\u76f8\u5173\u3002", "conclusion": "\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u4e00\u5b9a\u7a0b\u5ea6\u7684MovieLens-1M\u8bb0\u5fc6\uff0c\u4e14\u63a8\u8350\u6027\u80fd\u4e0e\u8bb0\u5fc6\u7a0b\u5ea6\u76f8\u5173\u3002"}}
{"id": "2505.09633", "pdf": "https://arxiv.org/pdf/2505.09633", "abs": "https://arxiv.org/abs/2505.09633", "authors": ["Nick Sunday"], "title": "Detecting Musical Deepfakes", "categories": ["cs.SD", "cs.LG"], "comment": "Submitted as part of coursework at UT Austin. Accompanying code\n  available at: https://github.com/nicksunday/deepfake-music-detector", "summary": "The proliferation of Text-to-Music (TTM) platforms has democratized music\ncreation, enabling users to effortlessly generate high-quality compositions.\nHowever, this innovation also presents new challenges to musicians and the\nbroader music industry. This study investigates the detection of AI-generated\nsongs using the FakeMusicCaps dataset by classifying audio as either deepfake\nor human. To simulate real-world adversarial conditions, tempo stretching and\npitch shifting were applied to the dataset. Mel spectrograms were generated\nfrom the modified audio, then used to train and evaluate a convolutional neural\nnetwork. In addition to presenting technical results, this work explores the\nethical and societal implications of TTM platforms, arguing that carefully\ndesigned detection systems are essential to both protecting artists and\nunlocking the positive potential of generative AI in music.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u68c0\u6d4bAI\u751f\u6210\u7684\u6b4c\u66f2\uff0c\u5e76\u63d0\u51fa\u4e86\u6280\u672f\u65b9\u6cd5\u548c\u4f26\u7406\u793e\u4f1a\u5f71\u54cd\u7684\u5206\u6790\u3002", "motivation": "\u968f\u7740Text-to-Music\uff08TTM\uff09\u5e73\u53f0\u7684\u666e\u53ca\uff0c\u97f3\u4e50\u521b\u4f5c\u53d8\u5f97\u66f4\u52a0\u6c11\u4e3b\u5316\uff0c\u4f46\u8fd9\u4e5f\u7ed9\u97f3\u4e50\u5bb6\u548c\u6574\u4e2a\u97f3\u4e50\u4ea7\u4e1a\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u68c0\u6d4bAI\u751f\u6210\u7684\u6b4c\u66f2\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528FakeMusicCaps\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c06\u97f3\u9891\u5206\u7c7b\u4e3a\u6df1\u5ea6\u4f2a\u9020\u6216\u4eba\u7c7b\u521b\u4f5c\u6765\u68c0\u6d4bAI\u751f\u6210\u7684\u6b4c\u66f2\u3002\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u8282\u594f\u62c9\u4f38\u548c\u97f3\u9ad8\u8f6c\u6362\u4ee5\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u7684\u5bf9\u6297\u6761\u4ef6\uff0c\u5e76\u4ece\u4fee\u6539\u540e\u7684\u97f3\u9891\u4e2d\u751f\u6210\u4e86\u6885\u5c14\u9891\u8c31\u56fe\uff0c\u7136\u540e\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u68c0\u6d4bAI\u751f\u6210\u6b4c\u66f2\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6280\u672f\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u8ba4\u4e3a\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u68c0\u6d4b\u7cfb\u7edf\u5bf9\u4e8e\u4fdd\u62a4\u827a\u672f\u5bb6\u548c\u91ca\u653e\u751f\u6210\u5f0fAI\u5728\u97f3\u4e50\u4e2d\u7684\u79ef\u6781\u6f5c\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.09643", "pdf": "https://arxiv.org/pdf/2505.09643", "abs": "https://arxiv.org/abs/2505.09643", "authors": ["Zhixuan Wang"], "title": "A Computational Approach to Epilepsy Treatment: An AI-optimized Global Natural Product Prescription System", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "Epilepsy is a prevalent neurological disease with millions of patients\nworldwide. Many patients have turned to alternative medicine due to the limited\nefficacy and side effects of conventional antiepileptic drugs. In this study,\nwe developed a computational approach to optimize herbal epilepsy treatment\nthrough AI-driven analysis of global natural products and statistically\nvalidated randomized controlled trials (RCTs). Our intelligent prescription\nsystem combines machine learning (ML) algorithms for herb-efficacy\ncharacterization, Bayesian optimization for personalized dosing, and\nmeta-analysis of RCTs for evidence-based recommendations. The system analyzed\n1,872 natural compounds from traditional Chinese medicine (TCM), Ayurveda, and\nethnopharmacological databases, integrating their bioactive properties with\nclinical outcomes from 48 RCTs covering 48 epilepsy conditions (n=5,216). Using\nLASSO regression and SHAP value analysis, we identified 17 high-efficacy herbs\n(e.g., Gastrodia elata [using \\'e for accented characters], Withania\nsomnifera), showing significant seizure reduction (p$<$0.01, Cohen's d=0.89)\nwith statistical significance confirmed by multiple testing (p$<$0.001). A\nrandomized double-blind validation trial (n=120) demonstrated 28.5\\% greater\nseizure frequency reduction with AI-optimized herbal prescriptions compared to\nconventional protocols (95\\% CI: 18.7-37.3\\%, p=0.003).", "AI": {"tldr": "A study developed an AI system to optimize herbal epilepsy treatment by analyzing natural products and clinical trials, resulting in significant seizure reduction.", "motivation": "Many patients with epilepsy turn to alternative medicine due to the limited efficacy and side effects of conventional antiepileptic drugs.", "method": "Developed a computational approach using AI-driven analysis of global natural products and statistically validated RCTs. Combined ML algorithms, Bayesian optimization, and meta-analysis of RCTs.", "result": "Identified 17 high-efficacy herbs with significant seizure reduction. A randomized double-blind validation trial showed 28.5% greater seizure frequency reduction with AI-optimized prescriptions.", "conclusion": "AI-optimized herbal prescriptions showed significant seizure reduction compared to conventional protocols."}}
{"id": "2505.09647", "pdf": "https://arxiv.org/pdf/2505.09647", "abs": "https://arxiv.org/abs/2505.09647", "authors": ["Leighton Pate Barnes", "Stephen Cameron", "Benjamin Howard"], "title": "On Unbiased Low-Rank Approximation with Minimum Distortion", "categories": ["cs.DS", "cs.IT", "cs.LG", "math.IT", "math.PR", "math.ST", "stat.TH"], "comment": null, "summary": "We describe an algorithm for sampling a low-rank random matrix $Q$ that best\napproximates a fixed target matrix $P\\in\\mathbb{C}^{n\\times m}$ in the\nfollowing sense: $Q$ is unbiased, i.e., $\\mathbb{E}[Q] = P$;\n$\\mathsf{rank}(Q)\\leq r$; and $Q$ minimizes the expected Frobenius norm error\n$\\mathbb{E}\\|P-Q\\|_F^2$. Our algorithm mirrors the solution to the efficient\nunbiased sparsification problem for vectors, except applied to the singular\ncomponents of the matrix $P$. Optimality is proven by showing that our\nalgorithm matches the error from an existing lower bound.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4e00\u4e2a\u4f4e\u79e9\u968f\u673a\u77e9\u9635Q\uff0c\u4f7f\u5176\u5728\u671f\u671bFrobenius\u8303\u6570\u8bef\u5dee\u65b9\u9762\u6700\u597d\u5730\u903c\u8fd1\u56fa\u5b9a\u7684\u76ee\u6807\u77e9\u9635P\u3002", "motivation": "\u6211\u4eec\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u751f\u6210\u4e00\u4e2a\u4f4e\u79e9\u968f\u673a\u77e9\u9635Q\uff0c\u4f7f\u5176\u5728\u671f\u671bFrobenius\u8303\u6570\u8bef\u5dee\u65b9\u9762\u6700\u597d\u5730\u903c\u8fd1\u56fa\u5b9a\u7684\u76ee\u6807\u77e9\u9635P\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b97\u6cd5\uff0c\u7528\u4e8e\u91c7\u6837\u4e00\u4e2a\u4f4e\u79e9\u968f\u673a\u77e9\u9635Q\uff0c\u8be5\u77e9\u9635\u5728\u671f\u671bFrobenius\u8303\u6570\u8bef\u5dee\u65b9\u9762\u6700\u597d\u5730\u903c\u8fd1\u56fa\u5b9a\u7684\u76ee\u6807\u77e9\u9635P\u3002\u8be5\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u89e3\u51b3\u5411\u91cf\u9ad8\u6548\u65e0\u504f\u7a00\u758f\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5e94\u7528\u4e8e\u77e9\u9635P\u7684\u5947\u5f02\u5206\u91cf\u3002", "result": "\u6211\u4eec\u7684\u7b97\u6cd5\u751f\u6210\u7684\u77e9\u9635Q\u662f\u65e0\u504f\u7684\uff0c\u5176\u79e9\u4e0d\u8d85\u8fc7r\uff0c\u5e76\u4e14\u6700\u5c0f\u5316\u4e86\u671f\u671bFrobenius\u8303\u6570\u8bef\u5dee\u3002", "conclusion": "\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u671f\u671bFrobenius\u8303\u6570\u8bef\u5dee\u65b9\u9762\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u5b83\u4e0e\u73b0\u6709\u7684\u4e0b\u9650\u76f8\u5339\u914d\u3002"}}
{"id": "2505.10273", "pdf": "https://arxiv.org/pdf/2505.10273", "abs": "https://arxiv.org/abs/2505.10273", "authors": ["Hexu Li", "Konstantinos Kalogiannis", "Ahmed Mohamed Hussain", "Panos Papadimitratos"], "title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": "Author's version; Accepted for presentation at the ACM Workshop on\n  Wireless Security and Machine Learning (WiseML 2025)", "summary": "Vehicle platooning, with vehicles traveling in close formation coordinated\nthrough Vehicle-to-Everything (V2X) communications, offers significant benefits\nin fuel efficiency and road utilization. However, it is vulnerable to\nsophisticated falsification attacks by authenticated insiders that can\ndestabilize the formation and potentially cause catastrophic collisions. This\npaper addresses this challenge: misbehavior detection in vehicle platooning\nsystems. We present AttentionGuard, a transformer-based framework for\nmisbehavior detection that leverages the self-attention mechanism to identify\nanomalous patterns in mobility data. Our proposal employs a multi-head\ntransformer-encoder to process sequential kinematic information, enabling\neffective differentiation between normal mobility patterns and falsification\nattacks across diverse platooning scenarios, including steady-state\n(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an\nextensive simulation dataset featuring various attack vectors (constant,\ngradual, and combined falsifications) and operational parameters (controller\ntypes, vehicle speeds, and attacker positions). Experimental results\ndemonstrate that AttentionGuard achieves up to 0.95 F1-score in attack\ndetection, with robust performance maintained during complex maneuvers.\nNotably, our system performs effectively with minimal latency (100ms decision\nintervals), making it suitable for real-time transportation safety\napplications. Comparative analysis reveals superior detection capabilities and\nestablishes the transformer-encoder as a promising approach for securing\nCooperative Intelligent Transport Systems (C-ITS) against sophisticated insider\nthreats.", "AI": {"tldr": "This paper presents AttentionGuard, a transformer-based framework for detecting misbehavior in vehicle platooning systems. It uses a multi-head transformer-encoder to process sequential kinematic information and identify anomalous patterns in mobility data. Experimental results show that AttentionGuard achieves high accuracy in attack detection with minimal latency, making it suitable for real-time transportation safety applications.", "motivation": "Vehicle platooning systems are vulnerable to sophisticated falsification attacks by authenticated insiders, which can destabilize the formation and cause catastrophic collisions. This paper addresses the challenge of misbehavior detection in vehicle platooning systems.", "method": "AttentionGuard is a transformer-based framework that uses the self-attention mechanism to identify anomalous patterns in mobility data. It employs a multi-head transformer-encoder to process sequential kinematic information, enabling effective differentiation between normal mobility patterns and falsification attacks.", "result": "Experimental results demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack detection, with robust performance maintained during complex maneuvers. It also performs effectively with minimal latency (100ms decision intervals), making it suitable for real-time transportation safety applications.", "conclusion": "AttentionGuard achieves up to 0.95 F1-score in attack detection, with robust performance during complex maneuvers. It performs effectively with minimal latency, making it suitable for real-time transportation safety applications. The transformer-encoder is a promising approach for securing C-ITS against sophisticated insider threats."}}
{"id": "2505.10300", "pdf": "https://arxiv.org/pdf/2505.10300", "abs": "https://arxiv.org/abs/2505.10300", "authors": ["Muzhe Wu", "Yanzhi Zhao", "Shuyi Han", "Michael Xieyang Liu", "Hong Shen"], "title": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Responsible AI (RAI) efforts increasingly emphasize the importance of\naddressing potential harms early in the AI development lifecycle through\nsocial-technical lenses. However, in cross-functional industry teams, this work\nis often stalled by a persistent knowledge handoff challenge: the difficulty of\ntransferring high-level, early-stage technical design rationales from technical\nexperts to non-technical or user-facing roles for ethical evaluation and harm\nidentification. Through literature review and a co-design study with 8\npractitioners, we unpack how this challenge manifests -- technical design\nchoices are rarely handed off in ways that support meaningful engagement by\nnon-technical roles; collaborative workflows lack shared, visual structures to\nsupport mutual understanding; and non-technical practitioners are left without\nscaffolds for systematic harm evaluation. Existing tools like JIRA or Google\nDocs, while useful for product tracking, are ill-suited for supporting joint\nharm identification across roles, often requiring significant extra effort to\nalign understanding. To address this, we developed AI LEGO, a web-based\nprototype that supports cross-functional AI practitioners in effectively\nfacilitating knowledge handoff and identifying harmful design choices in the\nearly design stages. Technical roles use interactive blocks to draft\ndevelopment plans, while non-technical roles engage with those blocks through\nstage-specific checklists and LLM-driven persona simulations to surface\npotential harms. In a study with 18 cross-functional practitioners, AI LEGO\nincreased the volume and likelihood of harms identified compared to baseline\nworksheets. Participants found that its modular structure and persona prompts\nmade harm identification more accessible, fostering clearer and more\ncollaborative RAI practices in early design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAI LEGO\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u539f\u578b\uff0c\u7528\u4e8e\u652f\u6301\u8de8\u804c\u80fdAI\u4ece\u4e1a\u8005\u6709\u6548\u4fc3\u8fdb\u77e5\u8bc6\u4ea4\u63a5\u5e76\u8bc6\u522b\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u7684\u6709\u5bb3\u8bbe\u8ba1\u9009\u62e9\u3002", "motivation": "\u5728\u8de8\u804c\u80fd\u884c\u4e1a\u56e2\u961f\u4e2d\uff0c\u7531\u4e8e\u96be\u4ee5\u5c06\u9ad8\u5c42\u6b21\u3001\u65e9\u671f\u9636\u6bb5\u7684\u6280\u672f\u8bbe\u8ba1\u7406\u7531\u4ece\u6280\u672f\u4e13\u5bb6\u8f6c\u79fb\u5230\u975e\u6280\u672f\u6216\u7528\u6237\u9762\u5411\u7684\u89d2\u8272\uff0c\u4ee5\u8fdb\u884c\u4f26\u7406\u8bc4\u4f30\u548c\u5371\u5bb3\u8bc6\u522b\uff0c\u56e0\u6b64\u8fd9\u9879\u5de5\u4f5c\u7ecf\u5e38\u53d7\u963b\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u4e0e8\u540d\u4ece\u4e1a\u8005\u7684\u5171\u540c\u8bbe\u8ba1\u7814\u7a76\uff0c\u6211\u4eec\u89e3\u6784\u4e86\u8fd9\u4e00\u6311\u6218\u7684\u8868\u73b0\u5f62\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86AI LEGO\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u539f\u578b\uff0c\u652f\u6301\u8de8\u804c\u80fdAI\u4ece\u4e1a\u8005\u6709\u6548\u4fc3\u8fdb\u77e5\u8bc6\u4ea4\u63a5\u5e76\u8bc6\u522b\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u7684\u6709\u5bb3\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u5728\u4e0e18\u540d\u8de8\u804c\u80fd\u4ece\u4e1a\u8005\u7684\u7814\u7a76\u4e2d\uff0cAI LEGO \u6bd4\u57fa\u7ebf\u5de5\u4f5c\u8868\u589e\u52a0\u4e86\u5371\u5bb3\u8bc6\u522b\u7684\u6570\u91cf\u548c\u53ef\u80fd\u6027\u3002\u53c2\u4e0e\u8005\u53d1\u73b0\u5176\u6a21\u5757\u5316\u7ed3\u6784\u548c\u89d2\u8272\u63d0\u793a\u4f7f\u5371\u5bb3\u8bc6\u522b\u66f4\u52a0\u5bb9\u6613\uff0c\u4fc3\u8fdb\u4e86\u65e9\u671f\u8bbe\u8ba1\u4e2d\u7684\u66f4\u6e05\u6670\u548c\u534f\u4f5c\u7684RAI\u5b9e\u8df5\u3002", "conclusion": "AI LEGO \u901a\u8fc7\u6a21\u5757\u5316\u7ed3\u6784\u548c\u89d2\u8272\u63d0\u793a\uff0c\u4f7f\u5371\u5bb3\u8bc6\u522b\u66f4\u52a0\u5bb9\u6613\uff0c\u4fc3\u8fdb\u4e86\u65e9\u671f\u8bbe\u8ba1\u4e2d\u7684\u66f4\u6e05\u6670\u548c\u534f\u4f5c\u7684RAI\u5b9e\u8df5\u3002"}}
{"id": "2505.09660", "pdf": "https://arxiv.org/pdf/2505.09660", "abs": "https://arxiv.org/abs/2505.09660", "authors": ["Saptarshi Saha", "Dhruv Vansraj Rathore", "Soumadeep Saha", "Utpal Garain", "David Doermann"], "title": "On Measuring Intrinsic Causal Attributions in Deep Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Quantifying the causal influence of input features within neural networks has\nbecome a topic of increasing interest. Existing approaches typically assess\ndirect, indirect, and total causal effects. This work treats NNs as structural\ncausal models (SCMs) and extends our focus to include intrinsic causal\ncontributions (ICC). We propose an identifiable generative post-hoc framework\nfor quantifying ICC. We also draw a relationship between ICC and Sobol'\nindices. Our experiments on synthetic and real-world datasets demonstrate that\nICC generates more intuitive and reliable explanations compared to existing\nglobal explanation techniques.", "AI": {"tldr": "\u672c\u6587\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bc6\u522b\u7684\u751f\u6210\u540e\u6846\u67b6\u6765\u91cf\u5316\u5185\u5728\u56e0\u679c\u8d21\u732e\uff08ICC\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cICC\u751f\u6210\u4e86\u6bd4\u73b0\u6709\u5168\u5c40\u89e3\u91ca\u6280\u672f\u66f4\u76f4\u89c2\u548c\u53ef\u9760\u7684\u89e3\u91ca\u3002", "motivation": "\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u8f93\u5165\u7279\u5f81\u7684\u56e0\u679c\u5f71\u54cd\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u8bc4\u4f30\u76f4\u63a5\u3001\u95f4\u63a5\u548c\u603b\u56e0\u679c\u6548\u5e94\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bc6\u522b\u7684\u751f\u6210\u540e\u6846\u67b6\u6765\u91cf\u5316\u5185\u5728\u56e0\u679c\u8d21\u732e\uff08ICC\uff09\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cICC\u6bd4\u73b0\u6709\u5168\u5c40\u89e3\u91ca\u6280\u672f\u751f\u6210\u4e86\u66f4\u76f4\u89c2\u548c\u53ef\u9760\u7684\u89e3\u91ca\u3002", "conclusion": "ICC\u751f\u6210\u4e86\u6bd4\u73b0\u6709\u5168\u5c40\u89e3\u91ca\u6280\u672f\u66f4\u76f4\u89c2\u548c\u53ef\u9760\u7684\u89e3\u91ca\u3002"}}
{"id": "2505.10315", "pdf": "https://arxiv.org/pdf/2505.10315", "abs": "https://arxiv.org/abs/2505.10315", "authors": ["Yang Li", "Xinyu Zhou", "Yitong Wang", "Liangxin Qian", "Jun Zhao"], "title": "Private Transformer Inference in MLaaS: A Survey", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86Private Transformer Inference (PTI)\u7684\u6982\u5ff5\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5bc6\u7801\u5b66\u6280\u672f\u4fdd\u62a4\u7528\u6237\u6570\u636e\u548c\u6a21\u578b\u9690\u79c1\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5206\u7c7b\u6cd5\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "Transformer\u6a21\u578b\u5728AI\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u4f46\u5728MLaaS\u4e2d\u7684\u90e8\u7f72\u5f15\u53d1\u4e86\u9690\u79c1\u95ee\u9898\uff0c\u56e0\u4e3a\u654f\u611f\u7528\u6237\u6570\u636e\u662f\u96c6\u4e2d\u5904\u7406\u7684\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4fdd\u62a4\u7528\u6237\u6570\u636e\u548c\u6a21\u578b\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u63a8\u7406\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86Private Transformer Inference (PTI)\u7684\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u5bc6\u7801\u5b66\u6280\u672f\u5982\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\u548c\u540c\u6001\u52a0\u5bc6\u6765\u5b9e\u73b0\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u6570\u636e\u548c\u6a21\u578b\u9690\u79c1\u3002\u6b64\u5916\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5206\u7c7b\u6cd5\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u672c\u6587\u56de\u987e\u4e86PTI\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5206\u7c7b\u6cd5\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u5e73\u8861\u8d44\u6e90\u6548\u7387\u4e0e\u9690\u79c1\uff0c\u5e76\u5f25\u5408\u9ad8\u6027\u80fd\u63a8\u7406\u4e0e\u6570\u636e\u9690\u79c1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u672c\u6587\u56de\u987e\u4e86\u6700\u8fd1\u7684PTI\u8fdb\u5c55\uff0c\u5f3a\u8c03\u4e86\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6311\u6218\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5206\u7c7b\u6cd5\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u5e73\u8861\u8d44\u6e90\u6548\u7387\u4e0e\u9690\u79c1\uff0c\u5e76\u5f25\u5408\u9ad8\u6027\u80fd\u63a8\u7406\u4e0e\u6570\u636e\u9690\u79c1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.09706", "pdf": "https://arxiv.org/pdf/2505.09706", "abs": "https://arxiv.org/abs/2505.09706", "authors": ["Hugo Gobato Souto", "Francisco Louzada Neto"], "title": "Forests for Differences: Robust Causal Inference Beyond Parametric DiD", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces the Difference-in-Differences Bayesian Causal Forest\n(DiD-BCF), a novel non-parametric model addressing key challenges in DiD\nestimation, such as staggered adoption and heterogeneous treatment effects.\nDiD-BCF provides a unified framework for estimating Average (ATE),\nGroup-Average (GATE), and Conditional Average Treatment Effects (CATE). A core\ninnovation, its Parallel Trends Assumption (PTA)-based reparameterization,\nenhances estimation accuracy and stability in complex panel data settings.\nExtensive simulations demonstrate DiD-BCF's superior performance over\nestablished benchmarks, particularly under non-linearity, selection biases, and\neffect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers\nsignificant conditional treatment effect heterogeneity related to county\npopulation, insights obscured by traditional methods. DiD-BCF offers a robust\nand versatile tool for more nuanced causal inference in modern DiD\napplications.", "AI": {"tldr": "This paper introduces DiD-BCF, a novel non-parametric model that addresses key challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects. It provides a unified framework for estimating ATE, GATE, and CATE. The model's PTA-based reparameterization enhances estimation accuracy and stability in complex panel data settings. Extensive simulations show its superior performance over established benchmarks, particularly under non-linearity, selection biases, and effect heterogeneity. When applied to U.S. minimum wage policy, it reveals significant conditional treatment effect heterogeneity related to county population, which traditional methods fail to capture.", "motivation": "The paper aims to address the challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects, by introducing a novel non-parametric model that can provide more accurate and stable estimates.", "method": "DiD-BCF is a novel non-parametric model that addresses key challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects. It provides a unified framework for estimating ATE, GATE, and CATE. Its core innovation is a PTA-based reparameterization that enhances estimation accuracy and stability in complex panel data settings.", "result": "Extensive simulations demonstrate DiD-BCF's superior performance over established benchmarks, particularly under non-linearity, selection biases, and effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers significant conditional treatment effect heterogeneity related to county population, insights obscured by traditional methods.", "conclusion": "DiD-BCF offers a robust and versatile tool for more nuanced causal inference in modern DiD applications."}}
{"id": "2505.10321", "pdf": "https://arxiv.org/pdf/2505.10321", "abs": "https://arxiv.org/abs/2505.10321", "authors": ["Julius Henke"], "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents", "categories": ["cs.CR", "cs.AI"], "comment": "24 pages, 1 figure, for implementation, see\n  https://github.com/JuliusHenke/autopentest", "summary": "A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aAutoPentest\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u5176\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u6548\u679c\u548c\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u4f7f\u7528\u662f\u4e00\u4e2a\u7814\u7a76\u70ed\u70b9\uff0c\u6709\u671b\u964d\u4f4e\u6210\u672c\u5e76\u5141\u8bb8\u66f4\u9ad8\u7684\u9891\u7387\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u76f8\u5173\u5de5\u4f5c\u7684\u56de\u987e\uff0c\u786e\u5b9a\u4e86\u6700\u4f73\u5b9e\u8df5\u548c\u5e38\u89c1\u7684\u8bc4\u4f30\u95ee\u9898\u3002\u7136\u540e\u6211\u4eec\u4ecb\u7ecd\u4e86AutoPentest\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u6267\u884c\u9ad8\u5ea6\u81ea\u4e3b\u7684\u9ed1\u76d2\u6e17\u900f\u6d4b\u8bd5\u7684\u5e94\u7528\u7a0b\u5e8f\u3002AutoPentest\u57fa\u4e8eOpenAI\u7684LLM GPT-4o\u548cLLM\u4ee3\u7406\u6846\u67b6LangChain\u3002", "result": "\u5728\u4e09\u4e2aCTF\u98ce\u683c\u7684Hack The Box\uff08HTB\uff09\u673a\u5668\u4e0a\u8fdb\u884c\u7684\u7814\u7a76\u8868\u660e\uff0c\u6211\u4eec\u7684\u5b9e\u73b0AutoPentest\u4e0e\u624b\u52a8\u4f7f\u7528ChatGPT-4o\u7528\u6237\u754c\u9762\u7684\u57fa\u7ebf\u65b9\u6cd5\u90fd\u80fd\u5b8c\u621015-25%\u7684\u5b50\u4efb\u52a1\uff0c\u800cAutoPentest\u7565\u80dc\u4e00\u7b79\u3002\u4f7f\u7528AutoPentest\u7684\u603b\u6210\u672c\u4e3a96.20\u7f8e\u5143\uff0c\u800c\u4e00\u4e2a\u6708\u7684ChatGPT Plus\u8ba2\u9605\u8d39\u7528\u4e3a20\u7f8e\u5143\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u8fdb\u4e00\u6b65\u7684\u5b9e\u65bd\u52aa\u529b\u548c\u672a\u6765\u66f4\u5f3a\u5927\u7684LLMs\u7684\u4f7f\u7528\u53ef\u80fd\u4f1a\u4f7f\u8fd9\u6210\u4e3a\u6f0f\u6d1e\u7ba1\u7406\u4e2d\u53ef\u884c\u7684\u4e00\u90e8\u5206\u3002"}}
{"id": "2505.09718", "pdf": "https://arxiv.org/pdf/2505.09718", "abs": "https://arxiv.org/abs/2505.09718", "authors": ["Daniel Dylewsky", "Sonia K\u00e9fi", "Madhur Anand", "Chris T. Bauch"], "title": "Neural models for prediction of spatially patterned phase transitions: methods and challenges", "categories": ["physics.comp-ph", "cs.LG"], "comment": null, "summary": "Dryland vegetation ecosystems are known to be susceptible to critical\ntransitions between alternative stable states when subjected to external\nforcing. Such transitions are often discussed through the framework of\nbifurcation theory, but the spatial patterning of vegetation, which is\ncharacteristic of drylands, leads to dynamics that are much more complex and\ndiverse than local bifurcations. Recent methodological developments in Early\nWarning Signal (EWS) detection have shown promise in identifying dynamical\nsignatures of oncoming critical transitions, with particularly strong\npredictive capabilities being demonstrated by deep neural networks. However, a\nmachine learning model trained on synthetic examples is only useful if it can\neffectively transfer to a test case of practical interest. These models'\ncapacity to generalize in this manner has been demonstrated for bifurcation\ntransitions, but it is not as well characterized for high-dimensional phase\ntransitions. This paper explores the successes and shortcomings of neural EWS\ndetection for spatially patterned phase transitions, and shows how these models\ncan be used to gain insight into where and how EWS-relevant information is\nencoded in spatiotemporal dynamics. A few paradigmatic test systems are used to\nillustrate how the capabilities of such models can be probed in a number of\nways, with particular attention to the performances of a number of proposed\nstatistical indicators for EWS and to the supplementary task of distinguishing\nbetween abrupt and continuous transitions. Results reveal that model\nperformance often changes dramatically when training and test data sources are\ninterchanged, which offers new insight into the criteria for model\ngeneralization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u68c0\u6d4b\u7a7a\u95f4\u6a21\u5f0f\u76f8\u53d8\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\uff08EWS\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6e90\u4e92\u6362\u65f6\u53ef\u80fd\u663e\u8457\u53d8\u5316\uff0c\u8fd9\u4e3a\u6a21\u578b\u6cdb\u5316\u6807\u51c6\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "motivation": "\u5e72\u5730\u690d\u88ab\u751f\u6001\u7cfb\u7edf\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u5bb9\u6613\u53d1\u751f\u4e34\u754c\u8f6c\u53d8\uff0c\u800c\u8fd9\u79cd\u8f6c\u53d8\u901a\u5e38\u901a\u8fc7\u5206\u5c94\u7406\u8bba\u6846\u67b6\u8fdb\u884c\u8ba8\u8bba\u3002\u7136\u800c\uff0c\u5e72\u5730\u690d\u88ab\u7684\u7a7a\u95f4\u6a21\u5f0f\u5bfc\u81f4\u7684\u52a8\u529b\u5b66\u6bd4\u5c40\u90e8\u5206\u5c94\u66f4\u4e3a\u590d\u6742\u548c\u591a\u6837\u3002\u8fd1\u5e74\u6765\uff0c\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\uff08EWS\uff09\u68c0\u6d4b\u7684\u65b9\u6cd5\u8fdb\u5c55\u8868\u660e\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bc6\u522b\u5373\u5c06\u53d1\u751f\u7684\u4e34\u754c\u8f6c\u53d8\u65b9\u9762\u5177\u6709\u5f88\u5f3a\u7684\u9884\u6d4b\u80fd\u529b\u3002\u4f46\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ec5\u5728\u80fd\u6709\u6548\u8f6c\u79fb\u5230\u5b9e\u9645\u611f\u5174\u8da3\u6848\u4f8b\u7684\u60c5\u51b5\u4e0b\u624d\u6709\u7528\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u5206\u5c94\u8f6c\u53d8\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u9ad8\u7ef4\u76f8\u53d8\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63cf\u8ff0\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u795e\u7ecf\u7f51\u7edcEWS\u68c0\u6d4b\u5728\u7a7a\u95f4\u6a21\u5f0f\u76f8\u53d8\u4e2d\u7684\u6210\u529f\u4e0e\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u68c0\u6d4b\u7a7a\u95f4\u6a21\u5f0f\u76f8\u53d8\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\uff08EWS\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u51e0\u4e2a\u5178\u578b\u7684\u6d4b\u8bd5\u7cfb\u7edf\u6765\u5206\u6790\u8fd9\u4e9b\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5e76\u7279\u522b\u5173\u6ce8\u4e86\u51e0\u79cd\u63d0\u51fa\u7684\u7edf\u8ba1\u6307\u6807\u5728EWS\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\u4ee5\u53ca\u533a\u5206\u7a81\u7136\u548c\u8fde\u7eed\u76f8\u53d8\u7684\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6e90\u4e92\u6362\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u5f80\u5f80\u4f1a\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u8fd9\u4e3a\u6a21\u578b\u6cdb\u5316\u6807\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u795e\u7ecf\u7f51\u7edcEWS\u68c0\u6d4b\u5728\u4e0d\u540c\u6d4b\u8bd5\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u7edf\u8ba1\u6307\u6807\u5728EWS\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u68c0\u6d4b\u7a7a\u95f4\u6a21\u5f0f\u76f8\u53d8\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u65b9\u9762\u7684\u6210\u529f\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u5e2e\u52a9\u7406\u89e3EWS\u76f8\u5173\u4fe1\u606f\u5728\u65f6\u7a7a\u52a8\u6001\u4e2d\u7684\u7f16\u7801\u65b9\u5f0f\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u5f53\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6e90\u4e92\u6362\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u53ef\u80fd\u4f1a\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u8fd9\u4e3a\u6a21\u578b\u6cdb\u5316\u6807\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.09734", "pdf": "https://arxiv.org/pdf/2505.09734", "abs": "https://arxiv.org/abs/2505.09734", "authors": ["Babak Esmaeili", "Nariman Niknejad", "Hamidreza Modares"], "title": "Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "comment": "Submitted to Asian Journal of Control", "summary": "This paper presents a risk-aware safe reinforcement learning (RL) control\ndesign for stochastic discrete-time linear systems. Rather than using a safety\ncertifier to myopically intervene with the RL controller, a risk-informed safe\ncontroller is also learned besides the RL controller, and the RL and safe\ncontrollers are combined together. Several advantages come along with this\napproach: 1) High-confidence safety can be certified without relying on a\nhigh-fidelity system model and using limited data available, 2) Myopic\ninterventions and convergence to an undesired equilibrium can be avoided by\ndeciding on the contribution of two stabilizing controllers, and 3) highly\nefficient and computationally tractable solutions can be provided by optimizing\nover a scalar decision variable and linear programming polyhedral sets. To\nlearn safe controllers with a large invariant set, piecewise affine controllers\nare learned instead of linear controllers. To this end, the closed-loop system\nis first represented using collected data, a decision variable, and noise. The\neffect of the decision variable on the variance of the safe violation of the\nclosed-loop system is formalized. The decision variable is then designed such\nthat the probability of safety violation for the learned closed-loop system is\nminimized. It is shown that this control-oriented approach reduces the data\nrequirements and can also reduce the variance of safety violations. Finally, to\nintegrate the safe and RL controllers, a new data-driven interpolation\ntechnique is introduced. This method aims to maintain the RL agent's optimal\nimplementation while ensuring its safety within environments characterized by\nnoise. The study concludes with a simulation example that serves to validate\nthe theoretical results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408RL\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5b89\u5168\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u9ad8\u4fdd\u771f\u7cfb\u7edf\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u7684\u5b89\u5168\u8ba4\u8bc1\u65b9\u6cd5\u53ef\u80fd\u8fc7\u4e8e\u77ed\u89c6\uff0c\u65e0\u6cd5\u6709\u6548\u907f\u514d\u4e0d\u671f\u671b\u7684\u5e73\u8861\u70b9\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u9ad8\u4fdd\u771f\u7cfb\u7edf\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u5b89\u5168\u63a7\u5236\u5668\uff0c\u4e0eRL\u63a7\u5236\u5668\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u4f18\u5316\u6807\u91cf\u51b3\u7b56\u53d8\u91cf\u548c\u7ebf\u6027\u89c4\u5212\u591a\u9762\u4f53\u96c6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8ba1\u7b97\u4e0a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u9a71\u52a8\u63d2\u503c\u6280\u672f\uff0c\u4ee5\u786e\u4fddRL\u4ee3\u7406\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51cf\u5c11\u6570\u636e\u9700\u6c42\uff0c\u5e76\u964d\u4f4e\u5b89\u5168\u8fdd\u89c4\u7684\u65b9\u5dee\u3002\u901a\u8fc7\u4eff\u771f\u793a\u4f8b\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63a7\u5236\u8bbe\u8ba1\uff0c\u7528\u4e8e\u968f\u673a\u79bb\u6563\u65f6\u95f4\u7ebf\u6027\u7cfb\u7edf\u3002\u901a\u8fc7\u7ed3\u5408RL\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5b89\u5168\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u9ad8\u4fdd\u771f\u7cfb\u7edf\u6a21\u578b\u7684\u4f9d\u8d56\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4eff\u771f\u793a\u4f8b\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002"}}
{"id": "2505.09748", "pdf": "https://arxiv.org/pdf/2505.09748", "abs": "https://arxiv.org/abs/2505.09748", "authors": ["Jitendra K Tugnait"], "title": "Learning Multi-Attribute Differential Graphs with Non-Convex Penalties", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "14 pages, 1 figures, 2 tables, published in IEEE Access, pp.\n  67065-67078, 2025", "summary": "We consider the problem of estimating differences in two multi-attribute\nGaussian graphical models (GGMs) which are known to have similar structure,\nusing a penalized D-trace loss function with non-convex penalties. The GGM\nstructure is encoded in its precision (inverse covariance) matrix. Existing\nmethods for multi-attribute differential graph estimation are based on a group\nlasso penalized loss function. In this paper, we consider a penalized D-trace\nloss function with non-convex (log-sum and smoothly clipped absolute deviation\n(SCAD)) penalties. Two proximal gradient descent methods are presented to\noptimize the objective function. Theoretical analysis establishing sufficient\nconditions for consistency in support recovery, convexity and estimation in\nhigh-dimensional settings is provided. We illustrate our approaches with\nnumerical examples based on synthetic and real data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u51f8\u60e9\u7f5a\u7684D-trace\u635f\u5931\u51fd\u6570\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e24\u4e2a\u76f8\u4f3c\u7ed3\u6784\u7684\u591a\u5c5e\u6027\u9ad8\u65af\u56fe\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u5c5e\u6027\u5dee\u5206\u56fe\u4f30\u8ba1\u65b9\u6cd5\u57fa\u4e8e\u7ec4Lasso\u60e9\u7f5a\u635f\u5931\u51fd\u6570\uff0c\u4f46\u672c\u6587\u65e8\u5728\u901a\u8fc7\u975e\u51f8\u60e9\u7f5a\u63d0\u9ad8\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u975e\u51f8\u60e9\u7f5a\uff08log-sum\u548cSCAD\uff09\u7684D-trace\u635f\u5931\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u4e24\u79cd\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u4f18\u5316\u76ee\u6807\u51fd\u6570\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u652f\u6301\u6062\u590d\u3001\u51f8\u6027\u548c\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u7684\u4f30\u8ba1\u7684\u4e00\u81f4\u6027\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u7684\u6570\u503c\u4f8b\u5b50\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528\u975e\u51f8\u60e9\u7f5a\u7684D-trace\u635f\u5931\u51fd\u6570\u6765\u4f30\u8ba1\u4e24\u4e2a\u5177\u6709\u76f8\u4f3c\u7ed3\u6784\u7684\u591a\u5c5e\u6027\u9ad8\u65af\u56fe\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09783", "pdf": "https://arxiv.org/pdf/2505.09783", "abs": "https://arxiv.org/abs/2505.09783", "authors": ["Jianfeng Jiao", "Xi Gao", "Jie Li"], "title": "Pure Component Property Estimation Framework Using Explainable Machine Learning Methods", "categories": ["stat.AP", "cs.LG"], "comment": null, "summary": "Accurate prediction of pure component physiochemical properties is crucial\nfor process integration, multiscale modeling, and optimization. In this work,\nan enhanced framework for pure component property prediction by using\nexplainable machine learning methods is proposed. In this framework, the\nmolecular representation method based on the connectivity matrix effectively\nconsiders atomic bonding relationships to automatically generate features. The\nsupervised machine learning model random forest is applied for feature ranking\nand pooling. The adjusted R2 is introduced to penalize the inclusion of\nadditional features, providing an assessment of the true contribution of\nfeatures. The prediction results for normal boiling point (Tb), liquid molar\nvolume, critical temperature (Tc) and critical pressure (Pc) obtained using\nArtificial Neural Network and Gaussian Process Regression models confirm the\naccuracy of the molecular representation method. Comparison with GC based\nmodels shows that the root-mean-square error on the test set can be reduced by\nup to 83.8%. To enhance the interpretability of the model, a feature analysis\nmethod based on Shapley values is employed to determine the contribution of\neach feature to the property predictions. The results indicate that using the\nfeature pooling method reduces the number of features from 13316 to 100 without\ncompromising model accuracy. The feature analysis results for Tb, Tc, and Pc\nconfirms that different molecular properties are influenced by different\nstructural features, aligning with mechanistic interpretations. In conclusion,\nthe proposed framework is demonstrated to be feasible and provides a solid\nfoundation for mixture component reconstruction and process integration\nmodelling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u7eaf\u7ec4\u5206\u6027\u8d28\u9884\u6d4b\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5b50\u8868\u793a\u65b9\u6cd5\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528Shapley\u503c\u8fdb\u884c\u7279\u5f81\u5206\u6790\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u7eaf\u7ec4\u5206\u7684\u7269\u7406\u5316\u5b66\u6027\u8d28\u5bf9\u4e8e\u8fc7\u7a0b\u96c6\u6210\u3001\u591a\u5c3a\u5ea6\u5efa\u6a21\u548c\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u63a5\u77e9\u9635\u5206\u5b50\u8868\u793a\u65b9\u6cd5\u548c\u968f\u673a\u68ee\u6797\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u589e\u5f3a\u6846\u67b6\uff0c\u4f7f\u7528\u8c03\u6574\u540e\u7684R2\u8bc4\u4f30\u7279\u5f81\u7684\u771f\u5b9e\u8d21\u732e\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eShapley\u503c\u7684\u7279\u5f81\u5206\u6790\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6a21\u578b\u83b7\u5f97\u7684\u6b63\u5e38\u6cb8\u70b9(Tb)\u3001\u6db2\u6001\u6469\u5c14\u4f53\u79ef\u3001\u4e34\u754c\u6e29\u5ea6(Tc)\u548c\u4e34\u754c\u538b\u529b(Pc)\u7684\u9884\u6d4b\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5206\u5b50\u8868\u793a\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002\u4e0e\u57fa\u4e8eGC\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5747\u65b9\u6839\u8bef\u5dee\u51cf\u5c11\u4e86\u9ad8\u8fbe83.8%\u3002\u7279\u5f81\u6c60\u5316\u65b9\u6cd5\u5c06\u7279\u5f81\u6570\u91cf\u4ece13316\u51cf\u5c11\u5230100\u800c\u4e0d\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027\u3002Tb\u3001Tc\u548cPc\u7684\u7279\u5f81\u5206\u6790\u7ed3\u679c\u786e\u8ba4\u4e86\u4e0d\u540c\u7684\u5206\u5b50\u6027\u8d28\u7531\u4e0d\u540c\u7684\u7ed3\u6784\u7279\u5f81\u5f71\u54cd\uff0c\u4e0e\u673a\u5236\u89e3\u91ca\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u88ab\u8bc1\u660e\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e3a\u6df7\u5408\u7ec4\u5206\u91cd\u6784\u548c\u8fc7\u7a0b\u96c6\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2505.09798", "pdf": "https://arxiv.org/pdf/2505.09798", "abs": "https://arxiv.org/abs/2505.09798", "authors": ["Bojan Ristov", "Stefan Eftimov", "Milena Trajanoska", "Dimitar Trajanov"], "title": "Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Public procurement plays a critical role in government operations, ensuring\nthe efficient allocation of resources and fostering economic growth. However,\ntraditional procurement data is often stored in rigid, tabular formats,\nlimiting its analytical potential and hindering transparency. This research\npresents a methodological framework for transforming structured procurement\ndata into a semantic knowledge graph, leveraging ontological modeling and\nautomated data transformation techniques. By integrating RDF and SPARQL-based\nquerying, the system enhances the accessibility and interpretability of\nprocurement records, enabling complex semantic queries and advanced analytics.\nFurthermore, by incorporating machine learning-driven predictive modeling, the\nsystem extends beyond conventional data analysis, offering insights into\nprocurement trends and risk assessment. This work contributes to the broader\nfield of public procurement intelligence by improving data transparency,\nsupporting evidence-based decision-making, and enabling in-depth analysis of\nprocurement activities in North Macedonia.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7ed3\u6784\u5316\u91c7\u8d2d\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u4e49\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u672c\u4f53\u5efa\u6a21\u3001RDF\u548cSPARQL\u67e5\u8be2\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u900f\u660e\u5ea6\u3001\u652f\u6301\u51b3\u7b56\u5e76\u4fc3\u8fdb\u91c7\u8d2d\u6d3b\u52a8\u7684\u6df1\u5165\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u91c7\u8d2d\u6570\u636e\u901a\u5e38\u5b58\u50a8\u5728\u521a\u6027\u7684\u8868\u683c\u683c\u5f0f\u4e2d\uff0c\u9650\u5236\u4e86\u5176\u5206\u6790\u6f5c\u529b\u5e76\u963b\u788d\u4e86\u900f\u660e\u5ea6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5\u6765\u63d0\u9ad8\u516c\u5171\u91c7\u8d2d\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u5206\u6790\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5316\u91c7\u8d2d\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u4e49\u77e5\u8bc6\u56fe\u8c31\uff0c\u5229\u7528\u672c\u4f53\u5efa\u6a21\u548c\u81ea\u52a8\u5316\u6570\u636e\u8f6c\u6362\u6280\u672f\uff0c\u5e76\u7ed3\u5408RDF\u548cSPARQL\u67e5\u8be2\u4ee5\u589e\u5f3a\u91c7\u8d2d\u8bb0\u5f55\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u9884\u6d4b\u5efa\u6a21\uff0c\u4ee5\u63d0\u4f9b\u91c7\u8d2d\u8d8b\u52bf\u548c\u98ce\u9669\u8bc4\u4f30\u7684\u89c1\u89e3\u3002", "result": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u8bed\u4e49\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u4e86\u91c7\u8d2d\u6570\u636e\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u590d\u6742\u8bed\u4e49\u67e5\u8be2\u548c\u9ad8\u7ea7\u5206\u6790\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u91c7\u8d2d\u8d8b\u52bf\u548c\u98ce\u9669\u8bc4\u4f30\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u8bed\u4e49\u77e5\u8bc6\u56fe\u8c31\uff0c\u63d0\u9ad8\u4e86\u516c\u5171\u91c7\u8d2d\u6570\u636e\u7684\u900f\u660e\u5ea6\uff0c\u652f\u6301\u57fa\u4e8e\u8bc1\u636e\u7684\u51b3\u7b56\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u5317\u9a6c\u5176\u987f\u91c7\u8d2d\u6d3b\u52a8\u7684\u6df1\u5165\u5206\u6790\u3002"}}
{"id": "2505.09803", "pdf": "https://arxiv.org/pdf/2505.09803", "abs": "https://arxiv.org/abs/2505.09803", "authors": ["Antony Sikorski", "Michael Ivanitskiy", "Nathan Lenssen", "Douglas Nychka", "Daniel McKenzie"], "title": "LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In many scientific and industrial applications, we are given a handful of\ninstances (a 'small ensemble') of a spatially distributed quantity (a 'field')\nbut would like to acquire many more. For example, a large ensemble of global\ntemperature sensitivity fields from a climate model can help farmers, insurers,\nand governments plan appropriately. When acquiring more data is prohibitively\nexpensive -- as is the case with climate models -- statistical emulation offers\nan efficient alternative for simulating synthetic yet realistic fields.\nHowever, parameter inference using maximum likelihood estimation (MLE) is\ncomputationally prohibitive, especially for large, non-stationary fields. Thus,\nmany recent works train neural networks to estimate parameters given spatial\nfields as input, sidestepping MLE completely. In this work we focus on a\npopular class of parametric, spatially autoregressive (SAR) models. We make a\nsimple yet impactful observation; because the SAR parameters can be arranged on\na regular grid, both inputs (spatial fields) and outputs (model parameters) can\nbe viewed as images. Using this insight, we demonstrate that image-to-image\n(I2I) networks enable faster and more accurate parameter estimation for a class\nof non-stationary SAR models with unprecedented complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5230\u56fe\u50cf\u7f51\u7edc\u7684\u7a7a\u95f4\u81ea\u56de\u5f52\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u5e73\u7a33SAR\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u66f4\u5feb\u548c\u66f4\u51c6\u786e\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1(MLE)\u5728\u8ba1\u7b97\u4e0a\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u578b\u3001\u975e\u5e73\u7a33\u7684\u573a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1SAR\u6a21\u578b\u7684\u53c2\u6570\u3002", "method": "\u672c\u6587\u5229\u7528\u7a7a\u95f4\u81ea\u56de\u5f52(SAR)\u6a21\u578b\u7684\u53c2\u6570\u53ef\u4ee5\u6392\u5217\u5728\u89c4\u5219\u7f51\u683c\u4e0a\u7684\u7279\u6027\uff0c\u5c06\u8f93\u5165\uff08\u7a7a\u95f4\u573a\uff09\u548c\u8f93\u51fa\uff08\u6a21\u578b\u53c2\u6570\uff09\u89c6\u4e3a\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528\u56fe\u50cf\u5230\u56fe\u50cf(I2I)\u7f51\u7edc\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u56fe\u50cf\u5230\u56fe\u50cf\u7f51\u7edc\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u5728\u975e\u5e73\u7a33SAR\u6a21\u578b\u4e0a\u5177\u6709\u66f4\u9ad8\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u50cf\u5230\u56fe\u50cf\u7f51\u7edc\u8fdb\u884c\u7a7a\u95f4\u81ea\u56de\u5f52\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u5e73\u7a33SAR\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u66f4\u5feb\u548c\u66f4\u51c6\u786e\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10371", "pdf": "https://arxiv.org/pdf/2505.10371", "abs": "https://arxiv.org/abs/2505.10371", "authors": ["Kai Sun", "Peibo Duan", "Levin Kuhlmann", "Beilun Wang", "Bin Zhang"], "title": "ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "The Spiking Neural Network (SNN) has drawn increasing attention for its\nenergy-efficient, event-driven processing and biological plausibility. To train\nSNNs via backpropagation, surrogate gradients are used to approximate the\nnon-differentiable spike function, but they only maintain nonzero derivatives\nwithin a narrow range of membrane potentials near the firing threshold,\nreferred to as the surrogate gradient support width gamma. We identify a major\nchallenge, termed the dilemma of gamma: a relatively large gamma leads to\noveractivation, characterized by excessive neuron firing, which in turn\nincreases energy consumption, whereas a small gamma causes vanishing gradients\nand weakens temporal dependencies. To address this, we propose a temporal\nInhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological\ninhibitory mechanisms. This model incorporates interconnected inhibitory units\nfor membrane potential and current, effectively mitigating overactivation while\npreserving gradient propagation. Theoretical analysis demonstrates ILIF\neffectiveness in overcoming the gamma dilemma, and extensive experiments on\nmultiple datasets show that ILIF improves energy efficiency by reducing firing\nrates, stabilizes training, and enhances accuracy. The code is available at\ngithub.com/kaisun1/ILIF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ILIF\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3SNN\u8bad\u7ec3\u4e2d\u7684gamma\u56f0\u5883\uff0c\u63d0\u9ad8\u4e86\u80fd\u91cf\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4ee3\u7406\u68af\u5ea6\u7684\u652f\u6301\u5bbd\u5ea6gamma\u5b58\u5728\u56f0\u5883\uff0c\u5373\u8f83\u5927\u7684gamma\u5bfc\u81f4\u8fc7\u5ea6\u6fc0\u6d3b\uff0c\u800c\u8f83\u5c0f\u7684gamma\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u5143\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u6291\u5236\u673a\u5236\u7684\u65f6\u5e8f\u6291\u5236\u6f0f\u7535\u79ef\u5206-\u53d1\u653e\uff08ILIF\uff09\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u819c\u7535\u4f4d\u548c\u7535\u6d41\u7684\u76f8\u4e92\u8fde\u63a5\u7684\u6291\u5236\u5355\u5143\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u5ea6\u6fc0\u6d3b\u95ee\u9898\u3002", "result": "ILIF\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u80fd\u591f\u964d\u4f4e\u8109\u51b2\u7387\uff0c\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\uff0c\u7a33\u5b9a\u8bad\u7ec3\u5e76\u589e\u5f3a\u51c6\u786e\u6027\u3002", "conclusion": "ILIF\u6a21\u578b\u6709\u6548\u5730\u89e3\u51b3\u4e86gamma\u56f0\u5883\uff0c\u63d0\u9ad8\u4e86\u80fd\u91cf\u6548\u7387\uff0c\u7a33\u5b9a\u4e86\u8bad\u7ec3\uff0c\u5e76\u589e\u5f3a\u4e86\u51c6\u786e\u6027\u3002"}}
{"id": "2505.10375", "pdf": "https://arxiv.org/pdf/2505.10375", "abs": "https://arxiv.org/abs/2505.10375", "authors": ["Rui Melo", "Claudia Mamede", "Andre Catarino", "Rui Abreu", "Henrique Lopes Cardoso"], "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "10 pages, 10 figures", "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u4f5c\u4e3aJava\u51fd\u6570\u4e2d\u9519\u8bef\u68c0\u6d4b\u7684\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0cSAEs\u53ef\u4ee5\u4ece\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u76f4\u63a5\u68c0\u6d4b\u8f6f\u4ef6\u9519\u8bef\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u6216\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u3002", "motivation": "\u8f6f\u4ef6\u6f0f\u6d1e\u5982\u7f13\u51b2\u533a\u6ea2\u51fa\u548cSQL\u6ce8\u5165\u662f\u5b89\u5168\u6f0f\u6d1e\u7684\u4e3b\u8981\u6765\u6e90\u3002\u4f20\u7edf\u7684\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u4ecd\u7136\u91cd\u8981\uff0c\u4f46\u53d7\u5230\u9ad8\u8bef\u62a5\u7387\u3001\u53ef\u6269\u5c55\u6027\u95ee\u9898\u548c\u5bf9\u4eba\u5de5\u52aa\u529b\u7684\u4f9d\u8d56\u7684\u9650\u5236\u3002\u8fd9\u4e9b\u9650\u5236\u4fc3\u4f7f\u4eba\u4eec\u5bf9\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5\u8fdb\u884c\u81ea\u52a8\u5316\u6f0f\u6d1e\u68c0\u6d4b\u548c\u5b89\u5168\u4ee3\u7801\u751f\u6210\u4ea7\u751f\u4e86\u5174\u8da3\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u5206\u7c7b\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u5b83\u4eec\u7684\u590d\u6742\u6027\u548c\u4e0d\u900f\u660e\u6027\u5bf9\u53ef\u89e3\u91ca\u6027\u548c\u90e8\u7f72\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u6211\u4eec\u63a2\u7d22\u4e86\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3aJava\u51fd\u6570\u4e2d\u9519\u8bef\u68c0\u6d4b\u7684\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5f53\u5e94\u7528\u4e8eGPT-2 Small\u548cGemma 2B\u7684\u8868\u793a\u65f6\uff0cSAEs\u7684\u6709\u6548\u6027\uff0c\u68c0\u67e5\u5b83\u4eec\u5728\u4e0d\u5fae\u8c03\u5e95\u5c42LLMs\u7684\u60c5\u51b5\u4e0b\u7a81\u51fa\u663e\u793a\u9519\u8bef\u884c\u4e3a\u7684\u80fd\u529b\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0cSAE\u884d\u751f\u7684\u7279\u5f81\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe89%\u7684F1\u5206\u6570\u7684\u9519\u8bef\u68c0\u6d4b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5fae\u8c03\u7684\u53d8\u538b\u5668\u7f16\u7801\u5668\u57fa\u7ebf\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u5b9e\u8bc1\u8bc1\u636e\uff0c\u8868\u660e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u53ef\u4ee5\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u68c0\u6d4b\u8f6f\u4ef6\u9519\u8bef\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u6216\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u3002"}}
{"id": "2505.10387", "pdf": "https://arxiv.org/pdf/2505.10387", "abs": "https://arxiv.org/abs/2505.10387", "authors": ["Artem Agafonov", "Konstantin Yakovlev"], "title": "Multi-Agent Path Finding For Large Agents Is Intractable", "categories": ["cs.MA", "cs.AI", "cs.CC"], "comment": null, "summary": "The multi-agent path finding (MAPF) problem asks to find a set of paths on a\ngraph such that when synchronously following these paths the agents never\nencounter a conflict. In the most widespread MAPF formulation, the so-called\nClassical MAPF, the agents sizes are neglected and two types of conflicts are\nconsidered: occupying the same vertex or using the same edge at the same time\nstep. Meanwhile in numerous practical applications, e.g. in robotics, taking\ninto account the agents' sizes is vital to ensure that the MAPF solutions can\nbe safely executed. Introducing large agents yields an additional type of\nconflict arising when one agent follows an edge and its body overlaps with the\nbody of another agent that is actually not using this same edge (e.g. staying\nstill at some distinct vertex of the graph). Until now it was not clear how\nharder the problem gets when such conflicts are to be considered while\nplanning. Specifically, it was known that Classical MAPF problem on an\nundirected graph can be solved in polynomial time, however no complete\npolynomial-time algorithm was presented to solve MAPF with large agents. In\nthis paper we, for the first time, establish that the latter problem is NP-hard\nand, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be\npresented. Our proof is based on the prevalent in the field technique of\nreducing the seminal 3SAT problem (which is known to be an NP-complete problem)\nto the problem at hand. In particular, for an arbitrary 3SAT formula we\nprocedurally construct a dedicated graph with specific start and goal vertices\nand show that the given 3SAT formula is satisfiable iff the corresponding path\nfinding instance has a solution.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u8bc1\u660e\u4e86\u8003\u8651\u5927\u4ee3\u7406\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898NP\u96be\uff0c\u8868\u660e\u5728P\u2260NP\u7684\u60c5\u51b5\u4e0b\uff0c\u65e0\u6cd5\u4e3a\u8be5\u95ee\u9898\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "motivation": "\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5982\u673a\u5668\u4eba\u9886\u57df\uff0c\u8003\u8651\u4ee3\u7406\u7684\u5927\u5c0f\u5bf9\u4e8e\u786e\u4fddMAPF\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u5b89\u5168\u6267\u884c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5f53\u8003\u8651\u8fd9\u4e9b\u51b2\u7a81\u65f6\uff0c\u95ee\u9898\u7684\u96be\u5ea6\u4f1a\u589e\u52a0\u591a\u5c11\u3002", "method": "\u901a\u8fc7\u5c06\u7ecf\u5178\u76843SAT\u95ee\u9898\uff08\u5df2\u77e5\u662fNP\u5b8c\u5168\u95ee\u9898\uff09\u5f52\u7ea6\u5230\u8be5\u95ee\u9898\uff0c\u4ece\u800c\u8bc1\u660e\u4e86\u8be5\u95ee\u9898\u7684NP\u96be\u6027\u3002", "result": "\u672c\u6587\u9996\u6b21\u8bc1\u660e\u4e86\u8003\u8651\u5927\u4ee3\u7406\u7684MAPF\u95ee\u9898\u662fNP\u96be\u7684\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u8bc1\u660e\u4e86\u8003\u8651\u5927\u4ee3\u7406\u7684MAPF\u95ee\u9898NP\u96be\uff0c\u56e0\u6b64\u5982\u679cP\u2260NP\uff0c\u5219\u65e0\u6cd5\u4e3a\u8be5\u95ee\u9898\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002"}}
{"id": "2505.09833", "pdf": "https://arxiv.org/pdf/2505.09833", "abs": "https://arxiv.org/abs/2505.09833", "authors": ["Tuba Girgin", "Emre Girgin", "Cagri Kilic"], "title": "Learning Rock Pushability on Rough Planetary Terrain", "categories": ["cs.RO", "cs.LG"], "comment": "Paper presented at the Workshop on Field Robotics, ICRA 2025,\n  Atlanta, GA, United States", "summary": "In the context of mobile navigation in unstructured environments, the\npredominant approach entails the avoidance of obstacles. The prevailing path\nplanning algorithms are contingent upon deviating from the intended path for an\nindefinite duration and returning to the closest point on the route after the\nobstacle is left behind spatially. However, avoiding an obstacle on a path that\nwill be used repeatedly by multiple agents can hinder long-term efficiency and\nlead to a lasting reliance on an active path planning system. In this study, we\npropose an alternative approach to mobile navigation in unstructured\nenvironments by leveraging the manipulation capabilities of a robotic\nmanipulator mounted on top of a mobile robot. Our proposed framework integrates\nexteroceptive and proprioceptive feedback to assess the push affordance of\nobstacles, facilitating their repositioning rather than avoidance. While our\npreliminary visual estimation takes into account the characteristics of both\nthe obstacle and the surface it relies on, the push affordance estimation\nmodule exploits the force feedback obtained by interacting with the obstacle\nvia a robotic manipulator as the guidance signal. The objective of our\nnavigation approach is to enhance the efficiency of routes utilized by multiple\nagents over extended periods by reducing the overall time spent by a fleet in\nenvironments where autonomous infrastructure development is imperative, such as\nlunar or Martian surfaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79fb\u52a8\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u969c\u788d\u7269\u800c\u4e0d\u662f\u907f\u5f00\u5b83\u4eec\uff0c\u4ee5\u63d0\u9ad8\u591a\u4e2a\u4ee3\u7406\u5728\u957f\u671f\u4f7f\u7528\u4e2d\u7684\u8def\u7ebf\u6548\u7387\u3002", "motivation": "\u907f\u514d\u969c\u788d\u7269\u53ef\u80fd\u4f1a\u964d\u4f4e\u957f\u671f\u6548\u7387\uff0c\u5e76\u5bfc\u81f4\u5bf9\u4e3b\u52a8\u8def\u5f84\u89c4\u5212\u7cfb\u7edf\u7684\u6301\u7eed\u4f9d\u8d56\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5229\u7528\u5b89\u88c5\u5728\u79fb\u52a8\u673a\u5668\u4eba\u9876\u90e8\u7684\u673a\u68b0\u81c2\u7684\u64cd\u7eb5\u80fd\u529b\uff0c\u901a\u8fc7\u6574\u5408\u5916\u611f\u53d7\u548c\u672c\u4f53\u611f\u53d7\u53cd\u9988\u6765\u8bc4\u4f30\u969c\u788d\u7269\u7684\u63a8\u52a8\u53ef\u80fd\u6027\uff0c\u4ece\u800c\u91cd\u65b0\u5b9a\u4f4d\u969c\u788d\u7269\u800c\u4e0d\u662f\u907f\u5f00\u5b83\u4eec\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u969c\u788d\u7269\u800c\u4e0d\u662f\u907f\u5f00\u5b83\u4eec\uff0c\u63d0\u9ad8\u4e86\u591a\u4e2a\u4ee3\u7406\u5728\u957f\u671f\u4f7f\u7528\u4e2d\u7684\u8def\u7ebf\u6548\u7387\u3002", "conclusion": "\u6211\u4eec\u7684\u5bfc\u822a\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u5728\u9700\u8981\u81ea\u4e3b\u57fa\u7840\u8bbe\u65bd\u5f00\u53d1\u7684\u73af\u5883\u4e2d\uff0c\u8230\u961f\u82b1\u8d39\u7684\u603b\u65f6\u95f4\uff0c\u6765\u63d0\u9ad8\u591a\u4e2a\u4ee3\u7406\u957f\u671f\u4f7f\u7528\u7684\u8def\u7ebf\u6548\u7387\u3002"}}
{"id": "2505.10393", "pdf": "https://arxiv.org/pdf/2505.10393", "abs": "https://arxiv.org/abs/2505.10393", "authors": ["Agustin Medina", "Marcelo Arlego", "Carlos A. Lamas"], "title": "Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training", "categories": ["cond-mat.str-el", "cs.AI"], "comment": "25 pages, 14 figures", "summary": "We investigate the efficient learning of magnetic phases using artificial\nneural networks trained on synthetic data, combining computational simplicity\nwith physics-informed strategies. Focusing on the diluted Ising model, which\nlacks an exact analytical solution, we explore two complementary approaches: a\nsupervised classification using simple dense neural networks, and an\nunsupervised detection of phase transitions using convolutional autoencoders\ntrained solely on idealized spin configurations.\n  To enhance model performance, we incorporate two key forms of\nphysics-informed guidance. First, we exploit architectural biases which\npreferentially amplify features related to symmetry breaking. Second, we\ninclude training configurations that explicitly break $\\mathbb{Z}_2$ symmetry,\nreinforcing the network's ability to detect ordered phases. These mechanisms,\nacting in tandem, increase the network's sensitivity to phase structure even in\nthe absence of explicit labels. We validate the machine learning predictions\nthrough comparison with direct numerical estimates of critical temperatures and\npercolation thresholds.\n  Our results show that synthetic, structured, and computationally efficient\ntraining schemes can reveal physically meaningful phase boundaries, even in\ncomplex systems. This framework offers a low-cost and robust alternative to\nconventional methods, with potential applications in broader condensed matter\nand statistical physics contexts.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u9ad8\u6548\u5b66\u4e60\u78c1\u76f8\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u8ba1\u7b97\u7b80\u5355\u6027\u548c\u7269\u7406\u6307\u5bfc\u7b56\u7565\u3002\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff08\u76d1\u7763\u5206\u7c7b\u548c\u65e0\u76d1\u7763\u68c0\u6d4b\uff09\u7814\u7a76\u7a00\u91ca\u4f0a\u8f9b\u6a21\u578b\u7684\u76f8\u53d8\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u7269\u7406\u6307\u5bfc\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u76f8\u8fb9\u754c\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u6211\u4eec\u65e8\u5728\u901a\u8fc7\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u9ad8\u6548\u5b66\u4e60\u78c1\u76f8\uff0c\u7ed3\u5408\u8ba1\u7b97\u7b80\u5355\u6027\u548c\u7269\u7406\u6307\u5bfc\u7b56\u7565\u3002\u7531\u4e8e\u7a00\u91ca\u4f0a\u8f9b\u6a21\u578b\u7f3a\u4e4f\u7cbe\u786e\u7684\u89e3\u6790\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u5176\u76f8\u53d8\u884c\u4e3a\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5bf9\u5408\u6210\u6570\u636e\u8fdb\u884c\u9ad8\u6548\u5b66\u4e60\uff0c\u7ed3\u5408\u8ba1\u7b97\u7b80\u5355\u6027\u548c\u7269\u7406\u6307\u5bfc\u7b56\u7565\u3002\u6211\u4eec\u4e13\u6ce8\u4e8e\u7a00\u91ca\u4f0a\u8f9b\u6a21\u578b\uff0c\u63a2\u7d22\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a\u4f7f\u7528\u7b80\u5355\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u7684\u76d1\u7763\u5206\u7c7b\uff0c\u4ee5\u53ca\u4f7f\u7528\u4ec5\u5728\u7406\u60f3\u81ea\u65cb\u914d\u7f6e\u4e0a\u8bad\u7ec3\u7684\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u65e0\u76d1\u7763\u76f8\u53d8\u68c0\u6d4b\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u5173\u952e\u7684\u7269\u7406\u6307\u5bfc\u5f62\u5f0f\uff1a\u9996\u5148\uff0c\u6211\u4eec\u5229\u7528\u67b6\u6784\u504f\u5dee\uff0c\u4f18\u5148\u653e\u5927\u4e0e\u5bf9\u79f0\u6027\u7834\u7f3a\u76f8\u5173\u7684\u7279\u5f81\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u914d\u7f6e\u4e2d\u5305\u62ec\u660e\u786e\u6253\u7834Z2\u5bf9\u79f0\u6027\u7684\u914d\u7f6e\uff0c\u4ee5\u589e\u5f3a\u7f51\u7edc\u68c0\u6d4b\u6709\u5e8f\u76f8\u7684\u80fd\u529b\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u3001\u7ed3\u6784\u5316\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\u53ef\u4ee5\u63ed\u793a\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u76f8\u8fb9\u754c\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\u7684\u65b9\u6848\uff0c\u5e76\u5177\u6709\u5728\u66f4\u5e7f\u6cdb\u7684\u51dd\u805a\u6001\u7269\u7406\u548c\u7edf\u8ba1\u7269\u7406\u80cc\u666f\u4e0b\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u3001\u7ed3\u6784\u5316\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\u53ef\u4ee5\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u63ed\u793a\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u76f8\u8fb9\u754c\u3002\u8fd9\u79cd\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5728\u66f4\u5e7f\u6cdb\u7684\u51dd\u805a\u6001\u7269\u7406\u548c\u7edf\u8ba1\u7269\u7406\u80cc\u666f\u4e0b\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.09843", "pdf": "https://arxiv.org/pdf/2505.09843", "abs": "https://arxiv.org/abs/2505.09843", "authors": ["Melissa Turcotte", "Fran\u00e7ois Labr\u00e8che", "Serge-Olivier Paquette"], "title": "Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts", "categories": ["cs.CR", "cs.LG", "stat.AP"], "comment": null, "summary": "Enterprise networks are growing ever larger with a rapidly expanding attack\nsurface, increasing the volume of security alerts generated from security\ncontrols. Security Operations Centre (SOC) analysts triage these alerts to\nidentify malicious activity, but they struggle with alert fatigue due to the\noverwhelming number of benign alerts. Organisations are turning to managed SOC\nproviders, where the problem is amplified by context switching and limited\nvisibility into business processes.\n  A novel system, named AACT, is introduced that automates SOC workflows by\nlearning from analysts' triage actions on cybersecurity alerts. It accurately\npredicts triage decisions in real time, allowing benign alerts to be closed\nautomatically and critical ones prioritised. This reduces the SOC queue\nallowing analysts to focus on the most severe, relevant or ambiguous threats.\nThe system has been trained and evaluated on both real SOC data and an open\ndataset, obtaining high performance in identifying malicious alerts from benign\nalerts.\n  Additionally, the system has demonstrated high accuracy in a real SOC\nenvironment, reducing alerts shown to analysts by 61% over six months, with a\nlow false negative rate of 1.36% over millions of alerts.", "AI": {"tldr": "AACT\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316SOC\u5de5\u4f5c\u6d41\u7a0b\uff0c\u51cf\u5c11\u8b66\u62a5\u6570\u91cf\u5e76\u63d0\u9ad8\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5206\u6790\u4eba\u5458\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002", "motivation": "\u4f01\u4e1a\u7f51\u7edc\u4e0d\u65ad\u6269\u5927\uff0c\u5b89\u5168\u8b66\u62a5\u6570\u91cf\u589e\u52a0\uff0c\u5bfc\u81f4SOC\u5206\u6790\u4eba\u5458\u9762\u4e34\u8b66\u62a5\u75b2\u52b3\u95ee\u9898\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u6258\u7ba1SOC\u670d\u52a1\u65f6\uff0c\u95ee\u9898\u56e0\u4e0a\u4e0b\u6587\u5207\u6362\u548c\u6709\u9650\u7684\u4e1a\u52a1\u6d41\u7a0b\u53ef\u89c1\u6027\u800c\u52a0\u5267\u3002", "method": "AACT\u7cfb\u7edf\u901a\u8fc7\u5b66\u4e60\u5206\u6790\u4eba\u5458\u5bf9\u7f51\u7edc\u5b89\u5168\u8b66\u62a5\u7684\u5206\u7c7b\u64cd\u4f5c\u6765\u81ea\u52a8\u5316SOC\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b9e\u65f6\u51c6\u786e\u9884\u6d4b\u5206\u7c7b\u51b3\u7b56\u3002", "result": "AACT\u7cfb\u7edf\u5728\u771f\u5b9eSOC\u6570\u636e\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u3002\u5728\u5b9e\u9645SOC\u73af\u5883\u4e2d\uff0c\u5b83\u51cf\u5c11\u4e8661%\u7684\u8b66\u62a5\u5c55\u793a\u7ed9\u5206\u6790\u4eba\u5458\uff0c\u4e14\u8bef\u62a5\u7387\u4ec5\u4e3a1.36%\u3002", "conclusion": "AACT\u7cfb\u7edf\u5728\u5b9e\u9645SOC\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5c55\u793a\u7ed9\u5206\u6790\u4eba\u5458\u7684\u8b66\u62a5\u6570\u91cf\uff0c\u5e76\u4fdd\u6301\u4e86\u4f4e\u8bef\u62a5\u7387\u3002"}}
{"id": "2505.10394", "pdf": "https://arxiv.org/pdf/2505.10394", "abs": "https://arxiv.org/abs/2505.10394", "authors": ["Meghyn Bienvenu", "Camille Bourgaux", "Atefe Khodadaditaghanaki"], "title": "Inconsistency Handling in DatalogMTL", "categories": ["cs.LO", "cs.AI", "cs.DB"], "comment": "This is an extended version of a paper appearing at the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI 2025). 24\n  pages", "summary": "In this paper, we explore the issue of inconsistency handling in DatalogMTL,\nan extension of Datalog with metric temporal operators. Since facts are\nassociated with time intervals, there are different manners to restore\nconsistency when they contradict the rules, such as removing facts or modifying\ntheir time intervals. Our first contribution is the definition of relevant\nnotions of conflicts (minimal explanations for inconsistency) and repairs\n(possible ways of restoring consistency) for this setting and the study of the\nproperties of these notions and the associated inconsistency-tolerant\nsemantics. Our second contribution is a data complexity analysis of the tasks\nof generating a single conflict / repair and query entailment under\nrepair-based semantics.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86DatalogMTL\u4e2d\u5904\u7406\u4e0d\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u751f\u6210\u51b2\u7a81/\u4fee\u590d\u548c\u67e5\u8be2\u8574\u542b\u7684\u6570\u636e\u590d\u6742\u6027\u3002", "motivation": "\u7531\u4e8e\u4e8b\u5b9e\u4e0e\u65f6\u95f4\u533a\u95f4\u76f8\u5173\u8054\uff0c\u5f53\u5b83\u4eec\u4e0e\u89c4\u5219\u77db\u76fe\u65f6\uff0c\u6709\u591a\u79cd\u65b9\u5f0f\u53ef\u4ee5\u6062\u590d\u4e00\u81f4\u6027\uff0c\u4f8b\u5982\u5220\u9664\u4e8b\u5b9e\u6216\u4fee\u6539\u5176\u65f6\u95f4\u533a\u95f4\u3002", "method": "\u5b9a\u4e49\u4e86\u51b2\u7a81\u548c\u4fee\u590d\u7684\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u751f\u6210\u5355\u4e2a\u51b2\u7a81/\u4fee\u590d\u548c\u67e5\u8be2\u8574\u542b\u7684\u6570\u636e\u590d\u6742\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u51b2\u7a81\u548c\u4fee\u590d\u7684\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u751f\u6210\u5355\u4e2a\u51b2\u7a81/\u4fee\u590d\u548c\u67e5\u8be2\u8574\u542b\u7684\u6570\u636e\u590d\u6742\u6027\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728DatalogMTL\u4e2d\u5904\u7406\u4e0d\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u76f8\u5173\u6982\u5ff5\u548c\u5c5e\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4fee\u590d\u7684\u8bed\u4e49\u4e0b\u7684\u67e5\u8be2\u8574\u542b\u95ee\u9898\u3002"}}
{"id": "2505.09972", "pdf": "https://arxiv.org/pdf/2505.09972", "abs": "https://arxiv.org/abs/2505.09972", "authors": ["Anchen Sun", "Tiantian Feng", "Gabriela Gutierrez", "Juan J Londono", "Anfeng Xu", "Batya Elbaum", "Shrikanth Narayanan", "Lynn K Perry", "Daniel S Messinger"], "title": "Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech", "categories": ["eess.AS", "cs.LG"], "comment": "8 pages, 2 figures, 5 tables", "summary": "This paper introduces an automated framework WSW2.0 for analyzing vocal\ninteractions in preschool classrooms, enhancing both accuracy and scalability\nthrough the integration of wav2vec2-based speaker classification and Whisper\n(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio\nrecordings (160 minutes from 12 children and 75 minutes from 5 teachers), were\nused to compare system outputs to expert human annotations. WSW2.0 achieves a\nweighted F1 score of .845, accuracy of .846, and an error-corrected kappa of\n.672 for speaker classification (child vs. teacher). Transcription quality is\nmoderate to high with word error rates of .119 for teachers and .238 for\nchildren. WSW2.0 exhibits relatively high absolute agreement intraclass\ncorrelations (ICC) with expert transcriptions for a range of classroom language\nfeatures. These include teacher and child mean utterance length, lexical\ndiversity, question asking, and responses to questions and other utterances,\nwhich show absolute agreement intraclass correlations between .64 and .98. To\nestablish scalability, we apply the framework to an extensive dataset spanning\ntwo years and over 1,592 hours of classroom audio recordings, demonstrating the\nframework's robustness for broad real-world applications. These findings\nhighlight the potential of deep learning and natural language processing\ntechniques to revolutionize educational research by providing accurate measures\nof key features of preschool classroom speech, ultimately guiding more\neffective intervention strategies and supporting early childhood language\ndevelopment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aWSW2.0\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5e7c\u513f\u56ed\u6559\u5ba4\u4e2d\u7684\u8bed\u97f3\u4e92\u52a8\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8ewav2vec2\u7684\u8bf4\u8bdd\u4eba\u5206\u7c7b\u548cWhisper\u8bed\u97f3\u8f6c\u5f55\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cWSW2.0\u5728\u8bf4\u8bdd\u4eba\u5206\u7c7b\u548c\u8bed\u97f3\u8f6c\u5f55\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u591a\u4e2a\u8bfe\u5802\u8bed\u8a00\u7279\u5f81\u4e0a\u4e0e\u4e13\u5bb6\u8f6c\u5f55\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5e7c\u513f\u56ed\u6559\u5ba4\u4e2d\u8bed\u97f3\u4e92\u52a8\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u8be5\u7814\u7a76\u5f00\u53d1\u4e86WSW2.0\u6846\u67b6\u3002", "method": "\u8be5\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6WSW2.0\uff0c\u7528\u4e8e\u5206\u6790\u5e7c\u513f\u56ed\u6559\u5ba4\u4e2d\u7684\u8bed\u97f3\u4e92\u52a8\uff0c\u901a\u8fc7\u96c6\u6210wav2vec2-based\u8bf4\u8bdd\u4eba\u5206\u7c7b\u548cWhisper\uff08large-v2\u548clarge-v3\uff09\u8bed\u97f3\u8f6c\u5f55\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "WSW2.0\u5728\u8bf4\u8bdd\u4eba\u5206\u7c7b\uff08\u513f\u7ae5\u4e0e\u6559\u5e08\uff09\u4e0a\u8fbe\u5230\u4e86\u52a0\u6743F1\u5206\u65700.845\u3001\u51c6\u786e\u73870.846\u548c\u8bef\u5dee\u6821\u6b63\u7684kappa\u503c0.672\u3002\u5bf9\u4e8e\u6559\u5e08\u548c\u513f\u7ae5\u7684\u8bed\u97f3\u8f6c\u5f55\u8d28\u91cf\u5206\u522b\u4e3a\u4e2d\u7b49\u81f3\u9ad8\uff0c\u8bcd\u9519\u8bef\u7387\u5206\u522b\u4e3a0.119\u548c0.238\u3002WSW2.0\u5728\u4e00\u7cfb\u5217\u8bfe\u5802\u8bed\u8a00\u7279\u5f81\u65b9\u9762\u4e0e\u4e13\u5bb6\u8f6c\u5f55\u6709\u76f8\u5bf9\u8f83\u9ad8\u7684\u7edd\u5bf9\u4e00\u81f4\u6027\uff0c\u5305\u62ec\u6559\u5e08\u548c\u513f\u7ae5\u7684\u5e73\u5747\u8bdd\u8bed\u957f\u5ea6\u3001\u8bcd\u6c47\u591a\u6837\u6027\u3001\u63d0\u95ee\u4ee5\u53ca\u5bf9\u95ee\u9898\u548c\u5176\u4ed6\u8bdd\u8bed\u7684\u56de\u5e94\uff0c\u8fd9\u4e9b\u7279\u5f81\u7684\u7edd\u5bf9\u4e00\u81f4\u6027ICC\u57280.64\u52300.98\u4e4b\u95f4\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u88ab\u5e94\u7528\u4e8e\u4e00\u4e2a\u6db5\u76d6\u4e24\u5e74\u65f6\u95f4\u8d85\u8fc71,592\u5c0f\u65f6\u8bfe\u5802\u97f3\u9891\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5e7f\u6cdb\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5728\u6559\u80b2\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u63d0\u4f9b\u51c6\u786e\u7684\u5b66\u524d\u73ed\u8bfe\u5802\u8a00\u8bed\u5173\u952e\u7279\u5f81\u6d4b\u91cf\uff0c\u6700\u7ec8\u6307\u5bfc\u66f4\u6709\u6548\u7684\u5e72\u9884\u7b56\u7565\u5e76\u652f\u6301\u65e9\u671f\u513f\u7ae5\u8bed\u8a00\u53d1\u5c55\u3002"}}
{"id": "2505.10442", "pdf": "https://arxiv.org/pdf/2505.10442", "abs": "https://arxiv.org/abs/2505.10442", "authors": ["Dechen Gao", "Hang Wang", "Hanchu Zhou", "Nejib Ammar", "Shatadal Mishra", "Ahmadreza Moradipari", "Iman Soltani", "Junshan Zhang"], "title": "IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Imitation learning (IL) and reinforcement learning (RL) each offer distinct\nadvantages for robotics policy learning: IL provides stable learning from\ndemonstrations, and RL promotes generalization through exploration. While\nexisting robot learning approaches using IL-based pre-training followed by\nRL-based fine-tuning are promising, this two-step learning paradigm often\nsuffers from instability and poor sample efficiency during the RL fine-tuning\nphase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning\nand Imitation Learning, for policy fine-tuning, which periodically injects IL\nupdates after multiple RL updates and hence can benefit from the stability of\nIL and the guidance of expert data for more efficient exploration throughout\nthe entire fine-tuning process. Since IL and RL involve different optimization\nobjectives, we develop gradient separation mechanisms to prevent destructive\ninterference during \\ABBR fine-tuning, by separating possibly conflicting\ngradient updates in orthogonal subspaces. Furthermore, we conduct rigorous\nanalysis, and our findings shed light on why interleaving IL with RL stabilizes\nlearning and improves sample-efficiency. Extensive experiments on 14 robot\nmanipulation and locomotion tasks across 3 benchmarks, including\nFurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \\ABBR can\nsignificantly improve sample efficiency and mitigate performance collapse\nduring online finetuning in both long- and short-horizon tasks with either\nsparse or dense rewards. IN-RIL, as a general plug-in compatible with various\nstate-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,\nfrom 12\\% to 88\\% with 6.3x improvement in the success rate on Robomimic\nTransport. Project page: https://github.com/ucd-dare/IN-RIL.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10033", "pdf": "https://arxiv.org/pdf/2505.10033", "abs": "https://arxiv.org/abs/2505.10033", "authors": ["Luis F. W. Batista", "St\u00e9phanie Aravecchia", "Seth Hutchinson", "C\u00e9dric Pradalier"], "title": "Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests", "categories": ["cs.RO", "cs.LG"], "comment": "Workshop on Field Robotics at ICRA 2025", "summary": "Despite significant advancements in Deep Reinforcement Learning (DRL) for\nAutonomous Surface Vehicles (ASVs), their robustness in real-world conditions,\nparticularly under external disturbances, remains insufficiently explored. In\nthis paper, we evaluate the resilience of a DRL-based agent designed to capture\nfloating waste under various perturbations. We train the agent using domain\nrandomization and evaluate its performance in real-world field tests, assessing\nits ability to handle unexpected disturbances such as asymmetric drag and an\noff-center payload. We assess the agent's performance under these perturbations\nin both simulation and real-world experiments, quantifying performance\ndegradation and benchmarking it against an MPC baseline. Results indicate that\nthe DRL agent performs reliably despite significant disturbances. Along with\nthe open-source release of our implementation, we provide insights into\neffective training strategies, real-world challenges, and practical\nconsiderations for deploying DRLbased ASV controllers.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8eDRL\u7684\u4ee3\u7406\u5728\u5404\u79cd\u6270\u52a8\u4e0b\u7684\u5f39\u6027\uff0c\u5305\u62ec\u4e0d\u5bf9\u79f0\u963b\u529b\u548c\u504f\u5fc3\u8f7d\u8377\u3002\u7ed3\u679c\u663e\u793a\uff0cDRL\u4ee3\u7406\u5728\u9762\u5bf9\u663e\u8457\u5e72\u6270\u65f6\u8868\u73b0\u53ef\u9760\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u81ea\u4e3b\u6c34\u9762\u822a\u884c\u5668\uff08ASVs\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5916\u90e8\u5e72\u6270\u4e0b\uff0c\u4ecd\u7f3a\u4e4f\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u9886\u57df\u968f\u673a\u5316\u8bad\u7ec3\u4ee3\u7406\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u73b0\u573a\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u8bc4\u4f30\u5176\u5904\u7406\u610f\u5916\u5e72\u6270\u7684\u80fd\u529b\uff0c\u5982\u4e0d\u5bf9\u79f0\u963b\u529b\u548c\u504f\u5fc3\u8f7d\u8377\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cDRL\u4ee3\u7406\u5728\u9762\u5bf9\u663e\u8457\u5e72\u6270\u65f6\u8868\u73b0\u53ef\u9760\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u5b58\u5728\u663e\u8457\u7684\u5e72\u6270\uff0cDRL\u4ee3\u7406\u4ecd\u7136\u80fd\u591f\u53ef\u9760\u5730\u5de5\u4f5c\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u3001\u73b0\u5b9e\u4e16\u754c\u7684\u6311\u6218\u548c\u90e8\u7f72DRL\u57fa\u4e8eASV\u63a7\u5236\u5668\u7684\u5b9e\u7528\u8003\u8651\u3002"}}
{"id": "2505.10443", "pdf": "https://arxiv.org/pdf/2505.10443", "abs": "https://arxiv.org/abs/2505.10443", "authors": ["Pedro Orvalho", "Marta Kwiatkowska"], "title": "Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?", "categories": ["cs.SE", "cs.AI"], "comment": "10 pages, 5 tables, 1 figure", "summary": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684LLMs\u662f\u5426\u80fd\u591f\u63a8\u7406Python\u7a0b\u5e8f\u6216\u53ea\u662f\u731c\u6d4b\u3002\u901a\u8fc7\u5e94\u7528\u4e94\u79cd\u4fdd\u6301\u8bed\u4e49\u7684\u4ee3\u7801\u53d8\u5f02\uff0c\u6211\u4eec\u53d1\u73b0\u4e00\u4e9bLLMs\u5728\u9ad8\u8fbe61%\u7684\u60c5\u51b5\u4e0b\u57fa\u4e8e\u9519\u8bef\u7684\u63a8\u7406\u4ea7\u751f\u6b63\u786e\u7684\u9884\u6d4b\uff0c\u4e14\u5b83\u4eec\u5728\u9762\u5bf9\u4ee3\u7801\u53d8\u5f02\u65f6\u7684\u9884\u6d4b\u53d8\u5316\u8868\u660e\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u548c\u9c81\u68d2\u6027\u5bf9\u4e8e\u5176\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u4f7f\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u800c\u672a\u8bc4\u4f30\u5176\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u89c2\u5bdf\u5230LLMs\u53ef\u4ee5\u901a\u8fc7\u6709\u7f3a\u9677\u7684\u903b\u8f91\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u4ee3\u7801\u7406\u89e3\u4e2d\u7c7b\u4f3c\u95ee\u9898\u7684\u62c5\u5fe7\u3002", "method": "\u6211\u4eec\u5e94\u7528\u4e86\u4e94\u79cd\u4fdd\u6301\u8bed\u4e49\u7684\u4ee3\u7801\u53d8\u5f02\uff0c\u5305\u62ec\u91cd\u547d\u540d\u53d8\u91cf\u3001\u955c\u50cf\u6bd4\u8f83\u8868\u8fbe\u5f0f\u3001\u4ea4\u6362if-else\u5206\u652f\u3001\u5c06for\u5faa\u73af\u8f6c\u6362\u4e3awhile\u5faa\u73af\u4ee5\u53ca\u5faa\u73af\u5c55\u5f00\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u516d\u79cdLLMs\uff0c\u5e76\u4f7f\u7528LiveCodeBench\u8fdb\u884c\u4e86\u4eba\u5de5\u4e13\u5bb6\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u6b63\u786e\u9884\u6d4b\u662f\u5426\u57fa\u4e8e\u5408\u7406\u7684\u63a8\u7406\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u4e9bLLMs\uff0c\u5982Llama3.2\uff0c\u5728\u591a\u8fbe61%\u7684\u60c5\u51b5\u4e0b\u57fa\u4e8e\u9519\u8bef\u7684\u63a8\u7406\u4ea7\u751f\u6b63\u786e\u7684\u9884\u6d4b\u3002\u6b64\u5916\uff0cLLMs\u5728\u9762\u5bf9\u4ee3\u7801\u53d8\u5f02\u65f6\u7ecf\u5e38\u6539\u53d8\u9884\u6d4b\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6709\u9650\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e00\u4e9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8fbe61%\u7684\u60c5\u51b5\u4e0b\u57fa\u4e8e\u9519\u8bef\u7684\u63a8\u7406\u4ea7\u751f\u6b63\u786e\u7684\u9884\u6d4b\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6709\u9650\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.10080", "pdf": "https://arxiv.org/pdf/2505.10080", "abs": "https://arxiv.org/abs/2505.10080", "authors": ["Weijie Xiong", "Zo\u00eb Holmes", "Armando Angrisani", "Yudai Suzuki", "Thiparat Chotibut", "Supanut Thanasilp"], "title": "Role of scrambling and noise in temporal information processing with quantum systems", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "comment": "14+35 pages, 6+5 figures, 1 table", "summary": "Scrambling quantum systems have been demonstrated as effective substrates for\ntemporal information processing. While their role in providing rich feature\nmaps has been widely studied, a theoretical understanding of their performance\nin temporal tasks is still lacking. Here we consider a general quantum\nreservoir processing framework that captures a broad range of physical\ncomputing models with quantum systems. We examine the scalability and memory\nretention of the model with scrambling reservoirs modelled by high-order\nunitary designs in both noiseless and noisy settings. In the former regime, we\nshow that measurement readouts become exponentially concentrated with\nincreasing reservoir size, yet strikingly do not worsen with the reservoir\niterations. Thus, while repeatedly reusing a small scrambling reservoir with\nquantum data might be viable, scaling up the problem size deteriorates\ngeneralization unless one can afford an exponential shot overhead. In contrast,\nthe memory of early inputs and initial states decays exponentially in both\nreservoir size and reservoir iterations. In the noisy regime, we also prove\nexponential memory decays with iterations for local noisy channels. Proving\nthese results required us to introduce new proof techniques for bounding\nconcentration in temporal quantum learning models.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u91cf\u5b50\u50a8\u5c42\u5904\u7406\u6846\u67b6\u5728\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u5728\u65e0\u566a\u58f0\u73af\u5883\u4e0b\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5728\u6269\u5927\u95ee\u9898\u89c4\u6a21\u65f6\u9700\u8981\u4ed8\u51fa\u6307\u6570\u7ea7\u7684\u91c7\u6837\u6210\u672c\u3002\u6b64\u5916\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u65e9\u671f\u8f93\u5165\u548c\u521d\u59cb\u72b6\u6001\u7684\u8bb0\u5fc6\u4f1a\u8fc5\u901f\u8870\u51cf\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5b50\u6df7\u6d17\u7cfb\u7edf\u5728\u63d0\u4f9b\u4e30\u5bcc\u7684\u7279\u5f81\u6620\u5c04\u65b9\u9762\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5bf9\u5176\u5728\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u7f3a\u4e4f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5206\u6790\u91cf\u5b50\u50a8\u5c42\u5904\u7406\u6846\u67b6\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53ef\u6269\u5c55\u6027\u548c\u8bb0\u5fc6\u4fdd\u6301\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u91cf\u5b50\u50a8\u5c42\u5904\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u7269\u7406\u8ba1\u7b97\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u9ad8\u9636\u9149\u8bbe\u8ba1\u5bf9\u6df7\u6d17\u50a8\u5c42\u8fdb\u884c\u4e86\u5efa\u6a21\u3002\u7814\u7a76\u4e86\u5728\u65e0\u566a\u58f0\u548c\u6709\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u8bb0\u5fc6\u4fdd\u6301\u80fd\u529b\u3002", "result": "\u5728\u65e0\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u6d4b\u91cf\u8bfb\u6570\u968f\u7740\u50a8\u5c42\u5927\u5c0f\u7684\u589e\u52a0\u800c\u5448\u6307\u6570\u96c6\u4e2d\uff0c\u4f46\u4e0d\u4f1a\u56e0\u50a8\u5c42\u8fed\u4ee3\u800c\u6076\u5316\u3002\u7136\u800c\uff0c\u6269\u5927\u95ee\u9898\u89c4\u6a21\u4f1a\u635f\u5bb3\u6cdb\u5316\u80fd\u529b\uff0c\u9664\u975e\u80fd\u591f\u627f\u62c5\u6307\u6570\u7ea7\u7684\u91c7\u6837\u5f00\u9500\u3002\u5728\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u65e9\u671f\u8f93\u5165\u548c\u521d\u59cb\u72b6\u6001\u7684\u8bb0\u5fc6\u4f1a\u968f\u7740\u50a8\u5c42\u5927\u5c0f\u548c\u8fed\u4ee3\u6b21\u6570\u5448\u6307\u6570\u8870\u51cf\u3002", "conclusion": "\u672c\u6587\u7ed3\u8bba\u662f\uff0c\u867d\u7136\u91cd\u590d\u4f7f\u7528\u5c0f\u89c4\u6a21\u7684\u91cf\u5b50\u6df7\u6d17\u50a8\u5c42\u5728\u5904\u7406\u91cf\u5b50\u6570\u636e\u65f6\u53ef\u80fd\u662f\u53ef\u884c\u7684\uff0c\u4f46\u6269\u5927\u95ee\u9898\u89c4\u6a21\u4f1a\u635f\u5bb3\u6cdb\u5316\u80fd\u529b\uff0c\u9664\u975e\u80fd\u591f\u627f\u62c5\u6307\u6570\u7ea7\u7684\u91c7\u6837\u5f00\u9500\u3002\u6b64\u5916\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u65e9\u671f\u8f93\u5165\u548c\u521d\u59cb\u72b6\u6001\u7684\u8bb0\u5fc6\u4f1a\u968f\u7740\u50a8\u5c42\u5927\u5c0f\u548c\u8fed\u4ee3\u6b21\u6570\u5448\u6307\u6570\u8870\u51cf\u3002"}}
{"id": "2505.10099", "pdf": "https://arxiv.org/pdf/2505.10099", "abs": "https://arxiv.org/abs/2505.10099", "authors": ["Sarat Moka", "Matias Quiroz", "Vali Asimit", "Samuel Muller"], "title": "A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection", "categories": ["stat.ML", "cs.LG", "math.OC", "q-fin.PM"], "comment": null, "summary": "Portfolio optimization involves selecting asset weights to minimize a\nrisk-reward objective, such as the portfolio variance in the classical\nminimum-variance framework. Sparse portfolio selection extends this by imposing\na cardinality constraint: only $k$ assets from a universe of $p$ may be\nincluded. The standard approach models this problem as a mixed-integer\nquadratic program and relies on commercial solvers to find the optimal\nsolution. However, the computational costs of such methods increase\nexponentially with $k$ and $p$, making them too slow for problems of even\nmoderate size. We propose a fast and scalable gradient-based approach that\ntransforms the combinatorial sparse selection problem into a constrained\ncontinuous optimization task via Boolean relaxation, while preserving\nequivalence with the original problem on the set of binary points. Our\nalgorithm employs a tunable parameter that transmutes the auxiliary objective\nfrom a convex to a concave function. This allows a stable convex starting\npoint, followed by a controlled path toward a sparse binary solution as the\ntuning parameter increases and the objective moves toward concavity. In\npractice, our method matches commercial solvers in asset selection for most\ninstances and, in rare instances, the solution differs by a few assets whilst\nshowing a negligible error in portfolio variance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a00\u758f\u6295\u8d44\u7ec4\u5408\u9009\u62e9\uff0c\u901a\u8fc7\u5e03\u5c14\u677e\u5f1b\u5c06\u7ec4\u5408\u7a00\u758f\u9009\u62e9\u95ee\u9898\u8f6c\u5316\u4e3a\u53d7\u7ea6\u675f\u7684\u8fde\u7eed\u4f18\u5316\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u95ee\u9898\u7684\u7b49\u4ef7\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u4e0a\u968f\u7740k\u548cp\u7684\u589e\u52a0\u800c\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4f7f\u5f97\u5b83\u4eec\u5bf9\u4e8e\u4e2d\u7b49\u89c4\u6a21\u7684\u95ee\u9898\u6765\u8bf4\u592a\u6162\u4e86\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e03\u5c14\u677e\u5f1b\u5c06\u7ec4\u5408\u7a00\u758f\u9009\u62e9\u95ee\u9898\u8f6c\u5316\u4e3a\u53d7\u7ea6\u675f\u7684\u8fde\u7eed\u4f18\u5316\u4efb\u52a1\uff0c\u540c\u65f6\u5728\u4e8c\u8fdb\u5236\u70b9\u96c6\u4e0a\u4fdd\u6301\u4e0e\u539f\u95ee\u9898\u7684\u7b49\u4ef7\u6027\u3002\u8be5\u7b97\u6cd5\u91c7\u7528\u4e86\u4e00\u4e2a\u53ef\u8c03\u53c2\u6570\uff0c\u5c06\u8f85\u52a9\u76ee\u6807\u4ece\u51f8\u51fd\u6570\u8f6c\u6362\u4e3a\u51f9\u51fd\u6570\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4e0e\u5546\u4e1a\u6c42\u89e3\u5668\u5728\u8d44\u4ea7\u9009\u62e9\u4e0a\u76f8\u5339\u914d\uff0c\u5e76\u4e14\u5728\u6781\u5c11\u6570\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u65b9\u6848\u4ec5\u76f8\u5dee\u51e0\u4e2a\u8d44\u4ea7\uff0c\u540c\u65f6\u663e\u793a\u51fa\u53ef\u5ffd\u7565\u7684\u7ec4\u5408\u65b9\u5dee\u8bef\u5dee\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4e0e\u5546\u4e1a\u6c42\u89e3\u5668\u5728\u8d44\u4ea7\u9009\u62e9\u4e0a\u76f8\u5339\u914d\uff0c\u5e76\u4e14\u5728\u6781\u5c11\u6570\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u65b9\u6848\u4ec5\u76f8\u5dee\u51e0\u4e2a\u8d44\u4ea7\uff0c\u540c\u65f6\u663e\u793a\u51fa\u53ef\u5ffd\u7565\u7684\u7ec4\u5408\u65b9\u5dee\u8bef\u5dee\u3002"}}
{"id": "2505.10139", "pdf": "https://arxiv.org/pdf/2505.10139", "abs": "https://arxiv.org/abs/2505.10139", "authors": ["Lorenz Vaitl", "Leon Klein"], "title": "Path Gradients after Flow Matching", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Boltzmann Generators have emerged as a promising machine learning tool for\ngenerating samples from equilibrium distributions of molecular systems using\nNormalizing Flows and importance weighting. Recently, Flow Matching has helped\nspeed up Continuous Normalizing Flows (CNFs), scale them to more complex\nmolecular systems, and minimize the length of the flow integration\ntrajectories. We investigate the benefits of using path gradients to fine-tune\nCNFs initially trained by Flow Matching, in the setting where a target energy\nis known. Our experiments show that this hybrid approach yields up to a\nthreefold increase in sampling efficiency for molecular systems, all while\nusing the same model, a similar computational budget and without the need for\nadditional sampling. Furthermore, by measuring the length of the flow\ntrajectories during fine-tuning, we show that path gradients largely preserve\nthe learned structure of the flow.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u8def\u5f84\u68af\u5ea6\u5bf9\u901a\u8fc7\u6d41\u5339\u914d\u521d\u59cb\u8bad\u7ec3\u7684\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\u8fdb\u884c\u5fae\u8c03\u7684\u597d\u5904\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u5206\u5b50\u7cfb\u7edf\u4e2d\u5c06\u91c7\u6837\u6548\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\uff0c\u540c\u65f6\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u578b\u3001\u76f8\u4f3c\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u91c7\u6837\u3002", "motivation": "Boltzmann\u751f\u6210\u5668\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u7528\u4e8e\u751f\u6210\u5206\u5b50\u7cfb\u7edf\u7684\u5e73\u8861\u5206\u5e03\u6837\u672c\u3002\u6700\u8fd1\uff0c\u6d41\u5339\u914d\u5e2e\u52a9\u52a0\u5feb\u4e86\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\uff0c\u5c06\u5176\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u5206\u5b50\u7cfb\u7edf\uff0c\u5e76\u51cf\u5c11\u4e86\u6d41\u79ef\u5206\u8f68\u8ff9\u7684\u957f\u5ea6\u3002", "method": "\u6211\u4eec\u7814\u7a76\u4e86\u5728\u5df2\u77e5\u76ee\u6807\u80fd\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u8def\u5f84\u68af\u5ea6\u5bf9\u901a\u8fc7\u6d41\u5339\u914d\u521d\u59cb\u8bad\u7ec3\u7684CNFs\u8fdb\u884c\u5fae\u8c03\u7684\u597d\u5904\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u5206\u5b50\u7cfb\u7edf\u4e2d\u5c06\u91c7\u6837\u6548\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\uff0c\u540c\u65f6\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u578b\u3001\u76f8\u4f3c\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u91c7\u6837\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6d4b\u91cf\u6d41\u8f68\u8ff9\u7684\u957f\u5ea6\uff0c\u6211\u4eec\u8bc1\u660e\u8def\u5f84\u68af\u5ea6\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4fdd\u7559\u4e86\u6d41\u7684\u5b66\u4e60\u7ed3\u6784\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u5206\u5b50\u7cfb\u7edf\u4e2d\u5c06\u91c7\u6837\u6548\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\uff0c\u540c\u65f6\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u578b\u3001\u76f8\u4f3c\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u91c7\u6837\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6d4b\u91cf\u6d41\u8f68\u8ff9\u7684\u957f\u5ea6\uff0c\u6211\u4eec\u8bc1\u660e\u8def\u5f84\u68af\u5ea6\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4fdd\u7559\u4e86\u6d41\u7684\u5b66\u4e60\u7ed3\u6784\u3002"}}
{"id": "2505.10160", "pdf": "https://arxiv.org/pdf/2505.10160", "abs": "https://arxiv.org/abs/2505.10160", "authors": ["Yannis Montreuil", "Axel Carlier", "Lai Xing Ng", "Wei Tsang Ooi"], "title": "One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which\nunifies prediction and deferral by learning a shared score-based model that\nselects the $k$ most cost-effective entities-labels or experts-per input. While\nexisting one-stage L2D methods are limited to deferring to a single expert, our\napproach jointly optimizes prediction and deferral across multiple entities\nthrough a single end-to-end objective. We define a cost-sensitive loss and\nderive a novel convex surrogate that is independent of the cardinality\nparameter $k$, enabling generalization across Top-$k$ regimes without\nretraining. Our formulation recovers the Top-1 deferral policy of prior\nscore-based methods as a special case, and we prove that our surrogate is both\nBayes-consistent and $\\mathcal{H}$-consistent under mild assumptions. We\nfurther introduce an adaptive variant, Top-$k(x)$, which dynamically selects\nthe number of consulted entities per input to balance predictive accuracy and\nconsultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage\nTop-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves\nsuperior accuracy-cost trade-offs by tailoring allocations to input complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u9636\u6bb5Top-k Learning-to-Defer\u6846\u67b6\uff0c\u80fd\u591f\u7edf\u4e00\u9884\u6d4b\u548cdefer\uff0c\u5e76\u901a\u8fc7\u6210\u672c\u654f\u611f\u7684\u635f\u5931\u548c\u51f8\u9762\u4ee3\u7406\u5b9e\u73b0\u8de8Top-k\u5236\u5ea6\u7684\u6cdb\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u53d8\u4f53Top-k(x)\uff0c\u4ee5\u66f4\u597d\u5730\u5e73\u8861\u51c6\u786e\u7387\u548c\u6210\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u9636\u6bb5L2D\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5411\u5355\u4e00\u4e13\u5bb6defer\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5355\u4e00\u7aef\u5230\u7aef\u76ee\u6807\u8054\u5408\u4f18\u5316\u591a\u4e2a\u5b9e\u4f53\u7684\u9884\u6d4b\u548cdefer\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u5355\u9636\u6bb5Top-k Learning-to-Defer\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u57fa\u4e8e\u5f97\u5206\u7684\u6a21\u578b\u6765\u7edf\u4e00\u9884\u6d4b\u548cdefer\uff0c\u8be5\u6a21\u578b\u4e3a\u6bcf\u4e2a\u8f93\u5165\u9009\u62e9k\u4e2a\u6700\u7ecf\u6d4e\u6709\u6548\u7684\u5b9e\u4f53-\u6807\u7b7e\u6216\u4e13\u5bb6\u3002\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6210\u672c\u654f\u611f\u7684\u635f\u5931\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u51f8\u9762\u4ee3\u7406\uff0c\u72ec\u7acb\u4e8e\u57fa\u6570\u53c2\u6570k\uff0c\u4f7f\u6cdb\u5316\u8de8Top-k\u5236\u5ea6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u516c\u5f0f\u6062\u590d\u4e86\u5148\u524d\u57fa\u4e8e\u5f97\u5206\u7684\u65b9\u6cd5\u7684Top-1 defer\u7b56\u7565\u4f5c\u4e3a\u7279\u4f8b\uff0c\u5e76\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u4ee3\u7406\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u662fBayes\u4e00\u81f4\u548cH\u4e00\u81f4\u7684\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u53d8\u4f53Top-k(x)\uff0c\u5b83\u52a8\u6001\u5730\u4e3a\u6bcf\u4e2a\u8f93\u5165\u9009\u62e9\u54a8\u8be2\u7684\u5b9e\u4f53\u6570\u91cf\u4ee5\u5e73\u8861\u9884\u6d4b\u51c6\u786e\u6027\u548c\u54a8\u8be2\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5355\u9636\u6bb5Top-k\u65b9\u6cd5\u5728CIFAR-10\u548cSVHN\u4e0a\u4e25\u683c\u4f18\u4e8eTop-1 defer\uff0c\u800cTop-k(x)\u901a\u8fc7\u6839\u636e\u8f93\u5165\u590d\u6742\u6027\u5b9a\u5236\u5206\u914d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728CIFAR-10\u548cSVHN\u6570\u636e\u96c6\u4e0a\u4e25\u683c\u4f18\u4e8eTop-1 deferral\uff0c\u800cTop-k(x)\u901a\u8fc7\u6839\u636e\u8f93\u5165\u590d\u6742\u6027\u5b9a\u5236\u5206\u914d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\u3002"}}
{"id": "2505.10522", "pdf": "https://arxiv.org/pdf/2505.10522", "abs": "https://arxiv.org/abs/2505.10522", "authors": ["Xinrui Wang", "Yan Jin"], "title": "Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has demonstrated remarkable potential in robotic\nmanipulation but faces challenges in sample inefficiency and lack of\ninterpretability, limiting its applicability in real world scenarios. Enabling\nthe agent to gain a deeper understanding and adapt more efficiently to diverse\nworking scenarios is crucial, and strategic knowledge utilization is a key\nfactor in this process. This paper proposes a Knowledge Capture, Adaptation,\nand Composition (KCAC) framework to systematically integrate knowledge transfer\ninto RL through cross-task curriculum learning. KCAC is evaluated using a two\nblock stacking task in the CausalWorld benchmark, a complex robotic\nmanipulation environment. To our knowledge, existing RL approaches fail to\nsolve this task effectively, reflecting deficiencies in knowledge capture. In\nthis work, we redesign the benchmark reward function by removing rigid\nconstraints and strict ordering, allowing the agent to maximize total rewards\nconcurrently and enabling flexible task completion. Furthermore, we define two\nself-designed sub-tasks and implement a structured cross-task curriculum to\nfacilitate efficient learning. As a result, our KCAC approach achieves a 40\npercent reduction in training time while improving task success rates by 10\npercent compared to traditional RL methods. Through extensive evaluation, we\nidentify key curriculum design parameters subtask selection, transition timing,\nand learning rate that optimize learning efficiency and provide conceptual\nguidance for curriculum based RL frameworks. This work offers valuable insights\ninto curriculum design in RL and robotic learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6KCAC\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u8bfe\u7a0b\u5b66\u4e60\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u6837\u672c\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u4f7f\u667a\u80fd\u4f53\u83b7\u5f97\u66f4\u6df1\u5165\u7684\u7406\u89e3\u5e76\u66f4\u9ad8\u6548\u5730\u9002\u5e94\u5404\u79cd\u5de5\u4f5c\u573a\u666f\u81f3\u5173\u91cd\u8981\uff0c\u800c\u6218\u7565\u77e5\u8bc6\u5229\u7528\u662f\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u6355\u83b7\u3001\u9002\u5e94\u548c\u7ec4\u5408\uff08KCAC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u8bfe\u7a0b\u5b66\u4e60\u5c06\u77e5\u8bc6\u8fc1\u79fb\u7cfb\u7edf\u5730\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u3002\u540c\u65f6\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u57fa\u51c6\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e24\u4e2a\u81ea\u5b9a\u4e49\u5b50\u4efb\u52a1\uff0c\u4ee5\u5b9e\u73b0\u7ed3\u6784\u5316\u7684\u8de8\u4efb\u52a1\u8bfe\u7a0b\u5b66\u4e60\u3002", "result": "KCAC\u65b9\u6cd5\u5728\u4e24\u5757\u5806\u53e0\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8640%\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u548c10%\u7684\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u8bc6\u522b\u4e86\u4f18\u5316\u5b66\u4e60\u6548\u7387\u7684\u5173\u952e\u8bfe\u7a0b\u8bbe\u8ba1\u53c2\u6570\uff0c\u5982\u5b50\u4efb\u52a1\u9009\u62e9\u3001\u8fc7\u6e21\u65f6\u673a\u548c\u5b66\u4e60\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u6355\u83b7\u3001\u9002\u5e94\u548c\u7ec4\u5408\uff08KCAC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u8bfe\u7a0b\u5b66\u4e60\u5c06\u77e5\u8bc6\u8fc1\u79fb\u7cfb\u7edf\u5730\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edfRL\u65b9\u6cd5\u76f8\u6bd4\uff0cKCAC\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u4e0a\u51cf\u5c11\u4e8640%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8610%\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u8bc6\u522b\u4e86\u4f18\u5316\u5b66\u4e60\u6548\u7387\u7684\u5173\u952e\u8bfe\u7a0b\u8bbe\u8ba1\u53c2\u6570\uff0c\u4e3a\u57fa\u4e8e\u8bfe\u7a0b\u7684RL\u6846\u67b6\u63d0\u4f9b\u4e86\u6982\u5ff5\u6027\u6307\u5bfc\u3002"}}
{"id": "2505.10537", "pdf": "https://arxiv.org/pdf/2505.10537", "abs": "https://arxiv.org/abs/2505.10537", "authors": ["Filippo Olimpieri", "Noemi Giustini", "Andrea Lacava", "Salvatore D'Oro", "Tommaso Melodia", "Francesca Cuomo"], "title": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps", "categories": ["cs.NI", "cs.AI"], "comment": "6 pages, 5 figures, 2 tables", "summary": "The O-RAN architecture is transforming cellular networks by adopting RAN\nsoftwarization and disaggregation concepts to enable data-driven monitoring and\ncontrol of the network. Such management is enabled by RICs, which facilitate\nnear-real-time and non-real-time network control through xApps and rApps.\nHowever, they face limitations, including latency overhead in data exchange\nbetween the RAN and RIC, restricting real-time monitoring, and the inability to\naccess user plain data due to privacy and security constraints, hindering use\ncases like beamforming and spectrum classification. In this paper, we leverage\nthe dApps concept to enable real-time RF spectrum classification with LibIQ, a\nnovel library for RF signals that facilitates efficient spectrum monitoring and\nsignal classification by providing functionalities to read I/Q samples as\ntime-series, create datasets and visualize time-series data through plots and\nspectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to\ndetect external RF signals, which are subsequently classified using a CNN\ninside the library. To achieve accurate spectrum analysis, we created an\nextensive dataset of time-series-based I/Q samples, representing distinct\nsignal types captured using a custom dApp running on a 5G deployment over the\nColosseum network emulator and an OTA testbed. We evaluate our model by\ndeploying LibIQ in heterogeneous scenarios with varying center frequencies,\ntime windows, and external RF signals. In real-time analysis, the model\nclassifies the processed I/Q samples, achieving an average accuracy of\napproximately 97.8\\% in identifying signal types across all scenarios. We\npledge to release both LibIQ and the dataset created as a publicly available\nframework upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLibIQ\u5e93\u548cCNN\u6a21\u578b\u7684\u5b9e\u65f6RF\u9891\u8c31\u5206\u7c7b\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u76f8\u5173\u5de5\u5177\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684RAN\u67b6\u6784\u5728\u6570\u636e\u4ea4\u6362\u5ef6\u8fdf\u548c\u7528\u6237\u539f\u59cb\u6570\u636e\u8bbf\u95ee\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u963b\u788d\u4e86\u5982\u6ce2\u675f\u6210\u5f62\u548c\u9891\u8c31\u5206\u7c7b\u7b49\u7528\u4f8b\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u5229\u7528dApps\u6982\u5ff5\uff0c\u901a\u8fc7LibIQ\u5e93\u5bf9I/Q\u6837\u672c\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u3001\u6570\u636e\u96c6\u521b\u5efa\u548c\u53ef\u89c6\u5316\u5206\u6790\uff0c\u5e76\u4f7f\u7528CNN\u8fdb\u884c\u4fe1\u53f7\u5206\u7c7b\u3002", "result": "\u901a\u8fc7LibIQ\u5e93\u548cCNN\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6RF\u9891\u8c31\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe\u5230\u7ea697.8%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86LibIQ\u5e93\u548c\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6RF\u9891\u8c31\u5206\u7c7b\u7684CNN\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7ea697.8%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u53d1\u5e03LibIQ\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2505.10547", "pdf": "https://arxiv.org/pdf/2505.10547", "abs": "https://arxiv.org/abs/2505.10547", "authors": ["Milan Ganai", "Rohan Sinha", "Christopher Agia", "Daniel Morton", "Marco Pavone"], "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "Website: https://milanganai.github.io/fortress/", "summary": "Foundation models can provide robust high-level reasoning on appropriate\nsafety interventions in hazardous scenarios beyond a robot's training data,\ni.e. out-of-distribution (OOD) failures. However, due to the high inference\nlatency of Large Vision and Language Models, current methods rely on manually\ndefined intervention policies to enact fallbacks, thereby lacking the ability\nto plan generalizable, semantically safe motions. To overcome these challenges\nwe present FORTRESS, a framework that generates and reasons about semantically\nsafe fallback strategies in real time to prevent OOD failures. At a low\nfrequency in nominal operations, FORTRESS uses multi-modal reasoners to\nidentify goals and anticipate failure modes. When a runtime monitor triggers a\nfallback response, FORTRESS rapidly synthesizes plans to fallback goals while\ninferring and avoiding semantically unsafe regions in real time. By bridging\nopen-world, multi-modal reasoning with dynamics-aware planning, we eliminate\nthe need for hard-coded fallbacks and human safety interventions. FORTRESS\noutperforms on-the-fly prompting of slow reasoning models in safety\nclassification accuracy on synthetic benchmarks and real-world ANYmal robot\ndata, and further improves system safety and planning success in simulation and\non quadrotor hardware for urban navigation.", "AI": {"tldr": "FORTRESS is a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. It eliminates the need for hard-coded fallbacks and human safety interventions by bridging open-world, multi-modal reasoning with dynamics-aware planning.", "motivation": "Current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, i.e., out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods are limited.", "method": "FORTRESS is a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. It uses multi-modal reasoners to identify goals and anticipate failure modes at a low frequency in nominal operations. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time.", "result": "FORTRESS eliminates the need for hard-coded fallbacks and human safety interventions by bridging open-world, multi-modal reasoning with dynamics-aware planning. It outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation.", "conclusion": "FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation."}}
{"id": "2505.10279", "pdf": "https://arxiv.org/pdf/2505.10279", "abs": "https://arxiv.org/abs/2505.10279", "authors": ["Gabriel R. Palma", "Sally McClean", "Brahim Allan", "Zeeshan Tariq", "Rafael A. Moral"], "title": "Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging", "categories": ["stat.ME", "cs.LG"], "comment": "21 pages", "summary": "TV customers today face many choices from many live channels and on-demand\nservices. Providing a personalised experience that saves customers time when\ndiscovering content is essential for TV providers. However, a reliable\nunderstanding of their behaviour and preferences is key. When creating\npersonalised recommendations for TV, the biggest challenge is understanding\nviewing behaviour within households when multiple people are watching. The\nobjective is to detect and combine individual profiles to make\nbetter-personalised recommendations for group viewing. Our challenge is that we\nhave little explicit information about who is watching the devices at any time\n(individuals or groups). Also, we do not have a way to combine more than one\nindividual profile to make better recommendations for group viewing. We propose\na novel framework using a Gaussian mixture model averaging to obtain point\nestimates for the number of household TV profiles and a Bayesian random walk\nmodel to introduce uncertainty. We applied our approach using data from real\ncustomers whose TV-watching data totalled approximately half a million\nobservations. Our results indicate that combining our framework with the\nselected features provides a means to estimate the number of household TV\nprofiles and their characteristics, including shifts over time and\nquantification of uncertainty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5e73\u5747\u548c\u8d1d\u53f6\u65af\u968f\u673a\u6e38\u8d70\u6a21\u578b\u6765\u4f30\u8ba1\u5bb6\u5ead\u7535\u89c6\u914d\u7f6e\u6587\u4ef6\u7684\u6570\u91cf\u53ca\u5176\u7279\u6027\u3002", "motivation": "TV\u63d0\u4f9b\u5546\u9700\u8981\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u4f53\u9a8c\uff0c\u4ee5\u8282\u7701\u5ba2\u6237\u5728\u53d1\u73b0\u5185\u5bb9\u65f6\u7684\u65f6\u95f4\u3002\u7136\u800c\uff0c\u53ef\u9760\u5730\u4e86\u89e3\u4ed6\u4eec\u7684\u884c\u4e3a\u548c\u504f\u597d\u662f\u5173\u952e\u3002\u5f53\u4e3a\u7535\u89c6\u521b\u5efa\u4e2a\u6027\u5316\u63a8\u8350\u65f6\uff0c\u6700\u5927\u7684\u6311\u6218\u662f\u7406\u89e3\u5bb6\u5ead\u4e2d\u591a\u4e2a\u4eba\u89c2\u770b\u65f6\u7684\u89c2\u770b\u884c\u4e3a\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5e73\u5747\u6765\u83b7\u5f97\u5bb6\u5ead\u7535\u89c6\u914d\u7f6e\u6587\u4ef6\u6570\u91cf\u7684\u70b9\u4f30\u8ba1\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u968f\u673a\u6e38\u8d70\u6a21\u578b\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u6211\u4eec\u7684\u6846\u67b6\u4e0e\u9009\u5b9a\u7684\u7279\u5f81\u7ed3\u5408\u53ef\u4ee5\u4f30\u8ba1\u5bb6\u5ead\u7535\u89c6\u914d\u7f6e\u6587\u4ef6\u7684\u6570\u91cf\u53ca\u5176\u7279\u6027\uff0c\u5305\u62ec\u968f\u65f6\u95f4\u7684\u53d8\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u6211\u4eec\u7684\u6846\u67b6\u4e0e\u9009\u5b9a\u7684\u7279\u5f81\u7ed3\u5408\u53ef\u4ee5\u4f30\u8ba1\u5bb6\u5ead\u7535\u89c6\u914d\u7f6e\u6587\u4ef6\u7684\u6570\u91cf\u53ca\u5176\u7279\u6027\uff0c\u5305\u62ec\u968f\u65f6\u95f4\u7684\u53d8\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u3002"}}
{"id": "2505.10319", "pdf": "https://arxiv.org/pdf/2505.10319", "abs": "https://arxiv.org/abs/2505.10319", "authors": ["John Nicol", "Markus Frohme"], "title": "Deconstructing Subset Construction -- Reducing While Determinizing", "categories": ["cs.FL", "cs.LG"], "comment": "19 pages, 2 figures", "summary": "We present a novel perspective on the NFA canonization problem, which\nintroduces intermediate minimization steps to reduce the exploration space\non-the-fly. Essential to our approach are so-called equivalence registries\nwhich manage information about equivalent states and allow for incorporating\nfurther optimization techniques such as convexity closures or simulation to\nboost performance. Due to the generality of our approach, these concepts can be\nembedded in classic subset construction or Brzozowski's approach. We evaluate\nour approach on a set of real-world examples from automatic sequences and\nobserve that we are able to improve especially worst-case scenarios. We\nimplement our approach in an open-source library for users to experiment with.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684NFA\u89c4\u8303\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e2d\u95f4\u6700\u5c0f\u5316\u6b65\u9aa4\u548c\u7b49\u4ef7\u6ce8\u518c\u8868\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684NFA\u89c4\u8303\u5316\u65b9\u6cd5\u5728\u5904\u7406\u67d0\u4e9b\u60c5\u51b5\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u6027\u80fd\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e2d\u95f4\u6700\u5c0f\u5316\u6b65\u9aa4\u548c\u7b49\u4ef7\u6ce8\u518c\u8868\uff0c\u4ee5\u51cf\u5c11\u63a2\u7d22\u7a7a\u95f4\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u6211\u4eec\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u6d4b\u8bd5\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u89c2\u5bdf\u5230\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.10367", "pdf": "https://arxiv.org/pdf/2505.10367", "abs": "https://arxiv.org/abs/2505.10367", "authors": ["Chuanqing Pu", "Feilong Fan", "Nengling Tai", "Songyuan Liu", "Jinming Yu"], "title": "A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy Trading in HEFTCom2024", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "Solution description of IEEE Hybrid Energy Forecasting and Trading\n  Competition (HEFTCom)", "summary": "Obtaining accurate probabilistic energy forecasts and making effective\ndecisions amid diverse uncertainties are routine challenges in future energy\nsystems. This paper presents the solution of team GEB, which ranked 3rd in\ntrading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid\nEnergy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution\nprovides accurate probabilistic forecasts for a wind-solar hybrid system, and\nachieves substantial trading revenue in the day-ahead electricity market. Key\ncomponents include: (1) a stacking-based approach combining sister forecasts\nfrom various Numerical Weather Predictions (NWPs) to provide wind power\nforecasts, (2) an online solar post-processing model to address the\ndistribution shift in the online test set caused by increased solar capacity,\n(3) a probabilistic aggregation method for accurate quantile forecasts of\nhybrid generation, and (4) a stochastic trading strategy to maximize expected\ntrading revenue considering uncertainties in electricity prices. This paper\nalso explores the potential of end-to-end learning to further enhance the\ntrading revenue by adjusting the distribution of forecast errors. Detailed case\nstudies are provided to validate the effectiveness of these proposed methods.\nCode for all mentioned methods is available for reproduction and further\nresearch in both industry and academia.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u56e2\u961fGEB\u5728IEEE\u6df7\u5408\u80fd\u6e90\u9884\u6d4b\u548c\u4ea4\u6613\u7ade\u8d5b2024\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u63d0\u9ad8\u4e86\u98ce-\u5149\u6df7\u5408\u7cfb\u7edf\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4ea4\u6613\u6536\u76ca\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5728\u672a\u6765\u7684\u80fd\u6e90\u7cfb\u7edf\u4e2d\uff0c\u83b7\u53d6\u51c6\u786e\u7684\u6982\u7387\u80fd\u6e90\u9884\u6d4b\u5e76\u5e94\u5bf9\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u662f\u5e38\u89c4\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u98ce-\u5149\u6df7\u5408\u7cfb\u7edf\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4ea4\u6613\u6536\u76ca\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u5806\u53e0\u7684\u98ce\u529b\u53d1\u7535\u9884\u6d4b\u65b9\u6cd5\u3001\u5728\u7ebf\u592a\u9633\u80fd\u540e\u5904\u7406\u6a21\u578b\u3001\u6982\u7387\u805a\u5408\u65b9\u6cd5\u4ee5\u53ca\u968f\u673a\u4ea4\u6613\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u7aef\u5230\u7aef\u5b66\u4e60\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4ea4\u6613\u6536\u76ca\u3002", "result": "\u56e2\u961fGEB\u5728HEFTCom2024\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c3\u540d\u3001\u7b2c4\u540d\u548c\u5b66\u751f\u56e2\u961f\u7b2c1\u540d\u7684\u597d\u6210\u7ee9\u3002\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5728\u98ce-\u5149\u6df7\u5408\u7cfb\u7edf\u7684\u6982\u7387\u9884\u6d4b\u548c\u4ea4\u6613\u6536\u76ca\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u56e2\u961fGEB\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5728IEEE\u6df7\u5408\u80fd\u6e90\u9884\u6d4b\u548c\u4ea4\u6613\u7ade\u8d5b2024\uff08HEFTCom2024\uff09\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u98ce-\u5149\u6df7\u5408\u7cfb\u7edf\u4e2d\u7684\u51c6\u786e\u6982\u7387\u9884\u6d4b\u548c\u663e\u8457\u4ea4\u6613\u6536\u76ca\u7684\u80fd\u529b\u3002"}}
{"id": "2505.10398", "pdf": "https://arxiv.org/pdf/2505.10398", "abs": "https://arxiv.org/abs/2505.10398", "authors": ["Alexandre Banks", "Randy Moore", "Sayem Nazmuz Zaman", "Alaa Eldin Abdelaal", "Septimiu E. Salcudean"], "title": "AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera in Surgical Robotics", "categories": ["cs.RO", "cs.HC", "cs.LG", "cs.SY", "eess.SP", "eess.SY", "J.3.2; J.2.7; I.2.9"], "comment": "13 pages, 9 figures", "summary": "Incorporating an autonomous auxiliary camera into robot-assisted minimally\ninvasive surgery (RAMIS) enhances spatial awareness and eliminates manual\nviewpoint control. Existing path planning methods for auxiliary cameras track\ntwo-dimensional surgical features but do not simultaneously account for camera\norientation, workspace constraints, and robot joint limits. This study presents\nAutoCam: an automatic auxiliary camera placement method to improve\nvisualization in RAMIS. Implemented on the da Vinci Research Kit, the system\nuses a priority-based, workspace-constrained control algorithm that combines\nheuristic geometric placement with nonlinear optimization to ensure robust\ncamera tracking. A user study (N=6) demonstrated that the system maintained\n99.84% visibility of a salient feature and achieved a pose error of 4.36 $\\pm$\n2.11 degrees and 1.95 $\\pm$ 5.66 mm. The controller was computationally\nefficient, with a loop time of 6.8 $\\pm$ 12.8 ms. An additional pilot study\n(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training\ntask, suggests that users can teleoperate just as effectively from AutoCam's\nviewpoint as from the endoscope's while still benefiting from AutoCam's\nimproved visual coverage of the scene. These results indicate that an auxiliary\ncamera can be autonomously controlled using the da Vinci patient-side\nmanipulators to track a salient feature, laying the groundwork for new\nmulti-camera visualization methods in RAMIS.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86AutoCam\uff0c\u4e00\u79cd\u81ea\u52a8\u8f85\u52a9\u6444\u50cf\u5934\u653e\u7f6e\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584RAMIS\u4e2d\u7684\u53ef\u89c6\u5316\u6548\u679c\u3002\u901a\u8fc7\u7ed3\u5408\u542f\u53d1\u5f0f\u51e0\u4f55\u5b9a\u4f4d\u548c\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8ddf\u8e2a\u663e\u8457\u7279\u5f81\uff0c\u5e76\u5728\u7528\u6237\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u8f85\u52a9\u6444\u50cf\u5934\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u53ea\u8ddf\u8e2a\u4e8c\u7ef4\u624b\u672f\u7279\u5f81\uff0c\u4f46\u6ca1\u6709\u540c\u65f6\u8003\u8651\u6444\u50cf\u5934\u65b9\u5411\u3001\u5de5\u4f5c\u7a7a\u95f4\u7ea6\u675f\u548c\u673a\u5668\u4eba\u5173\u8282\u9650\u5236\u3002", "method": "\u8be5\u7cfb\u7edf\u5728da Vinci Research Kit\u4e0a\u5b9e\u73b0\uff0c\u4f7f\u7528\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u3001\u53d7\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\u7684\u63a7\u5236\u7b97\u6cd5\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u51e0\u4f55\u5b9a\u4f4d\u548c\u975e\u7ebf\u6027\u4f18\u5316\u4ee5\u786e\u4fdd\u7a33\u5065\u7684\u6444\u50cf\u5934\u8ddf\u8e2a\u3002", "result": "\u7528\u6237\u7814\u7a76\uff08N=6\uff09\u663e\u793a\uff0c\u7cfb\u7edf\u4fdd\u6301\u4e8699.84%\u7684\u663e\u8457\u7279\u5f81\u53ef\u89c1\u6027\uff0c\u5e76\u5b9e\u73b0\u4e864.36\u00b12.11\u5ea6\u548c1.95\u00b15.66\u6beb\u7c73\u7684\u59ff\u6001\u8bef\u5dee\u3002\u63a7\u5236\u5668\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5faa\u73af\u65f6\u95f4\u4e3a6.8\u00b112.8\u6beb\u79d2\u3002\u4e00\u9879\u989d\u5916\u7684\u8bd5\u70b9\u7814\u7a76\uff08N=6\uff09\u8868\u660e\uff0c\u65b0\u624b\u53ef\u4ee5\u4eceAutoCam\u7684\u89c6\u89d2\u8fdb\u884c\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u6548\u679c\u4e0e\u4ece\u8179\u8154\u955c\u89c6\u89d2\u76f8\u5f53\uff0c\u540c\u65f6\u4ecd\u80fd\u53d7\u76ca\u4e8eAutoCam\u6539\u8fdb\u7684\u573a\u666f\u89c6\u89c9\u8986\u76d6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u4f7f\u7528da Vinci\u60a3\u8005\u4fa7\u673a\u68b0\u81c2\u81ea\u4e3b\u63a7\u5236\u8f85\u52a9\u6444\u50cf\u5934\u6765\u8ddf\u8e2a\u663e\u8457\u7279\u5f81\uff0c\u4e3aRAMIS\u4e2d\u7684\u65b0\u578b\u591a\u6444\u50cf\u5934\u53ef\u89c6\u5316\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.10444", "pdf": "https://arxiv.org/pdf/2505.10444", "abs": "https://arxiv.org/abs/2505.10444", "authors": ["Miguel Aguilera", "Sosuke Ito", "Artemy Kolchinsky"], "title": "Inferring entropy production in many-body systems using nonequilibrium MaxEnt", "categories": ["cond-mat.stat-mech", "cs.LG", "nlin.AO", "q-bio.NC"], "comment": null, "summary": "We propose a method for inferring entropy production (EP) in high-dimensional\nstochastic systems, including many-body systems and non-Markovian systems with\nlong memory. Standard techniques for estimating EP become intractable in such\nsystems due to computational and statistical limitations. We infer\ntrajectory-level EP and lower bounds on average EP by exploiting a\nnonequilibrium analogue of the Maximum Entropy principle, along with convex\nduality. Our approach uses only samples of trajectory observables (such as\nspatiotemporal correlation functions). It does not require reconstruction of\nhigh-dimensional probability distributions or rate matrices, nor any special\nassumptions such as discrete states or multipartite dynamics. It may be used to\ncompute a hierarchical decomposition of EP, reflecting contributions from\ndifferent kinds of interactions, and it has an intuitive physical\ninterpretation as a thermodynamic uncertainty relation. We demonstrate its\nnumerical performance on a disordered nonequilibrium spin model with 1000 spins\nand a large neural spike-train dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9ad8\u7ef4\u968f\u673a\u7cfb\u7edf\u4e2d\u63a8\u65ad\u71b5\u4ea7\u751f\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u91cd\u5efa\u9ad8\u7ef4\u6982\u7387\u5206\u5e03\u6216\u901f\u7387\u77e9\u9635\uff0c\u4e5f\u65e0\u9700\u7279\u6b8a\u5047\u8bbe\u3002", "motivation": "\u6807\u51c6\u7684\u71b5\u4ea7\u751f\u4f30\u8ba1\u6280\u672f\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u56e0\u4e3a\u5b58\u5728\u8ba1\u7b97\u548c\u7edf\u8ba1\u4e0a\u7684\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u5229\u7528\u975e\u5e73\u8861\u7cfb\u7edf\u7684\u6700\u5927\u71b5\u539f\u7406\u548c\u51f8\u5bf9\u5076\u6027\uff0c\u901a\u8fc7\u8f68\u8ff9\u53ef\u89c2\u6d4b\u91cf\uff08\u5982\u65f6\u7a7a\u76f8\u5173\u51fd\u6570\uff09\u6765\u63a8\u65ad\u8f68\u8ff9\u7ea7\u522b\u7684\u71b5\u4ea7\u751f\u548c\u5e73\u5747\u71b5\u4ea7\u751f\u7684\u4e0b\u9650\u3002", "result": "\u672c\u6587\u5728\u5177\u67091000\u4e2a\u81ea\u65cb\u7684\u65e0\u5e8f\u975e\u5e73\u8861\u81ea\u65cb\u6a21\u578b\u548c\u5927\u578b\u795e\u7ecf\u8109\u51b2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6570\u503c\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9ad8\u7ef4\u968f\u673a\u7cfb\u7edf\u4e2d\u63a8\u65ad\u71b5\u4ea7\u751f\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6570\u503c\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u71b5\u4ea7\u751f\u7684\u5c42\u6b21\u5206\u89e3\u3002"}}
{"id": "2505.10448", "pdf": "https://arxiv.org/pdf/2505.10448", "abs": "https://arxiv.org/abs/2505.10448", "authors": ["Conor Rosato", "Harvinder Lehal", "Simon Maskell", "Lee Devlin", "Malcolm Strens"], "title": "Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods", "categories": ["stat.ML", "cs.LG"], "comment": "45 pages", "summary": "Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when\nthe likelihood function is irregular and expensive to compute. We explore\nseveral sampling algorithms that make use of subset evaluations to reduce\ncomputational overhead. We adapt the subset samplers for this setting where\ngradient information is not available or is unreliable. To achieve this, we\nintroduce data-driven proxies in place of Taylor expansions and define a novel\ncomputation-cost aware adaptive controller. We undertake an extensive\nevaluation for a challenging disease modelling task and a configurable task\nwith similar irregularity in the likelihood surface. We find our improved\nversion of Hierarchical Importance with Nested Training Samples (HINTS), with\nadaptive proposals and a data-driven proxy, obtains the best sampling error in\na fixed computational budget. We conclude that subset evaluations can provide\ncheap and naturally-tempered exploration, while a data-driven proxy can\npre-screen proposals successfully in explored regions of the state space. These\ntwo elements combine through hierarchical delayed acceptance to achieve\nefficient, exact sampling.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4f3c\u7136\u51fd\u6570\u4e0d\u89c4\u5219\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u5b50\u96c6\u8bc4\u4f30\u548c\u6570\u636e\u9a71\u52a8\u7684\u4ee3\u7406\u6765\u4f18\u5316\u8d1d\u53f6\u65af\u63a8\u65ad\u4e2d\u7684\u91c7\u6837\u7b97\u6cd5\u3002", "motivation": "\u5f53\u4f3c\u7136\u51fd\u6570\u4e0d\u89c4\u5219\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u65f6\uff0c\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u65ad\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u51e0\u79cd\u5229\u7528\u5b50\u96c6\u8bc4\u4f30\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u91c7\u6837\u7b97\u6cd5\u3002", "method": "\u6211\u4eec\u9002\u5e94\u4e86\u5b50\u96c6\u91c7\u6837\u5668\uff0c\u4ee5\u5728\u6ca1\u6709\u68af\u5ea6\u4fe1\u606f\u6216\u68af\u5ea6\u4fe1\u606f\u4e0d\u53ef\u9760\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u91c7\u6837\u3002\u6211\u4eec\u5f15\u5165\u4e86\u6570\u636e\u9a71\u52a8\u7684\u4ee3\u7406\u4ee3\u66ff\u6cf0\u52d2\u5c55\u5f00\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u8ba1\u7b97\u6210\u672c\u610f\u8bc6\u81ea\u9002\u5e94\u63a7\u5236\u5668\u3002", "result": "\u6211\u4eec\u5728\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u75be\u75c5\u5efa\u6a21\u4efb\u52a1\u548c\u4e00\u4e2a\u5177\u6709\u7c7b\u4f3c\u4e0d\u89c4\u5219\u4f3c\u7136\u8868\u9762\u7684\u53ef\u914d\u7f6e\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u53d1\u73b0\u6539\u8fdb\u7684\u5206\u5c42\u91cd\u8981\u6027\u5d4c\u5957\u8bad\u7ec3\u6837\u672c\uff08HINTS\uff09\u7248\u672c\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u83b7\u5f97\u4e86\u6700\u4f73\u7684\u91c7\u6837\u8bef\u5dee\u3002", "conclusion": "\u5b50\u96c6\u8bc4\u4f30\u53ef\u4ee5\u63d0\u4f9b\u5ec9\u4ef7\u4e14\u81ea\u7136\u8c03\u8282\u7684\u63a2\u7d22\uff0c\u800c\u6570\u636e\u9a71\u52a8\u7684\u4ee3\u7406\u53ef\u4ee5\u5728\u72b6\u6001\u7a7a\u95f4\u7684\u63a2\u7d22\u533a\u57df\u6210\u529f\u5730\u9884\u7b5b\u9009\u63d0\u8bae\u3002\u8fd9\u4e24\u4e2a\u5143\u7d20\u901a\u8fc7\u5206\u5c42\u5ef6\u8fdf\u63a5\u53d7\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7cbe\u786e\u91c7\u6837\u3002"}}
{"id": "2505.10466", "pdf": "https://arxiv.org/pdf/2505.10466", "abs": "https://arxiv.org/abs/2505.10466", "authors": ["Juehang Qin", "Shixiao Liang", "Christopher Tunnell"], "title": "FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": "10 pages, 5 figures, and 2 tables in main text, two appendices", "summary": "Multi-modal and high-dimensional posteriors present significant challenges\nfor variational inference, causing mode-seeking behavior and collapse despite\nthe theoretical expressiveness of normalizing flows. Traditional annealing\nmethods require temperature schedules and hyperparameter tuning, falling short\nof the goal of truly black-box variational inference. We introduce FlowVAT, a\nconditional tempering approach for normalizing flow variational inference that\naddresses these limitations. Our method tempers both the base and target\ndistributions simultaneously, maintaining affine-invariance under tempering. By\nconditioning the normalizing flow on temperature, we leverage overparameterized\nneural networks' generalization capabilities to train a single flow\nrepresenting the posterior across a range of temperatures. This preserves modes\nidentified at higher temperatures when sampling from the variational posterior\nat $T = 1$, mitigating standard variational methods' mode-seeking behavior. In\nexperiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT\noutperforms traditional and adaptive annealing methods, finding more modes and\nachieving better ELBO values, particularly in higher dimensions where existing\napproaches fail. Our method requires minimal hyperparameter tuning and does not\nrequire an annealing schedule, advancing toward fully-automatic black-box\nvariational inference for complicated posteriors.", "AI": {"tldr": "FlowVAT \u662f\u4e00\u79cd\u7528\u4e8e\u6b63\u5219\u5316\u6d41\u53d8\u5206\u63a8\u65ad\u7684\u6761\u4ef6\u6e29\u5ea6\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u591a\u6a21\u6001\u548c\u9ad8\u7ef4\u540e\u9a8c\u5206\u5e03\u4e2d\u627e\u5230\u66f4\u591a\u6a21\u5f0f\u5e76\u83b7\u5f97\u66f4\u597d\u7684 ELBO \u503c\uff0c\u800c\u65e0\u9700\u6e29\u5ea6\u8c03\u5ea6\u548c\u6700\u5c0f\u7684\u8d85\u53c2\u6570\u8c03\u6574\u3002", "motivation": "\u4f20\u7edf\u6e29\u5ea6\u65b9\u6cd5\u9700\u8981\u6e29\u5ea6\u8c03\u5ea6\u548c\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u9ed1\u76d2\u53d8\u5206\u63a8\u65ad\u3002\u591a\u6a21\u6001\u548c\u9ad8\u7ef4\u540e\u9a8c\u5206\u5e03\u5bf9\u53d8\u5206\u63a8\u65ad\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5bfc\u81f4\u6a21\u5f0f\u641c\u7d22\u884c\u4e3a\u548c\u5d29\u6e83\u3002", "method": "FlowVAT \u662f\u4e00\u79cd\u6761\u4ef6\u6e29\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u6b63\u5219\u5316\u6d41\u53d8\u5206\u63a8\u65ad\u3002\u5b83\u540c\u65f6\u5bf9\u57fa\u7840\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u8fdb\u884c\u6e29\u5ea6\u8c03\u8282\uff0c\u5e76\u901a\u8fc7\u5c06\u6b63\u5219\u5316\u6d41\u6761\u4ef6\u5316\u4e3a\u6e29\u5ea6\uff0c\u5229\u7528\u8fc7\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u6765\u8bad\u7ec3\u4e00\u4e2a\u4ee3\u8868\u4e00\u7cfb\u5217\u6e29\u5ea6\u7684\u6d41\u3002", "result": "\u5728 2\u300110 \u548c 20 \u7ef4\u591a\u6a21\u6001\u5206\u5e03\u7684\u5b9e\u9a8c\u4e2d\uff0cFlowVAT \u8868\u73b0\u51fa\u8272\uff0c\u6bd4\u4f20\u7edf\u548c\u81ea\u9002\u5e94\u6e29\u5ea6\u65b9\u6cd5\u66f4\u597d\u5730\u627e\u5230\u6a21\u5f0f\u5e76\u83b7\u5f97\u66f4\u597d\u7684 ELBO \u503c\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u3002", "conclusion": "FlowVAT \u65b9\u6cd5\u5728\u591a\u6a21\u6001\u548c\u9ad8\u7ef4\u540e\u9a8c\u5206\u5e03\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u627e\u5230\u66f4\u591a\u6a21\u5f0f\u5e76\u83b7\u5f97\u66f4\u597d\u7684 ELBO \u503c\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u6e29\u5ea6\u8c03\u5ea6\u548c\u6700\u5c0f\u7684\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u63a8\u52a8\u4e86\u5168\u81ea\u52a8\u9ed1\u76d2\u53d8\u5206\u63a8\u65ad\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.10498", "pdf": "https://arxiv.org/pdf/2505.10498", "abs": "https://arxiv.org/abs/2505.10498", "authors": ["Sakshi Arya"], "title": "Batched Nonparametric Bandits via k-Nearest Neighbor UCB", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "68T05, 62L05, 62G08, 68Q32", "F.2.2; I.2.6"], "comment": "25 pages, 6 figures", "summary": "We study sequential decision-making in batched nonparametric contextual\nbandits, where actions are selected over a finite horizon divided into a small\nnumber of batches. Motivated by constraints in domains such as medicine and\nmarketing -- where online feedback is limited -- we propose a nonparametric\nalgorithm that combines adaptive k-nearest neighbor (k-NN) regression with the\nupper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully\nnonparametric, adapts to the context dimension, and is simple to implement.\nUnlike prior work relying on parametric or binning-based estimators, BaNk-UCB\nuses local geometry to estimate rewards and adaptively balances exploration and\nexploitation. We provide near-optimal regret guarantees under standard\nLipschitz smoothness and margin assumptions, using a theoretically motivated\nbatch schedule that balances regret across batches and achieves minimax-optimal\nrates. Empirical evaluations on synthetic and real-world datasets demonstrate\nthat BaNk-UCB consistently outperforms binning-based baselines.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u7b97\u6cd5BaNk-UCB\uff0c\u7ed3\u5408\u81ea\u9002\u5e94k\u8fd1\u90bb\u56de\u5f52\u548c\u4e0a\u7f6e\u4fe1\u754c\u539f\u5219\uff0c\u7528\u4e8e\u6279\u5904\u7406\u975e\u53c2\u6570\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u73b0\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u4e8e\u5206\u7bb1\u7684\u57fa\u7ebf\u3002", "motivation": "The study is motivated by constraints in domains such as medicine and marketing, where online feedback is limited, requiring a nonparametric algorithm that adapts to context dimension and balances exploration and exploitation.", "method": "BaNk-UCB combines adaptive k-nearest neighbor (k-NN) regression with the upper confidence bound (UCB) principle to address sequential decision-making in batched nonparametric contextual bandits.", "result": "BaNk-UCB provides near-optimal regret guarantees under standard Lipschitz smoothness and margin assumptions, achieving minimax-optimal rates with a theoretically motivated batch schedule.", "conclusion": "BaNk-UCB consistently outperforms binning-based baselines in empirical evaluations on synthetic and real-world datasets."}}
{"id": "2505.10511", "pdf": "https://arxiv.org/pdf/2505.10511", "abs": "https://arxiv.org/abs/2505.10511", "authors": ["Victor Zheleznov", "Stefan Bilbao", "Alec Wright", "Simon King"], "title": "Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations", "categories": ["cs.SD", "cs.LG", "eess.AS", "physics.comp-ph"], "comment": "Accepted for publication in Proceedings of the 28th International\n  Conference on Digital Audio Effects (DAFx25), Ancona, Italy, September 2025", "summary": "Modal synthesis methods are a long-standing approach for modelling\ndistributed musical systems. In some cases extensions are possible in order to\nhandle geometric nonlinearities. One such case is the high-amplitude vibration\nof a string, where geometric nonlinear effects lead to perceptually important\neffects including pitch glides and a dependence of brightness on striking\namplitude. A modal decomposition leads to a coupled nonlinear system of\nordinary differential equations. Recent work in applied machine learning\napproaches (in particular neural ordinary differential equations) has been used\nto model lumped dynamic systems such as electronic circuits automatically from\ndata. In this work, we examine how modal decomposition can be combined with\nneural ordinary differential equations for modelling distributed musical\nsystems. The proposed model leverages the analytical solution for linear\nvibration of system's modes and employs a neural network to account for\nnonlinear dynamic behaviour. Physical parameters of a system remain easily\naccessible after the training without the need for a parameter encoder in the\nnetwork architecture. As an initial proof of concept, we generate synthetic\ndata for a nonlinear transverse string and show that the model can be trained\nto reproduce the nonlinear dynamics of the system. Sound examples are\npresented.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6a21\u6001\u5206\u89e3\u4e0e\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u5206\u5e03\u5f0f\u97f3\u4e50\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u7ebf\u6027\u632f\u52a8\u7684\u89e3\u6790\u89e3\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u6001\u5408\u6210\u65b9\u6cd5\u5728\u5904\u7406\u51e0\u4f55\u975e\u7ebf\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u5206\u5e03\u5f0f\u97f3\u4e50\u7cfb\u7edf\u3002", "method": "\u672c\u6587\u7ed3\u5408\u4e86\u6a21\u6001\u5206\u89e3\u548c\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u7684\u65b9\u6cd5\uff0c\u4ee5\u5efa\u6a21\u5206\u5e03\u5f0f\u97f3\u4e50\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u7ebf\u6027\u632f\u52a8\u7684\u89e3\u6790\u89e3\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u884c\u4e3a\u3002", "result": "\u672c\u6587\u751f\u6210\u4e86\u4e00\u4e2a\u975e\u7ebf\u6027\u6a2a\u5411\u5f26\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u53ef\u4ee5\u8bad\u7ec3\u4ee5\u91cd\u73b0\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6a21\u6001\u5206\u89e3\u4e0e\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u5206\u5e03\u5f0f\u97f3\u4e50\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u7ebf\u6027\u632f\u52a8\u7684\u89e3\u6790\u89e3\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u884c\u4e3a\u3002"}}
