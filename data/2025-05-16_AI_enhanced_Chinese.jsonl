{"id": "2505.09649", "pdf": "https://arxiv.org/pdf/2505.09649", "abs": "https://arxiv.org/abs/2505.09649", "authors": ["Abisha Thapa Magar", "Anup Shakya"], "title": "Next Word Suggestion using Graph Neural Network", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language Modeling is a prevalent task in Natural Language Processing. The\ncurrently existing most recent and most successful language models often tend\nto build a massive model with billions of parameters, feed in a tremendous\namount of text data, and train with enormous computation resources which\nrequire millions of dollars. In this project, we aim to address an important\nsub-task in language modeling, i.e., context embedding. We propose an approach\nto exploit the Graph Convolution operation in GNNs to encode the context and\nuse it in coalition with LSTMs to predict the next word given a local context\nof preceding words. We test this on the custom Wikipedia text corpus using a\nvery limited amount of resources and show that this approach works fairly well\nto predict the next word.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u5377\u79ef\u548cLSTMs\u8fdb\u884c\u4e0a\u4e0b\u6587\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u53c2\u6570\u3001\u6587\u672c\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u6211\u4eec\u65e8\u5728\u89e3\u51b3\u8bed\u8a00\u5efa\u6a21\u4e2d\u7684\u4e00\u4e2a\u5b50\u4efb\u52a1\u2014\u2014\u4e0a\u4e0b\u6587\u5d4c\u5165\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u5377\u79ef\u64cd\u4f5c\u6765\u7f16\u7801\u4e0a\u4e0b\u6587\uff0c\u5e76\u5c06\u5176\u4e0eLSTMs\u7ed3\u5408\u4ee5\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684\u65b9\u6cd5\u3002", "result": "\u6211\u4eec\u5728\u81ea\u5b9a\u4e49\u7684Wikipedia\u6587\u672c\u8bed\u6599\u5e93\u4e0a\u6d4b\u8bd5\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u7684\u8d44\u6e90\u4e0b\u80fd\u591f\u6709\u6548\u5730\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002"}}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655", "abs": "https://arxiv.org/abs/2505.09655", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5DRA\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709GRPO\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u591a\u6837\u6027\u7684\u95ee\u9898\u3002DRA\u901a\u8fc7\u5f15\u5165\u5b50\u6a21\u6001\u4e92\u4fe1\u606f\u6765\u589e\u5f3a\u591a\u6837\u6027\u7684\u5956\u52b1\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684GRPO\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u89e3\u51b3\u65b9\u6848\u7ea7\u522b\u7684\u548c\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u65e0\u6cd5\u6355\u6349\u5230\u91c7\u6837\u5b8c\u6210\u4e4b\u95f4\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u4e86\u591a\u6837\u6027-\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86DRA\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5b50\u6a21\u6001\u4e92\u4fe1\u606f\uff08SMI\uff09\u6765\u51cf\u5c11\u5197\u4f59\u7684\u5b8c\u6210\u5e76\u589e\u5f3a\u591a\u6837\u6027\u7684\u5956\u52b1\u3002DRA\u53ef\u4ee5\u4e0eGRPO\u53ca\u5176\u53d8\u4f53DR.GRPO\u65e0\u7f1d\u96c6\u6210\uff0c\u5f62\u6210DRA-GRPO\u548cDGA-DR.GRPO\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u8fd1\u7684\u5f3a\u57fa\u7ebf\uff0c\u4f7f\u7528\u4ec57,000\u4e2a\u5fae\u8c03\u6837\u672c\u548c\u5927\u7ea655\u7f8e\u5143\u7684\u603b\u8bad\u7ec3\u6210\u672c\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523058.2%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5DRA\uff0c\u5b83\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09662", "pdf": "https://arxiv.org/pdf/2505.09662", "abs": "https://arxiv.org/abs/2505.09662", "authors": ["Philipp Schoenegger", "Francesco Salvi", "Jiacheng Liu", "Xiaoli Nan", "Ramit Debnath", "Barbara Fasolo", "Evelina Leivada", "Gabriel Recchia", "Fritz G\u00fcnther", "Ali Zarifhonarvar", "Joe Kwon", "Zahoor Ul Islam", "Marco Dehnert", "Daryl Y. H. Lee", "Madeline G. Reinecke", "David G. Kamper", "Mert Koba\u015f", "Adam Sandford", "Jonas Kgomo", "Luke Hewitt", "Shreya Kapoor", "Kerem Oktar", "Eyup Engin Kucuk", "Bo Feng", "Cameron R. Jones", "Izzy Gainsburg", "Sebastian Olschewski", "Nora Heinzelmann", "Francisco Cruz", "Ben M. Tappin", "Tao Ma", "Peter S. Park", "Rayan Onyonka", "Arthur Hjorth", "Peter Slattery", "Qingcheng Zeng", "Lennart Finke", "Igor Grossmann", "Alessandro Salatiello", "Ezra Karger"], "title": "Large Language Models Are More Persuasive Than Incentivized Human Persuaders", "categories": ["cs.CL", "I.2.7; H.1.2; K.4.1; H.5.2"], "comment": null, "summary": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u5728\u8bf4\u670d\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0AI\u5728\u8bf4\u670d\u65b9\u9762\u5df2\u7ecf\u8d85\u8d8a\u4e86\u4eba\u7c7b\uff0c\u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86\u5bf9\u9f50\u548c\u6cbb\u7406\u6846\u67b6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u5728\u8bf4\u670d\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8AI\u5728\u8bf4\u670d\u65b9\u9762\u7684\u6f5c\u529b\u53ca\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u76f4\u63a5\u6bd4\u8f83\u4e86\u524d\u6cbf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff1bClaude Sonnet 3.5\uff09\u4e0e\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u5728\u4ea4\u4e92\u5f0f\u3001\u5b9e\u65f6\u5bf9\u8bdd\u6d4b\u9a8c\u73af\u5883\u4e2d\u7684\u8bf4\u670d\u80fd\u529b\u3002\u5728\u8fd9\u4e2a\u9884\u6ce8\u518c\u7684\u5927\u89c4\u6a21\u6fc0\u52b1\u5b9e\u9a8c\u4e2d\uff0c\u53c2\u4e0e\u8005\uff08\u6d4b\u9a8c\u7b54\u9898\u8005\uff09\u5b8c\u6210\u4e86\u4e00\u4e2a\u5728\u7ebf\u6d4b\u9a8c\uff0c\u5176\u4e2d\u8bf4\u670d\u8005\uff08\u4eba\u7c7b\u6216LLM\uff09\u8bd5\u56fe\u8bf4\u670d\u6d4b\u9a8c\u7b54\u9898\u8005\u671d\u7740\u6b63\u786e\u6216\u9519\u8bef\u7684\u7b54\u6848\u65b9\u5411\u524d\u8fdb\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0cLLM\u8bf4\u670d\u8005\u5728\u65b9\u5411\u6027\u8bf4\u670d\u5c1d\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6bd4\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u66f4\u9ad8\u7684\u9075\u4ece\u6027\uff0c\u8fd9\u8868\u660e\u5728\u8bda\u5b9e\uff08\u671d\u5411\u6b63\u786e\u7b54\u6848\uff09\u548c\u6b3a\u9a97\uff08\u671d\u5411\u9519\u8bef\u7b54\u6848\uff09\u60c5\u5883\u4e0b\uff0cLLM\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u8bf4\u670d\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5f53\u5f15\u5bfc\u6d4b\u9a8c\u7b54\u9898\u8005\u671d\u5411\u6b63\u786e\u7b54\u6848\u65f6\uff0cLLM\u8bf4\u670d\u8005\u663e\u8457\u63d0\u9ad8\u4e86\u7b54\u9898\u8005\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u6536\u76ca\uff1b\u800c\u5f53\u5f15\u5bfc\u4ed6\u4eec\u671d\u5411\u9519\u8bef\u7b54\u6848\u65f6\uff0cLLM\u8bf4\u670d\u8005\u5219\u663e\u8457\u964d\u4f4e\u4e86\u4ed6\u4eec\u7684\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u6536\u76ca\u51cf\u5c11\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cAI\u7684\u8bf4\u670d\u80fd\u529b\u5df2\u7ecf\u8d85\u8fc7\u4e86\u90a3\u4e9b\u6709\u771f\u5b9e\u91d1\u94b1\u5956\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\u3002\u8fd9\u4e00\u53d1\u73b0\u51f8\u663e\u4e86\u65b0\u5174\u5bf9\u9f50\u548c\u6cbb\u7406\u6846\u67b6\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2505.09666", "pdf": "https://arxiv.org/pdf/2505.09666", "abs": "https://arxiv.org/abs/2505.09666", "authors": ["Yumin Choi", "Jinheon Baek", "Sung Ju Hwang"], "title": "System Prompt Optimization with Meta-Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u53cc\u5c42\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u5e76\u4f7f\u5176\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1\u548c\u9886\u57df\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u63a8\u5e7f\u7cfb\u7edf\u63d0\u793a\u5e76\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u9488\u5bf9\u5355\u4e2a\u67e5\u8be2\u6216\u4efb\u52a1\u7684\u7279\u5b9a\u7528\u6237\u63d0\u793a\u4e0a\uff0c\u800c\u5ffd\u7565\u4e86\u4e00\u65e6\u4f18\u5316\u540e\u53ef\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u4f7f\u7528\u7684\u7cfb\u7edf\u63d0\u793a\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u53cc\u5c42\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u7684\u65b0\u95ee\u9898\uff0c\u5176\u76ee\u6807\u662f\u8bbe\u8ba1\u5bf9\u5404\u79cd\u7528\u6237\u63d0\u793a\u5177\u6709\u9c81\u68d2\u6027\u5e76\u53ef\u8f6c\u79fb\u5230\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u7684\u7cfb\u7edf\u63d0\u793a\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5404\u79cd\u7528\u6237\u63d0\u793a\u4e0a\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\uff0c\u540c\u65f6\u4ee5\u8fed\u4ee3\u65b9\u5f0f\u66f4\u65b0\u7528\u6237\u63d0\u793a\uff0c\u4ee5\u786e\u4fdd\u5b83\u4eec\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u572814\u4e2a\u8de85\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u7cfb\u7edf\u63d0\u793a\u63a8\u5e7f\u5230\u5404\u79cd\u7528\u6237\u63d0\u793a\uff0c\u5e76\u4e14\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u63d0\u793a\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u7528\u6237\u63d0\u793a\u4e0a\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u572814\u4e2a\u8de85\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u7cfb\u7edf\u63d0\u793a\u63a8\u5e7f\u5230\u5404\u79cd\u7528\u6237\u63d0\u793a\uff0c\u5e76\u4e14\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u63d0\u793a\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u7528\u6237\u63d0\u793a\u4e0a\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.09746", "pdf": "https://arxiv.org/pdf/2505.09746", "abs": "https://arxiv.org/abs/2505.09746", "authors": ["Xabier Morales", "Ayah Elsayed", "Debbie Zhao", "Filip Loncaric", "Ainhoa Aguado", "Mireia Masias", "Gina Quill", "Marc Ramos", "Ada Doltra", "Ana Garcia", "Marta Sitges", "David Marlevi", "Alistair Young", "Martyn Nash", "Bart Bijnens", "Oscar Camara"], "title": "A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium", "categories": ["cs.CV"], "comment": null, "summary": "The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u4e2a\u4e13\u4e3a\u5de6\u5fc3\u623f4D\u6d41\u52a8MRI\u5206\u6790\u800c\u8bbe\u8ba1\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u4e2d\u5fc3\u7684\u6570\u636e\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u81ea\u52a8\u5206\u5272\uff0c\u540c\u65f6\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u4f5c\u4e3a\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u8d85\u58f0\u5206\u6790\u5728\u5de6\u5fc3\u623f\u8840\u6d41\u52a8\u529b\u5b66\u7684\u7406\u89e3\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u800c4D\u6d41\u52a8MRI\u7531\u4e8e\u4f4e\u901f\u548c\u6709\u9650\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u4f7f\u5f97\u5206\u6790\u8be5\u8154\u5ba4\u53d8\u5f97\u56f0\u96be\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u8ba1\u7b97\u6846\u67b6\u4ee5\u53ca\u591a\u6837\u5316\u7684\u91c7\u96c6\u534f\u8bae\u548c\u4f9b\u5e94\u5546\uff0c\u4f7f\u5f97\u6536\u96c6\u5927\u578b\u961f\u5217\u6765\u7814\u7a764D\u6d41\u52a8MRI\u63d0\u4f9b\u7684\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u7684\u9884\u540e\u4ef7\u503c\u53d8\u5f97\u590d\u6742\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u4e13\u4e3a\u5de6\u5fc3\u623f4D\u6d41\u52a8MRI\u5206\u6790\u800c\u8bbe\u8ba1\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u7684\u5168\u9762\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u6211\u4eec\u7684\u6846\u67b6\u80fd\u591f\u5904\u7406\u6765\u81ea\u4e0d\u540c\u4e2d\u5fc3\u7684\u4e0d\u540c\u8d28\u91cf\u7684\u6570\u636e\uff0c\u4ea7\u751f\u9ad8\u7cbe\u5ea6\u7684\u81ea\u52a8\u5206\u5272\uff08Dice > 0.9\u548cHausdorff 95 < 3 mm\uff09\uff0c\u5e76\u4e14\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u80fd\u91cf\u3001\u6da1\u5ea6\u548c\u538b\u529b\u53c2\u6570\u5728\u5404\u79cd\u75be\u75c5\u4e2d\u7684\u6f5c\u5728\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u4f5c\u7528\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u8bc1\u660e\u4e86\u5bf9\u4e0d\u540c\u4e2d\u5fc3\u7684\u6570\u636e\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u4e5f\u80fd\u4ea7\u751f\u9ad8\u7cbe\u5ea6\u7684\u81ea\u52a8\u5206\u5272\uff0c\u5e76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u80fd\u91cf\u3001\u6da1\u5ea6\u548c\u538b\u529b\u53c2\u6570\u5728\u5404\u79cd\u75be\u75c5\u4e2d\u7684\u6f5c\u5728\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u4f5c\u7528\u3002"}}
{"id": "2505.09659", "pdf": "https://arxiv.org/pdf/2505.09659", "abs": "https://arxiv.org/abs/2505.09659", "authors": ["Long Chen", "Xiaotian Song", "Yanan Sun"], "title": "LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Spiking Large Language Models (LLMs) have emerged as an energy-efficient\nalternative to conventional LLMs through their event-driven computation. To\neffectively obtain spiking LLMs, researchers develop different ANN-to-SNN\nconversion methods by leveraging pre-trained ANN parameters while inheriting\nthe energy efficiency of SNN. However, existing conversion methods struggle\nwith extreme activation outliers and incompatible nonlinear operations of\nANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for\nfully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel\nneurons to convert the activation outlier and nonlinear operation of ANN-based\nLLMs. Moreover, LAS tailors the spike-equivalent Transformer components for\nspiking LLMs, which can ensure full spiking conversion without any loss of\nperformance. Experimental results on six language models and two\nvision-language models demonstrate that LAS achieves loss-less conversion.\nNotably, on OPT-66B, LAS even improves the accuracy of 2\\% on the WSC task. In\naddition, the parameter and ablation studies further verify the effectiveness\nof LAS. The source code is available at https://github.com/lc783/LAS", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u635f\u5931\u7684ANN-SNN\u8f6c\u6362\u65b9\u6cd5\uff08LAS\uff09\uff0c\u7528\u4e8e\u5168\u8109\u51b2\u9a71\u52a8\u7684LLMs\u3002LAS\u901a\u8fc7\u5f15\u5165\u65b0\u578b\u795e\u7ecf\u5143\u548c\u5b9a\u5236\u7684\u8109\u51b2\u7b49\u6548Transformer\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e86\u65e0\u6027\u80fd\u635f\u5931\u7684\u8f6c\u6362\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8f6c\u6362\u65b9\u6cd5\u5728\u5904\u7406\u57fa\u4e8eANN\u7684LLMs\u7684\u6781\u7aef\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u4e0d\u517c\u5bb9\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u65e0\u635f\u5931\u7684\u8f6c\u6362\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u635f\u5931\u7684ANN-SNN\u8f6c\u6362\u65b9\u6cd5\uff0c\u79f0\u4e3aLAS\u3002LAS\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u578b\u795e\u7ecf\u5143\u6765\u8f6c\u6362\u57fa\u4e8eANN\u7684LLMs\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u5e76\u4e3a\u57fa\u4e8e\u8109\u51b2\u7684LLMs\u5b9a\u5236\u4e86\u7b49\u6548\u4e8e\u8109\u51b2\u7684Transformer\u7ec4\u4ef6\uff0c\u4ee5\u786e\u4fdd\u5b8c\u5168\u8109\u51b2\u8f6c\u6362\u800c\u4e0d\u4f1a\u6709\u4efb\u4f55\u6027\u80fd\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLAS\u5728\u516d\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u65e0\u635f\u5931\u8f6c\u6362\u3002\u7279\u522b\u5730\uff0c\u5728OPT-66B\u4e0a\uff0cLAS\u5728WSC\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e862%\u3002\u53c2\u6570\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86LAS\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLAS\u5b9e\u73b0\u4e86\u65e0\u635f\u5931\u8f6c\u6362\uff0c\u5e76\u5728OPT-66B\u4e0a\u63d0\u9ad8\u4e86WSC\u4efb\u52a1\u7684\u51c6\u786e\u73872%\u3002\u53c2\u6570\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86LAS\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09639", "pdf": "https://arxiv.org/pdf/2505.09639", "abs": "https://arxiv.org/abs/2505.09639", "authors": ["Quentin Cohen-Solal"], "title": "Study and improvement of search algorithms in two-players perfect information games", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Games, in their mathematical sense, are everywhere (game industries,\neconomics, defense, education, chemistry, biology, ...).Search algorithms in\ngames are artificial intelligence methods for playing such games.\nUnfortunately, there is no study on these algorithms that evaluates the\ngenerality of their performance. We propose to address this gap in the case of\ntwo-player zero-sum games with perfect information. Furthermore, we propose a\nnew search algorithm and we show that, for a short search time, it outperforms\nall studied algorithms on all games in this large experiment and that, for a\nmedium search time, it outperforms all studied algorithms on 17 of the 22\nstudied games.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u591a\u79cd\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u76ee\u524d\u5c1a\u65e0\u7814\u7a76\u8bc4\u4f30\u8fd9\u4e9b\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u9488\u5bf9\u4e24\u4eba\u96f6\u548c\u535a\u5f08\u4e14\u4fe1\u606f\u5b8c\u5168\u7684\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "result": "\u5728\u77ed\u65f6\u95f4\u641c\u7d22\u4e2d\uff0c\u65b0\u7b97\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u6e38\u620f\u4e2d\u90fd\u4f18\u4e8e\u6240\u6709\u7814\u7a76\u7684\u7b97\u6cd5\uff1b\u5728\u4e2d\u7b49\u65f6\u95f4\u641c\u7d22\u4e2d\uff0c\u5b83\u572822\u4e2a\u6d4b\u8bd5\u6e38\u620f\u4e2d\u768417\u4e2a\u4e2d\u4f18\u4e8e\u6240\u6709\u7814\u7a76\u7684\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u77ed\u65f6\u95f4\u641c\u7d22\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u6e38\u620f\u4e2d\u90fd\u4f18\u4e8e\u6240\u6709\u7814\u7a76\u7684\u7b97\u6cd5\uff1b\u5728\u4e2d\u7b49\u65f6\u95f4\u641c\u7d22\u4e2d\uff0c\u5b83\u572822\u4e2a\u6d4b\u8bd5\u6e38\u620f\u4e2d\u768417\u4e2a\u4e2d\u4f18\u4e8e\u6240\u6709\u7814\u7a76\u7684\u7b97\u6cd5\u3002"}}
{"id": "2505.09701", "pdf": "https://arxiv.org/pdf/2505.09701", "abs": "https://arxiv.org/abs/2505.09701", "authors": ["Xin Liu", "Lechen Zhang", "Sheza Munir", "Yiyang Gu", "Lu Wang"], "title": "VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at generating long-form responses, but\nevaluating their factuality remains challenging due to complex inter-sentence\ndependencies within the generated facts. Prior solutions predominantly follow a\ndecompose-decontextualize-verify pipeline but often fail to capture essential\ncontext and miss key relational facts. In this paper, we introduce VeriFact, a\nfactuality evaluation framework designed to enhance fact extraction by\nidentifying and resolving incomplete and missing facts to support more accurate\nverification results. Moreover, we introduce FactRBench , a benchmark that\nevaluates both precision and recall in long-form model responses, whereas prior\nwork primarily focuses on precision. FactRBench provides reference fact sets\nfrom advanced LLMs and human-written answers, enabling recall assessment.\nEmpirical evaluations show that VeriFact significantly enhances fact\ncompleteness and preserves complex facts with critical relational information,\nresulting in more accurate factuality evaluation. Benchmarking various open-\nand close-weight LLMs on FactRBench indicate that larger models within same\nmodel family improve precision and recall, but high precision does not always\ncorrelate with high recall, underscoring the importance of comprehensive\nfactuality assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 VeriFact \u548c FactRBench\uff0c\u5206\u522b\u7528\u4e8e\u63d0\u9ad8\u4e8b\u5b9e\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u5168\u9762\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u7684\u4e8b\u5b9e\u4e2d\u5b58\u5728\u590d\u6742\u7684\u53e5\u5b50\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e8b\u5b9e\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4ee5\u524d\u7684\u89e3\u51b3\u65b9\u6848\u5927\u591a\u9075\u5faa\u5206\u89e3-\u53bb\u4e0a\u4e0b\u6587\u5316-\u9a8c\u8bc1\u7684\u6d41\u7a0b\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5230\u5fc5\u8981\u7684\u4e0a\u4e0b\u6587\u5e76\u9057\u6f0f\u5173\u952e\u7684\u5173\u7cfb\u4e8b\u5b9e\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86 VeriFact\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e8b\u5b9e\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u548c\u89e3\u51b3\u4e0d\u5b8c\u6574\u548c\u7f3a\u5931\u7684\u4e8b\u5b9e\u6765\u589e\u5f3a\u4e8b\u5b9e\u63d0\u53d6\uff0c\u4ee5\u652f\u6301\u66f4\u51c6\u786e\u7684\u9a8c\u8bc1\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86 FactRBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u957f\u683c\u5f0f\u6a21\u578b\u54cd\u5e94\u4e2d\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u800c\u4e4b\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u7cbe\u5ea6\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cVeriFact \u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u5b8c\u6574\u6027\u5e76\u4fdd\u7559\u4e86\u5305\u542b\u5173\u952e\u5173\u7cfb\u4fe1\u606f\u7684\u590d\u6742\u4e8b\u5b9e\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u8bc4\u4f30\u3002\u5728 FactRBench \u4e0a\u5bf9\u5404\u79cd\u5f00\u6e90\u548c\u95ed\u6e90 LLM \u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u5927\u6a21\u578b\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u4f46\u9ad8\u7cbe\u5ea6\u5e76\u4e0d\u603b\u662f\u4e0e\u9ad8\u53ec\u56de\u7387\u76f8\u5173\uff0c\u8fd9\u7a81\u663e\u4e86\u5168\u9762\u4e8b\u5b9e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "conclusion": "VeriFact \u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u5b8c\u6574\u6027\u5e76\u4fdd\u7559\u4e86\u5305\u542b\u5173\u952e\u5173\u7cfb\u4fe1\u606f\u7684\u590d\u6742\u4e8b\u5b9e\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u8bc4\u4f30\u3002\u5728 FactRBench \u4e0a\u5bf9\u5404\u79cd\u5f00\u6e90\u548c\u95ed\u6e90 LLM \u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u5927\u6a21\u578b\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u4f46\u9ad8\u7cbe\u5ea6\u5e76\u4e0d\u603b\u662f\u4e0e\u9ad8\u53ec\u56de\u7387\u76f8\u5173\uff0c\u8fd9\u7a81\u663e\u4e86\u5168\u9762\u4e8b\u5b9e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.09827", "pdf": "https://arxiv.org/pdf/2505.09827", "abs": "https://arxiv.org/abs/2505.09827", "authors": ["Julian Tanke", "Takashi Shibuya", "Kengo Uchida", "Koichi Saito", "Yuki Mitsufuji"], "title": "Dyadic Mamba: Long-term Dyadic Human Motion Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 HuMoGen Workshop", "summary": "Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Dyadic Mamba\uff0c\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u53cc\u4eba\u4eba\u4f53\u8fd0\u52a8\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fde\u63a5\u5728\u4e2a\u4f53\u8fd0\u52a8\u5e8f\u5217\u4e4b\u95f4\u4fc3\u8fdb\u4fe1\u606f\u6d41\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDyadic Mamba\u5728\u77ed\u671f\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5728\u66f4\u957f\u7684\u5e8f\u5217\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u957f\u671f\u8fd0\u52a8\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u4ece\u6587\u672c\u63cf\u8ff0\u751f\u6210\u73b0\u5b9e\u7684\u53cc\u4eba\u8fd0\u52a8\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8d85\u8fc7\u5178\u578b\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u7684\u957f\u671f\u4e92\u52a8\u3002\u867d\u7136\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u5728\u77ed\u671f\u53cc\u4eba\u8fd0\u52a8\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u5728\u66f4\u957f\u7684\u5e8f\u5217\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86Dyadic Mamba\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u53cc\u4eba\u4eba\u4f53\u8fd0\u52a8\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u8fde\u63a5\u5728\u4e2a\u4f53\u8fd0\u52a8\u5e8f\u5217\u4e4b\u95f4\u4fc3\u8fdb\u4fe1\u606f\u6d41\uff0c\u6d88\u9664\u4e86\u5bf9\u590d\u6742\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u9700\u6c42\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86Dyadic Mamba\u5728\u6807\u51c6\u77ed\u671f\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u66f4\u957f\u7684\u5e8f\u5217\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u957f\u671f\u8fd0\u52a8\u5408\u6210\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u67b6\u6784\u4e3a\u89e3\u51b3\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u5408\u6210\u957f\u671f\u53cc\u4eba\u4eba\u4f53\u8fd0\u52a8\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2505.09663", "pdf": "https://arxiv.org/pdf/2505.09663", "abs": "https://arxiv.org/abs/2505.09663", "authors": ["Julian B\u00fcchel", "Iason Chalas", "Giovanni Acampa", "An Chen", "Omobayode Fagbohungbe", "Sidney Tsai", "Kaoutar El Maghraoui", "Manuel Le Gallo", "Abbas Rahimi", "Abu Sebastian"], "title": "Analog Foundation Models", "categories": ["cs.LG"], "comment": "43 pages, 8 figures, under review", "summary": "Analog in-memory computing (AIMC) is a promising compute paradigm to improve\nspeed and power efficiency of neural network inference beyond the limits of\nconventional von Neumann-based architectures. However, AIMC introduces\nfundamental challenges such as noisy computations and strict constraints on\ninput and output quantization. Because of these constraints and imprecisions,\noff-the-shelf LLMs are not able to achieve 4-bit-level performance when\ndeployed on AIMC-based hardware. While researchers previously investigated\nrecovering this accuracy gap on small, mostly vision-based models, a generic\nmethod applicable to LLMs pre-trained on trillions of tokens does not yet\nexist. In this work, we introduce a general and scalable method to robustly\nadapt LLMs for execution on noisy, low-precision analog hardware. Our approach\nenables state-of-the-art models $\\unicode{x2013}$ including\nPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\\unicode{x2013}$ to retain\nperformance comparable to 4-bit weight, 8-bit activation baselines, despite the\npresence of analog noise and quantization constraints. Additionally, we show\nthat as a byproduct of our training methodology, analog foundation models can\nbe quantized for inference on low-precision digital hardware. Finally, we show\nthat our models also benefit from test-time compute scaling, showing better\nscaling behavior than models trained with 4-bit weight and 8-bit static input\nquantization. Our work bridges the gap between high-capacity LLMs and efficient\nanalog hardware, offering a path toward energy-efficient foundation models.\nCode is available at https://github.com/IBM/analog-foundation-models .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u566a\u58f0\u548c\u4f4e\u7cbe\u5ea6\u6a21\u62df\u786c\u4ef6\u4e0a\u6267\u884c\u5927\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5b58\u5728\u6a21\u62df\u566a\u58f0\u548c\u91cf\u5316\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u4e0e4\u4f4d\u6743\u91cd\u30018\u4f4d\u6fc0\u6d3b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\uff08AIMC\uff09\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u53ef\u4ee5\u8d85\u8d8a\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u901f\u5ea6\u548c\u80fd\u6548\u3002\u7136\u800c\uff0cAIMC\u5e26\u6765\u4e86\u8bf8\u5982\u8ba1\u7b97\u566a\u58f0\u548c\u8f93\u5165\u8f93\u51fa\u91cf\u5316\u4e25\u683c\u9650\u5236\u7b49\u57fa\u672c\u6311\u6218\u3002\u7531\u4e8e\u8fd9\u4e9b\u9650\u5236\u548c\u4e0d\u7cbe\u786e\u6027\uff0c\u73b0\u6210\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u5230\u57fa\u4e8eAIMC\u7684\u786c\u4ef6\u65f6\u65e0\u6cd5\u8fbe\u52304\u4f4d\u7ea7\u522b\u7684\u6027\u80fd\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u4ee5\u7a33\u5065\u5730\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u5728\u566a\u58f0\u548c\u4f4e\u7cbe\u5ea6\u6a21\u62df\u786c\u4ef6\u4e0a\u6267\u884c\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5305\u62ecPhi-3-mini-4k-instruct\u548cLlama-3.2-1B-Instruct\uff09\u5728\u5b58\u5728\u6a21\u62df\u566a\u58f0\u548c\u91cf\u5316\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u4e0e4\u4f4d\u6743\u91cd\u30018\u4f4d\u6fc0\u6d3b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4f5c\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u7684\u526f\u4ea7\u54c1\uff0c\u6a21\u62df\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u88ab\u91cf\u5316\u4ee5\u5728\u4f4e\u7cbe\u5ea6\u6570\u5b57\u786c\u4ef6\u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7f29\u653e\u65b9\u9762\u4e5f\u53d7\u76ca\uff0c\u8868\u73b0\u51fa\u6bd4\u4f7f\u75284\u4f4d\u6743\u91cd\u548c8\u4f4d\u9759\u6001\u8f93\u5165\u91cf\u5316\u7684\u6a21\u578b\u66f4\u597d\u7684\u7f29\u653e\u884c\u4e3a\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u5f25\u5408\u4e86\u9ad8\u5bb9\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u548c\u9ad8\u6548\u6a21\u62df\u786c\u4ef6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8282\u80fd\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u8def\u5f84\u3002"}}
{"id": "2505.09640", "pdf": "https://arxiv.org/pdf/2505.09640", "abs": "https://arxiv.org/abs/2505.09640", "authors": ["Tom\u00e1s Capdevielle", "Santiago Cifuentes"], "title": "Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms", "categories": ["cs.AI", "68T01", "I.2.0"], "comment": "22 pages, 7 figures", "summary": "Given a classification model and a prediction for some input, there are\nheuristic strategies for ranking features according to their importance in\nregard to the prediction. One common approach to this task is rooted in\npropositional logic and the notion of \\textit{sufficient reason}. Through this\nconcept, the categories of relevant and necessary features were proposed in\norder to identify the crucial aspects of the input. This paper improves the\nexisting techniques and algorithms for deciding which are the relevant and/or\nnecessary features, showing in particular that necessity can be detected\nefficiently in complex models such as neural networks. We also generalize the\nnotion of relevancy and study associated problems. Moreover, we present a new\nglobal notion (i.e. that intends to explain whether a feature is important for\nthe behavior of the model in general, not depending on a particular input) of\n\\textit{usefulness} and prove that it is related to relevancy and necessity.\nFurthermore, we develop efficient algorithms for detecting it in decision trees\nand other more complex models, and experiment on three datasets to analyze its\npractical utility.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684'\u6709\u7528\u6027'\u6982\u5ff5\uff0c\u5e76\u5728\u590d\u6742\u6a21\u578b\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u8f93\u5165\u4e2d\u7684\u5173\u952e\u65b9\u9762\uff0c\u5e76\u5728\u590d\u6742\u6a21\u578b\u4e2d\u9ad8\u6548\u68c0\u6d4b\u5fc5\u8981\u6027\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u7684\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u65b0\u7684\u5168\u5c40\u6982\u5ff5'\u6709\u7528\u6027'\uff0c\u5e76\u5f00\u53d1\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u68c0\u6d4b\u51b3\u7b56\u6811\u548c\u5176\u4ed6\u590d\u6742\u6a21\u578b\u4e2d\u7684\u6709\u7528\u6027\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u76f8\u5173\u6027\u548c\u5fc5\u8981\u6027\u7684\u6982\u5ff5\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u5728\u590d\u6742\u6a21\u578b\uff08\u5982\u795e\u7ecf\u7f51\u7edc\uff09\u4e2d\u53ef\u4ee5\u9ad8\u6548\u68c0\u6d4b\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684'\u6709\u7528\u6027'\u6982\u5ff5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u6539\u8fdb\u4e86\u73b0\u6709\u6280\u672f\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5168\u5c40\u6982\u5ff5'\u6709\u7528\u6027'\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4e0e\u76f8\u5173\u6027\u548c\u5fc5\u8981\u6027\u7684\u5173\u7cfb\u3002"}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724", "abs": "https://arxiv.org/abs/2505.09724", "authors": ["Gino Carmona-D\u00edaz", "William Jim\u00e9nez-Leal", "Mar\u00eda Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Berm\u00fadez"], "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6587\u672c\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u5e94\u7528\u5206\u7c7b\u6cd5\u7684\u6b65\u9aa4\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u53ef\u80fd\u6027\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u5206\u6790\u6587\u672c\u5982\u5f00\u653e\u5f0f\u56de\u7b54\u3001\u6807\u9898\u6216\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u662f\u4e00\u4e2a\u8017\u65f6\u4e14\u6613\u53d7\u504f\u89c1\u5f71\u54cd\u7684\u8fc7\u7a0b\u3002LLMs\u662f\u7528\u4e8e\u6587\u672c\u5206\u6790\u7684\u6709\u524d\u9014\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u4f7f\u7528\u9884\u5b9a\u4e49\uff08\u81ea\u4e0a\u800c\u4e0b\uff09\u6216\u6570\u636e\u9a71\u52a8\uff08\u81ea\u4e0b\u800c\u4e0a\uff09\u7684\u5206\u7c7b\u6cd5\uff0c\u800c\u4e0d\u727a\u7272\u8d28\u91cf\u3002", "method": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5206\u6b65\u6559\u7a0b\uff0c\u901a\u8fc7\u7814\u7a76\u4eba\u5458\u4e0eLLMs\u4e4b\u95f4\u7684\u8fed\u4ee3\u548c\u534f\u4f5c\u8fc7\u7a0b\uff0c\u9ad8\u6548\u5730\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u5e94\u7528\u7528\u4e8e\u5206\u6790\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u7f16\u5199\u63d0\u793a\u6765\u5ba1\u67e5\u6570\u636e\u96c6\u5e76\u751f\u6210\u751f\u6d3b\u9886\u57df\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u548c\u76f4\u63a5\u4fee\u6539\u8bc4\u4f30\u548c\u4f18\u5316\u5206\u7c7b\u6cd5\uff0c\u6d4b\u8bd5\u5206\u7c7b\u6cd5\u5e76\u8bc4\u4f30\u7f16\u7801\u8005\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u7528\u9ad8\u7f16\u7801\u8005\u95f4\u53ef\u9760\u6027\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6587\u672c\u5206\u6790\u7684\u53ef\u80fd\u6027\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2505.09829", "pdf": "https://arxiv.org/pdf/2505.09829", "abs": "https://arxiv.org/abs/2505.09829", "authors": ["Tushar Kataria", "Shireen Y. Elhabian"], "title": "BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes", "categories": ["cs.CV"], "comment": null, "summary": "Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBoundarySeg\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5668\u5b98\u8fb9\u754c\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u5229\u7528\u4e24\u4e2a\u4efb\u52a1\u9884\u6d4b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u6765\u63d0\u4f9b\u989d\u5916\u76d1\u7763\uff0c\u4ece\u800c\u5728\u4e0d\u4f9d\u8d56\u672a\u6807\u8bb0\u6570\u636e\u6216\u589e\u52a0\u8ba1\u7b97\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u4e25\u683c\u7684\u9690\u79c1\u6cd5\u89c4\u548c\u6570\u636e\u4fdd\u62a4\u653f\u7b56\uff0c\u83b7\u53d6\u5927\u89c4\u6a21\u533b\u7597\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u6807\u6ce8\u533b\u5b66\u56fe\u50cf\u9700\u8981\u9886\u57df\u4e13\u5bb6\u624b\u52a8\u5206\u5272\u89e3\u5256\u7ed3\u6784\uff0c\u8fd9\u4f7f\u5f97\u8fc7\u7a0b\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u56e0\u6b64\uff0c\u534a\u76d1\u7763\u65b9\u6cd5\u56e0\u5176\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u800c\u53d7\u5230\u6b22\u8fce\u3002\u7136\u800c\uff0c\u534a\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u672a\u6807\u8bb0\u6570\u636e\u7684\u53ef\u7528\u6027\uff0c\u5f53\u8fd9\u4e9b\u6570\u636e\u7a00\u7f3a\u6216\u4e0d\u5b58\u5728\u65f6\uff0c\u5176\u6548\u679c\u4f1a\u4e0b\u964d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBoundarySeg\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u5c06\u5668\u5b98\u8fb9\u754c\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u5229\u7528\u4e24\u4e2a\u4efb\u52a1\u9884\u6d4b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u63d0\u4f9b\u989d\u5916\u76d1\u7763\u3002", "result": "\u8be5\u7b56\u7565\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u91cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u4e0e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u672a\u6807\u8bb0\u6570\u636e\u6216\u589e\u52a0\u8ba1\u7b97\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09702", "pdf": "https://arxiv.org/pdf/2505.09702", "abs": "https://arxiv.org/abs/2505.09702", "authors": ["Yezi Liu", "Prathyush Poduval", "Wenjun Huang", "Yang Ni", "Hanning Chen", "Mohsen Imani"], "title": "Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing", "categories": ["cs.LG"], "comment": null, "summary": "Graph unlearning is a crucial approach for protecting user privacy by erasing\nthe influence of user data on trained graph models. Recent developments in\ngraph unlearning methods have primarily focused on maintaining model prediction\nperformance while removing user information. However, we have observed that\nwhen user information is deleted from the model, the prediction distribution\nacross different sensitive groups often changes. Furthermore, graph models are\nshown to be prone to amplifying biases, making the study of fairness in graph\nunlearning particularly important. This raises the question: Does graph\nunlearning actually introduce bias? Our findings indicate that the predictions\nof post-unlearning models become highly correlated with sensitive attributes,\nconfirming the introduction of bias in the graph unlearning process. To address\nthis issue, we propose a fair graph unlearning method, FGU. To guarantee\nprivacy, FGU trains shard models on partitioned subgraphs, unlearns the\nrequested data from the corresponding subgraphs, and retrains the shard models\non the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing\nprocess: it first enables shard-level fairness by incorporating a fairness\nregularizer in the shard model retraining, and then achieves global-level\nfairness by aligning all shard models to minimize global disparity. Our\nexperiments demonstrate that FGU achieves superior fairness while maintaining\nprivacy and accuracy. Additionally, FGU is robust to diverse unlearning\nrequests, ensuring fairness and utility performance across various data\ndistributions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u5220\u9664\u8fc7\u7a0b\u4e2d\u662f\u5426\u5f15\u5165\u504f\u89c1\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u516c\u5e73\u7684\u56fe\u5220\u9664\u65b9\u6cd5FGU\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u5f53\u7528\u6237\u4fe1\u606f\u4ece\u6a21\u578b\u4e2d\u5220\u9664\u65f6\uff0c\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u4e4b\u95f4\u7684\u9884\u6d4b\u5206\u5e03\u5f80\u5f80\u4f1a\u6539\u53d8\u3002\u6b64\u5916\uff0c\u56fe\u6a21\u578b\u5bb9\u6613\u653e\u5927\u504f\u89c1\uff0c\u4f7f\u5f97\u7814\u7a76\u56fe\u5220\u9664\u4e2d\u7684\u516c\u5e73\u6027\u5c24\u4e3a\u91cd\u8981\u3002\u8fd9\u5f15\u53d1\u4e86\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1a\u56fe\u5220\u9664\u662f\u5426\u4f1a\u5f15\u5165\u504f\u89c1\uff1f", "method": "\u4e3a\u4e86\u4fdd\u8bc1\u9690\u79c1\uff0cFGU\u5728\u5206\u5272\u7684\u5b50\u56fe\u4e0a\u8bad\u7ec3\u788e\u7247\u6a21\u578b\uff0c\u4ece\u76f8\u5e94\u7684\u5b50\u56fe\u4e2d\u5220\u9664\u8bf7\u6c42\u7684\u6570\u636e\uff0c\u5e76\u5728\u4fee\u6539\u540e\u7684\u5b50\u56fe\u4e0a\u91cd\u65b0\u8bad\u7ec3\u788e\u7247\u6a21\u578b\u3002\u4e3a\u4e86\u786e\u4fdd\u516c\u5e73\u6027\uff0cFGU\u91c7\u7528\u4e86\u4e00\u4e2a\u53cc\u5c42\u53bb\u504f\u8fc7\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u5728\u788e\u7247\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u4e2d\u5f15\u5165\u516c\u5e73\u6b63\u5219\u5316\u6765\u5b9e\u73b0\u788e\u7247\u7ea7\u516c\u5e73\u6027\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u9f50\u6240\u6709\u788e\u7247\u6a21\u578b\u4ee5\u6700\u5c0f\u5316\u5168\u5c40\u5dee\u5f02\u6765\u5b9e\u73b0\u5168\u5c40\u7ea7\u516c\u5e73\u6027\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5220\u9664\u540e\u7684\u6a21\u578b\u7684\u9884\u6d4b\u4e0e\u654f\u611f\u5c5e\u6027\u9ad8\u5ea6\u76f8\u5173\uff0c\u8bc1\u5b9e\u4e86\u56fe\u5220\u9664\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u504f\u89c1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u7684\u56fe\u5220\u9664\u65b9\u6cd5FGU\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFGU\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u516c\u5e73\u6027\u3002\u6b64\u5916\uff0cFGU\u5bf9\u5404\u79cd\u5220\u9664\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u4e86\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.09737", "pdf": "https://arxiv.org/pdf/2505.09737", "abs": "https://arxiv.org/abs/2505.09737", "authors": ["Osher Elhadad", "Reuth Mirsky"], "title": "General Dynamic Goal Recognition", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted for publication at Generalization in Planning (GenPlan) as\n  part of AAAI 2025 workshops", "summary": "Understanding an agent's intent through its behavior is essential in\nhuman-robot interaction, interactive AI systems, and multi-agent\ncollaborations. This task, known as Goal Recognition (GR), poses significant\nchallenges in dynamic environments where goals are numerous and constantly\nevolving. Traditional GR methods, designed for a predefined set of goals, often\nstruggle to adapt to these dynamic scenarios. To address this limitation, we\nintroduce the General Dynamic GR problem - a broader definition of GR - aimed\nat enabling real-time GR systems and fostering further research in this area.\nExpanding on this foundation, this paper employs a model-free goal-conditioned\nRL approach to enable fast adaptation for GR across various changing tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e7f\u4e49\u52a8\u6001\u76ee\u6807\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u65e0\u6a21\u578b\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8de8\u53d8\u5316\u4efb\u52a1\u7684\u5feb\u901f\u76ee\u6807\u8bc6\u522b\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u76ee\u6807\u4f17\u591a\u4e14\u4e0d\u65ad\u53d8\u5316\uff0c\u4f20\u7edf\u7684\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u8fd9\u4e9b\u573a\u666f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5e7f\u6cdb\u7684\u76ee\u6807\u8bc6\u522b\u5b9a\u4e49\u548c\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u65e0\u6a21\u578b\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8de8\u5404\u79cd\u53d8\u5316\u4efb\u52a1\u7684\u76ee\u6807\u8bc6\u522b\u7684\u5feb\u901f\u9002\u5e94\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u7684\u53d8\u5316\u4efb\u52a1\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u76ee\u6807\u8bc6\u522b\u3002", "conclusion": "\u672c\u6587\u5f15\u5165\u4e86\u5e7f\u4e49\u52a8\u6001\u76ee\u6807\u8bc6\u522b\u95ee\u9898\uff0c\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u76ee\u6807\u8bc6\u522b\u7cfb\u7edf\u5e76\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.09738", "pdf": "https://arxiv.org/pdf/2505.09738", "abs": "https://arxiv.org/abs/2505.09738", "authors": ["Shaurya Sharthak", "Vinayak Pahalwan", "Adithya Kamath", "Adarsh Shirawalmath"], "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u8bcd\u5668\u79fb\u690d\u65b9\u6cd5TokenAdapt\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5206\u8bcd\u5668\u9501\u5b9a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u8bcd\u5668\u66ff\u6362\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u6b8b\u5dee\u5fae\u8c03\uff0c\u65e0\u6cd5\u5145\u5206\u4fdd\u7559\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u6216\u89e3\u51b3\u538b\u7f29\u6548\u7387\u95ee\u9898\u3002", "method": "TokenAdapt\u662f\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u5206\u8bcd\u5668\u79fb\u690d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u4f30\u8ba1\u548c\u5168\u5c40\u4f30\u8ba1\u6765\u521d\u59cb\u5316\u65b0\u7684\u552f\u4e00\u6807\u8bb0\u5d4c\u5165\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u591a\u8bcd\u8d85\u6807\u8bb0\u7684\u9884\u5206\u8bcd\u5b66\u4e60\u4ee5\u63d0\u9ad8\u538b\u7f29\u6548\u679c\u5e76\u51cf\u5c11\u788e\u7247\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86TokenAdapt\u7684\u6709\u6548\u6027\uff0c\u5176\u5728\u96f6\u6837\u672c\u56f0\u60d1\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8eReTok\u548cTransTokenizer\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u65b0\u8bad\u7ec3\u7684\u76ee\u6807\u5206\u8bcd\u5668\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TokenAdapt\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u5206\u8bcd\u5668\u9501\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.09858", "pdf": "https://arxiv.org/pdf/2505.09858", "abs": "https://arxiv.org/abs/2505.09858", "authors": ["Danush Kumar Venkatesh", "Isabel Funke", "Micha Pfeiffer", "Fiona Kolbinger", "Hanna Maria Schmeiser", "Juergen Weitz", "Marius Distler", "Stefanie Speidel"], "title": "Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models", "categories": ["cs.CV"], "comment": "Early accept at MICCAI 2025", "summary": "Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u6761\u4ef6\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u624b\u672f\u89c6\u9891\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u5e38\u5e38\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u6027\u80fd\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u5408\u6210\u624b\u672f\u89c6\u9891\u6765\u514b\u670d\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7279\u7684\u4e24\u9636\u6bb5\u3001\u6587\u672c\u6761\u4ef6\u6269\u6563\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u624b\u672f\u89c6\u9891\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5c06\u751f\u6210\u8fc7\u7a0b\u6761\u4ef6\u5316\u4e3a\u6587\u672c\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u4f7f\u75282D\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6355\u6349\u7a7a\u95f4\u5185\u5bb9\uff0c\u7136\u540e\u96c6\u6210\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u4ee5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u62d2\u7edd\u91c7\u6837\u7b56\u7565\u6765\u9009\u62e9\u6700\u5408\u9002\u7684\u5408\u6210\u6837\u672c\uff0c\u4ee5\u6709\u6548\u589e\u5f3a\u73b0\u6709\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6211\u4eec\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u2014\u2014\u624b\u672f\u52a8\u4f5c\u8bc6\u522b\u548c\u672f\u4e2d\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u6211\u4eec\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u89c6\u9891\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8868\u660e\u4f7f\u7528\u6211\u4eec\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u89c6\u9891\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.09704", "pdf": "https://arxiv.org/pdf/2505.09704", "abs": "https://arxiv.org/abs/2505.09704", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Charalampos Kalalas", "Paolo Dini"], "title": "Energy-Efficient Federated Learning for AIoT using Clustering Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While substantial research has been devoted to optimizing model performance,\nconvergence rates, and communication efficiency, the energy implications of\nfederated learning (FL) within Artificial Intelligence of Things (AIoT)\nscenarios are often overlooked in the existing literature. This study examines\nthe energy consumed during the FL process, focusing on three main\nenergy-intensive processes: pre-processing, communication, and local learning,\nall contributing to the overall energy footprint. We rely on the observation\nthat device/client selection is crucial for speeding up the convergence of\nmodel training in a distributed AIoT setting and propose two\nclustering-informed methods. These clustering solutions are designed to group\nAIoT devices with similar label distributions, resulting in clusters composed\nof nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity\noften encountered in real-world distributed learning applications. Throughout\nextensive numerical experimentation, we demonstrate that our clustering\nstrategies typically achieve high convergence rates while maintaining low\nenergy consumption when compared to other recent approaches available in the\nliterature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u4eba\u5de5\u667a\u80fd\u7269\u8054\u7f51\u573a\u666f\u4e2d\u7684\u80fd\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u4e2d\u5e38\u5e38\u5ffd\u89c6\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u4eba\u5de5\u667a\u80fd\u7269\u8054\u7f51\uff08AIoT\uff09\u573a\u666f\u4e2d\u7684\u80fd\u6e90\u5f71\u54cd\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u7814\u7a76FL\u8fc7\u7a0b\u4e2d\u7684\u80fd\u8017\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6848\uff0c\u65e8\u5728\u901a\u8fc7\u5c06\u5177\u6709\u76f8\u4f3c\u6807\u7b7e\u5206\u5e03\u7684AIoT\u8bbe\u5907\u5206\u7ec4\uff0c\u4ece\u800c\u51cf\u8f7b\u5b9e\u9645\u5206\u5e03\u5f0f\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u672c\u6587\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u805a\u7c7b\u7b56\u7565\u5728\u4e0e\u5176\u4ed6\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\u65f6\uff0c\u901a\u5e38\u80fd\u591f\u5b9e\u73b0\u8f83\u9ad8\u7684\u6536\u655b\u901f\u5ea6\u5e76\u4fdd\u6301\u8f83\u4f4e\u7684\u80fd\u8017\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u540c\u65f6\uff0c\u901a\u5e38\u80fd\u5b9e\u73b0\u8f83\u9ad8\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u4e86\u5b9e\u9645\u5206\u5e03\u5f0f\u5b66\u4e60\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002"}}
{"id": "2505.09755", "pdf": "https://arxiv.org/pdf/2505.09755", "abs": "https://arxiv.org/abs/2505.09755", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "title": "Explainability Through Human-Centric Design for XAI in Lung Cancer Detection", "categories": ["cs.AI"], "comment": null, "summary": "Deep learning models have shown promise in lung pathology detection from\nchest X-rays, but widespread clinical adoption remains limited due to opaque\nmodel decision-making. In prior work, we introduced ClinicXAI, a human-centric,\nexpert-guided concept bottleneck model (CBM) designed for interpretable lung\ncancer diagnosis. We now extend that approach and present XpertXAI, a\ngeneralizable expert-driven model that preserves human-interpretable clinical\nconcepts while scaling to detect multiple lung pathologies. Using a\nhigh-performing InceptionV3-based classifier and a public dataset of chest\nX-rays with radiology reports, we compare XpertXAI against leading post-hoc\nexplainability methods and an unsupervised CBM, XCBs. We assess explanations\nthrough comparison with expert radiologist annotations and medical ground\ntruth. Although XpertXAI is trained for multiple pathologies, our expert\nvalidation focuses on lung cancer. We find that existing techniques frequently\nfail to produce clinically meaningful explanations, omitting key diagnostic\nfeatures and disagreeing with radiologist judgments. XpertXAI not only\noutperforms these baselines in predictive accuracy but also delivers\nconcept-level explanations that better align with expert reasoning. While our\nfocus remains on explainability in lung cancer detection, this work illustrates\nhow human-centric model design can be effectively extended to broader\ndiagnostic contexts - offering a scalable path toward clinically meaningful\nexplainable AI in medical diagnostics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86XpertXAI\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u80ba\u90e8\u75c5\u7406\u68c0\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4e0e\u4e13\u5bb6\u63a8\u7406\u5bf9\u9f50\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u8bca\u65ad\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u80f8\u90e8X\u5149\u7247\u7684\u80ba\u90e8\u75c5\u7406\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u4e34\u5e8a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u5230\u6a21\u578b\u51b3\u7b56\u4e0d\u900f\u660e\u7684\u9650\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u6765\u63d0\u9ad8\u4e34\u5e8a\u91c7\u7eb3\u7387\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86XpertXAI\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eInceptionV3\u7684\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u4e86\u4e13\u5bb6\u6307\u5bfc\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\uff0c\u4ee5\u4fdd\u6301\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u6982\u5ff5\u5e76\u6269\u5c55\u5230\u68c0\u6d4b\u591a\u79cd\u80ba\u90e8\u75c5\u7406\u3002", "result": "XpertXAI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0e\u4e13\u5bb6\u63a8\u7406\u66f4\u4e00\u81f4\u7684\u6982\u5ff5\u7ea7\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u4ee5\u4eba\u4e3a\u672c\u7684\u6a21\u578b\u8bbe\u8ba1\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u8bca\u65ad\u573a\u666f\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86XpertXAI\u6a21\u578b\u5728\u80ba\u90e8\u75c5\u7406\u68c0\u6d4b\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684\u6a21\u578b\u8bbe\u8ba1\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.09794", "pdf": "https://arxiv.org/pdf/2505.09794", "abs": "https://arxiv.org/abs/2505.09794", "authors": ["J. Moreno-Casanova", "J. M. Au\u00f1\u00f3n", "A. M\u00e1rtinez-P\u00e9rez", "M. E. P\u00e9rez-Mart\u00ednez", "M. E. Gas-L\u00f3pez"], "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6280\u672f\uff0c\u7279\u522b\u662f\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\uff0c\u6765\u81ea\u52a8\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u4e0e\u80ba\u764c\u548c\u4e73\u817a\u764c\u76f8\u5173\u7684\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\u3002\u7814\u7a76\u5229\u7528\u4e86uQuery\u5de5\u5177\u548c\u5fae\u8c03\u540e\u7684bsc-bio-ehr-en3\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793aNLP\u6280\u672f\u5728\u8bc6\u522b\u67d0\u4e9b\u5b9e\u4f53\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u624b\u52a8\u4ece\u4e34\u5e8a\u62a5\u544a\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u8fc7\u7a0b\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u51fa\u9519\uff0c\u9650\u5236\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u6548\u7387\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6570\u636e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86GMV\u7684NLP\u5de5\u5177uQuery\u6765\u8bc6\u522b\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u76f8\u5173\u5b9e\u4f53\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u683c\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u57fa\u4e8eRoBERTa\u7684\u751f\u7269\u533b\u5b66\u8bed\u8a00\u6a21\u578bbsc-bio-ehr-en3\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4ee5\u6267\u884c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cNLP\u6280\u672f\u5728\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u5982MET\u548cPAT\u7b49\u5b9e\u4f53\u65b9\u9762\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8f83\u5c11\u89c1\u7684\u5b9e\u4f53\u5982EVOL\uff0c\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cNLP\u6280\u672f\u5728\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u5982MET\u548cPAT\u7b49\u5b9e\u4f53\u65b9\u9762\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8f83\u5c11\u89c1\u7684\u5b9e\u4f53\u5982EVOL\uff0c\u4ecd\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2505.09859", "pdf": "https://arxiv.org/pdf/2505.09859", "abs": "https://arxiv.org/abs/2505.09859", "authors": ["Andrew Jun Lee", "Taylor Webb", "Trevor Bihl", "Keith Holyoak", "Hongjing Lu"], "title": "Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction", "categories": ["cs.CV"], "comment": "Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society", "summary": "The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6982\u7387\u6a21\u5f0f\u5f52\u7eb3\uff08PSI\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5728\u5c11\u91cf\u793a\u4f8b\u7684\u7ed3\u6784\u5316\u8868\u793a\u4e0a\u6267\u884c\u7c7b\u6bd4\u6620\u5c04\u7684\u539f\u578b\u6a21\u578b\uff0c\u4ece\u800c\u5f62\u6210\u7ec4\u5408\u6982\u5ff5\u3002PSI\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5b66\u4e60\u8868\u73b0\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u4e24\u79cd\u6a21\u578b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u6784\u5316\u8868\u793a\u548c\u7c7b\u6bd4\u6620\u5c04\u5bf9\u4e8e\u5efa\u6a21\u5feb\u901f\u7684\u4eba\u7c7b\u7c7b\u4f3c\u5b66\u4e60\u7ec4\u5408\u89c6\u89c9\u6982\u5ff5\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5b66\u4e60\u4ece\u6709\u9650\u7684\u4f8b\u5b50\u4e2d\u5b66\u4e60\u65b0\u7684\u89c6\u89c9\u6982\u5ff5\u662f\u4eba\u7c7b\u8ba4\u77e5\u7684\u7279\u5f81\u3002\u4f20\u7edf\u7684\u7c7b\u522b\u5b66\u4e60\u6a21\u578b\u5c06\u6bcf\u4e2a\u4f8b\u5b50\u8868\u793a\u4e3a\u65e0\u7ed3\u6784\u7684\u7279\u5f81\u5411\u91cf\uff0c\u800c\u7ec4\u5408\u6982\u5ff5\u5b66\u4e60\u88ab\u8ba4\u4e3a\u4f9d\u8d56\u4e8e\uff081\uff09\u4f8b\u5b50\u7684\u7ed3\u6784\u5316\u8868\u793a\uff08\u4f8b\u5982\uff0c\u7531\u5bf9\u8c61\u53ca\u5176\u5173\u7cfb\u7ec4\u6210\u7684\u6709\u5411\u56fe\uff09\u548c\uff082\uff09\u901a\u8fc7\u7c7b\u6bd4\u6620\u5c04\u8bc6\u522b\u8de8\u4f8b\u5b50\u7684\u5171\u4eab\u5173\u7cfb\u7ed3\u6784\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u6982\u7387\u6a21\u5f0f\u5f52\u7eb3\uff08PSI\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u539f\u578b\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5728\u4ec5\u5c11\u6570\u793a\u4f8b\u7684\u7ed3\u6784\u5316\u8868\u793a\u4e0a\u6267\u884c\u7c7b\u6bd4\u6620\u5c04\uff0c\u5f62\u6210\u4e00\u4e2a\u79f0\u4e3a\u6a21\u5f0f\u7684\u7ec4\u5408\u6982\u5ff5\u3002PSI\u4f9d\u8d56\u4e8e\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u4f3c\u6027\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u6743\u8861\u5bf9\u8c61\u7ea7\u76f8\u4f3c\u6027\u548c\u5173\u7cfb\u76f8\u4f3c\u6027\uff0c\u5e76\u4e14\u6709\u4e00\u4e2a\u673a\u5236\u53ef\u4ee5\u653e\u5927\u5bf9\u5206\u7c7b\u76f8\u5173\u7684\u5173\u7cfb\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edf\u6a21\u578b\u4e2d\u7684\u9009\u62e9\u6027\u6ce8\u610f\u53c2\u6570\u3002", "result": "PSI\u4ea7\u751f\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5b66\u4e60\u8868\u73b0\uff0c\u5e76\u4f18\u4e8e\u4e24\u4e2a\u5bf9\u7167\u7ec4\uff1a\u4e00\u4e2a\u4f7f\u7528\u4ece\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u65e0\u7ed3\u6784\u7279\u5f81\u5411\u91cf\u7684\u539f\u578b\u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7ed3\u6784\u5316\u8868\u793a\u8f83\u5f31\u7684PSI\u53d8\u4f53\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0PSI\u7684\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8868\u73b0\u662f\u7531\u4e00\u79cd\u9002\u5e94\u6027\u7b56\u7565\u9a71\u52a8\u7684\uff0c\u8be5\u7b56\u7565\u589e\u52a0\u4e86\u5173\u7cfb\u76f8\u4f3c\u6027\u76f8\u5bf9\u4e8e\u5bf9\u8c61\u7ea7\u76f8\u4f3c\u6027\u7684\u6743\u91cd\uff0c\u5e76\u63d0\u9ad8\u4e86\u533a\u5206\u7c7b\u522b\u7684\u5173\u7cfb\u7684\u8d21\u732e\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u7ed3\u6784\u5316\u8868\u793a\u548c\u7c7b\u6bd4\u6620\u5c04\u5bf9\u4e8e\u5efa\u6a21\u5feb\u901f\u7684\u4eba\u7c7b\u7c7b\u4f3c\u5b66\u4e60\u7ec4\u5408\u89c6\u89c9\u6982\u5ff5\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6765\u521b\u5efa\u5fc3\u7406\u6a21\u578b\u3002"}}
{"id": "2505.09710", "pdf": "https://arxiv.org/pdf/2505.09710", "abs": "https://arxiv.org/abs/2505.09710", "authors": ["Konstantinos Fotopoulos", "Petros Maragos"], "title": "Training Deep Morphological Neural Networks as Universal Approximators", "categories": ["cs.LG"], "comment": null, "summary": "We investigate deep morphological neural networks (DMNNs). We demonstrate\nthat despite their inherent non-linearity, activations between layers are\nessential for DMNNs. We then propose several new architectures for DMNNs, each\nwith a different constraint on their parameters. For the first (resp. second)\narchitecture, we work under the constraint that the majority of parameters\n(resp. learnable parameters) should be part of morphological operations. We\nempirically show that our proposed networks can be successfully trained, and\nare more prunable than linear networks. To the best of our knowledge, we are\nthe first to successfully train DMNNs under such constraints, although the\ngeneralization capabilities of our networks remain limited. Finally, we propose\na hybrid network architecture combining linear and morphological layers,\nshowing empirically that the inclusion of morphological layers significantly\naccelerates the convergence of gradient descent with large batches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b0\u7684\u67b6\u6784\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u53ef\u8bad\u7ec3\u6027\u548c\u53ef\u526a\u679d\u6027\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u548c\u5f62\u6001\u5c42\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u67b6\u6784\u80fd\u52a0\u901f\u5927\u89c4\u6a21\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u4e86\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\uff0c\u5e76\u8bc1\u660e\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u5185\u5728\u7684\u975e\u7ebf\u6027\uff0c\u4f46\u5c42\u95f4\u6fc0\u6d3b\u5bf9\u4e8eDMNNs\u662f\u5fc5\u8981\u7684\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u7684DMNN\u67b6\u6784\uff0c\u6bcf\u79cd\u67b6\u6784\u90fd\u6709\u4e0d\u540c\u7684\u53c2\u6570\u7ea6\u675f\u3002\u7b2c\u4e00\uff08\u6216\u7b2c\u4e8c\uff09\u67b6\u6784\u4e0b\uff0c\u6211\u4eec\u5047\u8bbe\u5927\u591a\u6570\u53c2\u6570\uff08\u6216\u53ef\u5b66\u4e60\u53c2\u6570\uff09\u5e94\u5c5e\u4e8e\u5f62\u6001\u64cd\u4f5c\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u6210\u529f\u8bad\u7ec3\uff0c\u5e76\u4e14\u6bd4\u7ebf\u6027\u7f51\u7edc\u66f4\u5bb9\u6613\u526a\u679d\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u5728\u8fd9\u4e9b\u7ea6\u675f\u4e0b\u6210\u529f\u8bad\u7ec3DMNNs\u7684\u56e2\u961f\uff0c\u5c3d\u7ba1\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u7ebf\u6027\u548c\u5f62\u6001\u5c42\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5f62\u6001\u5c42\u7684\u5f15\u5165\u663e\u8457\u52a0\u901f\u4e86\u5927\u89c4\u6a21\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u3002"}}
{"id": "2505.09787", "pdf": "https://arxiv.org/pdf/2505.09787", "abs": "https://arxiv.org/abs/2505.09787", "authors": ["Ziruo Yi", "Ting Xiao", "Mark V. Albert"], "title": "A Multimodal Multi-Agent Framework for Radiology Report Generation", "categories": ["cs.AI"], "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce diagnostic\nreports from medical images, with the potential to enhance clinical workflows\nand reduce radiologists' workload. While recent approaches leveraging\nmultimodal large language models (MLLMs) and retrieval-augmented generation\n(RAG) have achieved strong results, they continue to face challenges such as\nfactual inconsistency, hallucination, and cross-modal misalignment. We propose\na multimodal multi-agent framework for RRG that aligns with the stepwise\nclinical reasoning workflow, where task-specific agents handle retrieval, draft\ngeneration, visual analysis, refinement, and synthesis. Experimental results\ndemonstrate that our approach outperforms a strong baseline in both automatic\nmetrics and LLM-based evaluations, producing more accurate, structured, and\ninterpretable reports. This work highlights the potential of clinically aligned\nmulti-agent frameworks to support explainable and trustworthy clinical AI\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff0c\u4ee5\u89e3\u51b3\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u9519\u4f4d\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u81ea\u52a8\u6307\u6807\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u7684\u65b9\u6cd5\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u9762\u4e34\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u9519\u4f4d\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0e\u9010\u6b65\u7684\u4e34\u5e8a\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b\u76f8\u4e00\u81f4\uff0c\u5176\u4e2d\u7279\u5b9a\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u5904\u7406\u68c0\u7d22\u3001\u8349\u7a3f\u751f\u6210\u3001\u89c6\u89c9\u5206\u6790\u3001\u7cbe\u70bc\u548c\u7efc\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u6307\u6807\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u4ea7\u751f\u4e86\u66f4\u51c6\u786e\u3001\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u7684\u62a5\u544a\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u4e34\u5e8a\u5bf9\u9f50\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u652f\u6301\u53ef\u89e3\u91ca\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u4e34\u5e8aAI\u5e94\u7528\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09807", "pdf": "https://arxiv.org/pdf/2505.09807", "abs": "https://arxiv.org/abs/2505.09807", "authors": ["Timour Ichmoukhamedov", "David Martens"], "title": "Exploring the generalization of LLM truth directions on conversational formats", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u4e2d\u771f\u7406\u65b9\u5411\u5728\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6dfb\u52a0\u56fa\u5b9a\u5173\u952e\u8bcd\u77ed\u8bed\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6709\u6548\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6784\u5efa\u53ef\u9760LLM\u8c0e\u8a00\u68c0\u6d4b\u5668\u7684\u6311\u6218\u3002", "motivation": "\u6700\u8fd1\u7684\u4e00\u4e9b\u5de5\u4f5c\u8ba4\u4e3aLLM\u5177\u6709\u4e00\u4e2a\u666e\u904d\u7684\u771f\u7406\u65b9\u5411\uff0c\u5176\u4e2d\u771f\u5b9e\u548c\u865a\u5047\u7684\u9648\u8ff0\u5728\u6a21\u578b\u7684\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u662f\u7ebf\u6027\u53ef\u5206\u7684\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u771f\u7406\u65b9\u5411\u5728\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u7814\u7a76\u3002", "method": "\u6211\u4eec\u63a2\u7d22\u4e86\u8fd9\u79cd\u771f\u76f8\u65b9\u5411\u5728\u5404\u79cd\u5bf9\u8bdd\u683c\u5f0f\u4e4b\u95f4\u7684\u6cdb\u5316\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7\u5728\u6bcf\u6bb5\u5bf9\u8bdd\u7684\u672b\u5c3e\u6dfb\u52a0\u4e00\u4e2a\u56fa\u5b9a\u7684\u5173\u952e\u8bcd\u77ed\u8bed\u6765\u63d0\u51fa\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u4ee5\u8c0e\u8a00\u7ed3\u675f\u7684\u7b80\u77ed\u5bf9\u8bdd\u4e4b\u95f4\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u8f83\u957f\u7684\u5bf9\u8bdd\u683c\u5f0f\u4e2d\uff0c\u5f53\u8c0e\u8a00\u51fa\u73b0\u5728\u8f93\u5165\u63d0\u793a\u7684\u65e9\u671f\u65f6\uff0c\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2a\u56fa\u5b9a\u7684\u5173\u952e\u8bcd\u77ed\u8bed\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u663e\u8457\u63d0\u9ad8\u4e86\u8fd9\u79cd\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u51fa\u4e86\u5728\u53ef\u9760\u5730\u68c0\u6d4bLLM\u8c0e\u8a00\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u63a8\u5e7f\u5230\u65b0\u7684\u8bbe\u7f6e\u3002"}}
{"id": "2505.09915", "pdf": "https://arxiv.org/pdf/2505.09915", "abs": "https://arxiv.org/abs/2505.09915", "authors": ["Zhe Xin", "Chenyang Wu", "Penghui Huang", "Yanyong Zhang", "Yinian Mao", "Guoquan Huang"], "title": "Large-Scale Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.", "AI": {"tldr": "LSG-SLAM is a large-scale 3DGS-based visual SLAM that uses stereo cameras and improves performance in large-scale outdoor scenarios.", "motivation": "Most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored.", "method": "LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. It introduces feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. Continuous Gaussian Splatting submaps are used to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition, and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. A structure refinement module enhances the reconstruction quality after global optimization of camera poses and Gaussian points.", "result": "LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches.", "conclusion": "LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches."}}
{"id": "2505.09716", "pdf": "https://arxiv.org/pdf/2505.09716", "abs": "https://arxiv.org/abs/2505.09716", "authors": ["George Dimitriadis. Spyridon Samothrakis"], "title": "Out-of-distribution generalisation is hard: evidence from ARC-like tasks", "categories": ["cs.LG", "cs.AI"], "comment": "Submission to NeurIPS 2025", "summary": "Out-of-distribution (OOD) generalisation is considered a hallmark of human\nand animal intelligence. To achieve OOD through composition, a system must\ndiscover the environment-invariant properties of experienced input-output\nmappings and transfer them to novel inputs. This can be realised if an\nintelligent system can identify appropriate, task-invariant, and composable\ninput features, as well as the composition methods, thus allowing it to act\nbased not on the interpolation between learnt data points but on the\ntask-invariant composition of those features. We propose that in order to\nconfirm that an algorithm does indeed learn compositional structures from data,\nit is not enough to just test on an OOD setup, but one also needs to confirm\nthat the features identified are indeed compositional. We showcase this by\nexploring two tasks with clearly defined OOD metrics that are not OOD solvable\nby three commonly used neural networks: a Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and a Transformer. In addition, we develop\ntwo novel network architectures imbued with biases that allow them to be\nsuccessful in OOD scenarios. We show that even with correct biases and almost\nperfect OOD performance, an algorithm can still fail to learn the correct\nfeatures for compositional generalisation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u9a8c\u8bc1\u7b97\u6cd5\u662f\u5426\u771f\u6b63\u5b66\u4e60\u5230\u7ec4\u5408\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u4ee5\u63d0\u9ad8OOD\u6027\u80fd\uff0c\u4f46\u53d1\u73b0\u5373\u4f7f\u6709\u826f\u597d\u7684\u6027\u80fd\uff0c\u7b97\u6cd5\u4ecd\u53ef\u80fd\u65e0\u6cd5\u6b63\u786e\u5b66\u4e60\u7ec4\u5408\u7279\u5f81\u3002", "motivation": "\u4e3a\u4e86\u786e\u8ba4\u7b97\u6cd5\u662f\u5426\u771f\u6b63\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u4e86\u7ec4\u5408\u7ed3\u6784\uff0c\u4e0d\u4ec5\u9700\u8981\u5728OOD\u8bbe\u7f6e\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8fd8\u9700\u8981\u786e\u8ba4\u6240\u8bc6\u522b\u7684\u7279\u5f81\u786e\u5b9e\u662f\u7ec4\u5408\u6027\u7684\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u4e24\u4e2a\u5177\u6709\u660e\u786eOOD\u6307\u6807\u7684\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u4e09\u79cd\u5e38\u7528\u7684\u795e\u7ecf\u7f51\u7edc\uff08MLP\u3001CNN\u548cTransformer\uff09\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u4ee5\u63d0\u9ad8OOD\u6027\u80fd\u3002", "result": "\u5c55\u793a\u4e86\u5373\u4f7f\u5728\u6b63\u786e\u7684\u504f\u5dee\u548c\u63a5\u8fd1\u5b8c\u7f8e\u7684OOD\u6027\u80fd\u4e0b\uff0c\u7b97\u6cd5\u4ecd\u7136\u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60\u5230\u6b63\u786e\u7684\u7ec4\u5408\u7279\u5f81\u3002", "conclusion": "\u5373\u4f7f\u5728\u6b63\u786e\u7684\u504f\u5dee\u548c\u51e0\u4e4e\u5b8c\u7f8e\u7684OOD\u6027\u80fd\u4e0b\uff0c\u7b97\u6cd5\u4ecd\u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60\u5230\u6b63\u786e\u7684\u7279\u5f81\u4ee5\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u3002"}}
{"id": "2505.09920", "pdf": "https://arxiv.org/pdf/2505.09920", "abs": "https://arxiv.org/abs/2505.09920", "authors": ["Shan Yang", "Yongli Zhu"], "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "This paper presents a study on using different offline reinforcement learning\nalgorithms for microgrid voltage regulation with solar power penetration. When\nenvironment interaction is unviable due to technical or safety reasons, the\nproposed approach can still obtain an applicable model through offline-style\ntraining on a previously collected dataset, lowering the negative impact of\nlacking online environment interactions. Experiment results on the IEEE 33-bus\nsystem demonstrate the feasibility and effectiveness of the proposed approach\non different offline datasets, including the one with merely low-quality\nexperience.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e0d\u540c\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5fae\u7535\u7f51\u7535\u538b\u8c03\u8282\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u4ea4\u4e92\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e4b\u524d\u6536\u96c6\u7684\u6570\u636e\u96c6\u8fdb\u884c\u79bb\u7ebf\u8bad\u7ec3\u6765\u83b7\u5f97\u9002\u7528\u7684\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u5305\u62ec\u4ec5\u5305\u542b\u4f4e\u8d28\u91cf\u7ecf\u9a8c\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u7531\u4e8e\u6280\u672f\u548c\u5b89\u5168\u539f\u56e0\u65e0\u6cd5\u8fdb\u884c\u73af\u5883\u4ea4\u4e92\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u83b7\u5f97\u9002\u7528\u7684\u6a21\u578b\uff0c\u4ee5\u964d\u4f4e\u7f3a\u4e4f\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e0d\u540c\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5fae\u7535\u7f51\u7535\u538b\u8c03\u8282\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u4ea4\u4e92\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e4b\u524d\u6536\u96c6\u7684\u6570\u636e\u96c6\u8fdb\u884c\u79bb\u7ebf\u8bad\u7ec3\u6765\u83b7\u5f97\u9002\u7528\u7684\u6a21\u578b\u3002", "result": "\u5728IEEE 33-bus\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u4ec5\u5305\u542b\u4f4e\u8d28\u91cf\u7ecf\u9a8c\u7684\u6570\u636e\u96c6\u4e0a\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u5305\u62ec\u4ec5\u5305\u542b\u4f4e\u8d28\u91cf\u7ecf\u9a8c\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825", "abs": "https://arxiv.org/abs/2505.09825", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86KRISTEVA\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89e3\u91ca\u6027\u63a8\u7406\u7684\u7ec6\u8bfb\u57fa\u51c6\uff0c\u5305\u542b1331\u4e2a\u9009\u62e9\u9898\u3002\u901a\u8fc7\u4e09\u4e2a\u9010\u6b65\u589e\u52a0\u96be\u5ea6\u7684\u4efb\u52a1\u96c6\uff0c\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u5b66\u4f5c\u54c1\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u4eec\u7684\u8868\u73b0\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "motivation": "\u7ec6\u8bfb\u4f5c\u4e3a\u6279\u5224\u6027\u601d\u7ef4\u7684\u57fa\u7840\uff0c\u88ab\u5e7f\u6cdb\u91c7\u7528\u4e3a\u5927\u5b66\u8bfe\u7a0b\u7684\u5fc5\u4fee\u90e8\u5206\uff0c\u4f46\u4ece\u672a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u8fc7\u8bc4\u4f30\uff0c\u800c\u591a\u5b66\u79d1\u57fa\u51c6\u5982MMLU\u5e76\u4e0d\u5305\u62ec\u6587\u5b66\u4f5c\u4e3a\u4e3b\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86KRISTEVA\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89e3\u91ca\u6027\u63a8\u7406\u7684\u7ec6\u8bfb\u57fa\u51c6\uff0c\u5305\u542b\u4ece\u8bfe\u5802\u6570\u636e\u4e2d\u6539\u7f16\u76841331\u4e2a\u9009\u62e9\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u9010\u6b65\u589e\u52a0\u96be\u5ea6\u7684\u4efb\u52a1\u96c6\uff0c\u4ee5\u8fd1\u4f3c\u7ec6\u8bfb\u8fc7\u7a0b\u7684\u4e0d\u540c\u5143\u7d20\uff0c\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7406\u89e3\u5e76\u63a8\u7406\u6587\u5b66\u4f5c\u54c1\u3002", "result": "\u6211\u4eec\u7684\u57fa\u7ebf\u7ed3\u679c\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e00\u4e9b\u5927\u5b66\u6c34\u5e73\u7684\u7ec6\u8bfb\u80fd\u529b\uff08\u51c6\u786e\u738749.7% - 69.7%\uff09\uff0c\u4f46\u5b83\u4eec\u7684\u8868\u73b0\u4ecd\u843d\u540e\u4e8e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u8bc4\u4f30\u8005\u3002", "conclusion": "\u5c3d\u7ba1\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e00\u4e9b\u5927\u5b66\u6c34\u5e73\u7684\u7ec6\u8bfb\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u572811\u9879\u4efb\u52a1\u4e2d\u768410\u9879\u4e0a\u4ecd\u843d\u540e\u4e8e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u8bc4\u4f30\u8005\u3002"}}
{"id": "2505.09926", "pdf": "https://arxiv.org/pdf/2505.09926", "abs": "https://arxiv.org/abs/2505.09926", "authors": ["Bin-Bin Gao", "Yue Zhu", "Jiangtao Yan", "Yuezhi Cai", "Weixi Zhang", "Meng Wang", "Jun Liu", "Yong Liu", "Lei Wang", "Chengjie Wang"], "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 15 figures, 22 tables", "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.", "AI": {"tldr": "AdaptCLIP is a simple yet effective method for universal visual anomaly detection that outperforms existing methods by learning adaptive visual and textual representations alternately and incorporating both contextual and aligned residual features in comparative learning.", "motivation": "Existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios.", "method": "AdaptCLIP is a simple yet effective method based on two key insights: adaptive visual and textual representations should be learned alternately rather than jointly, and comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters at its input or output ends.", "result": "AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. It achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains.", "conclusion": "AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods."}}
{"id": "2505.09733", "pdf": "https://arxiv.org/pdf/2505.09733", "abs": "https://arxiv.org/abs/2505.09733", "authors": ["Alpaslan Gokcen", "Ali Boyaci"], "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u8d28\u91cf\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6e05\u6d17\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9c81\u68d2\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u4fdd\u6301\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u4fdd\u6301\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u9762\u4e34\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff08\u5982\u566a\u58f0\u6807\u7b7e\u3001\u7f3a\u5931\u7c7b\u522b\u548c\u4e0d\u5e73\u8861\u5206\u5e03\uff09\u7684\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u8054\u90a6\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u95ee\u9898\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u566a\u58f0\u6e05\u6d17\u3001\u57fa\u4e8e\u6761\u4ef6GAN\u7684\u534f\u4f5c\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9c81\u68d2\u7684\u8054\u90a6\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\uff08MNIST\u548cFashion-MNIST\uff09\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u4e0d\u540c\u566a\u58f0\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\uff0c\u8054\u90a6\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5b8fF1\u5206\u6570\u65b9\u9762\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8ba1\u7b97\u53ef\u884c\u6027\u548c\u663e\u8457\u6027\u80fd\u63d0\u5347\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u786e\u4fdd\u4e86\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5e38\u89c1\u7684\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u7684\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2505.09923", "pdf": "https://arxiv.org/pdf/2505.09923", "abs": "https://arxiv.org/abs/2505.09923", "authors": ["Minjung Shin", "Donghyun Kim", "Jeh-Kwang Ryu"], "title": "\"There Is No Such Thing as a Dumb Question,\" But There Are Good Ones", "categories": ["cs.AI"], "comment": "8 pages, 4 figures and 4 tables. This work has been accepted for\n  presentation as a poster with full paper publication at CogSci 2025. This is\n  the final submission", "summary": "Questioning has become increasingly crucial for both humans and artificial\nintelligence, yet there remains limited research comprehensively assessing\nquestion quality. In response, this study defines good questions and presents a\nsystematic evaluation framework. We propose two key evaluation dimensions:\nappropriateness (sociolinguistic competence in context) and effectiveness\n(strategic competence in goal achievement). Based on these foundational\ndimensions, a rubric-based scoring system was developed. By incorporating\ndynamic contextual variables, our evaluation framework achieves structure and\nflexibility through semi-adaptive criteria. The methodology was validated using\nthe CAUS and SQUARE datasets, demonstrating the ability of the framework to\naccess both well-formed and problematic questions while adapting to varied\ncontexts. As we establish a flexible and comprehensive framework for question\nevaluation, this study takes a significant step toward integrating questioning\nbehavior with structured analytical methods grounded in the intrinsic nature of\nquestioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u95ee\u9898\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u5305\u542b\u9002\u5f53\u6027\u548c\u6709\u6548\u6027\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u53d8\u91cf\u5b9e\u73b0\u4e86\u7075\u6d3b\u6027\u548c\u7ed3\u6784\u5316\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u95ee\u9898\u8d28\u91cf\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u5b9a\u4e49\u597d\u95ee\u9898\u5e76\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff1a\u9002\u5f53\u6027\uff08\u60c5\u5883\u4e2d\u7684\u793e\u4f1a\u8bed\u8a00\u80fd\u529b\uff09\u548c\u6709\u6548\u6027\uff08\u76ee\u6807\u5b9e\u73b0\u7684\u6218\u7565\u80fd\u529b\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bc4\u5206\u91cf\u8868\u7684\u7cfb\u7edf\u3002\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u4e0a\u4e0b\u6587\u53d8\u91cf\uff0c\u8bc4\u4f30\u6846\u67b6\u901a\u8fc7\u534a\u81ea\u9002\u5e94\u6807\u51c6\u5b9e\u73b0\u4e86\u7ed3\u6784\u548c\u7075\u6d3b\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728CAUS\u548cSQUARE\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u826f\u597d\u5f62\u6210\u548c\u5b58\u5728\u95ee\u9898\u7684\u95ee\u9898\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u5168\u9762\u7684\u95ee\u9898\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u5c06\u63d0\u95ee\u884c\u4e3a\u4e0e\u7ed3\u6784\u5316\u5206\u6790\u65b9\u6cd5\u76f8\u7ed3\u5408\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.09852", "pdf": "https://arxiv.org/pdf/2505.09852", "abs": "https://arxiv.org/abs/2505.09852", "authors": ["Apollinaire Poli Nemkova", "Sarath Chandra Lingareddy", "Sagnik Ray Choudhury", "Mark V. Albert"], "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51b2\u7a81\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u53d1\u73b0\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u53ef\u4ee5\u63d0\u9ad8\u5176\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5229\u7528\u5176\u9884\u8bad\u7ec3\u6743\u91cd\u4e2d\u7684\u53c2\u6570\u5316\u77e5\u8bc6\u6765\u9884\u6d4b\u51b2\u7a81\u5347\u7ea7\u548c\u6b7b\u4ea1\u4eba\u6570\uff0c\u8fd9\u5bf9\u4e8e\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3001\u4eba\u9053\u4e3b\u4e49\u89c4\u5212\u548c\u653f\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53c2\u6570\u5316\u548c\u975e\u53c2\u6570\u5316\u8bbe\u7f6e\u4e0b\u7684\u51b2\u7a81\u9884\u6d4b\u80fd\u529b\uff0c\u5176\u4e2d\u53c2\u6570\u5316\u8bbe\u7f6e\u4ec5\u4f9d\u8d56\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u800c\u975e\u53c2\u6570\u5316\u8bbe\u7f6e\u5219\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51b2\u7a81\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u4fe1\u606f\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51b2\u7a81\u9884\u6d4b\u4e2d\u65e2\u6709\u4f18\u52bf\u4e5f\u6709\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u7ed3\u6784\u5316\u5916\u90e8\u77e5\u8bc6\u7684\u589e\u5f3a\u53ef\u4ee5\u63d0\u9ad8\u5176\u6027\u80fd\u3002"}}
{"id": "2505.09927", "pdf": "https://arxiv.org/pdf/2505.09927", "abs": "https://arxiv.org/abs/2505.09927", "authors": ["Siqi Yin", "Shaolei Liu", "Manning Wang"], "title": "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u57df\u9002\u5e94\uff08SFDA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u9002\u5e94\u3001\u6570\u636e\u4f9d\u8d56\u7684\u9891\u7387\u63d0\u793a\u548c\u98ce\u683c\u76f8\u5173\u5c42\u5fae\u8c03\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57df\u95f4\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u6e90\u57df\u9002\u5e94\uff08SFDA\uff09\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u548c\u57df\u95f4\u5dee\u5f02\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u540c\u65f6\u5728\u6709\u9650\u76d1\u7763\u4e0b\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u6548\u7387\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u57df\u9002\u5e94\uff08SFDA\uff09\u6846\u67b6\uff0c\u5305\u62ec\u9884\u9002\u5e94\u9636\u6bb5\u751f\u6210\u9884\u9002\u5e94\u6a21\u578b\u3001\u6570\u636e\u4f9d\u8d56\u7684\u9891\u7387\u63d0\u793a\u4ee5\u66f4\u6709\u6548\u5730\u5c06\u76ee\u6807\u57df\u56fe\u50cf\u8f6c\u6362\u4e3a\u6e90\u57df\u98ce\u683c\uff0c\u4ee5\u53ca\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8eSFDA\u7684\u98ce\u683c\u76f8\u5173\u5c42\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5728\u8de8\u6a21\u6001\u8179\u90e8\u548c\u5fc3\u810fSFDA\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u672c\u6587\u63d0\u51fa\u7684\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u57df\u9002\u5e94\uff08SFDA\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u548c\u57df\u95f4\u5dee\u5f02\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u8179\u90e8\u548c\u5fc3\u810fSFDA\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.09742", "pdf": "https://arxiv.org/pdf/2505.09742", "abs": "https://arxiv.org/abs/2505.09742", "authors": ["Yuan-Hang Zhang", "Massimiliano Di Ventra"], "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.NE"], "comment": "15 pages, 3 figures", "summary": "We propose a generative, end-to-end solver for black-box combinatorial\noptimization that emphasizes both sample efficiency and solution quality on NP\nproblems. Drawing inspiration from annealing-based algorithms, we treat the\nblack-box objective as an energy function and train a neural network to model\nthe associated Boltzmann distribution. By conditioning on temperature, the\nnetwork captures a continuum of distributions--from near-uniform at high\ntemperatures to sharply peaked around global optima at low\ntemperatures--thereby learning the structure of the energy landscape and\nfacilitating global optimization. When queries are expensive, the\ntemperature-dependent distributions naturally enable data augmentation and\nimprove sample efficiency. When queries are cheap but the problem remains hard,\nthe model learns implicit variable interactions, effectively \"opening\" the\nblack box. We validate our approach on challenging combinatorial tasks under\nboth limited and unlimited query budgets, showing competitive performance\nagainst state-of-the-art black-box optimizers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u6027\u7684\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\uff0c\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u57fa\u4e8e\u9000\u706b\u7684\u7b97\u6cd5\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u6a21\u62df\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u3002\u901a\u8fc7\u6761\u4ef6\u6e29\u5ea6\uff0c\u7f51\u7edc\u6355\u6349\u5230\u4e86\u4ece\u9ad8\u6e29\u4e0b\u7684\u8fd1\u4f3c\u5747\u5300\u5206\u5e03\u5230\u4f4e\u6e29\u4e0b\u56f4\u7ed5\u5168\u5c40\u6700\u4f18\u7684\u5c16\u9510\u5206\u5e03\u7684\u8fde\u7eed\u5206\u5e03\uff0c\u4ece\u800c\u5b66\u4e60\u4e86\u80fd\u91cf\u666f\u89c2\u7684\u7ed3\u6784\u5e76\u4fc3\u8fdb\u4e86\u5168\u5c40\u4f18\u5316\u3002\u5f53\u67e5\u8be2\u6602\u8d35\u65f6\uff0c\u6e29\u5ea6\u4f9d\u8d56\u7684\u5206\u5e03\u81ea\u7136\u5730\u5b9e\u73b0\u4e86\u6570\u636e\u589e\u5f3a\u5e76\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff1b\u5f53\u67e5\u8be2\u4fbf\u5b9c\u4f46\u95ee\u9898\u4ecd\u7136\u56f0\u96be\u65f6\uff0c\u6a21\u578b\u5b66\u4e60\u9690\u5f0f\u53d8\u91cf\u4ea4\u4e92\uff0c\u6709\u6548\u5730\u201c\u6253\u5f00\u201d\u9ed1\u76d2\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u67e5\u8be2\u6602\u8d35\u65f6\uff0c\u6e29\u5ea6\u4f9d\u8d56\u7684\u5206\u5e03\u81ea\u7136\u5730\u5b9e\u73b0\u4e86\u6570\u636e\u589e\u5f3a\u5e76\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff1b\u5f53\u67e5\u8be2\u4fbf\u5b9c\u4f46\u95ee\u9898\u4ecd\u7136\u56f0\u96be\u65f6\uff0c\u6a21\u578b\u5b66\u4e60\u9690\u5f0f\u53d8\u91cf\u4ea4\u4e92\uff0c\u6709\u6548\u5730\u201c\u6253\u5f00\u201d\u9ed1\u76d2\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u6027\u7684\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\uff0c\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u57fa\u4e8e\u9000\u706b\u7684\u7b97\u6cd5\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u6a21\u62df\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u3002\u901a\u8fc7\u6761\u4ef6\u6e29\u5ea6\uff0c\u7f51\u7edc\u6355\u6349\u5230\u4e86\u4ece\u9ad8\u6e29\u4e0b\u7684\u8fd1\u4f3c\u5747\u5300\u5206\u5e03\u5230\u4f4e\u6e29\u4e0b\u56f4\u7ed5\u5168\u5c40\u6700\u4f18\u7684\u5c16\u9510\u5206\u5e03\u7684\u8fde\u7eed\u5206\u5e03\uff0c\u4ece\u800c\u5b66\u4e60\u4e86\u80fd\u91cf\u666f\u89c2\u7684\u7ed3\u6784\u5e76\u4fc3\u8fdb\u4e86\u5168\u5c40\u4f18\u5316\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6709\u9650\u548c\u65e0\u9650\u67e5\u8be2\u9884\u7b97\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u7ade\u4e89\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09932", "pdf": "https://arxiv.org/pdf/2505.09932", "abs": "https://arxiv.org/abs/2505.09932", "authors": ["Kevin J McNamara", "Rhea Pritham Marpu"], "title": "Demystifying AI Agents: The Final Generation of Intelligence", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.MA"], "comment": null, "summary": "The trajectory of artificial intelligence (AI) has been one of relentless\nacceleration, evolving from rudimentary rule-based systems to sophisticated,\nautonomous agents capable of complex reasoning and interaction. This whitepaper\nchronicles this remarkable journey, charting the key technological\nmilestones--advancements in prompting, training methodologies, hardware\ncapabilities, and architectural innovations--that have converged to create the\nAI agents of today. We argue that these agents, exemplified by systems like\nOpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in\nAI development, potentially constituting the \"final generation\" of intelligence\nas we currently conceive it. We explore the capabilities and underlying\ntechnologies of these agents, grounded in practical examples, while also\nexamining the profound societal implications and the unprecedented pace of\nprogress that suggests intelligence is now doubling approximately every six\nmonths. The paper concludes by underscoring the critical need for wisdom and\nforesight in navigating the opportunities and challenges presented by this\npowerful new era of intelligence.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5206\u6790\u4e86\u5176\u5173\u952e\u6280\u672f\u8fdb\u6b65\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u6f5c\u529b\u4e0e\u793e\u4f1a\u5f71\u54cd\uff0c\u540c\u65f6\u547c\u5401\u5728\u5feb\u901f\u53d1\u5c55\u7684\u6280\u672f\u73af\u5883\u4e2d\u4fdd\u6301\u667a\u6167\u548c\u8fdc\u89c1\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u73b0\u72b6\u53ca\u5176\u5bf9\u672a\u6765\u793e\u4f1a\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5feb\u901f\u53d1\u5c55\u7684\u6280\u672f\u73af\u5883\u4e2d\u4fdd\u6301\u8c28\u614e\u548c\u524d\u77bb\u6027\u7684\u91cd\u8981\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u56de\u987e\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5206\u6790\u4e86\u5173\u952e\u7684\u6280\u672f\u91cc\u7a0b\u7891\uff0c\u5982\u63d0\u793a\u6280\u672f\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u786c\u4ef6\u80fd\u529b\u548c\u67b6\u6784\u521b\u65b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u548c\u6f5c\u5728\u5f71\u54cd\u3002", "result": "\u672c\u6587\u6307\u51fa\uff0c\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\uff08\u5982ChatGPT\u548cGrok\uff09\u4ee3\u8868\u4e86\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u7684\u9876\u5cf0\u9636\u6bb5\uff0c\u53ef\u80fd\u6784\u6210\u6211\u4eec\u76ee\u524d\u6240\u7406\u89e3\u7684\u2018\u6700\u7ec8\u4e00\u4ee3\u2019\u667a\u80fd\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u63d0\u5230\u667a\u80fd\u7684\u8fdb\u6b65\u901f\u5ea6\u975e\u5e38\u5feb\uff0c\u5927\u7ea6\u6bcf\u516d\u4e2a\u6708\u7ffb\u4e00\u756a\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u9700\u8981\u667a\u6167\u548c\u8fdc\u89c1\u6765\u5e94\u5bf9\u6240\u5e26\u6765\u7684\u673a\u9047\u548c\u6311\u6218\u3002"}}
{"id": "2505.09902", "pdf": "https://arxiv.org/pdf/2505.09902", "abs": "https://arxiv.org/abs/2505.09902", "authors": ["Martin Capdevila", "Esteban Villa Turek", "Ellen Karina Chumbe Fernandez", "Luis Felipe Polo Galvez", "Luis Cadavid", "Andrea Marroquin", "Rebeca Vargas Quesada", "Johanna Crew", "Nicole Vallejo Galarraga", "Christopher Rodriguez", "Diego Gutierrez", "Radhi Datla"], "title": "Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are, by definition, based on language. In an effort to\nunderscore the critical need for regional localized models, this paper examines\nprimary differences between variants of written Spanish across Latin America\nand Spain, with an in-depth sociocultural and linguistic contextualization\ntherein. We argue that these differences effectively constitute significant\ngaps in the quotidian use of Spanish among dialectal groups by creating\nsociolinguistic dissonances, to the extent that locale-sensitive AI models\nwould play a pivotal role in bridging these divides. In doing so, this approach\ninforms better and more efficient localization strategies that also serve to\nmore adequately meet inclusivity goals, while securing sustainable active daily\nuser growth in a major low-risk investment geographic area. Therefore,\nimplementing at least the proposed five sub variants of Spanish addresses two\nlines of action: to foment user trust and reliance on AI language models while\nalso demonstrating a level of cultural, historical, and sociolinguistic\nawareness that reflects positively on any internationalization strategy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u62c9\u4e01\u7f8e\u6d32\u548c\u897f\u73ed\u7259\u4e4b\u95f4\u897f\u73ed\u7259\u8bed\u4e66\u9762\u5f62\u5f0f\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u533a\u57df\u672c\u5730\u5316\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u89e3\u51b3\u793e\u4f1a\u8bed\u8a00\u4e0d\u548c\u8c10\u95ee\u9898\uff0c\u5e76\u4fc3\u8fdb\u7528\u6237\u5bf9\u4eba\u5de5\u667a\u80fd\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u4efb\u3002", "motivation": "\u5f3a\u8c03\u533a\u57df\u672c\u5730\u5316\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5dee\u5f02\u5728\u65b9\u8a00\u7fa4\u4f53\u7684\u65e5\u5e38\u4f7f\u7528\u4e2d\u9020\u6210\u4e86\u663e\u8457\u7684\u7a7a\u767d\uff0c\u4ece\u800c\u4ea7\u751f\u793e\u4f1a\u8bed\u8a00\u4e0d\u548c\u8c10\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6df1\u5165\u7684\u793e\u4f1a\u6587\u5316\u53ca\u8bed\u8a00\u80cc\u666f\u5206\u6790\uff0c\u8003\u5bdf\u4e86\u62c9\u4e01\u7f8e\u6d32\u548c\u897f\u73ed\u7259\u4e4b\u95f4\u4e66\u9762\u897f\u73ed\u7259\u8bed\u7684\u4e3b\u8981\u5dee\u5f02\u3002", "result": "\u8fd9\u79cd\u505a\u6cd5\u6709\u52a9\u4e8e\u5236\u5b9a\u66f4\u597d\u7684\u672c\u5730\u5316\u7b56\u7565\uff0c\u66f4\u6709\u6548\u5730\u6ee1\u8db3\u5305\u5bb9\u6027\u76ee\u6807\uff0c\u5e76\u786e\u4fdd\u5728\u4e3b\u8981\u4f4e\u98ce\u9669\u6295\u8d44\u5730\u7406\u533a\u57df\u7684\u53ef\u6301\u7eed\u6d3b\u8dc3\u7528\u6237\u589e\u957f\u3002", "conclusion": "\u5b9e\u65bd\u81f3\u5c11\u63d0\u51fa\u7684\u4e94\u4e2a\u897f\u73ed\u7259\u8bed\u5b50\u53d8\u4f53\u53ef\u4ee5\u5b9e\u73b0\u4e24\u4e2a\u884c\u52a8\u65b9\u5411\uff1a\u4fc3\u8fdb\u7528\u6237\u5bf9\u4eba\u5de5\u667a\u80fd\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u4efb\u548c\u4f9d\u8d56\uff0c\u540c\u65f6\u5c55\u793a\u4e00\u79cd\u53cd\u6620\u56fd\u9645\u6218\u7565\u79ef\u6781\u5f62\u8c61\u7684\u6587\u5316\u3001\u5386\u53f2\u548c\u793e\u4f1a\u8bed\u8a00\u610f\u8bc6\u3002"}}
{"id": "2505.09935", "pdf": "https://arxiv.org/pdf/2505.09935", "abs": "https://arxiv.org/abs/2505.09935", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Quoc Dai Tran"], "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VRU-CIPI\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bVRU\u5728\u4ea4\u53c9\u53e3\u7684\u8fc7\u8857\u610f\u56fe\uff0c\u7ed3\u5408GRU\u548c\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u63a8\u7406\uff0c\u5e76\u901a\u8fc7I2V\u901a\u4fe1\u63d0\u9ad8\u4e86\u4ea4\u53c9\u53e3\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u7406\u89e3\u5e76\u9884\u6d4b\u57ce\u5e02\u4ea4\u53c9\u53e3\u4e2d\u4eba\u7c7b\u884c\u4e3a\u5bf9\u4e8e\u63d0\u9ad8\u9053\u8def\u4f7f\u7528\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002VRU\u7684\u8fc7\u8857\u610f\u56fe\u7684\u8bef\u5224\u53ef\u80fd\u5bfc\u81f4\u4e0e\u8f66\u8f86\u7684\u5371\u9669\u51b2\u7a81\u3002", "method": "VRU-CIPI\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u5e8f\u5217\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff0c\u5229\u7528GRU\u6355\u6349VRU\u8fd0\u52a8\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u7f16\u7801\u4e0a\u4e0b\u6587\u548c\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u9884\u6d4bVRU\u7684\u8fc7\u8857\u610f\u56fe\u3002", "result": "VRU-CIPI\u5728UCF-VRU\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8696.45%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u6bcf\u79d233\u5e27\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002\u901a\u8fc7\u4e0eI2V\u901a\u4fe1\u7684\u96c6\u6210\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u53ca\u65f6\u6fc0\u6d3b\u8fc7\u8857\u4fe1\u53f7\u548c\u5411\u8054\u7f51\u8f66\u8f86\u63d0\u4f9b\u65e9\u671f\u8b66\u544a\uff0c\u4e3b\u52a8\u589e\u5f3a\u4ea4\u53c9\u53e3\u7684\u5b89\u5168\u6027\u3002", "conclusion": "VRU-CIPI\u6846\u67b6\u901a\u8fc7\u7ed3\u5408GRU\u548c\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728UCF-VRU\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4e0eI2V\u901a\u4fe1\u7684\u96c6\u6210\uff0c\u63d0\u9ad8\u4e86\u4ea4\u53c9\u53e3\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2505.09756", "pdf": "https://arxiv.org/pdf/2505.09756", "abs": "https://arxiv.org/abs/2505.09756", "authors": ["Zhaoyang Shi"], "title": "Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration", "categories": ["cs.LG", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "We propose a new framework for multi-agent reinforcement learning (MARL),\nwhere the agents cooperate in a time-evolving network with latent community\nstructures and mixed memberships. Unlike traditional neighbor-based or fixed\ninteraction graphs, our community-based framework captures flexible and\nabstract coordination patterns by allowing each agent to belong to multiple\noverlapping communities. Each community maintains shared policy and value\nfunctions, which are aggregated by individual agents according to personalized\nmembership weights. We also design actor-critic algorithms that exploit this\nstructure: agents inherit community-level estimates for policy updates and\nvalue learning, enabling structured information sharing without requiring\naccess to other agents' policies. Importantly, our approach supports both\ntransfer learning by adapting to new agents or tasks via membership estimation,\nand active learning by prioritizing uncertain communities during exploration.\nTheoretically, we establish convergence guarantees under linear function\napproximation for both actor and critic updates. To our knowledge, this is the\nfirst MARL framework that integrates community structure, transferability, and\nactive learning with provable guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u793e\u533a\u7ed3\u6784\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u5e76\u5177\u6709\u53ef\u8bc1\u660e\u7684\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u7684\u65b9\u6cd5\u5728\u5904\u7406\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u7075\u6d3b\u5730\u6355\u6349\u534f\u8c03\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u534f\u4f5c\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793e\u533a\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u53ef\u4ee5\u5c5e\u4e8e\u591a\u4e2a\u91cd\u53e0\u7684\u793e\u533a\uff0c\u5e76\u4e14\u6bcf\u4e2a\u793e\u533a\u7ef4\u62a4\u5171\u4eab\u7684\u7b56\u7565\u548c\u4ef7\u503c\u51fd\u6570\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u7684actor-critic\u7b97\u6cd5\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u7ee7\u627f\u793e\u533a\u7ea7\u522b\u7684\u4f30\u8ba1\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\u548c\u4ef7\u503c\u5b66\u4e60\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u652f\u6301\u901a\u8fc7\u6210\u5458\u8d44\u683c\u4f30\u8ba1\u9002\u5e94\u65b0\u667a\u80fd\u4f53\u6216\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u4ee5\u53ca\u901a\u8fc7\u4f18\u5148\u8003\u8651\u4e0d\u786e\u5b9a\u7684\u793e\u533a\u8fdb\u884c\u4e3b\u52a8\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u7ebf\u6027\u51fd\u6570\u8fd1\u4f3c\u4e0b\uff0c\u6f14\u5458\u548c\u8bc4\u8bba\u5bb6\u7684\u66f4\u65b0\u90fd\u6709\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u793e\u533a\u7ed3\u6784\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u5e76\u5177\u6709\u53ef\u8bc1\u660e\u7684\u4fdd\u8bc1\u3002"}}
{"id": "2505.09970", "pdf": "https://arxiv.org/pdf/2505.09970", "abs": "https://arxiv.org/abs/2505.09970", "authors": ["Mrinal Rawat", "Ambuje Gupta", "Rushil Goomer", "Alessandro Di Bari", "Neha Gupta", "Roberto Pieraccini"], "title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has\nbecome the foundation of modern agentic systems. Recent LLMs, such as\nDeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through\nthe generation of ample intermediate tokens, which help build a strong premise\nbefore producing the final output tokens. In this paper, we introduce Pre-Act,\na novel approach that enhances the agent's performance by creating a multi-step\nexecution plan along with the detailed reasoning for the given user input. This\nplan incrementally incorporates previous steps and tool outputs, refining\nitself after each step execution until the final response is obtained. Our\napproach is applicable to both conversational and non-conversational agents. To\nmeasure the performance of task-oriented agents comprehensively, we propose a\ntwo-level evaluation framework: (1) turn level and (2) end-to-end. Our\nturn-level evaluation, averaged across five models, shows that our approach,\nPre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While\nthis approach is effective for larger models, smaller models crucial for\npractical applications, where latency and cost are key constraints, often\nstruggle with complex reasoning tasks required for agentic systems. To address\nthis limitation, we fine-tune relatively small models such as Llama 3.1 (8B &\n70B) using the proposed Pre-Act approach. Our experiments show that the\nfine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action\naccuracy (turn-level) and a 28% improvement in goal completion rate\n(end-to-end) on the Almita (out-of-domain) dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Pre-Act \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6b65\u9aa4\u6267\u884c\u8ba1\u5212\u6765\u63d0\u9ad8\u4ee3\u7406\u6027\u80fd\uff0c\u5e76\u5728 Almita \u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7684 ReAct \u65b9\u6cd5\u5728\u5927\u578b\u6a21\u578b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7531\u4e8e\u5ef6\u8fdf\u548c\u6210\u672c\u9650\u5236\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5 Pre-Act\uff0c\u901a\u8fc7\u521b\u5efa\u591a\u6b65\u9aa4\u6267\u884c\u8ba1\u5212\u548c\u8be6\u7ec6\u63a8\u7406\u6765\u589e\u5f3a\u4ee3\u7406\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u5c42\u8bc4\u4f30\u6846\u67b6\uff08\u56de\u5408\u7ea7\u548c\u7aef\u5230\u7aef\uff09\u3002", "result": "Pre-Act \u5728 Almita \u6570\u636e\u96c6\u4e0a\u7684 Action Recall \u6bd4 ReAct \u9ad8 70%\uff0c\u5fae\u8c03\u540e\u7684 70B \u6a21\u578b\u5728\u52a8\u4f5c\u51c6\u786e\u6027\u548c\u76ee\u6807\u5b8c\u6210\u7387\u4e0a\u4f18\u4e8e GPT-4\u3002", "conclusion": "Pre-Act \u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\uff0c\u901a\u8fc7\u5fae\u8c03\u53ef\u4ee5\u5b9e\u73b0\u6bd4 GPT-4 \u66f4\u597d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924", "abs": "https://arxiv.org/abs/2505.09924", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\n\\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u529f\u80fd\u7684\u5171\u751f\u6c34\u5370\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8elogits\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6848\uff0c\u4ee5\u4f18\u5316\u6c34\u5370\u7684\u53ef\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5174\u8d77\u52a0\u5267\u4e86\u5bf9AI\u751f\u6210\u6587\u672c\u6ee5\u7528\u7684\u62c5\u5fe7\uff0c\u4f7f\u6c34\u5370\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4e3b\u6d41\u7684LLM\u6c34\u5370\u65b9\u6848\u5206\u4e3a\u4e24\u7c7b\uff1a\u57fa\u4e8elogits\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6848\u3002\u7136\u800c\uff0c\u5f53\u524d\u65b9\u6848\u5728\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u5171\u751f\u6c34\u5370\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u7b56\u7565\uff1a\u4e32\u884c\u3001\u5e76\u884c\u548c\u6df7\u5408\u3002\u6df7\u5408\u6846\u67b6\u5229\u7528\u6807\u8bb0\u71b5\u548c\u8bed\u4e49\u71b5\u81ea\u9002\u5e94\u5730\u5d4c\u5165\u6c34\u5370\uff0c\u4f18\u5316\u4e86\u53ef\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "result": "\u901a\u8fc7\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u76f8\u4fe1\u8fd9\u4e2a\u6846\u67b6\u4e3a\u591a\u79cd\u6c34\u5370\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.09939", "pdf": "https://arxiv.org/pdf/2505.09939", "abs": "https://arxiv.org/abs/2505.09939", "authors": ["Zhe Shan", "Lei Zhou", "Liu Mao", "Shaofan Chen", "Chuanqiu Ren", "Xia Xie"], "title": "Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IGARSS 2025", "summary": "In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u516b\u4e2a\u73b0\u5b9e\u573a\u666f\u548c\u76f8\u5e94\u7684\u56fe\u50cf\u8f6c\u6362\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u8be5\u4efb\u52a1\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u6f5c\u5728\u5371\u5bb3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e5\u76ca\u589e\u591a\u7684\u7d27\u6025\u60c5\u51b5\uff08\u5982\u81ea\u7136\u707e\u5bb3\u3001\u4eba\u4e3a\u4e8b\u6545\u548c\u519b\u4e8b\u6253\u51fb\uff09\u5e26\u6765\u7684\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u516b\u4e2a\u53ef\u80fd\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u51fa\u73b0\u5e76\u53ef\u80fd\u5bfc\u81f4\u975e\u6ce8\u518c\u95ee\u9898\u7684\u573a\u666f\uff0c\u5e76\u5f00\u53d1\u4e86\u9488\u5bf9\u5404\u79cd\u573a\u666f\u7684\u4e0d\u540c\u56fe\u50cf\u8f6c\u6362\u65b9\u6848\uff0c\u4ee5\u5c06\u73b0\u6709\u7684\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u975e\u6ce8\u518c\u7248\u672c\u3002", "result": "\u5c55\u793a\u4e86\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u5bf9\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u53ef\u80fd\u9020\u6210\u7684\u707e\u96be\u6027\u635f\u5bb3\u3002", "conclusion": "\u975e\u6ce8\u518c\u53d8\u5316\u68c0\u6d4b\u53ef\u80fd\u5bfc\u81f4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9020\u6210\u707e\u96be\u6027\u635f\u5bb3\u3002"}}
{"id": "2505.09768", "pdf": "https://arxiv.org/pdf/2505.09768", "abs": "https://arxiv.org/abs/2505.09768", "authors": ["Xiukun Wei", "Xueru Zhang"], "title": "Self-Consuming Generative Models with Adversarially Curated Data", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in generative models have made it increasingly difficult to\ndistinguish real data from model-generated synthetic data. Using synthetic data\nfor successive training of future model generations creates \"self-consuming\nloops\", which may lead to model collapse or training instability. Furthermore,\nsynthetic data is often subject to human feedback and curated by users based on\ntheir preferences. Ferbach et al. (2024) recently showed that when data is\ncurated according to user preferences, the self-consuming retraining loop\ndrives the model to converge toward a distribution that optimizes those\npreferences. However, in practice, data curation is often noisy or\nadversarially manipulated. For example, competing platforms may recruit\nmalicious users to adversarially curate data and disrupt rival models. In this\npaper, we study how generative models evolve under self-consuming retraining\nloops with noisy and adversarially curated data. We theoretically analyze the\nimpact of such noisy data curation on generative models and identify conditions\nfor the robustness of the retraining process. Building on this analysis, we\ndesign attack algorithms for competitive adversarial scenarios, where a\nplatform with a limited budget employs malicious users to misalign a rival's\nmodel from actual user preferences. Experiments on both synthetic and\nreal-world datasets demonstrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10034", "pdf": "https://arxiv.org/pdf/2505.10034", "abs": "https://arxiv.org/abs/2505.10034", "authors": ["Changzeng Fu", "Zelin Fu", "Xinhe Kuang", "Jiacheng Dong", "Qi Zhang", "Kaifeng Su", "Yikai Su", "Wenbo Shi", "Junfeng Yao", "Yuliang Zhao", "Shiqi Zhao", "Jiadong Wang", "Siyang Song", "Chaoran Liu", "Yuichiro Yoshikawa", "Bj\u00f6rn Schuller", "Hiroshi Ishiguro"], "title": "The First MPDD Challenge: Multimodal Personality-aware Depression Detection", "categories": ["cs.AI", "68T07", "I.2.0; H.5.1"], "comment": "This paper has been accepted as part of the MPDD Challenge in the\n  ACMMM 2025 Grand Challenge", "summary": "Depression is a widespread mental health issue affecting diverse age groups,\nwith notable prevalence among college students and the elderly. However,\nexisting datasets and detection methods primarily focus on young adults,\nneglecting the broader age spectrum and individual differences that influence\ndepression manifestation. Current approaches often establish a direct mapping\nbetween multimodal data and depression indicators, failing to capture the\ncomplexity and diversity of depression across individuals. This challenge\nincludes two tracks based on age-specific subsets: Track 1 uses the\nMPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses\nthe MPDD-Young dataset for detecting depression in younger participants. The\nMultimodal Personality-aware Depression Detection (MPDD) Challenge aims to\naddress this gap by incorporating multimodal data alongside individual\ndifference factors. We provide a baseline model that fuses audio and video\nmodalities with individual difference information to detect depression\nmanifestations in diverse populations. This challenge aims to promote the\ndevelopment of more personalized and accurate de pression detection methods,\nadvancing mental health research and fostering inclusive detection systems.\nMore details are available on the official challenge website:\nhttps://hacilab.github.io/MPDDChallenge.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u4eba\u683c\u611f\u77e5\u6291\u90c1\u68c0\u6d4b\u6311\u6218\uff08MPDD\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u4e2a\u4f53\u5dee\u5f02\u56e0\u7d20\u6765\u6539\u5584\u6291\u90c1\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5e74\u8f7b\u4eba\u8eab\u4e0a\uff0c\u5ffd\u89c6\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e74\u9f84\u8303\u56f4\u548c\u5f71\u54cd\u6291\u90c1\u8868\u73b0\u7684\u4e2a\u4f53\u5dee\u5f02\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u878d\u5408\u4e86\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\u4e0e\u4e2a\u4f53\u5dee\u5f02\u4fe1\u606f\u4ee5\u68c0\u6d4b\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u6291\u90c1\u8868\u73b0\u3002", "result": "\u8be5\u6311\u6218\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u4e2a\u4f53\u5dee\u5f02\u56e0\u7d20\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6311\u6218\u65e8\u5728\u4fc3\u8fdb\u66f4\u4e2a\u6027\u5316\u548c\u51c6\u786e\u7684\u6291\u90c1\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u63a8\u52a8\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u5e76\u4fc3\u8fdb\u5305\u5bb9\u6027\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930", "abs": "https://arxiv.org/abs/2505.09930", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Prompt optimization (PO) offers a practical alternative to fine-tuning large\nlanguage models (LLMs), enabling performance improvements without altering\nmodel weights. Existing methods typically rely on advanced, large-scale LLMs\nlike GPT-4 to generate optimized prompts. However, due to limited downward\ncompatibility, verbose, instruction-heavy prompts from advanced LLMs can\noverwhelm lightweight inference models and degrade response quality. In this\nwork, we rethink prompt optimization through the lens of interpretable design.\nWe first identify a set of model-agnostic prompt quality merits and empirically\nvalidate their effectiveness in enhancing prompt and response quality. We then\nintroduce MePO, a merit-guided, lightweight, and locally deployable prompt\noptimizer trained on our preference dataset built from merit-aligned prompts\ngenerated by a lightweight LLM. Unlike prior work, MePO avoids online\noptimization reliance, reduces cost and privacy concerns, and, by learning\nclear, interpretable merits, generalizes effectively to both large-scale and\nlightweight inference models. Experiments demonstrate that MePO achieves better\nresults across diverse tasks and model types, offering a scalable and robust\nsolution for real-world deployment. Our model and dataset are available at:\nhttps://github.com/MidiyaZhu/MePO", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMePO\u7684\u8f7b\u91cf\u7ea7\u3001\u672c\u5730\u53ef\u90e8\u7f72\u7684\u63d0\u793a\u4f18\u5316\u5668\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u6e05\u6670\u3001\u53ef\u89e3\u91ca\u7684\u4f18\u70b9\uff0c\u6709\u6548\u5730\u6cdb\u5316\u5230\u5927\u89c4\u6a21\u548c\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMePO\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u50cfGPT-4\u8fd9\u6837\u7684\u5148\u8fdb\u3001\u5927\u89c4\u6a21\u7684LLM\u6765\u751f\u6210\u4f18\u5316\u7684\u63d0\u793a\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5411\u4e0b\u517c\u5bb9\u6027\u6709\u9650\uff0c\u6765\u81ea\u5148\u8fdbLLM\u7684\u5197\u957f\u3001\u6307\u4ee4\u5bc6\u96c6\u7684\u63d0\u793a\u53ef\u80fd\u4f1a\u4f7f\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u578b\u4e0d\u582a\u91cd\u8d1f\u5e76\u964d\u4f4e\u54cd\u5e94\u8d28\u91cf\u3002", "method": "\u6211\u4eec\u9996\u5148\u786e\u5b9a\u4e86\u4e00\u7ec4\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u63d0\u793a\u8d28\u91cf\u4f18\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u63d0\u9ad8\u63d0\u793a\u548c\u54cd\u5e94\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86MePO\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u4f18\u70b9\u5f15\u5bfc\u7684\u8f7b\u91cf\u7ea7\u3001\u672c\u5730\u53ef\u90e8\u7f72\u7684\u63d0\u793a\u4f18\u5316\u5668\uff0c\u5b83\u662f\u5728\u6211\u4eec\u4ece\u7531\u8f7b\u91cf\u7ea7LLM\u751f\u6210\u7684\u5bf9\u9f50\u4f18\u70b9\u63d0\u793a\u6784\u5efa\u7684\u504f\u597d\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u3002", "result": "MePO\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cMePO\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u53ef\u5728https://github.com/MidiyaZhu/MePO\u83b7\u53d6\u3002"}}
{"id": "2505.09943", "pdf": "https://arxiv.org/pdf/2505.09943", "abs": "https://arxiv.org/abs/2505.09943", "authors": ["Jiakun Deng", "Kexuan Li", "Xingye Cui", "Jiaxuan Li", "Chang Long", "Tian Pu", "Zhenming Peng"], "title": "CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u7edc\u7ed3\u6784CSPENet\uff0c\u7528\u4e8e\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u6539\u8fdb\u8f6e\u5ed3\u4fe1\u606f\u7684\u611f\u77e5\u548c\u76ee\u6807\u5b9a\u4f4d\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5bc6\u96c6\u6742\u6ce2\u73af\u5883\u4e0b\u7684\u5fae\u5f31\u76ee\u6807\u5b9a\u4f4d\u548c\u8f6e\u5ed3\u4fe1\u606f\u611f\u77e5\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f6e\u5ed3\u611f\u77e5\u548c\u663e\u8457\u6027\u5148\u9a8c\u5d4c\u5165\u7f51\u7edc\uff08CSPENet\uff09\uff0c\u5305\u62ec\u4e00\u4e2a\u73af\u7ed5\u6536\u655b\u5148\u9a8c\u63d0\u53d6\u6a21\u5757\uff08SCPEM\uff09\u3001\u4e00\u4e2a\u53cc\u5206\u652f\u5148\u9a8c\u5d4c\u5165\u67b6\u6784\uff08DBPEA\uff09\u548c\u4e00\u4e2a\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff08AGFEM\uff09\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6NUDT-SIRST\u3001IRSTD-1k\u548cNUAA-SIRST\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684CSPENet\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684CSPENet\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.09792", "pdf": "https://arxiv.org/pdf/2505.09792", "abs": "https://arxiv.org/abs/2505.09792", "authors": ["Michael Kamfonas"], "title": "Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This case study applies a phased hyperparameter optimization process to\ncompare multitask natural language model variants that utilize multiphase\nlearning rate scheduling and optimizer parameter grouping. We employ short,\nBayesian optimization sessions that leverage multi-fidelity, hyperparameter\nspace pruning, progressive halving, and a degree of human guidance. We utilize\nthe Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn\nGaussian process minimization. Initially, we use efficient low-fidelity sprints\nto prune the hyperparameter space. Subsequent sprints progressively increase\ntheir model fidelity and employ hyperband pruning for efficiency. A second\naspect of our approach is using a meta-learner to tune threshold values to\nresolve classification probabilities during inference. We demonstrate our\nmethod on a collection of variants of the 2021 Joint Entity and Relation\nExtraction model proposed by Eberts and Ulges.", "AI": {"tldr": "\u672c\u6848\u4f8b\u7814\u7a76\u5e94\u7528\u4e86\u5206\u9636\u6bb5\u7684\u8d85\u53c2\u6570\u4f18\u5316\u8fc7\u7a0b\uff0c\u6bd4\u8f83\u4e86\u4f7f\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u4f18\u5316\u5668\u53c2\u6570\u5206\u7ec4\u7684\u591a\u4efb\u52a1\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u3002\u6211\u4eec\u91c7\u7528\u77ed\u65f6\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u4f1a\u8bdd\uff0c\u5229\u7528\u591a\u4fdd\u771f\u5ea6\u3001\u8d85\u53c2\u6570\u7a7a\u95f4\u526a\u679d\u3001\u6e10\u8fdb\u5f0f\u51cf\u534a\u548c\u4e00\u5b9a\u7a0b\u5ea6\u7684\u4eba\u7c7b\u6307\u5bfc\u3002\u6211\u4eec\u4f7f\u7528\u4e86Optuna TPE\u91c7\u6837\u5668\u548cHyperband\u526a\u679d\u5668\uff0c\u4ee5\u53caScikit-Learn\u9ad8\u65af\u8fc7\u7a0b\u6700\u5c0f\u5316\u3002\u6211\u4eec\u901a\u8fc7\u9ad8\u6548\u4f4e\u4fdd\u771f\u5ea6\u7684\u51b2\u523a\u6765\u526a\u679d\u8d85\u53c2\u6570\u7a7a\u95f4\uff0c\u968f\u540e\u7684\u51b2\u523a\u9010\u6b65\u589e\u52a0\u6a21\u578b\u4fdd\u771f\u5ea6\u5e76\u4f7f\u7528Hyperband\u526a\u679d\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4f7f\u7528\u4e86\u5143\u5b66\u4e60\u5668\u6765\u8c03\u6574\u5206\u7c7b\u6982\u7387\u7684\u9608\u503c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u591a\u4efb\u52a1\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u9700\u8981\u6709\u6548\u5730\u4f18\u5316\u5176\u8d85\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u5143\u5b66\u4e60\u5668\u6765\u8c03\u6574\u5206\u7c7b\u6982\u7387\u7684\u9608\u503c\u3002", "method": "\u6211\u4eec\u5e94\u7528\u4e86\u5206\u9636\u6bb5\u7684\u8d85\u53c2\u6570\u4f18\u5316\u8fc7\u7a0b\uff0c\u6bd4\u8f83\u4e86\u4f7f\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u4f18\u5316\u5668\u53c2\u6570\u5206\u7ec4\u7684\u591a\u4efb\u52a1\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u3002\u6211\u4eec\u91c7\u7528\u4e86\u77ed\u65f6\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u4f1a\u8bdd\uff0c\u5229\u7528\u591a\u4fdd\u771f\u5ea6\u3001\u8d85\u53c2\u6570\u7a7a\u95f4\u526a\u679d\u3001\u6e10\u8fdb\u5f0f\u51cf\u534a\u548c\u4e00\u5b9a\u7a0b\u5ea6\u7684\u4eba\u7c7b\u6307\u5bfc\u3002\u6211\u4eec\u4f7f\u7528\u4e86Optuna TPE\u91c7\u6837\u5668\u548cHyperband\u526a\u679d\u5668\uff0c\u4ee5\u53caScikit-Learn\u9ad8\u65af\u8fc7\u7a0b\u6700\u5c0f\u5316\u3002", "result": "\u6211\u4eec\u901a\u8fc7\u9ad8\u6548\u4f4e\u4fdd\u771f\u5ea6\u7684\u51b2\u523a\u6765\u526a\u679d\u8d85\u53c2\u6570\u7a7a\u95f4\uff0c\u968f\u540e\u7684\u51b2\u523a\u9010\u6b65\u589e\u52a0\u6a21\u578b\u4fdd\u771f\u5ea6\u5e76\u4f7f\u7528Hyperband\u526a\u679d\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4f7f\u7528\u4e86\u5143\u5b66\u4e60\u5668\u6765\u8c03\u6574\u5206\u7c7b\u6982\u7387\u7684\u9608\u503c\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u57282021\u5e74Eberts\u548cUlges\u63d0\u51fa\u7684\u8054\u5408\u5b9e\u4f53\u548c\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u7684\u53d8\u4f53\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2505.10074", "pdf": "https://arxiv.org/pdf/2505.10074", "abs": "https://arxiv.org/abs/2505.10074", "authors": ["Mohamed Abdelmagied", "Mohamed Amine Chatti", "Shoeb Joarder", "Qurat Ul Ain", "Rawaa Alatrash"], "title": "Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs", "categories": ["cs.AI", "cs.CY"], "comment": "Accepted at EMOOCs 2025", "summary": "Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.", "AI": {"tldr": "This paper proposes a Graph RAG pipeline that uses Educational Knowledge Graphs and Personal Knowledge Graphs to help learners understand new knowledge concepts in MOOCs.", "motivation": "MOOCs lack direct interaction between learners and instructors, making it challenging for learners to understand new knowledge concepts. LLMs are prone to hallucinations which limits their reliability. Current RAG systems do not actively guide learners toward their learning needs.", "method": "We propose a Graph RAG pipeline that leverages Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide learners to understand knowledge concepts in the MOOC platform CourseMapper. Specifically, we implement (1) a PKG-based Question Generation method to recommend personalized questions for learners in context, and (2) an EduKG-based Question Answering method that leverages the relationships between knowledge concepts in the EduKG to answer learner selected questions.", "result": "The results of the evaluation show the potential of Graph RAG to empower learners to understand new knowledge concepts in a personalized learning experience.", "conclusion": "Graph RAG has the potential to empower learners to understand new knowledge concepts in a personalized learning experience."}}
{"id": "2505.09945", "pdf": "https://arxiv.org/pdf/2505.09945", "abs": "https://arxiv.org/abs/2505.09945", "authors": ["Deeksha Prahlad", "Chanhee Lee", "Dongha Kim", "Hokeun Kim"], "title": "Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)", "summary": "The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "LLMs\u7ecf\u5e38\u7531\u4e8e\u8fc7\u5ea6\u62df\u5408\u800c\u4ea7\u751f\u989d\u5916\u548c\u9519\u8bef\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u3002\u95ee\u9898\u7684\u4e00\u4e2a\u6839\u672c\u539f\u56e0\u662f\u7f3a\u4e4f\u53ca\u65f6\u3001\u4e8b\u5b9e\u6027\u548c\u4e2a\u6027\u5316\u4fe1\u606f\u8f93\u5165\u5230LLM\u4e2d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u6765\u8f85\u52a9LLM\u751f\u6210\u4e2a\u6027\u5316\u7684\u7528\u6237\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7406\u89e3\u4e2a\u4eba\u4fe1\u606f\u548c\u751f\u6210\u51c6\u786e\u54cd\u5e94\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u4e2a\u4eba\u6570\u636e\u4f5c\u4e3a\u6587\u672c\u8f93\u5165\u7684\u57fa\u7ebfLLMs\uff0c\u540c\u65f6\u54cd\u5e94\u65f6\u95f4\u6709\u9002\u5ea6\u51cf\u5c11\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7406\u89e3\u4e2a\u4eba\u4fe1\u606f\u548c\u751f\u6210\u51c6\u786e\u54cd\u5e94\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u4e2a\u4eba\u6570\u636e\u4f5c\u4e3a\u6587\u672c\u8f93\u5165\u7684\u57fa\u7ebfLLMs\uff0c\u540c\u65f6\u54cd\u5e94\u65f6\u95f4\u6709\u9002\u5ea6\u51cf\u5c11\u3002"}}
{"id": "2505.09965", "pdf": "https://arxiv.org/pdf/2505.09965", "abs": "https://arxiv.org/abs/2505.09965", "authors": ["Hao Yang", "Tao Tan", "Shuai Tan", "Weiqin Yang", "Kunyan Cai", "Calvin Chen", "Yue Sun"], "title": "MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.", "AI": {"tldr": "MambaControl\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Mamba-based\u957f\u7a0b\u5efa\u6a21\u4e0e\u56fe\u5f15\u5bfc\u7684\u89e3\u5256\u63a7\u5236\uff0c\u4ee5\u53ca\u5085\u91cc\u53f6\u589e\u5f3a\u7684\u9891\u8c31\u56fe\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u533b\u5b66\u56fe\u50cf\u8f68\u8ff9\u9884\u6d4b\uff0c\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7eb5\u5411\u4f9d\u8d56\u6027\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u6355\u6349\u8fdb\u884c\u6027\u75be\u75c5\u7684\u590d\u6742\u65f6\u7a7a\u52a8\u6001\u548c\u4fdd\u6301\u89e3\u5256\u5b8c\u6574\u6027\u3002", "method": "MambaControl\u7ed3\u5408\u4e86\u57fa\u4e8eMamba\u7684\u957f\u7a0b\u5efa\u6a21\u4e0e\u56fe\u5f15\u5bfc\u7684\u89e3\u5256\u63a7\u5236\uff0c\u4ee5\u53ca\u5085\u91cc\u53f6\u589e\u5f3a\u7684\u9891\u8c31\u56fe\u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u533b\u5b66\u56fe\u50cf\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5b9a\u91cf\u548c\u533a\u57df\u8bc4\u4f30\u663e\u793a\uff0cMambaControl\u5728\u8fdb\u5c55\u9884\u6d4b\u8d28\u91cf\u548c\u89e3\u5256\u4fdd\u771f\u5ea6\u65b9\u9762\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "MambaControl\u5c55\u793a\u4e86\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5177\u6709\u4e2a\u6027\u5316\u9884\u540e\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09810", "pdf": "https://arxiv.org/pdf/2505.09810", "abs": "https://arxiv.org/abs/2505.09810", "authors": ["Daniel Waddington", "Cornel Constantinescu"], "title": "Lossless Compression for LLM Tensor Incremental Snapshots", "categories": ["cs.LG"], "comment": null, "summary": "During the training of Large Language Models (LLMs), tensor data is\nperiodically \"checkpointed\" to persistent storage to allow recovery of work\ndone in the event of failure. The volume of data that must be copied during\neach checkpoint, even when using reduced-precision representations such as\nbfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be\nmoved across a network and written to a storage system before the next epoch\noccurs. With a view to ultimately building an optimized checkpointing solution,\nthis paper presents experimental analysis of checkpoint data used to derive a\ndesign that maximizes the use of lossless compression to reduce the volume of\ndata. We examine how tensor data and its compressibility evolve during model\ntraining and evaluate the efficacy of existing common off-the-shelf general\npurpose compression engines combined with known data optimization techniques\nsuch as byte-grouping and incremental delta compression.\n  Leveraging our analysis we have built an effective compression solution,\nknown as Language Model Compressor (LMC), which is based on byte-grouping and\nHuffman encoding. LMC offers more compression performance than the best\nalternative (BZ2) but with an order-of-magnitude reduction in the time needed\nto perform the compression. We show that a 16-core parallel implementation of\nLMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76\nGiB/s respectively. This increase in performance ultimately reduces the CPU\nresources needed and provides more time to copy the data to the storage system\nbefore the next epoch thus allowing for higher-frequency checkpoints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u79f0\u4e3aLanguage Model Compressor (LMC)\uff0c\u5b83\u57fa\u4e8e\u5b57\u8282\u5206\u7ec4\u548c\u970d\u592b\u66fc\u7f16\u7801\u3002LMC\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u538b\u7f29\u65f6\u95f4\u3002", "motivation": "\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u9700\u8981\u5c06\u5f20\u91cf\u6570\u636e\u5b9a\u671f\u201c\u68c0\u67e5\u70b9\u201d\u5230\u6301\u4e45\u5316\u5b58\u50a8\uff0c\u4ee5\u4fbf\u5728\u53d1\u751f\u6545\u969c\u65f6\u6062\u590d\u5de5\u4f5c\u3002\u7136\u800c\uff0c\u6bcf\u6b21\u68c0\u67e5\u70b9\u6240\u9700\u590d\u5236\u7684\u6570\u636e\u91cf\u5f88\u5927\uff0c\u751a\u81f3\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u8868\u793a\uff08\u5982bfloat16\uff09\u65f6\u4e5f\u53ef\u80fd\u8fbe\u5230\u6570\u767eGB\u3002\u6b64\u5916\uff0c\u6570\u636e\u5fc5\u987b\u5728\u7f51\u7edc\u4e0a\u4f20\u8f93\u5e76\u5199\u5165\u5b58\u50a8\u7cfb\u7edf\uff0c\u624d\u80fd\u5728\u4e0b\u4e00\u4e2aepoch\u5f00\u59cb\u524d\u5b8c\u6210\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u4f18\u5316\u7684\u68c0\u67e5\u70b9\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u68c0\u67e5\u70b9\u6570\u636e\uff0c\u4ee5\u5236\u5b9a\u6700\u5927\u5316\u4f7f\u7528\u65e0\u635f\u538b\u7f29\u7684\u65b9\u6848\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u73b0\u6709\u5e38\u89c1\u7684\u901a\u7528\u538b\u7f29\u5f15\u64ce\u4ee5\u53ca\u5df2\u77e5\u7684\u6570\u636e\u4f18\u5316\u6280\u672f\uff0c\u5982\u5b57\u8282\u5206\u7ec4\u548c\u589e\u91cf\u5dee\u5206\u538b\u7f29\u7684\u6548\u679c\u3002", "result": "LMC\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u4f73\u66ff\u4ee3\u65b9\u6848\uff08BZ2\uff09\uff0c\u4f46\u538b\u7f29\u6240\u9700\u65f6\u95f4\u51cf\u5c11\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u300216\u6838\u5e76\u884c\u5b9e\u73b0\u7684LMC\u53ef\u4ee5\u8fbe\u52302.78 GiB/s\u7684\u538b\u7f29\u541e\u5410\u91cf\u548c3.76 GiB/s\u7684\u89e3\u538b\u7f29\u541e\u5410\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u79f0\u4e3aLanguage Model Compressor (LMC)\uff0c\u5b83\u57fa\u4e8e\u5b57\u8282\u5206\u7ec4\u548c\u970d\u592b\u66fc\u7f16\u7801\u3002LMC\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u4f73\u66ff\u4ee3\u65b9\u6848\uff08BZ2\uff09\uff0c\u4f46\u538b\u7f29\u6240\u9700\u65f6\u95f4\u51cf\u5c11\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002"}}
{"id": "2505.10093", "pdf": "https://arxiv.org/pdf/2505.10093", "abs": "https://arxiv.org/abs/2505.10093", "authors": ["Hsuan-Lei Shao"], "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI", "categories": ["cs.AI", "cs.CL", "I.2.4; H.3.3; J.5"], "comment": "4 pages, 4 figures", "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u8f85\u52a9\u7684\u65b9\u6cd5\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u5b66\u672f\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u4ea4\u4e92\u5f0f\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u901a\u8fc7\u751f\u6210\u5f0fAI\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u548c\u6807\u51c6\u5316\u5b9e\u4f53\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u5e76\u5229\u7528D3.js\u7cfb\u7edf\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6784\u5efa\u4e86\u9886\u57df\u7279\u5b9a\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u5411\u91cf\u6570\u636e\u5e93\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4ece\u7ebf\u6027\u6587\u672c\u6d88\u8d39\u5230\u57fa\u4e8e\u7f51\u7edc\u7684\u77e5\u8bc6\u5bfc\u822a\u7684\u8f6c\u53d8\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5730\u91cd\u65b0\u5ba1\u89c6\u548c\u91cd\u65b0\u7ec4\u7ec7\u57fa\u4e8e\u53f0\u6e7e\u7684\u4e2d\u56fd\u7814\u7a76\u5b66\u672f\u6210\u679c\uff0c\u4ee5\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u8f85\u52a9\u7684\u65b9\u6cd5\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u5b66\u672f\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u4ea4\u4e92\u5f0f\u7684\u77e5\u8bc6\u8868\u793a\u3002\u6211\u4eec\u5e94\u7528\u4e86\u751f\u6210\u5f0fAI\uff08GAI\uff09\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u4ece1996\u5e74\u81f32019\u5e74\u95f4\u53d1\u8868\u76841,367\u7bc7\u540c\u884c\u8bc4\u5ba1\u7684\u4e2d\u56fd\u7814\u7a76\u6587\u7ae0\u4e2d\u63d0\u53d6\u548c\u6807\u51c6\u5316\u5b9e\u4f53\u5173\u7cfb\u4e09\u5143\u7ec4\u3002\u8fd9\u4e9b\u4e09\u5143\u7ec4\u901a\u8fc7\u57fa\u4e8eD3.js\u7684\u8f7b\u91cf\u7ea7\u7cfb\u7edf\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6784\u6210\u4e86\u8be5\u9886\u57df\u7684\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u548c\u5411\u91cf\u6570\u636e\u5e93\u7684\u57fa\u7840\u3002", "result": "\u901a\u8fc7\u5c06\u6587\u672c\u5185\u5bb9\u5206\u89e3\u4e3a\u56fe\u7ed3\u6784\u7684\u77e5\u8bc6\u5355\u5143\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u4f7f\u4ece\u7ebf\u6027\u6587\u672c\u6d88\u8d39\u5230\u57fa\u4e8e\u7f51\u7edc\u7684\u77e5\u8bc6\u5bfc\u822a\u8303\u5f0f\u8f6c\u53d8\u6210\u4e3a\u53ef\u80fd\u3002\u5b83\u589e\u5f3a\u4e86\u5bf9CS\u6587\u732e\u7684\u5b66\u672f\u8bbf\u95ee\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4f20\u7edf\u672c\u4f53\u8bba\u6784\u5efa\u7684\u6570\u636e\u9a71\u52a8\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u589e\u5f3a\u533a\u57df\u7814\u7a76\u548c\u6570\u5b57\u4eba\u6587\u5b66\u79d1\uff0c\u5e76\u7a81\u51fa\u4e86\u5176\u652f\u6301\u91cd\u65b0\u6784\u60f3\u533a\u57df\u77e5\u8bc6\u7cfb\u7edf\u5b66\u672f\u57fa\u7840\u8bbe\u65bd\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10013", "pdf": "https://arxiv.org/pdf/2505.10013", "abs": "https://arxiv.org/abs/2505.10013", "authors": ["Lake Yin", "Fan Huang"], "title": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 1 figure", "summary": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8861\u91cfLLM\u9690\u6027\u504f\u89c1\u7684\u65b9\u6cd5DIF\uff0c\u5e76\u53d1\u73b0\u5176\u4e0e\u56de\u7b54\u51c6\u786e\u6027\u5448\u53cd\u5411\u5173\u7cfb\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u65b9\u6cd5\u6765\u8861\u91cfLLM\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709\u7684LLM\u903b\u8f91\u548c\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u793e\u4f1a\u4eba\u53e3\u5b66\u89d2\u8272\u6765\u8ba1\u7b97DIF\u57fa\u51c6\u3002", "result": "\u672c\u6587\u53d1\u73b0\uff0cDIF\u65b9\u6cd5\u53ef\u4ee5\u7edf\u8ba1\u9a8c\u8bc1LLM\u884c\u4e3a\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u5e76\u4e14\u53d1\u73b0\u56de\u7b54\u51c6\u786e\u6027\u548c\u9690\u6027\u504f\u89c1\u4e4b\u95f4\u5b58\u5728\u53cd\u5411\u8d8b\u52bf\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0cLLM\u4e2d\u7684\u9690\u6027\u504f\u89c1\u4e0d\u4ec5\u662f\u4f26\u7406\u95ee\u9898\uff0c\u4e5f\u662f\u6280\u672f\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u8fd9\u79cd\u504f\u89c1\u7684\u65b9\u6cd5DIF\u3002"}}
{"id": "2505.09967", "pdf": "https://arxiv.org/pdf/2505.09967", "abs": "https://arxiv.org/abs/2505.09967", "authors": ["Liqian Deng"], "title": "TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8eTexture Key Driver Factors (TKDF)\uff0c\u4ee5\u63d0\u9ad8\u91ce\u5916\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u8868\u8fbe\u76f8\u5173\u7279\u5f81\u7684\u7ec6\u5fae\u548c\u5c40\u90e8\u6027\u8d28\uff0c\u4ee5\u53ca\u9762\u90e8\u5916\u89c2\u7684\u590d\u6742\u53d8\u5316\uff0c\u91ce\u5916\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u6211\u4eec\u89c2\u5bdf\u5230\u67d0\u4e9b\u7eb9\u7406\u7ebf\u7d22\uff0c\u5982\u7709\u6bdb\u3001\u773c\u775b\u548c\u5634\u5df4\u5468\u56f4\u7684\u76ae\u80a4\u5fae\u5c0f\u53d8\u5316\uff0c\u662f\u60c5\u611f\u52a8\u6001\u7684\u4e3b\u8981\u6307\u6807\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542bTexture-Aware Feature Extractor (TAFE)\u548cDual Contextual Information Filtering (DCIF)\u7684FER\u67b6\u6784\u3002TAFE\u91c7\u7528\u5e26\u6709\u591a\u5206\u652f\u6ce8\u610f\u529b\u7684ResNet\u9aa8\u5e72\u7f51\u7edc\u6765\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7eb9\u7406\u8868\u793a\uff0c\u800cDCIF\u901a\u8fc7\u81ea\u9002\u5e94\u6c60\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u6765\u4f18\u5316\u8fd9\u4e9b\u7279\u5f81\u3002", "result": "\u5728RAF-DB\u548cKDEF\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728RAF-DB\u548cKDEF\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5c06TKDF\u7eb3\u5165FER\u6d41\u7a0b\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.09812", "pdf": "https://arxiv.org/pdf/2505.09812", "abs": "https://arxiv.org/abs/2505.09812", "authors": ["Anastasija Tashkova", "Stefan Eftimov", "Bojan Ristov", "Slobodan Kalajdziski"], "title": "Comparative Analysis of Stroke Prediction Models Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Stroke remains one of the most critical global health challenges, ranking as\nthe second leading cause of death and the third leading cause of disability\nworldwide. This study explores the effectiveness of machine learning algorithms\nin predicting stroke risk using demographic, clinical, and lifestyle data from\nthe Stroke Prediction Dataset. By addressing key methodological challenges such\nas class imbalance and missing data, we evaluated the performance of multiple\nmodels, including Logistic Regression, Random Forest, and XGBoost. Our results\ndemonstrate that while these models achieve high accuracy, sensitivity remains\na limiting factor for real-world clinical applications. In addition, we\nidentify the most influential predictive features and propose strategies to\nimprove machine learning-based stroke prediction. These findings contribute to\nthe development of more reliable and interpretable models for the early\nassessment of stroke risk.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u9884\u6d4b\u4e2d\u98ce\u98ce\u9669\u65b9\u9762\u7684\u6548\u679c\uff0c\u5e76\u53d1\u73b0\u4e86\u5f71\u54cd\u9884\u6d4b\u7684\u5173\u952e\u7279\u5f81\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u4e2d\u98ce\u4ecd\u7136\u662f\u5168\u7403\u6700\u91cd\u8981\u7684\u5065\u5eb7\u6311\u6218\u4e4b\u4e00\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u4f7f\u7528\u4eba\u53e3\u7edf\u8ba1\u3001\u4e34\u5e8a\u548c\u751f\u6d3b\u65b9\u5f0f\u6570\u636e\u9884\u6d4b\u4e2d\u98ce\u98ce\u9669\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u7b49\u591a\u79cd\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u7075\u654f\u5ea6\u4ecd\u7136\u662f\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u9650\u5236\u56e0\u7d20\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4e2d\u98ce\u98ce\u9669\u7684\u65e9\u671f\u8bc4\u4f30\u3002"}}
{"id": "2505.10188", "pdf": "https://arxiv.org/pdf/2505.10188", "abs": "https://arxiv.org/abs/2505.10188", "authors": ["Felix Liedeker", "Olivia Sanchez-Graillet", "Moana Seidler", "Christian Brandt", "J\u00f6rg Wellmer", "Philipp Cimiano"], "title": "A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support", "categories": ["cs.AI"], "comment": "Presented at 'The First Workshop on Natural Language Argument-Based\n  Explanations', co-located with ECAI 2024", "summary": "As the field of healthcare increasingly adopts artificial intelligence, it\nbecomes important to understand which types of explanations increase\ntransparency and empower users to develop confidence and trust in the\npredictions made by machine learning (ML) systems. In shared decision-making\nscenarios where doctors cooperate with ML systems to reach an appropriate\ndecision, establishing mutual trust is crucial. In this paper, we explore\ndifferent approaches to generating explanations in eXplainable AI (XAI) and\nmake their underlying arguments explicit so that they can be evaluated by\nmedical experts. In particular, we present the findings of a user study\nconducted with physicians to investigate their perceptions of various types of\nAI-generated explanations in the context of diagnostic decision support. The\nstudy aims to identify the most effective and useful explanations that enhance\nthe diagnostic process. In the study, medical doctors filled out a survey to\nassess different types of explanations. Further, an interview was carried out\npost-survey to gain qualitative insights on the requirements of explanations\nincorporated in diagnostic decision support. Overall, the insights gained from\nthis study contribute to understanding the types of explanations that are most\neffective.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u533b\u7597\u9886\u57df\u4e2d\uff0c\u4e0d\u540c\u7c7b\u578b\u7684AI\u751f\u6210\u89e3\u91ca\u5982\u4f55\u5f71\u54cd\u533b\u751f\u5bf9\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u4fe1\u4efb\u548c\u4f7f\u7528\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u8bbf\u8c08\u53d1\u73b0\u6700\u6709\u6548\u7684\u89e3\u91ca\u7c7b\u578b\u3002", "motivation": "\u968f\u7740\u533b\u7597\u9886\u57df\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u4eba\u5de5\u667a\u80fd\uff0c\u4e86\u89e3\u54ea\u4e9b\u7c7b\u578b\u7684\u89e3\u91ca\u53ef\u4ee5\u589e\u52a0\u900f\u660e\u5ea6\u5e76\u589e\u5f3a\u7528\u6237\u5bf9\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9884\u6d4b\u7684\u4fe1\u4efb\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5728\u5171\u4eab\u51b3\u7b56\u573a\u666f\u4e2d\uff0c\u5efa\u7acb\u76f8\u4e92\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e0e\u533b\u751f\u8fdb\u884c\u7528\u6237\u7814\u7a76\u548c\u8bbf\u8c08\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u7c7b\u578b\u7684AI\u751f\u6210\u89e3\u91ca\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u5728\u8bca\u65ad\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u8bbf\u8c08\uff0c\u53ef\u4ee5\u8bc6\u522b\u51fa\u6700\u6709\u6548\u7684\u89e3\u91ca\u7c7b\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u8bca\u65ad\u8fc7\u7a0b\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u8bbf\u8c08\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5728\u8bca\u65ad\u51b3\u7b56\u652f\u6301\u4e2d\u54ea\u4e9b\u7c7b\u578b\u7684\u89e3\u91ca\u6700\u6709\u6548\u3002"}}
{"id": "2505.10063", "pdf": "https://arxiv.org/pdf/2505.10063", "abs": "https://arxiv.org/abs/2505.10063", "authors": ["Han Peng", "Jinhao Jiang", "Zican Dong", "Wayne Xin Zhao", "Lei Fang"], "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability", "categories": ["cs.CL"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CAFE\uff0c\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u7c97\u5230\u7ec6\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6587\u6863\u95ee\u7b54\u80fd\u529b\u3002\u901a\u8fc7\u9010\u6b65\u6d88\u9664\u80cc\u666f\u548c\u5e72\u6270\u6587\u6863\u7684\u8d1f\u9762\u5f71\u54cd\uff0cCAFE\u4f7f\u54cd\u5e94\u66f4\u4f9d\u8d56\u4e8e\u8bc1\u636e\u6587\u6863\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCAFE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5e73\u8861\u68c0\u7d22\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u5b83\u4eec\u5728\u56de\u7b54\u95ee\u9898\u65b9\u9762\u7684\u6548\u679c\u3002", "method": "CAFE\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u7c97\u5230\u7ec6\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u6d88\u9664\u80cc\u666f\u548c\u5e72\u6270\u6587\u6863\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4f7f\u54cd\u5e94\u66f4\u4f9d\u8d56\u4e8e\u8bc1\u636e\u6587\u6863\u3002\u9996\u5148\uff0c\u4f7f\u7528\u68c0\u7d22\u5934\u8fdb\u884c\u7c97\u7c92\u5ea6\u8fc7\u6ee4\u4ee5\u8bc6\u522b\u548c\u6392\u5e8f\u76f8\u5173\u6587\u6863\u3002\u7136\u540e\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u5c06\u6ce8\u610f\u529b\u5f15\u5bfc\u5230\u6700\u76f8\u5173\u7684\u5185\u5bb9\u4e0a\u3002", "result": "CAFE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Mistral\u6a21\u578b\u4e0a\u5206\u522b\u6bd4SFT\u548cRAG\u65b9\u6cd5\u63d0\u5347\u4e8622.1%\u548c13.7%\u7684SubEM\u3002", "conclusion": "CAFE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Mistral\u6a21\u578b\u4e0a\u5206\u522b\u6bd4SFT\u548cRAG\u65b9\u6cd5\u63d0\u5347\u4e8622.1%\u548c13.7%\u7684SubEM\u3002"}}
{"id": "2505.09971", "pdf": "https://arxiv.org/pdf/2505.09971", "abs": "https://arxiv.org/abs/2505.09971", "authors": ["Yuan Gao", "Shaobo Xia", "Sheng Nie", "Cheng Wang", "Xiaohuan Xi", "Bisheng Yang"], "title": "APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "18 pages,12 figures", "summary": "Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86APCoTTA\uff0c\u4e00\u79cd\u9488\u5bf9ALS\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u8fde\u7eed\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u3001\u57fa\u4e8e\u71b5\u7684\u4e00\u81f4\u6027\u635f\u5931\u548c\u968f\u673a\u53c2\u6570\u63d2\u503c\u673a\u5236\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5728ALS\u70b9\u4e91\u4e0a\u7684\u8fde\u7eed\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08CTTA\uff09\u65b9\u6cd5\u6709\u9650\uff0c\u9762\u4e34\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u9519\u8bef\u7d2f\u79ef\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u6a21\u5757\uff0c\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u9009\u62e9\u4f4e\u7f6e\u4fe1\u5ea6\u5c42\u8fdb\u884c\u8bad\u7ec3\uff0c\u5176\u4f59\u5c42\u4fdd\u6301\u51bb\u7ed3\uff0c\u4ee5\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u3002\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u71b5\u7684\u4e00\u81f4\u6027\u635f\u5931\u548c\u968f\u673a\u53c2\u6570\u63d2\u503c\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u9519\u8bef\u7d2f\u79ef\u5e76\u5e73\u8861\u76ee\u6807\u9002\u5e94\u548c\u6e90\u77e5\u8bc6\u4fdd\u7559\u3002", "result": "APCoTTA\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5ISPRSC\u548cH3DC\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0cmIoU\u5206\u522b\u63d0\u9ad8\u4e86\u7ea69%\u548c14%\u3002", "conclusion": "APCoTTA\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0cmIoU\u5206\u522b\u63d0\u9ad8\u4e86\u7ea69%\u548c14%\u3002\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801\u5df2\u53d1\u5e03\u3002"}}
{"id": "2505.09820", "pdf": "https://arxiv.org/pdf/2505.09820", "abs": "https://arxiv.org/abs/2505.09820", "authors": ["Sajib Biswas", "Mao Nishino", "Samuel Jacob Chacko", "Xiuwen Liu"], "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u548cBregman\u6295\u5f71\u65b9\u6cd5\u7684\u8d8a\u72f1\u6280\u672f\uff0c\u8be5\u6280\u672f\u5728\u591a\u4e2aLLM\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u4f7f\u7528\u4e86\u8bf8\u5982\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u7b49\u6280\u672f\u5bf9\u8bb8\u591a\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u3002\u73b0\u6709\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u79bb\u6563\u7a7a\u95f4\u6216\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5bfb\u627e\u53ef\u80fd\u8d8a\u72f1\u7684token\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u6216\u65e0\u6548\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u548cBregman\u6295\u5f71\u65b9\u6cd5\u7684\u5185\u5728\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u4f18\u5316\u7684one-hot\u7f16\u7801\u59cb\u7ec8\u4f4d\u4e8e\u6982\u7387\u5355\u7eaf\u5f62\u5185\u3002", "result": "\u8be5\u6280\u672f\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6210\u529f\u7387\u8f83\u9ad8\u4e14\u6548\u7387\u826f\u597d\u3002", "conclusion": "\u8be5\u6280\u672f\u5728\u4e94\u4e2a\u5f00\u6e90LLM\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u6210\u529f\u7387\u9ad8\u4e8e\u5176\u4ed6\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u8d8a\u72f1\u6280\u672f\uff0c\u5e76\u4e14\u6548\u7387\u5f88\u9ad8\u3002"}}
{"id": "2505.10278", "pdf": "https://arxiv.org/pdf/2505.10278", "abs": "https://arxiv.org/abs/2505.10278", "authors": ["Taian Guo", "Haiyang Shen", "Jinsheng Huang", "Zhengyang Mao", "Junyu Luo", "Zhuoru Chen", "Xuhui Liu", "Bingyu Xia", "Luchen Liu", "Yun Ma", "Ming Zhang"], "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction", "categories": ["cs.AI"], "comment": null, "summary": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MASS\uff0c\u4e00\u79cd\u7528\u4e8e\u6295\u8d44\u7ec4\u5408\u6784\u5efa\u7684\u591a\u4ee3\u7406\u89c4\u6a21\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u4ee3\u7406\u6570\u91cf\u548c\u53cd\u5411\u4f18\u5316\u8fc7\u7a0b\u6765\u63d0\u9ad8\u5e02\u573a\u7406\u89e3\u548c\u4ee3\u7406\u5206\u5e03\u4f18\u5316\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u5de5\u4f5c\u53d7\u5230\u7eaf\u6a21\u62df\u6216\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7a0b\u7684\u9650\u5236\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u548c\u6548\u679c\u3002", "method": "MASS\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u4ee3\u7406\u6570\u91cf\u8fdb\u884c\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u4ee5\u83b7\u5f97\u5bf9\u5e02\u573a\u7684\u66f4\u597d\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u4f18\u5316\u8fc7\u7a0b\u7aef\u5230\u7aef\u5730\u4f18\u5316\u4ee3\u7406\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u6027\u80fd\u5b9e\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u3001\u56de\u6d4b\u5b9e\u9a8c\u3001\u66f4\u65b0\u6570\u636e\u548c\u80a1\u7968\u6c60\u7684\u5b9e\u9a8c\u3001\u7f29\u653e\u5b9e\u9a8c\u3001\u53c2\u6570\u654f\u611f\u6027\u5b9e\u9a8c\u548c\u53ef\u89c6\u5316\u5b9e\u9a8c\uff0cMASS\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684A\u80a1\u80a1\u7968\u6c60\u4e2d\u4e0e\u516d\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "MASS\u7684\u8303\u5f0f\u6709\u671b\u6269\u5c55\u5230\u5177\u6709\u7c7b\u4f3c\u7279\u5f81\u7684\u5176\u4ed6\u4efb\u52a1\u3002"}}
{"id": "2505.10066", "pdf": "https://arxiv.org/pdf/2505.10066", "abs": "https://arxiv.org/abs/2505.10066", "authors": ["Michael Fire", "Yitzhak Elbazis", "Adi Wasenstein", "Lior Rokach"], "title": "Dark LLMs: The Growing Threat of Unaligned AI Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "68T50, 68T05, 68P25", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.", "AI": {"tldr": "This paper discusses the vulnerability of Large Language Models (LLMs) to jailbreak attacks and highlights the growing threat posed by dark LLMs models. The research found a universal jailbreak attack that can compromise multiple state-of-the-art models, leading to harmful outputs. The study emphasizes the need for improved AI safety practices and highlights the risks associated with the proliferation of open-source LLMs.", "motivation": "To identify the growing threat posed by dark LLMs models and highlight the need for improved AI safety practices.", "method": "Research on the vulnerability of LLMs to jailbreak attacks and the threat posed by dark LLMs models.", "result": "A universal jailbreak attack was uncovered that effectively compromises multiple state-of-the-art models, enabling them to produce harmful outputs upon request.", "conclusion": "LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated without decisive intervention."}}
{"id": "2505.09986", "pdf": "https://arxiv.org/pdf/2505.09986", "abs": "https://arxiv.org/abs/2505.09986", "authors": ["Yimin Zhou", "Yichong Xia", "Sicheng Pan", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.", "AI": {"tldr": "HQUIC is a new underwater image compression method that leverages specific features of underwater images to achieve better performance than existing methods.", "motivation": "Contemporary underwater image compression algorithms fail to fully leverage the unique characteristics of underwater scenes, leading to suboptimal performance.", "method": "HQUIC employs an ALTC module to adaptively predict attenuation coefficients and global light information, uses a codebook to extract common objects, and dynamically weights multi-scale frequency components.", "result": "HQUIC demonstrates superior performance on diverse underwater datasets compared to existing methods.", "conclusion": "HQUIC outperforms state-of-the-art compression methods in underwater image compression."}}
{"id": "2505.09822", "pdf": "https://arxiv.org/pdf/2505.09822", "abs": "https://arxiv.org/abs/2505.09822", "authors": ["Changhao Shi", "Gal Mishne"], "title": "Learning Kronecker-Structured Graphs from Smooth Signals", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Graph learning, or network inference, is a prominent problem in graph signal\nprocessing (GSP). GSP generalizes the Fourier transform to non-Euclidean\ndomains, and graph learning is pivotal to applying GSP when these domains are\nunknown. With the recent prevalence of multi-way data, there has been growing\ninterest in product graphs that naturally factorize dependencies across\ndifferent ways. However, the types of graph products that can be learned are\nstill limited for modeling diverse dependency structures. In this paper, we\nstudy the problem of learning a Kronecker-structured product graph from smooth\nsignals. Unlike the more commonly used Cartesian product, the Kronecker product\nmodels dependencies in a more intricate, non-separable way, but posits harder\nconstraints on the graph learning problem. To tackle this non-convex problem,\nwe propose an alternating scheme to optimize each factor graph and provide\ntheoretical guarantees for its asymptotic convergence. The proposed algorithm\nis also modified to learn factor graphs of the strong product. We conduct\nexperiments on synthetic and real-world graphs and demonstrate our approach's\nefficacy and superior performance compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u5e73\u6ed1\u4fe1\u53f7\u4e2d\u5b66\u4e60Kronecker\u7ed3\u6784\u4ea7\u54c1\u56fe\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u66ff\u65b9\u6848\u6765\u4f18\u5316\u6bcf\u4e2a\u56e0\u5b50\u56fe\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u591a\u5411\u6570\u636e\u7684\u666e\u53ca\uff0c\u5bf9\u4ea7\u54c1\u56fe\u7684\u5174\u8da3\u589e\u52a0\uff0c\u4f46\u76ee\u524d\u53ef\u4ee5\u5b66\u4e60\u7684\u4ea7\u54c1\u56fe\u7c7b\u578b\u4ecd\u7136\u6709\u9650\uff0c\u65e0\u6cd5\u5efa\u6a21\u591a\u6837\u5316\u7684\u4f9d\u8d56\u7ed3\u6784\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u65b9\u6848\u6765\u4f18\u5316\u6bcf\u4e2a\u56e0\u5b50\u56fe\uff0c\u5e76\u63d0\u4f9b\u4e86\u5176\u6e10\u8fd1\u6536\u655b\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u4fee\u6539\u4e86\u7b97\u6cd5\u4ee5\u5b66\u4e60\u5f3a\u79ef\u56e0\u5b50\u56fe\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5e73\u6ed1\u4fe1\u53f7\u4e2d\u5b66\u4e60Kronecker\u7ed3\u6784\u4ea7\u54c1\u56fe\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.10309", "pdf": "https://arxiv.org/pdf/2505.10309", "abs": "https://arxiv.org/abs/2505.10309", "authors": ["Tuan Dung Nguyen", "Duncan J. Watts", "Mark E. Whiting"], "title": "Empirically evaluating commonsense intelligence in large language models with large-scale human judgments", "categories": ["cs.AI", "cs.HC", "cs.SI"], "comment": null, "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u5e38\u8bc6\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u4eba\u7c7b\u4e4b\u95f4\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u53d1\u73b0\u5c0f\u578b\u6a21\u578b\u5728\u67d0\u4e9b\u65b9\u9762\u6bd4\u5927\u578b\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5047\u8bbe\u4eba\u7c7b\u5e38\u8bc6\u662f\u540c\u8d28\u7684\uff0c\u4f46\u6700\u8fd1\u7684\u5b9e\u8bc1\u7814\u7a76\u663e\u793a\u4eba\u7c7b\u5728\u8ba4\u4e3a\u4ec0\u4e48\u662f\u5e38\u8bc6\u65b9\u9762\u5b58\u5728\u5de8\u5927\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8003\u8651\u8fd9\u79cd\u5f02\u8d28\u6027\u7684\u65b0\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4e2d\u7684\u5e38\u8bc6\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u5224\u65ad\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u7684\u4e00\u81f4\u6027\u6765\u4f53\u73b0\u4eba\u7c7b\u4e4b\u95f4\u7684\u5f02\u8d28\u6027\u3002", "result": "\u5927\u591a\u6570LLMs\u5728\u4e2a\u4f53\u5e38\u8bc6\u80fd\u529b\u4e0a\u4f4e\u4e8e\u4eba\u7c7b\u4e2d\u4f4d\u6570\uff0c\u800c\u5f53\u7528\u4f5c\u6a21\u62df\u5047\u8bbe\u4eba\u53e3\u65f6\uff0cLLMs\u4e0e\u771f\u5b9e\u4eba\u7c7b\u5728\u540c\u610f\u540c\u4e00\u7ec4\u9648\u8ff0\u7684\u7a0b\u5ea6\u4e0a\u4ec5\u6709\u9002\u5ea6\u7684\u76f8\u5173\u6027\u3002\u8f83\u5c0f\u7684\u3001\u5f00\u653e\u6743\u91cd\u7684\u6a21\u578b\u6bd4\u66f4\u5927\u7684\u3001\u4e13\u6709\u7684\u524d\u6cbf\u6a21\u578b\u66f4\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u8bc4\u4f30\u6846\u67b6\u5c06\u5e38\u8bc6\u667a\u80fd\u4e0e\u5176\u6587\u5316\u57fa\u7840\u8054\u7cfb\u8d77\u6765\uff0c\u6709\u52a9\u4e8e\u65e5\u76ca\u589e\u957f\u7684\u547c\u5401\uff0c\u5373\u8c03\u6574\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u4ee5\u9002\u5e94\u62e5\u6709\u4e0d\u540c\u4e14\u5e38\u5e38\u4e0d\u517c\u5bb9\u7684\u793e\u4f1a\u77e5\u8bc6\u5e93\u5b58\u7684\u4eba\u7c7b\u7fa4\u4f53\u3002"}}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081", "abs": "https://arxiv.org/abs/2505.10081", "authors": ["Wisdom Aduah", "Francois Meyer"], "title": "Designing and Contextualising Probes for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5730\u8c03\u67e5\u4e86PLMs\u5728\u975e\u6d32\u8bed\u8a00\u4e2d\u7684\u8bed\u8a00\u77e5\u8bc6\uff0c\u53d1\u73b0\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u8c03\u6574\u7684PLMs\u6bd4\u5927\u89c4\u6a21\u591a\u8bed\u8a00PLMs\u7f16\u7801\u4e86\u66f4\u591a\u7684\u76ee\u6807\u8bed\u8a00\u8bed\u8a00\u4fe1\u606f\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e3b\u52a8\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9002\u5e94\u7b56\u7565\u6210\u529f\u80cc\u540e\u7684\u5185\u90e8\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u4e0d\u65ad\u6539\u8fdb\uff0c\u4f46\u8fd9\u4e9b\u8fdb\u6b65\u7684\u539f\u56e0\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5730\u8c03\u67e5\u4e86PLMs\u5728\u975e\u6d32\u8bed\u8a00\u4e2d\u7684\u8bed\u8a00\u77e5\u8bc6\u3002", "method": "\u6211\u4eec\u4e3a\u516d\u79cd\u8bed\u8a00\u5b66\u4e0a\u591a\u6837\u5316\u7684\u975e\u6d32\u8bed\u8a00\u8bad\u7ec3\u4e86\u9010\u5c42\u63a2\u6d4b\u5668\uff0c\u4ee5\u5206\u6790\u8bed\u8a00\u7279\u5f81\u7684\u5206\u5e03\u60c5\u51b5\u3002\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u63a7\u5236\u4efb\u52a1\uff0c\u4ee5\u89e3\u91ca\u63a2\u6d4b\u5668\u6027\u80fd\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u8c03\u6574\u7684PLMs\u6bd4\u5927\u89c4\u6a21\u591a\u8bed\u8a00PLMs\u7f16\u7801\u4e86\u66f4\u591a\u7684\u76ee\u6807\u8bed\u8a00\u8bed\u8a00\u4fe1\u606f\u3002\u7ed3\u679c\u786e\u8ba4\u4e86\u4e4b\u524d\u7684\u53d1\u73b0\uff0c\u5373\u8bcd\u6cd5\u7ea7\u522b\u7684\u53e5\u6cd5\u4fe1\u606f\u96c6\u4e2d\u5728\u4e2d\u95f4\u5230\u6700\u540e\u4e00\u5c42\uff0c\u800c\u53e5\u5b50\u7ea7\u522b\u7684\u8bed\u4e49\u4fe1\u606f\u5206\u5e03\u5728\u6240\u6709\u5c42\u4e2d\u3002\u901a\u8fc7\u63a7\u5236\u4efb\u52a1\u548c\u63a2\u6d4b\u57fa\u7ebf\uff0c\u6211\u4eec\u786e\u8ba4\u6027\u80fd\u53cd\u6620\u4e86PLMs\u7684\u5185\u90e8\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u63a2\u6d4b\u5668\u7684\u8bb0\u5fc6\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5e94\u7528\u4e86\u5df2\u5efa\u7acb\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6765\u5206\u6790\u975e\u6d32\u8bed\u8a00\u7684PLMs\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e3b\u52a8\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9002\u5e94\u7b56\u7565\u6210\u529f\u80cc\u540e\u7684\u5185\u90e8\u673a\u5236\u3002"}}
{"id": "2505.09990", "pdf": "https://arxiv.org/pdf/2505.09990", "abs": "https://arxiv.org/abs/2505.09990", "authors": ["Long Cheng", "Jiafei Duan", "Yi Ru Wang", "Haoquan Fang", "Boyang Li", "Yushan Huang", "Elvis Wang", "Ainaz Eftekhar", "Jason Lee", "Wentao Yuan", "Rose Hendrix", "Noah A. Smith", "Fei Xia", "Dieter Fox", "Ranjay Krishna"], "title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing", "categories": ["cs.CV"], "comment": "10 Pages, Dataset and code:https://pointarena.github.io/", "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PointArena\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6307\u9488\u5728\u5404\u79cd\u63a8\u7406\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u5e73\u53f0\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aPoint-Bench\u6570\u636e\u96c6\u3001Point-Battle\u4e92\u52a8\u7ade\u6280\u573a\u548cPoint-Act\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cMolmo-72B\u8868\u73b0\u6700\u4f73\uff0c\u800c\u4e13\u6709\u6a21\u578b\u7684\u8868\u73b0\u4e5f\u5728\u63d0\u9ad8\u3002\u76d1\u7763\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6307\u9488\u4f5c\u4e3a\u4e00\u79cd\u57fa\u7840\u4e14\u76f4\u89c2\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5728\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e2d\u5b9a\u4f4d\u8bed\u8a00\uff0c\u5176\u5e94\u7528\u8303\u56f4\u6db5\u76d6\u673a\u5668\u4eba\u6280\u672f\u3001\u8f85\u52a9\u6280\u672f\u548c\u4ea4\u4e92\u5f0fAI\u7cfb\u7edf\u3002\u867d\u7136\u6700\u8fd1\u7684\u591a\u6a21\u6001\u6a21\u578b\u5f00\u59cb\u652f\u6301\u6307\u9488\u529f\u80fd\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u53ea\u5173\u6ce8\u53c2\u8003\u5bf9\u8c61\u5b9a\u4f4d\u4efb\u52a1\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86PointArena\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u591a\u79cd\u63a8\u7406\u573a\u666f\u7684\u591a\u6a21\u6001\u6307\u9488\u3002PointArena\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1) Point-Bench\uff0c\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e94\u4e2a\u63a8\u7406\u7c7b\u522b\u4e2d\u7684\u7ea61,000\u4e2a\u6307\u9488\u4efb\u52a1\uff1b(2) Point-Battle\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7684\u57fa\u4e8e\u7f51\u7edc\u7684\u7ade\u6280\u573a\uff0c\u4fc3\u8fdb\u76f2\u76ee\u7684\u6210\u5bf9\u6a21\u578b\u6bd4\u8f83\uff0c\u5df2\u7ecf\u6536\u96c6\u4e86\u8d85\u8fc74,500\u4e2a\u533f\u540d\u6295\u7968\uff1b(3) Point-Act\uff0c\u4e00\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u76f4\u63a5\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u6307\u9488\u80fd\u529b\u3002", "result": "\u6211\u4eec\u5bf9\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u4e13\u6709\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0cMolmo-72B\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c3d\u7ba1\u4e13\u6709\u6a21\u578b\u65e5\u76ca\u8868\u73b0\u51fa\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4e13\u95e8\u9488\u5bf9\u6307\u9488\u4efb\u52a1\u7684\u76d1\u7763\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002\u5728\u6211\u4eec\u7684\u591a\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\u4e2d\uff0c\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u7a81\u663e\u4e86\u7cbe\u786e\u6307\u9488\u80fd\u529b\u5728\u4f7f\u591a\u6a21\u6001\u6a21\u578b\u6709\u6548\u8fde\u63a5\u62bd\u8c61\u63a8\u7406\u4e0e\u5177\u4f53\u73b0\u5b9e\u884c\u52a8\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cMolmo-72B\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c3d\u7ba1\u4e13\u6709\u6a21\u578b\u65e5\u76ca\u8868\u73b0\u51fa\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4e13\u95e8\u9488\u5bf9\u6307\u9488\u4efb\u52a1\u7684\u76d1\u7763\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002\u5728\u6211\u4eec\u7684\u591a\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\u4e2d\uff0c\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u7a81\u663e\u4e86\u7cbe\u786e\u6307\u9488\u80fd\u529b\u5728\u4f7f\u591a\u6a21\u6001\u6a21\u578b\u6709\u6548\u8fde\u63a5\u62bd\u8c61\u63a8\u7406\u4e0e\u5177\u4f53\u73b0\u5b9e\u884c\u52a8\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2505.09847", "pdf": "https://arxiv.org/pdf/2505.09847", "abs": "https://arxiv.org/abs/2505.09847", "authors": ["Liyang Zhao", "Olurotimi Seton", "Himadeep Reddy Reddivari", "Suvendu Jena", "Shadow Zhao", "Rachit Kumar", "Changshuai Wei"], "title": "Causal Predictive Optimization and Generation for Business AI", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "comment": null, "summary": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u9500\u552e\u4f18\u5316\u548c\u5546\u4e1aAI\u65b9\u6cd5\uff0c\u79f0\u4e3a\u56e0\u679c\u9884\u6d4b\u4f18\u5316\u4e0e\u751f\u6210\uff08CPOG\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728LinkedIn\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002", "motivation": "\u9500\u552e\u8fc7\u7a0b\u7684\u4f18\u5316\u5bf9\u4e8eB2B\u4f01\u4e1a\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u9500\u552e\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u673a\u5668\u5b66\u4e60\u3001\u7ea6\u675f\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u5e26\u6b66\u88c5\u7b97\u6cd5\u4ee5\u53ca\u751f\u6210AI\u548c\u53cd\u9988\u5faa\u73af\u7684\u4e09\u5c42\u67b6\u6784\u3002", "result": "\u5728LinkedIn\u4e2d\u7684\u5b9e\u65bd\u5c55\u793a\u4e86CPOG\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u7cfb\u7edf\u7684\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u5206\u4eab\u4e86\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8be5\u9886\u57df\u7684\u5b66\u4e60\u548c\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u9500\u552e\u4f18\u5316\u548c\u5546\u4e1aAI\u65b9\u6cd5\uff0c\u5373\u56e0\u679c\u9884\u6d4b\u4f18\u5316\u4e0e\u751f\u6210\uff08CPOG\uff09\uff0c\u5e76\u5728LinkedIn\u4e2d\u6210\u529f\u5b9e\u65bd\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u6210\u679c\u3002"}}
{"id": "2505.10328", "pdf": "https://arxiv.org/pdf/2505.10328", "abs": "https://arxiv.org/abs/2505.10328", "authors": ["Alvin Combrink", "Stephie Do", "Kristofer Bengtsson", "Sabino Francesco Roselli", "Martin Fabian"], "title": "A Comparative Study of SMT and MILP for the Nurse Rostering Problem", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 3 figures", "summary": "The effects of personnel scheduling on the quality of care and working\nconditions for healthcare personnel have been thoroughly documented. However,\nthe ever-present demand and large variation of constraints make healthcare\nscheduling particularly challenging. This problem has been studied for decades,\nwith limited research aimed at applying Satisfiability Modulo Theories (SMT).\nSMT has gained momentum within the formal verification community in the last\ndecades, leading to the advancement of SMT solvers that have been shown to\noutperform standard mathematical programming techniques.\n  In this work, we propose generic constraint formulations that can model a\nwide range of real-world scheduling constraints. Then, the generic constraints\nare formulated as SMT and MILP problems and used to compare the respective\nstate-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired\nrostering problems. Experimental results show how each solver excels for\ncertain types of problems; the MILP solver generally performs better when the\nproblem is highly constrained or infeasible, while the SMT solver performs\nbetter otherwise. On real-world inspired problems containing a more varied set\nof shifts and personnel, the SMT solver excels. Additionally, it was noted\nduring experimentation that the SMT solver was more sensitive to the way the\ngeneric constraints were formulated, requiring careful consideration and\nexperimentation to achieve better performance. We conclude that SMT-based\nmethods present a promising avenue for future research within the domain of\npersonnel scheduling.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86SMT\u65b9\u6cd5\u5728\u4eba\u5458\u6392\u73ed\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86Z3\u548cGurobi\u7b49\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u73b0SMT\u65b9\u6cd5\u5728\u67d0\u4e9b\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "The effects of personnel scheduling on the quality of care and working conditions for healthcare personnel have been thoroughly documented. However, the ever-present demand and large variation of constraints make healthcare scheduling particularly challenging. This problem has been studied for decades, with limited research aimed at applying Satisfiability Modulo Theories (SMT).", "method": "We propose generic constraint formulations that can model a wide range of real-world scheduling constraints. Then, the generic constraints are formulated as SMT and MILP problems and used to compare the respective state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired rostering problems.", "result": "Experimental results show how each solver excels for certain types of problems; the MILP solver generally performs better when the problem is highly constrained or infeasible, while the SMT solver performs better otherwise. On real-world inspired problems containing a more varied set of shifts and personnel, the SMT solver excels. Additionally, it was noted during experimentation that the SMT solver was more sensitive to the way the generic constraints were formulated, requiring careful consideration and experimentation to achieve better performance.", "conclusion": "SMT-based methods present a promising avenue for future research within the domain of personnel scheduling."}}
{"id": "2505.10089", "pdf": "https://arxiv.org/pdf/2505.10089", "abs": "https://arxiv.org/abs/2505.10089", "authors": ["Wei Liu", "Sony Trenous", "Leonardo F. R. Ribeiro", "Bill Byrne", "Felix Hieber"], "title": "XRAG: Cross-lingual Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.", "AI": {"tldr": "The paper introduces XRAG, a new benchmark for evaluating LLMs in cross-lingual RAG settings. It highlights the challenges faced by LLMs in monolingual and multilingual retrieval scenarios.", "motivation": "The paper aims to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings where the user language does not match the retrieval results.", "method": "XRAG is constructed from recent news articles to ensure that its questions require external knowledge to be answered. It covers real-world scenarios of monolingual and multilingual retrieval and provides relevancy annotations for each retrieved document.", "result": "Experimental results on five LLMs uncover two previously unreported challenges in cross-lingual RAG: 1) in the monolingual retrieval setting, all evaluated models struggle with response language correctness; 2) in the multilingual retrieval setting, the main challenge lies in reasoning over retrieved information across languages rather than generation of non-English text.", "conclusion": "XRAG serves as a valuable benchmark for studying LLM reasoning abilities, even before considering the additional cross-lingual complexity."}}
{"id": "2505.09997", "pdf": "https://arxiv.org/pdf/2505.09997", "abs": "https://arxiv.org/abs/2505.09997", "authors": ["Jinhyun Jang", "Jiyeong Lee", "Kwanghoon Sohn"], "title": "Descriptive Image-Text Matching with Graded Contextual Similarity", "categories": ["cs.CV"], "comment": null, "summary": "Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.", "AI": {"tldr": "DITM improves image-text matching by considering descriptive flexibility and enhancing hierarchical reasoning, outperforming existing methods in representing complex relationships.", "motivation": "Existing approaches use sparse binary supervision, which neglects many-to-many image-text relationships and implicit connections from general to specific descriptions. DITM aims to address these limitations by learning graded contextual similarity.", "method": "DITM proposes descriptive image-text matching by exploring the descriptive flexibility of language, using cumulative term frequency-inverse document frequency (TF-IDF) to balance pairwise similarity according to sentence keywords. It refines false negative labeling and builds precise matching by aligning sentences in a generic-to-specific order.", "result": "Experiments on MS-COCO, Flickr30K, and CxC datasets show DITM's effectiveness in representing complex image-text relationships. It also enhances the model's hierarchical reasoning ability, as shown by analysis on the HierarCaps benchmark.", "conclusion": "DITM enhances the hierarchical reasoning ability of the model and demonstrates effectiveness in representing complex image-text relationships compared to state-of-the-art approaches."}}
{"id": "2505.09848", "pdf": "https://arxiv.org/pdf/2505.09848", "abs": "https://arxiv.org/abs/2505.09848", "authors": ["Aditya Raj", "Golrokh Mirzaei"], "title": "Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection", "categories": ["cs.LG", "eess.IV"], "comment": "11 pages", "summary": "Imaging and genomic data offer distinct and rich features, and their\nintegration can unveil new insights into the complex landscape of diseases. In\nthis study, we present a novel approach utilizing radiogenomic data including\nstructural MRI images and gene expression data, for Alzheimer's disease\ndetection. Our framework introduces a novel heterogeneous bipartite graph\nrepresentation learning featuring two distinct node types: genes and images.\nThe network can effectively classify Alzheimer's disease (AD) into three\ndistinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)\nclasses, utilizing a small dataset. Additionally, it identified which genes\nplay a significant role in each of these classification groups. We evaluate the\nperformance of our approach using metrics including classification accuracy,\nrecall, precision, and F1 score. The proposed technique holds potential for\nextending to radiogenomic-based classification to other diseases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6784\u53cc\u90e8\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5229\u7528\u7ed3\u6784MRI\u56fe\u50cf\u548c\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\uff0c\u5e76\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u6574\u5408\u6210\u50cf\u548c\u57fa\u56e0\u7ec4\u6570\u636e\u53ef\u4ee5\u63ed\u793a\u75be\u75c5\u590d\u6742\u666f\u89c2\u7684\u65b0\u89c1\u89e3\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6784\u53cc\u90e8\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u7ed3\u6784MRI\u56fe\u50cf\u548c\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5c06\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff0c\u5e76\u8bc6\u522b\u51fa\u6bcf\u4e2a\u5206\u7c7b\u7ec4\u4e2d\u7684\u91cd\u8981\u57fa\u56e0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u75be\u75c5\u7684\u57fa\u4e8e\u653e\u5c04\u57fa\u56e0\u7ec4\u7684\u5206\u7c7b\u3002"}}
{"id": "2505.10361", "pdf": "https://arxiv.org/pdf/2505.10361", "abs": "https://arxiv.org/abs/2505.10361", "authors": ["David Abel", "Michael Bowling", "Andr\u00e9 Barreto", "Will Dabney", "Shi Dong", "Steven Hansen", "Anna Harutyunyan", "Khimya Khetarpal", "Clare Lyle", "Razvan Pascanu", "Georgios Piliouras", "Doina Precup", "Jonathan Richens", "Mark Rowland", "Tom Schaul", "Satinder Singh"], "title": "Plasticity as the Mirror of Empowerment", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Our first finding is that\nplasticity is the mirror of empowerment: The agent's plasticity is identical to\nthe empowerment of the environment, and vice versa. Our second finding\nestablishes a tension between the plasticity and empowerment of an agent,\nsuggesting that agent design needs to be mindful of both characteristics. We\nexplore the implications of these findings, and suggest that plasticity,\nempowerment, and their relationship are essential to understanding agency.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86\u5851\u6599\u6027\u8fd9\u4e00\u6982\u5ff5\uff0c\u5c06\u5176\u4e0e\u8d4b\u6743\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u8ba8\u4ee3\u7406\u5982\u4f55\u88ab\u5176\u89c2\u5bdf\u5230\u7684\u5185\u5bb9\u6240\u5f71\u54cd\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5f71\u54cd\u7684\u7a0b\u5ea6\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u8bba\u91cf\u2014\u2014\u5e7f\u4e49\u5b9a\u5411\u4fe1\u606f\uff0c\u5b9a\u4e49\u4e86\u5851\u6599\u6027\u3002", "result": "\u5851\u6599\u6027\u662f\u8d4b\u6743\u6027\u7684\u955c\u50cf\uff0c\u4ee3\u7406\u7684\u5851\u6599\u6027\u7b49\u4e8e\u73af\u5883\u7684\u8d4b\u6743\u6027\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u540c\u65f6\uff0c\u4ee3\u7406\u7684\u5851\u6599\u6027\u548c\u8d4b\u6743\u6027\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\u3002", "conclusion": "\u5851\u6599\u6027\u548c\u8d4b\u6743\u6027\u53ca\u5176\u5173\u7cfb\u5bf9\u4e8e\u7406\u89e3\u4ee3\u7406\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113", "abs": "https://arxiv.org/abs/2505.10113", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86S-MedQA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u4e8e\u9886\u57df\u8f6c\u79fb\u800c\u975e\u77e5\u8bc6\u6ce8\u5165\u3002", "motivation": "\u4e3a\u4e86\u68c0\u9a8c\u77e5\u8bc6\u6ce8\u5165\u5728\u533b\u5b66QA\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a2\u8ba8\u5fae\u8c03\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u7684\u4f5c\u7528\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86S-MedQA\uff0c\u4e00\u4e2a\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4e34\u5e8a\u4e13\u4e1a\u4e2d\u7684\u82f1\u8bed\u533b\u5b66\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u96c6\u3002\u6211\u4eec\u4f7f\u7528S-MedQA\u6765\u68c0\u9a8c\u4e00\u4e2a\u4e0e\u77e5\u8bc6\u6ce8\u5165\u76f8\u5173\u7684\u6d41\u884c\u5047\u8bbe\u5728\u533b\u5b66QA\u7684\u77e5\u8bc6\u5bc6\u96c6\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "1) \u5728\u7279\u5b9a\u4e13\u4e1a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u5e76\u4e0d\u4e00\u5b9a\u80fd\u5728\u8be5\u4e13\u4e1a\u4e0a\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff1b2) \u65e0\u8bba\u5728\u54ea\u4e2a\u4e13\u4e1a\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6240\u6709\u4e13\u4e1a\u7684\u4e34\u5e8a\u76f8\u5173\u672f\u8bed\u7684token\u6982\u7387\u90fd\u4f1a\u4e00\u81f4\u589e\u52a0\u3002", "conclusion": "\u6211\u4eec\u76f8\u4fe1\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u4e8e\u9886\u57df\u8f6c\u79fb\uff08\u4f8b\u5982\uff0c\u4ece\u4e00\u822c\u5230\u533b\u5b66\uff09\uff0c\u800c\u4e0d\u662f\u77e5\u8bc6\u6ce8\u5165\uff0c\u5e76\u5efa\u8bae\u91cd\u65b0\u8003\u8651\u5fae\u8c03\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u7684\u89d2\u8272\u3002"}}
{"id": "2505.09998", "pdf": "https://arxiv.org/pdf/2505.09998", "abs": "https://arxiv.org/abs/2505.09998", "authors": ["Ying Zang", "Yuanqi Hu", "Xinyu Chen", "Yuxia Xu", "Suhui Wang", "Chunan Yu", "Lanyun Zhu", "Deyi Ji", "Xin Xu", "Tianrun Chen"], "title": "From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u8349\u56fe\u9a71\u52a8\u76843D\u670d\u88c5\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u666e\u901a\u7528\u6237\u80fd\u591f\u901a\u8fc7\u7b80\u5355\u76843D\u8349\u56fe\u5728AR/VR\u73af\u5883\u4e2d\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u5b57\u670d\u88c5\u3002", "motivation": "\u5728\u6c89\u6d78\u5f0f\u6d88\u8d39\u7535\u5b50\u4ea7\u54c1\u65f6\u4ee3\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5e0c\u671b\u901a\u8fc7\u865a\u62df\u65f6\u5c1a\u6765\u8868\u8fbe\u81ea\u5df1\u7684\u8eab\u4efd\uff0c\u4f46\u73b0\u6709\u76843D\u670d\u88c5\u8bbe\u8ba1\u5de5\u5177\u5bf9\u666e\u901a\u7528\u6237\u6765\u8bf4\u4ecd\u7136\u96be\u4ee5\u4f7f\u7528\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a3D\u8349\u56fe\u9a71\u52a8\u76843D\u670d\u88c5\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6761\u4ef6\u6269\u6563\u6a21\u578b\u3001\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u7684\u8349\u56fe\u7f16\u7801\u5668\u4ee5\u53ca\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u6613\u7528\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u6613\u7528\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0b\u4e00\u4ee3\u6d88\u8d39\u5e73\u53f0\u4e0a\u7684\u6c11\u4e3b\u5316\u65f6\u5c1a\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09851", "pdf": "https://arxiv.org/pdf/2505.09851", "abs": "https://arxiv.org/abs/2505.09851", "authors": ["Shun Wang", "Shun-Li Shang", "Zi-Kui Liu", "Wenrui Hao"], "title": "ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "9 pages, 4 figures", "summary": "Traditional entropy-based methods - such as cross-entropy loss in\nclassification problems - have long been essential tools for quantifying\nuncertainty and disorder in data and developing artificial intelligence\nalgorithms. However, the rapid growth of data across various domains has\nintroduced new challenges, particularly the integration of heterogeneous\ndatasets with intrinsic disparities. In this paper, we extend zentropy theory\ninto the data science domain by introducing intrinsic entropy, enabling more\neffective learning from heterogeneous data sources. We propose a\nzentropy-enhanced neural network (ZENN) that simultaneously learns both energy\nand intrinsic entropy components, capturing the underlying structure of\nmulti-source data. To support this, we redesign the neural network architecture\nto better reflect the intrinsic properties and variability inherent in diverse\ndatasets. We demonstrate the effectiveness of ZENN on classification tasks and\nenergy landscape reconstructions, showing its superior generalization\ncapabilities and robustness-particularly in predicting high-order derivatives.\nAs a practical application, we employ ZENN to reconstruct the Helmholtz energy\nlandscape of Fe3Pt using data generated from DFT and capture key material\nbehaviors, including negative thermal expansion and the critical point in the\ntemperature-pressure space. Overall, our study introduces a novel approach for\ndata-driven machine learning grounded in zentropy theory, highlighting ZENN as\na versatile and robust deep learning framework for scientific problems\ninvolving complex, heterogeneous datasets.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8ezentropy\u7406\u8bba\u7684\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u7684ZENN\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u5f3a\u5927\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u71b5\u7684\u65b9\u6cd5\u5728\u91cf\u5316\u6570\u636e\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u65e0\u5e8f\u6027\u4ee5\u53ca\u5f00\u53d1\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u65b9\u9762\u4e00\u76f4\u662f\u975e\u5e38\u91cd\u8981\u7684\u5de5\u5177\u3002\u7136\u800c\uff0c\u5404\u4e2a\u9886\u57df\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6574\u5408\u5177\u6709\u5185\u5728\u5dee\u5f02\u7684\u5f02\u6784\u6570\u636e\u96c6\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u5185\u5728\u71b5\uff0c\u5c06zentropy\u7406\u8bba\u6269\u5c55\u5230\u6570\u636e\u79d1\u5b66\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cdzentropy\u589e\u5f3a\u578b\u795e\u7ecf\u7f51\u7edc\uff08ZENN\uff09\uff0c\u8be5\u7f51\u7edc\u540c\u65f6\u5b66\u4e60\u80fd\u91cf\u548c\u5185\u5728\u71b5\u7ec4\u4ef6\uff0c\u4ee5\u6355\u6349\u591a\u6e90\u6570\u636e\u7684\u5e95\u5c42\u7ed3\u6784\u3002\u6211\u4eec\u91cd\u65b0\u8bbe\u8ba1\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u7684\u56fa\u6709\u5c5e\u6027\u548c\u53d8\u5f02\u6027\u3002", "result": "\u6211\u4eec\u5728\u5206\u7c7b\u4efb\u52a1\u548c\u80fd\u91cf\u666f\u89c2\u91cd\u6784\u4e2d\u5c55\u793a\u4e86ZENN\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u4e86\u5176\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u9884\u6d4b\u9ad8\u9636\u5bfc\u6570\u65b9\u9762\u3002\u4f5c\u4e3a\u5b9e\u9645\u5e94\u7528\uff0c\u6211\u4eec\u4f7f\u7528ZENN\u91cd\u5efa\u4e86Fe3Pt\u7684Helmholtz\u80fd\u91cf\u666f\u89c2\uff0c\u5e76\u6355\u83b7\u4e86\u5173\u952e\u6750\u6599\u884c\u4e3a\uff0c\u5305\u62ec\u8d1f\u70ed\u81a8\u80c0\u548c\u6e29\u5ea6-\u538b\u529b\u7a7a\u95f4\u4e2d\u7684\u4e34\u754c\u70b9\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8ezentropy\u7406\u8bba\u7684\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86ZENN\u4f5c\u4e3a\u5904\u7406\u590d\u6742\u3001\u5f02\u6784\u6570\u636e\u96c6\u7684\u79d1\u5b66\u95ee\u9898\u7684\u591a\u529f\u80fd\u4e14\u7a33\u5065\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2505.10399", "pdf": "https://arxiv.org/pdf/2505.10399", "abs": "https://arxiv.org/abs/2505.10399", "authors": ["Kaivalya Rawal", "Zihao Fu", "Eoin Delaney", "Chris Russell"], "title": "Evaluating Model Explanations without Ground Truth", "categories": ["cs.AI", "cs.LG", "I.2.6"], "comment": "https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth", "summary": "There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\u8bc4\u4f30\u6846\u67b6AXE\uff0c\u8be5\u6846\u67b6\u4e0d\u4f9d\u8d56\u4e8e\u7406\u60f3\u201c\u5730\u9762\u771f\u5b9e\u201d\u89e3\u91ca\uff0c\u80fd\u591f\u72ec\u7acb\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\uff0c\u5e76\u7528\u4e8e\u68c0\u6d4b\u89e3\u91ca\u516c\u5e73\u6027\u6d17\u767d\u3002", "motivation": "\u5f53\u524d\u7684\u89e3\u91ca\u8bc4\u4f30\u6846\u67b6\u901a\u8fc7\u4e0e\u7406\u60f3\u7684\u201c\u5730\u9762\u771f\u5b9e\u201d\u89e3\u91ca\u8fdb\u884c\u6bd4\u8f83\u6216\u9a8c\u8bc1\u6a21\u578b\u5bf9\u91cd\u8981\u8f93\u5165\u7684\u654f\u611f\u6027\u6765\u8861\u91cf\u8d28\u91cf\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u4e2a\u671f\u671b\u7684\u539f\u5219\u6765\u6307\u5bfc\u5c40\u90e8\u7279\u5f81\u91cd\u8981\u6027\u89e3\u91ca\u7684\u8bc4\u4f30\u7b56\u7565\u7684\u53d1\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fd9\u4e9b\u539f\u5219\u7684\u6846\u67b6AXE\u3002", "result": "\u901a\u8fc7\u4e0e\u57fa\u7ebf\u6bd4\u8f83\uff0c\u9a8c\u8bc1\u4e86AXE\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u68c0\u6d4b\u89e3\u91ca\u516c\u5e73\u6027\u6d17\u767d\u65b9\u9762\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u9700\u8981\u7406\u60f3\u201c\u5730\u9762\u771f\u5b9e\u201d\u89e3\u91ca\u7684\u6846\u67b6AXE\uff0c\u53ef\u4ee5\u72ec\u7acb\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\uff0c\u5e76\u80fd\u591f\u68c0\u6d4b\u89e3\u91ca\u516c\u5e73\u6027\u6d17\u767d\u3002"}}
{"id": "2505.10143", "pdf": "https://arxiv.org/pdf/2505.10143", "abs": "https://arxiv.org/abs/2505.10143", "authors": ["Longchao Da", "Parth Mitesh Shah", "Kuan-Ru Liou", "Jiaxing Zhang", "Hua Wei"], "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4; H.3.3"], "comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track", "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6GE-Chat\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6280\u672f\u624b\u6bb5\u5b9e\u73b0\u51c6\u786e\u7684\u8bc1\u636e\u68c0\u7d22\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u7684\u8f93\u51fa\u5e76\u4e0d\u603b\u662f\u53ef\u9760\u7684\uff0c\u7528\u6237\u9700\u8981\u624b\u52a8\u8bc4\u4f30\u3002\u5e7b\u89c9\u54cd\u5e94\u5e38\u5e38\u5e26\u6765\u4fe1\u4efb\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u8bba\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6GE-Chat\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u4ee3\u7406\u7684\u54cd\u5e94\uff0c\u5e76\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u903b\u8f91\u751f\u6210\u3001\u591a\u8df3\u5b50\u56fe\u641c\u7d22\u548c\u57fa\u4e8e\u8574\u542b\u7684\u53e5\u5b50\u751f\u6210\u6280\u672f\u5b9e\u73b0\u51c6\u786e\u7684\u8bc1\u636e\u68c0\u7d22\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u81ea\u7531\u5f62\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u8bc6\u522b\u7cbe\u786e\u8bc1\u636e\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u68c0\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u8bba\u8d44\u6e90\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u5e76\u6709\u52a9\u4e8e\u5224\u65ad\u5176\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6GE-Chat\uff0c\u4ee5\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u54cd\u5e94\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u5e76\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u903b\u8f91\u751f\u6210\u3001\u591a\u8df3\u5b50\u56fe\u641c\u7d22\u548c\u57fa\u4e8e\u8574\u542b\u7684\u53e5\u5b50\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u73b0\u6709\u6a21\u578b\u5728\u81ea\u7531\u5f62\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u8bc6\u522b\u7cbe\u786e\u8bc1\u636e\u7684\u80fd\u529b\uff0c\u4e3a\u68c0\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u8bba\u7684\u8d44\u6e90\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u5e76\u6709\u52a9\u4e8e\u5224\u65ad\u5176\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.10016", "pdf": "https://arxiv.org/pdf/2505.10016", "abs": "https://arxiv.org/abs/2505.10016", "authors": ["Shijie Lyu"], "title": "Application of YOLOv8 in monocular downward multiple Car Target detection", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "comment": "Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering", "summary": "Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u6539\u8fdb\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u591a\u5c3a\u5ea6\u3001\u5c0f\u76ee\u6807\u548c\u8fdc\u8ddd\u79bb\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u5f53\u524d\u7684\u6280\u672f\u5982\u96f7\u8fbe\u3001\u6444\u50cf\u5934\u548c\u8f66\u8f86\u4f20\u611f\u5668\u7f51\u7edc\u5b58\u5728\u6210\u672c\u9ad8\u3001\u6613\u53d7\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u5f71\u54cd\u4ee5\u53ca\u5206\u8fa8\u7387\u6709\u9650\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u6539\u8fdb\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u96c6\u6210\u7ed3\u6784\u91cd\u65b0\u53c2\u6570\u5316\u6280\u672f\u3001\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u7f51\u7edc\u6a21\u578b\u548c\u65b0\u7684\u68c0\u6d4b\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u591a\u5c3a\u5ea6\u3001\u5c0f\u76ee\u6807\u548c\u8fdc\u8ddd\u79bb\u76ee\u6807\u7684\u9ad8\u6548\u7cbe\u786e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u540e\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5927\u76ee\u6807\u548c\u5c0f\u76ee\u6807\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523065%\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6539\u8fdb\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u9002\u5408\u81ea\u52a8\u9a7e\u9a76\u6bd4\u8d5b\uff0c\u5982Formula Student Autonomous China (FSAC)\uff0c\u5728\u5355\u76ee\u6807\u548c\u5c0f\u76ee\u6807\u68c0\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.09854", "pdf": "https://arxiv.org/pdf/2505.09854", "abs": "https://arxiv.org/abs/2505.09854", "authors": ["Harikrishna Kuttivelil", "Katia Obraczka"], "title": "Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence", "categories": ["cs.LG", "cs.ET", "cs.MA", "cs.SI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "As demand for intelligent services rises and edge devices become more\ncapable, distributed learning at the network edge has emerged as a key enabling\ntechnology. While existing paradigms like federated learning (FL) and\ndecentralized FL (DFL) enable privacy-preserving distributed learning in many\nscenarios, they face potential challenges in connectivity and synchronization\nimposed by resource-constrained and infrastructure-less environments. While\nmore robust, gossip learning (GL) algorithms have generally been designed for\nhomogeneous data distributions and may not suit all contexts. This paper\nintroduces Chisme, a novel suite of protocols designed to address the\nchallenges of implementing robust intelligence in the network edge,\ncharacterized by heterogeneous data distributions, episodic connectivity, and\nlack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and\nasynchronous GL (Chisme-GL) variants that enable collaborative yet\ndecentralized model training that considers underlying data heterogeneity. We\nintroduce a data similarity heuristic that allows agents to opportunistically\ninfer affinity with each other using the existing communication of model\nupdates in decentralized FL and GL. We leverage the heuristic to extend DFL's\nmodel aggregation and GL's model merge mechanisms for better personalized\ntraining while maintaining collaboration. While Chisme-DFL is a synchronous\ndecentralized approach whose resource utilization scales linearly with network\nsize, Chisme-GL is fully asynchronous and has a lower, constant resource\nrequirement independent of network size. We demonstrate that Chisme methods\noutperform their standard counterparts in model training over distributed and\nheterogeneous data in network scenarios ranging from less connected and\nreliable networks to fully connected and lossless networks.", "AI": {"tldr": "This paper introduces Chisme, a novel suite of protocols for robust intelligence at the network edge, addressing challenges of heterogeneous data distributions, episodic connectivity, and lack of infrastructure. It includes synchronous DFL and asynchronous GL variants, along with a data similarity heuristic for better personalized training while maintaining collaboration.", "motivation": "Existing paradigms like federated learning (FL) and decentralized FL (DFL) face challenges in connectivity and synchronization in resource-constrained and infrastructure-less environments. Gossip learning (GL) algorithms are robust but generally designed for homogeneous data distributions, which may not suit all contexts.", "method": "Chisme introduces a novel suite of protocols, including synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL), which enable collaborative yet decentralized model training. It also introduces a data similarity heuristic that allows agents to infer affinity with each other using existing communication of model updates in decentralized FL and GL.", "result": "Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.", "conclusion": "Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks."}}
{"id": "2505.10468", "pdf": "https://arxiv.org/pdf/2505.10468", "abs": "https://arxiv.org/abs/2505.10468", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "categories": ["cs.AI"], "comment": "32 pages, 14 figures, 11 tables", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "AI": {"tldr": "\u672c\u6587\u533a\u5206\u4e86AI\u4ee3\u7406\u548cAgentic AI\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u8bbe\u8ba1\u54f2\u5b66\u3001\u80fd\u529b\u4ee5\u53ca\u5e94\u7528\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u5404\u81ea\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u6f84\u6e05AI\u4ee3\u7406\u548cAgentic AI\u5728\u8bbe\u8ba1\u54f2\u5b66\u548c\u80fd\u529b\u4e0a\u7684\u4e0d\u540c\uff0c\u672c\u6587\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u6df1\u5165\u7814\u7a76\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u6982\u5ff5\u5206\u7c7b\u6cd5\u3001\u5e94\u7528\u6620\u5c04\u548c\u6311\u6218\u5206\u6790\uff0c\u6279\u5224\u6027\u5730\u533a\u5206\u4e86AI\u4ee3\u7406\u548cAgentic AI\uff0c\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u8bbe\u8ba1\u54f2\u5b66\u548c\u80fd\u529b\u5dee\u5f02\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6bd4\u8f83\u5206\u6790\uff0c\u6db5\u76d6\u4e86\u4e24\u79cd\u8303\u5f0f\u7684\u67b6\u6784\u6f14\u53d8\u3001\u64cd\u4f5c\u673a\u5236\u3001\u4ea4\u4e92\u98ce\u683c\u548c\u81ea\u4e3b\u6c34\u5e73\uff0c\u5e76\u8ba8\u8bba\u4e86\u5404\u81ea\u7684\u5e94\u7528\u9886\u57df\u548c\u72ec\u7279\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u5f00\u53d1\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684AI\u4ee3\u7406\u548cAgentic AI\u9a71\u52a8\u7684\u7cfb\u7edf\u63d0\u4f9b\u660e\u786e\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2505.10182", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.", "AI": {"tldr": "This study evaluates Reasoning CPT, a method that uses synthetic data to improve reasoning capabilities in large language models. The results show that Reasoning CPT improves performance across multiple domains and allows models to adjust their reasoning depth based on problem difficulty.", "motivation": "The study aims to evaluate Reasoning CPT, a form of continual pretraining that uses synthetic data to reconstruct hidden thought processes underlying texts, as an alternative to task-specific signals.", "method": "The study applies Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compares it to standard CPT on the MMLU benchmark.", "result": "Reasoning CPT improves performance across all evaluated domains, with significant gains on challenging problems. Models trained with hidden thoughts adjust the depth of their reasoning according to problem difficulty.", "conclusion": "Reasoning CPT consistently improves performance across all evaluated domains, and reasoning skills acquired in one domain transfer effectively to others."}}
{"id": "2505.10027", "pdf": "https://arxiv.org/pdf/2505.10027", "abs": "https://arxiv.org/abs/2505.10027", "authors": ["Shijie Lyu"], "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)", "summary": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u548c\u573a\u666f\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u9065\u611f\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5b9e\u9645\u610f\u4e49\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u53d6\u5f97\u4e86\u4e00\u5b9a\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u548c\u4fdd\u6301\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u5728LDM\u6a21\u578b\u7684\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f18\u5316\u51b3\u7b56\u76ee\u6807\u3002", "result": "\u5728RESISC45\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cPSNR\u63d0\u9ad8\u4e863-4dB\uff0cSSIM\u63d0\u9ad8\u4e860.08-0.11\uff0cLPIPS\u964d\u4f4e\u4e860.06-0.10\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u5316\u548c\u590d\u6742\u7684\u81ea\u7136\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u548c\u573a\u666f\u9002\u5e94\u6027\u65b9\u9762\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2505.09855", "pdf": "https://arxiv.org/pdf/2505.09855", "abs": "https://arxiv.org/abs/2505.09855", "authors": ["Alexander Y. Ku", "Thomas L. Griffiths", "Stephanie C. Y. Chan"], "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u4e2d\u7684\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\uff1a\u6743\u91cd\u5185\u5b66\u4e60\uff08IWL\uff09\u548c\u4e0a\u4e0b\u6587\u5185\u5b66\u4e60\uff08ICL\uff09\u3002\u901a\u8fc7\u501f\u9274\u8fdb\u5316\u751f\u7269\u5b66\u7684\u9002\u5e94\u7b56\u7565\uff0c\u6211\u4eec\u53d1\u73b0\u73af\u5883\u7a33\u5b9a\u6027\u548c\u7ebf\u7d22\u53ef\u9760\u6027\u5f71\u54cd\u4e86\u8fd9\u4e24\u79cd\u6a21\u5f0f\u7684\u5e73\u8861\u3002\u9ad8\u73af\u5883\u7a33\u5b9a\u6027\u503e\u5411\u4e8eIWL\uff0c\u800c\u9ad8\u7ebf\u7d22\u53ef\u9760\u6027\u4fc3\u8fdbICL\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5b66\u4e60\u52a8\u6001\u7684\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u76f8\u5bf9\u6210\u672c\u5047\u8bbe\u6765\u89e3\u91ca\u8fd9\u4e9b\u53d8\u5316\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6211\u4eec\u53d7\u5230\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u7c7b\u4f3c\u9002\u5e94\u7b56\u7565\u7684\u542f\u53d1\u3002", "method": "\u6211\u4eec\u4ece\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u7c7b\u4f3c\u7684\u9002\u5e94\u7b56\u7565\u4e2d\u83b7\u5f97\u7075\u611f\uff0c\u5982\u9057\u4f20\u7f16\u7801\uff08\u7c7b\u4f3c\u4e8eIWL\uff09\u548c\u8868\u578b\u53ef\u5851\u6027\uff08\u7c7b\u4f3c\u4e8eICL\uff09\u3002\u6211\u4eec\u5b9e\u9a8c\u64cd\u4f5c\u4e86\u8fd9\u4e9b\u53ef\u9884\u6d4b\u6027\u7684\u7ef4\u5ea6\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5b83\u4eec\u5bf9Transformer\u4e2dICL/IWL\u5e73\u8861\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u73af\u5883\u7a33\u5b9a\u6027\u660e\u663e\u6709\u5229\u4e8eIWL\uff0c\u800c\u9ad8\u7ebf\u7d22\u53ef\u9760\u6027\u589e\u5f3a\u4e86ICL\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u7a33\u5b9a\u6027\u8f83\u4f4e\u65f6\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u52a8\u6001\u663e\u793a\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u65f6\u7a7a\u6f14\u53d8\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\uff0c\u5177\u6709\u8bb8\u591a\u7c7b\u522b\u7684\u5206\u7c7b\uff09\uff0c\u4f1a\u53d1\u751f\u5178\u578b\u7684ICL\u5230IWL\u7684\u8f6c\u53d8\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\uff0c\u8f83\u5c11\u7c7b\u522b\u6216\u8f83\u6162\u7684ICL\u83b7\u53d6\uff09\u53ef\u80fd\u521d\u59cb\u4e3aIWL\u9636\u6bb5\uff0c\u968f\u540e\u88abICL\u4e3b\u5bfc\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u4e86\u4e00\u4e2a\u76f8\u5bf9\u6210\u672c\u5047\u8bbe\uff0c\u7528\u4e8e\u89e3\u91ca\u8fd9\u4e9b\u5b66\u4e60\u6a21\u5f0f\u7684\u8f6c\u53d8\uff0c\u786e\u7acb\u4e86\u53ef\u9884\u6d4b\u6027\u4f5c\u4e3a\u63a7\u5236Transformer\u4e2d\u9002\u5e94\u7b56\u7565\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u6765\u7406\u89e3ICL\u5e76\u6307\u5bfc\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2505.10543", "pdf": "https://arxiv.org/pdf/2505.10543", "abs": "https://arxiv.org/abs/2505.10543", "authors": ["Annie Wong", "Thomas B\u00e4ck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "title": "Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u81ea\u6211\u53cd\u601d\u3001\u542f\u53d1\u5f0f\u7a81\u53d8\u548c\u89c4\u5212\u4f5c\u4e3a\u63d0\u793a\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u4ee5\u6d4b\u8bd5\u4ee3\u7406\u7684\u9002\u5e94\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u5927\u7684\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u6218\u7565\u63d0\u793a\u53ef\u4ee5\u7f29\u5c0f\u8fd9\u79cd\u6027\u80fd\u5dee\u8ddd\u3002\u592a\u957f\u7684\u63d0\u793a\u4f1a\u8d1f\u9762\u5f71\u54cd\u8f83\u5c0f\u6a21\u578b\u7684\u57fa\u672c\u53cd\u5e94\u4efb\u52a1\uff0c\u800c\u8f83\u5927\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u591a\u7684\u9c81\u68d2\u6027\u3002\u5148\u8fdb\u7684\u63d0\u793a\u6280\u672f\u4e3b\u8981\u5bf9\u590d\u6742\u6e38\u620f\u4e2d\u7684\u8f83\u5c0f\u6a21\u578b\u6709\u76ca\uff0c\u4f46\u5bf9\u5df2\u7ecf\u8868\u73b0\u826f\u597d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6539\u5584\u8f83\u5c11\u3002\u7136\u800c\uff0c\u5148\u8fdb\u7684\u63a8\u7406\u65b9\u6cd5\u4f1a\u4ea7\u751f\u9ad8\u5ea6\u53ef\u53d8\u7684\u7ed3\u679c\uff0c\u53ef\u80fd\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\u5e76\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u5f53\u524d\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5982\u89c4\u5212\u3001\u63a8\u7406\u548c\u7a7a\u95f4\u534f\u8c03\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6301\u7eed\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4f5c\u4e3a\u81ea\u6211\u5b66\u4e60\u548c\u63a8\u7406\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u771f\u6b63\u6f5c\u529b\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u81ea\u6211\u53cd\u601d\u3001\u542f\u53d1\u5f0f\u7a81\u53d8\u548c\u89c4\u5212\u4f5c\u4e3a\u63d0\u793a\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u4ee5\u6d4b\u8bd5\u4ee3\u7406\u7684\u9002\u5e94\u80fd\u529b\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5404\u79cd\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u3002", "result": "\u8f83\u5927\u7684\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u6218\u7565\u63d0\u793a\u53ef\u4ee5\u7f29\u5c0f\u8fd9\u79cd\u6027\u80fd\u5dee\u8ddd\u3002\u592a\u957f\u7684\u63d0\u793a\u4f1a\u8d1f\u9762\u5f71\u54cd\u8f83\u5c0f\u6a21\u578b\u7684\u57fa\u672c\u53cd\u5e94\u4efb\u52a1\uff0c\u800c\u8f83\u5927\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u591a\u7684\u9c81\u68d2\u6027\u3002\u5148\u8fdb\u7684\u63d0\u793a\u6280\u672f\u4e3b\u8981\u5bf9\u590d\u6742\u6e38\u620f\u4e2d\u7684\u8f83\u5c0f\u6a21\u578b\u6709\u76ca\uff0c\u4f46\u5bf9\u5df2\u7ecf\u8868\u73b0\u826f\u597d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6539\u5584\u8f83\u5c11\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u5148\u8fdb\u7684\u63a8\u7406\u65b9\u6cd5\u4f1a\u4ea7\u751f\u9ad8\u5ea6\u53ef\u53d8\u7684\u7ed3\u679c\uff1a\u5f53\u63a8\u7406\u548c\u51b3\u7b56\u4e00\u81f4\u65f6\uff0c\u5b83\u4eec\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u4e5f\u53ef\u80fd\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\u5e76\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5982\u89c4\u5212\u3001\u63a8\u7406\u548c\u7a7a\u95f4\u534f\u8c03\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6301\u7eed\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u4ecd\u5b58\u5728\u6839\u672c\u6027\u7684\u4e0d\u8db3\uff0c\u53ef\u80fd\u65e0\u6cd5\u4ec5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u63d0\u793a\u6765\u5b8c\u5168\u514b\u670d\u3002"}}
{"id": "2505.10185", "pdf": "https://arxiv.org/pdf/2505.10185", "abs": "https://arxiv.org/abs/2505.10185", "authors": ["Seongyun Lee", "Seungone Kim", "Minju Seo", "Yongrae Jo", "Dongyoung Go", "Hyeonbin Hwang", "Jinho Park", "Xiang Yue", "Sean Welleck", "Graham Neubig", "Moontae Lee", "Minjoon Seo"], "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CoT Encyclopedie\uff0c\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u548c\u5f15\u5bfc\u6a21\u578b\u63a8\u7406\u7684\u81ea\u4e0b\u800c\u4e0a\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u63d0\u53d6\u63a8\u7406\u6807\u51c6\uff0c\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u63a8\u5bfc\u51fa\u5bf9\u6bd4\u6027\u6807\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u53ef\u89e3\u91ca\u548c\u5168\u9762\uff0c\u5e76\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u4eba\u7c7b\u76f4\u89c9\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u884c\u4e3a\u7684\u5168\u90e8\u591a\u6837\u6027\u3002\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u5206\u6790\u6a21\u578b\u63a8\u7406\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CoT Encyclopedie\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u4e0b\u800c\u4e0a\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u5f15\u5bfc\u6a21\u578b\u63a8\u7406\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4ece\u6a21\u578b\u751f\u6210\u7684CoT\u4e2d\u81ea\u52a8\u63d0\u53d6\u591a\u6837\u7684\u63a8\u7406\u6807\u51c6\uff0c\u5c06\u5b83\u4eec\u5d4c\u5165\u5230\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u805a\u7c7b\u6210\u4ee3\u8868\u6027\u7684\u7c7b\u522b\uff0c\u5e76\u63a8\u5bfc\u51fa\u5bf9\u6bd4\u6027\u6807\u51c6\u6765\u89e3\u91ca\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6846\u67b6\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u5168\u9762\u7684\u5206\u6790\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u7406\u89e3\u53ef\u4ee5\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff1a\u6211\u4eec\u53ef\u4ee5\u9884\u6d4b\u6a21\u578b\u53ef\u80fd\u4f7f\u7528\u7684\u7b56\u7565\uff0c\u5e76\u5f15\u5bfc\u5b83\u8d70\u5411\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5206\u6790\u548c\u5f15\u5bfc\u6a21\u578b\u63a8\u7406\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u5bf9\u63a8\u7406\u884c\u4e3a\u7684\u5f71\u54cd\u6bd4\u6570\u636e\u9886\u57df\u66f4\u5927\u3002"}}
{"id": "2505.10030", "pdf": "https://arxiv.org/pdf/2505.10030", "abs": "https://arxiv.org/abs/2505.10030", "authors": ["Miit Daga", "Dhriti Parikh", "Swarna Priya Ramu"], "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication", "summary": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86DeepSeqCoco\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u6930\u5b50\u6811\u56fe\u50cf\u4e2d\u51c6\u786e\u81ea\u52a8\u5730\u8bc6\u522b\u75be\u75c5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u6930\u5b50\u6811\u75be\u75c5\u5bf9\u519c\u4e1a\u4ea7\u91cf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5728\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff0c\u4f20\u7edf\u7684\u8015\u4f5c\u5b9e\u8df5\u9650\u5236\u4e86\u65e9\u671f\u8bca\u65ad\u548c\u5e72\u9884\u3002\u5f53\u524d\u7684\u75be\u75c5\u8bc6\u522b\u65b9\u6cd5\u662f\u624b\u52a8\u7684\u3001\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u4e14\u4e0d\u53ef\u6269\u5c55\u7684\u3002", "method": "DeepSeqCoco\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u6930\u5b50\u6811\u56fe\u50cf\u4e2d\u51c6\u786e\u81ea\u52a8\u5730\u8bc6\u522b\u75be\u75c5\u3002\u8be5\u6a21\u578b\u5728\u5404\u79cd\u4f18\u5316\u5668\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5305\u62ecSGD\u3001Adam\u548c\u6df7\u5408\u914d\u7f6e\uff0c\u4ee5\u627e\u5230\u51c6\u786e\u5ea6\u3001\u635f\u5931\u6700\u5c0f\u5316\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeepSeqCoco\u53ef\u4ee5\u8fbe\u5230\u9ad8\u8fbe99.5%\u7684\u51c6\u786e\u7387\uff08\u6bd4\u73b0\u6709\u6a21\u578b\u9ad8\u51fa\u6700\u591a5%\uff09\uff0c\u6df7\u5408SGD-Adam\u663e\u793a\u51fa\u6700\u4f4e\u7684\u9a8c\u8bc1\u635f\u59312.81%\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u65f6\u95f4\u6700\u591a\u51cf\u5c11\u4e8618%\uff0c\u9884\u6d4b\u65f6\u95f4\u6700\u591a\u51cf\u5c11\u4e8685%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cDeepSeqCoco\u6a21\u578b\u5728\u7cbe\u5ea6\u519c\u4e1a\u4e2d\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u53ef\u4ee5\u63d0\u4f9b\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u75be\u75c5\u76d1\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2505.09861", "pdf": "https://arxiv.org/pdf/2505.09861", "abs": "https://arxiv.org/abs/2505.09861", "authors": ["John Bencina", "Erkut Aykutlug", "Yue Chen", "Zerui Zhang", "Stephanie Sorenson", "Shao Tang", "Changshuai Wei"], "title": "LiDDA: Data Driven Attribution at LinkedIn", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ME"], "comment": null, "summary": "Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u5f52\u56e0\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u7ea7\u522b\u7684\u6570\u636e\u548c\u5916\u90e8\u5b8f\u89c2\u56e0\u7d20\uff0c\u5e76\u5728LinkedIn\u4e0a\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u65bd\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u5f52\u56e0\u662f\u73b0\u4ee3\u8425\u9500\u667a\u80fd\u7684\u57fa\u7840\uff0c\u5bf9\u4e8e\u4efb\u4f55\u8425\u9500\u4e1a\u52a1\u548c\u5e7f\u544a\u5e73\u53f0\u90fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u7ea7\u522b\u7684\u6570\u636e\u548c\u5916\u90e8\u56e0\u7d20\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u5f52\u56e0\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u6210\u5458\u7ea7\u6570\u636e\u3001\u805a\u5408\u7ea7\u6570\u636e\u4ee5\u53ca\u5916\u90e8\u5b8f\u89c2\u56e0\u7d20\u7684\u6574\u5408\u3002", "result": "\u672c\u6587\u5728LinkedIn\u5927\u89c4\u6a21\u5b9e\u65bd\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u663e\u8457\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u5206\u4eab\u4e86\u5bf9\u8425\u9500\u548c\u5e7f\u544a\u6280\u672f\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u7684\u5b66\u4e60\u548c\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u5f52\u56e0\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728LinkedIn\u7684\u5927\u89c4\u6a21\u5b9e\u65bd\u548c\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u5206\u4eab\u4e86\u5bf9\u8425\u9500\u548c\u5e7f\u544a\u6280\u672f\u9886\u57df\u7684\u5e7f\u6cdb\u9002\u7528\u7684\u5b66\u4e60\u548c\u89c1\u89e3\u3002"}}
{"id": "2306.07615", "pdf": "https://arxiv.org/pdf/2306.07615", "abs": "https://arxiv.org/abs/2306.07615", "authors": ["Heqin Zhu", "Quan Quan", "Qingsong Yao", "Zaiyi Liu", "S. Kevin Zhou"], "title": "UOD: Universal One-shot Detection of Anatomical Landmarks", "categories": ["cs.CV", "cs.AI"], "comment": "Eealy accepted by MICCAI 2023. 11pages, 4 figures, 2 tables. arXiv\n  admin note: text overlap with arXiv:2203.06433", "summary": "One-shot medical landmark detection gains much attention and achieves great\nsuccess for its label-efficient training process. However, existing one-shot\nlearning methods are highly specialized in a single domain and suffer domain\npreference heavily in the situation of multi-domain unlabeled data. Moreover,\none-shot learning is not robust that it faces performance drop when annotating\na sub-optimal image. To tackle these issues, we resort to developing a\ndomain-adaptive one-shot landmark detection framework for handling multi-domain\nmedical images, named Universal One-shot Detection (UOD). UOD consists of two\nstages and two corresponding universal models which are designed as\ncombinations of domain-specific modules and domain-shared modules. In the first\nstage, a domain-adaptive convolution model is self-supervised learned to\ngenerate pseudo landmark labels. In the second stage, we design a\ndomain-adaptive transformer to eliminate domain preference and build the global\ncontext for multi-domain data. Even though only one annotated sample from each\ndomain is available for training, the domain-shared modules help UOD aggregate\nall one-shot samples to detect more robust and accurate landmarks. We\ninvestigated both qualitatively and quantitatively the proposed UOD on three\nwidely-used public X-ray datasets in different anatomical domains (i.e., head,\nhand, chest) and obtained state-of-the-art performances in each domain. The\ncode is available at\nhttps://github.com/heqin-zhu/UOD_universal_oneshot_detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUOD\u7684\u9886\u57df\u81ea\u9002\u5e94\u5355\u6b21\u5730\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u9886\u57df\u533b\u5b66\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6b21\u5b66\u4e60\u65b9\u6cd5\u9ad8\u5ea6\u4e13\u4e1a\u5316\u4e8e\u5355\u4e00\u9886\u57df\uff0c\u5e76\u4e14\u5728\u591a\u9886\u57df\u672a\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u53d7\u5230\u9886\u57df\u504f\u597d\u7684\u4e25\u91cd\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u5355\u6b21\u5b66\u4e60\u5728\u6807\u6ce8\u6b21\u4f18\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "UOD\u7531\u4e24\u4e2a\u9636\u6bb5\u548c\u4e24\u4e2a\u76f8\u5e94\u7684\u901a\u7528\u6a21\u578b\u7ec4\u6210\uff0c\u8fd9\u4e9b\u6a21\u578b\u8bbe\u8ba1\u4e3a\u9886\u57df\u7279\u5b9a\u6a21\u5757\u548c\u9886\u57df\u5171\u4eab\u6a21\u5757\u7684\u7ec4\u5408\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u751f\u6210\u4f2a\u5730\u6807\u6807\u7b7e\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u9886\u57df\u81ea\u9002\u5e94\u53d8\u538b\u5668\u6d88\u9664\u9886\u57df\u504f\u597d\u5e76\u6784\u5efa\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5373\u4f7f\u6bcf\u4e2a\u9886\u57df\u53ea\u63d0\u4f9b\u4e00\u4e2a\u6ce8\u91ca\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u9886\u57df\u5171\u4eab\u6a21\u5757\u5e2e\u52a9UOD\u805a\u5408\u6240\u6709\u5355\u6b21\u6837\u672c\u4ee5\u68c0\u6d4b\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u7684\u5730\u6807\u3002", "conclusion": "UOD\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u516c\u5171X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u57df\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.10202", "pdf": "https://arxiv.org/pdf/2505.10202", "abs": "https://arxiv.org/abs/2505.10202", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "YiMing Cheng", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVQ-Logits\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u5927\u5e45\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u5c42\u7684\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002\u901a\u8fc7\u5c06\u5927\u7684V * dmodel\u8f93\u51fa\u5d4c\u5165\u77e9\u9635\u66ff\u6362\u4e3a\u4e00\u4e2a\u5c0f\u7684\u3001\u5171\u4eab\u7684K\u4e2a\u5d4c\u5165\u5411\u91cf\u7684\u4ee3\u7801\u672c\uff08K << V\uff09\uff0c\u6bcf\u4e2a\u8bcd\u6c47\u8868\u4e2d\u7684\u6807\u8bb0\u88ab\u6620\u5c04\u5230\u8fd9\u4e9bK\u4e2a\u4ee3\u7801\u672c\u5411\u91cf\u4e4b\u4e00\u3002LLM\u5728\u7d27\u51d1\u7684\u4ee3\u7801\u672c\u4e0a\u9884\u6d4blogits\uff0c\u7136\u540e\u4f7f\u7528\u5b66\u4e60\u6216\u9884\u5206\u914d\u7684\u6620\u5c04\u9ad8\u6548\u5730\u201c\u6563\u5f00\u201d\u5230\u5b8c\u6574\u7684\u8bcd\u6c47\u7a7a\u95f4\u3002\u5b9e\u9a8c\u8868\u660e\uff0cVQ-Logits\u53ef\u4ee5\u5728\u8f93\u51fa\u5c42\u5b9e\u73b0\u9ad8\u8fbe99%\u7684\u53c2\u6570\u51cf\u5c11\uff0c\u5e76\u5728logit\u8ba1\u7b97\u4e2d\u5b9e\u73b06\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e0e\u5b8c\u6574softmax\u57fa\u7ebf\u76f8\u6bd4\uff0c\u56f0\u60d1\u5ea6\u4ec5\u589e\u52a04%\u3002", "motivation": "Large Language Models (LLMs) have achieved remarkable success but face significant computational and memory challenges, particularly due to their extensive output vocabularies. The final linear projection layer, mapping hidden states to vocabulary-sized logits, often constitutes a substantial portion of the model's parameters and computational cost during inference.", "method": "VQ-Logits replaces the large V * dmodel output embedding matrix with a small, shared codebook of K embedding vectors (K << V ). Each token in the vocabulary is mapped to one of these K codebook vectors. The LLM predicts logits over this compact codebook, which are then efficiently 'scattered' to the full vocabulary space using the learned or preassigned mapping.", "result": "We demonstrate through extensive experiments on standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines. We further provide detailed ablation studies on codebook size, initialization, and learning strategies, showcasing the robustness and effectiveness of our approach.", "conclusion": "VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines."}}
{"id": "2505.10046", "pdf": "https://arxiv.org/pdf/2505.10046", "abs": "https://arxiv.org/abs/2505.10046", "authors": ["Bingda Tang", "Boyang Zheng", "Xichen Pan", "Sayak Paul", "Saining Xie"], "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.", "AI": {"tldr": "\u672c\u6587\u4e0d\u63cf\u8ff0\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u800c\u662f\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u4e00\u4e2a\u91cd\u8981\u7684\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u8bbe\u8ba1\u7a7a\u95f4\u8fdb\u884c\u4e86\u6df1\u5165\u63a2\u8ba8\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6269\u6563\u53d8\u538b\u5668\uff08DiTs\uff09\u7684\u6df1\u5ea6\u878d\u5408\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\uff0c\u800c\u4e0d\u662f\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u7684\u8be6\u7ec6\u6bd4\u8f83\uff0c\u5173\u952e\u8bbe\u8ba1\u7ec6\u8282\u548c\u8bad\u7ec3\u65b9\u6848\u901a\u5e38\u672a\u88ab\u516c\u5f00\u3002\u8fd9\u4e9b\u5dee\u8ddd\u5bfc\u81f4\u4e86\u5bf9\u8be5\u65b9\u6cd5\u5b9e\u9645\u6f5c\u529b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u5173\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u8fdb\u884c\u53d7\u63a7\u6bd4\u8f83\uff0c\u5206\u6790\u91cd\u8981\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u63d0\u4f9b\u53ef\u6e05\u6670\u590d\u73b0\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u4e3a\u591a\u6a21\u6001\u751f\u6210\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6570\u636e\u70b9\u548c\u5b9e\u7528\u6307\u5357\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u4e3a\u591a\u6a21\u6001\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2505.09864", "pdf": "https://arxiv.org/pdf/2505.09864", "abs": "https://arxiv.org/abs/2505.09864", "authors": ["Aditya Panangat"], "title": "BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks", "categories": ["cs.LG"], "comment": "6 pages, 0 figures, 2 tables", "summary": "Over the past decade, the use of machine learning has increased\nexponentially. Models are far more complex than ever before, growing to\ngargantuan sizes and housing millions of weights. Unfortunately, the fact that\nlarge models have become the state of the art means that it often costs\nmillions of dollars to train and operate them. These expenses not only hurt\ncompanies but also bar non-wealthy individuals from contributing to new\ndevelopments and force consumers to pay greater prices for AI. Current methods\nused to prune models, such as iterative magnitude pruning, have shown great\naccuracy but require an iterative training sequence that is incredibly\ncomputationally and environmentally taxing. To solve this problem, BINGO is\nintroduced. BINGO, during the training pass, studies specific subsets of a\nneural network one at a time to gauge how significant of a role each weight\nplays in contributing to a network's accuracy. By the time training is done,\nBINGO generates a significance score for each weight, allowing for\ninsignificant weights to be pruned in one shot. BINGO provides an\naccuracy-preserving pruning technique that is less computationally intensive\nthan current methods, allowing for a world where AI growth does not have to\nmean model growth, as well.", "AI": {"tldr": "BINGO is a new pruning technique that reduces the computational cost of training large machine learning models while preserving their accuracy.", "motivation": "The increasing complexity and cost of large machine learning models have made them inaccessible to non-wealthy individuals and increased costs for consumers. Current pruning methods are computationally and environmentally taxing.", "method": "BINGO studies specific subsets of a neural network one at a time during the training pass to gauge how significant each weight plays in contributing to a network's accuracy. It generates a significance score for each weight, allowing for insignificant weights to be pruned in one shot.", "result": "BINGO offers a less computationally intensive pruning technique that preserves accuracy, making AI development more accessible and sustainable.", "conclusion": "BINGO provides an accuracy-preserving pruning technique that is less computationally intensive than current methods, allowing for a world where AI growth does not have to mean model growth."}}
{"id": "2410.13778", "pdf": "https://arxiv.org/pdf/2410.13778", "abs": "https://arxiv.org/abs/2410.13778", "authors": ["Michelangelo Olmo Nogara Notarianni", "Filippo Leveni", "Diego Stucchi", "Luca Frittoli", "Giacomo Boracchi"], "title": "Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "AALTD workshop at ECML 2024 (https://ecml-aaltd.github.io/aaltd2024/)", "summary": "We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.", "AI": {"tldr": "KQT-EWMA \u662f\u4e00\u79cd\u975e\u53c2\u6570\u53d8\u5316\u68c0\u6d4b\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86 KQT \u76f4\u65b9\u56fe\u548c EWMA \u7edf\u8ba1\u91cf\uff0c\u7528\u4e8e\u5728\u7ebf\u76d1\u63a7\u591a\u53d8\u91cf\u6570\u636e\u6d41\u3002\u5b83\u80fd\u591f\u63a7\u5236 ARL_0\uff0c\u5e76\u4e14\u5728\u68c0\u6d4b\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u591a\u6570\u975e\u53c2\u6570\u53d8\u5316\u68c0\u6d4b\u6d4b\u8bd5\u5f88\u5c11\u80fd\u4e8b\u5148\u63a7\u5236 ARL_0\uff0c\u800c KQT-EWMA \u80fd\u591f\u901a\u8fc7\u64cd\u4f5c\u9884\u5b9a\u4e49\u7684 ARL_0 \u6765\u63a7\u5236\u8bef\u62a5\u3002", "method": "KQT-EWMA \u7ed3\u5408\u4e86 Kernel-QuantTree (KQT) \u76f4\u65b9\u56fe\u548c EWMA \u7edf\u8ba1\u91cf\uff0c\u7528\u4e8e\u5728\u7ebf\u76d1\u63a7\u591a\u53d8\u91cf\u6570\u636e\u6d41\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKQT-EWMA \u5728\u63a7\u5236 ARL_0 \u7684\u540c\u65f6\uff0c\u68c0\u6d4b\u5ef6\u8fdf\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "KQT-EWMA \u53ef\u4ee5\u63a7\u5236 ARL_0\uff0c\u540c\u65f6\u5728\u68c0\u6d4b\u5ef6\u8fdf\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f4e\u3002"}}
{"id": "2505.10218", "pdf": "https://arxiv.org/pdf/2505.10218", "abs": "https://arxiv.org/abs/2505.10218", "authors": ["Zongsheng Wang", "Kaili Sun", "Bowen Wu", "Qun Yu", "Ying Li", "Baoxun Wang"], "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward", "categories": ["cs.CL"], "comment": null, "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RAIDEN-R1\uff0c\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4ee3\u7406\u7684\u89d2\u8272\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5f15\u5165\u53ef\u9a8c\u8bc1\u7684\u89d2\u8272\u610f\u8bc6\u5956\u52b1\u548c\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4ee3\u7406\uff08RPCAs\uff09\u5728\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u9762\u4e34\u6301\u7eed\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u96be\u4ee5\u91cf\u5316\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6RAIDEN-R1\uff0c\u96c6\u6210\u4e86\u53ef\u9a8c\u8bc1\u7684\u89d2\u8272\u610f\u8bc6\u5956\u52b1\uff08VRAR\uff09\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u5355\u4e2a\u548c\u591a\u9879\u6316\u6398\u7b56\u7565\uff0c\u901a\u8fc7\u8bc4\u4f30\u7279\u5b9a\u89d2\u8272\u7684\u5173\u952e\u56e0\u7d20\u6765\u751f\u6210\u53ef\u91cf\u5316\u7684\u5956\u52b1\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u591aLLM\u534f\u4f5c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u89d2\u8272\u610f\u8bc6\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u4ee5\u63d0\u9ad8\u63a8\u7406\u8fde\u8d2f\u6027\u3002", "result": "\u5728RAIDEN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAIDEN-R1\u8868\u73b0\u51fa\u8272\uff1a\u6211\u4eec\u768414B-GRPO\u6a21\u578b\u5728\u57fa\u4e8e\u811a\u672c\u7684\u77e5\u8bc6\u548c\u5bf9\u8bdd\u8bb0\u5fc6\u6307\u6807\u4e0a\u5206\u522b\u8fbe\u5230\u4e8688.04%\u548c88.65%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u3002\u6848\u4f8b\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u6a21\u578b\u5728\u89e3\u51b3\u51b2\u7a81\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u7ef4\u6301\u7b2c\u4e00\u4eba\u79f0\u53d9\u8ff0\u4e00\u81f4\u6027\u65b9\u9762\u7684\u80fd\u529b\u589e\u5f3a\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86RPCA\u8bad\u7ec3\u4e2d\u975e\u91cf\u5316\u6027\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u89d2\u8272\u611f\u77e5\u63a8\u7406\u6a21\u5f0f\u7684\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86RPCA\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.10049", "pdf": "https://arxiv.org/pdf/2505.10049", "abs": "https://arxiv.org/abs/2505.10049", "authors": ["Jinlong Fan", "Xuepu Zeng", "Jing Zhang", "Mingming Gong", "Yuxiang Yang", "Dacheng Tao"], "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86200\u591a\u7bc7\u5173\u4e8e\u4f7f\u7528\u8f90\u5c04\u573a\u8fdb\u884c\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u4e2d\u7684\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u8868\u793a\u548c\u91cd\u5efa\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u8bb8\u591a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u6982\u8ff0\uff0c\u5e2e\u52a9\u4ed6\u4eec\u4e86\u89e3\u8be5\u9886\u57df\u7684\u6982\u5ff5\u539f\u7406\u548c\u5b9e\u8df5\u524d\u6cbf\u3002", "method": "\u672c\u6587\u901a\u8fc7\u591a\u4e2a\u5173\u952e\u89c6\u89d2\u5bf9\u8fd9\u4e9b\u5de5\u4f5c\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u8bc4\u4f30\uff0c\u5305\u62ec\u8fd0\u52a8\u8868\u793a\u8303\u5f0f\u3001\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u52a8\u6001\u7684\u91cd\u5efa\u6280\u672f\u3001\u8f85\u52a9\u4fe1\u606f\u96c6\u6210\u7b56\u7565\u4ee5\u53ca\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u5730\u5206\u6790\u4e86200\u591a\u7bc7\u5173\u4e8e\u4f7f\u7528\u8f90\u5c04\u573a\u8fdb\u884c\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u8bba\u6587\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790200\u591a\u7bc7\u5173\u4e8e\u4f7f\u7528\u8f90\u5c04\u573a\u8fdb\u884c\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\uff0c\u5e76\u5bf9\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u548c\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u6279\u5224\u6027\u5ba1\u89c6\u3002"}}
{"id": "2505.09901", "pdf": "https://arxiv.org/pdf/2505.09901", "abs": "https://arxiv.org/abs/2505.09901", "authors": ["Ziyuan Zhang", "Darcy Wang", "Ningyuan Chen", "Rodrigo Mansur", "Vahid Sarhangian"], "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4eba\u7c7b\u548cMAB\u7b97\u6cd5\u7684\u63a2\u7d22-\u5229\u7528\u7b56\u7565\uff0c\u5e76\u53d1\u73b0\u63a8\u7406\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u6211\u4eec\u60f3\u4e86\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u5e76\u4e14\u80fd\u5426\u5b9e\u73b0\u76f8\u5f53\uff08\u6216\u66f4\u597d\uff09\u7684\u6027\u80fd\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u9009\u62e9\u6a21\u578b\u6765\u6355\u6349\u4ee3\u7406\u7684\u63a2\u7d22-\u5229\u7528\u7b56\u7565\uff0c\u5e76\u7814\u7a76\u901a\u8fc7\u63d0\u793a\u7b56\u7565\u548c\u589e\u5f3a\u63a8\u7406\u7684\u6a21\u578b\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u63a8\u7406\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u8fd9\u79cd\u884c\u4e3a\u7531\u968f\u673a\u548c\u5b9a\u5411\u63a2\u7d22\u7684\u6df7\u5408\u7ec4\u6210\u3002\u5728\u7b80\u5355\u7684\u9759\u6001\u4efb\u52a1\u4e2d\uff0c\u63a8\u7406\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u6c34\u5e73\u7684\u968f\u673a\u548c\u5b9a\u5411\u63a2\u7d22\u3002\u7136\u800c\uff0c\u5728\u66f4\u590d\u6742\u3001\u975e\u9759\u6001\u7684\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5339\u914d\u4eba\u7c7b\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6709\u6548\u7684\u5b9a\u5411\u63a2\u7d22\u65b9\u9762\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u76f8\u4f3c\u7684\u9057\u61be\u503c\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u548c\u81ea\u52a8\u5316\u51b3\u7b56\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u53ef\u80fd\u7684\u6539\u8fdb\u9886\u57df\u3002"}}
{"id": "2505.03084", "pdf": "https://arxiv.org/pdf/2505.03084", "abs": "https://arxiv.org/abs/2505.03084", "authors": ["Shashank Kapoor", "Sanjay Surendranath Girija", "Lakshit Arora", "Dipen Pradhan", "Ankit Shetgaonkar", "Aman Raj"], "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in IEEE COMPSAC 2025", "summary": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9488\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u56db\u79cd\u6a21\u6001\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u4e16\u754c\u4e2d\u5bf9\u6297\u6027\u5a01\u80c1\u666f\u89c2\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u591a\u6a21\u6001\u5bf9\u6297\u6027\u5a01\u80c1\u6f14\u5316\u7684\u5168\u9762\u6982\u8ff0\u3002", "motivation": "\u7531\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u5bf9\u6297\u6027\u5a01\u80c1\u7684\u89c6\u89d2\uff0c\u4ee5\u4fbf\u91c7\u53d6\u5fc5\u8981\u7684\u9884\u9632\u63aa\u65bd\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u591a\u6a21\u6001\u4e16\u754c\u7684\u5b9e\u8df5\u8005\u5bfc\u5411\u7684\u653b\u51fb\u7c7b\u578b\u6982\u8ff0\u3002", "method": "\u672c\u6587\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u56db\u79cd\u6a21\u6001\u7684\u5bf9\u6297\u6027\u653b\u51fb\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u4ee5\u63d0\u4f9b\u591a\u6a21\u6001\u5bf9\u6297\u6027\u5a01\u80c1\u7684\u5168\u9762\u6982\u8ff0\u3002", "result": "\u672c\u6587\u9996\u6b21\u5168\u9762\u603b\u7ed3\u4e86\u591a\u6a21\u6001\u4e16\u754c\u4e2d\u7684\u5a01\u80c1\u666f\u89c2\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6297\u6027\u5a01\u80c1\u6f14\u5316\u7684\u89c6\u56fe\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u8c03\u67e5\u9488\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u56db\u79cd\u6a21\u6001\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u4e16\u754c\u4e2d\u5bf9\u6297\u6027\u5a01\u80c1\u666f\u89c2\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u591a\u6a21\u6001\u5bf9\u6297\u6027\u5a01\u80c1\u6f14\u5316\u7684\u5168\u9762\u603b\u7ed3\u3002"}}
{"id": "2505.10260", "pdf": "https://arxiv.org/pdf/2505.10260", "abs": "https://arxiv.org/abs/2505.10260", "authors": ["Poli Apollinaire Nemkova", "Solomon Ubani", "Mark V. Albert"], "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u5148\u8fdbLLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u6761\u4ef6\u4e0b\u5bf9\u5305\u542b\u4fc4\u8bed\u548c\u4e4c\u514b\u5170\u8bed\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u7684\u590d\u6742\u6587\u672c\u6570\u636e\u96c6\u8fdb\u884c\u4e8c\u5206\u7c7b\u6807\u6ce8\u7684\u80fd\u529b\uff0c\u91cd\u70b9\u662f\u8bc6\u522b\u5176\u4e2d\u7684\u4eba\u6743\u4fb5\u72af\u53c2\u8003\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86LLM\u5728\u5904\u7406\u4e3b\u89c2\u548c\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u5224\u65ad\u65f6\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u5728\u8de8\u8bed\u8a00\u9002\u5e94\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7cfb\u7edf\u7684\u65e5\u76ca\u590d\u6742\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u5305\u62ec\u9700\u8981\u7ec6\u5fae\u6587\u672c\u7406\u89e3\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u654f\u611f\u4e14\u7279\u5b9a\u9886\u57df\u7684\u4efb\u52a1\uff0c\u5982\u8bc6\u522b\u4eba\u6743\u4fb5\u72af\u53c2\u8003\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76LLMs\u7684\u53ef\u9760\u6027\u4e0e\u9002\u7528\u6027\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6700\u5148\u8fdb\u7684LLM\uff08GPT-3.5\u3001GPT-4\u3001LLAMA3\u3001Mistral 7B\u548cClaude-2\uff09\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u6761\u4ef6\u4e0b\u5bf9\u5305\u542b\u4fc4\u8bed\u548c\u4e4c\u514b\u5170\u8bed\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u7684\u590d\u6742\u6587\u672c\u6570\u636e\u96c6\u8fdb\u884c\u4e8c\u5206\u7c7b\u6807\u6ce8\u7684\u80fd\u529b\uff0c\u91cd\u70b9\u662f\u8bc6\u522b\u5176\u4e2d\u7684\u4eba\u6743\u4fb5\u72af\u53c2\u8003\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u6807\u6ce8\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u4e86\u6bcf\u79cd\u6a21\u578b\u8868\u73b0\u51fa\u7684\u72ec\u7279\u9519\u8bef\u6a21\u5f0f\u548c\u5206\u6b67\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540cLLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u6807\u6ce8\u6027\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5904\u7406\u4e3b\u89c2\u548c\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u5224\u65ad\u65f6\u7684\u4f18\u7f3a\u70b9\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u9002\u5e94\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06LLM\u8f93\u51fa\u4e0e\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4\uff0c\u6709\u52a9\u4e8e\u7406\u89e3LLM\u5728\u591a\u8bed\u8a00\u80cc\u666f\u4e0b\u5bf9\u654f\u611f\u9886\u57df\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u672c\u8d28\u4e0a\u4e3b\u89c2\u548c\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u5224\u65ad\uff0c\u8fd9\u5bf9\u4e8e\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.10055", "pdf": "https://arxiv.org/pdf/2505.10055", "abs": "https://arxiv.org/abs/2505.10055", "authors": ["Ijazul Haq", "Yingjie Zhang", "Irfan Ali Khan"], "title": "PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8ePashto OCR\u7684\u5408\u6210\u6570\u636e\u96c6PsOCR\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0Gemini\u8868\u73b0\u6700\u4f73\uff0c\u800cQwen-7B\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u7531\u4e8ePashto\u8bed\u8a00\u7684\u811a\u672c\u5177\u6709\u8fde\u7b14\u5b57\u7279\u6027\u4e14\u7ed3\u6784\u5316\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u56e0\u6b64\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684Pashto OCR\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u4e00\u767e\u4e07\u5f20\u56fe\u50cf\u7684\u5408\u6210Pashto OCR\u6570\u636e\u96c6PsOCR\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5728\u5355\u8bcd\u3001\u884c\u548c\u6587\u6863\u7ea7\u522b\u4e0a\u5e26\u6709\u8fb9\u754c\u6846\u6ce8\u91ca\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\uff08\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cTransformer\uff09\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u9009\u62e9\u4e8610,000\u5f20\u56fe\u50cf\u7684\u57fa\u51c6\u5b50\u96c6\u6765\u8bc4\u4f30\u591a\u4e2a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGemini\u5728\u6240\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\uff0cQwen-7B\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u5bf9\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728Pashto\u8bed\u8a00OCR\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u8fdb\u884c\u4e86\u6df1\u5165\u8bc4\u4f30\uff0c\u5e76\u4e3aPashto OCR\u4ee5\u53ca\u5176\u4ed6\u7c7b\u4f3c\u811a\u672c\uff08\u5982\u963f\u62c9\u4f2f\u8bed\u3001\u6ce2\u65af\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\uff09\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09907", "pdf": "https://arxiv.org/pdf/2505.09907", "abs": "https://arxiv.org/abs/2505.09907", "authors": ["Linwei Zhang", "LuFeng", "Ruijia Liang"], "title": "Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "With the growing demand for healthy foods, agricultural product price\nforecasting has become increasingly important. Hass avocados, as a high-value\ncrop, exhibit complex price fluctuations influenced by factors such as\nseasonality, region, and weather. Traditional prediction models often struggle\nwith highly nonlinear and dynamic data. To address this, we propose a hybrid\ndeep learning model, TCN-MLP-Attention Architecture, combining Temporal\nConvolutional Networks (TCN) for sequential feature extraction, Multi-Layer\nPerceptrons (MLP) for nonlinear interactions, and an Attention mechanism for\ndynamic feature weighting. The dataset used covers over 50,000 records of Hass\navocado sales across the U.S. from 2015 to 2018, including variables such as\nsales volume, average price, time, region, weather, and variety type, collected\nfrom point-of-sale systems and the Hass Avocado Board. After systematic\npreprocessing, including missing value imputation and feature normalization,\nthe proposed model was trained and evaluated. Experimental results demonstrate\nthat the TCN-MLP-Attention model achieves excellent predictive performance,\nwith an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.\nThis research provides a scalable and effective approach for time series\nforecasting in agricultural markets and offers valuable insights for\nintelligent supply chain management and price strategy optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bTCN-MLP-Attention\uff0c\u7528\u4e8e\u54c8\u65af\u725b\u6cb9\u679c\u4ef7\u683c\u9884\u6d4b\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4e3a\u519c\u4e1a\u5e02\u573a\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "motivation": "\u968f\u7740\u5bf9\u5065\u5eb7\u98df\u54c1\u9700\u6c42\u7684\u589e\u957f\uff0c\u519c\u4ea7\u54c1\u4ef7\u683c\u9884\u6d4b\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u54c8\u65af\u725b\u6cb9\u679c\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u4ef7\u503c\u4f5c\u7269\uff0c\u5176\u4ef7\u683c\u6ce2\u52a8\u53d7\u5230\u5b63\u8282\u6027\u3001\u5730\u533a\u548c\u5929\u6c14\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5728\u5904\u7406\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u52a8\u6001\u6570\u636e\u65f6\u5e38\u5e38\u9047\u5230\u56f0\u96be\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0cTCN-MLP-Attention\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u7528\u4e8e\u5e8f\u5217\u7279\u5f81\u63d0\u53d6\uff0c\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7528\u4e8e\u975e\u7ebf\u6027\u4ea4\u4e92\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\u7528\u4e8e\u52a8\u6001\u7279\u5f81\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTCN-MLP-Attention\u6a21\u578b\u5177\u6709\u51fa\u8272\u7684\u9884\u6d4b\u6027\u80fd\uff0cRMSE\u4e3a1.23\uff0cMSE\u4e3a1.51\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u519c\u4e1a\u5e02\u573a\u7684\u667a\u80fd\u4f9b\u5e94\u94fe\u7ba1\u7406\u548c\u4ef7\u683c\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.09593", "pdf": "https://arxiv.org/pdf/2505.09593", "abs": "https://arxiv.org/abs/2505.09593", "authors": ["Filippo Leveni", "Guilherme Weigert Cassales", "Bernhard Pfahringer", "Albert Bifet", "Giacomo Boracchi"], "title": "Online Isolation Forest", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at International Conference on Machine Learning (ICML 2024)", "summary": "The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.", "AI": {"tldr": "Online-iForest \u662f\u4e00\u79cd\u4e13\u4e3a\u6d41\u5f0f\u6570\u636e\u8bbe\u8ba1\u7684\u65b0\u578b\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6240\u6709\u7ade\u4e89\u5bf9\u624b\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u68c0\u6d4b\u5f02\u5e38\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u7ea6\u675f\uff0c\u4f9d\u8d56\u4e8e\u5b9a\u671f\u91cd\u65b0\u8bad\u7ec3\u4ee5\u9002\u5e94\u5728\u7ebf\u73af\u5883\u3002", "method": "Online-iForest \u662f\u4e00\u79cd\u4e13\u4e3a\u6d41\u5f0f\u6761\u4ef6\u8bbe\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u8ddf\u8e2a\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cOnline-iForest \u4e0e\u5728\u7ebf\u66ff\u4ee3\u65b9\u6848\u76f8\u5f53\uff0c\u5e76\u4e14\u4e0e\u5b9a\u671f\u91cd\u65b0\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u79bb\u7ebf\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u76f8\u5ab2\u7f8e\u3002", "conclusion": "Online-iForest \u662f\u4e00\u79cd\u5728\u5b9e\u65f6\u6570\u636e\u6d41\u4e2d\u68c0\u6d4b\u5f02\u5e38\u7684\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u9700\u8981\u5feb\u901f\u8bc6\u522b\u5f02\u5e38\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.10261", "pdf": "https://arxiv.org/pdf/2505.10261", "abs": "https://arxiv.org/abs/2505.10261", "authors": ["Rui Yang", "Huitao Li", "Matthew Yu Heng Wong", "Yuhe Ke", "Xin Li", "Kunyu Yu", "Jingchi Liao", "Jonathan Chong Kai Liew", "Sabarinath Vinod Nair", "Jasmine Chiat Ling Ong", "Irene Li", "Douglas Teodoro", "Chuan Hong", "Daniel Shu Wei Ting", "Nan Liu"], "title": "The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e8619,123\u9879\u7814\u7a76\uff0c\u53d1\u73b0\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u6027\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f46\u4f26\u7406\u4f7f\u7528\u8fd9\u4e9b\u6280\u672f\u5bf9\u533b\u7597\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5728\u4e0d\u540c\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u5dee\u5f02\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5b83\u4eec\u7684\u4f18\u52a3\u3002", "method": "\u5206\u6790\u4e8619,123\u9879\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5728\u4e0d\u540c\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u751f\u6210\u5f0fLLMs\u5728\u5f00\u653e\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u800c\u4f20\u7edfNLP\u5728\u4fe1\u606f\u63d0\u53d6\u548c\u5206\u6790\u4efb\u52a1\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "\u968f\u7740\u8fd9\u4e9b\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u786e\u4fdd\u5b83\u4eec\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u4f26\u7406\u4f7f\u7528\u5b83\u4eec\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002"}}
{"id": "2505.10072", "pdf": "https://arxiv.org/pdf/2505.10072", "abs": "https://arxiv.org/abs/2505.10072", "authors": ["Rui-Yang Ju", "Sheng-Yen Huang", "Yi-Ping Hung"], "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.", "AI": {"tldr": "ToonifyGB is a two-stage framework that uses an improved StyleGAN to generate a stylized video, which is then used to learn a stylized neutral head model and expression blendshapes for rendering stylized avatars with arbitrary expressions.", "motivation": "The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based framework, has become widely used for facial image stylization. However, it is limited in its ability to synthesize diverse stylized 3D head avatars using Gaussian blendshapes.", "method": "ToonifyGB is an efficient two-stage framework that first generates a stylized video using an improved StyleGAN, and then learns a stylized neutral head model and a set of expression blendshapes from the generated video to render stylized avatars with arbitrary expressions.", "result": "ToonifyGB successfully generates a more stable video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, and efficiently generate high-quality animation in the next stage. It also learns a stylized neutral head model and a set of expression blendshapes from the generated video, allowing for the efficient rendering of stylized avatars with arbitrary expressions.", "conclusion": "ToonifyGB can efficiently render stylized avatars with arbitrary expressions, and its effectiveness has been validated on benchmark datasets using two styles: Arcane and Pixar."}}
{"id": "2505.09922", "pdf": "https://arxiv.org/pdf/2505.09922", "abs": "https://arxiv.org/abs/2505.09922", "authors": ["Zichen Liu", "Wei Zhang", "Tiejun Li"], "title": "Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity", "categories": ["cs.LG"], "comment": "22 pages", "summary": "Euclidean diffusion models have achieved remarkable success in generative\nmodeling across diverse domains, and they have been extended to manifold case\nin recent advances. Instead of explicitly utilizing the structure of special\nmanifolds as studied in previous works, we investigate direct sampling of the\nEuclidean diffusion models for general manifold-constrained data in this paper.\nWe reveal the multiscale singularity of the score function in the embedded\nspace of manifold, which hinders the accuracy of diffusion-generated samples.\nWe then present an elaborate theoretical analysis of the singularity structure\nof the score function by separating it along the tangential and normal\ndirections of the manifold. To mitigate the singularity and improve the\nsampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces\nnon-isotropic noise along the normal direction to reduce scale discrepancies,\nand (2) Tango-DM, which trains only the tangential component of the score\nfunction using a tangential-only loss function. Numerical experiments\ndemonstrate that our methods achieve superior performance on distributions over\nvarious manifolds with complex geometries.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u76f4\u63a5\u91c7\u6837\u6b27\u51e0\u91cc\u5f97\u6269\u6563\u6a21\u578b\u4ee5\u5904\u7406\u4e00\u822c\u6d41\u5f62\u7ea6\u675f\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\u6765\u51cf\u8f7b\u5f97\u5206\u51fd\u6570\u7684\u5947\u5f02\u6027\u548c\u63d0\u9ad8\u91c7\u6837\u7cbe\u5ea6\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u4e86\u76f4\u63a5\u91c7\u6837\u6b27\u51e0\u91cc\u5f97\u6269\u6563\u6a21\u578b\u4ee5\u5904\u7406\u4e00\u822c\u6d41\u5f62\u7ea6\u675f\u6570\u636e\uff0c\u56e0\u4e3a\u73b0\u6709\u5de5\u4f5c\u672a\u663e\u5f0f\u5229\u7528\u7279\u6b8a\u6d41\u5f62\u7684\u7ed3\u6784\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1aNiso-DM\uff0c\u5b83\u6cbf\u6cd5\u5411\u5f15\u5165\u975e\u5404\u5411\u540c\u6027\u566a\u58f0\u4ee5\u51cf\u5c11\u5c3a\u5ea6\u5dee\u5f02\uff1bTango-DM\uff0c\u5b83\u4ec5\u4f7f\u7528\u4ec5\u5207\u5411\u7684\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u5f97\u5206\u51fd\u6570\u7684\u5207\u5411\u5206\u91cf\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u5177\u6709\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u7684\u6d41\u5f62\u4e0a\u7684\u5206\u5e03\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u5177\u6709\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u7684\u6d41\u5f62\u4e0a\u7684\u5206\u5e03\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.10282", "pdf": "https://arxiv.org/pdf/2505.10282", "abs": "https://arxiv.org/abs/2505.10282", "authors": ["Dubai Li", "Nan Jiang", "Kangping Huang", "Ruiqi Tu", "Shuyu Ouyang", "Huayu Yu", "Lin Qiao", "Chen Yu", "Tianshu Zhou", "Danyang Tong", "Qian Wang", "Mengtao Li", "Xiaofeng Zeng", "Yu Tian", "Xinping Tian", "Jingsong Li"], "title": "From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making", "categories": ["cs.CL"], "comment": null, "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86Quicker\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5faa\u8bc1\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u8bc1\u636e\u7efc\u5408\u5e76\u751f\u6210\u7b26\u5408\u6807\u51c6\u4e34\u5e8a\u6307\u5357\u5f00\u53d1\u6d41\u7a0b\u7684\u4e34\u5e8a\u5efa\u8bae\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQuicker\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u5c06\u4e34\u5e8a\u8bc1\u636e\u6574\u5408\u5230\u5b9e\u65f6\u5b9e\u8df5\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5de5\u4f5c\u91cf\u5927\u3001\u4e13\u4e1a\u6d41\u7a0b\u590d\u6742\u4e14\u65f6\u95f4\u6709\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u81ea\u52a8\u5316\u8bc1\u636e\u7efc\u5408\u5de5\u5177\u6765\u652f\u6301\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86Quicker\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5faa\u8bc1\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u8bc1\u636e\u7efc\u5408\u5e76\u751f\u6210\u7b26\u5408\u6807\u51c6\u4e34\u5e8a\u6307\u5357\u5f00\u53d1\u6d41\u7a0b\u7684\u4e34\u5e8a\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQuicker\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7ec6\u7c92\u5ea6\u7684\u95ee\u9898\u5206\u89e3\uff0c\u68c0\u7d22\u7075\u654f\u5ea6\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\uff0c\u6587\u732e\u7b5b\u9009\u6027\u80fd\u63a5\u8fd1\u5168\u9762\u7eb3\u5165\u76f8\u5173\u7814\u7a76\u3002\u6b64\u5916\uff0cQuicker\u8f85\u52a9\u7684\u8bc1\u636e\u8bc4\u4f30\u6709\u6548\u652f\u6301\u4e86\u4eba\u7c7b\u8bc4\u5ba1\u5458\uff0c\u800cQuicker\u7684\u5efa\u8bae\u6bd4\u4e34\u5e8a\u533b\u751f\u7684\u5efa\u8bae\u66f4\u5168\u9762\u4e14\u903b\u8f91\u66f4\u8fde\u8d2f\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86Quicker\u5728\u5e2e\u52a9\u533b\u751f\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u5730\u505a\u51fa\u57fa\u4e8e\u8bc1\u636e\u7684\u4e34\u5e8a\u51b3\u7b56\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10088", "pdf": "https://arxiv.org/pdf/2505.10088", "abs": "https://arxiv.org/abs/2505.10088", "authors": ["Yuncheng Guo", "Xiaodong Gu"], "title": "MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file", "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff08MMRL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u91cf\u6837\u672c\u6570\u636e\u4e0b\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u5171\u4eab\u7684\u3001\u53ef\u5b66\u4e60\u7684\u3001\u6a21\u6001\u65e0\u5173\u7684\u8868\u793a\u7a7a\u95f4\uff0cMMRL\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86MMRL++\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u53c2\u6570\u6548\u7387\u548c\u6a21\u6001\u5185\u4ea4\u4e92\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMMRL\u548cMMRL++\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8de8\u4e0d\u540c\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u4f7f\u7528\u6709\u9650\u7684\u5c11\u6837\u672c\u6570\u636e\u9002\u5e94\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u4f1a\u8fc7\u62df\u5408\uff0c\u524a\u5f31\u4e86\u5b83\u4eec\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff08MMRL\uff09\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u5171\u4eab\u7684\u3001\u53ef\u5b66\u4e60\u7684\u3001\u6a21\u6001\u65e0\u5173\u7684\u8868\u793a\u7a7a\u95f4\u3002MMRL\u5c06\u7a7a\u95f4\u6807\u8bb0\u6295\u5f71\u5230\u6587\u672c\u548c\u56fe\u50cf\u7f16\u7801\u5668\u4e2d\u4f5c\u4e3a\u8868\u793a\u6807\u8bb0\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86MMRL++\uff0c\u8fd9\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u4ea4\u4e92\u611f\u77e5\u7684\u6269\u5c55\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u589e\u5f3a\u4e86\u6a21\u6001\u5185\u4ea4\u4e92\u3002", "result": "MMRL\u548cMMRL++\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u4eec\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u5728\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u6cdb\u5316\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "conclusion": "MMRL\u548cMMRL++\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u6cdb\u5316\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2505.09925", "pdf": "https://arxiv.org/pdf/2505.09925", "abs": "https://arxiv.org/abs/2505.09925", "authors": ["Yutao Yang", "Jie Zhou", "Junsong Li", "Qianjun Pan", "Bihao Zhan", "Qin Chen", "Xipeng Qiu", "Liang He"], "title": "Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an interactive continual learning paradigm where AI\nmodels dynamically learn new skills from real-time human feedback while\nretaining prior knowledge. This paradigm distinctively addresses two major\nlimitations of traditional continual learning: (1) dynamic model updates using\nstreaming, real-time human-annotated data, rather than static datasets with\nfixed labels, and (2) the assumption of clean labels, by explicitly handling\nthe noisy feedback common in real-world interactions. To tackle these problems,\nwe propose RiCL, a Reinforced interactive Continual Learning framework\nleveraging Large Language Models (LLMs) to learn new skills effectively from\ndynamic feedback. RiCL incorporates three key components: a temporal\nconsistency-aware purifier to automatically discern clean from noisy samples in\ndata streams; an interaction-aware direct preference optimization strategy to\nalign model behavior with human intent by reconciling AI-generated and\nhuman-provided feedback; and a noise-resistant contrastive learning module that\ncaptures robust representations by exploiting inherent data relationships, thus\navoiding reliance on potentially unreliable labels. Extensive experiments on\ntwo benchmark datasets (FewRel and TACRED), contaminated with realistic noise\npatterns, demonstrate that our RiCL approach substantially outperforms existing\ncombinations of state-of-the-art online continual learning and noisy-label\nlearning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u5f0f\u6301\u7eed\u5b66\u4e60\u6846\u67b6RiCL\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u5b9e\u65f6\u4eba\u7c7b\u53cd\u9988\u4e2d\u52a8\u6001\u5b66\u4e60\u65b0\u6280\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\u3002RiCL\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u566a\u58f0\u53cd\u9988\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(1) \u4f7f\u7528\u6d41\u5f0f\u5b9e\u65f6\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u52a8\u6001\u6a21\u578b\u66f4\u65b0\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u56fa\u5b9a\u6807\u7b7e\u7684\u9759\u6001\u6570\u636e\u96c6\uff1b(2) \u5047\u8bbe\u6807\u7b7e\u662f\u5e72\u51c0\u7684\uff0c\u800c\u6ca1\u6709\u660e\u786e\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u4e2d\u5e38\u89c1\u7684\u566a\u58f0\u53cd\u9988\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86RiCL\uff0c\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u52a8\u6001\u53cd\u9988\u4e2d\u6709\u6548\u5b66\u4e60\u65b0\u6280\u80fd\u7684\u5f3a\u5316\u4ea4\u4e92\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u3002RiCL\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u51c0\u5316\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u533a\u5206\u6570\u636e\u6d41\u4e2d\u7684\u5e72\u51c0\u6837\u672c\u548c\u566a\u58f0\u6837\u672c\uff1b\u4e00\u79cd\u4ea4\u4e92\u611f\u77e5\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u534f\u8c03AI\u751f\u6210\u7684\u53cd\u9988\u548c\u4eba\u7c7b\u63d0\u4f9b\u7684\u53cd\u9988\u6765\u4f7f\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\uff1b\u4ee5\u53ca\u4e00\u79cd\u6297\u566a\u58f0\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u901a\u8fc7\u5229\u7528\u6570\u636e\u7684\u5185\u5728\u5173\u7cfb\u6765\u6355\u83b7\u7a33\u5065\u7684\u8868\u793a\uff0c\u4ece\u800c\u907f\u514d\u4f9d\u8d56\u53ef\u80fd\u4e0d\u53ef\u9760\u7684\u6807\u7b7e\u3002", "result": "RiCL\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08FewRel\u548cTACRED\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u88ab\u73b0\u5b9e\u566a\u58f0\u6a21\u5f0f\u6c61\u67d3\uff0c\u7ed3\u679c\u663e\u793aRiCL\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u548c\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u7684\u7ec4\u5408\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RiCL\u65b9\u6cd5\u5728\u4e24\u4e2a\u5305\u542b\u73b0\u5b9e\u566a\u58f0\u6a21\u5f0f\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff08FewRel\u548cTACRED\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u548c\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u7684\u7ec4\u5408\u3002"}}
{"id": "2505.10320", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "AI": {"tldr": "J1 is a reinforcement learning method for training LLM-as-a-Judge models that improves their judgment ability by using verifiable rewards. It outperforms other models and provides analysis of different training strategies.", "motivation": "The quality of evaluation is a bottleneck for AI progress, and powerful LLM-as-a-Judge models are a core solution. Improving their judgment ability requires finding the best training recipes.", "method": "J1 uses reinforcement learning to train LLM-as-a-Judge models. It converts both verifiable and non-verifiable prompts into judgment tasks with verifiable rewards, which incentivize thinking and reduce bias.", "result": "J1 outperforms all other existing 8B or 70B models, including those distilled from DeepSeek-R1. It also outperforms o1-mini and even R1 on some benchmarks despite being a smaller model.", "conclusion": "J1 is a reinforcement learning approach that improves the judgment ability of LLM-as-a-Judge models by converting prompts to judgment tasks with verifiable rewards. It outperforms other models, including larger ones, and provides insights into effective training strategies."}}
{"id": "2505.10118", "pdf": "https://arxiv.org/pdf/2505.10118", "abs": "https://arxiv.org/abs/2505.10118", "authors": ["Yangfu Li", "Hongjian Zhan", "Tianyi Chen", "Qi Liu", "Yue Lu"], "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering", "categories": ["cs.CV", "cs.CL"], "comment": "31 pages,9 figures,conference", "summary": "Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.", "AI": {"tldr": "This paper proposes Multi-Objective Balanced Covering (MoB) for visual token pruning, which addresses the inconsistency in performance caused by static strategies. MoB derives a closed-form error bound based on the Hausdorff distance and reveals a trade-off between prompt alignment and visual preservation. It reformulates pruning as a bi-objective covering problem, enabling efficient budget allocation. Experiments show that MoB achieves high performance with minimal visual tokens, making it suitable for various vision-language tasks.", "motivation": "Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance.", "method": "We derive the first closed-form error bound for visual token pruning based on the Hausdorff distance, uniformly characterizing the contributions of both objectives. Moreover, leveraging \u03b5-covering theory, we reveal an intrinsic trade-off between these objectives and quantify their optimal attainment levels under a fixed budget. To practically handle this trade-off, we propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem. In this framework, the attainment trade-off reduces to budget allocation via greedy radius trading.", "result": "MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5\u00d7 with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.", "conclusion": "MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. Extensive experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5\u00d7 with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks."}}
{"id": "2505.09949", "pdf": "https://arxiv.org/pdf/2505.09949", "abs": "https://arxiv.org/abs/2505.09949", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Samgyu Yang", "Abdulrahman Faden"], "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors", "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u6570\u636e\uff0c\u8bc6\u522b\u4e8b\u6545\u539f\u56e0\u5e76\u63d0\u4f9b\u5168\u9762\u89e3\u91ca\u3002\u7ed3\u679c\u663e\u793aLLM\u80fd\u6709\u6548\u8bc6\u522b\u4e3b\u8981\u4e8b\u6545\u539f\u56e0\uff0c\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\u53ef\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u6539\u5584\u4ea4\u901a\u5b89\u5168\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u7406\u89e3\u5bfc\u81f4\u4ea4\u901a\u4e8b\u6545\u7684\u56e0\u7d20\u5e76\u5236\u5b9a\u7b56\u7565\u4ee5\u51cf\u8f7b\u5176\u4e25\u91cd\u7a0b\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u5404\u79cd\u56e0\u7d20\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u4ee5\u53ca\u6bcf\u6b21\u4e8b\u6545\u7684\u72ec\u7279\u7279\u5f81\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5206\u6790\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03Llama3 8B\u6a21\u578b\u6765\u589e\u5f3a\u5176\u5bf9\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u53ca\u5176\u76f8\u5173\u56e0\u7d20\u7684\u7406\u89e3\u3002\u8be5\u6a21\u578b\u4f7f\u7528QLoRA\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u96f6\u6837\u672c\u5206\u7c7b\u8bc6\u522b\u4e8b\u6545\u539f\u56e0\uff0c\u65e0\u9700\u9884\u6807\u8bb0\u6570\u636e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLLM\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u4e3b\u8981\u4e8b\u6545\u539f\u56e0\uff0c\u5982\u9152\u540e\u9a7e\u9a76\u3001\u8d85\u901f\u3001\u5371\u9669\u9a7e\u9a76\u548c\u9a7e\u9a76\u5458\u5206\u5fc3\u3002\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\uff08\u5982\u9053\u8def\u7ef4\u62a4\uff09\u53ef\u4ee5\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002\u7814\u7a76\u4eba\u5458\u5728\u4ea4\u901a\u5b89\u5168\u9886\u57df\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u548c\u95ee\u5377\u8c03\u67e5\u7ed3\u679c\uff0888.89%\uff09\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u9002\u7528\u6027\u548c\u6539\u5584\u4ea4\u901a\u5b89\u5168\u63aa\u65bd\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4ea4\u901a\u4e8b\u6545\u7684\u590d\u6742\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u9762\u5206\u6790\u4e8b\u6545\u539f\u56e0\u548c\u5176\u4ed6\u76f8\u5173\u56e0\u7d20\u65b9\u9762\u7684\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u5b83\u4e3a\u89c4\u5212\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u6f5c\u5728\u7684\u5e94\u5bf9\u63aa\u65bd\uff0c\u4ee5\u5236\u5b9a\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u4ea4\u901a\u5b89\u5168\u63aa\u65bd\u3002"}}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354", "abs": "https://arxiv.org/abs/2505.10354", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.", "AI": {"tldr": "This paper proposes LDIR, a low-dimensional, dense, and interpretable text embedding method that offers both semantic representation and traceability.", "motivation": "Existing text embedding methods have excellent performance but lack traceability and interpretability. Bag-of-words, while interpretable, has poor performance. Benara et al. (2024) proposed interpretable text embeddings using large language models, but they are high-dimensional. We aim to create a low-dimensional, dense, and interpretable text embedding method.", "method": "We propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling.", "result": "LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions.", "conclusion": "LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions."}}
{"id": "2505.10124", "pdf": "https://arxiv.org/pdf/2505.10124", "abs": "https://arxiv.org/abs/2505.10124", "authors": ["Ziad Kheil", "Lucas Robinet", "Laurent Risser", "Soleakhena Ken"], "title": "IMITATE: Image Registration with Context for unknown time frame recovery", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE ISBI 2025", "summary": "In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u914d\u51c6\u5f62\u5f0f\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e0e\u672a\u77e5\u6761\u4ef6\u76f8\u5173\u7684\u56fe\u50cf\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6216\u66f4\u591a\u5df2\u77e5\u56fe\u50cf\u53ca\u5176\u76f8\u5173\u6761\u4ef6\u3002\u901a\u8fc7\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u6761\u4ef6U-Net\u67b6\u6784\u6765\u5b9e\u9645\u5efa\u6a21\u8fd9\u79cd\u5f62\u5f0f\uff0c\u8be5\u67b6\u6784\u5145\u5206\u5229\u7528\u4e86\u6761\u4ef6\u4fe1\u606f\uff0c\u5e76\u4e0d\u9700\u8981\u4efb\u4f55\u56fa\u5b9a\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u653e\u5c04\u6cbb\u7597\u4e2d\u4e0d\u540c\u547c\u5438\u5e45\u5ea6\u7684\u56fe\u50cf\u79fb\u52a8\u80bf\u7624\uff0c\u5229\u7528\u80f8\u8179\u533a\u57df\u76844D-CT\uff083D+t\uff09\u626b\u63cf\u8fdb\u884c\u5904\u7406\u3002\u8be5\u5e94\u7528\u7279\u522b\u590d\u6742\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5c06\u4e00\u7cfb\u52172D\u5207\u7247\u62fc\u63a5\u6210\u4e0d\u540c\u5668\u5b98\u4f4d\u7f6e\u7684\u591a\u4e2a3D\u4f53\u79ef\u3002\u6807\u51c6\u65b9\u6cd5\u7684\u8fd0\u52a8\u63d2\u503c\u5728\u7ec4\u88c5\u540e\u7684\u4f53\u79ef\u4e2d\u4f1a\u4ea7\u751f\u5df2\u77e5\u7684\u91cd\u5efa\u4f2a\u5f71\uff0c\u8fd9\u662f\u7531\u4e8e\u4e0d\u89c4\u5219\u7684\u60a3\u8005\u547c\u5438\u3001\u6ede\u540e\u6548\u5e94\u548c\u547c\u5438\u4fe1\u53f7\u4e0e\u5185\u90e8\u8fd0\u52a8\u7684\u76f8\u5173\u6027\u8f83\u5dee\u6240\u81f4\u3002\u57284D-CT\u4e34\u5e8a\u6570\u636e\u4e0a\u83b7\u5f97\u7684\u7ed3\u679c\u5c55\u793a\u4e86\u901a\u8fc7\u5b9e\u65f6\u5ef6\u8fdf\u5b9e\u73b0\u7684\u65e0\u4f2a\u5f71\u4f53\u79ef\u3002\u4ee3\u7801\u53ef\u5728https://github.com/Kheil-Z/IMITATE \u4e0a\u516c\u5f00\u83b7\u53d6\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u653e\u5c04\u6cbb\u7597\u4e2d\u7531\u4e8e\u4e0d\u89c4\u5219\u547c\u5438\u3001\u6ede\u540e\u6548\u5e94\u548c\u547c\u5438\u4fe1\u53f7\u4e0e\u5185\u90e8\u8fd0\u52a8\u76f8\u5173\u6027\u5dee\u800c\u5bfc\u81f4\u7684\u56fe\u50cf\u91cd\u5efa\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u914d\u51c6\u5f62\u5f0f\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6216\u66f4\u591a\u5df2\u77e5\u56fe\u50cf\u53ca\u5176\u76f8\u5173\u6761\u4ef6\uff0c\u4ee5\u4f30\u8ba1\u672a\u77e5\u6761\u4ef6\u76f8\u5173\u7684\u56fe\u50cf\u3002\u901a\u8fc7\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u6761\u4ef6U-Net\u67b6\u6784\u6765\u5b9e\u9645\u5efa\u6a21\u8fd9\u79cd\u5f62\u5f0f\uff0c\u8be5\u67b6\u6784\u5145\u5206\u5229\u7528\u4e86\u6761\u4ef6\u4fe1\u606f\uff0c\u5e76\u4e0d\u9700\u8981\u4efb\u4f55\u56fa\u5b9a\u56fe\u50cf\u3002", "result": "\u57284D-CT\u4e34\u5e8a\u6570\u636e\u4e0a\u83b7\u5f97\u7684\u7ed3\u679c\u5c55\u793a\u4e86\u901a\u8fc7\u5b9e\u65f6\u5ef6\u8fdf\u5b9e\u73b0\u7684\u65e0\u4f2a\u5f71\u4f53\u79ef\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u56fe\u50cf\u914d\u51c6\u5f62\u5f0f\u80fd\u591f\u6709\u6548\u89e3\u51b3\u653e\u5c04\u6cbb\u7597\u4e2d\u7531\u4e8e\u4e0d\u89c4\u5219\u547c\u5438\u3001\u6ede\u540e\u6548\u5e94\u548c\u547c\u5438\u4fe1\u53f7\u4e0e\u5185\u90e8\u8fd0\u52a8\u76f8\u5173\u6027\u5dee\u5bfc\u81f4\u7684\u56fe\u50cf\u91cd\u5efa\u4f2a\u5f71\u95ee\u9898\u3002"}}
{"id": "2505.09952", "pdf": "https://arxiv.org/pdf/2505.09952", "abs": "https://arxiv.org/abs/2505.09952", "authors": ["Tianyu Huai", "Jie Zhou", "Yuxuan Cai", "Qin Chen", "Wen Wu", "Xingjiao Wu", "Xipeng Qiu", "Liang He"], "title": "Task-Core Memory Management and Consolidation for Long-term Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to Neurips2025", "summary": "In this paper, we focus on a long-term continual learning (CL) task, where a\nmodel learns sequentially from a stream of vast tasks over time, acquiring new\nknowledge while retaining previously learned information in a manner akin to\nhuman learning. Unlike traditional CL settings, long-term CL involves handling\na significantly larger number of tasks, which exacerbates the issue of\ncatastrophic forgetting. Our work seeks to address two critical questions: 1)\nHow do existing CL methods perform in the context of long-term CL? and 2) How\ncan we mitigate the catastrophic forgetting that arises from prolonged\nsequential updates? To tackle these challenges, we propose a novel framework\ninspired by human memory mechanisms for long-term continual learning (Long-CL).\nSpecifically, we introduce a task-core memory management strategy to\nefficiently index crucial memories and adaptively update them as learning\nprogresses. Additionally, we develop a long-term memory consolidation mechanism\nthat selectively retains hard and discriminative samples, ensuring robust\nknowledge retention. To facilitate research in this area, we construct and\nrelease two multi-modal and textual benchmarks, MMLongCL-Bench and\nTextLongCL-Bench, providing a valuable resource for evaluating long-term CL\napproaches. Experimental results show that Long-CL outperforms the previous\nstate-of-the-art by 7.4\\% and 6.5\\% AP on the two benchmarks, respectively,\ndemonstrating the effectiveness of our approach.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356", "abs": "https://arxiv.org/abs/2505.10356", "authors": ["Chunyu Ye", "Shaonan Wang"], "title": "Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli", "categories": ["cs.CL"], "comment": null, "summary": "Decoding thoughts from brain activity offers valuable insights into human\ncognition and enables promising applications in brain-computer interaction.\nWhile prior studies have explored language reconstruction from fMRI data, they\nare typically limited to single-modality inputs such as images or audio. In\ncontrast, human thought is inherently multimodal. To bridge this gap, we\npropose a unified and flexible framework for reconstructing coherent language\nfrom brain recordings elicited by diverse input modalities-visual, auditory,\nand textual. Our approach leverages visual-language models (VLMs), using\nmodality-specific experts to jointly interpret information across modalities.\nExperiments demonstrate that our method achieves performance comparable to\nstate-of-the-art systems while remaining adaptable and extensible. This work\nadvances toward more ecologically valid and generalizable mind decoding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u79cd\u8f93\u5165\u6a21\u6001\u4e2d\u91cd\u5efa\u8fde\u8d2f\u7684\u8bed\u8a00\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u76f8\u5f53\uff0c\u5e76\u4e14\u5177\u6709\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u8f93\u5165\uff0c\u5982\u56fe\u50cf\u6216\u97f3\u9891\uff0c\u800c\u4eba\u7c7b\u601d\u7ef4\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7531\u4e0d\u540c\u8f93\u5165\u6a21\u6001\uff08\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\uff09\u5f15\u53d1\u7684\u8111\u8bb0\u5f55\u4e2d\u91cd\u5efa\u8fde\u8d2f\u7684\u8bed\u8a00\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u4f7f\u7528\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u4e13\u5bb6\u6765\u5171\u540c\u89e3\u91ca\u8de8\u6a21\u6001\u7684\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u671d\u7740\u66f4\u751f\u6001\u6709\u6548\u548c\u53ef\u63a8\u5e7f\u7684\u601d\u7ef4\u89e3\u7801\u8fc8\u8fdb\u3002"}}
{"id": "2505.10152", "pdf": "https://arxiv.org/pdf/2505.10152", "abs": "https://arxiv.org/abs/2505.10152", "authors": ["Yikang Wei"], "title": "Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization", "categories": ["cs.CV"], "comment": "IJCAI 2025", "summary": "Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5MCSAD\uff0c\u901a\u8fc7\u591a\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u98ce\u683c\u589e\u5f3a\u65b9\u6cd5\u8981\u4e48\u63a2\u7d22\u5b64\u7acb\u6e90\u57df\u5185\u7684\u6570\u636e\u98ce\u683c\uff0c\u8981\u4e48\u5728\u6570\u636e\u53bb\u4e2d\u5fc3\u5316\u573a\u666f\u4e0b\u8de8\u73b0\u6709\u6e90\u57df\u63d2\u503c\u98ce\u683c\u4fe1\u606f\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6709\u9650\u7684\u98ce\u683c\u7a7a\u95f4\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u65b9\u6cd5\uff08MCSAD\uff09\uff0c\u901a\u8fc7\u591a\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u6a21\u5757\u751f\u6210\u66f4\u5e7f\u6cdb\u98ce\u683c\u7a7a\u95f4\u7684\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u548c\u7c7b\u5173\u7cfb\u96c6\u6210\u84b8\u998f\u8fdb\u884c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u548c\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u9886\u57df\u4e2d\u5f88\u597d\u5730\u6cdb\u5316\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.09955", "pdf": "https://arxiv.org/pdf/2505.09955", "abs": "https://arxiv.org/abs/2505.09955", "authors": ["Jaeho Kim", "Seulki Lee"], "title": "TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 Accept", "summary": "Unsupervised domain adaptation (UDA) for time series data remains a critical\nchallenge in deep learning, with traditional pseudo-labeling strategies failing\nto capture temporal patterns and channel-wise shifts between domains, producing\nsub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that\naddresses these limitations by modeling the joint distribution $P(\\mathbf{X},\ny)$ of the source domain through code transition matrices, where the codes are\nderived from vector quantization (VQ) of time series patches. Our method\nconstructs class- and channel-wise code transition matrices from the source\ndomain and employs Bayes' rule for target domain adaptation, generating\npseudo-labels based on channel-wise weighted class-conditional likelihoods.\nTransPL offers three key advantages: explicit modeling of temporal transitions\nand channel-wise shifts between different domains, versatility towards\ndifferent UDA scenarios (e.g., weakly-supervised UDA), and explainable\npseudo-label generation. We validate TransPL's effectiveness through extensive\nanalysis on four time series UDA benchmarks and confirm that it consistently\noutperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%\naccuracy improvement, 4.9% F1 improvement), while providing interpretable\ninsights into the domain adaptation process through its learned code transition\nmatrices.", "AI": {"tldr": "TransPL \u662f\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u8054\u5408\u5206\u5e03\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u65f6\u95f4\u8fc7\u6e21\u548c\u901a\u9053\u53d8\u5316\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u4f2a\u6807\u7b7e\u7b56\u7565\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u6a21\u5f0f\u548c\u57df\u4e4b\u95f4\u7684\u901a\u9053\u5dee\u5f02\uff0c\u5bfc\u81f4\u6b21\u4f18\u4f2a\u6807\u7b7e\u3002", "method": "TransPL \u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\u5efa\u6a21\u6e90\u57df\u7684\u8054\u5408\u5206\u5e03 P(X, y)\uff0c\u5176\u4e2d\u4ee3\u7801\u6765\u81ea\u65f6\u95f4\u5e8f\u5217\u5757\u7684\u5411\u91cf\u91cf\u5316 (VQ)\u3002\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u7c7b\u548c\u901a\u9053\u76f8\u5173\u7684\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\uff0c\u5e76\u5229\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u8fdb\u884c\u76ee\u6807\u57df\u9002\u5e94\uff0c\u57fa\u4e8e\u901a\u9053\u52a0\u6743\u7684\u7c7b\u6761\u4ef6\u4f3c\u7136\u751f\u6210\u4f2a\u6807\u7b7e\u3002", "result": "TransPL \u5728\u56db\u4e2a\u65f6\u95f4\u5e8f\u5217 UDA \u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u7387\u548c F1 \u5206\u6570\u4e0a\u5206\u522b\u63d0\u9ad8\u4e86 6.1% \u548c 4.9%\u3002", "conclusion": "TransPL \u7684\u6709\u6548\u6027\u901a\u8fc7\u5728\u56db\u4e2a\u65f6\u95f4\u5e8f\u5217 UDA \u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5206\u6790\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u4e14\u5b83\u5728\u4f2a\u6807\u7b7e\u751f\u6210\u65b9\u9762\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.10389", "pdf": "https://arxiv.org/pdf/2505.10389", "abs": "https://arxiv.org/abs/2505.10389", "authors": ["Benjamin White", "Anastasia Shimorina"], "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bbe\u8ba1\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u4e86\u5355\u4e00\u7684\u591a\u9886\u57df\u6a21\u578b\u5728\u5904\u7406\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u5206\u7c7b\u6cd5\u65f6\u7684\u8868\u73b0\u4e0e\u4e13\u95e8\u7684\u5355\u9886\u57df\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u64cd\u4f5c\u590d\u6742\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bbe\u8ba1\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u89e3\u51b3\u8de8\u9886\u57df\u548c\u8de8\u8bed\u8a00\u7684\u56db\u5143\u7ec4\u60c5\u611f\u63d0\u53d6\u95ee\u9898\u3002", "method": "\u672c\u6587\u4f7f\u7528\u5185\u90e8\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4e86\u5355\u4e00\u5fae\u8c03\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u540c\u65f6\u6709\u6548\u5904\u7406\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u672c\u6587\u5c55\u793a\u4e86\u5355\u4e00\u7684\u591a\u9886\u57df\u6a21\u578b\u5728\u5904\u7406\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u5206\u7c7b\u6cd5\u65f6\u7684\u8868\u73b0\u4e0e\u4e13\u95e8\u7684\u5355\u9886\u57df\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u64cd\u4f5c\u590d\u6742\u6027\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5355\u4e00\u7684\u591a\u9886\u57df\u6a21\u578b\u5728\u5904\u7406\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u5206\u7c7b\u6cd5\u65f6\u7684\u8868\u73b0\u4e0e\u4e13\u95e8\u7684\u5355\u9886\u57df\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u64cd\u4f5c\u590d\u6742\u6027\u3002"}}
{"id": "2505.10169", "pdf": "https://arxiv.org/pdf/2505.10169", "abs": "https://arxiv.org/abs/2505.10169", "authors": ["Matthias K\u00fcmmerer", "Harneet Khanuja", "Matthias Bethge"], "title": "Modeling Saliency Dataset Bias", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u50cf\u663e\u8457\u6027\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u96c6\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u67b6\u6784\u6765\u89e3\u51b3\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u95ee\u9898\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\uff0c\u5e76\u63ed\u793a\u4e86\u590d\u6742\u7684\u591a\u5c3a\u5ea6\u663e\u8457\u6027\u6548\u5e94\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u56fe\u50cf\u663e\u8457\u6027\u9884\u6d4b\u65b9\u6cd5\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u63a5\u8fd1\u9ec4\u91d1\u6807\u51c6\u6027\u80fd\u6c34\u5e73\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u8de8\u591a\u4e2a\u663e\u8457\u6027\u6570\u636e\u96c6\u9884\u6d4b\u6ce8\u89c6\u70b9\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6570\u636e\u96c6\u5b58\u5728\u504f\u5dee\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u67b6\u6784\uff0c\u6269\u5c55\u4e86\u4e00\u4e2a\u5927\u90e8\u5206\u6570\u636e\u96c6\u65e0\u5173\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u4ec5\u5305\u542b\u5c11\u4e8e20\u4e2a\u6570\u636e\u96c6\u7279\u5b9a\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u63a7\u5236\u53ef\u89e3\u91ca\u7684\u673a\u5236\uff0c\u5982\u591a\u5c3a\u5ea6\u7ed3\u6784\u3001\u4e2d\u5fc3\u504f\u5dee\u548c\u6ce8\u89c6\u6269\u6563\u3002", "result": "\u5f53\u6a21\u578b\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\u5e94\u7528\u4e8e\u53e6\u4e00\u4e2a\u6570\u636e\u96c6\u65f6\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u7ea640%\uff09\u3002\u589e\u52a0\u6570\u636e\u96c6\u591a\u6837\u6027\u5e76\u4e0d\u80fd\u89e3\u51b3\u8fd9\u79cd\u8de8\u6570\u636e\u96c6\u5dee\u8ddd\uff0c\u8fd160%\u7684\u5dee\u8ddd\u5f52\u56e0\u4e8e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u504f\u5dee\u3002\u9002\u5e94\u8fd9\u4e9b\u53c2\u6570\u5230\u65b0\u6570\u636e\u53ef\u4ee5\u89e3\u51b3\u8d85\u8fc775%\u7684\u6cdb\u5316\u5dee\u8ddd\uff0c\u5176\u4e2d\u5927\u90e8\u5206\u6539\u8fdb\u53ef\u4ee5\u901a\u8fc7\u4ec550\u4e2a\u6837\u672c\u5b9e\u73b0\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728MIT/Tuebingen Saliency Benchmark\u7684\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5373\u4f7f\u7eaf\u7cb9\u4ece\u4e0d\u76f8\u5173\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6cdb\u5316\uff0c\u4f46\u5f53\u9002\u5e94\u5230\u76f8\u5e94\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u65f6\uff0c\u6548\u679c\u6709\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u7a7a\u95f4\u663e\u8457\u6027\u5c5e\u6027\u7684\u6709\u4ef7\u503c\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u7ed3\u5408\u7edd\u5bf9\u548c\u76f8\u5bf9\u5927\u5c0f\u7684\u590d\u6742\u591a\u5c3a\u5ea6\u6548\u5e94\u3002"}}
{"id": "2505.09959", "pdf": "https://arxiv.org/pdf/2505.09959", "abs": "https://arxiv.org/abs/2505.09959", "authors": ["Zengxia Guo", "Bohui An", "Zhongqi Lu"], "title": "Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated reinforcement learning (FRL) methods usually share the encrypted\nlocal state or policy information and help each client to learn from others\nwhile preserving everyone's privacy. In this work, we propose that sharing the\napproximated behavior metric-based state projection function is a promising way\nto enhance the performance of FRL and concurrently provides an effective\nprotection of sensitive information. We introduce FedRAG, a FRL framework to\nlearn a computationally practical projection function of states for each client\nand aggregating the parameters of projection functions at a central server. The\nFedRAG approach shares no sensitive task-specific information, yet provides\ninformation gain for each client. We conduct extensive experiments on the\nDeepMind Control Suite to demonstrate insightful results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedRAG\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u8fd1\u4f3c\u884c\u4e3a\u5ea6\u91cf\u72b6\u6001\u6295\u5f71\u51fd\u6570\u6765\u63d0\u9ad8FRL\u6027\u80fd\u5e76\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5171\u4eab\u52a0\u5bc6\u7684\u672c\u5730\u72b6\u6001\u6216\u7b56\u7565\u4fe1\u606f\uff0c\u4ee5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u8ba9\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4ece\u4ed6\u4eba\u5b66\u4e60\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u4fdd\u62a4\u654f\u611f\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FedRAG\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u8ba1\u7b97\u4e0a\u53ef\u884c\u7684\u72b6\u6001\u6295\u5f71\u51fd\u6570\uff0c\u5e76\u5728\u4e2d\u592e\u670d\u52a1\u5668\u805a\u5408\u6295\u5f71\u51fd\u6570\u7684\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedRAG\u65b9\u6cd5\u5728DeepMind Control Suite\u4e0a\u53d6\u5f97\u4e86\u6709\u89c1\u89e3\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u63d0\u4f9b\u4fe1\u606f\u589e\u76ca\u800c\u4e0d\u5171\u4eab\u654f\u611f\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u3002", "conclusion": "\u5171\u4eab\u57fa\u4e8e\u8fd1\u4f3c\u884c\u4e3a\u5ea6\u91cf\u7684\u72b6\u6001\u6295\u5f71\u51fd\u6570\u662f\u4e00\u79cd\u589e\u5f3aFRL\u6027\u80fd\u5e76\u6709\u6548\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u7684\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.10402", "pdf": "https://arxiv.org/pdf/2505.10402", "abs": "https://arxiv.org/abs/2505.10402", "authors": ["Yihong Dong", "Yuchen Liu", "Xue Jiang", "Zhi Jin", "Ge Li"], "title": "Rethinking Repetition Problems of LLMs in Code Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025 (main)", "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RPG\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u8f7b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\u4ecd\u7136\u5b58\u5728\uff0c\u5c24\u5176\u662f\u7ed3\u6784\u91cd\u590d\u95ee\u9898\uff0c\u8fd9\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "RPG\uff08\u57fa\u4e8e\u8bed\u6cd5\u7684\u91cd\u590d\u60e9\u7f5a\uff09\u65b9\u6cd5\u5229\u7528\u8bed\u6cd5\u89c4\u5219\u8bc6\u522b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\uff0c\u5e76\u6218\u7565\u6027\u5730\u8870\u51cf\u5bfc\u81f4\u91cd\u590d\u7684\u5173\u952e\u6807\u8bb0\u7684\u53ef\u80fd\u6027\uff0c\u4ece\u800c\u51cf\u8f7b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\u3002", "result": "RPG\u5728CodeRepetEval\u6570\u636e\u96c6\u4ee5\u53caHumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u3002", "conclusion": "RPG\u663e\u8457\u4f18\u4e8e\u6700\u4f73\u57fa\u7ebf\uff0c\u5728CodeRepetEval\u6570\u636e\u96c6\u4ee5\u53caHumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.10205", "pdf": "https://arxiv.org/pdf/2505.10205", "abs": "https://arxiv.org/abs/2505.10205", "authors": ["Umair Haroon", "Ahmad AlMughrabi", "Thanasis Zoumpekas", "Ricardo Marques", "Petia Radeva"], "title": "VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VolE\u6846\u67b6\uff0c\u5229\u7528\u79fb\u52a8\u8bbe\u5907\u9a71\u52a8\u76843D\u91cd\u5efa\u6765\u4f30\u8ba1\u98df\u54c1\u4f53\u79ef\uff0c\u65e0\u9700\u53c2\u8003\u548c\u6df1\u5ea6\u4fe1\u606f\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u7684\u98df\u54c1\u4f53\u79ef\u4f30\u8ba1\u5bf9\u4e8e\u533b\u7597\u8425\u517b\u7ba1\u7406\u548c\u5065\u5eb7\u76d1\u6d4b\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u53d7\u5230\u5355\u6838\u6570\u636e\u3001\u4e13\u7528\u786c\u4ef6\u6216\u53c2\u8003\u7269\u4f53\u6821\u51c6\u7684\u9650\u5236\u3002", "method": "VolE\u5229\u7528\u79fb\u52a8\u8bbe\u5907\u9a71\u52a8\u76843D\u91cd\u5efa\u6765\u4f30\u8ba1\u98df\u54c1\u4f53\u79ef\uff0c\u901a\u8fc7\u81ea\u7531\u8fd0\u52a8\u62cd\u6444\u56fe\u50cf\u548c\u76f8\u673a\u4f4d\u7f6e\u751f\u6210\u7cbe\u786e\u76843D\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u98df\u54c1\u89c6\u9891\u5206\u5272\u751f\u6210\u98df\u54c1\u63a9\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVolE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862.22%\u7684MAPE\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u4f53\u79ef\u4f30\u8ba1\u6280\u672f\u3002", "conclusion": "VolE\u5728\u98df\u54c1\u4f53\u79ef\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.09969", "pdf": "https://arxiv.org/pdf/2505.09969", "abs": "https://arxiv.org/abs/2505.09969", "authors": ["Ali Azimi Lamir", "Shiva Razzagzadeh", "Zeynab Rezaei"], "title": "A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study presents a machine learning-based framework for heart disease\nprediction using the heart-disease dataset, comprising 303 samples with 14\nfeatures. The methodology involves data preprocessing, model training, and\nevaluation using three classifiers: Logistic Regression, K-Nearest Neighbors\n(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and\nRandomizedSearchCV was employed to enhance model performance. The Random Forest\nclassifier outperformed other models, achieving an accuracy of 91% and an\nF1-score of 0.89. Evaluation metrics, including precision, recall, and\nconfusion matrix, revealed balanced performance across classes. The proposed\nmodel demonstrates strong potential for aiding clinical decision-making by\neffectively predicting heart disease. Limitations such as dataset size and\ngeneralizability underscore the need for future studies using larger and more\ndiverse datasets. This work highlights the utility of machine learning in\nhealthcare, offering insights for further advancements in predictive\ndiagnostics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5fc3\u810f\u75c5\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u79cd\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u4e2d\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523091%\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u9700\u8981\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5bf9\u5fc3\u810f\u75c5\u8fdb\u884c\u9884\u6d4b\uff0c\u4ee5\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u4e09\u79cd\u5206\u7c7b\u5668\uff1a\u903b\u8f91\u56de\u5f52\u3001K-\u6700\u8fd1\u90bb\uff08KNN\uff09\u548c\u968f\u673a\u68ee\u6797\u3002\u901a\u8fc7GridSearchCV\u548cRandomizedSearchCV\u8fdb\u884c\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u4e3a91%\uff0cF1\u5f97\u5206\u4e3a0.89\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u6df7\u6dc6\u77e9\u9635\uff0c\u663e\u793a\u4e86\u5404\u7c7b\u522b\u4e4b\u95f4\u7684\u5e73\u8861\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u6570\u636e\u96c6\u7684\u5927\u5c0f\u548c\u6cdb\u5316\u80fd\u529b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u672a\u6765\u4f7f\u7528\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u7814\u7a76\u3002"}}
{"id": "2505.10409", "pdf": "https://arxiv.org/pdf/2505.10409", "abs": "https://arxiv.org/abs/2505.10409", "authors": ["Yue Guo", "Jae Ho Sohn", "Gondy Leroy", "Trevor Cohen"], "title": "Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u666e\u901a\u8bed\u8a00\u6458\u8981\uff08PLSs\uff09\u7684\u8d28\u91cf\uff0c\u5e76\u53d1\u73b0\u5c3d\u7ba1LLMs\u751f\u6210\u7684PLS\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4eba\u7c7b\u64b0\u5199\u7684PLS\u5728\u7406\u89e3\u65b9\u9762\u66f4\u4f18\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u9002\u7528\u6027\u7684\u8d28\u7591\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u4f18\u5316\u666e\u901a\u8bfb\u8005\u7406\u89e3\u7684\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\uff0c\u81ea\u52a8\u5316\u751f\u6210PLS\u7684\u65b9\u6cd5\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\uff0c\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u65e0\u6cd5\u76f4\u63a5\u8861\u91cf\u53ef\u7406\u89e3\u6027\u7684\u81ea\u52a8\u8bc4\u5206\u6216\u4ece\u4fbf\u5229\u6837\u672c\u4e2d\u83b7\u5f97\u7684\u4e3b\u89c2Likert\u91cf\u8868\u8bc4\u5206\uff0c\u8fd9\u4e9b\u8bc4\u5206\u7684\u63a8\u5e7f\u6027\u6709\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fddPLS\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e9a\u9a6c\u900aMechanical Turk\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u4f17\u5305\u8bc4\u4f30\uff0c\u6709150\u540d\u53c2\u4e0e\u8005\u53c2\u4e0e\u3002\u6211\u4eec\u901a\u8fc7\u4e3b\u89c2Likert\u91cf\u8868\u8bc4\u5206\uff08\u5173\u6ce8\u7b80\u5355\u6027\u3001\u4fe1\u606f\u91cf\u3001\u8fde\u8d2f\u6027\u548c\u5fe0\u5b9e\u6027\uff09\u548c\u5ba2\u89c2\u7684\u591a\u9879\u9009\u62e9\u7406\u89e3\u548c\u56de\u5fc6\u6d4b\u91cf\u6765\u8bc4\u4f30PLS\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u68c0\u67e5\u4e8610\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136LLMs\u751f\u6210\u7684PLS\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684PLS\u96be\u4ee5\u533a\u5206\uff0c\u4f46\u4eba\u7c7b\u64b0\u5199\u7684PLS\u5728\u7406\u89e3\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u7528\u4e8e\u8bc4\u4f30PLS\u9002\u7528\u6027\u7684\u8d28\u7591\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u751f\u6210\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684PLS\u96be\u4ee5\u533a\u5206\u7684\u6458\u8981\uff0c\u4f46\u4eba\u7c7b\u64b0\u5199\u7684PLS\u5728\u7406\u89e3\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u7528\u4e8e\u8bc4\u4f30PLS\u9002\u7528\u6027\u7684\u8d28\u7591\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u8d85\u8d8a\u8868\u9762\u8d28\u91cf\u7684\u8bc4\u4f30\u6846\u67b6\u4ee5\u53ca\u660e\u786e\u4f18\u5316\u666e\u901a\u8bfb\u8005\u7406\u89e3\u7684\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2505.10223", "pdf": "https://arxiv.org/pdf/2505.10223", "abs": "https://arxiv.org/abs/2505.10223", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MIDL 2025", "summary": "Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u7b49\u66ff\u4ee3\u589e\u5f3a\u7b56\u7565\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0d\u660e\u786e\u9488\u5bf9\u7279\u5b9a\u5206\u5e03\u504f\u79fb\u6e90\u7684\u60c5\u51b5\u4e0b\uff0c\u51cf\u8f7b\u4e86\u591a\u79cd\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u901a\u5e38\u5728\u7cbe\u5fc3\u6311\u9009\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u662f\u7531\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002\u4f20\u7edf\u89c6\u89c9\u4e00\u81f4\u7684\u589e\u5f3a\u7b56\u7565\u7f3a\u4e4f\u5e94\u5bf9\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u6240\u9700\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u66ff\u4ee3\u589e\u5f3a\u7b56\u7565\uff0c\u91cd\u70b9\u662fMixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0d\u660e\u786e\u9488\u5bf9\u7279\u5b9a\u5206\u5e03\u504f\u79fb\u6e90\u7684\u60c5\u51b5\u4e0b\uff0c\u51cf\u8f7b\u4e86\u591a\u79cd\u53d8\u5316\u7684\u5f71\u54cd\u3002", "result": "\u8fd9\u4e9b\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5fc3\u810f\u7535\u5f71MRI\u548c\u524d\u5217\u817aMRI\u5206\u5272\u4e2d\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u6210\u50cf\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u5b9a\u91cf\u5206\u6790\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u4fc3\u8fdb\u7279\u5f81\u8868\u793a\u7684\u53ef\u5206\u6027\u548c\u7d27\u51d1\u6027\u6765\u589e\u5f3a\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8868\u793a\u3002", "conclusion": "\u8fd9\u4e9b\u589e\u5f3a\u65b9\u6cd5\u901a\u8fc7\u4fc3\u8fdb\u7279\u5f81\u8868\u793a\u7684\u53ef\u5206\u6027\u548c\u7d27\u51d1\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u6210\u50cf\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728nnU-Net\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u5b9e\u73b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u533b\u5b66\u5206\u5272\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.09983", "pdf": "https://arxiv.org/pdf/2505.09983", "abs": "https://arxiv.org/abs/2505.09983", "authors": ["Changxun Zhu", "Qilong Wu", "Lingjuan Lyu", "Shibei Xue"], "title": "Sybil-based Virtual Data Poisoning Attacks in Federated Learning", "categories": ["cs.CR", "cs.LG"], "comment": "7 pages, 6 figures, accepted by IEEE Codit 2025", "summary": "Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8esybil\u7684\u865a\u62df\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210sybil\u8282\u70b9\u6765\u653e\u5927\u4e2d\u6bd2\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u76ee\u6807\u6a21\u578b\u83b7\u53d6\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3002\u5728\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5bb9\u6613\u53d7\u5230\u6076\u610f\u5bf9\u624b\u7684\u4e2d\u6bd2\u653b\u51fb\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9ad8\u6602\u7684\u6210\u672c\u624d\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u653b\u51fb\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8esybil\u7684\u865a\u62df\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\uff0c\u5176\u4e2d\u6076\u610f\u5ba2\u6237\u7aef\u751f\u6210sybil\u8282\u70b9\u4ee5\u653e\u5927\u4e2d\u6bd2\u6a21\u578b\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u5339\u914d\u7684\u865a\u62df\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e09\u79cd\u9488\u5bf9\u76ee\u6807\u6a21\u578b\u83b7\u53d6\u7684\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5728\u7ebf\u672c\u5730\u3001\u5728\u7ebf\u5168\u5c40\u548c\u79bb\u7ebf\u573a\u666f\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u4e2d\u4f18\u4e8e\u5176\u4ed6\u653b\u51fb\u7b97\u6cd5\uff0c\u56e0\u4e3a\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u975e\u72ec\u7acb\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u4e0b\u83b7\u5f97\u5168\u5c40\u76ee\u6807\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u4e2d\u4f18\u4e8e\u5176\u4ed6\u653b\u51fb\u7b97\u6cd5\uff0c\u56e0\u4e3a\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u975e\u72ec\u7acb\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u4e0b\u83b7\u5f97\u5168\u5c40\u76ee\u6807\u6a21\u578b\u3002"}}
{"id": "2505.10413", "pdf": "https://arxiv.org/pdf/2505.10413", "abs": "https://arxiv.org/abs/2505.10413", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Zhicheng Dou"], "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.", "AI": {"tldr": "LongRefiner is an efficient refiner that leverages the structural characteristics of long documents to reduce computational costs and improve performance in RAG applications.", "motivation": "Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance.", "method": "LongRefiner employs dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model.", "result": "Experiments on seven QA datasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline.", "conclusion": "LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications."}}
{"id": "2505.10231", "pdf": "https://arxiv.org/pdf/2505.10231", "abs": "https://arxiv.org/abs/2505.10231", "authors": ["Haozhe Luo", "Ziyu Zhou", "Zixin Shu", "Aur\u00e9lie Pahud de Mortanges", "Robert Berke", "Mauricio Reyes"], "title": "On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u4eba\u673a\u5bf9\u9f50\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u5c06\u4eba\u7c7b\u89c1\u89e3\u7eb3\u5165\u53ef\u4ee5\u51cf\u5c11\u516c\u5e73\u5dee\u8ddd\u5e76\u63d0\u9ad8\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8fc7\u5ea6\u5bf9\u9f50\u53ef\u80fd\u4f1a\u5f15\u5165\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u4ea7\u751f\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u4e4b\u95f4\u7684\u516c\u5e73\u6027\u5dee\u8ddd\u3002", "method": "\u6211\u4eec\u63d0\u4f9b\u4e86\u5bf9\u8fd9\u4e00\u9886\u57df\u7684\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u63a2\u7d22\uff0c\u5373\u4eba\u673a\u5bf9\u9f50\u548c\u516c\u5e73\u6027\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u4eba\u7c7b\u89c1\u89e3\u7eb3\u5165\u53ef\u4ee5\u6301\u7eed\u51cf\u5c11\u516c\u5e73\u5dee\u8ddd\u5e76\u63d0\u9ad8\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5c3d\u7ba1\u8fc7\u5ea6\u5bf9\u9f50\u53ef\u80fd\u4f1a\u5f15\u5165\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u4eba\u7c7b\u89c1\u89e3\u7eb3\u5165\u53ef\u4ee5\u6301\u7eed\u51cf\u5c11\u516c\u5e73\u5dee\u8ddd\u5e76\u63d0\u9ad8\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5c3d\u7ba1\u8fc7\u5ea6\u5bf9\u9f50\u53ef\u80fd\u4f1a\u5f15\u5165\u6027\u80fd\u6743\u8861\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u6821\u51c6\u7b56\u7565\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u4eba\u673a\u5bf9\u9f50\u4f5c\u4e3a\u5f00\u53d1\u516c\u5e73\u3001\u7a33\u5065\u548c\u6cdb\u5316\u533b\u5b66AI\u7cfb\u7edf\u7684\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u4e13\u5bb6\u6307\u5bfc\u548c\u81ea\u52a8\u5316\u6548\u7387\u3002"}}
{"id": "2505.10003", "pdf": "https://arxiv.org/pdf/2505.10003", "abs": "https://arxiv.org/abs/2505.10003", "authors": ["Tianyu Jiao", "Zhuoran Xiao", "Yihang Huang", "Chenhui Ye", "Yijia Feng", "Liyu Cai", "Jiang Chang", "Fangkun Liu", "Yin Xu", "Dazhi He", "Yunfeng Guan", "Wenjun Zhang"], "title": "AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAI2MMUM\u7684\u591a\u6a21\u6001\u901a\u7528\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u548c\u6267\u884c\u5404\u79cd\u7a7a\u4e2d\u63a5\u53e3\u4efb\u52a1\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a6G\u5bfc\u5411\u7684\u901a\u7528\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u5e76\u6267\u884c\u5404\u79cd\u7a7a\u4e2d\u63a5\u53e3\u4efb\u52a1\uff0c\u5df2\u6210\u4e3a\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u7684\u4e00\u4e2a\u5171\u540c\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4efb\u52a1\u611f\u77e5\u7684\u4eba\u5de5\u667a\u80fd-\u7a7a\u6c14\u63a5\u53e3\u591a\u6a21\u6001\u901a\u7528\u6a21\u578b\uff08AI2MMUM\uff09\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u56fa\u5b9a\u4efb\u52a1\u5173\u952e\u8bcd\u548c\u53ef\u5b66\u4e60\u7684\u9690\u5f0f\u524d\u7f00\u63d0\u793a\u6765\u589e\u5f3a\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u5934\u76f4\u63a5\u8f93\u51fa\u4efb\u52a1\u76ee\u6807\u3002", "result": "AI2MMUM\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u7269\u7406\u73af\u5883/\u65e0\u7ebf\u4fe1\u9053\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "AI2MMUM\u5728\u57fa\u4e8eWAIR-D\u548cDeepMIMO\u6570\u636e\u96c6\u7684\u4e94\u4e2a\u4ee3\u8868\u6027\u7269\u7406\u73af\u5883/\u65e0\u7ebf\u4fe1\u9053\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10446", "pdf": "https://arxiv.org/pdf/2505.10446", "abs": "https://arxiv.org/abs/2505.10446", "authors": ["Zemin Huang", "Zhiyang Chen", "Zijun Wang", "Tiancheng Li", "Guo-Jun Qi"], "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.", "AI": {"tldr": "DCoLT is a new reasoning framework for diffusion language models that uses reinforcement learning to optimize the entire reasoning trajectory, leading to improved performance on math and code generation tasks.", "motivation": "To improve the reasoning capabilities of diffusion language models by optimizing the entire reasoning trajectory with RL, allowing more flexible and effective reasoning compared to traditional Chain-of-Thought (CoT) methods.", "method": "DCoLT treats each intermediate step in the reverse diffusion process as a latent 'thinking' action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL). It allows bidirectional, non-linear reasoning and uses a probabilistic policy for continuous-time models and a ranking-based Unmasking Policy Module (UPM) for discrete-time models.", "result": "DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. DCoLT-reinforced LLaDA shows significant improvements in reasoning accuracy on math and code generation tasks.", "conclusion": "DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both, and DCoLT-reinforced LLaDA boosts its reasoning accuracy significantly on various tasks."}}
{"id": "2505.10238", "pdf": "https://arxiv.org/pdf/2505.10238", "abs": "https://arxiv.org/abs/2505.10238", "authors": ["Yanbo Ding"], "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MTVCrafter\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u76f4\u63a5\u5efa\u6a21\u539f\u59cb3D\u8fd0\u52a8\u5e8f\u5217\uff084D\u8fd0\u52a8\uff09\u7684\u4eba\u7c7b\u56fe\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u51654DMoT\u548cMV-DiT\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u5927\u7684\u65f6\u7a7a\u7ebf\u7d22\u548c\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e2D\u6e32\u67d3\u7684\u59ff\u6001\u56fe\u50cf\u8fdb\u884c\u8fd0\u52a8\u6307\u5bfc\uff0c\u8fd9\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u4e22\u5931\u4e86\u5f00\u653e\u4e16\u754c\u52a8\u753b\u4e2d\u7684\u91cd\u89813D\u4fe1\u606f\u3002", "method": "\u63d0\u51faMTVCrafter\u6846\u67b6\uff0c\u5f15\u51654DMoT\u548cMV-DiT\u6a21\u578b\uff0c\u76f4\u63a5\u5efa\u6a21\u539f\u59cb\u76843D\u8fd0\u52a8\u5e8f\u5217\uff08\u53734D\u8fd0\u52a8\uff09\u8fdb\u884c\u4eba\u7c7b\u56fe\u50cf\u52a8\u753b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMTVCrafter\u5728FID-VID\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff086.98\uff09\uff0c\u6bd4\u7b2c\u4e8c\u540d\u9ad8\u51fa65%\u3002MTVCrafter\u8fd8\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u5404\u79cd\u5f00\u653e\u4e16\u754c\u89d2\u8272\u3002", "conclusion": "MTVCrafter\u6807\u5fd7\u7740\u8be5\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u4e3a\u57fa\u4e8e\u59ff\u6001\u7684\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.10007", "pdf": "https://arxiv.org/pdf/2505.10007", "abs": "https://arxiv.org/abs/2505.10007", "authors": ["Zijun Chen", "Shengbo Wang", "Nian Si"], "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10493", "pdf": "https://arxiv.org/pdf/2505.10493", "abs": "https://arxiv.org/abs/2505.10493", "authors": ["Shaohan Wang", "Licheng Zhang", "Zheren Fu", "Zhendong Mao"], "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u8bad\u7ec3\u6846\u67b6CL-RAG\uff0c\u901a\u8fc7\u6784\u5efa\u4e0d\u540c\u96be\u5ea6\u7684\u8bad\u7ec3\u6570\u636e\u5e76\u5206\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684RAG\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f18\u5316\u68c0\u7d22\u5668\u6216\u751f\u6210\u5668\uff0c\u4f46\u6587\u6863\u7684\u6709\u6548\u6027\u5728\u4e0d\u540c\u7528\u6237\u67e5\u8be2\u4e2d\u5dee\u5f02\u5f88\u5927\uff0c\u8fd9\u963b\u788d\u4e86\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u5728\u8bad\u7ec3\u4e2d\u7684\u9002\u5e94\u6027\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u5b66\u4e60\u7684\u542f\u53d1\uff0c\u8bfe\u7a0b\u5b66\u4e60\u901a\u8fc7\u4ece\u7b80\u5355\u5230\u56f0\u96be\u7684\u6837\u672c\u8bad\u7ec3\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u5c06\u5176\u6574\u5408\u5230RAG\u7cfb\u7edf\u7684\u8bad\u7ec3\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u8bad\u7ec3\u6846\u67b6\uff0c\u79f0\u4e3aCL-RAG\u3002\u901a\u8fc7\u6837\u672c\u6f14\u5316\u5206\u522b\u6784\u5efa\u5177\u6709\u591a\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u7136\u540e\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u5206\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "CL-RAG\u6846\u67b6\u5728\u56db\u4e2a\u5f00\u653e\u57df\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u5148\u8fdb\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u4e862%\u52304%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CL-RAG\u6846\u67b6\u5728\u56db\u4e2a\u5f00\u653e\u57df\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u5148\u8fdb\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u4e862%\u52304%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.10250", "pdf": "https://arxiv.org/pdf/2505.10250", "abs": "https://arxiv.org/abs/2505.10250", "authors": ["Wenhao Shen", "Wanqi Yin", "Xiaofeng Yang", "Cheng Chen", "Chaoyue Song", "Zhongang Cai", "Lei Yang", "Hao Wang", "Guosheng Lin"], "title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR", "summary": "Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.", "AI": {"tldr": "ADHMR is a framework that improves human mesh recovery by aligning a diffusion-based model through preference optimization, resulting in better performance than existing methods.", "motivation": "The motivation is to address the issues of misalignment with 2D image observations and weak robustness to in-the-wild images in probabilistic methods for human mesh recovery.", "method": "ADHMR is a framework that aligns a diffusion-based HMR model using preference optimization. It involves training a human mesh prediction assessment model called HMR-Scorer, creating a preference dataset, and finetuning the base model with direct preference optimization.", "result": "ADHMR outperforms current state-of-the-art methods in human mesh recovery from single images.", "conclusion": "ADHMR outperforms current state-of-the-art methods in human mesh recovery from single images."}}
{"id": "2505.10010", "pdf": "https://arxiv.org/pdf/2505.10010", "abs": "https://arxiv.org/abs/2505.10010", "authors": ["Jing-Cheng Pang", "Kaiyuan Li", "Yidi Wang", "Si-Hang Yang", "Shengyi Jiang", "Yang Yu"], "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts", "categories": ["cs.LG"], "comment": null, "summary": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86ImagineBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u57fa\u51c6\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5229\u7528\u771f\u5b9e\u8f68\u8ff9\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u60f3\u8c61\u8f68\u8ff9\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u5229\u7528\u60f3\u8c61\u8f68\u8ff9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u7b97\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u57fa\u51c6\uff0c\u8be5\u9886\u57df\u7684\u53d1\u5c55\u53d7\u5230\u963b\u788d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u79bb\u7ebfRL\u7b97\u6cd5\u5728\u5229\u7528LLM\u751f\u6210\u8f68\u8ff9\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e86ImagineBench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5229\u7528\u771f\u5b9e\u8f68\u8ff9\u548cLLM\u751f\u6210\u8f68\u8ff9\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\uff0c\u53d1\u73b0\u7b80\u5355\u5730\u5e94\u7528\u73b0\u6709\u7b97\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u786c\u4efb\u52a1\u7684\u6210\u529f\u7387\u4ec5\u4e3a35.44%\uff0c\u800c\u4f7f\u7528\u771f\u5b9e\u8f68\u8ff9\u8bad\u7ec3\u7684\u65b9\u6cd5\u5219\u8fbe\u523064.37%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u66f4\u597d\u5730\u5229\u7528LLM\u751f\u6210\u7684\u60f3\u8c61\u8f68\u8ff9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u673a\u4f1a\uff0c\u5305\u62ec\u66f4\u597d\u5730\u5229\u7528\u60f3\u8c61\u8f68\u8ff9\u3001\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u6301\u7eed\u5b66\u4e60\uff0c\u4ee5\u53ca\u6269\u5c55\u5230\u591a\u6a21\u6001\u4efb\u52a1\u3002"}}
{"id": "2505.10494", "pdf": "https://arxiv.org/pdf/2505.10494", "abs": "https://arxiv.org/abs/2505.10494", "authors": ["Yutao Mou", "Xiao Deng", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30LLM\u4ee3\u7801\u5b89\u5168\u7684\u591a\u4efb\u52a1\u57fa\u51c6CoV-Eval\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u5224\u65ad\u6a21\u578bVC-Judge\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u5927\u591a\u6570LLM\u80fd\u8bc6\u522b\u6709\u6f0f\u6d1e\u7684\u4ee3\u7801\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u548c\u4fee\u590d\u6f0f\u6d1e\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "motivation": "\u5f53\u524d\u7684\u4ee3\u7801\u5b89\u5168\u57fa\u51c6\u4ec5\u4e13\u6ce8\u4e8e\u5355\u4e00\u7684\u8bc4\u4f30\u4efb\u52a1\u548c\u8303\u5f0f\uff0c\u5982\u4ee3\u7801\u8865\u5168\u548c\u751f\u6210\uff0c\u7f3a\u4e4f\u5728\u5b89\u5168\u4ee3\u7801\u751f\u6210\u3001\u6f0f\u6d1e\u4fee\u590d\u548c\u8bc6\u522b\u7b49\u7ef4\u5ea6\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CoV-Eval\uff0c\u4e00\u4e2a\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\u7684\u591a\u4efb\u52a1\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLM\u4ee3\u7801\u5b89\u5168\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86VC-Judge\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u5224\u65ad\u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u9ad8\u6548\u548c\u53ef\u9760\u5730\u5ba1\u67e5LLM\u751f\u6210\u7684\u7a0b\u5e8f\u4e2d\u7684\u6f0f\u6d1e\u3002", "result": "\u5c3d\u7ba1\u5927\u591a\u6570LLM\u80fd\u591f\u5f88\u597d\u5730\u8bc6\u522b\u6709\u6f0f\u6d1e\u7684\u4ee3\u7801\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u503e\u5411\u4e8e\u751f\u6210\u4e0d\u5b89\u5168\u7684\u4ee3\u7801\uff0c\u5e76\u4e14\u5728\u8bc6\u522b\u7279\u5b9a\u6f0f\u6d1e\u7c7b\u578b\u548c\u6267\u884c\u4fee\u590d\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u548c\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86LLM\u4ee3\u7801\u5b89\u5168\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u4f18\u5316\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2505.10257", "pdf": "https://arxiv.org/pdf/2505.10257", "abs": "https://arxiv.org/abs/2505.10257", "authors": ["Hao Lu", "Jiaqi Tang", "Jiyao Wang", "Yunfan LU", "Xu Cao", "Qingyong Hu", "Yin Wang", "Yuting Zhang", "Tianxin Xie", "Yunpeng Zhang", "Yong Chen", "Jiayu. Gao", "Bin Huang", "Dengbo He", "Shuiguang Deng", "Hao Chen", "Ying-Cong Chen"], "title": "Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot", "categories": ["cs.CV"], "comment": null, "summary": "The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SAGE DeeR\uff0c\u8fd9\u662f\u4e00\u79cd\u8d85\u7ea7\u5bf9\u9f50\u548c\u901a\u7528\u7684\u9a7e\u9a76\u4ee3\u7406\uff0c\u80fd\u591f\u6839\u636e\u4e0d\u540c\u4eba\u7684\u504f\u597d\u548c\u504f\u89c1\u505a\u51fa\u4e0d\u540c\u53cd\u5e94\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u89d2\u548c\u591a\u6a21\u5f0f\u8f93\u5165\u7406\u89e3\u7528\u6237\u7684\u884c\u4e3a\u548c\u51b3\u7b56\u3002", "motivation": "\u667a\u80fd\u9a7e\u9a76\u8231\u9700\u8981\u5339\u914d\u4e0d\u540c\u7528\u6237\u7684\u8212\u9002\u6027\u3001\u4ea4\u4e92\u6027\u548c\u5b89\u5168\u6027\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u4e0d\u540c\u4eba\u7684\u504f\u597d\u548c\u504f\u89c1\u505a\u51fa\u4e0d\u540c\u53cd\u5e94\u7684\u4ee3\u7406\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAGE DeeR\u7684\u8d85\u7ea7\u5bf9\u9f50\u548c\u901a\u7528\u9a7e\u9a76\u4ee3\u7406\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u548c\u591a\u6a21\u5f0f\u8f93\u5165\u7406\u89e3\u7528\u6237\u751f\u7406\u6307\u6807\u3001\u9762\u90e8\u60c5\u7eea\u3001\u624b\u90e8\u52a8\u4f5c\u3001\u8eab\u4f53\u52a8\u4f5c\u3001\u9a7e\u9a76\u573a\u666f\u548c\u884c\u4e3a\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u7a7a\u95f4\u4e2d\u7684\u9690\u542b\u601d\u7ef4\u94fe\u6765\u63d0\u9ad8\u5176\u901a\u7528\u6027\u548c\u8d85\u7ea7\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684SAGE DeeR\u5b9e\u73b0\u4e86\u4e09\u4e2a\u4eae\u70b9\uff1a\u8d85\u7ea7\u5bf9\u9f50\u3001\u901a\u7528\u6027\u548c\u81ea\u6211\u6fc0\u53d1\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u5e76\u5efa\u7acb\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf\u9a7e\u9a76\u4ee3\u7406\u7684\u611f\u77e5\u51b3\u7b56\u80fd\u529b\u548c\u8d85\u7ea7\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSAGE DeeR\u7684\u8d85\u7ea7\u5bf9\u9f50\u548c\u901a\u7528\u9a7e\u9a76\u4ee3\u7406\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u7684\u8212\u9002\u6027\u3001\u4ea4\u4e92\u6027\u548c\u5b89\u5168\u6027\u9700\u6c42\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u5e76\u5efa\u7acb\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2505.10037", "pdf": "https://arxiv.org/pdf/2505.10037", "abs": "https://arxiv.org/abs/2505.10037", "authors": ["Takafumi Ito", "Lysenko Artem", "Tatsuhiko Tsunoda"], "title": "Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "comment": "10 pages, 3 figures", "summary": "Quantum-classical Hybrid Machine Learning (QHML) models are recognized for\ntheir robust performance and high generalization ability even for relatively\nsmall datasets. These qualities offer unique advantages for anti-cancer drug\nresponse prediction, where the number of available samples is typically small.\nHowever, such hybrid models appear to be very sensitive to the data encoding\nused at the interface of a neural network and a quantum circuit, with\nsuboptimal choices leading to stability issues. To address this problem, we\npropose a novel strategy that uses a normalization function based on a\nmoderated gradient version of the $\\tanh$. This method transforms the outputs\nof the neural networks without concentrating them at the extreme value ranges.\nOur idea was evaluated on a dataset of gene expression and drug response\nmeasurements for various cancer cell lines, where we compared the prediction\nperformance of a classical deep learning model and several QHML models. These\nresults confirmed that QHML performed better than the classical models when\ndata was optimally normalized. This study opens up new possibilities for\nbiomedical data analysis using quantum computers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6297\u764c\u836f\u7269\u53cd\u5e94\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4f7f\u5176\u5728\u6297\u764c\u836f\u7269\u53cd\u5e94\u9884\u6d4b\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5bf9\u795e\u7ecf\u7f51\u7edc\u548c\u91cf\u5b50\u7535\u8def\u63a5\u53e3\u5904\u7684\u6570\u636e\u7f16\u7801\u975e\u5e38\u654f\u611f\uff0c\u5bfc\u81f4\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u6b63\u68af\u5ea6\u7248\u672c\u7684$\tanh$\u7684\u5f52\u4e00\u5316\u51fd\u6570\u7684\u65b0\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u6570\u636e\u7f16\u7801\u654f\u611f\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u6570\u636e\u7ecf\u8fc7\u6700\u4f18\u5f52\u4e00\u5316\u65f6\uff0c\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f7f\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u8fdb\u884c\u751f\u7269\u533b\u5b66\u6570\u636e\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507", "abs": "https://arxiv.org/abs/2505.10507", "authors": ["Benedikt Ebing", "Goran Glava\u0161"], "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8bcd\u5bf9\u9f50\uff08WAs\uff09\u7684\u6807\u7b7e\u6295\u5f71\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\uff08XLT\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u7b56\u7565\uff0c\u7ed3\u5408\u4e86translate-train\u548ctranslate-test\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6295\u5f71\uff0c\u5e76\u63d0\u9ad8\u4e86XLT\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u8bcd\u5bf9\u9f50\uff08WAs\uff09\u7684\u6807\u7b7e\u6295\u5f71\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\uff08XLT\uff09\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u4f4e\u7ea7\u8bbe\u8ba1\u51b3\u7b56\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7684\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u58f0\u79f0\u5728XLT\u7684\u6807\u7b7e\u6295\u5f71\u4e2d\u4f18\u4e8eWAs\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u91cd\u65b0\u8bc4\u4f30WAs\u5728XLT\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u8bcd\u5bf9\u9f50\uff08WAs\uff09\u5728\u6807\u7b7e\u6295\u5f71\u4e2d\u7684\u5e94\u7528\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4f4e\u7ea7\u8bbe\u8ba1\u51b3\u7b56\u5bf9token\u7ea7XLT\u7684\u5f71\u54cd\uff0c\u5305\u62ec\uff1a(i) \u5728\uff08\u591a\uff09token\u8de8\u5ea6\u4e4b\u95f4\u6295\u5f71\u6807\u7b7e\u7684\u7b97\u6cd5\uff0c(ii) \u51cf\u5c11\u566a\u58f0\u6620\u5c04\u6807\u7b7e\u6570\u91cf\u7684\u8fc7\u6ee4\u7b56\u7565\uff0c\u4ee5\u53ca(iii) \u7ffb\u8bd1\u53e5\u5b50\u7684\u9884\u5206\u8bcd\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u7ed3\u5408\u4e86translate-train\u548ctranslate-test\u9884\u6d4b\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u8fd9\u4e9b\u4f4e\u7ea7\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u57fa\u4e8e\u7ffb\u8bd1\u7684XLT\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u901a\u8fc7\u4f18\u5316\u9009\u62e9\uff0c\u57fa\u4e8eWAs\u7684XLT\u6027\u80fd\u81f3\u5c11\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u6211\u4eec\u5f15\u5165\u7684\u65b0\u6295\u5f71\u7b56\u7565\u7ed3\u5408\u4e86translate-train\u548ctranslate-test\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6295\u5f71\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u96c6\u6210\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u4f4e\u7ea7WA\u8bbe\u8ba1\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86XLT\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u4f18\u5316\u9009\u62e9\uff0c\u57fa\u4e8e\u8bcd\u5bf9\u9f50\u7684XLT\u53ef\u4ee5\u8fbe\u5230\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u7ed3\u5408\u4e86translate-train\u548ctranslate-test\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6295\u5f71\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u4e5f\u51cf\u5c11\u4e86\u5bf9\u4f4e\u7ea7WA\u8bbe\u8ba1\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u5728token\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684XLT\u3002"}}
{"id": "2505.10258", "pdf": "https://arxiv.org/pdf/2505.10258", "abs": "https://arxiv.org/abs/2505.10258", "authors": ["Michael Hubbertz", "Pascal Colling", "Qi Han", "Tobias Meisen"], "title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper was accepted at the CVPR WAD 2025 Workshop", "summary": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u6620\u5c04\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u975e\u6b63\u5f0f\u8def\u7ebf\u6765\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5730\u56fe\u521b\u5efa\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u907f\u514d\u624b\u52a8\u6807\u6ce8\u7684\u5927\u91cf\u5de5\u4f5c\uff0c\u81ea\u52a8\u5316\u5730\u56fe\u521b\u5efa\u7684\u65b9\u6cd5\u5df2\u7ecf\u51fa\u73b0\u3002\u7136\u800c\uff0c\u5728\u7ebf\u6620\u5c04\u4ecd\u7136\u9762\u4e34\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u4f20\u611f\u5668\u906e\u6321\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u6cdb\u5316\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u6620\u5c04\u65b9\u6cd5\uff0c\u5c06\u9a7e\u9a76\u8005\u4f7f\u7528\u7684\u975e\u6b63\u5f0f\u8def\u7ebf\u6574\u5408\u5230\u5730\u56fe\u521b\u5efa\u8fc7\u7a0b\u4e2d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ece\u81ea\u8f66\u548c\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u805a\u5408\u8f68\u8ff9\u6570\u636e\uff0c\u6784\u5efa\u5168\u9762\u7684\u5168\u5c40\u5730\u56fe\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0e\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6620\u5c04\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u73af\u5883\u548c\u4f20\u611f\u5668\u914d\u7f6e\u7684\u6539\u8fdb\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2505.10039", "pdf": "https://arxiv.org/pdf/2505.10039", "abs": "https://arxiv.org/abs/2505.10039", "authors": ["Hang Chen", "Jiaying Zhu", "Xinyu Yang", "Wenya Wang"], "title": "Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Circuit discovery has gradually become one of the prominent methods for\nmechanistic interpretability, and research on circuit completeness has also\ngarnered increasing attention. Methods of circuit discovery that do not\nguarantee completeness not only result in circuits that are not fixed across\ndifferent runs but also cause key mechanisms to be omitted. The nature of\nincompleteness arises from the presence of OR gates within the circuit, which\nare often only partially detected in standard circuit discovery methods. To\nthis end, we systematically introduce three types of logic gates: AND, OR, and\nADDER gates, and decompose the circuit into combinations of these logical\ngates. Through the concept of these gates, we derive the minimum requirements\nnecessary to achieve faithfulness and completeness. Furthermore, we propose a\nframework that combines noising-based and denoising-based interventions, which\ncan be easily integrated into existing circuit discovery methods without\nsignificantly increasing computational complexity. This framework is capable of\nfully identifying the logic gates and distinguishing them within the circuit.\nIn addition to the extensive experimental validation of the framework's ability\nto restore the faithfulness, completeness, and sparsity of circuits, using this\nframework, we uncover fundamental properties of the three logic gates, such as\ntheir proportions and contributions to the output, and explore how they behave\namong the functionalities of language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc6\u522b\u7535\u8def\u4e2d\u7684\u903b\u8f91\u95e8\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u6a21\u578b\u529f\u80fd\u4e2d\u7684\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u7535\u8def\u5728\u4e0d\u540c\u8fd0\u884c\u4e2d\u4e0d\u4e00\u81f4\uff0c\u5e76\u4e14\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u673a\u5236\u3002OR \u95e8\u7684\u5b58\u5728\u662f\u4e0d\u5b8c\u6574\u6027\u7684\u6839\u6e90\uff0c\u901a\u5e38\u5728\u6807\u51c6\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u4e2d\u53ea\u80fd\u90e8\u5206\u68c0\u6d4b\u5230\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u5f15\u5165\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\uff1aAND\u3001OR \u548c ADDER \u95e8\uff0c\u5e76\u5c06\u7535\u8def\u5206\u89e3\u4e3a\u8fd9\u4e9b\u903b\u8f91\u95e8\u7684\u7ec4\u5408\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u566a\u58f0\u548c\u53bb\u566a\u5e72\u9884\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u4e2d\uff0c\u800c\u4e0d\u4f1a\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b8c\u5168\u8bc6\u522b\u903b\u8f91\u95e8\u5e76\u5728\u7535\u8def\u4e2d\u533a\u5206\u5b83\u4eec\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6062\u590d\u7535\u8def\u5fe0\u5b9e\u6027\u3001\u5b8c\u6574\u6027\u548c\u7a00\u758f\u6027\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u5982\u5b83\u4eec\u7684\u6bd4\u4f8b\u548c\u5bf9\u8f93\u51fa\u7684\u8d21\u732e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u566a\u58f0\u548c\u53bb\u566a\u5e72\u9884\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5168\u9762\u8bc6\u522b\u903b\u8f91\u95e8\u5e76\u5728\u7535\u8def\u4e2d\u533a\u5206\u5b83\u4eec\u3002\u6b64\u5916\uff0c\u8fd8\u63ed\u793a\u4e86\u4e09\u79cd\u903b\u8f91\u95e8\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u6a21\u578b\u529f\u80fd\u4e2d\u7684\u884c\u4e3a\u3002"}}
{"id": "2505.10518", "pdf": "https://arxiv.org/pdf/2505.10518", "abs": "https://arxiv.org/abs/2505.10518", "authors": ["Anastasios Gerontopoulos", "Spyros Gidaris", "Nikos Komodakis"], "title": "Multi-Token Prediction Needs Registers", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.", "AI": {"tldr": "MuToR is a novel approach to multi-token prediction that improves language model pretraining by introducing learnable register tokens, offering compatibility with existing models and demonstrating effectiveness across various tasks.", "motivation": "Multi-token prediction has shown promise for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. The paper aims to address this limitation by proposing an effective and compatible approach.", "method": "MuToR interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. It introduces only a negligible number of additional parameters, requires no architectural changes, and remains aligned with the next-token pretraining objective.", "result": "MuToR demonstrates effectiveness and versatility across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains.", "conclusion": "MuToR is a simple and effective approach to multi-token prediction that offers several key advantages, including negligible additional parameters, compatibility with off-the-shelf pretrained language models, and alignment with the next-token pretraining objective. It demonstrates effectiveness and versatility across various use cases in both language and vision domains."}}
{"id": "2505.10267", "pdf": "https://arxiv.org/pdf/2505.10267", "abs": "https://arxiv.org/abs/2505.10267", "authors": ["Pavel Korotaev", "Petr Surovtsev", "Alexander Kapitanov", "Karina Kvanchiani", "Aleksandr Nagaev"], "title": "HandReader: Advanced Techniques for Efficient Fingerspelling Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/ai-forever/handreader", "summary": "Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HandReader\uff0c\u4e00\u79cd\u7528\u4e8e\u624b\u6307\u62fc\u5199\u8bc6\u522b\u7684\u65b0\u578b\u67b6\u6784\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u5173\u6ce8\u4e8e\u5904\u7406\u89c6\u9891\u7684\u65f6\u95f4\u7ef4\u5ea6\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u63d0\u9ad8\u624b\u6307\u62fc\u5199\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HandReader\u5305\u542b\u4e09\u4e2a\u67b6\u6784\uff1aHandReader_RGB\u4f7f\u7528\u4e86\u65b0\u7684\u65f6\u95f4\u79fb\u4f4d\u81ea\u9002\u5e94\u6a21\u5757\uff08TSAM\uff09\u6765\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u89c6\u9891\u7684RGB\u7279\u5f81\uff1bHandReader_KP\u57fa\u4e8e\u63d0\u51fa\u7684\u65f6\u7a7a\u59ff\u6001\u7f16\u7801\u5668\uff08TPE\uff09\u5728\u5173\u952e\u70b9\u4e0a\u64cd\u4f5c\uff1bHandReader_RGB+KP\u662f\u4e00\u79cd\u7ed3\u5408\u7f16\u7801\u5668\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u5229\u7528RGB\u548c\u5173\u952e\u70b9\u6a21\u6001\u3002", "result": "HandReader\u7684\u6bcf\u4e2a\u6a21\u578b\u90fd\u5177\u6709\u72ec\u7279\u7684\u4f18\u52bf\uff0c\u5e76\u5728ChicagoFSWild\u548cChicagoFSWild+\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u4fc4\u7f57\u65af\u624b\u6307\u62fc\u5199\u7684\u7b2c\u4e00\u5f00\u653e\u6570\u636e\u96c6Znaki\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86HandReader\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u624b\u6307\u62fc\u5199\u8bc6\u522b\u7684\u4e09\u79cd\u67b6\u6784\u7684\u96c6\u5408\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86Znaki\u6570\u636e\u96c6\uff0c\u5e76\u4e14HandReader\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u516c\u5f00\u53ef\u7528\u7684\u3002"}}
{"id": "2505.10040", "pdf": "https://arxiv.org/pdf/2505.10040", "abs": "https://arxiv.org/abs/2505.10040", "authors": ["Lei Song", "Jiaxing Li", "Shihan Guan", "Youyong Kong"], "title": "Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their\ncapacity to preserve previously acquired knowledge amid the assimilation of\nnovel information. Rehearsal-based techniques revisit historical examples,\nadopted as a principal strategy to alleviate this phenomenon. However, memory\nexplosion and privacy infringements impose significant constraints on their\nutility. Non-Exemplar methods circumvent the prior issues through Prototype\nReplay (PR), yet feature drift presents new challenges. In this paper, our\nempirical findings reveal that Prototype Contrastive Learning (PCL) exhibits\nless pronounced drift than conventional PR. Drawing upon PCL, we propose\nInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar\nContinual Graph Learning (NECGL). Exploiting graph structural information, we\nformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature\ndistributions towards high-impact nodes to augment the model's capacity for\nassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)\nsafeguards task memory by regularizing discontinuities in class relationships.\nMoreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,\nfostering greater inter-class discriminability. Evaluations on four node\nclassification benchmark datasets demonstrate that our method outperforms\nexisting state-of-the-art methods, achieving a better trade-off between\nplasticity and stability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u793a\u4f8b\u8fde\u7eed\u56fe\u5b66\u4e60\u8303\u5f0fIPAL\uff0c\u901a\u8fc7\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff08PCL\uff09\u51cf\u5c11\u7279\u5f81\u6f02\u79fb\uff0c\u5e76\u5229\u7528\u56fe\u7ed3\u6784\u4fe1\u606f\u5236\u5b9aTopology-Integrated Gaussian Prototypes (TIGP)\u6765\u589e\u5f3a\u6a21\u578b\u5438\u6536\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86Instance-Prototype Affinity Distillation (IPAD)\u548cDecision Boundary Perception (DBP)\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u7c7b\u95f4\u53ef\u533a\u5206\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "Graph Neural Networks (GNN) \u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u8fd9\u524a\u5f31\u4e86\u5b83\u4eec\u5728\u5438\u6536\u65b0\u4fe1\u606f\u65f6\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u57fa\u4e8e\u91cd\u653e\u7684\u6280\u672f\u901a\u8fc7\u56de\u987e\u5386\u53f2\u793a\u4f8b\u6765\u7f13\u89e3\u8fd9\u4e00\u73b0\u8c61\uff0c\u4f46\u5185\u5b58\u7206\u70b8\u548c\u9690\u79c1\u4fb5\u72af\u9650\u5236\u4e86\u5176\u6548\u7528\u3002\u975e\u793a\u4f8b\u65b9\u6cd5\u901a\u8fc7\u539f\u578b\u91cd\u653e\uff08PR\uff09\u89c4\u907f\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7279\u5f81\u6f02\u79fb\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Instance-Prototype Affinity Learning (IPAL)\uff0c\u8fd9\u662f\u4e00\u79cd\u975e\u793a\u4f8b\u8fde\u7eed\u56fe\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\u3002\u6211\u4eec\u5229\u7528\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u5236\u5b9a\u4e86Topology-Integrated Gaussian Prototypes (TIGP)\uff0c\u6307\u5bfc\u7279\u5f81\u5206\u5e03\u5411\u9ad8\u5f71\u54cd\u8282\u70b9\u53d1\u5c55\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5438\u6536\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\u3002Instance-Prototype Affinity Distillation (IPAD)\u901a\u8fc7\u89c4\u8303\u7c7b\u5173\u7cfb\u4e2d\u7684\u4e0d\u8fde\u7eed\u6027\u6765\u4fdd\u62a4\u4efb\u52a1\u8bb0\u5fc6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728PCL\u4e2d\u5d4c\u5165\u4e86\u4e00\u4e2aDecision Boundary Perception (DBP)\u673a\u5236\uff0c\u4fc3\u8fdb\u7c7b\u95f4\u53ef\u533a\u5206\u6027\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff08PCL\uff09\u6bd4\u4f20\u7edf\u7684PR\u8868\u73b0\u51fa\u66f4\u5c0f\u7684\u6f02\u79fb\u3002\u57fa\u4e8ePCL\uff0c\u6211\u4eec\u63d0\u51fa\u4e86IPAL\uff0c\u4e00\u79cd\u7528\u4e8e\u975e\u793a\u4f8b\u8fde\u7eed\u56fe\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56db\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527", "abs": "https://arxiv.org/abs/2505.10527", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "title": "WorldPM: Scaling Human Preference Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "AI": {"tldr": "This paper proposes WorldPM, a preference modeling approach that emphasizes scaling potential. It shows that WorldPM improves the generalization performance across human preference datasets of varying sizes and achieves significant improvements in in-house and public evaluation sets.", "motivation": "The motivation is to find similar scaling laws in preference modeling as those in language modeling, which demonstrate how test loss scales as a power law with model and dataset sizes.", "method": "WorldPM is proposed to emphasize the scaling potential of preference modeling. Preference data is collected from public forums, and extensive training is conducted using 15M-scale data across models ranging from 1.5B to 72B parameters.", "result": "WorldPM shows distinct patterns across different evaluation metrics: adversarial metrics scale up with increased training data and base model size, objective metrics show emergent behavior in larger language models, and subjective metrics do not demonstrate scaling trends. WorldPM improves the generalization performance across human preference datasets of varying sizes, with performance gains exceeding 5% on many key subtasks. It also shows significant improvements in in-house and public evaluation sets.", "conclusion": "WorldPM is effective for preference fine-tuning and improves the generalization performance across human preference datasets of varying sizes. It also shows significant improvements in in-house and public evaluation sets."}}
{"id": "2505.10281", "pdf": "https://arxiv.org/pdf/2505.10281", "abs": "https://arxiv.org/abs/2505.10281", "authors": ["Mengqiu Xu", "Kaixin Chen", "Heng Guo", "Yixiang Huang", "Ming Wu", "Zhenwei Shi", "Chuang Zhang", "Jun Guo"], "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MFogHub\uff0c\u4e00\u4e2a\u7528\u4e8e\u6d77\u6d0b\u96fe\u68c0\u6d4b\u548c\u9884\u6d4b\u7684\u591a\u533a\u57df\u3001\u591a\u536b\u661f\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4fc3\u8fdb\u5168\u7403\u8303\u56f4\u5185\u7684\u6d77\u6d0b\u96fe\u52a8\u6001\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u533a\u57df\u6216\u536b\u661f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u7684\u591a\u6837\u6027\uff0c\u5e76\u963b\u788d\u4e86\u5bf9\u6d77\u6d0b\u96fe\u5185\u5728\u7279\u6027\u7684\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86MFogHub\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u533a\u57df\u548c\u591a\u536b\u661f\u7684\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u6765\u81ea15\u4e2a\u6cbf\u6d77\u96fe\u6613\u53d1\u5730\u533a\u7684\u6807\u6ce8\u6d77\u6d0b\u96fe\u89c2\u6d4b\u6570\u636e\u548c\u516d\u9897\u9759\u6b62\u536b\u661f\u7684\u6570\u636e\u3002", "result": "MFogHub\u5305\u542b\u8d85\u8fc768,000\u4e2a\u9ad8\u5206\u8fa8\u7387\u6837\u672c\uff0c\u80fd\u591f\u63ed\u793a\u7531\u4e8e\u5730\u533a\u548c\u536b\u661f\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u6ce2\u52a8\uff0c\u5e76\u4f5c\u4e3a\u5f00\u53d1\u6709\u9488\u5bf9\u6027\u548c\u53ef\u6269\u5c55\u7684\u96fe\u9884\u6d4b\u6280\u672f\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "\u901a\u8fc7MFogHub\uff0c\u6211\u4eec\u65e8\u5728\u5728\u5168\u7403\u8303\u56f4\u5185\u63a8\u8fdb\u6d77\u6d0b\u96fe\u52a8\u6001\u7684\u5b9e\u9645\u76d1\u6d4b\u548c\u79d1\u5b66\u7406\u89e3\u3002"}}
{"id": "2505.10050", "pdf": "https://arxiv.org/pdf/2505.10050", "abs": "https://arxiv.org/abs/2505.10050", "authors": ["Fahad Almalki", "Mehedi Masud"], "title": "Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional machine learning models often prioritize predictive accuracy,\noften at the expense of model transparency and interpretability. The lack of\ntransparency makes it difficult for organizations to comply with regulatory\nrequirements and gain stakeholders trust. In this research, we propose a fraud\ndetection framework that combines a stacking ensemble of well-known gradient\nboosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable\nartificial intelligence (XAI) techniques are used to enhance the transparency\nand interpretability of the model's decisions. We used SHAP (SHapley Additive\nExplanations) for feature selection to identify the most important features.\nFurther efforts were made to explain the model's predictions using Local\nInterpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots\n(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection\ndataset, which includes more than 590,000 real transaction records, was used to\nevaluate the proposed model. The model achieved a high performance with an\naccuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent\nrelated approaches. These results indicate that combining high prediction\naccuracy with transparent interpretability is possible and could lead to a more\nethical and trustworthy solution in financial fraud detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u6b3a\u8bc8\u68c0\u6d4b\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u4f18\u5148\u8003\u8651\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u800c\u727a\u7272\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u7f3a\u4e4f\u900f\u660e\u5ea6\u4f7f\u5f97\u7ec4\u7ec7\u96be\u4ee5\u9075\u5b88\u76d1\u7ba1\u8981\u6c42\u5e76\u83b7\u5f97\u5229\u76ca\u76f8\u5173\u8005\u7684\u4fe1\u4efb\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u4f17\u6240\u5468\u77e5\u7684\u68af\u5ea6\u63d0\u5347\u6a21\u578b\uff08XGBoost\u3001LightGBM \u548c CatBoost\uff09\u7684\u5806\u53e0\u96c6\u6210\u7684\u6b3a\u8bc8\u68c0\u6d4b\u6846\u67b6\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u4e86\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\u6765\u589e\u5f3a\u6a21\u578b\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u8be5\u6a21\u578b\u5728 IEEE-CIS \u6b3a\u8bc8\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8fbe\u5230 99%\uff0cAUC-ROC \u5f97\u5206\u8fbe\u5230 0.99\uff0c\u4f18\u4e8e\u51e0\u79cd\u6700\u8fd1\u7684\u76f8\u5173\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u900f\u660e\u7684\u53ef\u89e3\u91ca\u6027\u76f8\u7ed3\u5408\u662f\u53ef\u80fd\u7684\uff0c\u5e76\u4e14\u53ef\u80fd\u5728\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u5e26\u6765\u66f4\u9053\u5fb7\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554", "abs": "https://arxiv.org/abs/2505.10554", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5bf9\u9f50\u6a21\u578b\u7684\u5143\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u5f15\u53d1\u4e00\u4e9b\u9ad8\u7ea7\u63a8\u7406\u884c\u4e3a\uff0c\u4f46\u8fd9\u4e9b\u884c\u4e3a\u7684\u65f6\u95f4\u548c\u4e00\u81f4\u6027\u4e0d\u53ef\u9884\u6d4b\uff0c\u9650\u5236\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u7ba1\u9053\uff1a\u4e2a\u4f53\u5bf9\u9f50\u3001\u53c2\u6570\u7a7a\u95f4\u5408\u5e76\u548c\u9886\u57df\u7279\u5b9a\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u5f0f\u5bf9\u9f50\u6a21\u578b\u7684\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u7c7b\u6bd4\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u6307\u4ee4\u8c03\u4f18\u57fa\u7ebf\u63d0\u9ad8\u4e86\u8d85\u8fc710%\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u9886\u57df\u7279\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e862%\u7684\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5bf9\u9f50\u6a21\u578b\u7684\u5143\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.10289", "pdf": "https://arxiv.org/pdf/2505.10289", "abs": "https://arxiv.org/abs/2505.10289", "authors": ["Yue Wang", "Shuai Xu", "Xuelin Zhu", "Yicong Li"], "title": "MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\uff08MSCI\uff09\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u4e0d\u8db3\u3002MSCI\u901a\u8fc7\u5229\u7528CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e2d\u95f4\u5c42\u4fe1\u606f\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u805a\u5408\u5668\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5206\u9636\u6bb5\u4ea4\u4e92\u673a\u5236\u5c06\u5176\u878d\u5165\u6587\u672c\u8868\u793a\u4e2d\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u7ec6\u7c92\u5ea6\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86MSCI\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4e8eCLIP\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u6e90\u4e8e\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u5c42\u6b21\u8de8\u6a21\u6001\u4ea4\u4e92\uff08MSCI\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6709\u6548\u5730\u63a2\u7d22\u548c\u5229\u7528\u4e86CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e2d\u95f4\u5c42\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u81ea\u9002\u5e94\u805a\u5408\u5668\uff0c\u5206\u522b\u4ece\u4f4e\u7ea7\u89c6\u89c9\u7279\u5f81\u4e2d\u63d0\u53d6\u5c40\u90e8\u4fe1\u606f\uff0c\u5e76\u4ece\u9ad8\u7ea7\u89c6\u89c9\u7279\u5f81\u4e2d\u6574\u5408\u5168\u5c40\u4fe1\u606f\u3002\u8fd9\u4e9b\u5173\u952e\u4fe1\u606f\u901a\u8fc7\u5206\u9636\u6bb5\u7684\u4ea4\u4e92\u673a\u5236\u9010\u6b65\u878d\u5165\u6587\u672c\u8868\u793a\u4e2d\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u7ec6\u7c92\u5ea6\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\u3002\u6b64\u5916\uff0cMSCI\u6839\u636e\u4e0d\u540c\u7684\u7ec4\u5408\u4ee5\u53ca\u540c\u4e00\u7ec4\u5408\u4e2d\u7684\u4e0d\u540c\u5143\u7d20\u52a8\u6001\u8c03\u6574\u5168\u5c40\u548c\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u4f7f\u5176\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5404\u79cd\u573a\u666f\u3002", "result": "MSCI\u6a21\u578b\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5177\u6709\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "MSCI\u6a21\u578b\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.10057", "pdf": "https://arxiv.org/pdf/2505.10057", "abs": "https://arxiv.org/abs/2505.10057", "authors": ["Tiancong Cheng", "Ying Zhang", "Yuxuan Liang", "Roger Zimmermann", "Zhiwen Yu", "Bin Guo"], "title": "JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation", "categories": ["cs.LG"], "comment": null, "summary": "Depth estimation and scene segmentation are two important tasks in\nintelligent transportation systems. A joint modeling of these two tasks will\nreduce the requirement for both the storage and training efforts. This work\nexplores how the multi-task distillation could be used to improve such unified\nmodeling. While existing solutions transfer multiple teachers' knowledge in a\nstatic way, we propose a self-adaptive distillation method that can dynamically\nadjust the knowledge amount from each teacher according to the student's\ncurrent learning ability. Furthermore, as multiple teachers exist, the\nstudent's gradient update direction in the distillation is more prone to be\nerroneous where knowledge forgetting may occur. To avoid this, we propose a\nknowledge trajectory to record the most essential information that a model has\nlearnt in the past, based on which a trajectory-based distillation loss is\ndesigned to guide the student to follow the learning curve similarly in a\ncost-effective way. We evaluate our method on multiple benchmarking datasets\nincluding Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,\nour method achieves a clearly improvement. The code is provided in the\nsupplementary materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6df1\u5ea6\u4f30\u8ba1\u548c\u573a\u666f\u5206\u5272\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u77e5\u8bc6\u91cf\u548c\u907f\u514d\u77e5\u8bc6\u9057\u5fd8\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u4f30\u8ba1\u548c\u573a\u666f\u5206\u5272\u662f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u8054\u5408\u5efa\u6a21\u53ef\u4ee5\u51cf\u5c11\u5b58\u50a8\u548c\u8bad\u7ec3\u9700\u6c42\u3002\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u9759\u6001\u65b9\u5f0f\u4f20\u9012\u591a\u4e2a\u6559\u5e08\u7684\u77e5\u8bc6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u77e5\u8bc6\u9057\u5fd8\u7684\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u84b8\u998f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6839\u636e\u5b66\u751f\u5f53\u524d\u7684\u5b66\u4e60\u80fd\u529b\u52a8\u6001\u8c03\u6574\u4ece\u6bcf\u4e2a\u6559\u5e08\u83b7\u53d6\u7684\u77e5\u8bc6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u7684\u84b8\u998f\u635f\u5931\u6765\u907f\u514d\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u6211\u4eec\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ecCityscapes\u548cNYU-v2\uff09\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u660e\u663e\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.10292", "pdf": "https://arxiv.org/pdf/2505.10292", "abs": "https://arxiv.org/abs/2505.10292", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation", "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "31 pages, 14 figures", "summary": "Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b4,178\u4e2a\u6545\u4e8b\u7684StoryReasoning\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u53d9\u4e8b\u7cfb\u7edf\u4e2d\u7684\u6307\u79f0\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03Qwen2.5-VL 7B\u521b\u5efa\u4e86Qwen Storyteller\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "motivation": "\u89c6\u89c9\u53d9\u4e8b\u7cfb\u7edf\u96be\u4ee5\u5728\u5e27\u4e4b\u95f4\u4fdd\u6301\u89d2\u8272\u8eab\u4efd\u4e00\u81f4\u5e76\u6b63\u786e\u5173\u8054\u52a8\u4f5c\u4e0e\u9002\u5f53\u4e3b\u4f53\uff0c\u5bfc\u81f4\u6307\u79f0\u5e7b\u89c9\u95ee\u9898\u3002\u901a\u8fc7\u5728\u89c6\u89c9\u5143\u7d20\u4e0a\u5bf9\u89d2\u8272\u3001\u7269\u4f53\u548c\u5176\u4ed6\u5b9e\u4f53\u8fdb\u884c\u5b9a\u4f4d\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u4eba\u8138\u8bc6\u522b\u7684\u8de8\u5e27\u7269\u4f53\u518d\u8bc6\u522b\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7528\u4e8e\u663e\u5f0f\u53d9\u4e8b\u5efa\u6a21\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5c06\u6587\u672c\u5143\u7d20\u4e0e\u591a\u5e27\u89c6\u89c9\u5b9e\u4f53\u5173\u8054\u7684\u63a5\u5730\u65b9\u6848\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b4,178\u4e2a\u6545\u4e8b\u7684StoryReasoning\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6545\u4e8b\u5728\u5e27\u4e4b\u95f4\u4fdd\u6301\u89d2\u8272\u548c\u7269\u4f53\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u683c\u8868\u793a\u663e\u5f0f\u5efa\u6a21\u591a\u5e27\u5173\u7cfb\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u975e\u5fae\u8c03\u6a21\u578b\u76f8\u6bd4\uff0c\u5e73\u5747\u6bcf\u4e2a\u6545\u4e8b\u7684\u5e7b\u89c9\u51cf\u5c11\u4e8612.3%\u3002", "conclusion": "\u901a\u8fc7\u5fae\u8c03Qwen2.5-VL 7B\uff0c\u521b\u5efa\u4e86Qwen Storyteller\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u6545\u4e8b\u4e2d\u7269\u4f53\u5f15\u7528\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6267\u884c\u7aef\u5230\u7aef\u7684\u7269\u4f53\u68c0\u6d4b\u3001\u518d\u8bc6\u522b\u548c\u5730\u6807\u68c0\u6d4b\uff0c\u5e76\u4e14\u5e73\u5747\u6bcf\u4e2a\u6545\u4e8b\u7684\u5e7b\u89c9\u51cf\u5c11\u4e8612.3%\u3002"}}
{"id": "2505.10083", "pdf": "https://arxiv.org/pdf/2505.10083", "abs": "https://arxiv.org/abs/2505.10083", "authors": ["Chengsen Wang", "Qi Qi", "Zhongwen Rao", "Lujia Pan", "Jingyu Wang", "Jianxin Liao"], "title": "ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6ChronoSteer\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u4fee\u8ba2\u6307\u4ee4\u6765\u5f15\u5bfc\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5229\u7528\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u7684\u80fd\u529b\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u5206\u522b\u5728\u6587\u672c\u63a8\u7406\u548c\u65f6\u95f4\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u3002\u6574\u5408\u4e24\u8005\u7684\u4f18\u52bf\u4ee5\u6784\u5efa\u4e00\u4e2a\u540c\u65f6\u5229\u7528\u65f6\u95f4\u548c\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u672a\u6765\u63a8\u7406\u7684\u591a\u6a21\u6001\u6a21\u578b\u5df2\u6210\u4e3a\u5173\u952e\u7814\u7a76\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff1a\u4f7f\u7528LLM\u5c06\u6587\u672c\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u4fee\u8ba2\u6307\u4ee4\uff0c\u7136\u540e\u7528\u4e8e\u5f15\u5bfcTSFM\u7684\u8f93\u51fa\u3002\u5f15\u5165\u4e86ChronoSteer\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u4fee\u8ba2\u6307\u4ee4\u8fdb\u884c\u5f15\u5bfc\u7684\u591a\u6a21\u6001TSFM\u3002\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u3002", "result": "ChronoSteer\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u76f8\u6bd4\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e8625.7%\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u9ad8\u4e8622.5%\u3002", "conclusion": "ChronoSteer\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u76f8\u6bd4\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e8625.7%\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u9ad8\u4e8622.5%\u3002"}}
{"id": "2505.10294", "pdf": "https://arxiv.org/pdf/2505.10294", "abs": "https://arxiv.org/abs/2505.10294", "authors": ["Guillaume Balezo", "Roger Trullo", "Albert Pla Planas", "Etienne Decenciere", "Thomas Walter"], "title": "MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models", "categories": ["cs.CV", "q-bio.TO", "68T07 (Primary), 92C55 (Secondary)", "I.4.9; I.2.10; I.5.4; J.3"], "comment": null, "summary": "Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.", "AI": {"tldr": "MIPHEI is a model that predicts multiplex immunofluorescence signals from H&E images, achieving high accuracy in cell-type classification and offering potential for large-scale analysis of H&E datasets.", "motivation": "To bridge the gap between histopathological analysis and multiplex immunofluorescence (mIF) due to cost and logistical constraints.", "method": "MIPHEI (Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H&E images.", "result": "MIPHEI achieves accurate cell-type classification from H&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers.", "conclusion": "MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H&E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes."}}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117", "abs": "https://arxiv.org/abs/2505.10117", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfei Wang", "Jun Luo"], "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.", "AI": {"tldr": "This paper introduces MiCo, a hierarchical language agent framework that uses LLMs to solve the ODMBP problem in cloud services. It achieves high performance in large-scale and complex environments.", "motivation": "Traditional optimization methods, heuristic approaches, and learning-based methods face limitations in adapting to real-time changes, rigid strategies, and lack of generalizability and interpretability in solving the Online Dynamic Multidimensional Bin Packing (ODMBP) problem in cloud services.", "method": "The paper proposes a hierarchical language agent framework named MiCo, which formulates ODMBP as a Semi-Markov Decision Process with Options (SMDP-Option) and employs a two-stage architecture: Option Miner and Option Composer. Option Miner discovers non-context-aware strategies using LLMs, while Option Composer integrates these strategies with contextual ones.", "result": "MiCo achieves a 96.9% competitive ratio in large-scale scenarios involving more than 10,000 virtual machines and maintains high performance under nonstationary request flows and diverse configurations.", "conclusion": "MiCo demonstrates its effectiveness in complex and large-scale cloud environments by achieving a high competitive ratio and maintaining performance under nonstationary request flows and diverse configurations."}}
{"id": "2505.10351", "pdf": "https://arxiv.org/pdf/2505.10351", "abs": "https://arxiv.org/abs/2505.10351", "authors": ["Jie Zhu", "Jirong Zha", "Ding Li", "Leye Wang"], "title": "A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability", "categories": ["cs.CV"], "comment": "An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders", "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6210\u5458\u63a8\u7406\u65b9\u6cd5PartCrop\uff0c\u7528\u4e8e\u653b\u51fb\u81ea\u76d1\u7763\u89c6\u89c9\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86\u9632\u5fa1\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u9690\u79c1\u65b9\u9762\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u9886\u57df\u3002\u7531\u4e8e\u653b\u51fb\u8005\u901a\u5e38\u9762\u5bf9\u7684\u662f\u9ed1\u76d2\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u73b0\u5b9e\u7684\u6210\u5458\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPartCrop\u7684\u7edf\u4e00\u6210\u5458\u63a8\u7406\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6a21\u578b\u5171\u4eab\u7684\u90e8\u5206\u611f\u77e5\u80fd\u529b\u548c\u5bf9\u8bad\u7ec3\u6570\u636e\u66f4\u5f3a\u7684\u90e8\u5206\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u4e24\u79cd\u5e38\u89c1\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684PartCrop-v2\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86PartCrop\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6240\u6709\u9632\u5fa1\u65b9\u6cd5\u90fd\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6210\u5458\u63a8\u7406\u65b9\u6cd5PartCrop\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u8bad\u7ec3\u534f\u8bae\u548c\u7ed3\u6784\u7684\u81ea\u76d1\u7763\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u4e24\u79cd\u5e38\u89c1\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684PartCrop-v2\u65b9\u6cd5\u3002"}}
{"id": "2505.10120", "pdf": "https://arxiv.org/pdf/2505.10120", "abs": "https://arxiv.org/abs/2505.10120", "authors": ["Guillaume Godin"], "title": "All You Need Is Synthetic Task Augmentation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 3 Figures, 6 tables", "summary": "Injecting rule-based models like Random Forests into differentiable neural\nnetwork frameworks remains an open challenge in machine learning. Recent\nadvancements have demonstrated that pretrained models can generate efficient\nmolecular embeddings. However, these approaches often require extensive\npretraining and additional techniques, such as incorporating posterior\nprobabilities, to boost performance. In our study, we propose a novel strategy\nthat jointly trains a single Graph Transformer neural network on both sparse\nmultitask molecular property experimental targets and synthetic targets derived\nfrom XGBoost models trained on Osmordred molecular descriptors. These synthetic\ntasks serve as independent auxiliary tasks. Our results show consistent and\nsignificant performance improvement across all 19 molecular property prediction\ntasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms\nthe XGBoost single-task learner. This demonstrates that synthetic task\naugmentation is an effective method for enhancing neural model performance in\nmultitask molecular property prediction without the need for feature injection\nor pretraining.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u53d8\u6362\u5668\u795e\u7ecf\u7f51\u7edc\u548cXGBoost\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u591a\u4efb\u52a1\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u5c06\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u5982\u968f\u673a\u68ee\u6797\u6ce8\u5165\u53ef\u5fae\u5206\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u4ecd\u7136\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u8fdb\u5c55\u8868\u660e\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u9ad8\u6548\u7684\u5206\u5b50\u5d4c\u5165\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u9884\u8bad\u7ec3\u548c\u989d\u5916\u7684\u6280\u672f\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u5c06\u5355\u4e2a\u56fe\u53d8\u6362\u5668\u795e\u7ecf\u7f51\u7edc\u540c\u65f6\u8bad\u7ec3\u5728\u7a00\u758f\u591a\u4efb\u52a1\u5206\u5b50\u5c5e\u6027\u5b9e\u9a8c\u76ee\u6807\u548c\u4eceXGBoost\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u76ee\u6807\u4e0a\uff0c\u8fd9\u4e9b\u5408\u6210\u4efb\u52a1\u4f5c\u4e3a\u72ec\u7acb\u7684\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6240\u670919\u4e2a\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u90fd\u6709\u6301\u7eed\u4e14\u663e\u8457\u7684\u63d0\u5347\u3002\u572819\u4e2a\u76ee\u6807\u4e2d\u768416\u4e2a\u4e2d\uff0c\u591a\u4efb\u52a1\u56fe\u53d8\u6362\u5668\u4f18\u4e8eXGBoost\u5355\u4efb\u52a1\u5b66\u4e60\u8005\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5c55\u793a\u4e86\u5408\u6210\u4efb\u52a1\u589e\u5f3a\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u7279\u5f81\u6ce8\u5165\u6216\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u591a\u4efb\u52a1\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4e2d\u795e\u7ecf\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10352", "pdf": "https://arxiv.org/pdf/2505.10352", "abs": "https://arxiv.org/abs/2505.10352", "authors": ["Shihao Zou", "Qingfeng Li", "Wei Ji", "Jingjing Li", "Yongkui Yang", "Guoqi Li", "Chao Dong"], "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\\mathcal{O}(T)$ Complexity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u8109\u51b2\u7684\u89c6\u9891Transformer\uff08SpikeVideoFormer\uff09\uff0c\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u8109\u51b2\u7684\u6c49\u660e\u6ce8\u610f\u529b\uff08SDHA\uff09\uff0c\u5e76\u5728\u591a\u79cd\u89c6\u9891\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eSNN\u7684Transformer\u4e3b\u8981\u5173\u6ce8\u5355\u56fe\u50cf\u4efb\u52a1\uff0c\u800c\u672a\u80fd\u5145\u5206\u5229\u7528SNN\u5728\u89c6\u9891\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u4f18\u52bf\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u8109\u51b2\u7684\u89c6\u9891Transformer\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u7684\u6c49\u660e\u6ce8\u610f\u529b\uff08SDHA\uff09\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5206\u6790\u4e86\u5404\u79cd\u57fa\u4e8e\u8109\u51b2\u7684\u7a7a\u95f4-\u65f6\u95f4\u6ce8\u610f\u529b\u8bbe\u8ba1\uff0c\u4ee5\u627e\u5230\u4e00\u79cd\u6700\u4f18\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u89c6\u9891\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5206\u7c7b\u3001\u4eba\u4f53\u59ff\u6001\u8ddf\u8e2a\u548c\u8bed\u4e49\u5206\u5272\u3002\u6b64\u5916\uff0c\u5b83\u5728\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684SNN\u65b9\u6cd5\u548c\u6700\u8fd1\u7684ANN\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86SpikeVideoFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u8109\u51b2\u7684\u89c6\u9891Transformer\uff0c\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684SNN\u65b9\u6cd5\u548c\u6700\u8fd1\u7684ANN\u65b9\u6cd5\u3002"}}
{"id": "2505.10125", "pdf": "https://arxiv.org/pdf/2505.10125", "abs": "https://arxiv.org/abs/2505.10125", "authors": ["Wujun Zhou", "Shu Ding", "ZeLin Li", "Wei Wang"], "title": "Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning enables the clients to collaboratively train a global\nmodel, which is aggregated from local models. Due to the heterogeneous data\ndistributions over clients and data privacy in federated learning, it is\ndifficult to train local models to achieve a well-performed global model. In\nthis paper, we introduce the adaptability of local models, i.e., the average\nperformance of local models on data distributions over clients, and enhance the\nperformance of the global model by improving the adaptability of local models.\nSince each client does not know the data distributions over other clients, the\nadaptability of the local model cannot be directly optimized. First, we provide\nthe property of an appropriate local model which has good adaptability on the\ndata distributions over clients. Then, we formalize the property into the local\ntraining objective with a constraint and propose a feasible solution to train\nthe local model. Extensive experiments on federated learning benchmarks\ndemonstrate that our method significantly improves the adaptability of local\nmodels and achieves a well-performed global model that consistently outperforms\nthe baseline methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10420", "pdf": "https://arxiv.org/pdf/2505.10420", "abs": "https://arxiv.org/abs/2505.10420", "authors": ["Andrei Arhire", "Radu Timofte"], "title": "Learned Lightweight Smartphone ISP with Unpaired Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW 2025", "summary": "The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u914d\u5bf9\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u5b66\u4e60\u7684ISP\uff0c\u6d88\u9664\u4e86\u5bf9\u539f\u59cb\u56fe\u50cf\u548c\u5730\u9762\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\u7684\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5bf9\u6297\u8bad\u7ec3\u548c\u591a\u672f\u8bed\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u7ed3\u679c\u3002", "motivation": "\u5728\u5f00\u53d1\u53ef\u5b66\u4e60\u7684ISP\u65f6\uff0c\u83b7\u53d6\u50cf\u7d20\u5bf9\u9f50\u7684\u914d\u5bf9\u6570\u636e\u662f\u4e00\u4e2a\u56f0\u96be\u4e14\u6602\u8d35\u7684\u6b65\u9aa4\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u5b66\u4e60\u7684ISP\uff0c\u6d88\u9664\u4e86\u539f\u59cb\u56fe\u50cf\u548c\u5177\u6709\u5339\u914d\u5185\u5bb9\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\u3002\u6211\u4eec\u7684\u65e0\u914d\u5bf9\u65b9\u6cd5\u4f7f\u7528\u4e86\u7531\u5bf9\u6297\u8bad\u7ec3\u5f15\u5bfc\u7684\u591a\u672f\u8bed\u635f\u5931\u51fd\u6570\uff0c\u5176\u4e2d\u591a\u4e2a\u5224\u522b\u5668\u5904\u7406\u6765\u81ea\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u7279\u5f81\u56fe\uff0c\u4ee5\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\u7684\u540c\u65f6\u4ece\u76ee\u6807RGB\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u989c\u8272\u548c\u7eb9\u7406\u7279\u6027\u3002", "result": "\u6211\u4eec\u7684\u65e0\u914d\u5bf9\u5b66\u4e60\u7b56\u7565\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e76\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5c55\u793a\u4e86\u65e0\u914d\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u5f3a\u5927\u529b\u91cf\u3002"}}
{"id": "2505.10128", "pdf": "https://arxiv.org/pdf/2505.10128", "abs": "https://arxiv.org/abs/2505.10128", "authors": ["Huy Q. Le", "Latif U. Khan", "Choong Seon Hong"], "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": "IWCMC 2025", "summary": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6FedAPC\uff0c\u901a\u8fc7\u539f\u578b\u589e\u5f3a\u6765\u63d0\u9ad8\u5728\u9886\u57df\u5f02\u8d28\u6027\u4e0b\u7684\u5168\u5c40\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728Office-10\u548cDigits\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5141\u8bb8\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u534f\u4f5c\u8bad\u7ec3\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u9690\u79c1\uff0c\u4f7f\u5176\u6210\u4e3a\u9690\u79c1\u654f\u611f\u5e94\u7528\u7684\u6d41\u884c\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u7279\u522b\u662f\u9886\u57df\u5f02\u8d28\u6027\uff0cFL\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u8fd9\u963b\u788d\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6536\u655b\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86FedAPC\uff08\u8054\u90a6\u589e\u5f3a\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u7279\u5f81\u591a\u6837\u6027\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002FedAPC\u5229\u7528\u589e\u5f3a\u6570\u636e\u7684\u5e73\u5747\u7279\u5f81\u751f\u6210\u539f\u578b\uff0c\u4ee5\u6355\u83b7\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u3002\u901a\u8fc7\u5c06\u672c\u5730\u7279\u5f81\u4e0e\u5168\u5c40\u539f\u578b\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u4efb\u4f55\u7279\u5b9a\u9886\u57df\u7684\u8fc7\u62df\u5408\u3002", "result": "\u5728Office-10\u548cDigits\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728Office-10\u548cDigits\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10453", "pdf": "https://arxiv.org/pdf/2505.10453", "abs": "https://arxiv.org/abs/2505.10453", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "title": "Vision language models have difficulty recognizing virtual objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u865a\u62df\u7269\u4f53\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5176\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u573a\u666f\u4e2d\u865a\u62df\u7269\u4f53\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u5bf9\u573a\u666f\u7684\u638c\u63e1\u7a0b\u5ea6\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790\u5b83\u4eec\u5904\u7406\u865a\u62df\u7269\u4f53\u7684\u80fd\u529b\u3002", "result": "\u672c\u6587\u53d1\u73b0\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u865a\u62df\u7269\u4f53\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u865a\u62df\u7269\u4f53\u7684\u63cf\u8ff0\u53ef\u4ee5\u5e2e\u52a9\u6d4b\u8bd5AI\u7cfb\u7edf\u5bf9\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u865a\u62df\u7269\u4f53\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2505.10147", "pdf": "https://arxiv.org/pdf/2505.10147", "abs": "https://arxiv.org/abs/2505.10147", "authors": ["Yash", "Nikhil Karamchandani", "Avishek Ghosh"], "title": "Near Optimal Best Arm Identification for Clustered Bandits", "categories": ["cs.LG", "cs.MA"], "comment": "To be published in ICML 2025", "summary": "This work investigates the problem of best arm identification for multi-agent\nmulti-armed bandits. We consider $N$ agents grouped into $M$ clusters, where\neach cluster solves a stochastic bandit problem. The mapping between agents and\nbandits is a priori unknown. Each bandit is associated with $K$ arms, and the\ngoal is to identify the best arm for each agent under a $\\delta$-probably\ncorrect ($\\delta$-PC) framework, while minimizing sample complexity and\ncommunication overhead.\n  We propose two novel algorithms: Clustering then Best Arm Identification\n(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a\ntwo-phase approach that first clusters agents based on the bandit problems they\nare learning, followed by identifying the best arm for each cluster. BAI-Cl\nreverses the sequence by identifying the best arms first and then clustering\nagents accordingly. Both algorithms leverage the successive elimination\nframework to ensure computational efficiency and high accuracy.\n  We establish $\\delta$-PC guarantees for both methods, derive bounds on their\nsample complexity, and provide a lower bound for this problem class. Moreover,\nwhen $M$ is small (a constant), we show that the sample complexity of a variant\nof BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic\nand real-world datasets (MovieLens, Yelp) demonstrate the superior performance\nof the proposed algorithms in terms of sample and communication efficiency,\nparticularly in settings where $M \\ll N$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u6837\u672c\u590d\u6742\u6027\u548c\u901a\u4fe1\u5f00\u9500\u65b9\u9762\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u591a\u81c2\u8001\u864e\u673a\u4e2d\u6700\u4f73\u81c2\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6837\u672c\u590d\u6742\u6027\u548c\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5\uff1aClustering then Best Arm Identification (Cl-BAI) \u548c Best Arm Identification then Clustering (BAI-Cl)\u3002\u8fd9\u4e24\u79cd\u7b97\u6cd5\u90fd\u5229\u7528\u4e86\u8fde\u7eed\u6d88\u9664\u6846\u67b6\u4ee5\u786e\u4fdd\u8ba1\u7b97\u6548\u7387\u548c\u9ad8\u7cbe\u5ea6\u3002", "result": "\u672c\u6587\u5efa\u7acb\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u03b4-PC\u4fdd\u8bc1\uff0c\u63a8\u5bfc\u4e86\u5b83\u4eec\u7684\u6837\u672c\u590d\u6742\u6027\u754c\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be5\u95ee\u9898\u7c7b\u7684\u4e0b\u754c\u3002\u5f53M\u662f\u5c0f\u7684\uff08\u5e38\u6570\uff09\u65f6\uff0cBAI-Cl\u7684\u4e00\u4e2a\u53d8\u79cd\u5728\u987a\u5e8f\u610f\u4e49\u4e0a\u662f\u6700\u4f18\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6837\u672c\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5(Cl-BAI\u548cBAI-Cl)\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u5728\u6837\u672c\u590d\u6742\u6027\u548c\u901a\u4fe1\u5f00\u9500\u65b9\u9762\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222", "abs": "https://arxiv.org/abs/2505.10222", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.", "AI": {"tldr": "ComplexFormer is a new attention mechanism that improves the integration of semantic and positional information by using complex numbers and allows each head to learn distinct strategies for processing data.", "motivation": "The paper aims to address the challenges of integrating positional information effectively while allowing flexibility in multi-head attention mechanisms. Prior methods often model semantic and positional differences separately or apply uniform adjustments across heads, which may limit representational capacity.", "method": "ComplexFormer introduces Complex Multi-Head Attention (CMHA), which allows each head to independently model semantic and positional differences within the complex plane. It includes a per-head Euler transformation and a per-head adaptive differential rotation mechanism.", "result": "Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show that ComplexFormer achieves superior performance, significantly lower generation perplexity, and improved long-context coherence compared to strong baselines like RoPE-Transformers.", "conclusion": "ComplexFormer demonstrates strong parameter efficiency and offers a more expressive, adaptable attention mechanism compared to existing methods like RoPE-Transformers."}}
{"id": "2505.10473", "pdf": "https://arxiv.org/pdf/2505.10473", "abs": "https://arxiv.org/abs/2505.10473", "authors": ["Fengdi Zhang", "Hongkun Cao", "Ruqi Huang"], "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.", "AI": {"tldr": "ControlGS\u662f\u4e00\u79cd3DGS\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\uff0c\u5e76\u652f\u6301\u5e7f\u6cdb\u7684\u8c03\u6574\u8303\u56f4\u548c\u65e0\u7ea7\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8ba9\u7528\u6237\u76f4\u89c2\u8c03\u6574\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u7684\u80fd\u529b\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u786c\u4ef6\u548c\u901a\u4fe1\u7ea6\u675f\u3002", "method": "ControlGS\u662f\u4e00\u79cd3DGS\u4f18\u5316\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u8fd0\u884c\u4f7f\u7528\u56fa\u5b9a\u8bbe\u7f6e\u548c\u7528\u6237\u6307\u5b9a\u7684\u8d85\u53c2\u6570\u6765\u5b9e\u73b0\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u8de8\u573a\u666f\u4e00\u81f4\u7684\u6570\u91cf-\u8d28\u91cf\u63a7\u5236\u3002", "result": "ControlGS\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u9ad8\u65af\u6570\u91cf\uff0c\u5e76\u4e14\u652f\u6301\u5e7f\u6cdb\u7684\u8c03\u6574\u8303\u56f4\u548c\u65e0\u7ea7\u63a7\u5236\u3002", "conclusion": "ControlGS\u80fd\u591f\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u8fd0\u884c\u81ea\u52a8\u627e\u5230\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7406\u60f3\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u70b9\uff0c\u5e76\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\uff0c\u540c\u65f6\u652f\u6301\u5e7f\u6cdb\u7684\u8c03\u6574\u8303\u56f4\u548c\u65e0\u7ea7\u63a7\u5236\u3002"}}
{"id": "2505.10167", "pdf": "https://arxiv.org/pdf/2505.10167", "abs": "https://arxiv.org/abs/2505.10167", "authors": ["Saikat Barua", "Mostafizur Rahman", "Shehenaz Khaled", "Md Jafor Sadek", "Rafiul Islam", "Shahnewaz Siddique"], "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "16 pages, 6 figures, 7 equations", "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6QuXAI\uff0c\u57fa\u4e8eQ-MEDLEY\uff0c\u4ee5\u63d0\u9ad8\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\uff0cXAI\u5728\u91cf\u5b50\u7cfb\u7edf\u4e2d\u4ecd\u5904\u4e8e\u521d\u671f\u9636\u6bb5\uff0c\u7f3a\u4e4f\u9488\u5bf9\u91c7\u7528\u91cf\u5316\u7279\u5f81\u7f16\u7801\u5e76\u968f\u540e\u8fdb\u884c\u7ecf\u5178\u5b66\u4e60\u7684HQML\u67b6\u6784\u7684\u7a33\u5065\u5168\u5c40\u548c\u5c40\u90e8\u89e3\u91ca\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86QuXAI\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8eQ-MEDLEY\uff0c\u7528\u4e8e\u89e3\u91ca\u8fd9\u4e9b\u6df7\u5408\u7cfb\u7edf\u4e2d\u7684\u7279\u5f81\u91cd\u8981\u6027\u3002\u5b83\u6d89\u53ca\u6784\u5efa\u5305\u542b\u91cf\u5b50\u7279\u5f81\u6620\u5c04\u7684HQML\u6a21\u578b\uff0c\u5e76\u5229\u7528Q-MEDLEY\u7ed3\u5408\u57fa\u4e8e\u7279\u5f81\u7684\u63a8\u65ad\uff0c\u4fdd\u7559\u91cf\u5b50\u53d8\u6362\u9636\u6bb5\u5e76\u53ef\u89c6\u5316\u7ed3\u679c\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQ-MEDLEY\u80fd\u591f\u660e\u786eHQML\u6a21\u578b\u4e2d\u6709\u5f71\u54cd\u529b\u7684\u7ecf\u5178\u65b9\u9762\uff0c\u5e76\u533a\u5206\u5176\u566a\u58f0\uff0c\u5728\u7ecf\u5178\u9a8c\u8bc1\u8bbe\u7f6e\u4e2d\u4e0e\u73b0\u6709\u7684XAI\u6280\u672f\u7ade\u4e89\u826f\u597d\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86Q-MEDLEY\u4e2d\u590d\u5408\u7ed3\u6784\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86QuXAI\u6846\u67b6\uff0c\u57fa\u4e8eQ-MEDLEY\uff0c\u4ee5\u63d0\u9ad8\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08HQML\uff09\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u7684\u91cf\u5b50\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u4f7f\u7528\u3002"}}
{"id": "2505.10481", "pdf": "https://arxiv.org/pdf/2505.10481", "abs": "https://arxiv.org/abs/2505.10481", "authors": ["Ilya Ovodov", "Petr Surovtsev", "Karina Kvanchiani", "Alexander Kapitanov", "Alexander Nagaev"], "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Logos\uff0c\u4e00\u4e2a\u65b0\u578b\u7684\u4fc4\u8bed\u624b\u8bed\uff08RSL\uff09\u6570\u636e\u96c6\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684ISLR\u6570\u636e\u96c6\uff0c\u4e5f\u662f\u6700\u5927\u7684RSL\u6570\u636e\u96c6\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4f7f\u7528\u591a\u4e2a\u5206\u7c7b\u5934\u7684\u8054\u5408\u8bad\u7ec3\u6700\u80fd\u63d0\u9ad8\u76ee\u6807\u4f4e\u8d44\u6e90\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u660e\u786e\u6807\u8bb0\u89c6\u89c9\u76f8\u4f3c\u7684\u624b\u52bf\uff0c\u6211\u4eec\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u7684\u89c6\u89c9\u7f16\u7801\u5668\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\uff08ISLR\uff09\u4efb\u52a1\u7684\u4e24\u4e2a\u65b9\u9762\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u6709\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u4f46\u5927\u591a\u6570\u4e2a\u522b\u624b\u8bed\u7684\u6570\u636e\u91cf\u6709\u9650\uff0c\u8fd9\u7ed9\u8de8\u8bed\u8a00ISLR\u6a21\u578b\u8bad\u7ec3\u5e26\u6765\u4e86\u6311\u6218\uff0c\u5305\u62ec\u8fc1\u79fb\u5b66\u4e60\u3002\u5176\u6b21\uff0c\u76f8\u4f3c\u7684\u624b\u52bf\u53ef\u80fd\u6709\u4e0d\u540c\u7684\u8bed\u4e49\u542b\u4e49\uff0c\u8fd9\u5bfc\u81f4\u6570\u636e\u96c6\u6807\u6ce8\u7684\u6a21\u7cca\u6027\uff0c\u5e76\u5f15\u53d1\u5173\u4e8e\u5982\u4f55\u6807\u6ce8\u8fd9\u4e9b\u624b\u52bf\u7684\u6700\u4f73\u7b56\u7565\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Logos\uff0c\u4e00\u4e2a\u65b0\u578b\u7684\u4fc4\u8bed\u624b\u8bed\uff08RSL\uff09\u6570\u636e\u96c6\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684ISLR\u6570\u636e\u96c6\uff0c\u4e5f\u662f\u6700\u5927\u7684RSL\u6570\u636e\u96c6\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4f7f\u7528\u591a\u4e2a\u5206\u7c7b\u5934\u7684\u8054\u5408\u8bad\u7ec3\u6700\u80fd\u63d0\u9ad8\u76ee\u6807\u4f4e\u8d44\u6e90\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002Logos\u6570\u636e\u96c6\u7684\u5173\u952e\u7279\u70b9\u662f\u660e\u786e\u6807\u6ce8\u7684\u89c6\u89c9\u76f8\u4f3c\u7684\u624b\u52bf\u7ec4\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u660e\u786e\u6807\u8bb0\u89c6\u89c9\u76f8\u4f3c\u7684\u624b\u52bf\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u7684\u89c6\u89c9\u7f16\u7801\u5668\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9884\u8bad\u7ec3\u5728Logos\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u5176\u4ed6\u8bed\u8a00SLR\u4efb\u52a1\u7684\u901a\u7528\u7f16\u7801\u5668\uff0c\u5305\u62ec\u5c11\u6837\u672c\u5b66\u4e60\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u4f7f\u7528\u591a\u4e2a\u5206\u7c7b\u5934\u7684\u8054\u5408\u8bad\u7ec3\u6700\u80fd\u63d0\u9ad8\u76ee\u6807\u4f4e\u8d44\u6e90\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u660e\u786e\u6807\u8bb0\u89c6\u89c9\u76f8\u4f3c\u7684\u624b\u52bf\uff0c\u6211\u4eec\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u7684\u89c6\u89c9\u7f16\u7801\u5668\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u51fa\u7684\u8d21\u732e\uff0c\u6211\u4eec\u5728WLASL\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728AUTSL\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u4ec5\u4f7f\u7528\u5355\u6d41\u6a21\u578b\u5904\u7406\u7eafRGB\u89c6\u9891\u3002\u6e90\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u516c\u5f00\u7684\u3002"}}
{"id": "2505.10172", "pdf": "https://arxiv.org/pdf/2505.10172", "abs": "https://arxiv.org/abs/2505.10172", "authors": ["Zeyan Li", "Libing Chen", "Yin Tang"], "title": "Does Scaling Law Apply in Time Series Forecasting?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Rapid expansion of model size has emerged as a key challenge in time series\nforecasting. From early Transformer with tens of megabytes to recent\narchitectures like TimesNet with thousands of megabytes, performance gains have\noften come at the cost of exponentially increasing parameter counts. But is\nthis scaling truly necessary? To question the applicability of the scaling law\nin time series forecasting, we propose Alinear, an ultra-lightweight\nforecasting model that achieves competitive performance using only k-level\nparameters. We introduce a horizon-aware adaptive decomposition mechanism that\ndynamically rebalances component emphasis across different forecast lengths,\nalongside a progressive frequency attenuation strategy that achieves stable\nprediction in various forecasting horizons without incurring the computational\noverhead of attention mechanisms. Extensive experiments on seven benchmark\ndatasets demonstrate that Alinear consistently outperforms large-scale models\nwhile using less than 1% of their parameters, maintaining strong accuracy\nacross both short and ultra-long forecasting horizons. Moreover, to more fairly\nevaluate model efficiency, we propose a new parameter-aware evaluation metric\nthat highlights the superiority of ALinear under constrained model budgets. Our\nanalysis reveals that the relative importance of trend and seasonal components\nvaries depending on data characteristics rather than following a fixed pattern,\nvalidating the necessity of our adaptive design. This work challenges the\nprevailing belief that larger models are inherently better and suggests a\nparadigm shift toward more efficient time series modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578bAlinear\uff0c\u8be5\u6a21\u578b\u5728\u4f7f\u7528\u6781\u5c11\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u6311\u6218\u4e86\u5927\u578b\u6a21\u578b\u66f4\u597d\u7684\u4f20\u7edf\u89c2\u5ff5\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u5feb\u901f\u6269\u5c55\uff0c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u5173\u952e\u6311\u6218\u3002\u5c3d\u7ba1\u6027\u80fd\u63d0\u5347\u901a\u5e38\u4f34\u968f\u7740\u53c2\u6570\u6570\u91cf\u7684\u6307\u6570\u589e\u957f\uff0c\u4f46\u8fd9\u79cd\u6269\u5c55\u662f\u5426\u771f\u6b63\u5fc5\u8981\u4ecd\u503c\u5f97\u8d28\u7591\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Alinear\uff0c\u8fd9\u662f\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u9884\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528k\u7ea7\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e0e\u65f6\u95f4\u8303\u56f4\u76f8\u5173\u7684\u81ea\u9002\u5e94\u5206\u89e3\u673a\u5236\uff0c\u4ee5\u53ca\u4e00\u79cd\u6e10\u8fdb\u5f0f\u9891\u7387\u8870\u51cf\u7b56\u7565\uff0c\u4ee5\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u9884\u6d4b\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAlinear\u5728\u4f7f\u7528\u4e0d\u5230\u51761%\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u5e76\u5728\u77ed\u671f\u548c\u8d85\u957f\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u4fdd\u6301\u5f3a\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u66f4\u516c\u5e73\u5730\u8bc4\u4f30\u6a21\u578b\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u6311\u6218\u4e86\u5927\u578b\u6a21\u578b\u672c\u8d28\u4e0a\u66f4\u597d\u7684\u666e\u904d\u4fe1\u5ff5\uff0c\u5e76\u5efa\u8bae\u5411\u66f4\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2505.10483", "pdf": "https://arxiv.org/pdf/2505.10483", "abs": "https://arxiv.org/abs/2505.10483", "authors": ["Yi Li", "Haonan Wang", "Qixiang Zhang", "Boyu Xiao", "Chenchang Hu", "Hualiang Wang", "Xiaomeng Li"], "title": "UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric", "summary": "The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86UniEval\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u9996\u4e2a\u65e0\u9700\u989d\u5916\u6a21\u578b\u3001\u56fe\u50cf\u6216\u6ce8\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5UniBench\u548c\u76f8\u5e94\u7684UniScore\u5ea6\u91cf\u6807\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniBench\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0cUniScore\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u8d85\u8d8a\u4e86\u5f53\u524d\u6307\u6807\u3002\u6b64\u5916\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u548c\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u63ed\u793a\u4e86\u901a\u7528\u6a21\u578b\u7684\u72ec\u7279\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\uff0c\u5bfc\u81f4\u8bc4\u4f30\u8fc7\u7a0b\u590d\u6742\u4e14\u4e0d\u5168\u9762\u3002\u73b0\u6709\u7684\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u7279\u5b9a\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f46\u5b58\u5728\u8bb8\u591a\u9650\u5236\uff0c\u5982\u7f3a\u4e4f\u6574\u4f53\u7ed3\u679c\u3001\u989d\u5916\u8bc4\u4f30\u6a21\u578b\u7684\u9519\u8bef\u3001\u4f9d\u8d56\u5927\u91cf\u6807\u8bb0\u56fe\u50cf\u3001\u57fa\u51c6\u591a\u6837\u6027\u4e0d\u8db3\u4ee5\u53ca\u6307\u4ee4\u8ddf\u968f\u8bc4\u4f30\u7684\u5ea6\u91cf\u6807\u51c6\u6709\u9650\u3002", "method": "\u5f15\u5165\u4e86UniEval\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u6a21\u578b\u3001\u56fe\u50cf\u6216\u6ce8\u91ca\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5UniBench\u548c\u76f8\u5e94\u7684UniScore\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniBench\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0cUniScore\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u8d85\u8d8a\u4e86\u5f53\u524d\u6307\u6807\u3002\u6b64\u5916\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u548c\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u63ed\u793a\u4e86\u901a\u7528\u6a21\u578b\u7684\u72ec\u7279\u4ef7\u503c\u3002", "conclusion": "UniEval\u6846\u67b6\u7684\u5f15\u5165\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5316\u4e14\u7edf\u4e00\u7684\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eUniBench\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0cUniScore\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u8d85\u8d8a\u4e86\u5f53\u524d\u6307\u6807\u3002\u6b64\u5916\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u548c\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u63ed\u793a\u4e86\u901a\u7528\u6a21\u578b\u7684\u72ec\u7279\u4ef7\u503c\u3002"}}
{"id": "2505.10192", "pdf": "https://arxiv.org/pdf/2505.10192", "abs": "https://arxiv.org/abs/2505.10192", "authors": ["Prashant P. Shinde", "Priyadarshini P. Pai", "Shashishekar P. Adiga", "K. Subramanya Mayya", "Yongbeom Seo", "Myungsoo Hwang", "Heeyoung Go", "Changmin Park"], "title": "Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u68c0\u6d4b\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u7684\u5fae\u5c0f\u7f3a\u9677\u7684\u53ef\u80fd\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cYOLOv8\u5728\u68c0\u6d4b\u8f83\u5c0f\u7f3a\u9677\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8e\u7f3a\u9677\u6ce8\u91ca\u7684\u8d28\u91cf\u6570\u636e\u7f3a\u4e4f\u826f\u597d\u7684\u4ee3\u8868\u6027\uff0c\u7279\u522b\u662f\u66f4\u5c0f\u7684\u7f3a\u9677\uff0c\u8fd9\u963b\u788d\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7f3a\u9677\u68c0\u6d4b\u6a21\u578b\u5728\u5236\u9020\u7ebf\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u6211\u4eec\u4eba\u5de5\u751f\u6210\u5177\u6709\u5df2\u77e5\u7f3a\u9677\u5206\u5e03\u7684\u7ebf\u56fe\u6848\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\uff08SEM\uff09\u56fe\u50cf\uff0c\u5e76\u81ea\u4e3b\u6807\u6ce8\u5b83\u4eec\u3002\u7136\u540e\uff0c\u6211\u4eec\u91c7\u7528\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6765\u7814\u7a76\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\u4f5c\u4e3a\u7f3a\u9677\u5c3a\u5bf8\u7684\u51fd\u6570\uff0c\u6bd4\u8282\u8ddd\u5bbd\u5ea6\u5c0f\u5f97\u591a\u3002", "result": "YOLOv8\u5728\u68c0\u6d4b\u8f83\u5c0f\u7f3a\u9677\u65b9\u9762\u8868\u73b0\u51fa\u6700\u4f73\u7684\u5e73\u5747\u7cbe\u5ea6\uff0896%\uff09\uff0c\u800cEfficientNet\u4e3a83%\uff0cSSD\u4e3a77%\u3002\u5f53\u5728\u771f\u5b9eSEM\u6570\u636e\u4e0a\u6d4b\u8bd5\u65f6\uff0cYOLOv8\u6a21\u578b\u6b63\u786e\u68c0\u6d4b\u4e8684.6%\u7684\u6865\u63a5\u7f3a\u9677\u548c78.3%\u7684\u65ad\u88c2\u7f3a\u9677\u3002", "conclusion": "\u8fd9\u4e9b\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u53d1\u7a33\u5065\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u66ff\u4ee3\u54c1\u3002"}}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465", "abs": "https://arxiv.org/abs/2505.10465", "authors": ["Yizhou liu", "Ziming Liu", "Jeff Gore"], "title": "Superposition Yields Robust Neural Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u8868\u793a\u53e0\u52a0\u5bf9\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\uff0c\u4f5c\u8005\u53d1\u73b0\u5f53\u53e0\u52a0\u8f83\u5f31\u65f6\uff0c\u635f\u5931\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u7f29\u653e\u53d6\u51b3\u4e8e\u57fa\u7840\u7279\u5f81\u9891\u7387\uff1b\u800c\u5728\u5f3a\u53e0\u52a0\u4e0b\uff0c\u635f\u5931\u4e0e\u6a21\u578b\u7ef4\u5ea6\u6210\u53cd\u6bd4\u3002\u4ed6\u4eec\u8fd8\u5206\u6790\u4e86\u56db\u4e2a\u5f00\u6e90LLM\u5bb6\u65cf\uff0c\u5e76\u53d1\u73b0\u5b83\u4eec\u7b26\u5408\u6a21\u578b\u9884\u6d4b\u3002\u7ed3\u8bba\u662f\u8868\u793a\u53e0\u52a0\u662f\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u91cd\u8981\u673a\u5236\u3002", "motivation": "\u4e86\u89e3\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u8d77\u6e90\uff0c\u5373\u635f\u5931\u968f\u7740\u6a21\u578b\u5927\u5c0f\u5448\u5e42\u5f8b\u4e0b\u964d\u7684\u73b0\u8c61\u3002", "method": "\u4ece\u4e24\u4e2a\u7ecf\u9a8c\u539f\u7406\u51fa\u53d1\u2014\u2014LLM\u8868\u793a\u6bd4\u5b83\u4eec\u7684\u6a21\u578b\u7ef4\u5ea6\uff08\u5bbd\u5ea6\uff09\u66f4\u591a\u7684\u4e1c\u897f\uff08\u5373\u8868\u793a\u662f\u53e0\u52a0\u7684\uff09\uff0c\u4ee5\u53ca\u8bed\u8a00\u4e2d\u7684\u5355\u8bcd\u6216\u6982\u5ff5\u51fa\u73b0\u7684\u9891\u7387\u4e0d\u540c\u2014\u2014\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\u6765\u7814\u7a76\u635f\u5931\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u7f29\u653e\u5173\u7cfb\u3002", "result": "\u5f53\u53e0\u52a0\u8f83\u5f31\u65f6\uff0c\u635f\u5931\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u7f29\u653e\u53d6\u51b3\u4e8e\u57fa\u7840\u7279\u5f81\u9891\u7387\uff1b\u5982\u679c\u7279\u5f81\u9891\u7387\u9075\u5faa\u5e42\u5f8b\uff0c\u635f\u5931\u4e5f\u662f\u5982\u6b64\u3002\u5728\u5f3a\u53e0\u52a0\u4e0b\uff0c\u635f\u5931\u4e0e\u6a21\u578b\u7ef4\u5ea6\u6210\u53cd\u6bd4\u3002\u8fd9\u79cd\u7a33\u5065\u7684\u7f29\u653e\u884c\u4e3a\u88ab\u51e0\u4f55\u89e3\u91ca\uff1a\u5f53\u8bb8\u591a\u5411\u91cf\u88ab\u538b\u7f29\u5230\u8f83\u4f4e\u7ef4\u7a7a\u95f4\u65f6\uff0c\u5411\u91cf\u4e4b\u95f4\u7684\u5e72\u6270\uff08\u5e73\u65b9\u91cd\u53e0\uff09\u4e0e\u8be5\u7ef4\u5ea6\u6210\u53cd\u6bd4\u3002\u6211\u4eec\u5206\u6790\u4e86\u56db\u4e2a\u5f00\u6e90LLM\u5bb6\u65cf\uff0c\u5e76\u53d1\u73b0\u5b83\u4eec\u8868\u73b0\u51fa\u5f3a\u53e0\u52a0\uff0c\u5e76\u5b9a\u91cf\u5730\u7b26\u5408\u6211\u4eec\u7684\u73a9\u5177\u6a21\u578b\u9884\u6d4b\u3002", "conclusion": "\u8868\u793a\u8d85\u53e0\u52a0\u662f\u89c2\u5bdf\u5230\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u91cd\u8981\u673a\u5236\u3002\u6211\u4eec\u9884\u8ba1\u8fd9\u4e9b\u89c1\u89e3\u5c06\u6fc0\u53d1\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u548c\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10496", "pdf": "https://arxiv.org/pdf/2505.10496", "abs": "https://arxiv.org/abs/2505.10496", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CheXGenBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u5408\u6210\u80f8\u90e8X\u5149\u7247\u751f\u6210\u7684\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u4fdd\u771f\u5ea6\u3001\u9690\u79c1\u98ce\u9669\u548c\u4e34\u5e8a\u6548\u7528\u3002\u901a\u8fc7\u6807\u51c6\u5316\u7684\u6570\u636e\u5212\u5206\u548c\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u4f4e\u6548\u7387\uff0c\u5e76\u53d1\u5e03\u4e86SynthCheX-75K\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u533b\u5b66\u9886\u57df\u7684\u8bc4\u4f30\u53d7\u5230\u4e86\u65b9\u6cd5\u4e0d\u4e00\u81f4\u3001\u8fc7\u65f6\u7684\u67b6\u6784\u6bd4\u8f83\u548c\u8131\u79bb\u8bc4\u4f30\u6807\u51c6\u7684\u963b\u788d\uff0c\u8fd9\u4e9b\u6807\u51c6\u5f88\u5c11\u6d89\u53ca\u5408\u6210\u6837\u672c\u7684\u5b9e\u9645\u4e34\u5e8a\u4ef7\u503c\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CheXGenBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e25\u683c\u4e14\u591a\u65b9\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u80f8\u90e8X\u5149\u7247\u751f\u6210\uff0c\u540c\u65f6\u8bc4\u4f30\u4fdd\u771f\u5ea6\u3001\u9690\u79c1\u98ce\u9669\u548c\u4e34\u5e8a\u6548\u7528\u3002\u901a\u8fc7\u6807\u51c6\u5316\u7684\u6570\u636e\u5212\u5206\u548c\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u542b\u8d85\u8fc720\u4e2a\u5b9a\u91cf\u6307\u6807\uff0c\u7cfb\u7edf\u5730\u5206\u6790\u751f\u6210\u8d28\u91cf\u3001\u6f5c\u5728\u7684\u9690\u79c1\u6f0f\u6d1e\u4ee5\u53ca\u572811\u79cd\u9886\u5148\u6587\u672c\u5230\u56fe\u50cf\u67b6\u6784\u4e2d\u7684\u4e0b\u6e38\u4e34\u5e8a\u9002\u7528\u6027\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u4e2d\u7684\u5173\u952e\u4f4e\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u8bc4\u4f30\u751f\u6210\u4fdd\u771f\u5ea6\u65b9\u9762\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u548c\u65e0\u4fe1\u606f\u7684\u6bd4\u8f83\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u4e3a\u533b\u5b66AI\u793e\u533a\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u7684\u57fa\u51c6\uff0c\u4f7f\u5ba2\u89c2\u548c\u53ef\u91cd\u590d\u7684\u6bd4\u8f83\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u4fc3\u8fdb\u4e86\u73b0\u6709\u548c\u672a\u6765\u751f\u6210\u6a21\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u96c6SynthCheX-75K\uff0c\u4ee5\u652f\u6301\u8be5\u5173\u952e\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.10198", "pdf": "https://arxiv.org/pdf/2505.10198", "abs": "https://arxiv.org/abs/2505.10198", "authors": ["Mariano Ferrero", "Jos\u00e9 Omar Chelotti", "Luciano Sebasti\u00e1n Martinez-Rau", "Leandro Vignolo", "Mart\u00edn Pires", "Julio Ricardo Galli", "Leonardo Luis Giovanini", "Hugo Leonardo Rufiner"], "title": "A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals", "categories": ["cs.LG"], "comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence", "summary": "Monitoring feeding behaviour is a relevant task for efficient herd management\nand the effective use of available resources in grazing cattle. The ability to\nautomatically recognise animals' feeding activities through the identification\nof specific jaw movements allows for the improvement of diet formulation, as\nwell as early detection of metabolic problems and symptoms of animal\ndiscomfort, among other benefits. The use of sensors to obtain signals for such\nmonitoring has become popular in the last two decades. The most frequently\nemployed sensors include accelerometers, microphones, and cameras, each with\nits own set of advantages and drawbacks. An unexplored aspect is the\nsimultaneous use of multiple sensors with the aim of combining signals in order\nto enhance the precision of the estimations. In this direction, this work\nintroduces a deep neural network based on the fusion of acoustic and inertial\nsignals, composed of convolutional, recurrent, and dense layers. The main\nadvantage of this model is the combination of signals through the automatic\nextraction of features independently from each of them. The model has emerged\nfrom an exploration and comparison of different neural network architectures\nproposed in this work, which carry out information fusion at different levels.\nFeature-level fusion has outperformed data and decision-level fusion by at\nleast a 0.14 based on the F1-score metric. Moreover, a comparison with\nstate-of-the-art machine learning methods is presented, including traditional\nand deep learning approaches. The proposed model yielded an F1-score value of\n0.802, representing a 14% increase compared to previous methods. Finally,\nresults from an ablation study and post-training quantization evaluation are\nalso reported.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u548c\u60ef\u6027\u4fe1\u53f7\u878d\u5408\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u5976\u725b\u8fdb\u98df\u884c\u4e3a\u7684\u76d1\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u76d1\u6d4b\u5976\u725b\u7684\u8fdb\u98df\u884c\u4e3a\u5bf9\u4e8e\u9ad8\u6548\u7684\u7267\u573a\u7ba1\u7406\u548c\u8d44\u6e90\u5229\u7528\u81f3\u5173\u91cd\u8981\u3002\u81ea\u52a8\u8bc6\u522b\u52a8\u7269\u7684\u8fdb\u98df\u6d3b\u52a8\u53ef\u4ee5\u6539\u5584\u996e\u98df\u914d\u65b9\uff0c\u4ee5\u53ca\u65e9\u671f\u68c0\u6d4b\u4ee3\u8c22\u95ee\u9898\u548c\u52a8\u7269\u4e0d\u9002\u7b49\u75c7\u72b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u548c\u60ef\u6027\u4fe1\u53f7\u878d\u5408\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5305\u542b\u5377\u79ef\u3001\u5faa\u73af\u548c\u5bc6\u96c6\u5c42\u3002", "result": "\u7279\u5f81\u7ea7\u878d\u5408\u4f18\u4e8e\u6570\u636e\u7ea7\u548c\u51b3\u7b56\u7ea7\u878d\u5408\uff0cF1\u5206\u6570\u81f3\u5c11\u63d0\u9ad8\u4e860.14\u3002\u6b64\u5916\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u7684F1\u5206\u6570\u4e3a0.802\uff0c\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8614%\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u878d\u5408\u58f0\u5b66\u548c\u60ef\u6027\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u5bf9\u5976\u725b\u8fdb\u98df\u884c\u4e3a\u76d1\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728F1\u5206\u6570\u4e0a\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8614%\u3002"}}
{"id": "2505.10475", "pdf": "https://arxiv.org/pdf/2505.10475", "abs": "https://arxiv.org/abs/2505.10475", "authors": ["Mouxiang Chen", "Binyuan Hui", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Jianling Sun", "Junyang Lin", "Zhongxin Liu"], "title": "Parallel Scaling Law for Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.", "AI": {"tldr": "ParScale\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u5e76\u884c\u8ba1\u7b97\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u7684\u65b9\u6cd5\u5728\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u65f6\u9700\u8981\u4ed8\u51fa\u663e\u8457\u7684\u7a7a\u95f4\u6216\u65f6\u95f4\u6210\u672c\uff0c\u800cParScale\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6269\u5c55\u65b9\u5f0f\u3002", "method": "ParScale\u901a\u8fc7\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u589e\u52a0\u6a21\u578b\u7684\u5e76\u884c\u8ba1\u7b97\u6765\u5b9e\u73b0\u6269\u5c55\uff0c\u4f7f\u7528P\u79cd\u591a\u6837\u4e14\u53ef\u5b66\u4e60\u7684\u53d8\u6362\u5bf9\u8f93\u5165\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u52a8\u6001\u805a\u5408P\u4e2a\u8f93\u51fa\u3002", "result": "ParScale\u53ef\u4ee5\u4f7f\u7528\u66f4\u5c11\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u6765\u8fbe\u5230\u4e0e\u53c2\u6570\u6269\u5c55\u76f8\u540c\u7684\u6548\u679c\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u8fdb\u4e00\u6b65\u51cf\u5c11\u8bad\u7ec3\u9884\u7b97\u3002", "conclusion": "ParScale\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5e76\u4e14\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u90e8\u7f72\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2505.10497", "pdf": "https://arxiv.org/pdf/2505.10497", "abs": "https://arxiv.org/abs/2505.10497", "authors": ["Iurii Medvedev", "Nuno Goncalves"], "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53cc\u5206\u652f\u5206\u7c7b\u7b56\u7565\u6765\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5bf9\u6297\u4eba\u8138\u4f2a\u88c5\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5df2\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u4e14\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7684\u4eba\u8138\u8bc6\u522b\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4eba\u8138\u8bc6\u522b\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e5f\u589e\u52a0\u4e86\u5176\u66b4\u9732\u4e8e\u6f14\u793a\u653b\u51fb\uff08\u5305\u62ec\u4eba\u8138\u4f2a\u88c5\uff09\u7684\u98ce\u9669\uff0c\u8fd9\u4f1a\u5e26\u6765\u4e25\u91cd\u7684\u5b89\u5168\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u73b0\u4ee3\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5fc5\u987b\u5177\u5907\u5bf9\u6297\u8fd9\u4e9b\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53cc\u5206\u652f\u5206\u7c7b\u7b56\u7565\u6765\u4fee\u6539\u5206\u7c7b\u4efb\u52a1\uff0c\u4ece\u800c\u6709\u6548\u5904\u7406\u4eba\u8138\u4f2a\u88c5\u6807\u7b7e\u7684\u6a21\u7cca\u6027\u3002\u8fd9\u79cd\u9002\u5e94\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u4f2a\u88c5\u56fe\u50cf\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u5176\u533a\u5206\u4f2a\u88c5\u6837\u672c\u548c\u771f\u5b9e\u6837\u672c\u7684\u80fd\u529b\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u5bf9\u6297\u4eba\u8138\u4f2a\u88c5\u653b\u51fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5df2\u88ab\u9a8c\u8bc1\u5728\u516c\u5171\u57fa\u51c6\u4e0a\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u5bf9\u6297\u4eba\u8138\u4f2a\u88c5\u653b\u51fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5177\u6709\u666e\u904d\u9002\u7528\u6027\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7684\u4eba\u8138\u8bc6\u522b\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u57fa\u4e8e\u5206\u7c7b\u7684\u8bc6\u522b\u65b9\u6cd5\u3002"}}
{"id": "2505.10213", "pdf": "https://arxiv.org/pdf/2505.10213", "abs": "https://arxiv.org/abs/2505.10213", "authors": ["Mohammadmahdi Ghasemloo", "Alireza Moradi"], "title": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u5730\u5c06\u7ed3\u6784\u5316\u7684\u65f6\u95f4\u4fe1\u606f\u6ce8\u5165LLMs\uff0c\u4ee5\u63d0\u9ad8\u5176\u9884\u6d4b\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u77e5\u8bc6\u7684\u9884\u6d4b\u663e\u8457\u4f18\u4e8e\u65e0\u8f85\u52a9\u4fe1\u606f\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u5efa\u7acb\u6700\u4f73\u5b9e\u8df5\u6765\u8d85\u8d8a\u4f20\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u63d0\u9ad8LLMs\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u4ee5\u6ee1\u8db3\u80fd\u6e90\u7cfb\u7edf\u3001\u91d1\u878d\u548c\u533b\u7597\u7b49\u9886\u57df\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u5c06\u7ed3\u6784\u5316\u7684\u65f6\u95f4\u4fe1\u606f\u6ce8\u5165LLMs\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u77e5\u8bc6\u7684\u9884\u6d4b\u663e\u8457\u4f18\u4e8e\u65e0\u8f85\u52a9\u4fe1\u606f\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u77e5\u8bc6\u8fc1\u79fb\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u5f25\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7279\u5b9a\u9886\u57df\u9884\u6d4b\u4efb\u52a1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.10495", "pdf": "https://arxiv.org/pdf/2505.10495", "abs": "https://arxiv.org/abs/2505.10495", "authors": ["Vibha Belavadi", "Tushar Vatsa", "Dewang Sultania", "Suhas Suresha", "Ishita Verma", "Cheng Chen", "Tracy Holloway King", "Michael Friedrich"], "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing", "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u7531\u5668\u7684\u67b6\u6784\uff0c\u5229\u7528\u9886\u57df\u8d44\u6e90\u548c\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\uff0c\u4ee5\u89e3\u51b3LLM\u5728\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e2d\u56e0\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51fd\u6570\u5206\u7c7b\u548cAPI\u53c2\u6570\u9009\u62e9\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "motivation": "\u5728\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\u4e2d\uff0c\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8868\u8fbe\u9700\u6c42\uff0c\u8fd9\u4e9b\u67e5\u8be2\u5fc5\u987b\u6620\u5c04\u5230API\u8c03\u7528\u3002\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u7279\u5b9a\u7684\u6570\u636e\u4ee5\u53ca\u9690\u79c1\u9650\u5236\uff0c\u9700\u8981\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u590d\u5236\u771f\u5b9e\u4e16\u754c\u7684\u5206\u5e03\uff0c\u5bfc\u81f4\u5fae\u8c03\u540e\u7684\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u7531\u5668\u7684\u67b6\u6784\uff0c\u5229\u7528\u9886\u57df\u8d44\u6e90\uff08\u5982\u5185\u5bb9\u5143\u6570\u636e\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff09\u4ee5\u53ca\u6587\u672c\u5230\u6587\u672c\u548c\u89c6\u89c9\u5230\u6587\u672c\u7684\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u4e00\u7ec4\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u51fd\u6570\u5206\u7c7b\u51c6\u786e\u7387\u548cAPI\u53c2\u6570\u9009\u62e9\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002\u4f7f\u7528\u6211\u4eec\u7684\u5408\u6210\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u8868\u660e\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.10533", "pdf": "https://arxiv.org/pdf/2505.10533", "abs": "https://arxiv.org/abs/2505.10533", "authors": ["Aaryan Sharma", "Shivansh Gupta", "Samar Agarwal", "Vishak Prasad C.", "Ganesh Ramakrishnan"], "title": "Enhancing Multi-Image Question Answering via Submodular Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u6a21\u51fd\u6570\u9009\u62e9\u76f8\u5173\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u5728\u591a\u4e2a\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u591a\u4e2a\u56fe\u50cf\u7684\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u68c0\u7d22\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e86\u57fa\u4e8e\u5b50\u6a21\u51fd\u6570\u7684\u5b50\u96c6\u9009\u62e9\u6280\u672f\uff0c\u5982GraphCut\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u951a\u70b9\u7684\u67e5\u8be2\u548c\u6570\u636e\u589e\u5f3a\u6765\u6539\u8fdb\u68c0\u7d22\u6846\u67b6\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u91cf\u56fe\u50cf\u65f6\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u91cf\u56fe\u50cf\u65f6\u3002"}}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526", "abs": "https://arxiv.org/abs/2505.10526", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.", "AI": {"tldr": "MASSV is a method that accelerates vision-language models by transforming small language models into effective multimodal drafters through a two-phase approach, achieving significant speedups in inference.", "motivation": "Applying speculative decoding to vision-language models presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context.", "method": "MASSV transforms existing small language models into effective multimodal drafters through a two-phase approach. It first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions.", "result": "Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks.", "conclusion": "MASSV provides a scalable, architecture-compatible method for accelerating both current and future vision-language models (VLMs)."}}
{"id": "2505.10541", "pdf": "https://arxiv.org/pdf/2505.10541", "abs": "https://arxiv.org/abs/2505.10541", "authors": ["Pengfei Wang", "Guohai Xu", "Weinong Wang", "Junjie Yang", "Jie Lou", "Yunhua Xue"], "title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u548c\u6ce8\u610f\u529b\u51c6\u786e\u6027\u6307\u6807\uff0c\u4ee5\u8bc4\u4f30MLLMs\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5355\u6a21\u6001\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e86\u89c6\u89c9\u8f93\u5165\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5b9a\u4e49\u9690\u5f0f\u89c6\u89c9\u8bef\u89e3\uff08IVM\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u89e3\u8026\u56e0\u679c\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u7684\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u63ed\u793a\u4e86\u968f\u7740\u7f51\u7edc\u5c42\u6570\u52a0\u6df1\uff0c\u6ce8\u610f\u529b\u5206\u5e03\u9010\u6e10\u96c6\u4e2d\u5728\u4e0e\u6b63\u786e\u7b54\u6848\u76f8\u5173\u7684\u56fe\u50cf\u4e0a\u3002\u57fa\u4e8e\u6b64\uff0c\u5f15\u5165\u4e86\u6ce8\u610f\u529b\u51c6\u786e\u6027\u6307\u6807\u548c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u91cf\u5316IVM\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u51c6\u786e\u6027\u6307\u6807\u548c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u80fd\u591f\u76f4\u63a5\u901a\u8fc7\u5185\u90e8\u673a\u5236\u8bc4\u4f30\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u5355\u6a21\u6001\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u91cf\u5316MLLMs\u4e2d\u9690\u5f0f\u89c6\u89c9\u8bef\u89e3\uff08IVM\uff09\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u51c6\u786e\u6027\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5355\u6a21\u6001\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.10259", "pdf": "https://arxiv.org/pdf/2505.10259", "abs": "https://arxiv.org/abs/2505.10259", "authors": ["Xiangwen Zhuge", "Xu Shen", "Zeyu Wang", "Fan Dang", "Xuan Ding", "Danyang Li", "Yahui Han", "Tianxiang Hao", "Zheng Yang"], "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices", "categories": ["cs.LG"], "comment": null, "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 SpecOffload\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u901a\u8fc7\u5c06\u63a8\u6d4b\u89e3\u7801\u5d4c\u5165\u5230\u5378\u8f7d\u8fc7\u7a0b\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 GPU \u5229\u7528\u7387\u548c\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728 GPU \u5185\u5b58\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5c06\u6a21\u578b\u6743\u91cd\u5378\u8f7d\u5230 CPU \u5185\u5b58\uff0c\u5bfc\u81f4 CPU \u548c GPU \u4e4b\u95f4\u7684 I/O \u5f00\u9500\u5927\uff0cGPU \u6838\u5fc3\u5229\u7528\u7387\u4f4e\uff0cGPU \u5185\u5b58\u5bf9\u6027\u80fd\u5f71\u54cd\u5c0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a SpecOffload \u7684\u9ad8\u541e\u5410\u63a8\u7406\u5f15\u64ce\uff0c\u5c06\u63a8\u6d4b\u89e3\u7801\u5d4c\u5165\u5230\u5378\u8f7d\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5229\u7528 GPU \u8d44\u6e90\u5b58\u50a8\u548c\u6267\u884c\u8349\u7a3f\u6a21\u578b\u6765\u52a0\u901f\u63a8\u7406\u3002", "result": "\u4e0e\u6700\u4f73\u57fa\u7ebf\u76f8\u6bd4\uff0cSpecOffload \u63d0\u9ad8\u4e86 GPU \u6838\u5fc3\u5229\u7528\u7387 4.49 \u500d\uff0c\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u541e\u5410\u91cf 2.54 \u500d\u3002", "conclusion": "SpecOffload \u63d0\u9ad8\u4e86 GPU \u6838\u5fc3\u5229\u7528\u7387\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u541e\u5410\u91cf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.10551", "pdf": "https://arxiv.org/pdf/2505.10551", "abs": "https://arxiv.org/abs/2505.10551", "authors": ["Yiwen Liu", "Jessica Bader", "Jae Myung Kim"], "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025", "summary": "With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u884c\u6027\u5728\u751f\u6210\u7528\u4e8eCLIP\u57fa\u7840\u5206\u7c7b\u5668\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u65f6\u662f\u5426\u5fc5\u8981\uff0c\u5e76\u53d1\u73b0\u5176\u5f71\u54cd\u5f88\u5c0f\u3002", "motivation": "\u7814\u7a76\u53ef\u884c\u6027\u662f\u5426\u5728\u751f\u6210\u7528\u4e8eCLIP\u57fa\u7840\u5206\u7c7b\u5668\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u65f6\u662f\u5fc5\u8981\u7684\uff0c\u7279\u522b\u662f\u9488\u5bf9\u80cc\u666f\u3001\u989c\u8272\u548c\u7eb9\u7406\u8fd9\u4e09\u4e2a\u76ee\u6807\u5c5e\u6027\u3002", "method": "\u5f15\u5165\u4e86VariReal\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u6700\u5c0f\u5730\u7f16\u8f91\u7ed9\u5b9a\u7684\u6e90\u56fe\u50cf\u4ee5\u5305\u542b\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u63d0\u793a\u7ed9\u51fa\u7684\u53ef\u884c\u6216\u4e0d\u53ef\u884c\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u53ef\u884c\u6027\u5bf9LoRA\u5fae\u8c03\u7684CLIP\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\uff0c\u4e14\u5728\u4e09\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684top-1\u51c6\u786e\u7387\u5dee\u5f02\u5c0f\u4e8e0.3%\u3002\u6b64\u5916\uff0c\u5c5e\u6027\u5bf9\u53ef\u884c/\u4e0d\u53ef\u884c\u56fe\u50cf\u662f\u5426\u5bf9\u6297\u6027\u5730\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u6709\u5f71\u54cd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u884c\u6027\u5bf9LoRA\u5fae\u8c03\u7684CLIP\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\uff0c\u4e14\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u6df7\u5408\u53ef\u884c\u548c\u4e0d\u53ef\u884c\u56fe\u50cf\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002"}}
{"id": "2505.10262", "pdf": "https://arxiv.org/pdf/2505.10262", "abs": "https://arxiv.org/abs/2505.10262", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Lajos Hanzo"], "title": "Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The charging scheduling problem of Electric Buses (EBs) is investigated based\non Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is\nconceived, where the time horizon includes multiple charging and operating\nperiods in a day, while each period is further divided into multiple time\nsteps. To overcome the challenge of long-range multi-phase planning with sparse\nreward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP\ninto a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical\nDouble Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is\nproposed for simultaneously solving the decision problems arising at different\ntemporal resolutions. As a result, the high-level agent learns an effective\npolicy for prescribing the charging targets for every charging period, while\nthe low-level agent learns an optimal policy for setting the charging power of\nevery time step within a single charging period, with the aim of minimizing the\ncharging costs while meeting the charging target. It is proved that the flat\npolicy constructed by superimposing the optimal high-level policy and the\noptimal low-level policy performs as well as the optimal policy of the original\nMDP. Since jointly learning both levels of policies is challenging due to the\nnon-stationarity of the high-level agent and the sampling inefficiency of the\nlow-level agent, we divide the joint learning process into two phases and\nexploit our new HER algorithm to manipulate the experience replay buffers for\nboth levels of agents. Numerical experiments are performed with the aid of\nreal-world data to evaluate the performance of the proposed algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7535\u52a8\u516c\u4ea4\u8f66\u5145\u7535\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u548c\u65b0\u7684\u7b97\u6cd5\u4f18\u5316\u5145\u7535\u7b56\u7565\uff0c\u4ee5\u964d\u4f4e\u5145\u7535\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u957f\u8ddd\u79bb\u591a\u9636\u6bb5\u89c4\u5212\u4e2d\u7a00\u758f\u5956\u52b1\u7684\u6311\u6218\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5145\u7535\u6210\u672c\u5e76\u6ee1\u8db3\u5145\u7535\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08HDRL\uff09\u65b9\u6cd5\uff0c\u5c06\u539f\u59cbMDP\u5206\u89e3\u4e3a\u4e00\u4e2a\u9ad8\u5c42\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08SMDP\uff09\u548c\u591a\u4e2a\u4f4e\u5c42MDP\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u53cc\u6df1\u5ea6Q\u7f51\u7edc\uff08HDDQN\uff09-\u4e8b\u540e\u7ecf\u9a8c\u56de\u653e\uff08HER\uff09\u7b97\u6cd5\u6765\u89e3\u51b3\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u7684\u51b3\u7b56\u95ee\u9898\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9ad8\u5c42\u7b56\u7565\u548c\u4f4e\u5c42\u7b56\u7565\u7684\u6700\u4f18\u7b56\u7565\u53e0\u52a0\uff0c\u6784\u5efa\u7684\u5e73\u5766\u7b56\u7565\u5728\u6027\u80fd\u4e0a\u4e0e\u539f\u59cbMDP\u7684\u6700\u4f18\u7b56\u7565\u76f8\u5f53\u3002"}}
{"id": "2505.10557", "pdf": "https://arxiv.org/pdf/2505.10557", "abs": "https://arxiv.org/abs/2505.10557", "authors": ["Ke Wang", "Junting Pan", "Linda Wei", "Aojun Zhou", "Weikang Shi", "Zimu Lu", "Han Xiao", "Yunqiao Yang", "Houxing Ren", "Mingjie Zhan", "Hongsheng Li"], "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u4ee3\u7801\u4f5c\u4e3a\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u76d1\u7763\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5171\u540c\u5f00\u53d1\u56fe\u50cf\u5230\u4ee3\u7801\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u6700\u5927\u7684\u56fe\u50cf-\u4ee3\u7801\u6570\u636e\u96c6ImgCode-8.6M\uff0c\u5e76\u5229\u7528\u8be5\u6a21\u578b\u5408\u6210\u65b0\u7684\u6570\u5b66\u56fe\u5f62\uff0c\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u5b66\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6MM-MathInstruct-3M\u3002\u6700\u7ec8\u63d0\u51fa\u7684MathCoder-VL\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u6570\u636e\u96c6\u4e3b\u8981\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u6570\u5b66\u56fe\u5f62\u4e2d\u5bf9\u95ee\u9898\u89e3\u51b3\u81f3\u5173\u91cd\u8981\u7684\u590d\u6742\u7ec6\u8282\uff0c\u963b\u788d\u4e86\u5f53\u524dLMMs\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u5c55\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u5229\u7528\u4ee3\u7801\u4f5c\u4e3a\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u76d1\u7763\uff0c\u56e0\u4e3a\u4ee3\u7801\u672c\u8d28\u4e0a\u5305\u542b\u4e86\u751f\u6210\u76f8\u5e94\u56fe\u5f62\u6240\u9700\u7684\u6240\u6709\u4fe1\u606f\uff0c\u5efa\u7acb\u4e86\u4e24\u79cd\u6a21\u6001\u4e4b\u95f4\u7684\u7cbe\u786e\u8054\u7cfb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u6a21\u578b\u5728\u73af\u7684\u65b9\u6cd5\u5171\u540c\u5f00\u53d1\u56fe\u50cf\u5230\u4ee3\u7801\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u5f97\u5230\u4e86\u4e00\u4e2a\u56fe\u50cf\u5230\u4ee3\u7801\u6a21\u578bFigCodifier\u548cImgCode-8.6M\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5927\u7684\u56fe\u50cf-\u4ee3\u7801\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528FigCodifier\u5408\u6210\u65b0\u7684\u6570\u5b66\u56fe\u5f62\uff0c\u7136\u540e\u6784\u5efaMM-MathInstruct-3M\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u5b66\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MathCoder-VL\uff0c\u5728ImgCode-8.6M\u4e0a\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\u8bad\u7ec3\uff0c\u5e76\u5728MM-MathInstruct-3M\u4e0a\u8fdb\u884c\u591a\u6a21\u6001\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u7684\u5fae\u8c03\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u6240\u6709\u516d\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u5f00\u6e90SOTA\uff0c\u5e76\u5728MathVista\u7684\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u5b50\u96c6\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u548cClaude 3.5 Sonnet\uff0c\u5206\u522b\u63d0\u9ad8\u4e868.9%\u548c9.2%\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u6240\u6709\u516d\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u5f00\u6e90SOTA\uff0c\u5e76\u5728MathVista\u7684\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u5b50\u96c6\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u548cClaude 3.5 Sonnet\uff0c\u5206\u522b\u63d0\u9ad8\u4e868.9%\u548c9.2%\u3002\u6570\u636e\u96c6\u548c\u6a21\u578b\u5c06\u5728https://github.com/mathllm/MathCoder\u4e0a\u53d1\u5e03\u3002"}}
{"id": "2505.10264", "pdf": "https://arxiv.org/pdf/2505.10264", "abs": "https://arxiv.org/abs/2505.10264", "authors": ["Francesco Diana", "Andr\u00e9 Nusser", "Chuan Xu", "Giovanni Neglia"], "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u91cd\u8981\u9650\u5236\uff0c\u4f8b\u5982\u4f9d\u8d56\u4e8e\u5bf9\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u7684\u5047\u8bbe\uff0c\u6216\u8005\u5f53\u6279\u91cf\u5927\u5c0f\u8d85\u8fc7\u51e0\u5341\u4e2a\u6837\u672c\u65f6\u6548\u7387\u663e\u8457\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u8fde\u63a5\u5c42\u7684\u65b0\u51e0\u4f55\u89c6\u89d2\u7684\u65b0\u578b\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u9020\u6076\u610f\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u5927\u89c4\u6a21\u6570\u636e\u6279\u6b21\u7684\u5b8c\u7f8e\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u5728\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u6bd4\u73b0\u6709\u6280\u672f\u5927\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u6570\u636e\u6279\u6b21\u7684\u5b8c\u7f8e\u91cd\u5efa\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.10562", "pdf": "https://arxiv.org/pdf/2505.10562", "abs": "https://arxiv.org/abs/2505.10562", "authors": ["Wenxuan Wang", "Fan Zhang", "Yufeng Cui", "Haiwen Diao", "Zhuoyan Luo", "Huchuan Lu", "Jing Liu", "Xinlong Wang"], "title": "End-to-End Vision Tokenizer Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u6807\u8bb0\u4f18\u5316\u65b9\u6cd5\uff08ETT\uff09\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5316\u548c\u76ee\u6807\u81ea\u56de\u5f52\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u89c6\u89c9\u6807\u8bb0\u5668\u7684\u4f18\u5316\u4e0e\u4e0b\u6e38\u8bad\u7ec3\u5206\u79bb\uff0c\u9690\u542b\u5730\u5047\u8bbe\u89c6\u89c9\u6807\u8bb0\u53ef\u4ee5\u8de8\u5404\u79cd\u4efb\u52a1\u6cdb\u5316\uff0c\u4f8b\u5982\u56fe\u50cf\u751f\u6210\u548c\u89c6\u89c9\u95ee\u7b54\u3002\u7136\u800c\uff0c\u4e3a\u4f4e\u7ea7\u91cd\u5efa\u4f18\u5316\u7684\u89c6\u89c9\u6807\u8bb0\u5668\u5bf9\u4e8e\u9700\u8981\u4e0d\u540c\u8868\u793a\u548c\u8bed\u4e49\u7684\u4e0b\u6e38\u4efb\u52a1\u662f\u65e0\u5173\u7684\u3002\u8fd9\u79cd\u89e3\u8026\u8303\u5f0f\u5f15\u5165\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u4e0d\u5339\u914d\uff1a\u89c6\u89c9\u6807\u8bb0\u5316\u7684\u635f\u5931\u53ef\u80fd\u6210\u4e3a\u76ee\u6807\u4efb\u52a1\u7684\u8868\u793a\u74f6\u9888\u3002", "method": "ETT\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u6807\u8bb0\u4f18\u5316\u65b9\u6cd5\uff0c\u5b83\u5b9e\u73b0\u4e86\u89c6\u89c9\u6807\u8bb0\u5316\u548c\u76ee\u6807\u81ea\u56de\u5f52\u4efb\u52a1\u4e4b\u95f4\u7684\u8054\u5408\u4f18\u5316\u3002\u4e0e\u4e4b\u524d\u4ec5\u4f7f\u7528\u51bb\u7ed3\u89c6\u89c9\u6807\u8bb0\u5668\u7684\u79bb\u6563\u7d22\u5f15\u7684\u81ea\u56de\u5f52\u6a21\u578b\u4e0d\u540c\uff0cETT\u5229\u7528\u4e86\u89c6\u89c9\u6807\u8bb0\u5668\u4ee3\u7801\u672c\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u91cd\u5efa\u548c\u6807\u9898\u76ee\u6807\u5bf9\u89c6\u89c9\u6807\u8bb0\u5668\u8fdb\u884c\u4e86\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u6807\u8bb0\u4f18\u5316\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u76f8\u6bd4\u51bb\u7ed3\u6807\u8bb0\u5668\u57fa\u7ebf\u63d0\u9ad8\u4e862-6%\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u7684\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "ETT\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u7684\u91cd\u5efa\u80fd\u529b\u3002\u8fd9\u79cd\u65b9\u6cd5\u7b80\u5355\u4e14\u5f3a\u5927\uff0c\u53ef\u4ee5\u589e\u5f3a\u9664\u4e86\u56fe\u50cf\u751f\u6210\u548c\u7406\u89e3\u4e4b\u5916\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2505.10271", "pdf": "https://arxiv.org/pdf/2505.10271", "abs": "https://arxiv.org/abs/2505.10271", "authors": ["Rafael Pablos Sarabia", "Joachim Nyborg", "Morten Birk", "Jeppe Liborius Sj\u00f8rup", "Anders Lillevang Vesterholt", "Ira Assent"], "title": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u6b27\u6d32\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u6982\u7387\u964d\u6c34\u9884\u6d4b\uff0c\u8be5\u6a21\u578b\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u6e90\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "motivation": "\u514b\u670d\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u77ed\u9884\u62a5\u63d0\u524d\u65f6\u95f4\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u6b27\u6d328\u5c0f\u65f6\u65f6\u95f4\u8303\u56f4\u5185\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u6982\u7387\u964d\u6c34\u9884\u6d4b\uff0c\u8be5\u6a21\u578b\u6709\u6548\u5730\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u6e90\uff08\u5305\u62ec\u96f7\u8fbe\u3001\u536b\u661f\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\uff09\uff0c\u540c\u65f6\u6355\u6349\u957f\u8ddd\u79bb\u76f8\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u4e00\u81f4\u7684\u6982\u7387\u56fe\u5b9e\u73b0\u51c6\u786e\u7684\u9884\u6d4b\u548c\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u5f53\u524d\u7684\u64cd\u4f5c\u6027NWP\u7cfb\u7edf\u3001\u57fa\u4e8e\u5916\u63a8\u7684\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u73b0\u5728\u9884\u62a5\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u6b27\u6d32\u9ad8\u5206\u8fa8\u7387\u964d\u6c34\u9884\u62a5\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u786e\u4fdd\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u3002"}}
{"id": "2505.10565", "pdf": "https://arxiv.org/pdf/2505.10565", "abs": "https://arxiv.org/abs/2505.10565", "authors": ["Zehan Wang", "Siyu Chen", "Lihe Yang", "Jialei Wang", "Ziang Zhang", "Hengshuang Zhao", "Zhou Zhao"], "title": "Depth Anything with Any Prior", "categories": ["cs.CV"], "comment": "Home page: https://prior-depth-anything.github.io/", "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u6d4b\u91cf\u548c\u6df1\u5ea6\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u751f\u6210\u51c6\u786e\u3001\u5bc6\u96c6\u4e14\u8be6\u7ec6\u7684\u5ea6\u91cf\u6df1\u5ea6\u56fe\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u6d41\u7a0b\u6574\u5408\u4e24\u79cd\u4e92\u8865\u7684\u6df1\u5ea6\u6e90\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u4e0d\u5b8c\u6574\u4f46\u7cbe\u786e\u7684\u5ea6\u91cf\u4fe1\u606f\uff0c\u800c\u6df1\u5ea6\u9884\u6d4b\u65b9\u6cd5\u5219\u63d0\u4f9b\u76f8\u5bf9\u4f46\u5b8c\u6574\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6846\u67b6\u6765\u7ed3\u5408\u8fd9\u4e24\u79cd\u4e92\u8865\u7684\u6df1\u5ea6\u6e90\uff0c\u751f\u6210\u51c6\u786e\u3001\u5bc6\u96c6\u4e14\u8be6\u7ec6\u7684\u5ea6\u91cf\u6df1\u5ea6\u56fe\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ece\u7c97\u5230\u7ec6\u7684\u6d41\u7a0b\uff0c\u9010\u6b65\u6574\u5408\u4e24\u79cd\u4e92\u8865\u7684\u6df1\u5ea6\u6e90\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u50cf\u7d20\u7ea7\u5ea6\u91cf\u5bf9\u9f50\u548c\u8ddd\u79bb\u611f\u77e5\u52a0\u6743\uff0c\u901a\u8fc7\u663e\u5f0f\u4f7f\u7528\u6df1\u5ea6\u9884\u6d4b\u6765\u9884\u586b\u5145\u591a\u6837\u5316\u7684\u5ea6\u91cf\u5148\u9a8c\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u6a21\u578b\u6765\u7ec6\u5316\u6df1\u5ea6\u5148\u9a8c\u7684\u56fa\u6709\u566a\u58f0\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u57287\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u60ca\u4eba\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6df1\u5ea6\u8865\u5168\u3001\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u4efb\u52a1\u4e2d\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6df1\u5ea6\u8865\u5168\u3001\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u672a\u89c1\u8fc7\u7684\u6df7\u5408\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u5207\u6362\u9884\u6d4b\u6a21\u578b\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u7684\u6539\u8fdb\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002"}}
{"id": "2505.10272", "pdf": "https://arxiv.org/pdf/2505.10272", "abs": "https://arxiv.org/abs/2505.10272", "authors": ["Niklas Dexheimer", "Sascha Gaudlitz", "Johannes Schmidt-Hieber"], "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Hebbian learning is a key principle underlying learning in biological neural\nnetworks. It postulates that synaptic changes occur locally, depending on the\nactivities of pre- and postsynaptic neurons. While Hebbian learning based on\nneuronal firing rates is well explored, much less is known about learning rules\nthat account for precise spike-timing. We relate a Hebbian\nspike-timing-dependent plasticity rule to noisy gradient descent with respect\nto a natural loss function on the probability simplex. This connection allows\nus to prove that the learning rule eventually identifies the presynaptic neuron\nwith the highest activity. We also discover an intrinsic connection to noisy\nmirror descent.", "AI": {"tldr": "\u672c\u6587\u5c06Hebbian\u7684\u5c16\u5cf0\u65f6\u95f4\u4f9d\u8d56\u53ef\u5851\u6027\u89c4\u5219\u4e0e\u566a\u58f0\u68af\u5ea6\u4e0b\u964d\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u4e86\u8be5\u89c4\u5219\u80fd\u591f\u8bc6\u522b\u51fa\u6d3b\u52a8\u6700\u9ad8\u7684\u524d\u7a81\u89e6\u795e\u7ecf\u5143\uff0c\u5e76\u53d1\u73b0\u5176\u4e0e\u566a\u58f0\u955c\u9762\u4e0b\u964d\u6709\u5185\u5728\u8054\u7cfb\u3002", "motivation": "\u4e86\u89e3\u57fa\u4e8e\u7cbe\u786e\u5c16\u5cf0\u65f6\u95f4\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u57fa\u4e8e\u795e\u7ecf\u5143\u653e\u7535\u7387\u7684Hebbian\u5b66\u4e60\u3002", "method": "\u5c06Hebbian\u7684\u5c16\u5cf0\u65f6\u95f4\u4f9d\u8d56\u53ef\u5851\u6027\u89c4\u5219\u4e0e\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u7684\u81ea\u7136\u635f\u5931\u51fd\u6570\u7684\u566a\u58f0\u68af\u5ea6\u4e0b\u964d\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u8be5\u5b66\u4e60\u89c4\u5219\u6700\u7ec8\u80fd\u591f\u8bc6\u522b\u51fa\u6d3b\u52a8\u6700\u9ad8\u7684\u524d\u7a81\u89e6\u795e\u7ecf\u5143\uff0c\u5e76\u4e14\u4e0e\u566a\u58f0\u955c\u9762\u4e0b\u964d\u6709\u5185\u5728\u8054\u7cfb\u3002", "conclusion": "\u8be5\u5b66\u4e60\u89c4\u5219\u6700\u7ec8\u80fd\u591f\u8bc6\u522b\u51fa\u6d3b\u52a8\u6700\u9ad8\u7684\u524d\u7a81\u89e6\u795e\u7ecf\u5143\u3002"}}
{"id": "2505.10566", "pdf": "https://arxiv.org/pdf/2505.10566", "abs": "https://arxiv.org/abs/2505.10566", "authors": ["Yen-Chi Cheng", "Krishna Kumar Singh", "Jae Shin Yoon", "Alex Schwing", "Liangyan Gui", "Matheus Gadelha", "Paul Guerrero", "Nanxuan Zhao"], "title": "3D-Fixup: Advancing Photo Editing with 3D Priors", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. Project page: https://3dfixup.github.io/", "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3D-Fixup\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5b66\u4e60\u76843D\u5148\u9a8c\u77e5\u8bc6\u6307\u5bfc2D\u56fe\u50cf\u7f16\u8f91\u3002\u8be5\u6846\u67b6\u652f\u6301\u56f0\u96be\u7684\u7f16\u8f91\u60c5\u51b5\uff0c\u5982\u5bf9\u8c61\u5e73\u79fb\u548c3D\u65cb\u8f6c\u3002\u901a\u8fc7\u6574\u54083D\u5148\u9a8c\u77e5\u8bc6\uff0c3D-Fixup\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u5e76\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u73b0\u5b9e\u56fe\u50cf\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5efa\u6a21\u56fe\u50cf\u5148\u9a8c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f463D\u611f\u77e5\u56fe\u50cf\u7f16\u8f91\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5bf9\u8c61\u4ec5\u901a\u8fc7\u5355\u4e2a\u56fe\u50cf\u6307\u5b9a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e863D-Fixup\uff0c\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5b66\u4e60\u76843D\u5148\u9a8c\u77e5\u8bc6\u6307\u5bfc2D\u56fe\u50cf\u7f16\u8f91\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e863D-Fixup\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5b66\u4e60\u76843D\u5148\u9a8c\u77e5\u8bc6\u6307\u5bfc2D\u56fe\u50cf\u7f16\u8f91\u3002\u6211\u4eec\u5229\u7528\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002\u6211\u4eec\u8f6c\u5411\u89c6\u9891\u6570\u636e\u4ee5\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5bf9\uff0c\u5373\u6e90\u5e27\u548c\u76ee\u6807\u5e27\u3002\u6211\u4eec\u7ed3\u5408\u6765\u81eaImage-to-3D\u6a21\u578b\u76843D\u6307\u5bfc\uff0c\u901a\u8fc7\u5c062D\u4fe1\u606f\u663e\u5f0f\u6295\u5f71\u52303D\u7a7a\u95f4\u6765\u89e3\u51b3\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4ee5\u786e\u4fdd\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u9ad8\u8d28\u91cf\u76843D\u6307\u5bfc\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b3D\u5148\u9a8c\u77e5\u8bc6\uff0c3D-Fixup\u6709\u6548\u5730\u652f\u6301\u4e86\u590d\u6742\u7684\u3001\u8eab\u4efd\u4e00\u81f4\u76843D\u611f\u77e5\u7f16\u8f91\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u5e76\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u73b0\u5b9e\u56fe\u50cf\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b3D\u5148\u9a8c\u77e5\u8bc6\uff0c3D-Fixup\u6709\u6548\u5730\u652f\u6301\u4e86\u590d\u6742\u7684\u3001\u8eab\u4efd\u4e00\u81f4\u76843D\u611f\u77e5\u7f16\u8f91\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u5e76\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u73b0\u5b9e\u56fe\u50cf\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2505.10296", "pdf": "https://arxiv.org/pdf/2505.10296", "abs": "https://arxiv.org/abs/2505.10296", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Dusit Niyato"], "title": "Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The growing adoption of Electric Buses (EBs) represents a significant step\ntoward sustainable development. By utilizing Internet of Things (IoT) systems,\ncharging stations can autonomously determine charging schedules based on\nreal-time data. However, optimizing EB charging schedules remains a critical\nchallenge due to uncertainties in travel time, energy consumption, and\nfluctuating electricity prices. Moreover, to address real-world complexities,\ncharging policies must make decisions efficiently across multiple time scales\nand remain scalable for large EB fleets. In this paper, we propose a\nHierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the\noriginal Markov Decision Process (MDP) into two augmented MDPs. To solve these\nMDPs and enable multi-timescale decision-making, we introduce a novel HDRL\nalgorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization\nEnhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic\n(DAC) algorithm for large-scale EB fleets are addressed through enhancements at\nboth decision levels. At the high level, we redesign the decentralized actor\nnetwork and integrate an attention mechanism to extract relevant global state\ninformation for each EB, decreasing the size of neural networks. At the low\nlevel, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is\nincorporated into the DAC framework, enabling decentralized and coordinated\ncharging power decisions, reducing computational complexity and enhancing\nconvergence speed. Extensive experiments with real-world data demonstrate the\nsuperior performance and scalability of DAC-MAPPO-E in optimizing EB fleet\ncharging schedules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08DAC-MAPPO-E\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u961f\u7684\u5145\u7535\u8c03\u5ea6\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u7531\u4e8e\u65c5\u884c\u65f6\u95f4\u3001\u80fd\u8017\u548c\u7535\u4ef7\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u5145\u7535\u8c03\u5ea6\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u5145\u7535\u653f\u7b56\u5fc5\u987b\u5728\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u9ad8\u6548\u51b3\u7b56\uff0c\u5e76\u4e14\u80fd\u591f\u6269\u5c55\u5230\u5927\u89c4\u6a21\u7684\u7535\u52a8\u516c\u4ea4\u8f66\u961f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08HDRL\uff09\u65b9\u6cd5\uff0c\u5c06\u539f\u59cb\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u91cd\u65b0\u6784\u9020\u6210\u4e24\u4e2a\u589e\u5f3a\u7684MDP\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u6f14\u5458-\u8bc4\u8bba\u5bb6\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u589e\u5f3a\uff08DAC-MAPPO-E\uff09\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86DAC-MAPPO-E\u5728\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u961f\u5145\u7535\u8c03\u5ea6\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08DAC-MAPPO-E\uff09\uff0c\u5728\u4f18\u5316\u7535\u52a8\u516c\u4ea4\u8f66\u961f\u5145\u7535\u8c03\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.10297", "pdf": "https://arxiv.org/pdf/2505.10297", "abs": "https://arxiv.org/abs/2505.10297", "authors": ["Chibueze Peace Obioma", "Youcheng Sun", "Mustafa A. Mustafa"], "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Submitted to ESORICS 2025", "summary": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFeRA\u7684\u65b0\u8054\u90a6\u5b66\u4e60\u9632\u5fa1\u673a\u5236\uff0c\u5229\u7528\u8de8\u5ba2\u6237\u7aef\u6ce8\u610f\u529b\u6765\u533a\u5206\u826f\u6027\u4e0e\u6076\u610f\u5ba2\u6237\u7aef\u3002FeRA\u57fa\u4e8e\u8868\u793a\u91cd\u6784\u8bef\u5dee\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\uff0c\u6709\u6548\u8bc6\u522b\u51fa\u5185\u90e8\u6fc0\u6d3b\u4e0e\u7fa4\u4f53\u5171\u8bc6\u663e\u8457\u504f\u79bb\u7684\u5ba2\u6237\u7aef\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFeRA\u5728\u5404\u79cd\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u540e\u95e8\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e3b\u8981\u4efb\u52a1\u7684\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u901a\u8fc7\u5728\u8fb9\u7f18\u8fdb\u884c\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\u6765\u589e\u5f3a\u9690\u79c1\u5e76\u51cf\u5c11\u901a\u4fe1\u6210\u672c\uff0c\u4f46\u6b64\u7c7b\u8bbe\u5907\u7684\u5f02\u8d28\u6027\u4ea7\u751f\u4e86\u591a\u6837\u5316\u7684\u3001\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\uff0c\u4f7f\u5f97\u68c0\u6d4b\u540e\u95e8\u653b\u51fb\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u8054\u90a6\u4ee3\u8868\u6ce8\u610f\u529b\u7684\u9632\u5fa1\u673a\u5236FeRA\uff0c\u5229\u7528\u8de8\u5ba2\u6237\u7aef\u6ce8\u610f\u529b\u6765\u533a\u5206\u826f\u6027\u5ba2\u6237\u7aef\u548c\u6076\u610f\u5ba2\u6237\u7aef\u3002FeRA\u57fa\u4e8e\u8868\u793a\u91cd\u6784\u8bef\u5dee\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\uff0c\u6709\u6548\u5730\u8bc6\u522b\u51fa\u5185\u90e8\u6fc0\u6d3b\u4e0e\u7fa4\u4f53\u5171\u8bc6\u663e\u8457\u504f\u79bb\u7684\u5ba2\u6237\u7aef\u3002", "result": "FeRA\u5728\u5404\u79cd\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u5305\u62ec\u5178\u578b\u7684\u8fb9\u7f18\u8bbe\u5907\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5206\u5e03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u6709\u6548\u964d\u4f4e\u4e86\u540e\u95e8\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e3b\u8981\u4efb\u52a1\u7684\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "FeRA\u5728\u5404\u79cd\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u5305\u62ec\u5178\u578b\u7684\u8fb9\u7f18\u8bbe\u5907\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5206\u5e03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u6709\u6548\u964d\u4f4e\u4e86\u540e\u95e8\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e3b\u8981\u4efb\u52a1\u7684\u9ad8\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u662f\u6a21\u578b\u65e0\u5173\u3001\u653b\u51fb\u65e0\u5173\u7684\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u6807\u8bb0\u7684\u53c2\u8003\u6570\u636e\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u3002"}}
{"id": "2505.10441", "pdf": "https://arxiv.org/pdf/2505.10441", "abs": "https://arxiv.org/abs/2505.10441", "authors": ["Filippo Leveni", "Luca Magri", "Giacomo Boracchi", "Cesare Alippi"], "title": "PIF: Anomaly detection via preference embedding", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Pattern Recognition (ICPR\n  2020)", "summary": "We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5PIF\uff0c\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u9694\u79bb\u65b9\u6cd5\u548c\u504f\u597d\u5d4c\u5165\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5d4c\u5165\u9ad8\u7ef4\u7a7a\u95f4\u5e76\u4f7f\u7528PI-Forest\u8ba1\u7b97\u5f02\u5e38\u5f97\u5206\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePIF\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7ed3\u6784\u5316\u6a21\u5f0f\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u3002", "method": "PIF\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u9694\u79bb\u65b9\u6cd5\u7684\u4f18\u70b9\u4e0e\u504f\u597d\u5d4c\u5165\u7684\u7075\u6d3b\u6027\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5d4c\u5165\u9ad8\u7ef4\u7a7a\u95f4\u5e76\u4f7f\u7528\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5PI-Forest\u8ba1\u7b97\u5f02\u5e38\u5f97\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePIF\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0cPI-Forest\u5728\u6d4b\u91cf\u4efb\u610f\u8ddd\u79bb\u548c\u9694\u79bb\u504f\u597d\u7a7a\u95f4\u4e2d\u7684\u70b9\u65b9\u9762\u66f4\u6709\u6548\u3002", "conclusion": "PIF\u5728\u68c0\u6d4b\u5f02\u5e38\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14PI-Forest\u5728\u6d4b\u91cf\u4efb\u610f\u8ddd\u79bb\u548c\u9694\u79bb\u504f\u597d\u7a7a\u95f4\u4e2d\u7684\u70b9\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2505.10307", "pdf": "https://arxiv.org/pdf/2505.10307", "abs": "https://arxiv.org/abs/2505.10307", "authors": ["Yiyang Zhao", "Chengpei Wu", "Lilin Zhang", "Ning Yang"], "title": "Negative Metric Learning for Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Graph contrastive learning (GCL) often suffers from false negatives, which\ndegrades the performance on downstream tasks. The existing methods addressing\nthe false negative issue usually rely on human prior knowledge, still leading\nGCL to suboptimal results. In this paper, we propose a novel Negative Metric\nLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative\nMetric Network (NMN) to build a negative metric space, in which false negatives\ncan be distinguished better from true negatives based on their distance to\nanchor node. To overcome the lack of explicit supervision signals for NML, we\npropose a joint training scheme with bi-level optimization objective, which\nimplicitly utilizes the self-supervision signals to iteratively optimize the\nencoder and the negative metric network. The solid theoretical analysis and the\nextensive experiments conducted on widely used benchmarks verify the\nsuperiority of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1f\u5ea6\u91cf\u5b66\u4e60\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08NML-GCL\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8d1f\u5ea6\u91cf\u7f51\u7edc\u548c\u53cc\u5c42\u4f18\u5316\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5047\u8d1f\u4f8b\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u5047\u8d1f\u4f8b\u95ee\u9898\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4eba\u5de5\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6548\u679c\u4e0d\u7406\u60f3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "NML-GCL\u91c7\u7528\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u8d1f\u5ea6\u91cf\u7f51\u7edc\uff08NMN\uff09\u6765\u6784\u5efa\u8d1f\u5ea6\u91cf\u7a7a\u95f4\uff0c\u4ee5\u66f4\u597d\u5730\u533a\u5206\u5047\u8d1f\u4f8b\u548c\u771f\u8d1f\u4f8b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u8bad\u7ec3\u65b9\u6848\uff0c\u7ed3\u5408\u53cc\u5c42\u4f18\u5316\u76ee\u6807\uff0c\u9690\u5f0f\u5229\u7528\u81ea\u76d1\u7763\u4fe1\u53f7\u8fed\u4ee3\u4f18\u5316\u7f16\u7801\u5668\u548c\u8d1f\u5ea6\u91cf\u7f51\u7edc\u3002", "result": "NML-GCL\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "NML-GCL\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8d1f\u5ea6\u91cf\u7f51\u7edc\u548c\u53cc\u5c42\u4f18\u5316\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5e76\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2505.10457", "pdf": "https://arxiv.org/pdf/2505.10457", "abs": "https://arxiv.org/abs/2505.10457", "authors": ["Matteo Gambella", "Vicente Javier Castro Solar", "Manuel Roveri"], "title": "SEAL: Searching Expandable Architectures for Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "8 pages, 5 figures", "summary": "Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.", "AI": {"tldr": "SEAL\u662f\u4e00\u79cd\u57fa\u4e8eNAS\u7684\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u548c\u8de8\u84b8\u998f\u8bad\u7ec3\u6765\u5e73\u8861\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eNAS\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5728\u6bcf\u4e2a\u4efb\u52a1\u4e2d\u6269\u5c55\u6a21\u578b\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u6765\u5e73\u8861\u6a21\u578b\u7684\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "SEAL\u662f\u4e00\u79cd\u57fa\u4e8eNAS\u7684\u6846\u67b6\uff0c\u9488\u5bf9\u6570\u636e\u589e\u91cf\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002\u5b83\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7ed3\u6784\uff0c\u5728\u5fc5\u8981\u65f6\u8fdb\u884c\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u8de8\u84b8\u998f\u8bad\u7ec3\u4fdd\u6301\u7a33\u5b9a\u6027\u3002NAS\u7ec4\u4ef6\u5171\u540c\u641c\u7d22\u67b6\u6784\u548c\u6700\u4f73\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEAL\u6709\u6548\u51cf\u5c11\u4e86\u9057\u5fd8\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u4f4e\u7684\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u5c06NAS\u548c\u9009\u62e9\u6027\u6269\u5c55\u76f8\u7ed3\u5408\u5728\u589e\u91cf\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u524d\u666f\u3002"}}
{"id": "2505.10322", "pdf": "https://arxiv.org/pdf/2505.10322", "abs": "https://arxiv.org/abs/2505.10322", "authors": ["Yijie Zhou", "Shi Pu"], "title": "Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6b65\u53bb\u4e2d\u5fc3\u5316\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff08ADSGD\uff09\uff0c\u5b83\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6536\u655b\u6027\u80fd\uff0c\u5e76\u4e14\u5bf9\u901a\u4fe1\u548c\u8ba1\u7b97\u5ef6\u8fdf\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u5bf9\u4e8e\u5728\u6ca1\u6709\u4e2d\u592e\u63a7\u5236\u7684\u60c5\u51b5\u4e0b\u5229\u7528\u5206\u5e03\u5f0f\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u8ba1\u7b97\u901f\u5ea6\u5f02\u6784\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u901a\u4fe1\u5ef6\u8fdf\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u5728\u5b9e\u9645\u5047\u8bbe\u4e0b\u7684\u5f02\u6b65\u53bb\u4e2d\u5fc3\u5316\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08ADSGD\uff09\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u5f02\u6b65\u968f\u673a\u5757\u5750\u6807\u4e0b\u964d\uff08ASBCD\uff09\u6765\u7406\u89e3\u5176\u6536\u655b\u6027\u3002", "result": "ADSGD \u5728\u5404\u79cd\u573a\u666f\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u5899\u949f\u6536\u655b\u65f6\u95f4\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u5176\u6536\u655b\u7ed3\u679c\u4e0d\u4f9d\u8d56\u4e8e\u6570\u636e\u5f02\u8d28\u6027\u7684\u6709\u754c\u5047\u8bbe\u3002", "conclusion": "ADSGD \u662f\u4e00\u79cd\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u7b97\u6cd5\uff0c\u56e0\u5176\u7b80\u5355\u6027\u3001\u5185\u5b58\u548c\u901a\u4fe1\u6548\u7387\u4ee5\u53ca\u5bf9\u901a\u4fe1\u548c\u8ba1\u7b97\u5ef6\u8fdf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.10325", "pdf": "https://arxiv.org/pdf/2505.10325", "abs": "https://arxiv.org/abs/2505.10325", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aALERT\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u7279\u5f81\u5206\u5e03\u53d8\u5316\u5e76\u89e6\u53d1\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u7684\u4e24\u4e2a\u7528\u4f8b\u3002", "motivation": "\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u7279\u5f81\u5206\u5e03\u7684\u53d8\u5316\u53ef\u80fd\u4f1a\u964d\u4f4eAI\u6a21\u578b\u7684\u6027\u80fd\u5e76\u5bfc\u81f4\u4e0d\u826f\u884c\u4e3a\u3002\u4e3a\u4e86\u5e94\u5bf9\u672a\u88ab\u68c0\u6d4b\u5230\u7684\u6a21\u578b\u9000\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ALERT\u3002", "method": "ALERT\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u8868\u793a\u5b66\u4e60\u3001\u7edf\u8ba1\u6d4b\u8bd5\u548c\u6548\u7528\u8bc4\u4f30\u3002\u8868\u793a\u5b66\u4e60\u4f7f\u7528MLP\uff0c\u7edf\u8ba1\u6d4b\u8bd5\u4f7f\u7528Kolmogorov-Smirnov\u548cPopulation Stability Index\u6d4b\u8bd5\uff0c\u6548\u7528\u8bc4\u4f30\u4f7f\u7528\u65b0\u51fd\u6570\u3002", "result": "ALERT\u5728\u4e24\u4e2a\u65e0\u7ebf\u7f51\u7edc\u7528\u4f8b\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u6587\u732e\u4e2d\u5341\u79cd\u6807\u51c6\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "ALERT\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u7279\u5f81\u5206\u5e03\u53d8\u5316\u5e76\u89e6\u53d1\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u7684\u4e24\u4e2a\u7528\u4f8b\u3002"}}
{"id": "2505.10330", "pdf": "https://arxiv.org/pdf/2505.10330", "abs": "https://arxiv.org/abs/2505.10330", "authors": ["Jonathan Clifford Balloch"], "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change", "categories": ["cs.LG", "cs.AI"], "comment": "PhD Dissertation, 131 pages", "summary": "Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u5173\u952e\u80fd\u529b\uff1a\u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u53ca\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\u5fc5\u987b\u5728\u53d8\u5316\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u3002\u867d\u7136\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u6570\u636e\u5bc6\u96c6\u4e14\u5047\u8bbe\u4e16\u754c\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u95f4\u4e4b\u95f4\u4e0d\u4f1a\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u5f53\u6761\u4ef6\u53d8\u5316\u65f6\uff0c\u4f20\u7edfRL\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u3002\u8fd9\u5e26\u6765\u4e86\u6839\u672c\u6027\u7684\u6311\u6218\uff1a\u5982\u4f55\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u9047\u5230\u65b0\u73af\u5883\u53d8\u5316\u65f6\uff0c\u6709\u6548\u5730\u8c03\u6574\u884c\u4e3a\u800c\u4e0d\u707e\u96be\u6027\u5730\u9057\u5fd8\u6709\u7528\u7684\u77e5\u8bc6\uff1f", "method": "\u672c\u6587\u8ba8\u8bba\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u80fd\u529b\uff1a\u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u53ca\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u9002\u5e94\u9700\u8981\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a(1) \u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u4ece\u76f8\u5173\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u4ee5\u53ca(2) \u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u5728\u4e0d\u5e72\u6270\u53ef\u91cd\u7528\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u66f4\u65b0\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u9002\u5e94\u9700\u8981\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a(1) \u4f18\u5148\u63a2\u7d22\u548c\u91c7\u6837\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u4ece\u76f8\u5173\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u4ee5\u53ca(2) \u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u5728\u4e0d\u5e72\u6270\u53ef\u91cd\u7528\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u66f4\u65b0\u3002"}}
{"id": "2505.10331", "pdf": "https://arxiv.org/pdf/2505.10331", "abs": "https://arxiv.org/abs/2505.10331", "authors": ["Luca Muscarnera", "Luigi Loreti", "Giovanni Todeschini", "Alessio Fumagalli", "Francesco Regazzoni"], "title": "Emergence of Structure in Ensembles of Random Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6a21\u578b\uff0c\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6e29\u5ea6\u4e0b\u5206\u7c7b\u662f\u6700\u4f18\u7684\uff0c\u4e14\u8be5\u6e29\u5ea6\u5177\u6709\u666e\u904d\u6027\u3002", "motivation": "\u968f\u673a\u6027\u5728\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7684\u8bb8\u591a\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\u3002\u7cfb\u7edf\u7531\u968f\u673a\u7ec4\u4ef6\u7ec4\u6210\u65f6\uff0c\u901a\u5e38\u8868\u73b0\u51fa\u770b\u4f3c\u786e\u5b9a\u7684\u5b8f\u89c2\u884c\u4e3a\uff0c\u8fd9\u4fc3\u4f7f\u6211\u4eec\u7814\u7a76\u8fd9\u4e9b\u7cfb\u7edf\u7684\u96c6\u4f53\u884c\u4e3a\u53ca\u5176\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u7406\u8bba\u6a21\u578b\uff0c\u901a\u8fc7\u91c7\u7528\u5206\u7c7b\u635f\u5931\u4f5c\u4e3a\u80fd\u91cf\u7684Gibbs\u6d4b\u5ea6\u5bf9\u96c6\u6210\u8fdb\u884c\u52a0\u6743\uff0c\u4ee5\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u3002\u6211\u4eec\u901a\u8fc7\u5206\u6790\u548c\u6570\u503c\u9a8c\u8bc1\u8bc1\u660e\u4e86\u6700\u4f18\u6e29\u5ea6\u7684\u5b58\u5728\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u7c7b\u6bd4\u89e3\u91ca\u4e86\u81ea\u7ec4\u7ec7\u73b0\u8c61\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6e29\u5ea6\u53c2\u6570\u4e0b\uff0c\u5206\u7c7b\u662f\u76f8\u5bf9\u4e8e\u635f\u5931\u6700\u4f18\u7684\uff0c\u5e76\u4e14\u8fd9\u79cd\u6700\u4f18\u6e29\u5ea6\u4e0d\u4f9d\u8d56\u4e8e\u6559\u5e08\u5206\u7c7b\u5668\u6216\u968f\u673a\u5206\u7c7b\u5668\u7684\u6570\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u3001\u65e0\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6a21\u578b\uff0c\u7528\u4e8e\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6e29\u5ea6\u53c2\u6570\u4e0b\uff0c\u5206\u7c7b\u662f\u76f8\u5bf9\u4e8e\u635f\u5931\uff08\u6216\u80fd\u91cf\uff09\u6700\u4f18\u7684\uff0c\u5e76\u4e14\u8fd9\u79cd\u6700\u4f18\u6e29\u5ea6\u4e0d\u4f9d\u8d56\u4e8e\u6559\u5e08\u5206\u7c7b\u5668\u6216\u968f\u673a\u5206\u7c7b\u5668\u7684\u6570\u91cf\uff0c\u8868\u660e\u4e86\u6240\u89c2\u5bdf\u5230\u884c\u4e3a\u7684\u666e\u904d\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u3001\u65e0\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.10344", "pdf": "https://arxiv.org/pdf/2505.10344", "abs": "https://arxiv.org/abs/2505.10344", "authors": ["Alan Jeffares", "Liyuan Liu"], "title": "An Introduction to Discrete Variational Autoencoders", "categories": ["cs.LG"], "comment": "Tutorial paper", "summary": "Variational Autoencoders (VAEs) are well-established as a principled approach\nto probabilistic unsupervised learning with neural networks. Typically, an\nencoder network defines the parameters of a Gaussian distributed latent space\nfrom which we can sample and pass realizations to a decoder network. This model\nis trained to reconstruct its inputs and is optimized through the evidence\nlower bound. In recent years, discrete latent spaces have grown in popularity,\nsuggesting that they may be a natural choice for many data modalities (e.g.\ntext). In this tutorial, we provide a rigorous, yet practical, introduction to\ndiscrete variational autoencoders -- specifically, VAEs in which the latent\nspace is made up of latent variables that follow a categorical distribution. We\nassume only a basic mathematical background with which we carefully derive each\nstep from first principles. From there, we develop a concrete training recipe\nand provide an example implementation, hosted at\nhttps://github.com/alanjeffares/discreteVAE.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u793a\u4f8b\u5b9e\u73b0\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u5728\u8bb8\u591a\u6570\u636e\u6a21\u6001\uff08\u5982\u6587\u672c\uff09\u4e2d\u53d8\u5f97\u6d41\u884c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u5e94\u7528\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3002", "method": "\u672c\u6587\u4ece\u57fa\u672c\u6570\u5b66\u80cc\u666f\u51fa\u53d1\uff0c\u8be6\u7ec6\u63a8\u5bfc\u4e86\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6bcf\u4e2a\u6b65\u9aa4\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5177\u4f53\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u4e25\u8c28\u800c\u5b9e\u7528\u7684\u4ecb\u7ecd\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e00\u4e2a\u793a\u4f8b\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08DVAEs\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u793a\u4f8b\u5b9e\u73b0\u3002"}}
{"id": "2505.10347", "pdf": "https://arxiv.org/pdf/2505.10347", "abs": "https://arxiv.org/abs/2505.10347", "authors": ["Gabriel S. Gama", "Valdir Grassi Jr"], "title": "Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86SMTOs\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u8868\u73b0\u826f\u597d\uff0c\u56fa\u5b9a\u6743\u91cd\u53ef\u4ee5\u5b9e\u73b0\u4e0eSMTOs\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7edf\u4e00\u635f\u5931\u7684\u8868\u73b0\u4e0eSMTOs\u76f8\u4f3c\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u6700\u8fd1\u7684\u6279\u8bc4\uff0c\u5373SMTOs\u7684\u7ed3\u679c\u53ef\u80fd\u53d7\u5230\u4e0d\u826f\u8d85\u53c2\u6570\u4f18\u5316\u548c\u7f3a\u4e4f\u6b63\u5219\u5316\u7684\u5e72\u6270\uff0c\u4ee5\u53ca\u63a2\u7d22SMTOs\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u8bc4\u4f30\u5bf9SMTOs\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u4e00\u4e9b\u6700\u65b0\u7684\u65b9\u6cd5\uff0c\u5728\u66f4\u590d\u6742\u7684\u591a\u4efb\u52a1\u95ee\u9898\u4e0a\u6f84\u6e05\u8fd9\u79cd\u884c\u4e3a\u3002", "result": "SMTOs\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u56fa\u5b9a\u6743\u91cd\u53ef\u4ee5\u5b9e\u73b0\u4e0eSMTOs\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7edf\u4e00\u635f\u5931\u7684\u8868\u73b0\u4e0eSMTOs\u76f8\u4f3c\u3002", "conclusion": "SMTOs\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u56fa\u5b9a\u6743\u91cd\u53ef\u4ee5\u4e0eSMTOs\u7ade\u4e89\u6027\u5730\u8868\u73b0\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7edf\u4e00\u635f\u5931\u7684\u8868\u73b0\u4e0eSMTOs\u76f8\u4f3c\u3002"}}
{"id": "2505.10360", "pdf": "https://arxiv.org/pdf/2505.10360", "abs": "https://arxiv.org/abs/2505.10360", "authors": ["Victor Petr\u00e9n Bach Hansen", "Lasse Krogsb\u00f8ll", "Jonas Lyngs\u00f8", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maal\u00f8e"], "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": null, "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFactsR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u7597\u54a8\u8be2\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u63d0\u53d6\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u9012\u5f52\u751f\u6210\u6700\u7ec8\u7684\u75c5\u5386\u8bb0\u5f55\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u75c5\u5386\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u5e76\u5c06\u4e34\u5e8a\u533b\u751f\u7eb3\u5165\u75c5\u5386\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u540c\u65f6\u4e3a\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u5f00\u8f9f\u4e86\u65b0\u7684\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u7684AI\u75c5\u5386\u8bb0\u5f55\u89e3\u51b3\u65b9\u6848\u5728\u751f\u6210\u75c5\u5386\u65f6\u4ecd\u7136\u4f9d\u8d56\u4e8e\u4e00\u6b21\u6216\u5c11\u91cf\u63d0\u793a\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u75c5\u5386\u5197\u957f\u4e14\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u548c\u9519\u8bef\uff0c\u8fd9\u53ef\u80fd\u5bf9\u60a3\u8005\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u7b80\u6d01\u7684\u75c5\u5386\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFactsR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u7597\u54a8\u8be2\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u63d0\u53d6\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u9012\u5f52\u751f\u6210\u6700\u7ec8\u7684\u75c5\u5386\u8bb0\u5f55\u3002", "result": "FactsR\u65b9\u6cd5\u901a\u8fc7\u5c06\u4e34\u5e8a\u533b\u751f\u7eb3\u5165\u75c5\u5386\u751f\u6210\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u75c5\u5386\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u5e76\u4e3a\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u5f00\u8f9f\u4e86\u65b0\u7684\u5e94\u7528\u573a\u666f\u3002", "conclusion": "FactsR\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u75c5\u5386\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\uff0c\u5e76\u5c06\u4e34\u5e8a\u533b\u751f\u7eb3\u5165\u75c5\u5386\u751f\u6210\u8fc7\u7a0b\uff0c\u4ece\u800c\u6539\u5584\u60a3\u8005\u5b89\u5168\u3002"}}
{"id": "2505.10392", "pdf": "https://arxiv.org/pdf/2505.10392", "abs": "https://arxiv.org/abs/2505.10392", "authors": ["Aryan Mishra", "Lizhen Lin"], "title": "Schreier-Coset Graph Propagation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure , preprint", "summary": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SCGP\uff0c\u4e00\u79cd\u57fa\u4e8e\u7fa4\u8bba\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7Schreier-coset\u5d4c\u5165\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\uff0c\u800c\u65e0\u9700\u6539\u53d8\u8f93\u5165\u56fe\u62d3\u6251\u3002SCGP\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u957f\u8ddd\u79bb\u6d88\u606f\u4f20\u9012\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5904\u7406\u5206\u5c42\u548c\u6a21\u5757\u5316\u56fe\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5982\u56fe\u91cd\u5e03\u7ebf\u548c\u6297\u74f6\u9888\u67b6\u6784\uff08\u5982Cayley\u56fe\u548cexpander\u56fe\uff09\uff0c\u867d\u7136\u907f\u514d\u4e86\u8fc7\u538b\u7f29\u95ee\u9898\uff0c\u4f46\u5f15\u5165\u4e86\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002Cayley\u56fe\u5728SL(2,Z_n)\u4e0a\u6784\u9020\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7406\u8bba\u6027\u8d28\uff0c\u4f46\u7531\u4e8e\u8282\u70b9\u589e\u957f\u4e3aO(n^3)\uff0c\u5bfc\u81f4\u9ad8\u5185\u5b58\u4f7f\u7528\u3002", "method": "SCGP\u662f\u4e00\u79cd\u57fa\u4e8e\u7fa4\u8bba\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7Schreier-coset\u5d4c\u5165\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\uff0c\u800c\u65e0\u9700\u6539\u53d8\u8f93\u5165\u56fe\u62d3\u6251\u3002\u5b83\u5c06\u65e0\u74f6\u9888\u7684\u8fde\u63a5\u6a21\u5f0f\u5d4c\u5165\u5230\u7d27\u51d1\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u957f\u8ddd\u79bb\u6d88\u606f\u4f20\u9012\u7684\u6548\u7387\u3002", "result": "SCGP\u5728\u6807\u51c6\u8282\u70b9\u548c\u56fe\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e\u6216\u8d85\u8fc7expander\u56fe\u548c\u91cd\u5e03\u7ebfGNN\u57fa\u7ebf\u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0cSCGP\u5728\u5904\u7406\u5206\u5c42\u548c\u6a21\u5757\u5316\u56fe\u7ed3\u6784\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "SCGP\u5728\u5904\u7406\u5206\u5c42\u548c\u6a21\u5757\u5316\u56fe\u7ed3\u6784\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u63a8\u7406\u5ef6\u8fdf\u3001\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u9002\u5408\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u3002"}}
{"id": "2505.10407", "pdf": "https://arxiv.org/pdf/2505.10407", "abs": "https://arxiv.org/abs/2505.10407", "authors": ["Wenhao Ding", "Choon Hwai Yap", "Kangjun Ji", "Sim\u00e3o Castro"], "title": "Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning", "categories": ["cs.LG", "68T07"], "comment": "10 pages, 2 figures", "summary": "A generative model for the mesh geometry of intracranial aneurysms (IA) is\ncrucial for training networks to predict blood flow forces in real time, which\nis a key factor affecting disease progression. This need is necessitated by the\nabsence of a large IA image datasets. Existing shape generation methods\nstruggle to capture realistic IA features and ignore the relationship between\nIA pouches and parent vessels, limiting physiological realism and their\ngeneration cannot be controlled to have specific morphological measurements. We\npropose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh\ngenerator. In the first stage, AneuG generates low-dimensional Graph Harmonic\nDeformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,\nconstrained to morphing energy statistics truths. GHD enables more accurate\nshape encoding than alternatives. In the second stage, AneuG generates parent\nvessels conditioned on GHD tokens, by generating vascular centreline and\npropagating the cross-section. AneuG's IA shape generation can further be\nconditioned to have specific clinically relevant morphological measurements.\nThis is useful for studies to understand shape variations represented by\nclinical measurements, and for flow simulation studies to understand effects of\nspecific clinical shape parameters on fluid dynamics. Source code and\nimplementation details are available at\nhttps://github.com/anonymousaneug/AneuG.", "AI": {"tldr": "AneuG is a two-stage VAE-based IA mesh generator that can generate realistic IA shapes with specific morphological measurements, which is useful for studies on shape variations and flow simulations.", "motivation": "The absence of a large IA image dataset necessitates the need for a generative model to train networks for predicting blood flow forces in real time, which is crucial for understanding disease progression.", "method": "AneuG is a two-stage VAE-based IA mesh generator. In the first stage, it generates low-dimensional GHD tokens to encode and reconstruct aneurysm pouch shapes. In the second stage, it generates parent vessels conditioned on GHD tokens by generating vascular centreline and propagating the cross-section.", "result": "AneuG can generate realistic IA shapes with specific morphological measurements, which can be used for studies on shape variations and flow simulations.", "conclusion": "AneuG can generate realistic IA shapes with specific morphological measurements, which is useful for studies on shape variations and flow simulations."}}
{"id": "2505.10422", "pdf": "https://arxiv.org/pdf/2505.10422", "abs": "https://arxiv.org/abs/2505.10422", "authors": ["Daniel Weitekamp", "Christopher MacLellan", "Erik Harpstead", "Kenneth Koedinger"], "title": "Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency", "categories": ["cs.LG"], "comment": "To appear in CogSci 2025", "summary": "Human learning relies on specialization -- distinct cognitive mechanisms\nworking together to enable rapid learning. In contrast, most modern neural\nnetworks rely on a single mechanism: gradient descent over an objective\nfunction. This raises the question: might human learners' relatively rapid\nlearning from just tens of examples instead of tens of thousands in data-driven\ndeep learning arise from our ability to use multiple specialized mechanisms of\nlearning in combination? We investigate this question through an ablation\nanalysis of inductive human learning simulations in online tutoring\nenvironments. Comparing reinforcement learning to a more data-efficient\n3-mechanism symbolic rule induction approach, we find that decomposing learning\ninto multiple distinct mechanisms significantly improves data efficiency,\nbringing it in line with human learning. Furthermore, we show that this\ndecomposition has a greater impact on efficiency than the distinction between\nsymbolic and subsymbolic learning alone. Efforts to align data-driven machine\nlearning with human learning often overlook the stark difference in learning\nefficiency. Our findings suggest that integrating multiple specialized learning\nmechanisms may be key to bridging this gap.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u5728\u7ebf\u8f85\u5bfc\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u4eba\u7c7b\u5b66\u4e60\u6a21\u62df\uff0c\u53d1\u73b0\u5c06\u5b66\u4e60\u5206\u89e3\u4e3a\u591a\u4e2a\u4e0d\u540c\u673a\u5236\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4f7f\u5176\u4e0e\u4eba\u7c7b\u5b66\u4e60\u4e00\u81f4\uff0c\u5e76\u6307\u51fa\u6574\u5408\u591a\u4e2a\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u662f\u5f25\u5408\u673a\u5668\u5b66\u4e60\u4e0e\u4eba\u7c7b\u5b66\u4e60\u5dee\u8ddd\u7684\u5173\u952e\u3002", "motivation": "\u4eba\u7c7b\u5b66\u4e60\u4f9d\u8d56\u4e8e\u4e13\u4e1a\u5316\uff0c\u800c\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u673a\u5236\u3002\u8fd9\u5f15\u53d1\u4e86\u95ee\u9898\uff1a\u4eba\u7c7b\u5b66\u4e60\u8005\u80fd\u5426\u901a\u8fc7\u4f7f\u7528\u591a\u4e2a\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u7ec4\u5408\u6765\u5b9e\u73b0\u5feb\u901f\u5b66\u4e60\uff1f", "method": "\u6211\u4eec\u901a\u8fc7\u5728\u7ebf\u8f85\u5bfc\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u4eba\u7c7b\u5b66\u4e60\u6a21\u62df\u7684\u6d88\u878d\u5206\u6790\u6765\u7814\u7a76\u8fd9\u4e2a\u95ee\u9898\u3002\u6bd4\u8f83\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u66f4\u9ad8\u6548\u7684\u6570\u636e3\u673a\u5236\u7b26\u53f7\u89c4\u5219\u5f52\u7eb3\u65b9\u6cd5\u3002", "result": "\u5c06\u5b66\u4e60\u5206\u89e3\u4e3a\u591a\u4e2a\u4e0d\u540c\u7684\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u4f7f\u5176\u4e0e\u4eba\u7c7b\u5b66\u4e60\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u5206\u89e3\u5bf9\u6548\u7387\u7684\u5f71\u54cd\u6bd4\u7b26\u53f7\u548c\u975e\u7b26\u53f7\u5b66\u4e60\u7684\u533a\u522b\u66f4\u5927\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408\u591a\u4e2a\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u662f\u5f25\u5408\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u4e0e\u4eba\u7c7b\u5b66\u4e60\u4e4b\u95f4\u5dee\u8ddd\u7684\u5173\u952e\u3002"}}
{"id": "2505.10423", "pdf": "https://arxiv.org/pdf/2505.10423", "abs": "https://arxiv.org/abs/2505.10423", "authors": ["Ari Karchmer", "Eran Malach"], "title": "The Power of Random Features and the Limits of Distribution-Free Gradient Descent", "categories": ["cs.LG"], "comment": null, "summary": "We study the relationship between gradient-based optimization of parametric\nmodels (e.g., neural networks) and optimization of linear combinations of\nrandom features. Our main result shows that if a parametric model can be\nlearned using mini-batch stochastic gradient descent (bSGD) without making\nassumptions about the data distribution, then with high probability, the target\nfunction can also be approximated using a polynomial-sized combination of\nrandom features. The size of this combination depends on the number of gradient\nsteps and numerical precision used in the bSGD process. This finding reveals\nfundamental limitations of distribution-free learning in neural networks\ntrained by gradient descent, highlighting why making assumptions about data\ndistributions is often crucial in practice. Along the way, we also introduce a\nnew theoretical framework called average probabilistic dimension complexity\n(adc), which extends the probabilistic dimension complexity developed by Kamath\net al. (2020). We prove that adc has a polynomial relationship with statistical\nquery dimension, and use this relationship to demonstrate an infinite\nseparation between adc and standard dimension complexity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u68af\u5ea6\u4e0b\u964d\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5206\u5e03\u65e0\u5173\u5b66\u4e60\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6211\u4eec\u5e0c\u671b\u7406\u89e3\u68af\u5ea6\u4e0b\u964d\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u5206\u5e03\u65e0\u5173\u5b66\u4e60\u7684\u5c40\u9650\u6027\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5206\u6790\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u4e0e\u968f\u673a\u7279\u5f81\u7ebf\u6027\u7ec4\u5408\u4f18\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u79f0\u4e3a\u5e73\u5747\u6982\u7387\u7ef4\u5ea6\u590d\u6742\u5ea6\uff08adc\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5982\u679c\u53ef\u4ee5\u4f7f\u7528\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08bSGD\uff09\u5b66\u4e60\u53c2\u6570\u6a21\u578b\u800c\u4e0d\u505a\u6570\u636e\u5206\u5e03\u5047\u8bbe\uff0c\u5219\u76ee\u6807\u51fd\u6570\u53ef\u4ee5\u7528\u591a\u9879\u5f0f\u5927\u5c0f\u7684\u968f\u673a\u7279\u5f81\u7ec4\u5408\u8fd1\u4f3c\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u5206\u5e03\u65e0\u5173\u5b66\u4e60\u7684\u6839\u672c\u9650\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5b9e\u8df5\u4e2d\u5bf9\u6570\u636e\u5206\u5e03\u505a\u51fa\u5047\u8bbe\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.10425", "pdf": "https://arxiv.org/pdf/2505.10425", "abs": "https://arxiv.org/abs/2505.10425", "authors": ["Jingyao Wang", "Wenwen Qiang", "Zeen Song", "Changwen Zheng", "Hui Xiong"], "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.", "AI": {"tldr": "L2T is a framework for large language models that optimizes reasoning by reducing computational complexity and improving efficiency.", "motivation": "Existing methods overlook the trade-off between reasoning effectiveness and computational efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens.", "method": "L2T is an information-theoretic reinforcement fine-tuning framework that treats each query-response interaction as a hierarchical session of multiple episodes. It proposes a universal dense process reward, which quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. A method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix is also proposed.", "result": "Theoretical analyses show that L2T significantly reduces computational complexity with high estimation accuracy. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency.", "conclusion": "L2T demonstrates advantages across different tasks by boosting both reasoning effectiveness and efficiency."}}
{"id": "2505.10432", "pdf": "https://arxiv.org/pdf/2505.10432", "abs": "https://arxiv.org/abs/2505.10432", "authors": ["Randy J. Chase", "Katherine Haynes", "Lander Ver Hoef", "Imme Ebert-Uphoff"], "title": "Score-based diffusion nowcasting of GOES imagery", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\u5728\u73b0\u5728\u9884\u62a5\u4e91\u548c\u964d\u6c34\u4e2d\u7684\u5e94\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u80fd\u591f\u5e73\u6d41\u73b0\u6709\u7684\u4e91\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\uff0c\u5305\u62ec\u5bf9\u6d41\u7684\u5f00\u59cb\u3002\u6700\u4f73\u7684\u6a21\u578b\u662fCorrDiff\u65b9\u6cd5\uff0c\u5728\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u548c\u4f20\u7edf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6269\u6563\u6a21\u578b\u8fd8\u80fd\u76f4\u63a5\u751f\u6210\u96c6\u5408\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6821\u51c6\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\u5728\u6a21\u62df\u4e91\u548c\u964d\u6c34\u65f6\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9700\u8981\u4e9a\u7f51\u683c\u53c2\u6570\u5316\u3002\u673a\u5668\u5b66\u4e60\u5df2\u88ab\u7528\u4e8e\u9884\u6d4b\u4e91\u548c\u964d\u6c34\uff0c\u4f46\u65e9\u671f\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f1a\u4ea7\u751f\u6a21\u7cca\u7684\u9884\u6d4b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u8f83\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u73b0\u5728\u9884\u62a5\u4e91\u548c\u964d\u6c34\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u8f83\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u73b0\u5728\u9884\u62a5\uff08\u96f6\u5230\u4e09\u5c0f\u65f6\u7684\u9884\u62a5\uff09\u4e91\u548c\u964d\u6c34\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\u7684\u80cc\u666f\u548c\u76f4\u89c9\uff0c\u4ece\u800c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d77\u70b9\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5728\u9884\u62a5\u9759\u6b62\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u5e94\u7528\u3002\u6211\u4eec\u6d4b\u8bd5\u4e86\u4e09\u79cd\u4e3b\u8981\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\uff1a\u6807\u51c6\u7684\u57fa\u4e8e\u8bc4\u5206\u7684\u6269\u6563\u6a21\u578b\uff08Diff\uff09\uff1b\u6b8b\u5dee\u6821\u6b63\u6269\u6563\u6a21\u578b\uff08CorrDiff\uff09\uff1b\u4ee5\u53ca\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u3002", "result": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u4e0d\u4ec5\u5e73\u6d41\u73b0\u6709\u7684\u4e91\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\uff0c\u5305\u62ec\u5bf9\u6d41\u7684\u5f00\u59cb\u3002\u8fd9\u4e9b\u7ed3\u679c\u4ee4\u4eba\u60ca\u8bb6\uff0c\u56e0\u4e3a\u9884\u6d4b\u4ec5\u4ee5\u8fc7\u53bb20\u5206\u949f\u7684\u7ea2\u5916\u536b\u661f\u56fe\u50cf\u4e3a\u8d77\u70b9\u3002\u6700\u4f73\u7684\u6269\u6563\u6a21\u578b\u662fCorrDiff\u65b9\u6cd5\uff0c\u5728\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u6bd4\u6240\u6709\u5176\u4ed6\u6269\u6563\u6a21\u578b\u3001\u4f20\u7edf\u7684U-Net\u548c\u4e00\u4e2a\u6301\u7eed\u6027\u9884\u6d4b\u9ad8\u51fa1\u52302\u5f00\u5c14\u6587\u3002\u6269\u6563\u6a21\u578b\u8fd8\u80fd\u591f\u76f4\u63a5\u8fdb\u884c\u96c6\u5408\u751f\u6210\uff0c\u8fd9\u663e\u793a\u4e86\u826f\u597d\u7684\u6821\u51c6\uff0c\u96c6\u5408\u7684\u8303\u56f4\u4e0e\u8bef\u5dee\u6709\u5f88\u597d\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u4e0d\u4ec5\u5e73\u6d41\u73b0\u6709\u7684\u4e91\uff0c\u8fd8\u80fd\u751f\u6210\u548c\u6d88\u6563\u4e91\uff0c\u5305\u62ec\u5bf9\u6d41\u7684\u5f00\u59cb\u3002\u8fd9\u4e9b\u7ed3\u679c\u4ee4\u4eba\u60ca\u8bb6\uff0c\u56e0\u4e3a\u9884\u6d4b\u4ec5\u4ee5\u8fc7\u53bb20\u5206\u949f\u7684\u7ea2\u5916\u536b\u661f\u56fe\u50cf\u4e3a\u8d77\u70b9\u3002\u6700\u4f73\u7684\u6269\u6563\u6a21\u578b\u662fCorrDiff\u65b9\u6cd5\uff0c\u5728\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u6bd4\u6240\u6709\u5176\u4ed6\u6269\u6563\u6a21\u578b\u3001\u4f20\u7edf\u7684U-Net\u548c\u4e00\u4e2a\u6301\u7eed\u6027\u9884\u6d4b\u9ad8\u51fa1\u52302\u5f00\u5c14\u6587\u3002\u6269\u6563\u6a21\u578b\u8fd8\u80fd\u591f\u76f4\u63a5\u8fdb\u884c\u96c6\u5408\u751f\u6210\uff0c\u8fd9\u663e\u793a\u4e86\u826f\u597d\u7684\u6821\u51c6\uff0c\u96c6\u5408\u7684\u8303\u56f4\u4e0e\u8bef\u5dee\u6709\u5f88\u597d\u7684\u76f8\u5173\u6027\u3002"}}
{"id": "2505.10438", "pdf": "https://arxiv.org/pdf/2505.10438", "abs": "https://arxiv.org/abs/2505.10438", "authors": ["David Grasev"], "title": "Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "51 pages, 28 figures", "summary": "Gas turbine engines represent complex highly nonlinear dynamical systems.\nDeriving their physics-based models can be challenging as it requires\nperformance characteristics, that are not always available, and one often has\nto make many simplifying assumptions. In this paper, the limitations of\nconventional experimental methods used to derive component-level and locally\nlinear parameter-varying models are discussed and addressed by employing\nidentification techniques based on data collected from standard engine\noperation under closed-loop control. The rotor dynamics were estimated using\nthe sparse identification of nonlinear dynamics. Subsequently, the autonomous\npart of the dynamics was mapped into an optimally constructed Koopman\neigenfunction space. The process included eigenvalue optimization using\nmetaheuristic algorithms and temporal projection, followed by gradient-based\neigenfunction identification. The resulting Koopman model was validated against\nan in-house reference component-level model. A globally optimal nonlinear\nfeedback controller and a Kalman estimator were then designed in the\neigenfunction space and compared to the classical and gain-scheduled\nproportional-integral controllers, as well as a proposed internal model control\napproach. The eigenmode structure allowed targeting individual modes during the\noptimization process, resulting in a better performance tuning. The results\nshowed that the Koopman-based controller outperformed the other benchmark\ncontrollers in both reference tracking and disturbance rejection, under\nsea-level and varying flight conditions, due to its global nature.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u5e76\u89e3\u51b3\u4e86\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u5728\u63a8\u5bfc\u71c3\u6c14\u8f6e\u673a\u53d1\u52a8\u673a\u7ec4\u4ef6\u7ea7\u548c\u5c40\u90e8\u7ebf\u6027\u53c2\u6570\u53d8\u5316\u6a21\u578b\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8bc6\u522b\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eKoopman\u7684\u63a7\u5236\u5668\uff0c\u8be5\u63a7\u5236\u5668\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u63a7\u5236\u5668\u3002", "motivation": "\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u5728\u63a8\u5bfc\u7ec4\u4ef6\u7ea7\u548c\u5c40\u90e8\u7ebf\u6027\u53c2\u6570\u53d8\u5316\u6a21\u578b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u71c3\u6c14\u8f6e\u673a\u53d1\u52a8\u673a\u7684\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8bc6\u522b\u6280\u672f\u4f30\u8ba1\u8f6c\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u5c06\u52a8\u529b\u5b66\u7684\u81ea\u4e3b\u90e8\u5206\u6620\u5c04\u5230\u6700\u4f18\u6784\u9020\u7684Koopman\u7279\u5f81\u51fd\u6570\u7a7a\u95f4\u4e2d\u3002\u968f\u540e\u8fdb\u884c\u4e86\u7279\u5f81\u503c\u4f18\u5316\uff0c\u4f7f\u7528\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u548c\u65f6\u95f4\u6295\u5f71\uff0c\u7136\u540e\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u7279\u5f81\u51fd\u6570\u8bc6\u522b\u3002", "result": "\u57fa\u4e8eKoopman\u7684\u63a7\u5236\u5668\u5728\u53c2\u8003\u8ddf\u8e2a\u548c\u6270\u52a8\u6291\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u6d77\u5e73\u9762\u548c\u4e0d\u540c\u98de\u884c\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eKoopman\u7684\u63a7\u5236\u5668\u5728\u53c2\u8003\u8ddf\u8e2a\u548c\u6270\u52a8\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u63a7\u5236\u5668\uff0c\u8fd9\u662f\u7531\u4e8e\u5176\u5168\u5c40\u6027\u8d28\u3002"}}
{"id": "2505.10482", "pdf": "https://arxiv.org/pdf/2505.10482", "abs": "https://arxiv.org/abs/2505.10482", "authors": ["Ningyuan Yang", "Jiaxuan Gao", "Feng Gao", "Yi Wu", "Chao Yu"], "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures", "summary": "Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.", "AI": {"tldr": "This paper introduces NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy, achieving sample efficiency comparable to MLP+PPO and outperforming existing methods in various benchmarks.", "motivation": "Existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models due to the computational intractability of action likelihood estimation during the denoising process.", "method": "Introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy.", "result": "NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy.", "conclusion": "NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks."}}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472", "abs": "https://arxiv.org/abs/2505.10472", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u51c6\u786e\u3001\u5b89\u5168\u548c\u53ef\u8bbf\u95ee\u7684\u764c\u75c7\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u7528LLMs\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u611f\u67d3\u529b\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800c\u533b\u5b66LLMs\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002\u7136\u800c\uff0c\u533b\u5b66LLMs\u503e\u5411\u4e8e\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6f5c\u5728\u5371\u5bb3\u3001\u6bd2\u6027\u548c\u504f\u89c1\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u5176\u5728\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u8868\u73b0\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5065\u5eb7\u901a\u4fe1\u4e2d\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u548c\u5b89\u5168\u6027\u7684\u4e8c\u5143\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9700\u8981\u6709\u610f\u8bbe\u8ba1\u6a21\u578b\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u6539\u8fdb\u7684\u5efa\u8bae\u3002", "motivation": "\u6709\u6548\u7684\u4e73\u817a\u764c\u548c\u5bab\u9888\u764c\u6c9f\u901a\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u5065\u5eb7\u6311\u6218\uff0c\u516c\u4f17\u5bf9\u764c\u75c7\u9884\u9632\u3001\u7b5b\u67e5\u548c\u6cbb\u7597\u7684\u7406\u89e3\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bca\u65ad\u5ef6\u8fdf\u548c\u6cbb\u7597\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u51c6\u786e\u3001\u5b89\u5168\u548c\u53ef\u8bbf\u95ee\u7684\u764c\u75c7\u76f8\u5173\u4fe1\u606f\u4ee5\u652f\u6301\u60a3\u8005\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f30\u4e86\u4e94\u79cd\u901a\u7528\u548c\u4e09\u79cd\u533b\u5b66LLMs\uff0c\u5305\u62ec\u8bed\u8a00\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3001\u6c9f\u901a\u53ef\u53ca\u6027\u548c\u611f\u67d3\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u5b9a\u91cf\u6307\u6807\u3001\u5b9a\u6027\u4e13\u5bb6\u8bc4\u5206\u548c\u7edf\u8ba1\u5206\u6790\uff08Welch's ANOVA\u3001Games-Howell\u548cHedges' g\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u901a\u7528LLMs\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u611f\u67d3\u529b\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800c\u533b\u5b66LLMs\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002\u7136\u800c\uff0c\u533b\u5b66LLMs\u503e\u5411\u4e8e\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6f5c\u5728\u5371\u5bb3\u3001\u6bd2\u6027\u548c\u504f\u89c1\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u5176\u5728\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u548c\u5b89\u5168\u6027\u5728\u5065\u5eb7\u901a\u4fe1\u4e2d\u5b58\u5728\u4e8c\u5143\u6027\u3002\u9700\u8981\u6709\u610f\u5730\u8bbe\u8ba1\u6a21\u578b\uff0c\u5e76\u9488\u5bf9\u51cf\u5c11\u4f24\u5bb3\u548c\u504f\u89c1\u4ee5\u53ca\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u611f\u67d3\u529b\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u764c\u75c7\u6c9f\u901a\u4e2d\u7684LLMs\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u4e3a\u6539\u8fdbAI\u751f\u6210\u7684\u5065\u5eb7\u5185\u5bb9\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u6307\u5bfc\u4e86\u672a\u6765\u51c6\u786e\u3001\u5b89\u5168\u548c\u53ef\u8bbf\u95ee\u7684\u6570\u5b57\u5065\u5eb7\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.10515", "pdf": "https://arxiv.org/pdf/2505.10515", "abs": "https://arxiv.org/abs/2505.10515", "authors": ["Seongun Kim", "Sol A Kim", "Geonhyeong Kim", "Enver Menadjiev", "Chanwoo Lee", "Seongwook Chung", "Nari Kim", "Jaesik Choi"], "title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.", "AI": {"tldr": "PnPXAI \u662f\u4e00\u79cd\u901a\u7528\u7684 XAI \u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709 XAI \u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u67b6\u6784\u3001\u63a8\u8350\u89e3\u91ca\u65b9\u6cd5\u548c\u4f18\u5316\u8d85\u53c2\u6570\u6765\u63d0\u9ad8\u89e3\u91ca\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684 XAI \u6846\u67b6\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u6a21\u6001\u7684\u7075\u6d3b\u6027\u4e0d\u8db3\u3001\u652f\u6301\u7684 XAI \u65b9\u6cd5\u6570\u91cf\u6709\u9650\u4ee5\u53ca\u7f3a\u4e4f\u8bc4\u4f30\u548c\u4f18\u5316\u9636\u6bb5\uff0c\u5bfc\u81f4\u96be\u4ee5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63a8\u5e7f XAI \u6280\u672f\u3002", "method": "PnPXAI \u91c7\u7528\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\uff08PnP\uff09\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u63a8\u8350\u9002\u7528\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316\u8d85\u53c2\u6570\u4ee5\u83b7\u5f97\u6700\u4f73\u89e3\u91ca\u6548\u679c\u3002", "result": "PnPXAI \u901a\u8fc7\u7528\u6237\u8c03\u67e5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u533b\u5b66\u548c\u91d1\u878d\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "PnPXAI \u662f\u4e00\u4e2a\u901a\u7528\u7684 XAI \u6846\u67b6\uff0c\u53ef\u4ee5\u652f\u6301\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u67b6\u6784\u3001\u63a8\u8350\u9002\u7528\u7684\u89e3\u91ca\u65b9\u6cd5\u548c\u4f18\u5316\u8d85\u53c2\u6570\u6765\u63d0\u4f9b\u6700\u4f73\u89e3\u91ca\u3002"}}
{"id": "2505.10484", "pdf": "https://arxiv.org/pdf/2505.10484", "abs": "https://arxiv.org/abs/2505.10484", "authors": ["Andrea Baisero", "Rupali Bhati", "Shuo Liu", "Aathira Pillai", "Christopher Amato"], "title": "Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Value function decomposition methods for cooperative multi-agent\nreinforcement learning compose joint values from individual per-agent\nutilities, and train them using a joint objective. To ensure that the action\nselection process between individual utilities and joint values remains\nconsistent, it is imperative for the composition to satisfy the\nindividual-global max (IGM) property. Although satisfying IGM itself is\nstraightforward, most existing methods (e.g., VDN, QMIX) have limited\nrepresentation capabilities and are unable to represent the full class of IGM\nvalues, and the one exception that has no such limitation (QPLEX) is\nunnecessarily complex. In this work, we present a simple formulation of the\nfull class of IGM values that naturally leads to the derivation of QFIX, a\nnovel family of value function decomposition models that expand the\nrepresentation capabilities of prior models by means of a thin \"fixing\" layer.\nWe derive multiple variants of QFIX, and implement three variants in two\nwell-known multi-agent frameworks. We perform an empirical evaluation on\nmultiple SMACv2 and Overcooked environments, which confirms that QFIX (i)\nsucceeds in enhancing the performance of prior methods, (ii) learns more stably\nand performs better than its main competitor QPLEX, and (iii) achieves this\nwhile employing the simplest and smallest mixing models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86QFIX\uff0c\u4e00\u79cd\u65b0\u7684\u4ef7\u503c\u51fd\u6570\u5206\u89e3\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u4ef7\u503c\u51fd\u6570\u5206\u89e3\u65b9\u6cd5\uff08\u5982VDN\u3001QMIX\uff09\u5728\u8868\u793a\u80fd\u529b\u4e0a\u6709\u9650\uff0c\u65e0\u6cd5\u8868\u793a\u6240\u6709IGM\u503c\uff0c\u800c\u552f\u4e00\u7684\u4f8b\u5916QPLEX\u5219\u8fc7\u4e8e\u590d\u6742\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u6574\u7684IGM\u503c\u7684\u7b80\u5355\u516c\u5f0f\uff0c\u5e76\u63a8\u5bfc\u51faQFIX\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u4ef7\u503c\u51fd\u6570\u5206\u89e3\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u4e00\u4e2a\u8584\u7684\u201c\u4fee\u590d\u201d\u5c42\u6269\u5c55\u4e86\u5148\u524d\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "QFIX\u5728\u591a\u4e2aSMACv2\u548cOvercooked\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u80fd\u591f\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5b66\u4e60\u66f4\u7a33\u5b9a\uff0c\u5e76\u4e14\u8868\u73b0\u4f18\u4e8eQPLEX\u3002", "conclusion": "QFIX\u80fd\u591f\u589e\u5f3a\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5b66\u4e60\u66f4\u7a33\u5b9a\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u6700\u7b80\u5355\u548c\u6700\u5c0f\u7684\u6df7\u5408\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u5176\u4e3b\u8981\u7ade\u4e89\u5bf9\u624bQPLEX\u3002"}}
{"id": "2505.10559", "pdf": "https://arxiv.org/pdf/2505.10559", "abs": "https://arxiv.org/abs/2505.10559", "authors": ["Ziming Liu", "Yizhou Liu", "Jeff Gore", "Max Tegmark"], "title": "Neural Thermodynamic Laws for Large Language Model Training", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "comment": "18 pages, 10 figures", "summary": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u795e\u7ecf\u70ed\u529b\u5b66\u5b9a\u5f8b\uff08NTL\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u70ed\u529b\u5b66\u91cf\u548c\u539f\u7406\uff0c\u5e76\u4e3a\u5b66\u4e60\u7387\u8c03\u5ea6\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89c4\u5f8b\uff0c\u7279\u522b\u662f\u5728\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u4e4b\u5916\u3002", "method": "\u901a\u8fc7\u5047\u8bbe\u6cb3\u6d41\u5c71\u8c37\u635f\u5931\u666f\u89c2\uff0c\u5c55\u793a\u4e86\u5173\u952e\u70ed\u529b\u5b66\u91cf\u548c\u7ecf\u5178\u70ed\u529b\u5b66\u539f\u7406\u7684\u81ea\u7136\u51fa\u73b0\u3002", "result": "NTL\u6846\u67b6\u63d0\u4f9b\u4e86\u5bf9LLM\u8bad\u7ec3\u52a8\u6001\u7684\u65b0\u89c1\u89e3\uff0c\u5e76\u4ea7\u751f\u4e86\u76f4\u89c2\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\u8bbe\u8ba1\u6307\u5357\u3002", "conclusion": "NTL\u6846\u67b6\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u5b66\u4e60\u7387\u8c03\u5ea6\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u6307\u5bfc\u3002"}}
{"id": "2505.10545", "pdf": "https://arxiv.org/pdf/2505.10545", "abs": "https://arxiv.org/abs/2505.10545", "authors": ["Amira Alakhdar", "Barnabas Poczos", "Newell Washburn"], "title": "Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design", "categories": ["cs.LG"], "comment": null, "summary": "Developing bioactive molecules remains a central, time- and cost-heavy\nchallenge in drug discovery, particularly for novel targets lacking structural\nor functional data. Pharmacophore modeling presents an alternative for\ncapturing the key features required for molecular bioactivity against a\nbiological target. In this work, we present PharmaDiff, a\npharmacophore-conditioned diffusion model for 3D molecular generation.\nPharmaDiff employs a transformer-based architecture to integrate an atom-based\nrepresentation of the 3D pharmacophore into the generative process, enabling\nthe precise generation of 3D molecular graphs that align with predefined\npharmacophore hypotheses. Through comprehensive testing, PharmaDiff\ndemonstrates superior performance in matching 3D pharmacophore constraints\ncompared to ligand-based drug design methods. Additionally, it achieves higher\ndocking scores across a range of proteins in structure-based drug design,\nwithout the need for target protein structures. By integrating pharmacophore\nmodeling with 3D generative techniques, PharmaDiff offers a powerful and\nflexible framework for rational drug design.", "AI": {"tldr": "PharmaDiff is a pharmacophore-conditioned diffusion model that integrates an atom-based representation of the 3D pharmacophore into the generative process, enabling precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses. It outperforms ligand-based drug design methods and achieves higher docking scores without requiring target protein structures.", "motivation": "Developing bioactive molecules remains a central, time- and cost-heavy challenge in drug discovery, particularly for novel targets lacking structural or functional data. Pharmacophore modeling presents an alternative for capturing the key features required for molecular bioactivity against a biological target.", "method": "PharmaDiff is a pharmacophore-conditioned diffusion model that uses a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process.", "result": "PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods. Additionally, it achieves higher docking scores across a range of proteins in structure-based drug design, without the need for target protein structures.", "conclusion": "PharmaDiff offers a powerful and flexible framework for rational drug design by integrating pharmacophore modeling with 3D generative techniques."}}
{"id": "2505.10556", "pdf": "https://arxiv.org/pdf/2505.10556", "abs": "https://arxiv.org/abs/2505.10556", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "categories": ["cs.LG", "physics.ao-ph"], "comment": "Kermani and Naderi share first authorship. 20 pages, 6 figures and 1\n  table", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u7a7f\u6234\u8bbe\u5907\u751f\u7406\u6570\u636e\u548c\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u6570\u636e\uff0c\u5229\u7528AI\u6a21\u578b\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u7684\u5065\u5eb7\u53cd\u5e94\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7a7a\u6c14\u6c61\u67d3\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u800c\u6c14\u5019\u53d8\u5316\u5e26\u6765\u7684\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u6c61\u67d3\u6c34\u5e73\u3002\u8fd1\u5e74\u6765\uff0c\u4e2a\u4eba\u4f20\u611f\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97\u884c\u4e3a\u548c\u751f\u7406\u6570\u636e\u7684\u6536\u96c6\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u5e26\u6765\u4e86\u65b0\u7684\u6539\u8fdb\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u548cAI\u6280\u672f\u6765\u76d1\u6d4b\u548c\u9884\u6d4b\u4e2a\u4f53\u5065\u5eb7\u7ed3\u679c\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u7a7f\u6234\u8bbe\u5907\u751f\u7406\u6570\u636e\u548c\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5229\u7528AI\u6a21\u578b\uff08\u5982\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\uff09\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4ee5\u76d1\u6d4b\u548c\u9884\u6d4b\u4e2a\u4f53\u5065\u5eb7\u7ed3\u679c\u3002\u540c\u65f6\u5e94\u7528\u4e86\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "AI\u6a21\u578b\uff08\u5982\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\uff09\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u65f6\u95f4\u4f9d\u8d56\u7684\u5065\u5eb7\u4fe1\u53f7\uff0c\u5e76\u6355\u6349\u6c61\u67d3\u7684\u975e\u7ebf\u6027\u53cd\u5e94\u3002\u901a\u8fc7\u4f7f\u7528\u4e2a\u4eba\u667a\u80fd\u624b\u8868\u6570\u636e\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86AI\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7528\u6237\u751f\u6210\u6570\u636e\u4e2d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u7a7f\u6234\u5065\u8eab\u8bbe\u5907\u7684\u751f\u7406\u6570\u636e\u548c\u5b9e\u65f6\u73af\u5883\u66b4\u9732\u6570\u636e\uff0c\u9884\u6d4b\u4e2a\u4eba\u5bf9\u6c61\u67d3\u7684\u5065\u5eb7\u53cd\u5e94\u3002AI\u6a21\u578b\u5728\u4e91\u57fa\u7840\u6a21\u5757\u5316\u6846\u67b6\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u65f6\u95f4\u4f9d\u8d56\u7684\u5065\u5eb7\u4fe1\u53f7\uff0c\u5e76\u6355\u6349\u6c61\u67d3\u7684\u975e\u7ebf\u6027\u53cd\u5e94\u3002\u8f6c\u79fb\u5b66\u4e60\u7684\u5e94\u7528\u63d0\u9ad8\u4e86AI\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7528\u6237\u751f\u6210\u6570\u636e\u4e2d\u7684\u9002\u5e94\u6027\u3002"}}
