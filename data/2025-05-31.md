<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 163]
- [cs.AI](#cs.AI) [总数: 37]
- [stat.ML](#stat.ML) [总数: 10]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs](https://arxiv.org/abs/2505.22686)
*Ange-Clement Akazan, Verlon Roel Mbingui, Gnankan Landry Regis N'guessan, Issa Karambal*

**主要类别:** cs.LG

**概要:** 本研究在热带非洲的两个城市阿比让和基加立，使用深度递归神经网络（LSTM、GRU、BiLSTM、BiGRU）和基于Kolmogorov-Arnold模型（KAN 和 TKAN）进行每日温度、降水和气压预测。引入了两种改进的TKAN变体，分别采用GeLU和MiSH激活函数。KAN在温度预测中表现优异（R²接近1），而TKAN变体在低降雨量条件下的降水预测中表现出色。对于气压预测，传统RNNs具有竞争力，优于KAN模型。


<details>
  <summary>更多</summary>
  
**动机:** 热带非洲地区天气模式复杂且非线性，现有的预报方法难以应对这些挑战。准确的天气预报对管理风险和经济规划至关重要，特别是在极端天气事件对生计有严重影响的地区。因此，需要探索更高效的天气预报方法以适应该地区的特殊需求。

**方法:** 研究采用了多种深度学习模型进行天气预报，包括深度递归神经网络（LSTM、GRU、BiLSTM、BiGRU）和基于Kolmogorov-Arnold的模型（KAN 和 TKAN）。此外，还开发了两种定制的TKAN变体，将原始SiLU激活函数替换为GeLU和MiSH。使用2010年至2024年的气象站数据对所有模型进行了评估，涵盖了温度、降水和气压三个指标。

**结果:** KAN模型在温度预测中表现出色，其R²值接近1，MSE小于0.0014°C²。对于降水预测，TKAN变体在低降雨量条件下能有效减少绝对误差。在气压预测方面，传统的RNNs（如LSTM和GRU）显示出高度竞争力，其R²值约为0.83至0.86，优于KAN模型。定制的TKAN模型在所有数据集上都优于标准TKAN。

**结论:** 基于样条的神经架构（如KAN和TKAN）在天气预报中具有巨大潜力，特别是在温度和降水预测中。尽管传统RNNs在气压预测中表现更好，但整体结果表明，这些新模型能够提供高效和数据驱动的预报方案，适合复杂的热带天气模式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Localized+Weather+Prediction+Using+Kolmogorov-Arnold+Network-Based+Models+and+Deep+RNNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22686，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22686&send_immediately=true&force_search=false)

**原文摘要:** Weather forecasting is crucial for managing risks and economic planning,
particularly in tropical Africa, where extreme events severely impact
livelihoods. Yet, existing forecasting methods often struggle with the region's
complex, non-linear weather patterns. This study benchmarks deep recurrent
neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and
Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily
forecasting of temperature, precipitation, and pressure in two tropical cities:
Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce
two customized variants of $ \texttt{TKAN}$ that replace its original
$\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH},
respectively. Using station-level meteorological data spanning from 2010 to
2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$
achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,
$\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize
absolute errors for precipitation forecasting in low-rainfall regimes. The
customized $\texttt{TKAN}$ models demonstrate improvements over the standard
$\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly
competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming
$\texttt{KAN}$-based models in this task. These results highlight the potential
of spline-based neural architectures for efficient and data-efficient
forecasting.

</details>


### [2] [SlimLLM: Accurate Structured Pruning for Large Language Models](https://arxiv.org/abs/2505.22689)
*Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang*

**主要类别:** cs.LG

**概要:** Large language models (LLMs) have great capabilities but high computational costs. SlimLLM, a new structured pruning method, evaluates sub-module importance holistically and uses strategies like linear regression and layer-based ratios to minimize performance loss. It outperforms other methods on LLaMA benchmark.


<details>
  <summary>更多</summary>
  
**动机:** To reduce the enormous computational costs associated with deploying and applying large language models (LLMs), making them more efficient without significant performance loss.

**方法:** SlimLLM evaluates the importance of channels and attention heads based on the entire unit rather than individual elements, considers interdependence among elements, uses linear regression for output matrix recovery, and proposes layer-based importance ratio for determining pruning ratios.

**结果:** SlimLLM achieves state-of-the-art performance on the LLaMA benchmark, outperforming other methods.

**结论:** SlimLLM is an effective and fast structured pruning method that successfully reduces the parameters of LLMs while maintaining or improving performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SlimLLM%3A+Accurate+Structured+Pruning+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22689&send_immediately=true&force_search=false)

**原文摘要:** Large language models(LLMs) have garnered significant attention and
demonstrated impressive capabilities in a wide range of applications. However,
due to their enormous computational costs, the deployment and application of
LLMs are often severely limited. To address this issue, structured pruning is
an effective solution to compress the parameters of LLMs. Determining the
importance of each sub-module in LLMs and minimizing performance loss are
critical issues that need to be carefully addressed in structured pruning. In
this paper, we propose an effective and fast structured pruning method named
SlimLLM for large language models. For channel and attention head pruning, we
evaluate the importance based on the entire channel or head, rather than merely
aggregating the importance of individual elements within a sub-module. This
approach enables a more holistic consideration of the interdependence among
elements within the sub-module. In addition, we design a simple linear
regression strategy for the output matrix to quickly recover performance. We
also propose layer-based importance ratio to determine the pruning ratio for
each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other
methods and achieves state-of-the-art performance.

</details>


### [3] [MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning](https://arxiv.org/abs/2505.22694)
*Dacao Zhang, Kun Zhang, Shimao Chu, Le Wu, Xin Li, Si Wei*

**主要类别:** cs.LG

**概要:** 提出了一种新的多任务参数高效微调方法MoRE，通过联合训练低秩专家和自适应选择器，在不增加推理成本的情况下显著提高了LLM在多任务场景中的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LoRA方法及其改进版要么专注于单任务场景，要么为多任务分别训练多个LoRA模块，这限制了其在多任务场景中的效率和效果。

**方法:** 提出了一种名为Mixture of Low-Rank Experts (MoRE)的新方法，用以改善多任务参数高效微调。该方法通过将不同任务与不同秩的LoRA模块（称为低秩专家）对齐，并设计了一个新的自适应秩选择器来为每个任务选择合适的专家。通过联合训练这些低秩专家，MoRE增强了LoRA在多任务场景中的适应性和效率。

**结果:** 广泛的实验结果表明，相较于传统的LoRA及其变体，MoRE显著提升了LLM在多任务场景中的性能，并且没有增加额外的推理成本。

**结论:** MoRE是一种有效的多任务PEFT方法，它不仅提升了LLM在多任务场景中的性能，而且保持了与传统LoRA相同的推理成本。此外，作者还开源了模型和代码，促进了社区的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoRE%3A+A+Mixture+of+Low-Rank+Experts+for+Adaptive+Multi-Task+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22694，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22694&send_immediately=true&force_search=false)

**原文摘要:** With the rapid development of Large Language Models (LLMs),
Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
attention, which aims to achieve efficient fine-tuning of LLMs with fewer
parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)
introduces low-rank matrices to approximate the incremental tuning parameters
and achieves impressive performance over multiple scenarios. After that, plenty
of improvements have been proposed for further improvement. However, these
methods either focus on single-task scenarios or separately train multiple LoRA
modules for multi-task scenarios, limiting the efficiency and effectiveness of
LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in
this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for
multi-task PEFT. Specifically, instead of using an individual LoRA for each
task, we align different ranks of LoRA module with different tasks, which we
named low-rank experts. Moreover, we design a novel adaptive rank selector to
select the appropriate expert for each task. By jointly training low-rank
experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task
scenarios. Finally, we conduct extensive experiments over multiple multi-task
benchmarks along with different LLMs to verify model performance. Experimental
results demonstrate that compared to traditional LoRA and its variants, MoRE
significantly improves the performance of LLMs in multi-task scenarios and
incurs no additional inference cost. We also release the model and code to
facilitate the community.

</details>


### [4] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu, Siyuan Feng, Hao Liu, Hai Yang*

**主要类别:** cs.LG

**概要:** 提出LLM-ODDR框架，利用大型语言模型优化网约车订单分配和司机重新定位操作，解决传统方法忽视司机收入公平性、可解释性和适应性的问题。该框架包含多目标引导的订单价值评估、公平意识的订单分配和时空需求感知的司机重新定位三个关键组件。实验表明，LLM-ODDR在实际数据集上显著优于传统方法，并且是首次将LLMs应用于网约车ODDR任务的研究。


<details>
  <summary>更多</summary>
  
**动机:** 网约车平台在动态城市环境中面临优化订单分配和司机重新定位操作的重大挑战。传统的组合优化、基于规则的启发式算法和强化学习方法往往忽视了司机收入公平性、可解释性和对现实世界动态的适应性。

**方法:** LLM-ODDR框架包括三个关键组件：1）多目标引导的订单价值精炼，通过考虑多个目标来评估订单的整体价值；2）公平意识的订单分配，平衡平台收入与司机收入公平性；3）时空需求感知的司机重新定位，根据历史模式和预测供应优化空闲车辆的放置。此外，还开发了JointDR-GPT，一个针对ODDR任务优化的微调模型，具有领域知识。

**结果:** 在曼哈顿出租车运营的实际数据集上的广泛实验表明，该框架在有效性、对异常条件的适应性和决策可解释性方面显著优于传统方法。

**结论:** 这是首次探索LLMs作为网约车ODDR任务中的决策代理，为在智能交通系统中集成先进的语言模型提供了基础见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-ODDR%3A+A+Large+Language+Model+Framework+for+Joint+Order+Dispatching+and+Driver+Repositioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22695&send_immediately=true&force_search=false)

**原文摘要:** Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


### [5] [When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?](https://arxiv.org/abs/2505.22696)
*Eleni Nisioti, Joachim Winther Pedersen, Erwan Plantec, Milton L. Montero, Sebastian Risi*

**主要类别:** cs.LG

**概要:** 研究探讨了神经进化（NE）在技能迁移中的潜力，并通过两个基准测试表明，NE方法在不同任务间的迁移能力优于强化学习（RL）。


<details>
  <summary>更多</summary>
  
**动机:** 技能在任务间持续高效地迁移是生物智能的标志，也是人工系统长期追求的目标。然而，强化学习（RL）在面对任务变化时表现出脆弱性和灾难性遗忘问题。神经进化（NE）因其鲁棒性、可扩展性和逃离局部最优的能力而受到关注，但其迁移学习能力尚未被深入研究。

**方法:** 研究引入了两个基准测试：1) stepping gates，要求神经网络模仿逻辑电路，重点在于模块化重复和变化；2) ecorobot，在Brax物理引擎中加入墙壁和障碍物等对象，并允许轻松切换不同的机器人形态。两者都包含课程设计，用于评估在复杂度递增的任务间进行技能迁移的能力。

**结果:** 实证分析显示，NE方法在迁移能力上存在差异，且通常优于RL基线方法。

**结论:** 研究表明NE具有构建更适应性强的智能体的潜力，同时也指出了将NE扩展到复杂现实世界问题时面临的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Does+Neuroevolution+Outcompete+Reinforcement+Learning+in+Transfer+Learning+Tasks%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22696&send_immediately=true&force_search=false)

**原文摘要:** The ability to continuously and efficiently transfer skills across tasks is a
hallmark of biological intelligence and a long-standing goal in artificial
systems. Reinforcement learning (RL), a dominant paradigm for learning in
high-dimensional control tasks, is known to suffer from brittleness to task
variations and catastrophic forgetting. Neuroevolution (NE) has recently gained
attention for its robustness, scalability, and capacity to escape local optima.
In this paper, we investigate an understudied dimension of NE: its transfer
learning capabilities. To this end, we introduce two benchmarks: a) in stepping
gates, neural networks are tasked with emulating logic circuits, with designs
that emphasize modular repetition and variation b) ecorobot extends the Brax
physics engine with objects such as walls and obstacles and the ability to
easily switch between different robotic morphologies. Crucial in both
benchmarks is the presence of a curriculum that enables evaluating skill
transfer across tasks of increasing complexity. Our empirical analysis shows
that NE methods vary in their transfer abilities and frequently outperform RL
baselines. Our findings support the potential of NE as a foundation for
building more adaptable agents and highlight future challenges for scaling NE
to complex, real-world problems.

</details>


### [6] [Update Your Transformer to the Latest Release: Re-Basin of Task Vectors](https://arxiv.org/abs/2505.22697)
*Filippo Rinaldi, Giacomo Capitani, Lorenzo Bonicelli, Donato Crisostomi, Federico Bolelli, Elisa Ficarra, Emanuele Rodolà, Simone Calderara, Angelo Porrello*

**主要类别:** cs.LG

**概要:** 本论文研究了如何在无需重新训练和数据的情况下，将微调知识无缝转移到新的预训练模型中。提出了一种基于权重排列的两层方法，适用于Transformer模型，并通过大量实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当基础预训练模型更新或重新训练时，微调模型会变得过时，需要重新训练。因此，研究如何在无需重新训练的情况下，将微调知识转移到新的模型版本上是一个重要问题。

**方法:** 论文从模型重盆地（model re-basin）原则出发，提出了一种基于权重排列的方法来重新调整原始基础模型的修改（即任务向量）。特别地，该方法针对Transformer模型设计，考虑了残差连接和多头注意力层的挑战。具体来说，提出了一种基于谱理论的两层方法：首先对注意力头进行排列，然后调整选定头对内的参数。

**结果:** 通过在视觉和文本任务上的广泛实验，该方法成功实现了将微调知识无缝转移到新的预训练模型中，且无需任何训练步骤或数据点。

**结论:** 本文提出了一种无需重新训练和数据的方法，可以将微调知识转移到新的预训练模型中，特别是在Transformer架构中表现良好。这为未来减少模型更新带来的重复劳动提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Update+Your+Transformer+to+the+Latest+Release%3A+Re-Basin+of+Task+Vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22697，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22697&send_immediately=true&force_search=false)

**原文摘要:** Foundation models serve as the backbone for numerous specialized models
developed through fine-tuning. However, when the underlying pretrained model is
updated or retrained (e.g., on larger and more curated datasets), the
fine-tuned model becomes obsolete, losing its utility and requiring retraining.
This raises the question: is it possible to transfer fine-tuning to a new
release of the model? In this work, we investigate how to transfer fine-tuning
to a new checkpoint without having to re-train, in a data-free manner. To do
so, we draw principles from model re-basin and provide a recipe based on weight
permutations to re-base the modifications made to the original base model,
often called task vector. In particular, our approach tailors model re-basin
for Transformer models, taking into account the challenges of residual
connections and multi-head attention layers. Specifically, we propose a
two-level method rooted in spectral theory, initially permuting the attention
heads and subsequently adjusting parameters within select pairs of heads.
Through extensive experiments on visual and textual tasks, we achieve the
seamless transfer of fine-tuned knowledge to new pre-trained backbones without
relying on a single training step or datapoint. Code is available at
https://github.com/aimagelab/TransFusion.

</details>


### [7] [Private Rate-Constrained Optimization with Applications to Fair Learning](https://arxiv.org/abs/2505.22703)
*Mohammad Yaghini, Tudor Cebere, Michael Menart, Aurélien Bellet, Nicolas Papernot*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为RaCO-DP的新算法，用于在差分隐私（DP）环境下解决具有预测率约束的模型优化问题。该方法通过估计每个优化步骤中的小批量直方图来处理样本间的依赖性，并证明了算法的收敛性。实验表明，在群体公平性约束下，该方法在公平性和效用的权衡上优于现有的隐私学习方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的差分隐私优化技术（如DP-SGD）依赖于损失函数能够分解为每个样本的贡献。然而，当引入预测率约束时，这些约束会导致样本间产生依赖性，违反了解损失函数的可分解性要求。因此，需要一种新的方法来解决这种带有约束的优化问题。

**方法:** 作者开发了RaCO-DP算法，这是一种差分隐私版本的随机梯度下降上升（SGDA）算法，用于解决带预测率约束的问题。通过将拉格朗日公式应用于问题中，RaCO-DP解决了样本间的依赖性，并将额外的隐私成本归结为在每个优化步骤中对小批量数据进行私有直方图估计。

**结果:** 理论分析表明，RaCO-DP算法通过利用对偶参数的线性结构，成功证明了其收敛性。此外，实证结果展示了该方法在群体公平性约束下的学习任务中，能够在公平性和效用的权衡上显著优于现有的隐私学习方法。

**结论:** RaCO-DP算法为在差分隐私环境下解决带预测率约束的优化问题提供了一种有效的方法。它不仅解决了样本间依赖性带来的挑战，还通过实验证明了其在公平性和效用权衡上的优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Private+Rate-Constrained+Optimization+with+Applications+to+Fair+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22703&send_immediately=true&force_search=false)

**原文摘要:** Many problems in trustworthy ML can be formulated as minimization of the
model error under constraints on the prediction rates of the model for
suitably-chosen marginals, including most group fairness constraints
(demographic parity, equality of odds, etc.). In this work, we study such
constrained minimization problems under differential privacy (DP). Standard DP
optimization techniques like DP-SGD rely on the loss function's decomposability
into per-sample contributions. However, rate constraints introduce inter-sample
dependencies, violating the decomposability requirement. To address this, we
develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)
algorithm which solves the Lagrangian formulation of rate constraint problems.
We demonstrate that the additional privacy cost of incorporating these
constraints reduces to privately estimating a histogram over the mini-batch at
each optimization step. We prove the convergence of our algorithm through a
novel analysis of SGDA that leverages the linear structure of the dual
parameter. Finally, empirical results on learning under group fairness
constraints demonstrate that our method Pareto-dominates existing private
learning approaches in fairness-utility trade-offs.

</details>


### [8] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
*Aniruddha Nrusimha, William Brandon, Mayank Mishra, Yikang Shen, Rameswar Panda, Jonathan Ragan-Kelley, Yoon Kim*

**主要类别:** cs.LG

**概要:** 本论文提出了FlashFormer，这是一个为加速基于变压器的大语言模型单一批次推理而设计的概念验证内核。在不同的模型尺寸和量化设置下，相较于现有的最先进的推理内核，观察到了显著的速度提升。


<details>
  <summary>更多</summary>
  
**动机:** 现代大型语言模型的规模和计算特性促使了对训练和推理进行优化的专用内核的开发兴趣。然而，尽管现有内核主要针对大批次训练和推理设置优化计算利用率，低批次推理（其中内存带宽和内核启动开销是重要因素）对于边缘部署和延迟敏感型应用等许多应用场景仍然重要。

**方法:** 描述并实现了一个名为FlashFormer的概念验证内核，该内核专注于加速基于变压器的大语言模型的单一批次推理。

**结果:** 在各种模型尺寸和量化设置下，与现有的最先进推理内核相比，观察到了非平凡的速度提升。

**结论:** FlashFormer作为针对单一批次推理优化的内核，在提升推理速度方面展现了潜力，尤其适用于低批次推理场景，如边缘部署和延迟敏感型应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlashFormer%3A+Whole-Model+Kernels+for+Efficient+Low-Batch+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22758，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22758&send_immediately=true&force_search=false)

**原文摘要:** The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [9] [Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study](https://arxiv.org/abs/2505.22841)
*Franck Gabriel, François Ged, Maria Han Veiga, Emmanuel Schertzer*

**主要类别:** cs.LG

**概要:** This paper investigates the issue of memorization in diffusion models and proposes a kernel-smoothed empirical score method to regularize the model, demonstrating both theoretically and empirically that this approach enhances generalization and reduces memorization.


<details>
  <summary>更多</summary>
  
**动机:** Diffusion models, despite their success in high-fidelity generative sampling, are prone to memorization due to overfitting of the learned score function to the finite dataset.

**方法:** Interpreting the empirical score as a noisy version of the true score and introducing a kernel-smoothed empirical score to reduce variance. Analyzing the bias-variance trade-off and deriving asymptotic bounds on the Kullback-Leibler divergence between the true and generated distributions.

**结果:** Theoretical analysis shows that regularization on the score function is equivalent to increasing the size of the training dataset, thus helping to prevent memorization. Experiments on synthetic and MNIST datasets illustrate these findings.

**结论:** Diffusion models can be regularized effectively through kernel-smoothed empirical scores, which helps prevent memorization and enhances generalization. Even simple regularization methods can significantly improve the model's performance without additional learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kernel-Smoothed+Scores+for+Denoising+Diffusion%3A+A+Bias-Variance+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22841&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models now set the benchmark in high-fidelity generative sampling,
yet they can, in principle, be prone to memorization. In this case, their
learned score overfits the finite dataset so that the reverse-time SDE samples
are mostly training points. In this paper, we interpret the empirical score as
a noisy version of the true score and show that its covariance matrix is
asymptotically a re-weighted data PCA. In large dimension, the small time limit
makes the noise variance blow up while simultaneously reducing spatial
correlation. To reduce this variance, we introduce a kernel-smoothed empirical
score and analyze its bias-variance trade-off. We derive asymptotic bounds on
the Kullback-Leibler divergence between the true distribution and the one
generated by the modified reverse SDE. Regularization on the score has the same
effect as increasing the size of the training dataset, and thus helps prevent
memorization. A spectral decomposition of the forward diffusion suggests better
variance control under some regularity conditions of the true data
distribution. Reverse diffusion with kernel-smoothed empirical score can be
reformulated as a gradient descent drifted toward a Log-Exponential
Double-Kernel Density Estimator (LED-KDE). This perspective highlights two
regularization mechanisms taking place in denoising diffusions: an initial
Gaussian kernel first diffuses mass isotropically in the ambient space, while a
second kernel applied in score space concentrates and spreads that mass along
the data manifold. Hence, even a straightforward regularization-without any
learning-already mitigates memorization and enhances generalization.
Numerically, we illustrate our results with several experiments on synthetic
and MNIST datasets.

</details>


### [10] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/abs/2505.22764)
*Divya Shanmugam, Helen Lu, Swami Sankaranarayanan, John Guttag*

**主要类别:** cs.LG

**概要:** 本文探讨了在推理阶段使用测试时增强（TTA）技术来减少共形分类器产生的预测集合大小的方法，该方法灵活、计算高效且无需重新训练模型，在多个数据集和模型上的实验表明其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 共形分类器虽然能够提供预测类别集合包含真实类别的概率保证，但常常生成过大的预测集合，缺乏实用性。

**方法:** 提出利用测试时增强（TTA）技术，通过在推理过程中引入归纳偏置，减少共形分类器生成的预测集合大小。此方法可与任何共形评分结合，无需重新训练模型。

**结果:** 实验跨越三个数据集、三种模型、两种已建立的共形评分方法等，结果表明TTA能平均减少10%-14%的预测集合大小。

**结论:** 测试时增强是共形分类器管道中有用的补充，特别是在处理不同分布偏移时，能有效减小预测集合大小。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-time+augmentation+improves+efficiency+in+conformal+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22764，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22764&send_immediately=true&force_search=false)

**原文摘要:** A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [11] [Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints](https://arxiv.org/abs/2505.22881)
*Hyungki Im, Wyame Benslimane, Paul Grigas*

**主要类别:** cs.LG

**概要:** 本研究扩展了情境随机线性优化（CSLO），处理依赖于机器学习模型预测的不确定参数的不等式约束。通过构建情境不确定性集合，引入了SPO-RC损失函数及其凸代理SPO-RC+，并证明了Fisher一致性。实验表明，结合截断数据集和重要性重加权技术可以有效应对约束中的不确定性并提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有文献中大多数情境随机线性优化研究未涉及依赖于不确定参数预测的不等式约束，而这些约束在实际问题中很常见。因此，需要一种方法来处理这种约束的不确定性。

**方法:** 使用情境不确定性集合（如通过符合预测方法构建）来处理约束不确定性；引入SPO-RC损失函数，测量决策误差；提出其凸代理SPO-RC+并证明其Fisher一致性；通过在截断数据集上训练并使用重要性重加权技术纠正样本选择偏差。

**结果:** 在分数背包和合金生产问题实例的实验中，SPO-RC+能够有效应对约束中的不确定性，并且结合截断数据集与重要性重加权技术进一步提升了性能。

**结论:** 所提出的SPO-RC+方法及改进技术为处理具有不确定约束的情境随机线性优化问题提供了一种有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smart+Surrogate+Losses+for+Contextual+Stochastic+Linear+Optimization+with+Robust+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22881&send_immediately=true&force_search=false)

**原文摘要:** We study an extension of contextual stochastic linear optimization (CSLO)
that, in contrast to most of the existing literature, involves inequality
constraints that depend on uncertain parameters predicted by a machine learning
model. To handle the constraint uncertainty, we use contextual uncertainty sets
constructed via methods like conformal prediction. Given a contextual
uncertainty set method, we introduce the "Smart Predict-then-Optimize with
Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the
SPO loss that measures decision error of predicted objective parameters. We
also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with
SPO-RC. To enhance performance, we train on truncated datasets where true
constraint parameters lie within the uncertainty sets, and we correct the
induced sample selection bias using importance reweighting techniques. Through
experiments on fractional knapsack and alloy production problem instances, we
demonstrate that SPO-RC+ effectively handles uncertainty in constraints and
that combining truncation with importance reweighting can further improve
performance.

</details>


### [12] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/abs/2505.22768)
*Mert Onur Cakiroglu, Idil Bilge Altun, Hasan Kurban, Elham Buxton, Mehmet Dalkilic*

**主要类别:** cs.LG

**概要:** DRAGON是一种新的编码器，它使用多变量de Bruijn图（MdBGs）将符号表示与神经建模连接起来。通过离散化连续序列并将其映射到固定的图结构上，利用基于图的注意力机制实现动态上下文恢复，并增强传统的基于CNN的编码器。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列预测对于基础模型来说仍然是一项具有挑战性的任务，因为存在时间异质性、高维度和缺乏固有的符号结构等问题。为了克服这些挑战，需要一种方法来弥合符号表示与神经网络建模之间的差距。

**方法:** 提出了一种名为DRAGON的新型编码器，引入了多变量de Bruijn图（MdBGs），将连续输入序列离散化，并将其映射到固定的图结构上，通过图基注意力机制实现动态上下文恢复，并作为辅助模块集成到双分支架构中，增强传统基于CNN的编码器。

**结果:** 实验结果表明，DRAGON能够有效地增强传统基于CNN的编码器，改善时间序列预测的性能。

**结论:** DRAGON提供了一种新的途径来解决时间序列预测中的挑战，特别是通过结合符号表示和神经网络建模，可以显著提高预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multivariate+de+Bruijn+Graphs%3A+A+Symbolic+Graph+Framework+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22768&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [13] [Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing](https://arxiv.org/abs/2505.23320)
*Connor Cooper, Geoffrey I. Webb, Daniel F. Schmidt*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的参数估计方法，使用对数线性回归来近似分层狄利克雷过程(HDPs)的行为。该方法具有灵活性和简单性，并且在处理分类数据时，不仅性能优于HDP平滑技术，而且速度提高了几个数量级，同时与随机森林的性能相当。


<details>
  <summary>更多</summary>
  
**动机:** 贝叶斯网络分类器(BNCs)尽管具有可解释性、高度可扩展性和可调复杂度的优点，但传统学习BNCs的方法在性能上一直逊色于领先的分类方法（如随机森林）。虽然最近采用分层狄利克雷过程(HDPs)的参数平滑技术使BNCs在分类数据上的表现与随机森林相当，但这些技术相对僵化，且需要复杂的采样过程。

**方法:** 作者引入了一种新的参数估计方法，利用对数线性回归来近似HDPs的行为。作为一种线性模型，这种方法非常灵活且易于解释，还可以利用大量关于学习线性模型的文献。

**结果:** 实验结果表明，新方法在性能上可以超越HDP平滑技术，同时速度提高了几个数量级，并且在分类数据上仍然能够与随机森林保持竞争力。

**结论:** 提出的对数线性回归方法为BNCs的参数估计提供了一个更灵活和高效的替代方案，显著提高了计算效率，同时保持了较高的分类性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Parameter+Estimation+for+Bayesian+Network+Classifiers+using+Hierarchical+Linear+Smoothing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23320&send_immediately=true&force_search=false)

**原文摘要:** Bayesian network classifiers (BNCs) possess a number of properties desirable
for a modern classifier: They are easily interpretable, highly scalable, and
offer adaptable complexity. However, traditional methods for learning BNCs have
historically underperformed when compared to leading classification methods
such as random forests. Recent parameter smoothing techniques using
hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve
performance competitive with random forests on categorical data, but these
techniques are relatively inflexible, and require a complicated, specialized
sampling process. In this paper, we introduce a novel method for parameter
estimation that uses a log-linear regression to approximate the behaviour of
HDPs. As a linear model, our method is remarkably flexible and simple to
interpret, and can leverage the vast literature on learning linear models. Our
experiments show that our method can outperform HDP smoothing while being
orders of magnitude faster, remaining competitive with random forests on
categorical data.

</details>


### [14] [Calibrated Value-Aware Model Learning with Stochastic Environment Models](https://arxiv.org/abs/2505.22772)
*Claas Voelcker, Anastasiia Pedan, Arash Ahmadian, Romina Abachi, Igor Gilitschenski, Amir-massoud Farahmand*

**主要类别:** cs.LG

**概要:** 这篇论文探讨了价值感知模型学习的损失函数，特别是MuZero损失，并揭示其作为未校准代理损失的问题，提出了解决方案并研究了确定性和随机模型在预测准确值方面的作用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管MuZero损失在实际应用中被广泛使用，但对其理论上的优缺点的研究却很少。

**方法:** 分析价值感知模型学习的损失函数家族（包括MuZero损失），揭示其未校准问题，并提出修正方法；同时研究损失校准、潜在模型架构和辅助损失之间的相互作用。

**结果:** 发现这些常用损失并非总是能恢复正确的模型和价值函数，并证明即使确定性模型足以预测准确值，学习校准的随机模型仍具优势。

**结论:** 提出了对MuZero损失的修正以解决未校准问题，并强调了校准随机模型的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Calibrated+Value-Aware+Model+Learning+with+Stochastic+Environment+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22772，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22772&send_immediately=true&force_search=false)

**原文摘要:** The idea of value-aware model learning, that models should produce accurate
value estimates, has gained prominence in model-based reinforcement learning.
The MuZero loss, which penalizes a model's value function prediction compared
to the ground-truth value function, has been utilized in several prominent
empirical works in the literature. However, theoretical investigation into its
strengths and weaknesses is limited. In this paper, we analyze the family of
value-aware model learning losses, which includes the popular MuZero loss. We
show that these losses, as normally used, are uncalibrated surrogate losses,
which means that they do not always recover the correct model and value
function. Building on this insight, we propose corrections to solve this issue.
Furthermore, we investigate the interplay between the loss calibration, latent
model architectures, and auxiliary losses that are commonly employed when
training MuZero-style agents. We show that while deterministic models can be
sufficient to predict accurate values, learning calibrated stochastic models is
still advantageous.

</details>


### [15] [Epistemic Errors of Imperfect Multitask Learners When Distributions Shift](https://arxiv.org/abs/2505.23496)
*Sabina J. Sloman, Michele Caprio, Samuel Kaski*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种原则性的认识误差定义，并提供了通用的分解认识误差界限，首次专门考虑认识误差并涵盖所有认识不确定性来源。此外，还提供了贝叶斯迁移学习和分布偏移情况下的认识误差界限及泛化界限，以及对负迁移的新定义和验证。


<details>
  <summary>更多</summary>
  
**动机:** 在数据存在噪声的情况下，统计学习的目标是解决测试时遇到的数据的认识不确定性。然而，在许多现实世界的学习场景中，仅凭训练数据无法解决这些不确定性问题，例如多任务学习、目标数据与源数据系统性差异（分布偏移）以及不完美的学习过程。因此，需要一种新的方法来定义和量化认识误差。

**方法:** 作者引入了认识误差的原则性定义，并提供了一个通用的、可分解的认识误差界限。该误差界限能够单独归因于学习过程和环境的多个方面。同时，还推导出针对贝叶斯迁移学习和分布偏移情况下的认识误差界限及相关泛化界限。最后，提出了负迁移的新定义，并通过合成实验验证其见解。

**结果:** 通过理论分析和实验验证，证明了所提出的认识误差界限的有效性和适用性，特别是在贝叶斯迁移学习和分布偏移等场景中。同时，新定义的负迁移也得到了实验支持。

**结论:** 本文为认识误差提供了一个新的理论框架，有助于更好地理解和量化统计学习中的认识不确定性。这一框架可以应用于多种学习场景，并为进一步研究迁移学习和分布偏移等问题提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Epistemic+Errors+of+Imperfect+Multitask+Learners+When+Distributions+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23496&send_immediately=true&force_search=false)

**原文摘要:** When data are noisy, a statistical learner's goal is to resolve epistemic
uncertainty about the data it will encounter at test-time, i.e., to identify
the distribution of test (target) data. Many real-world learning settings
introduce sources of epistemic uncertainty that can not be resolved on the
basis of training (source) data alone: The source data may arise from multiple
tasks (multitask learning), the target data may differ systematically from the
source data tasks (distribution shift), and/or the learner may not arrive at an
accurate characterization of the source data (imperfect learning). We introduce
a principled definition of epistemic error, and provide a generic,
decompositional epistemic error bound. Our error bound is the first to (i)
consider epistemic error specifically, (ii) accommodate all the sources of
epistemic uncertainty above, and (iii) separately attribute the error to each
of multiple aspects of the learning procedure and environment. As corollaries
of the generic result, we provide (i) epistemic error bounds specialized to the
settings of Bayesian transfer learning and distribution shift within
$\epsilon$-neighborhoods, and (ii) a set of corresponding generalization
bounds. Finally, we provide a novel definition of negative transfer, and
validate its insights in a synthetic experimental setting.

</details>


### [16] [Machine Learning Models Have a Supply Chain Problem](https://arxiv.org/abs/2505.22778)
*Sarah Meiklejohn, Hayden Blauzvern, Mihai Maruseac, Spencer Schrock, Laurent Simon, Ilia Shumailov*

**主要类别:** cs.LG

**概要:** 强大的机器学习模型虽然为缺乏技术或资源的用户提供了便利，但开放的ML模型生态系统也带来了显著的供应链风险。本文探讨了如何利用Sigstore解决方案为开放的ML模型带来透明度，例如通过签名模型和证明数据集属性等方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管强大的机器学习模型在线上可轻松获取，为非专业用户提供便利，但开放生态系统伴随许多潜在风险，包括模型被恶意替换、使用脆弱框架训练或基于受限/中毒数据训练等问题。

**方法:** 研究Sigstore在开放ML模型中的应用，探索其如何通过让模型发布者签名模型并证明数据集属性等方式，为开放ML模型生态系统带来透明度。

**结果:** 揭示了Sigstore可以有效增强开放ML模型供应链的透明度，从而降低相关风险。

**结论:** 需要采用类似Sigstore的工具来改善开放ML模型生态系统的安全性与透明性，以应对当前显著的供应链风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Machine+Learning+Models+Have+a+Supply+Chain+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22778，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22778&send_immediately=true&force_search=false)

**原文摘要:** Powerful machine learning (ML) models are now readily available online, which
creates exciting possibilities for users who lack the deep technical expertise
or substantial computing resources needed to develop them. On the other hand,
this type of open ecosystem comes with many risks. In this paper, we argue that
the current ecosystem for open ML models contains significant supply-chain
risks, some of which have been exploited already in real attacks. These include
an attacker replacing a model with something malicious (e.g., malware), or a
model being trained using a vulnerable version of a framework or on restricted
or poisoned data. We then explore how Sigstore, a solution designed to bring
transparency to open-source software supply chains, can be used to bring
transparency to open ML models, in terms of enabling model publishers to sign
their models and prove properties about the datasets they use.

</details>


### [17] [On Transferring Transferability: Towards a Theory for Size Generalization](https://arxiv.org/abs/2505.23599)
*Eitan Levin, Yuxin Ma, Mateo Díaz, Soledad Villar*

**主要类别:** cs.LG

**概要:** 本论文探讨了在不同维度间模型性能转移的可能性，并提出了一种通用框架来确保模型的可转移性。


<details>
  <summary>更多</summary>
  
**动机:** 许多现代学习任务需要处理可变大小的输入，因此需要与维度无关的架构。目前关于图神经网络的工作已经探索了低维数据训练的模型是否可以将其性能转移到高维输入上。

**方法:** 作者引入了一个通用框架，用于跨维度的可转移性分析。该框架将转移性对应于一个极限空间中的连续性，通过识别小问题实例和等价的大问题实例形成。这种识别由数据和学习任务驱动。作者在现有架构上实例化了该框架，并实现了必要的更改以确保其可转移性。

**结果:** 作者提供了设计新的可转移模型的设计原则。数值实验支持了他们的研究发现。

**结论:** 通过提出的框架和方法，作者证明了模型在不同维度间转移性能的可行性，并为未来设计可转移模型提供了指导原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Transferring+Transferability%3A+Towards+a+Theory+for+Size+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23599&send_immediately=true&force_search=false)

**原文摘要:** Many modern learning tasks require models that can take inputs of varying
sizes. Consequently, dimension-independent architectures have been proposed for
domains where the inputs are graphs, sets, and point clouds. Recent work on
graph neural networks has explored whether a model trained on low-dimensional
data can transfer its performance to higher-dimensional inputs. We extend this
body of work by introducing a general framework for transferability across
dimensions. We show that transferability corresponds precisely to continuity in
a limit space formed by identifying small problem instances with equivalent
large ones. This identification is driven by the data and the learning task. We
instantiate our framework on existing architectures, and implement the
necessary changes to ensure their transferability. Finally, we provide design
principles for designing new transferable models. Numerical experiments support
our findings.

</details>


### [18] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)
*Marco Fumero, Luca Moschella, Emanuele Rodolà, Francesco Locatello*

**主要类别:** cs.LG

**概要:** 本研究提出了一种将神经网络模型解释为作用于潜在流形上的动力系统的方法，揭示了自编码器模型隐式定义的潜在向量场，并利用该向量场作为表示工具来分析模型和数据的特性。


<details>
  <summary>更多</summary>
  
**动机:** 当前神经网络通常将高维数据压缩到低维潜在空间中，但对模型内部动态行为的理解有限。本文旨在通过动力系统视角重新解释神经模型，探索其在潜在流形上的行为模式。

**方法:** 通过反复应用编码-解码映射，揭示自编码器模型隐式定义的潜在向量场；分析标准训练过程引入的归纳偏差如何导致吸引子点的出现；利用该向量场作为表示工具，用于分析模型的泛化与记忆特性、提取先验知识以及识别异常样本。

**结果:** 验证了向量场表示可以有效分析神经模型的泛化和记忆状态、提取网络参数中的先验知识、识别分布外样本，并在视觉基础模型上展示了方法的实际可行性和效果。

**结论:** 将神经模型解释为作用于潜在流形上的动力系统提供了一种新的分析框架，有助于深入理解模型的行为和数据特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Navigating+the+Latent+Space+Dynamics+of+Neural+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22785，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22785&send_immediately=true&force_search=false)

**原文摘要:** Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [19] [Inference-time Scaling of Diffusion Models through Classical Search](https://arxiv.org/abs/2505.23614)
*Xiangcheng Zhang, Haowei Lin, Haotian Ye, James Zou, Jianzhu Ma, Yitao Liang, Yilun Du*

**主要类别:** cs.LG

**概要:** 通过结合局部和全局搜索策略，本研究提出了一种在扩散模型推理阶段高效适应多种测试目标的通用框架，显著提升了性能与效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩散模型在推理阶段难以有效适应多样化的测试目标，因此需要一种新的方法来优化生成输出以满足不同的需求。

**方法:** 提出一个综合框架，利用退火Langevin MCMC进行理论支持的局部搜索，并采用广度优先和深度优先树搜索实现高效的全局探索。

**结果:** 在包括规划、离线强化学习和图像生成等多个具有挑战性的领域中，该方法均显著提高了性能和效率。

**结论:** 经典搜索算法为扩散模型的推理时间扩展提供了一个有原则且实用的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inference-time+Scaling+of+Diffusion+Models+through+Classical+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23614，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23614&send_immediately=true&force_search=false)

**原文摘要:** Classical search algorithms have long underpinned modern artificial
intelligence. In this work, we tackle the challenge of inference-time control
in diffusion models -- adapting generated outputs to meet diverse test-time
objectives -- using principles from classical search. We propose a general
framework that orchestrates local and global search to efficiently navigate the
generative space. It employs a theoretically grounded local search via annealed
Langevin MCMC and performs compute-efficient global exploration using
breadth-first and depth-first tree search. We evaluate our approach on a range
of challenging domains, including planning, offline reinforcement learning, and
image generation. Across all tasks, we observe significant gains in both
performance and efficiency. These results show that classical search provides a
principled and practical foundation for inference-time scaling in diffusion
models. Project page at diffusion-inference-scaling.github.io.

</details>


### [20] [Efficient Preimage Approximation for Neural Network Certification](https://arxiv.org/abs/2505.22798)
*Anton Björklund, Mykola Zaitsev, Marta Kwiatkowska*

**主要类别:** cs.LG

**概要:** 这篇论文通过改进PREMAP算法，利用更紧密的界限、自适应蒙特卡洛采样和改进的分支启发式方法，提高了神经网络认证的效率，特别是在针对补丁攻击的鲁棒性和可靠性认证方面。


<details>
  <summary>更多</summary>
  
**动机:** 在安全和安保关键应用中，对人工智能的依赖日益增加，需要有效的神经网络认证方法。特别是对于'补丁攻击'，如交通标志的部分被遮挡等情况，现有的预映像近似方法（包括最先进的PREMAP算法）在可扩展性方面存在困难。

**方法:** 提出针对PREMAP算法的新改进，包括更紧密的界限、自适应蒙特卡洛采样和改进的分支启发式方法。这些改进使得算法能够更有效地应用于强化学习控制基准测试，并扩展到以前无法处理的卷积神经网络。

**结果:** 改进后的算法在强化学习控制基准上至少提高了十倍的效率，并成功扩展到之前不可行的卷积神经网络。结果表明预映像近似方法在可靠性和鲁棒性认证中的潜力。

**结论:** 预映像近似方法通过算法改进可以显著提升其在神经网络认证中的效率和可扩展性，特别是在应对补丁攻击等实际挑战时具有重要价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Preimage+Approximation+for+Neural+Network+Certification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22798，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22798&send_immediately=true&force_search=false)

**原文摘要:** The growing reliance on artificial intelligence in safety- and
security-critical applications demands effective neural network certification.
A challenging real-world use case is certification against ``patch attacks'',
where adversarial patches or lighting conditions obscure parts of images, for
example traffic signs. One approach to certification, which also gives
quantitative coverage estimates, utilizes preimages of neural networks, i.e.,
the set of inputs that lead to a specified output. However, these preimage
approximation methods, including the state-of-the-art PREMAP algorithm,
struggle with scalability. This paper presents novel algorithmic improvements
to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved
branching heuristics. We demonstrate efficiency improvements of at least an
order of magnitude on reinforcement learning control benchmarks, and show that
our method scales to convolutional neural networks that were previously
infeasible. Our results demonstrate the potential of preimage approximation
methodology for reliability and robustness certification.

</details>


### [21] [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents](https://arxiv.org/abs/2505.23720)
*Arun Verma, Indrajit Saha, Makoto Yokoo, Bryan Kian Hsiang Low*

**主要类别:** cs.LG

**概要:** 本文研究了多代理情境下的上下文bandit问题，提出COBRA算法来抑制代理的战略行为，无需使用金钱激励，并保证激励相容和次线性后悔界。


<details>
  <summary>更多</summary>
  
**动机:** 现有上下文bandit工作假设代理如实报告其arm，这在许多实际应用中不现实（如在线平台卖家可能虚假描述产品质量）。

**方法:** 提出COBRA算法，用于处理涉及战略代理的上下文bandit问题，通过非金钱激励手段抑制代理的战略行为。

**结果:** 实验结果验证了所提算法在不同性能方面的表现。

**结论:** COBRA算法可以有效地处理战略代理问题，同时保持激励相容和次线性后悔界的性能保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是COBRA%3A+Contextual+Bandit+Algorithm+for+Ensuring+Truthful+Strategic+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23720&send_immediately=true&force_search=false)

**原文摘要:** This paper considers a contextual bandit problem involving multiple agents,
where a learner sequentially observes the contexts and the agent's reported
arms, and then selects the arm that maximizes the system's overall reward.
Existing work in contextual bandits assumes that agents truthfully report their
arms, which is unrealistic in many real-life applications. For instance,
consider an online platform with multiple sellers; some sellers may
misrepresent product quality to gain an advantage, such as having the platform
preferentially recommend their products to online users. To address this
challenge, we propose an algorithm, COBRA, for contextual bandit problems
involving strategic agents that disincentivize their strategic behavior without
using any monetary incentives, while having incentive compatibility and a
sub-linear regret guarantee. Our experimental results also validate the
different performance aspects of our proposed algorithm.

</details>


### [22] [CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment](https://arxiv.org/abs/2505.22803)
*Pedro Mendes, Paolo Romano, David Garlan*

**主要类别:** cs.LG

**概要:** CLUE是一种新的方法，它通过在训练过程中明确对齐预测不确定性与观测误差，来提高神经网络模型的校准质量。该方法具有完全可微分性、领域无关性和与标准训练流程兼容的特点，并在多项实验中展现出优越的校准质量和竞争力的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的校准技术通常依赖于事后调整或粗粒度分箱方法，在可扩展性、可微性和跨域泛化能力方面存在局限性。

**方法:** CLUE（通过学习不确定性-误差对齐进行校准）采用一种新颖的方法，在训练期间明确对齐预测不确定性与观测误差。它基于良好校准的模型应生成与其实证损失相匹配的不确定性估计的原则，提出了一种新的损失函数，该函数使用不确定性和损失的汇总统计信息作为代理，联合优化预测性能和校准。

**结果:** 通过广泛的实验（包括视觉、回归和语言建模任务，以及分布外和领域转移场景），CLUE在不增加显著计算开销的情况下，实现了优于现有最先进方法的校准质量和竞争性的预测性能。

**结论:** CLUE提供了一种改进神经网络模型校准质量的有效方法，具有良好的可扩展性、可微性和跨域泛化能力，适用于标准训练流程并在多种任务上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLUE%3A+Neural+Networks+Calibration+via+Learning+Uncertainty-Error+alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22803&send_immediately=true&force_search=false)

**原文摘要:** Reliable uncertainty estimation is critical for deploying neural networks
(NNs) in real-world applications. While existing calibration techniques often
rely on post-hoc adjustments or coarse-grained binning methods, they remain
limited in scalability, differentiability, and generalization across domains.
In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error
Alignment), a novel approach that explicitly aligns predicted uncertainty with
observed error during training, grounded in the principle that well-calibrated
models should produce uncertainty estimates that match their empirical loss.
CLUE adopts a novel loss function that jointly optimizes predictive performance
and calibration, using summary statistics of uncertainty and loss as proxies.
The proposed method is fully differentiable, domain-agnostic, and compatible
with standard training pipelines. Through extensive experiments on vision,
regression, and language modeling tasks, including out-of-distribution and
domain-shift scenarios, we demonstrate that CLUE achieves superior calibration
quality and competitive predictive performance with respect to state-of-the-art
approaches without imposing significant computational overhead.

</details>


### [23] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/abs/2505.22813)
*Josiah Couch, Miao Li, Rima Arnaout, Ramy Arnaout*

**主要类别:** cs.LG

**概要:** 在优化机器学习分类器的普遍追求中，研究发现数据集质量是一个独立于数据集大小、类别平衡和模型架构的内在属性，并且与分类器性能强相关。


<details>
  <summary>更多</summary>
  
**动机:** 之前的研究表明，除了模型架构、数据集大小和类别平衡外，数据集质量也影响测试时性能，但不确定质量是数据集和模型架构的联合属性还是数据集本身的内在属性。

**方法:** 创建数千个数据集，控制其大小和类别平衡，使用包括随机森林、支持向量机到深度网络在内的多种架构训练分类器，分析不同子集上跨架构的分类器性能相关性。

**结果:** 发现分类器性能在不同架构间具有强相关性（R²=0.79），表明数据集质量是独立于大小、类别平衡和模型架构的内在属性，并且数据集质量似乎是其构成类别的更基本属性的涌现属性。

**结论:** 数据集质量作为独立于其他因素的性能相关指标，应成为优化机器学习分类的一个单独目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是X-Factor%3A+Quality+Is+a+Dataset-Intrinsic+Property，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22813&send_immediately=true&force_search=false)

**原文摘要:** In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [24] [Preference Learning with Response Time](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni, Sahasrajit Sarmasarkar, Vasilis Syrgkanis*

**主要类别:** cs.LG

**概要:** 这篇论文研究了将反应时间数据整合到人类偏好学习框架中，以更有效地获取奖励模型。通过引入新的方法论和Neyman-正交损失函数，利用证据积累漂移扩散（EZ）模型来结合反应时间和二元选择数据，显著提高了样本效率，并在理论分析和实验验证上展示了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管二元偏好数据在微调基础模型、生成式AI系统和其他大规模模型中变得至关重要，但用户决策中的宝贵时间信息尚未被充分利用。因此，需要探索如何将反应时间数据与偏好学习相结合，以提高奖励模型的获取效率。

**方法:** 提出了一种新的方法，将反应时间信息与二元选择数据结合，使用Evidence Accumulation Drift Diffusion (EZ) 模型解释反应时间对偏好强度的指示意义；开发了Neyman-正交损失函数，实现了与先验已知期望反应时间相匹配的理论最优收敛率；并扩展了这些保证至非参数奖励函数空间，建立了更复杂奖励模型的收敛性质。

**结果:** 理论分析表明，传统偏好学习的错误率随奖励幅度呈指数增长，而反应时间增强的方法将其降低为多项式增长，极大地提高了样本效率；实验验证了该方法在图像偏好学习中的有效性。

**结论:** 将反应时间数据纳入偏好学习框架可以显著提高奖励模型的学习效率，并为更复杂的奖励模型提供了理论支持和实际应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preference+Learning+with+Response+Time，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22820，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22820&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [25] [PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow](https://arxiv.org/abs/2505.22825)
*Michael Klamkin, Mathieu Tanneau, Pascal Van Hentenryck*

**主要类别:** cs.LG

**概要:** 本论文介绍了PGLearn，一个包含标准化数据集和评估工具的综合套件，用于机器学习在最优潮流问题中的应用。PGLearn提供的数据集代表了真实的运行条件，并首次包含了大规模系统的时序数据。它支持多种OPF公式，并提供了公开可用的数据集和评估工具，以促进该领域的研究和创新。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习技术在最优潮流问题中的应用受到了广泛关注，这是因为现代和未来电网的能源生产中增加了波动性和规模。然而，由于缺乏标准化的数据集和评估指标，从生成和解决OPF实例到训练和基准测试机器学习模型，这一领域的发展受到了阻碍。

**方法:** PGLearn通过明确捕捉数据生成中的全局和局部变化，并首次包含大规模系统的时序数据，提供具有代表性的数据集。此外，它支持多种OPF公式（如AC、DC和二阶锥公式），并提供了一个强大的工具包，用于训练、评估和基准测试机器学习模型。

**结果:** PGLearn使得该领域的研究民主化，减少了数据生成的负担，并使各种方法能够进行公平比较。这促进了开放、标准化的数据集和评估指标的应用，从而加速了该领域的研究和创新。

**结论:** 通过提供开放和标准化的数据集和评估工具，PGLearn旨在民主化和加速机器学习在最优潮流问题中的应用研究和创新。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PGLearn+--+An+Open-Source+Learning+Toolkit+for+Optimal+Power+Flow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22825&send_immediately=true&force_search=false)

**原文摘要:** Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have
recently garnered significant attention, reflecting a broader trend of
leveraging ML to approximate and/or accelerate the resolution of complex
optimization problems. These developments are necessitated by the increased
volatility and scale in energy production for modern and future grids. However,
progress in ML for OPF is hindered by the lack of standardized datasets and
evaluation metrics, from generating and solving OPF instances, to training and
benchmarking machine learning models. To address this challenge, this paper
introduces PGLearn, a comprehensive suite of standardized datasets and
evaluation tools for ML and OPF. PGLearn provides datasets that are
representative of real-life operating conditions, by explicitly capturing both
global and local variability in the data generation, and by, for the first
time, including time series data for several large-scale systems. In addition,
it supports multiple OPF formulations, including AC, DC, and second-order cone
formulations. Standardized datasets are made publicly available to democratize
access to this field, reduce the burden of data generation, and enable the fair
comparison of various methodologies. PGLearn also includes a robust toolkit for
training, evaluating, and benchmarking machine learning models for OPF, with
the goal of standardizing performance evaluation across the field. By promoting
open, standardized datasets and evaluation metrics, PGLearn aims at
democratizing and accelerating research and innovation in machine learning
applications for optimal power flow problems. Datasets are available for
download at https://www.huggingface.co/PGLearn.

</details>


### [26] [Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies](https://arxiv.org/abs/2505.22829)
*Chenruo Liu, Kenan Tang, Yao Qin, Qi Lei*

**主要类别:** cs.LG

**概要:** 本文通过全面分析分布偏移和AI安全的概念和方法协同作用，建立了两者之间的两种连接类型，提供了一个统一的观点，鼓励在分布偏移和AI安全研究之间进行基本整合。


<details>
  <summary>更多</summary>
  
**动机:** 之前的讨论通常集中在狭窄的案例或非正式的类比上，本文旨在通过建立分布偏移和AI安全问题之间的联系，提供一个统一的观点。

**方法:** 建立两类连接：(1)针对特定偏移类型的方法可以帮助实现相应的安全目标；(2)某些偏移和安全问题可以相互形式化地转换，从而使其方法能够相互适应。

**结果:** 提供了分布偏移和AI安全研究的基本整合的统一视角。

**结论:** 鼓励在分布偏移和AI安全研究之间进行基本整合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Distribution+Shift+and+AI+Safety%3A+Conceptual+and+Methodological+Synergies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22829&send_immediately=true&force_search=false)

**原文摘要:** This paper bridges distribution shift and AI safety through a comprehensive
analysis of their conceptual and methodological synergies. While prior
discussions often focus on narrow cases or informal analogies, we establish two
types connections between specific causes of distribution shift and
fine-grained AI safety issues: (1) methods addressing a specific shift type can
help achieve corresponding safety goals, or (2) certain shifts and safety
issues can be formally reduced to each other, enabling mutual adaptation of
their methods. Our findings provide a unified perspective that encourages
fundamental integration between distribution shift and AI safety research.

</details>


### [27] [How Do Diffusion Models Improve Adversarial Robustness?](https://arxiv.org/abs/2505.22839)
*Liu Yuezhang, Xue-Xin Wei*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Do+Diffusion+Models+Improve+Adversarial+Robustness%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22839，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22839&send_immediately=true&force_search=false)

**原文摘要:** Recent findings suggest that diffusion models significantly enhance empirical
adversarial robustness. While some intuitive explanations have been proposed,
the precise mechanisms underlying these improvements remain unclear. In this
work, we systematically investigate how and how well diffusion models improve
adversarial robustness. First, we observe that diffusion models intriguingly
increase, rather than decrease, the $\ell_p$ distance to clean
samples--challenging the intuition that purification denoises inputs closer to
the original data. Second, we find that the purified images are heavily
influenced by the internal randomness of diffusion models, where a compression
effect arises within each randomness configuration. Motivated by this
observation, we evaluate robustness under fixed randomness and find that the
improvement drops to approximately 24% on CIFAR-10--substantially lower than
prior reports approaching 70%. Importantly, we show that this remaining
robustness gain strongly correlates with the model's ability to compress the
input space, revealing the compression rate as a reliable robustness indicator
without requiring gradient-based analysis. Our findings provide novel insights
into the mechanisms underlying diffusion-based purification, and offer guidance
for developing more effective and principled adversarial purification systems.

</details>


### [28] [Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction](https://arxiv.org/abs/2505.22840)
*Dharambir Mahto, Prashant Yadav, Mahesh Banavar, Jim Keany, Alan T Joseph, Srinivas Kilambi*

**主要类别:** cs.LG

**概要:** Sepsis影响全球超过4890万人，导致1100万人死亡。尽管医学进步，但由于症状不具体和复杂的病理生理学，预测sepsis仍然是一个挑战。本文介绍了一种机器学习评分系统SXI++ LNM，它通过利用多种算法和深度神经网络来改进sepsis预测。本研究旨在提高临床应用中的鲁棒性，并评估SXI++ LNM对sepsis预测的预测性能。模型使用深度神经网络构建，并在不同的数据集分布下进行训练和测试。该模型在三个用例中超越了现有技术，在AUC得分为0.99（95% CI：0.98-1.00）的情况下表现优异，精确度为99.9%（95% CI：99.8-100.0），准确率为99.99%（95% CI：99.98-100.0），可靠性高。


<details>
  <summary>更多</summary>
  
**动机:** Sepsis是一种危及生命的疾病，由于其非特异性症状和复杂的病理生理学，使得预测变得困难。因此需要更精确、更可靠的预测工具来改善患者的预后和降低死亡率。

**方法:** 开发了一个名为SXI++ LNM的机器学习评分系统，利用多种算法和深度神经网络进行sepsis预测。模型通过不同场景下的数据集分布进行训练和测试，以评估其预测性能。

**结果:** SXI++ LNM在三个用例中超越了现有技术，取得了AUC 0.99、精确度99.9%和准确率99.99%的优异成绩，证明了其高可靠性和高性能。

**结论:** SXI++ LNM作为一种先进的机器学习模型，在sepsis预测方面表现出色，具有高度的准确性和鲁棒性，适用于临床应用，有望显著改善sepsis管理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Development+and+Validation+of+SXI%2B%2B+LNM+Algorithm+for+Sepsis+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22840，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22840&send_immediately=true&force_search=false)

**原文摘要:** Sepsis is a life-threatening condition affecting over 48.9 million people
globally and causing 11 million deaths annually. Despite medical advancements,
predicting sepsis remains a challenge due to non-specific symptoms and complex
pathophysiology. The SXI++ LNM is a machine learning scoring system that
refines sepsis prediction by leveraging multiple algorithms and deep neural
networks. This study aims to improve robustness in clinical applications and
evaluates the predictive performance of the SXI++ LNM for sepsis prediction.
The model, utilizing a deep neural network, was trained and tested using
multiple scenarios with different dataset distributions. The model's
performance was assessed against unseen test data, and accuracy, precision, and
area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the
state of the art in three use cases, achieving an AUC of 0.99 (95% CI:
0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)
and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.

</details>


### [29] [RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation](https://arxiv.org/abs/2505.22846)
*Nikita Khramov, Andrei Kozyrev, Gleb Solovev, Anton Podkopaev*

**主要类别:** cs.LG

**概要:** 本研究探讨了交互式定理证明与生成式人工智能结合的多种方法，并提出了一种基于自注意力嵌入模型的Rocq证明生成新方法，显著提升了生成器性能。同时，采用多阶段代理系统进行Rocq证明编写，并展示了其高效性及多代理辩论在证明合成规划阶段的应用。


<details>
  <summary>更多</summary>
  
**动机:** 交互式定理证明与生成式人工智能结合已被证明是富有成效的研究方向，但现有方法仍有改进空间。因此，本文旨在评估不同的Rocq生成方法，并探索提高生成性能的新途径。

**方法:** 1. 提出一种基于自注意力嵌入模型的检索方法，用于彻底的前提选择以生成Rocq证明。
2. 使用多阶段代理系统进行Rocq证明编写，该系统专为形式化验证设计。
3. 在证明合成的规划阶段引入多代理辩论机制。

**结果:** 1. 新提出的基于自注意力嵌入模型的方法使生成器性能相对提高了28%。
2. 多阶段代理系统在Rocq证明生成任务中表现出高有效性。
3. 通过消融研究验证了多代理辩论在证明合成规划阶段的有效性。

**结论:** 彻底的前提选择对于生成高质量的Rocq证明至关重要，而基于自注意力嵌入模型的检索方法能够显著提升生成性能。此外，多阶段代理系统和多代理辩论机制为Rocq证明生成提供了高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RocqStar%3A+Leveraging+Similarity-driven+Retrieval+and+Agentic+Systems+for+Rocq+generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22846，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22846&send_immediately=true&force_search=false)

**原文摘要:** Interactive Theorem Proving was repeatedly shown to be fruitful combined with
Generative Artificial Intelligence. This paper assesses multiple approaches to
Rocq generation and illuminates potential avenues for improvement. We highlight
the importance of thorough premise selection for generating Rocq proofs and
propose a novel approach, leveraging retrieval via a self-attentive embedder
model. The evaluation of the designed approach shows up to 28% relative
increase of the generator's performance. We tackle the problem of writing Rocq
proofs using a multi-stage agentic system, tailored for formal verification,
and demonstrate its high effectiveness. We conduct an ablation study and show
the use of multi-agent debate on the planning stage of proof synthesis.

</details>


### [30] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/abs/2505.22861)
*Carlota Parés-Morlans, Michelle Yi, Claire Chen, Sarah A. Wu, Rika Antonova, Tobias Gerstenberg, Jeannette Bohg*

**主要类别:** cs.LG

**概要:** 在涉及复杂对象交互的任务中，由于动力学未知，执行前的规划变得困难。这类任务需要智能体通过探索环境中的因果关系来迭代改进其行动。本文提出了一种名为Causal-PIK的方法，该方法利用贝叶斯优化和物理信息核（Physics-Informed Kernel）推理因果关系，从而有效地搜索最佳下一步行动。实验结果表明，在虚拟工具和PHYRE物理推理基准测试中，Causal-PIK的表现优于现有技术，并且达到目标所需的行动更少。此外，与人类研究相比，包括我们在PHYRE基准上进行的新用户研究，Causal-PIK在对人类问题解决者也非常具有挑战性的任务上仍然保持竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 在动力学未知的情况下，复杂对象交互任务使得执行前的规划变得困难。因此，需要一种方法帮助智能体通过探索环境中的因果关系来迭代改进其行动。

**方法:** 提出了一种名为Causal-PIK的方法，该方法结合了贝叶斯优化和物理信息核（Physics-Informed Kernel），以推理因果关系并指导高效搜索最佳下一步行动。

**结果:** 在虚拟工具和PHYRE物理推理基准测试中，Causal-PIK的表现优于现有技术，并且达到目标所需的行动更少。此外，在对人类问题解决者也非常具有挑战性的任务上，Causal-PIK仍然保持竞争力。

**结论:** Causal-PIK是一种有效的因果推理方法，能够在复杂对象交互任务中引导智能体找到最佳行动策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal-PIK%3A+Causality-based+Physical+Reasoning+with+a+Physics-Informed+Kernel，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22861&send_immediately=true&force_search=false)

**原文摘要:** Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [31] [Scaling Offline RL via Efficient and Expressive Shortcut Models](https://arxiv.org/abs/2505.22866)
*Nicolas Espinosa-Dice, Yiyi Zhang, Yiding Chen, Bradley Guo, Owen Oertell, Gokul Swamy, Kiante Brantley, Wen Sun*

**主要类别:** cs.LG

**概要:** Diffusion和flow模型是强大的生成方法，但将其应用于离线强化学习（RL）具有挑战性。本文提出了可扩展离线强化学习（SORL），这是一种新的离线RL算法，利用shortcut模型进行训练和推理的扩展。SORL策略可以捕捉复杂的数据分布，并通过单阶段训练过程简单高效地训练。测试时，SORL通过使用学习到的Q函数作为验证器引入顺序和并行推理扩展。实验表明，SORL在一系列离线RL任务中表现出色，并且随着测试时间计算量的增加表现出正向扩展行为。


<details>
  <summary>更多</summary>
  
**动机:** Diffusion和flow模型虽然强大，但其噪声采样过程的迭代性质使得在离线RL中的策略优化变得困难。因此需要一种能够克服这一问题的新方法。

**方法:** 提出SORL算法，利用shortcut模型进行训练和推理扩展。策略通过单阶段训练过程训练，并在测试时使用学习到的Q函数作为验证器进行顺序和并行推理扩展。

**结果:** SORL在一系列离线RL任务中表现出色，并且随着测试时间计算量的增加表现出正向扩展行为。

**结论:** SORL是一种有效的离线RL算法，能够在训练和推理过程中进行扩展，适用于复杂的离线RL任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Offline+RL+via+Efficient+and+Expressive+Shortcut+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22866&send_immediately=true&force_search=false)

**原文摘要:** Diffusion and flow models have emerged as powerful generative approaches
capable of modeling diverse and multimodal behavior. However, applying these
models to offline reinforcement learning (RL) remains challenging due to the
iterative nature of their noise sampling processes, making policy optimization
difficult. In this paper, we introduce Scalable Offline Reinforcement Learning
(SORL), a new offline RL algorithm that leverages shortcut models - a novel
class of generative models - to scale both training and inference. SORL's
policy can capture complex data distributions and can be trained simply and
efficiently in a one-stage training procedure. At test time, SORL introduces
both sequential and parallel inference scaling by using the learned Q-function
as a verifier. We demonstrate that SORL achieves strong performance across a
range of offline RL tasks and exhibits positive scaling behavior with increased
test-time compute. We release the code at
nico-espinosadice.github.io/projects/sorl.

</details>


### [32] [On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning](https://arxiv.org/abs/2505.22899)
*Naram Mhaisen, George Iosifidis*

**主要类别:** cs.LG

**概要:** 本论文重新审视了FTRL框架在在线凸优化中的应用，提出了通过乐观组合未来成本和线性化过去成本来实现动态后悔界的方法。这种方法不仅克服了FTRL在动态环境中的局限性，还提供了对后悔项的精细控制，并避免了循环依赖问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的FTRL框架在动态环境中表现不佳，因其倾向于生成'懒惰'迭代。然而，研究发现FTRL也能够产生'敏捷'迭代，这为改进动态后悔界提供了可能性。

**方法:** 作者通过乐观组合未来成本和线性化过去成本的方法改进了FTRL框架，这种方法可以修剪部分历史成本，从而在贪婪更新和敏捷更新之间找到平衡。此外，该方法实现了最小递归正则化（类似AdaFTRL）。

**结果:** 新分析方法不仅恢复了已知的动态后悔界，还提供了对后悔项更精细的控制，消除了乐观预测中的循环依赖问题，并同步了算法状态与迭代过程。

**结论:** 论文指出，限制动态后悔的不是FTRL的'懒惰'投影风格，而是算法状态（线性化历史）与其迭代之间的解耦。通过修剪历史成本，可以同步这两者，从而改善动态后悔性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Dynamic+Regret+of+Following+the+Regularized+Leader%3A+Optimism+with+History+Pruning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22899&send_immediately=true&force_search=false)

**原文摘要:** We revisit the Follow the Regularized Leader (FTRL) framework for Online
Convex Optimization (OCO) over compact sets, focusing on achieving dynamic
regret guarantees. Prior work has highlighted the framework's limitations in
dynamic environments due to its tendency to produce "lazy" iterates. However,
building on insights showing FTRL's ability to produce "agile" iterates, we
show that it can indeed recover known dynamic regret bounds through optimistic
composition of future costs and careful linearization of past costs, which can
lead to pruning some of them. This new analysis of FTRL against dynamic
comparators yields a principled way to interpolate between greedy and agile
updates and offers several benefits, including refined control over regret
terms, optimism without cyclic dependence, and the application of minimal
recursive regularization akin to AdaFTRL. More broadly, we show that it is not
the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but
the decoupling of the algorithm's state (linearized history) from its iterates,
allowing the state to grow arbitrarily. Instead, pruning synchronizes these two
when necessary.

</details>


### [33] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/abs/2505.22904)
*Youngsoo Choi, Siu Wun Cheung, Youngkyu Kim, Ping-Hsuan Tsai, Alejandro N. Diaz, Ivan Zanardi, Seung Whan Chung, Dylan Matthew Copeland, Coleman Kendrick, William Anderson, Traian Iliescu, Matthias Heinkenschloss*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种关于计算科学中基础模型的正式定义，并引入了数据驱动的有限元方法（DD-FEM）作为实现这些模型的一种框架。


<details>
  <summary>更多</summary>
  
**动机:** 当前“基础模型”这一术语在计算科学中的应用缺乏统一定义，可能导致混淆和科学意义的稀释。

**方法:** 作者提出了计算科学中基础模型的正式定义，基于通用性、可重用性和可扩展性的核心价值。并介绍了数据驱动的有限元方法（DD-FEM），该方法结合了经典有限元方法的模块化结构和数据驱动学习的表现力。

**结果:** DD-FEM解决了实现计算科学基础模型的关键挑战，包括可扩展性、适应性和物理一致性。

**结论:** 通过将传统数值方法与现代AI范式相结合，这项工作为评估和发展未来计算科学的基础模型提供了严谨的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Defining+Foundation+Models+for+Computational+Science%3A+A+Call+for+Clarity+and+Rigor，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22904，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22904&send_immediately=true&force_search=false)

**原文摘要:** The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [34] [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/abs/2505.22913)
*Donghyeon Joo, Helya Hosseini, Ramyad Hadidi, Bahar Asgari*

**主要类别:** cs.LG

**概要:** 本研究展示了无结构稀疏性对LLM的KV缓存压缩有显著改善，能够在不影响精度或无需微调的情况下实现高达70%的稀疏性。通过每令牌幅度基剪枝策略，显著超越了先前的结构化剪枝方案。同时，采用位图基础的稀疏格式和自定义注意力内核，实现了KV缓存的高效压缩（最多可达密集推理的45%），并提升了上下文长度和吞吐量（最高达2.23倍）。


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型中，KV缓存的大小是解码性能的主要瓶颈，因为对于长上下文长度，内存开销非常高。因此，探索有效的压缩方法以减少内存占用、提升计算效率变得至关重要。

**方法:** 1. 系统性地探索了不同的剪枝策略，并发现基于每令牌幅度的剪枝在无结构稀疏性下非常有效。
2. 使用位图基础的稀疏格式和自定义注意力内核，能够直接在压缩后的缓存上进行计算，从而加速内存受限的操作。
3. 实现了一种可以处理任意稀疏模式的压缩方法，补偿了运行时剪枝和压缩的开销。

**结果:** - 在不影响准确性和无需微调的情况下，实现了高达70%的稀疏性。
- KV缓存压缩达到了密集推理的45%，使上下文长度更长，吞吐量提高至2.23倍。
- 自定义注意力内核与位图基础格式相结合，显著加速了解码计算中的内存操作。

**结论:** 无结构稀疏性结合适当的剪枝策略和自定义计算内核，为大语言模型的KV缓存提供了高效的压缩解决方案，显著提升了模型的解码性能和吞吐量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mustafar%3A+Promoting+Unstructured+Sparsity+for+KV+Cache+Pruning+in+LLM+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22913，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22913&send_immediately=true&force_search=false)

**原文摘要:** We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.

</details>


### [35] [Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking](https://arxiv.org/abs/2505.22922)
*Athanasios Glentis, Jiaxiang Li, Qiulin Shang, Andi Han, Ioannis Tsaknakis, Quan Wei, Mingyi Hong*

**主要类别:** cs.LG

**概要:** 大语言模型（LLMs）快速发展，但训练和微调需要大量计算资源。尽管一些方法如LoRA在微调中有效，但在预训练中的应用更具挑战性。本文探讨了参数或内存高效方法是否能在保持与完整模型训练相当性能的同时提高预训练效率，并提出两种技术——权重重构和动量重置，以增强低秩方法的性能，使其在1B模型上比GaLore和Fira等流行算法表现更好，同时使用约25%更少的内存。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型参数量的增加，训练和微调所需的内存和计算资源也大幅增加，而现有高效方法主要集中在微调阶段，在预训练阶段的应用更具挑战性。因此，需要探索参数或内存高效的方法来提升预训练效率，同时缩小与完整模型训练之间的性能差距。

**方法:** 1. 进行全面调查，总结高效的预训练方法。
2. 对多个具有代表性的内存高效预训练方法进行基准评估，考察不同模型尺寸下的性能。
3. 提出两种实用技术：权重重构和动量重置，以改进低秩方法的性能。

**结果:** - 在适当选择优化器和超参数的情况下，全秩训练表现最佳。
- 在低秩方法中引入高秩更新是提升其性能的关键。
- 使用提出的权重重构和动量重置技术，可以使低秩方法在1B模型上达到比GaLore和Fira更低的困惑度，同时减少约25%的内存使用。

**结论:** 参数或内存高效的预训练方法可以通过适当的优化和技术创新，在减少资源消耗的同时，实现与完整模型训练相近的性能。本文提出的权重重构和动量重置技术为改进低秩方法提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Parameter+and+Memory+Efficient+Pretraining+for+LLM%3A+Recent+Algorithmic+Advances+and+Benchmarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22922，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22922&send_immediately=true&force_search=false)

**原文摘要:** Fueled by their remarkable ability to tackle diverse tasks across multiple
domains, large language models (LLMs) have grown at an unprecedented rate, with
some recent models containing trillions of parameters. This growth is
accompanied by substantial computational challenges, particularly regarding the
memory and compute resources required for training and fine-tuning. Numerous
approaches have been explored to address these issues, such as LoRA. While
these methods are effective for fine-tuning, their application to pre-training
is significantly more challenging due to the need to learn vast datasets.
Motivated by this issue, we aim to address the following questions: Can
parameter- or memory-efficient methods enhance pre-training efficiency while
achieving performance comparable to full-model training? How can the
performance gap be narrowed? To this end, the contributions of this work are
the following. (1) We begin by conducting a comprehensive survey that
summarizes state-of-the-art methods for efficient pre-training. (2) We perform
a benchmark evaluation of several representative memory efficient pre-training
approaches to comprehensively evaluate their performance across model sizes. We
observe that with a proper choice of optimizer and hyperparameters, full-rank
training delivers the best performance, as expected. We also notice that
incorporating high-rank updates in low-rank approaches is the key to improving
their performance. (3) Finally, we propose two practical techniques, namely
weight refactorization and momentum reset, to enhance the performance of
efficient pre-training methods. We observe that applying these techniques to
the low-rank method (on a 1B model) can achieve a lower perplexity than popular
memory efficient algorithms such as GaLore and Fira, while simultaneously using
about 25% less memory.

</details>


### [36] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/abs/2505.22935)
*Jipeng Li, Yanning Shen*

**主要类别:** cs.LG

**概要:** 论文探讨了在图扩散模型（GDMs）中，是否可以不需要显式的噪声水平条件，通过理论和实验证明无条件GDMs能达到与有条件模型相当或更好的性能，并减少参数和计算时间。


<details>
  <summary>更多</summary>
  
**动机:** 挑战图扩散模型中显式噪声水平条件的必要性，探索是否可以通过隐式推断噪声水平来简化模型。

**方法:** 以Bernoulli边翻转破坏为理论框架，扩展到包含更复杂的结构-属性噪声场景；使用GDSS和DiGress等模型进行广泛的实证评估。

**结果:** 无条件GDMs在性能上与有条件模型相当甚至更好，同时减少了4-6%的参数和8-10%的计算时间。

**结论:** 图数据的高维特性本身往往包含了足够的去噪信息，未来可以设计更简单、更高效的GDM架构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Noise+Conditioning+Necessary%3F+A+Unified+Theory+of+Unconditional+Graph+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22935&send_immediately=true&force_search=false)

**原文摘要:** Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [37] [Directed Graph Grammars for Sequence-based Learning](https://arxiv.org/abs/2505.22949)
*Michael Sun, Orion Foo, Gang Liu, Wojciech Matusik, Jie Chen*

**主要类别:** cs.LG

**概要:** 本论文提出了一种基于文法的方法，将有向无环图（DAG）转换为紧凑且等效的序列表示形式。该方法通过无歧义文法的推导规则唯一地表示DAG，并可以看作是对数据的无损压缩。这种表示形式可用于生成模型、属性预测和结构化数据上的贝叶斯优化等任务。


<details>
  <summary>更多</summary>
  
**动机:** 尽管有许多有效的DAG编码器，但以原则性方式解码它们仍然具有挑战性，因为DAG的节点可以有许多不同的拓扑顺序。

**方法:** 作者提出了一个基于文法的方法来构建DAG的紧凑且等效的序列表示形式。具体来说，他们将图视为在无歧义文法上的推导，其中DAG对应于唯一的生产规则序列。

**结果:** 此方法能够提供一种紧凑、等效且无损的DAG序列表示形式，适用于多种任务，如生成模型、属性预测和贝叶斯优化。

**结论:** 提出的基于文法的方法成功地为DAG提供了紧凑且等效的序列表示形式，这有助于构建生成模型、学习潜在空间以及进行结构化数据的贝叶斯优化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Directed+Graph+Grammars+for+Sequence-based+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22949&send_immediately=true&force_search=false)

**原文摘要:** Directed acyclic graphs (DAGs) are a class of graphs commonly used in
practice, with examples that include electronic circuits, Bayesian networks,
and neural architectures. While many effective encoders exist for DAGs, it
remains challenging to decode them in a principled manner, because the nodes of
a DAG can have many different topological orders. In this work, we propose a
grammar-based approach to constructing a principled, compact and equivalent
sequential representation of a DAG. Specifically, we view a graph as
derivations over an unambiguous grammar, where the DAG corresponds to a unique
sequence of production rules. Equivalently, the procedure to construct such a
description can be viewed as a lossless compression of the data. Such a
representation has many uses, including building a generative model for graph
generation, learning a latent space for property prediction, and leveraging the
sequence representational continuity for Bayesian Optimization over structured
data. Code is available at https://github.com/shiningsunnyday/induction.

</details>


### [38] [MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming](https://arxiv.org/abs/2505.22967)
*Chengqi Zheng, Jianda Chen, Yueming Lyu, Wen Zheng Terence Ng, Haopeng Zhang, Yew-Soon Ong, Ivor Tsang, Haiyan Yin*

**主要类别:** cs.LG

**概要:** MermaidFlow 是一个通过安全约束图演化重新定义代理搜索空间的框架，它使用 Mermaid 作为可验证的中间表示，并通过领域感知的进化算子实现语义正确性和结构多样性，从而在代理推理基准上提高了成功率和收敛速度。


<details>
  <summary>更多</summary>
  
**动机:** 当前自主代理推理的工作流生成方法由于不受约束的 LLM 驱动构建，常常产生脆弱且不可执行的计划。因此需要一种新方法来确保工作流的安全性和可执行性。

**方法:** MermaidFlow 将工作流表示为可验证的中间表示形式（使用 Mermaid 图形语言），并设计了领域感知的进化算子（如交叉、变异、插入和删除）以保持语义正确性并促进结构多样性。

**结果:** 在不修改任务设置或评估协议的情况下，MermaidFlow 在代理推理基准上实现了更高的成功率和更快的收敛到可执行计划的速度。

**结论:** 安全约束图演化为鲁棒且可解释的代理推理系统提供了可扩展和模块化的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MermaidFlow%3A+Redefining+Agentic+Workflow+Generation+via+Safety-Constrained+Evolutionary+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22967&send_immediately=true&force_search=false)

**原文摘要:** Despite the promise of autonomous agentic reasoning, existing workflow
generation methods frequently produce fragile, unexecutable plans due to
unconstrained LLM-driven construction. We introduce MermaidFlow, a framework
that redefines the agentic search space through safety-constrained graph
evolution. At its core, MermaidFlow represent workflows as a verifiable
intermediate representation using Mermaid, a structured and human-interpretable
graph language. We formulate domain-aware evolutionary operators, i.e.,
crossover, mutation, insertion, and deletion, to preserve semantic correctness
while promoting structural diversity, enabling efficient exploration of a
high-quality, statically verifiable workflow space. Without modifying task
settings or evaluation protocols, MermaidFlow achieves consistent improvements
in success rates and faster convergence to executable plans on the agent
reasoning benchmark. The experimental results demonstrate that
safety-constrained graph evolution offers a scalable, modular foundation for
robust and interpretable agentic reasoning systems.

</details>


### [39] [EquiReg: Equivariance Regularized Diffusion for Inverse Problems](https://arxiv.org/abs/2505.22973)
*Bahareh Tolooshams, Aditi Chandrashekar, Rayhan Zirvi, Abbas Mammadov, Jiachen Yao, Chuwei Wang, Anima Anandkumar*

**主要类别:** cs.LG

**概要:** 提出了一种名为Equivariance Regularized (EquiReg)扩散的新方法，用于正则化基于扩散的逆问题求解器中的后验采样过程。通过重新加权扩散轨迹和惩罚偏离数据流形的轨迹，EquiReg在多种图像恢复任务中超越了现有的最佳扩散模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前许多扩散模型在处理逆问题时，由于似然项的不可处理性，依赖各向同性高斯近似，导致重建结果不一致且不稳定。需要一种新方法来改进这一过程。

**方法:** 提出EquivReg框架，通过重新加权扩散轨迹并惩罚偏离数据流形的轨迹来增强重建效果。定义了一个新的分布相关等变误差，并利用该误差低的函数来正则化扩散采样过程。

**结果:** 在多个线性和非线性图像恢复任务以及偏微分方程重构中，EquiReg的表现优于现有的最佳扩散模型。

**结论:** EquivReg提供了一种通用框架，能够有效改善基于扩散模型的逆问题求解器的性能，尤其是在保持数据流形一致性方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EquiReg%3A+Equivariance+Regularized+Diffusion+for+Inverse+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22973，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22973&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models represent the state-of-the-art for solving inverse problems
such as image restoration tasks. In the Bayesian framework, diffusion-based
inverse solvers incorporate a likelihood term to guide the prior sampling
process, generating data consistent with the posterior distribution. However,
due to the intractability of the likelihood term, many current methods rely on
isotropic Gaussian approximations, which lead to deviations from the data
manifold and result in inconsistent, unstable reconstructions. We propose
Equivariance Regularized (EquiReg) diffusion, a general framework for
regularizing posterior sampling in diffusion-based inverse problem solvers.
EquiReg enhances reconstructions by reweighting diffusion trajectories and
penalizing those that deviate from the data manifold. We define a new
distribution-dependent equivariance error, empirically identify functions that
exhibit low error for on-manifold samples and higher error for off-manifold
samples, and leverage these functions to regularize the diffusion sampling
process. When applied to a variety of solvers, EquiReg outperforms
state-of-the-art diffusion models in both linear and nonlinear image
restoration tasks, as well as in reconstructing partial differential equations.

</details>


### [40] [A Computational Approach to Improving Fairness in K-means Clustering](https://arxiv.org/abs/2505.22984)
*Guancheng Zhou, Haiping Xu, Hongkang Xu, Chenyu Li, Donghui Yan*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种两阶段优化方法，以改进K-means聚类的公平性。首先进行聚类，然后调整一小部分数据点的聚类归属，同时提出了两种计算效率高的算法来识别对公平性代价高昂的数据点。实验表明该方法能显著提高公平性，同时对聚类质量影响很小。


<details>
  <summary>更多</summary>
  
**动机:** K-means聚类算法可能在进一步分析或解释时存在重大弱点，某些聚类可能会从一个子群体中不成比例地包含更多（或更少）的点，就某些敏感变量而言，例如性别或种族。这种公平性问题可能导致偏差和意外的社会后果。

**方法:** 本工作采用了一个两阶段的优化公式——首先进行聚类，然后调整一小部分选定数据点的聚类成员身份。并提出了两种计算效率高的算法来识别那些对公平性代价高昂的数据点，一个关注的是集群外部最近的数据点，另一个则关注高度“混合”的数据点。

**结果:** 基准数据集上的实验显示，公平性有了显著的改善，而对聚类质量的影响最小。

**结论:** 所提出的算法可以很容易地扩展到一大类聚类算法或公平性度量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Computational+Approach+to+Improving+Fairness+in+K-means+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22984，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22984&send_immediately=true&force_search=false)

**原文摘要:** The popular K-means clustering algorithm potentially suffers from a major
weakness for further analysis or interpretation. Some cluster may have
disproportionately more (or fewer) points from one of the subpopulations in
terms of some sensitive variable, e.g., gender or race. Such a fairness issue
may cause bias and unexpected social consequences. This work attempts to
improve the fairness of K-means clustering with a two-stage optimization
formulation--clustering first and then adjust cluster membership of a small
subset of selected data points. Two computationally efficient algorithms are
proposed in identifying those data points that are expensive for fairness, with
one focusing on nearest data points outside of a cluster and the other on
highly 'mixed' data points. Experiments on benchmark datasets show substantial
improvement on fairness with a minimal impact to clustering quality. The
proposed algorithms can be easily extended to a broad class of clustering
algorithms or fairness metrics.

</details>


### [41] [Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition](https://arxiv.org/abs/2505.22985)
*Masaharu Kagiyama, Tsuyoshi Okita*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为PatchEchoClassifier的新模型，该模型基于回声状态网络（ESN）机制，专为时间序列数据分类设计。通过知识蒸馏框架，将高容量MLP-Mixer的知识转移到轻量级的基于水库的学生模型上，从而在显著降低计算成本的同时实现了超过80%的准确率。实验结果表明，PatchEchoClassifier在边缘计算环境中进行实时和节能的人类活动识别具有很大的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的时间序列分类方法，例如DeepConvLSTM等卷积基线模型，在计算成本和能耗方面较高，不适用于资源受限的边缘计算环境。因此需要一种既高效又节能的分类器来处理一维传感器信号的人类活动识别任务。

**方法:** 1. 引入了PatchEchoClassifier模型，结合了回声状态网络（ESN）和tokenizer以提取补丁级别的表示。
2. 提出了一个知识蒸馏框架，用以从高容量的MLP-Mixer教师模型向轻量级的水库学生模型转移知识。
3. 在多个HAR数据集上进行了实验评估，比较了PatchEchoClassifier与现有模型的性能和计算成本。

**结果:** 实验结果显示，PatchEchoClassifier在多个HAR数据集上达到了超过80%的准确率，同时其计算成本仅为DeepConvLSTM的约六分之一（FLOPS显著减少）。

**结论:** PatchEchoClassifier是一种有前景的解决方案，能够在边缘计算环境中实现高效的、节能的人类活动识别。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Distillation+for+Reservoir-based+Classifier%3A+Human+Activity+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22985，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22985&send_immediately=true&force_search=false)

**原文摘要:** This paper aims to develop an energy-efficient classifier for time-series
data by introducing PatchEchoClassifier, a novel model that leverages a
reservoir-based mechanism known as the Echo State Network (ESN). The model is
designed for human activity recognition (HAR) using one-dimensional sensor
signals and incorporates a tokenizer to extract patch-level representations. To
train the model efficiently, we propose a knowledge distillation framework that
transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight
reservoir-based student model. Experimental evaluations on multiple HAR
datasets demonstrate that our model achieves over 80 percent accuracy while
significantly reducing computational cost. Notably, PatchEchoClassifier
requires only about one-sixth of the floating point operations (FLOPS) compared
to DeepConvLSTM, a widely used convolutional baseline. These results suggest
that PatchEchoClassifier is a promising solution for real-time and
energy-efficient human activity recognition in edge computing environments.

</details>


### [42] [Model-Preserving Adaptive Rounding](https://arxiv.org/abs/2505.22988)
*Albert Tseng, Zhaofeng Sun, Christopher De Sa*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的量化算法YAQA，通过Kronecker-factored近似方法优化整个模型的KL散度，减少量化误差，并在多个模型和任务上实现了30%的KL散度降低及最佳下游任务性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前的后训练量化（PTQ）方法主要关注独立最小化每一层的激活误差，但忽略了后续层的影响，可能导致模型输出分布与原始模型差距较大。因此需要一种新的量化方法来全局优化模型的整体表现。

**方法:** 引入了Yet Another Quantization Algorithm (YAQA)，该方法使用Kronecker-factored近似技术计算线性层相对于完整模型KL散度的Hessian矩阵，并结合一个与量化器无关的自适应取整算法，提供理论保证。

**结果:** 实验结果表明，YAQA在广泛的模型和量化器上能够将KL散度降低约30%，同时在下游任务上达到最先进的性能水平。

**结论:** YAQA作为一种新型的量化算法，通过优化整个模型的KL散度显著提高了量化模型的精度，并且在实际应用中表现出色，为大规模语言模型的压缩提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-Preserving+Adaptive+Rounding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22988&send_immediately=true&force_search=false)

**原文摘要:** The main goal of post-training quantization (PTQ) is to produced a compressed
model whose output distribution is as close to the original model's as
possible. To do this tractably, almost all LLM PTQ algorithms quantize linear
layers by independently minimizing the immediate activation error. However,
this localized objective ignores the effect of subsequent layers, so reducing
it does not necessarily give a closer model. In this work, we introduce Yet
Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses
Kronecker-factored approximations of each linear layer's Hessian with respect
to the \textit{full model} KL divergence. YAQA consists of two components:
Kronecker-factored sketches of the full layerwise Hessian that can be tractably
computed for hundred-billion parameter LLMs, and a quantizer-independent
rounding algorithm that uses these sketches and comes with theoretical
guarantees. Across a wide range of models and quantizers, YAQA empirically
reduces the KL divergence to the original model by $\approx 30\%$ while
achieving state of the art performance on downstream tasks.

</details>


### [43] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/abs/2505.22991)
*Behzad Kamgar-Parsi, Behrooz Kamgar-Parsi*

**主要类别:** cs.LG

**概要:** 论文探讨了在未标记数据集中寻找有意义聚类数量的问题，提出了理想聚类假设下的正则化k-means算法，并分析了加法和乘法正则化对λ的严格边界的影响。


<details>
  <summary>更多</summary>
  
**动机:** 许多应用中需要确定未标记数据集中的聚类数量，而目前缺乏系统性指导来设置正则化参数λ的值。

**方法:** 假设聚类为理想球体，推导出正则化参数λ的严格界限，并比较加法和乘法正则化的效果以减少多解问题。

**结果:** 实验表明，加法正则化的k-means算法常产生多解，而结合乘法正则化可以减少这种不确定性；同时展示了当聚类偏离理想假设时算法的表现。

**结论:** 正则化k-means算法可以通过适当的λ值选择和正则化形式，在一定程度上解决聚类数量估计问题，但实际效果依赖于数据分布特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Number+of+Clusters+in+a+Dataset%3A+A+Regularized+K-means+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22991，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22991&send_immediately=true&force_search=false)

**原文摘要:** Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [44] [Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation](https://arxiv.org/abs/2505.22994)
*Ari S. Benjamin, Kyle Daruwalla, Christian Pehle, Anthony M. Zador*

**主要类别:** cs.LG

**概要:** 提出了一种受神经调节启发的方法，通过在权重空间中优化平滑流形来学习任务上下文变量的平滑参数化函数。这种方法能够更好地泛化到分布外样本，并优于传统的输入拼接条件方法。


<details>
  <summary>更多</summary>
  
**动机:** 在人工神经网络中，通常通过将上下文作为输入注入来实现跨任务的知识重用，但大脑使用的是神经调节策略，即根据神经调节剂调整参数本身。因此，研究者希望从神经调节中汲取灵感，提出一种新的方法来更有效地学习一系列相似任务。

**方法:** 研究者提出学习权重，使其成为任务上下文变量的平滑参数化函数。不是优化一个权重向量，而是优化权重空间中的一个具有预定义拓扑结构的平滑流形。他们推导出一种形式化的处理方法，将流形的优化视为在体积移动约束下最小化损失函数的过程，类似于梯度下降。推理时，条件选择该流形上的一个点作为特定子任务的有效权重矩阵。

**结果:** 通过训练具有不同拓扑结构（如直线和椭圆）的流形，验证了该方法的有效性。尽管这些参数化简单，但它们的表现优于通过输入拼接进行条件相同的网络，并且对分布外样本有更好的泛化能力。

**结论:** 在权重空间上通过低维流形调制权重提供了一个有原则且有效的传统条件方法的替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Walking+the+Weight+Manifold%3A+a+Topological+Approach+to+Conditioning+Inspired+by+Neuromodulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22994，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22994&send_immediately=true&force_search=false)

**原文摘要:** One frequently wishes to learn a range of similar tasks as efficiently as
possible, re-using knowledge across tasks. In artificial neural networks, this
is typically accomplished by conditioning a network upon task context by
injecting context as input. Brains have a different strategy: the parameters
themselves are modulated as a function of various neuromodulators such as
serotonin. Here, we take inspiration from neuromodulation and propose to learn
weights which are smoothly parameterized functions of task context variables.
Rather than optimize a weight vector, i.e. a single point in weight space, we
optimize a smooth manifold in weight space with a predefined topology. To
accomplish this, we derive a formal treatment of optimization of manifolds as
the minimization of a loss functional subject to a constraint on volumetric
movement, analogous to gradient descent. During inference, conditioning selects
a single point on this manifold which serves as the effective weight matrix for
a particular sub-task. This strategy for conditioning has two main advantages.
First, the topology of the manifold (whether a line, circle, or torus) is a
convenient lever for inductive biases about the relationship between tasks.
Second, learning in one state smoothly affects the entire manifold, encouraging
generalization across states. To verify this, we train manifolds with several
topologies, including straight lines in weight space (for conditioning on e.g.
noise level in input data) and ellipses (for rotated images). Despite their
simplicity, these parameterizations outperform conditioning identical networks
by input concatenation and better generalize to out-of-distribution samples.
These results suggest that modulating weights over low-dimensional manifolds
offers a principled and effective alternative to traditional conditioning.

</details>


### [45] [LLM Agents for Bargaining with Utility-based Feedback](https://arxiv.org/abs/2505.22998)
*Jihwan Oh, Murad Aghazada, Se-Young Yun, Taehyeon Kim*

**主要类别:** cs.LG

**概要:** 为了提升大型语言模型（LLMs）在真实世界谈判中的能力，本文提出了一个以效用反馈为核心的综合框架。包括BargainArena数据集、基于效用理论的评估指标以及结构化反馈机制，这些有助于LLMs改进其策略和对手意识推理能力。实验表明，结构化反馈机制能显著提升LLMs的表现。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在谈判方面存在战略深度不足和难以适应复杂人类因素的问题，而现有基准测试往往无法捕捉到这种现实世界的复杂性。因此需要一种新的方法来增强LLMs在真实谈判场景中的表现。

**方法:** 提出了一种综合框架，包含：1) BargainArena数据集，具有六个复杂场景；2) 基于效用理论的人类对齐且经济基础的评估指标；3) 结构化反馈机制，允许LLMs通过迭代改进其谈判策略。

**结果:** 实验结果表明，LLMs通常表现出与人类偏好不一致的谈判策略，但使用结构化反馈机制后，其性能显著提高，展现出更深层次的战略和对手意识推理能力。

**结论:** 本文提出的框架有效提升了LLMs在谈判任务中的表现，特别是在战略深度和对手意识方面。这为未来研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+Agents+for+Bargaining+with+Utility-based+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22998&send_immediately=true&force_search=false)

**原文摘要:** Bargaining, a critical aspect of real-world interactions, presents challenges
for large language models (LLMs) due to limitations in strategic depth and
adaptation to complex human factors. Existing benchmarks often fail to capture
this real-world complexity. To address this and enhance LLM capabilities in
realistic bargaining, we introduce a comprehensive framework centered on
utility-based feedback. Our contributions are threefold: (1) BargainArena, a
novel benchmark dataset with six intricate scenarios (e.g., deceptive
practices, monopolies) to facilitate diverse strategy modeling; (2)
human-aligned, economically-grounded evaluation metrics inspired by utility
theory, incorporating agent utility and negotiation power, which implicitly
reflect and promote opponent-aware reasoning (OAR); and (3) a structured
feedback mechanism enabling LLMs to iteratively refine their bargaining
strategies. This mechanism can positively collaborate with in-context learning
(ICL) prompts, including those explicitly designed to foster OAR. Experimental
results show that LLMs often exhibit negotiation strategies misaligned with
human preferences, and that our structured feedback mechanism significantly
improves their performance, yielding deeper strategic and opponent-aware
reasoning.

</details>


### [46] [Hybrid Cross-domain Robust Reinforcement Learning](https://arxiv.org/abs/2505.23003)
*Linh Le Pham Van, Minh Hoang Nguyen, Hung Le, Hung The Tran, Sunil Gupta*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为HYDRO的混合跨域鲁棒强化学习框架，旨在通过使用在线模拟器补充有限的离线数据集，解决鲁棒强化学习中的样本效率问题。HYDRO通过测量和最小化模拟器与最坏情况模型之间的性能差距，采用新颖的不确定性过滤和优先级采样方法来选择最相关和可靠的模拟器样本。实验表明，HYDRO在各种任务中优于现有方法，提高了离线鲁棒RL的样本效率。


<details>
  <summary>更多</summary>
  
**动机:** 当前的鲁棒强化学习方法需要大量的离线数据，这在特定任务和环境中既昂贵又耗时。虽然使用不完美的模拟器可以更快、更便宜、更安全地收集数据，但可能会导致动态不匹配的问题。因此，需要一种能够有效利用有限离线数据并克服模拟器动态不匹配的方法。

**方法:** 提出了HYDRO框架，该框架通过在线模拟器补充有限的离线数据集。具体而言，HYDRO通过测量和最小化模拟器与不确定性集合中最坏情况模型之间的性能差距，采用不确定性过滤和优先级采样的技术，以选择最相关和可靠的模拟器样本。

**结果:** 实验结果表明，HYDRO在多种任务上表现优于现有的方法，证明了其在提高离线鲁棒强化学习样本效率方面的潜力。

**结论:** HYDRO作为一种混合跨域鲁棒强化学习框架，成功地解决了鲁棒强化学习中样本效率低下的问题，展示了其在实际应用中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Cross-domain+Robust+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23003&send_immediately=true&force_search=false)

**原文摘要:** Robust reinforcement learning (RL) aims to learn policies that remain
effective despite uncertainties in its environment, which frequently arise in
real-world applications due to variations in environment dynamics. The robust
RL methods learn a robust policy by maximizing value under the worst-case
models within a predefined uncertainty set. Offline robust RL algorithms are
particularly promising in scenarios where only a fixed dataset is available and
new data cannot be collected. However, these approaches often require extensive
offline data, and gathering such datasets for specific tasks in specific
environments can be both costly and time-consuming. Using an imperfect
simulator offers a faster, cheaper, and safer way to collect data for training,
but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,
the first Hybrid Cross-Domain Robust RL framework designed to address these
challenges. HYDRO utilizes an online simulator to complement the limited amount
of offline datasets in the non-trivial context of robust RL. By measuring and
minimizing performance gaps between the simulator and the worst-case models in
the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized
sampling to select the most relevant and reliable simulator samples. Our
extensive experiments demonstrate HYDRO's superior performance over existing
methods across various tasks, underscoring its potential to improve sample
efficiency in offline robust RL.

</details>


### [47] [QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining](https://arxiv.org/abs/2505.23004)
*Kyle R. Chickering, Bangzheng Li, Muhao Chen*

**主要类别:** cs.LG

**概要:** 提出QLIP模型，作为CLIP视觉编码器的替代方案，通过创新的图像四叉树方法改进了视觉理解和多模态表示学习，无需重新训练整个模型即可提升LLaVA等多模态大语言模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** CLIP视觉编码器存在局限性，例如只能处理固定输入分辨率以及无法为不同图像生成分离的嵌入。此外，替换现有模型的视觉编码器通常需要高昂的计算成本，因为这通常需要重新训练整个模型管道。

**方法:** 识别出CLIP视觉编码器的两个问题根源：中观偏差和插值偏差，并提出QLIP作为CLIP的直接替代方案。QLIP基于图像四叉树设计，用内容感知的补丁化方法取代标准的均匀网格补丁，从而增强粗粒度和细粒度的视觉理解能力。

**结果:** 实验结果表明，QLIP在不需重新训练或微调的情况下，提升了LLaVA v1.5模型系列在多种模型尺寸下的通用视觉问答准确率。特别是在挑战性的$V^{\ast}$基准上，QLIP将详细理解性能提高了多达13.6%。

**结论:** QLIP可以无缝集成到现有的多模态大语言模型中，仅需几行代码即可增强视觉理解能力，解决了CLIP视觉编码器的局限性，显著提升了多模态模型的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QLIP%3A+A+Dynamic+Quadtree+Vision+Prior+Enhances+MLLM+Performance+Without+Retraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23004&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) encode images into visual tokens,
aligning visual and textual signals within a shared latent space to facilitate
crossmodal representation learning. The CLIP model is a widely adopted
foundational vision language model whose vision encoder has played a critical
role in the development of MLLMs such as LLaVA. However, the CLIP vision
encoder suffers from notable limitations including being constrained to only
handling fixed input resolutions and a failure to produce separated embeddings
for dissimilar images. Replacing the vision encoder of an existing model
typically incurs substantial computational costs because such a change often
necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the
CLIP vision encoder: mesoscopic bias and interpolation bias. To address these
issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly
integrated with existing MLLMs with only a few lines of code and can enhance
both coarse-grained and fine-grained visual understanding, without re-training.
QLIP is designed around an image quadtree which replaces the standard uniform
grid patches with a novel content aware patchification. Our experimental
results demonstrate that QLIP improves the general visual question answering
accuracy of the LLaVA v1.5 model series across various model sizes--without
requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts
detailed understanding performance on the challenging $V^{\ast}$ benchmark by
up to 13.6 percent.

</details>


### [48] [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009)
*Ruskin Raj Manku, Yuzhi Tang, Xingjian Shi, Mu Li, Alex Smola*

**主要类别:** cs.LG

**概要:** 提出EmergentTTS-Eval，一个涵盖六种复杂TTS场景的全面基准测试框架。该框架自动化生成测试用例和评估，并使用大型音频语言模型（LALM）作为评估者。通过评估多个先进的TTS系统，证明了此方法能有效揭示细微性能差异并具有与人类偏好高度相关的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的TTS基准测试常无法充分评估模型对细腻且语义复杂的文本处理能力，因此需要更全面、自动化的评估框架。

**方法:** 基于EmergentTTS构建EmergentTTS-Eval，包含六个挑战性场景；利用小型人类编写种子提示结合LLMs生成1,645个多样测试用例；采用模型作为评判方式，使用LALM从多维度评估语音质量。

**结果:** 成功区分了多种先进TTS系统的细微性能差异，模型作为评判者的评估方法表现出高鲁棒性和与人类偏好高度相关的结果。

**结论:** EmergentTTS-Eval为TTS系统提供了一个强大的自动化评估工具，其开源代码和数据集有助于进一步研究和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EmergentTTS-Eval%3A+Evaluating+TTS+Models+on+Complex+Prosodic%2C+Expressiveness%2C+and+Linguistic+Challenges+Using+Model-as-a-Judge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23009，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23009&send_immediately=true&force_search=false)

**原文摘要:** Text-to-Speech (TTS) benchmarks often fail to capture how well models handle
nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we
introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six
challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic
complexity, complex pronunciation (e.g. URLs, formulas), and questions.
Crucially, our framework automates both test-case generation and evaluation,
making the benchmark easily extensible. Starting from a small set of
human-written seed prompts, we iteratively extend them using LLMs to target
specific structural, phonetic and prosodic challenges, resulting in 1,645
diverse test cases. Moreover, we employ a model-as-a-judge approach, using a
Large Audio Language Model (LALM) to assess the speech across multiple
dimensions such as expressed emotion, prosodic, intonational, and pronunciation
accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,
such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,
demonstrating its ability to reveal fine-grained performance differences.
Results show that the model-as-a-judge approach offers robust TTS assessment
and a high correlation with human preferences. We open source the evaluation
$\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the
$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

</details>


### [49] [Scalable Complexity Control Facilitates Reasoning Ability of LLMs](https://arxiv.org/abs/2505.23013)
*Liangkai Hang, Junjie Yao, Zhiwei Bai, Tianyi Chen, Yang Chen, Rongjie Diao, Hezhou Li, Pengxiao Lin, Zhiwei Wang, Cheng Xu, Zhongwang Zhang, Zhangchen Zhou, Zhiyu Li, Zehao Lin, Kai Chen, Feiyu Xiong, Yaoyu Zhang, Weinan E, Hongkang Yang, Zhi-Qin John Xu*

**主要类别:** cs.LG

**概要:** 通过调整初始化速率和权重衰减系数来控制模型复杂度，可以改进大语言模型（LLMs）在不同模型规模和数据规模下的扩展规律。相比固定初始化标准差，采用恒定初始化速率（标准差的指数）能让扩展规律更快下降。这表明复杂度控制是提升LLMs的一个有希望的方向。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，大语言模型的推理能力迅速发展，因此研究者对能够可靠增强其泛化能力的基础方法产生了兴趣。本研究探索了通过控制模型复杂度来改进LLMs的方法。

**方法:** 通过调整初始化速率和权重衰减系数实现模型复杂度控制，并对比分析2.4B参数模型在1T token预训练数据上的基准性能表现，其中使用不同的复杂度超参数。此外，研究比较了固定初始化标准差与采用恒定初始化速率的效果。

**结果:** 结果表明，模型复杂度控制能一致地改善LLMs在不同模型规模和数据规模下的扩展规律；采用恒定初始化速率相较于固定初始化标准差，能让扩展规律更快地下降。

**结论:** 复杂度控制是一个有前景的研究方向，有助于持续推动大语言模型的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Complexity+Control+Facilitates+Reasoning+Ability+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23013，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23013&send_immediately=true&force_search=false)

**原文摘要:** The reasoning ability of large language models (LLMs) has been rapidly
advancing in recent years, attracting interest in more fundamental approaches
that can reliably enhance their generalizability. This work demonstrates that
model complexity control, conveniently implementable by adjusting the
initialization rate and weight decay coefficient, improves the scaling law of
LLMs consistently over varying model sizes and data sizes. This gain is further
illustrated by comparing the benchmark performance of 2.4B models pretrained on
1T tokens with different complexity hyperparameters. Instead of fixing the
initialization std, we found that a constant initialization rate (the exponent
of std) enables the scaling law to descend faster in both model and data sizes.
These results indicate that complexity control is a promising direction for the
continual advancement of LLMs.

</details>


### [50] [Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2505.23014)
*Juwei Yue, Haikuo Li, Jiawei Sheng, Xiaodong Li, Taoyu Su, Tingwen Liu, Li Guo*

**主要类别:** cs.LG

**概要:** 本文提出了一种基于双曲偏微分方程（PDE）的消息传递机制，增强了谱图神经网络（spectral GNNs）的性能和可解释性。通过求解双曲PDE系统，节点特征可以分解为描述图拓扑结构的特征向量基的叠加，从而显式提取拓扑特性，并引入多项式近似任意滤波函数。实验表明该方法具有强灵活性并显著提升多种谱GNN在不同图任务上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 传统图神经网络（GNNs）在空间域学习节点特征，与拓扑结构无关，难以确保捕捉到拓扑特征。因此需要一种能明确映射节点表示到特定解空间的方法，以增强对图拓扑结构的理解和表征能力。

**方法:** 将消息传递建模为一个双曲偏微分方程系统，形成动态系统，将节点表示显式映射到由描述图拓扑结构的特征向量张成的解空间。通过时间分解节点特征为特征向量基的叠加，建立与谱图神经网络（spectral GNNs）的联系，作为其消息传递的增强范式，并引入多项式来近似任意滤波函数。

**结果:** 广泛的实验验证了双曲PDE范式不仅具备强大的灵活性，而且显著提高了各种谱GNN在不同图任务中的性能。

**结论:** 基于双曲PDE的消息传递机制能够增强谱GNN的性能和可解释性，提供了一种灵活且有效的工具来处理图数据的拓扑特征。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyperbolic-PDE+GNN%3A+Spectral+Graph+Neural+Networks+in+the+Perspective+of+A+System+of+Hyperbolic+Partial+Differential+Equations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23014&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) leverage message passing mechanisms to learn the
topological features of graph data. Traditional GNNs learns node features in a
spatial domain unrelated to the topology, which can hardly ensure topological
features. In this paper, we formulates message passing as a system of
hyperbolic partial differential equations (hyperbolic PDEs), constituting a
dynamical system that explicitly maps node representations into a particular
solution space. This solution space is spanned by a set of eigenvectors
describing the topological structure of graphs. Within this system, for any
moment in time, a node features can be decomposed into a superposition of the
basis of eigenvectors. This not only enhances the interpretability of message
passing but also enables the explicit extraction of fundamental characteristics
about the topological structure. Furthermore, by solving this system of
hyperbolic partial differential equations, we establish a connection with
spectral graph neural networks (spectral GNNs), serving as a message passing
enhancement paradigm for spectral GNNs.We further introduce polynomials to
approximate arbitrary filter functions. Extensive experiments demonstrate that
the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also
significantly enhances the performance of various spectral GNNs across diverse
graph tasks.

</details>


### [51] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.23017)
*Xingjian Wu, Xiangfei Qiu, Hongfan Gao, Jilin Hu, Bin Yang, Chenjuan Guo*

**主要类别:** cs.LG

**概要:** 提出了一种名为$K^2$VAE的新模型，通过将非线性时间序列转化为线性动力系统并优化预测，从而在短期和长期概率时间序列预测中超越现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列预测方法擅长短期预测，但在长期预测方面存在不足，尤其是在处理非线性动态时会显著影响预测精度，并增加生成模型的迭代成本。

**方法:** 引入了$K^2$VAE模型，该模型利用KoopmanNet将非线性时间序列转换为线性动力系统，同时采用KalmanNet来优化预测并建模不确定性，以减少长期预测中的误差累积。

**结果:** 广泛的实验表明，$K^2$VAE在短期和长期的概率时间序列预测中均优于现有最先进方法，提供了更高效和精确的解决方案。

**结论:** $K^2$VAE是一种有效的生成模型，能够克服长期概率时间序列预测中的挑战，提供更精确和高效的预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24K%5E2%24VAE%3A+A+Koopman-Kalman+Enhanced+Variational+AutoEncoder+for+Probabilistic+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23017，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23017&send_immediately=true&force_search=false)

**原文摘要:** Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [52] [SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference](https://arxiv.org/abs/2505.23022)
*Yinghao Tang, Tingfeng Lan, Xiuqi Huang, Hui Lu, Wei Chen*

**主要类别:** cs.LG

**概要:** SCORPIO 是一个以 SLO 为导向的大型语言模型服务系统，通过 TTFT Guard 和 TPOT Guard 提高系统的有效吞吐量和 SLO 达成率，相较于现有基线最多可提升 14.4 倍的有效吞吐量和 46.5% 的 SLO 遵守率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型服务系统过于关注最大吞吐量，而忽视了如首次生成令牌时间（TTFT）和每输出令牌时间（TPOT）等服务质量目标（SLOs），导致 SLO 达成效果不佳。

**方法:** SCORPIO 利用 SLO 异质性进行自适应调度，在准入控制、队列管理和批量选择等方面优化性能。系统包含 TTFT Guard（采用最小截止期限优先重排和拒绝不可达请求）和 TPOT Guard（使用基于 VBS 的准入控制和新颖的基于信用的批量机制），两者均由预测模块支持。

**结果:** 评估结果表明，与最先进的基线相比，SCORPIO 将系统有效吞吐量提高了最高 14.4 倍，并将 SLO 遵守率提升了最高 46.5%。

**结论:** SCORPIO 成功地通过最大化系统有效吞吐量和 SLO 达成率来应对具有异构 SLO 的工作负载，为未来 LLM 服务系统的设计提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCORPIO%3A+Serving+the+Right+Requests+at+the+Right+Time+for+Heterogeneous+SLOs+in+LLM+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23022，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23022&send_immediately=true&force_search=false)

**原文摘要:** Existing Large Language Model (LLM) serving systems prioritize maximum
throughput. They often neglect Service Level Objectives (SLOs) such as Time to
First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal
SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving
system designed to maximize system goodput and SLO attainment for workloads
with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for
adaptive scheduling across admission control, queue management, and batch
selection. SCORPIO features a TTFT Guard, which employs least-deadline-first
reordering and rejects unattainable requests, and a TPOT Guard, which utilizes
a VBS-based admission control and a novel credit-based batching mechanism. Both
guards are supported by a predictive module. Evaluations demonstrate that
SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%
compared to state-of-the-art baselines.

</details>


### [53] [An Empirical Study of Federated Prompt Learning for Vision Language Model](https://arxiv.org/abs/2505.23024)
*Zhihao Wang, Wenke Huang, Tian Chen, Zekun Shi, Guancheng Wan, Yu Qiao, Bin Yang, Jian Wang, Bing Li, Mang Ye*

**主要类别:** cs.LG

**概要:** 本论文探讨了在联邦学习场景中，语言提示学习（LPT）和视觉提示学习（VPT）的行为差异，并通过广泛实验评估不同配置对联邦提示学习（FPL）鲁棒性的影响，同时探索了在标签倾斜和领域偏移共存的复杂场景下提升提示学习的策略。研究结果为优化隐私保护环境下视觉语言模型的提示学习提供了实际见解。


<details>
  <summary>更多</summary>
  
**动机:** 当前对于在联邦学习场景中应用提示学习与视觉语言模型的研究尚不充分，特别是在数据异质性挑战（如标签倾斜和领域偏移）下的表现尚未被系统研究。

**方法:** 1. 系统研究语言提示学习和视觉提示学习在联邦学习环境中的行为差异。
2. 通过大量实验评估不同联邦学习和提示配置（如客户端规模、聚合策略和提示长度）对联邦提示学习鲁棒性的影响。
3. 探索在标签倾斜和领域偏移共存的情况下提升提示学习的策略，包括在计算资源允许时结合使用两种提示类型。

**结果:** 发现语言提示学习和视觉提示学习在面对数据异质性挑战时具有不同的行为特性。同时，多种策略可以有效增强提示学习在复杂联邦学习场景中的性能。

**结论:** 本研究为在联邦学习环境中优化提示学习提供了实用见解，有助于更广泛地部署视觉语言模型于隐私保护的应用场景中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Empirical+Study+of+Federated+Prompt+Learning+for+Vision+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23024&send_immediately=true&force_search=false)

**原文摘要:** The Vision Language Model (VLM) excels in aligning vision and language
representations, and prompt learning has emerged as a key technique for
adapting such models to downstream tasks. However, the application of prompt
learning with VLM in federated learning (\fl{}) scenarios remains
underexplored. This paper systematically investigates the behavioral
differences between language prompt learning (LPT) and vision prompt learning
(VPT) under data heterogeneity challenges, including label skew and domain
shift. We conduct extensive experiments to evaluate the impact of various \fl{}
and prompt configurations, such as client scale, aggregation strategies, and
prompt length, to assess the robustness of Federated Prompt Learning (FPL).
Furthermore, we explore strategies for enhancing prompt learning in complex
scenarios where label skew and domain shift coexist, including leveraging both
prompt types when computational resources allow. Our findings offer practical
insights into optimizing prompt learning in federated settings, contributing to
the broader deployment of VLMs in privacy-preserving environments.

</details>


### [54] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/abs/2505.23027)
*Minh Nguyen Nhat To, Paul F RWilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi, Rahul G. Krishnan*

**主要类别:** cs.LG

**概要:** The paper proposes Diverse Prototypical Ensembles (DPEs), an ensemble method using diverse classifiers to handle subpopulation shifts in machine learning models, achieving better worst-group accuracy across nine real-world datasets.


<details>
  <summary>更多</summary>
  
**动机:** Subpopulation shift can significantly degrade the performance of machine learning models. Current methods rely on assumptions about subpopulations and annotations which are often unavailable for many real-world datasets.

**方法:** An ensemble of diverse classifiers is used to adaptively capture risk associated with subpopulations. A feature extractor network's standard linear classification layer is replaced with a mixture of prototypical classifiers, each trained to focus on different features and samples.

**结果:** DPEs outperform prior state-of-the-art methods in worst-group accuracy across nine real-world datasets covering diverse domains and kinds of subpopulation shift.

**结论:** DPEs provide an effective solution for handling subpopulation shifts without relying on specific assumptions or annotations about subpopulations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diverse+Prototypical+Ensembles+Improve+Robustness+to+Subpopulation+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23027，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23027&send_immediately=true&force_search=false)

**原文摘要:** The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [55] [Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks](https://arxiv.org/abs/2505.23032)
*Dongwoo Lee, Dong Bok Lee, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Frank Hutter, Seon Joo Kim, Hae Beom Lee*

**主要类别:** cs.LG

**概要:** 本文提出了一种基于贝叶斯框架的Prior-data Fitted Networks (PFNs) 方法，用于神经网络扩展定律的外推。通过设计先验分布，该方法能够采样出无数类似于真实世界神经扩展定律的合成函数，从而实现对扩展行为的元学习。实验表明，该方法在数据有限的情况下表现优异，特别是在贝叶斯主动学习中，展示了其在实际应用中可靠且具有不确定性感知的外推潜力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩展定律研究大多依赖点估计方法，无法量化不确定性，这对需要决策的实际应用（如计算资源投资）来说是一个重要缺陷。因此，需要一种能够进行不确定性感知的扩展行为预测方法。

**方法:** 作者采用了一个贝叶斯框架，并基于Prior-data Fitted Networks (PFNs) 设计了一种新的方法。通过构建一个先验分布，可以生成大量类似真实世界神经扩展定律的合成函数，使PFN能够进行元学习以完成外推任务。

**结果:** 该方法在真实世界的神经扩展定律上进行了验证，并与现有的点估计方法和贝叶斯方法进行了比较。结果表明，该方法在数据有限的情况下（例如贝叶斯主动学习场景）表现尤为出色。

**结论:** 提出的贝叶斯框架下的PFN方法能够在实际应用中提供可靠的、具有不确定性感知的扩展行为预测，尤其适用于数据有限的情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Neural+Scaling+Laws+Extrapolation+with+Prior-Fitted+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23032&send_immediately=true&force_search=false)

**原文摘要:** Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [56] [From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data](https://arxiv.org/abs/2505.23042)
*Siwen Wang, Shitou Zhang, Wan-Lin Chen, Dung Truong, Tzyy-Ping Jung*

**主要类别:** cs.LG

**概要:** 近期大型语言模型的进步推动了基础模型在各领域的开发。本研究通过在研究生课堂收集的现实世界压力分类数据集上微调最先进的基础EEG模型LaBraM，评估了大型EEG模型（LEMs）的有效性。与主要使用受控临床环境数据评估LEMs的先前研究不同，我们的工作评估了它们在现实世界环境中的适用性。我们训练了一个二元分类器，使用从18名研究生在一节课期间记录的静息状态EEG数据来区分正常和升高的压力状态。表现最佳的微调模型在5秒窗口内达到了90.47%的平衡准确率，显著优于传统压力分类器的准确性和推理效率。我们进一步评估了在随机数据打乱和减少通道数量情况下的微调LEMs的鲁棒性。这些结果表明LEMs能够有效处理现实世界的EEG数据，并突显了它们通过从以模型为中心转向以数据为中心的设计来革新脑机接口应用的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 近期大型语言模型的发展启发了各领域基础模型的开发，然而大多数关于大型EEG模型（LEMs）的研究仅限于受控临床环境的数据评估。因此，有必要探索LEMs在真实世界环境中的表现，特别是在压力分类任务中的能力。这促使研究团队在真实世界数据集上微调一个先进的基础EEG模型LaBraM，以评估其在实际场景中的适用性。

**方法:** 研究人员在研究生课堂环境中收集了一个现实世界的压力分类数据集，该数据集包含来自18名研究生在一节课期间的静息状态EEG数据。他们使用LaBraM这一最先进的基础EEG模型进行微调，并构建了一个二元分类器来区分正常和升高的压力状态。为了评估模型的性能，他们采用了5秒时间窗口的EEG数据，并与传统压力分类器进行了比较。此外，还对微调后的LEMs在随机数据打乱和减少通道数量条件下的鲁棒性进行了测试。

**结果:** 经过微调的最佳模型在5秒窗口的EEG数据上实现了90.47%的平衡准确率，显著优于传统压力分类器。此外，该模型在面对随机数据打乱和减少通道数量时仍表现出良好的鲁棒性，证明了其在真实世界环境中的有效性。

**结论:** 本研究表明，大型EEG模型（LEMs）能够高效处理真实世界中的EEG数据，并在压力分类任务中展现出卓越的性能。这种能力不仅提升了压力检测的准确性，还提高了推理效率。更重要的是，LEMs的应用可能标志着脑机接口设计从以模型为中心向以数据为中心的转变，为未来相关应用的发展提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Theory+to+Application%3A+Fine-Tuning+Large+EEG+Model+with+Real-World+Stress+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23042&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models have inspired the development of
foundation models across various domains. In this study, we evaluate the
efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art
foundation EEG model, on a real-world stress classification dataset collected
in a graduate classroom. Unlike previous studies that primarily evaluate LEMs
using data from controlled clinical settings, our work assesses their
applicability to real-world environments. We train a binary classifier that
distinguishes between normal and elevated stress states using resting-state EEG
data recorded from 18 graduate students during a class session. The
best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a
5-second window, significantly outperforming traditional stress classifiers in
both accuracy and inference efficiency. We further evaluate the robustness of
the fine-tuned LEM under random data shuffling and reduced channel counts.
These results demonstrate the capability of LEMs to effectively process
real-world EEG data and highlight their potential to revolutionize
brain-computer interface applications by shifting the focus from model-centric
to data-centric design.

</details>


### [57] [ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation](https://arxiv.org/abs/2505.23048)
*Tianci Bu, Le Zhou, Wenchuan Yang, Jianhong Mou, Kang Yang, Suoyi Tan, Feng Yao, Jingyuan Wang, Xin Lu*

**主要类别:** cs.LG

**概要:** 轨迹数据对于各种应用至关重要，但由于设备限制和多样化的收集场景，常常存在不完整性。现有的插补方法依赖于稀疏轨迹或旅行信息（如速度）来推断缺失点，但这些方法假设稀疏轨迹保留了基本的行为模式，这对数据获取提出了很高的要求，并且忽略了大规模人类轨迹嵌入的潜力。为了解决这个问题，我们提出了ProDiff，一种仅使用两个端点作为最小信息的轨迹插补框架。它集成了原型学习以嵌入人类运动模式，并采用去噪扩散概率模型进行强大的时空重建。联合训练与定制的损失函数确保了有效的插补。ProDiff在FourSquare上提高了6.28%的准确率，在WuXi上提高了2.52%，并且生成轨迹与真实轨迹之间的相关性达到0.927，证明了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的轨迹插补方法依赖于稀疏轨迹或旅行信息，假设这些稀疏轨迹保留了基本行为模式，这不仅对数据获取提出了很高要求，还忽略了大规模人类轨迹嵌入的潜力。因此，需要一个更高效、利用更少信息的方法来进行轨迹插补。

**方法:** 提出了一种名为ProDiff的轨迹插补框架，该框架仅使用两个端点作为最小信息。通过集成原型学习来嵌入人类运动模式，并采用去噪扩散概率模型进行强大的时空重建。此外，使用定制的损失函数进行联合训练以确保有效的插补。

**结果:** ProDiff在多个数据集上的表现优于现有方法：在FourSquare数据集上准确率提高6.28%，在WuXi数据集上提高2.52%。进一步分析表明，生成轨迹与真实轨迹之间的相关性高达0.927。

**结论:** ProDiff是一种有效的轨迹插补框架，能够利用最少的信息（两个端点）实现高精度的轨迹重建。其性能优于现有方法，并展示了生成轨迹与真实轨迹的高度相关性，证明了该方法的有效性和潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProDiff%3A+Prototype-Guided+Diffusion+for+Minimal+Information+Trajectory+Imputation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23048&send_immediately=true&force_search=false)

**原文摘要:** Trajectory data is crucial for various applications but often suffers from
incompleteness due to device limitations and diverse collection scenarios.
Existing imputation methods rely on sparse trajectory or travel information,
such as velocity, to infer missing points. However, these approaches assume
that sparse trajectories retain essential behavioral patterns, which place
significant demands on data acquisition and overlook the potential of
large-scale human trajectory embeddings. To address this, we propose ProDiff, a
trajectory imputation framework that uses only two endpoints as minimal
information. It integrates prototype learning to embed human movement patterns
and a denoising diffusion probabilistic model for robust spatiotemporal
reconstruction. Joint training with a tailored loss function ensures effective
imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by
6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927
correlation between generated and real trajectories, demonstrating the
effectiveness of our approach.

</details>


### [58] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
*Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, Yanmin Qian*

**主要类别:** cs.LG

**概要:** 本文提出了DenoiseRotator方法，它通过重新分配参数重要性使模型更容易被修剪。该方法与现有修剪技术无缝集成，实验表明其能有效减少修剪后的性能下降。


<details>
  <summary>更多</summary>
  
**动机:** 修剪大型语言模型（LLMs）时，通常会因移除不重要的权重而导致性能显著下降，特别是在半结构化稀疏性约束下。现有的修剪方法主要关注估计单个权重的重要性，这限制了它们保持模型关键能力的能力。

**方法:** 提出了一种通过最小化归一化重要性分数的信息熵，将重要性集中在更小的权重子集上的方法，从而增强修剪鲁棒性。具体实现是通过DenoiseRotator应用可学习的正交变换到模型的权重矩阵上。该方法与现有的修剪技术（如Magnitude、SparseGPT和Wanda）无缝集成，并且模型无关。

**结果:** 在LLaMA3、Qwen2.5和Mistral模型上进行实验，分别在50%非结构化和2:4半结构化稀疏性下进行测试，结果表明DenoiseRotator可以持续改善困惑度和零样本准确性。

**结论:** DenoiseRotator在LLaMA3、Qwen2.5和Mistral模型上进行了评估，能够显著提高困惑度和零样本准确性。例如，在以2:4半结构化稀疏性修剪的LLaMA3-70B上，DenoiseRotator将与密集模型的困惑度差距减少了58%，从8.1降至3.4点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DenoiseRotator%3A+Enhance+Pruning+Robustness+for+LLMs+via+Importance+Concentration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23049&send_immediately=true&force_search=false)

**原文摘要:** Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [59] [CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents](https://arxiv.org/abs/2505.23055)
*Zhen Xiang, Aliyah R. Hsu, Austin V. Zane, Aaron E. Kornblith, Margaret J. Lin-Martore, Jasmanpreet C. Kaur, Vasuda M. Dokiparthi, Bo Li, Bin Yu*

**主要类别:** cs.LG

**概要:** 在急诊科（ED）中，临床决策复杂且节奏快。虽然临床决策规则（CDR）能帮助做出一致且准确的诊断，但医生的认知负担限制了其使用。本文介绍了一种新的基于LLM的系统CDR-Agent，它可以自动识别和应用适当的CDRs。通过两个新数据集验证，CDR-Agent在CDR选择上比单独的LLM基线提高了56.3%（合成数据集）和8.7%（CDR-Bench数据集）的准确性，并显著减少了计算开销。此外，CDR-Agent还能有效减少不必要的干预，同时成功识别大多数阳性诊断病例。


<details>
  <summary>更多</summary>
  
**动机:** 临床决策在急诊科尤其复杂和快速，而现有的CDRs由于医生的认知负担重，难以迅速回忆和应用。因此，需要一种工具来减轻医生的认知负担并提高决策效率和准确性。

**方法:** 引入了一个名为CDR-Agent的新型LLM系统，该系统可以根据非结构化的临床笔记自主识别和应用最合适的CDRs。为了验证CDR-Agent的有效性，研究人员创建了两个新的数据集：synthetic和CDR-Bench。

**结果:** CDR-Agent在CDR选择上的准确率相比单独的LLM基线提升了56.3%（synthetic数据集）和8.7%（CDR-Bench数据集）。它还显著减少了计算开销，并且在减少不必要干预的同时，成功识别了大多数阳性诊断病例。

**结论:** CDR-Agent不仅提高了CDR选择的准确性，还有效地减少了计算开销和不必要的干预，为急诊科及其他诊所提供了更高效的决策支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CDR-Agent%3A+Intelligent+Selection+and+Execution+of+Clinical+Decision+Rules+Using+Large+Language+Model+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23055，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23055&send_immediately=true&force_search=false)

**原文摘要:** Clinical decision-making is inherently complex and fast-paced, particularly
in emergency departments (EDs) where critical, rapid and high-stakes decisions
are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools
that combine signs, symptoms, and clinical variables into decision trees to
make consistent and accurate diagnoses. CDR usage is often hindered by the
clinician's cognitive load, limiting their ability to quickly recall and apply
the appropriate rules. We introduce CDR-Agent, a novel LLM-based system
designed to enhance ED decision-making by autonomously identifying and applying
the most appropriate CDRs based on unstructured clinical notes. To validate
CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although
CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\%
(synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM
baseline in CDR selection. Moreover, CDR-Agent significantly reduces
computational overhead. Using these datasets, we demonstrated that CDR-Agent
not only selects relevant CDRs efficiently, but makes cautious yet effective
imaging decisions by minimizing unnecessary interventions while successfully
identifying most positively diagnosed cases, outperforming traditional LLM
prompting approaches. Code for our work can be found at:
https://github.com/zhenxianglance/medagent-cdr-agent

</details>


### [60] [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061)
*Tarun Suresh, Debangshu Banerjee, Shubham Ugare, Sasa Misailovic, Gagandeep Singh*

**主要类别:** cs.LG

**概要:** 扩散大语言模型（Diffusion LLMs）在运行时效率上具有显著潜力，但无法可靠地执行用户指定的形式约束。本文提出了一种基于动态规划的受限解码策略DINGO，可以高效且严格地满足任何用户指定的正则表达式，同时保留输出分布。在标准符号数学和JSON生成基准测试中，DINGO比无约束推理提高了68个百分点。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩散模型无法可靠地施加用户指定的形式约束（如正则表达式），这使得它们在需要结构化输出的任务中表现不佳。

**方法:** 提出了DINGO，一种基于动态规划的受限解码策略，能够在并行预测token块的过程中严格满足用户指定的正则表达式，同时保留模型预测分布。

**结果:** 在标准符号数学和JSON生成基准测试中，DINGO取得了显著改进，相较于无约束推理提升了68个百分点。

**结论:** DINGO为扩散LLMs提供了一种有效的方法来处理形式约束任务，使其在需要结构化输出的应用中更加可靠。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DINGO%3A+Constrained+Inference+for+Diffusion+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23061&send_immediately=true&force_search=false)

**原文摘要:** Diffusion LLMs have emerged as a promising alternative to conventional
autoregressive LLMs, offering significant potential for improved runtime
efficiency. However, existing diffusion models lack the ability to provably
enforce user-specified formal constraints, such as regular expressions, which
makes them unreliable for tasks that require structured outputs, such as
fixed-schema JSON generation. Unlike autoregressive models that generate tokens
sequentially, diffusion LLMs predict a block of tokens in parallel. This
parallelism makes traditional constrained decoding algorithms, which are
designed for sequential token prediction, ineffective at preserving the true
output distribution. To address this limitation, we propose DINGO, a dynamic
programming-based constrained decoding strategy that is both efficient and
provably distribution-preserving. DINGO enables sampling of output strings with
the highest probability under the model's predicted distribution, while
strictly satisfying any user-specified regular expression. On standard symbolic
math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point
improvement over unconstrained inference

</details>


### [61] [Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data](https://arxiv.org/abs/2505.23062)
*Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe*

**主要类别:** cs.LG

**概要:** CompFlow是一种基于流匹配和最优传输理论的方法，通过建模目标动态为条件流并使用Wasserstein距离估计动态差距，提出乐观主动数据收集策略以优先探索高动态差距区域，并在多个RL基准上优于强基线。


<details>
  <summary>更多</summary>
  
**动机:** 离线数据可以提高强化学习的样本效率，但源和目标环境之间的动态差异会削弱这一优势。现有方法通过惩罚或过滤掉高动态差异区域的源转换来解决此问题，但这些方法在源和目标动态支持不重叠时效果不佳。

**方法:** CompFlow将目标动态建模为基于源域流输出分布的条件流，而不是直接从高斯先验中学习。该方法通过Wasserstein距离提供对动态差距的合理估计，并引入乐观主动数据收集策略，优先探索高动态差距区域。

**结果:** 理论上证明了乐观主动数据收集策略可以减少与最优策略的性能差距。实证结果表明，CompFlow在多个具有动态变化的RL基准上优于强基线。

**结论:** CompFlow通过改进的目标动态学习和合理估计动态差距，在处理源和目标环境动态差异方面表现出色，并能有效提升强化学习的样本效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Composite+Flow+Matching+for+Reinforcement+Learning+with+Shifted-Dynamics+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23062，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23062&send_immediately=true&force_search=false)

**原文摘要:** Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.

</details>


### [62] [Equivariant Spherical Transformer for Efficient Molecular Modeling](https://arxiv.org/abs/2505.23086)
*Junyi An, Xinyu Lu, Chao Qu, Yunfei Shi, Peijia Lin, Qianwei Tang, Licheng Xu, Fenglei Cao, Yuan Qi*

**主要类别:** cs.LG

**概要:** SE(3)-等变图神经网络（GNNs）通过使用群表示显著推动了分子系统建模的发展。然而，它们基于张量积卷积的消息传递过程受到非线性不足和群表示不完整性的限制，从而限制了表达能力。为克服这些限制，本文引入了一种新的框架——等变球面变换器（Equivariant Spherical Transformer, EST），该框架在傅里叶变换后的群表示空间域内利用了Transformer结构。理论与实证表明，EST可以涵盖张量积的函数空间，同时实现更优的表达能力。此外，EST的等变归纳偏置通过傅里叶变换的均匀采样策略得到保证。实验结果表明，EST在包括OC20和QM9在内的多个分子基准上达到了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的SE(3)-等变图神经网络（GNNs）在分子系统建模中表现出色，但其基于张量积卷积的消息传递过程存在非线性不足和群表示不完整性的问题，这限制了模型的表达能力。因此，需要一种新方法来增强模型的表达能力和灵活性，以进一步提升分子建模性能。

**方法:** 本文提出了一种名为等变球面变换器（Equivariant Spherical Transformer, EST）的新框架。该框架将Transformer结构应用于傅里叶变换后的群表示空间域，从而增强模型的非线性能力和表达能力。EST通过均匀采样策略确保了等变归纳偏置，并能够涵盖张量积的函数空间，提供更强的表达能力。

**结果:** 实验结果表明，EST在多个分子基准测试任务（如OC20和QM9）上达到了最先进的性能，验证了其在分子系统建模中的优越性和有效性。

**结论:** 等变球面变换器（EST）作为一种新型框架，通过结合Transformer结构和群表示理论，有效解决了现有SE(3)-等变GNNs在表达能力和灵活性方面的局限性，为分子系统建模提供了更强大的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Equivariant+Spherical+Transformer+for+Efficient+Molecular+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23086，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23086&send_immediately=true&force_search=false)

**原文摘要:** SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced
molecular system modeling by employing group representations. However, their
message passing processes, which rely on tensor product-based convolutions, are
limited by insufficient non-linearity and incomplete group representations,
thereby restricting expressiveness. To overcome these limitations, we introduce
the Equivariant Spherical Transformer (EST), a novel framework that leverages a
Transformer structure within the spatial domain of group representations after
Fourier transform. We theoretically and empirically demonstrate that EST can
encompass the function space of tensor products while achieving superior
expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed
through a uniform sampling strategy for the Fourier transform. Our experiments
demonstrate state-of-the-art performance by EST on various molecular
benchmarks, including OC20 and QM9.

</details>


### [63] [Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification](https://arxiv.org/abs/2505.23063)
*Denis Mamba Kabala, Adel Hafiane, Laurent Bobelin, Raphael Canals*

**主要类别:** cs.LG

**概要:** 本论文提出了一种基于验证损失的去中心化联邦学习（DFL）框架，用于作物病害检测与分类任务。通过在PlantVillage数据集上的实验表明，该方法提高了模型精度、收敛速度，并增强了异构数据环境下的泛化能力和鲁棒性，适合隐私保护的农业应用。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度学习模型（如CNN和ViT）虽然在植物病害图像分类中表现出色，但大规模部署受到数据隐私问题的限制。而传统的集中式联邦学习（FL）容易出现单点故障且可扩展性有限。因此，需要一种新的框架来解决这些问题。

**方法:** 提出了一种新的去中心化联邦学习（DFL）框架，利用验证损失（Loss_val）指导模型在节点之间的共享，并通过自适应损失函数（由权重参数控制）校正本地训练。

**结果:** 实验结果表明，DFL方法不仅提高了模型的准确性和收敛速度，还增强了其在异构数据环境中的泛化能力和鲁棒性。

**结论:** 所提出的DFL框架在作物病害检测与分类任务中表现优异，特别适合于需要保护隐私的农业应用场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Loss-Guided+Model+Sharing+and+Local+Learning+Correction+in+Decentralized+Federated+Learning+for+Crop+Disease+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23063，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23063&send_immediately=true&force_search=false)

**原文摘要:** Crop disease detection and classification is a critical challenge in
agriculture, with major implications for productivity, food security, and
environmental sustainability. While deep learning models such as CNN and ViT
have shown excellent performance in classifying plant diseases from images,
their large-scale deployment is often limited by data privacy concerns.
Federated Learning (FL) addresses this issue, but centralized FL remains
vulnerable to single-point failures and scalability limits. In this paper, we
introduce a novel Decentralized Federated Learning (DFL) framework that uses
validation loss (Loss_val) both to guide model sharing between peers and to
correct local training via an adaptive loss function controlled by weighting
parameter. We conduct extensive experiments using PlantVillage datasets with
three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the
impact of weighting parameter, the number of shared models, the number of
clients, and the use of Loss_val versus Loss_train of other clients. Results
demonstrate that our DFL approach not only improves accuracy and convergence
speed, but also ensures better generalization and robustness across
heterogeneous data environments making it particularly well-suited for
privacy-preserving agricultural applications.

</details>


### [64] [CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2505.23116)
*Pengfei Zhou, Yunlong Liu, Junli Liang, Qi Song, Xiangyang Li*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的线性预测模型CrossLinear，通过引入交叉相关嵌入模块，能够有效捕获变量间的依赖关系，并在短期和长期预测任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列预测中，区分内生变量和外生变量的依赖关系是一个重要挑战，传统模型往往难以高效处理这一问题，容易导致效率低下或过拟合。

**方法:** CrossLinear模型通过引入一个轻量级的交叉相关嵌入模块来捕获变量间的依赖关系，同时采用分块处理和全局线性头来捕获短期和长期的时间依赖关系。该模块专注于捕获时间不变和直接变量依赖，忽略时间变化或间接依赖，从而降低过拟合风险。

**结果:** 在12个真实世界数据集上的广泛实验表明，CrossLinear在短期和长期预测任务中均表现出优越性能。消融研究验证了交叉相关嵌入模块的有效性，且该模块具有良好的通用性，可作为不同领域预测任务的插件。

**结论:** CrossLinear模型提供了一种有效的方法来应对时间序列预测中外生变量建模的挑战，其交叉相关嵌入模块显著提升了预测精度并具有广泛的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CrossLinear%3A+Plug-and-Play+Cross-Correlation+Embedding+for+Time+Series+Forecasting+with+Exogenous+Variables，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23116，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23116&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting with exogenous variables is a critical emerging
paradigm that presents unique challenges in modeling dependencies between
variables. Traditional models often struggle to differentiate between
endogenous and exogenous variables, leading to inefficiencies and overfitting.
In this paper, we introduce CrossLinear, a novel Linear-based forecasting model
that addresses these challenges by incorporating a plug-and-play
cross-correlation embedding module. This lightweight module captures the
dependencies between variables with minimal computational cost and seamlessly
integrates into existing neural networks. Specifically, it captures
time-invariant and direct variable dependencies while disregarding time-varying
or indirect dependencies, thereby mitigating the risk of overfitting in
dependency modeling and contributing to consistent performance improvements.
Furthermore, CrossLinear employs patch-wise processing and a global linear head
to effectively capture both short-term and long-term temporal dependencies,
further improving its forecasting precision. Extensive experiments on 12
real-world datasets demonstrate that CrossLinear achieves superior performance
in both short-term and long-term forecasting tasks. The ablation study
underscores the effectiveness of the cross-correlation embedding module.
Additionally, the generalizability of this module makes it a valuable plug-in
for various forecasting tasks across different domains. Codes are available at
https://github.com/mumiao2000/CrossLinear.

</details>


### [65] [Multi-Modal Learning with Bayesian-Oriented Gradient Calibration](https://arxiv.org/abs/2505.23071)
*Peizheng Guo, Jingyao Wang, Huijie Guo, Jiangmeng Li, Chuxiong Sun, Changwen Zheng, Wenwen Qiang*

**主要类别:** cs.LG

**概要:** 在多模态学习（MML）中，现有方法主要以固定权重聚合梯度并平等地对待所有维度，忽略了每种模ality的内在梯度不确定性。为了解决这个问题，本文提出了BOGC-MML，一种基于贝叶斯的梯度校准方法，用于明确建模梯度不确定性，并引导模型优化向最佳方向发展。通过将每个模态的梯度视为随机变量，推导其概率分布，并将其精度转换为标量证据，从而量化每个模态在每个梯度维度上的置信度。使用这些证据，本文明确量化了每个维度的不确定性，并通过简化的Dempster-Shafer规则进行融合。最终的不确定性加权聚合产生了一个校准的更新方向，在不同维度之间平衡了敏感性和保守性。广泛的实验表明了该方法的有效性和优势。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态学习方法主要以固定权重聚合梯度，并且平等地对待所有维度，忽略了每种模态的内在梯度不确定性。这可能导致敏感维度的过度更新和不敏感维度的更新不足，从而影响性能和学习效果。

**方法:** 提出了一种名为BOGC-MML的方法，首先将每个模态的梯度建模为随机变量并推导其概率分布，捕捉梯度空间中的全部不确定性。然后提出了一种有效的方法，将每个梯度分布的精度转换为标量证据，用以量化每个模态在每个梯度维度上的置信度。最后，使用这些证据明确量化每个维度的不确定性，并通过简化的Dempster-Shafer规则进行融合，得到校准的更新方向。

**结果:** 通过广泛的实验验证，BOGC-MML方法在多个基准数据集上表现出有效性，并展示了其相对于其他方法的优势。

**结论:** BOGC-MML是一种有效的多模态学习方法，能够通过明确建模梯度不确定性来引导模型优化。它通过校准的更新方向平衡了不同维度的敏感性和保守性，显著提升了多模态学习的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+Learning+with+Bayesian-Oriented+Gradient+Calibration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23071，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23071&send_immediately=true&force_search=false)

**原文摘要:** Multi-Modal Learning (MML) integrates information from diverse modalities to
improve predictive accuracy. However, existing methods mainly aggregate
gradients with fixed weights and treat all dimensions equally, overlooking the
intrinsic gradient uncertainty of each modality. This may lead to (i) excessive
updates in sensitive dimensions, degrading performance, and (ii) insufficient
updates in less sensitive dimensions, hindering learning. To address this
issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for
MML to explicitly model the gradient uncertainty and guide the model
optimization towards the optimal direction. Specifically, we first model each
modality's gradient as a random variable and derive its probability
distribution, capturing the full uncertainty in the gradient space. Then, we
propose an effective method that converts the precision (inverse variance) of
each gradient distribution into a scalar evidence. This evidence quantifies the
confidence of each modality in every gradient dimension. Using these evidences,
we explicitly quantify per-dimension uncertainties and fuse them via a reduced
Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a
calibrated update direction that balances sensitivity and conservatism across
dimensions. Extensive experiments on multiple benchmark datasets demonstrate
the effectiveness and advantages of the proposed method.

</details>


### [66] [Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking](https://arxiv.org/abs/2505.23117)
*Yuatyong Chaichana, Thanapat Trachu, Peerat Limkonchotiwat, Konpat Preechakul, Tirasan Khandhawit, Ekapol Chuangsuwanich*

**主要类别:** cs.LG

**概要:** 在大规模训练时代，模型融合已经成为创建多任务模型的有效工具。现有的融合方法通常假设权重矩阵中相同位置的条目具有相同功能，但忽略了微调神经网络的复杂性。本文提出了一种名为Decom-Renorm-Merge (DRM) 的简单而有效的方法，该方法利用奇异值分解将权重矩阵分解并协调到一个对齐的联合空间中，从而使逐项融合成为可能。实验结果表明，DRM 在多种设置下优于几种最先进的融合技术。此外，分析表明重新归一化是创建稳健且均匀的联合空间以进行融合的关键组件，显著提升了方法性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有模型融合方法假设权重矩阵中相同位置的条目具有相同功能，但忽略了微调神经网络中神经元可能发展出不同的特征组成，导致直接逐项融合存在问题。因此需要一种新的方法来解决这一问题。

**方法:** 提出了一种名为 Decom-Renorm-Merge (DRM) 的方法，该方法使用奇异值分解 (SVD) 将权重矩阵分解并协调到一个对齐的联合空间中，从而使逐项融合成为可能。关键步骤包括分解、重新归一化和融合。

**结果:** DRM 方法在全微调和低秩适应设置下优于几种最先进的融合技术。重新归一化被证明是创建稳健且均匀的联合空间以进行融合的关键组件，显著提升了 DRM 的性能。

**结论:** Decom-Renorm-Merge (DRM) 是一种简单而有效的方法，通过奇异值分解将权重矩阵分解并协调到一个对齐的联合空间中，解决了现有方法中直接逐项融合的问题。重新归一化是该方法成功的关键因素之一。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decom-Renorm-Merge%3A+Model+Merging+on+the+Right+Space+Improves+Multitasking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23117&send_immediately=true&force_search=false)

**原文摘要:** In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.

</details>


### [67] [Gradient Boosting Decision Tree with LSTM for Investment Prediction](https://arxiv.org/abs/2505.23084)
*Chang Yu, Fang Liu, Jie Zhu, Shaobo Guo, Yifan Gao, Zhongheng Yang, Meiwei Liu, Qianwen Xing*

**主要类别:** cs.LG

**概要:** This paper proposes a hybrid framework that combines LSTM networks with LightGBM and CatBoost for stock price prediction.


<details>
  <summary>更多</summary>
  
**动机:** To improve the accuracy of stock price prediction by leveraging the strengths of both sequential models (LSTM) and tree-based models (LightGBM, CatBoost).

**方法:** The framework processes time-series financial data using seven models: ANNs, CNNs, BiLSTM, vanilla LSTM, XGBoost, LightGBM, and standard NNs. Performance is evaluated using key metrics like MAE, R-squared, MSE, and RMSE across different time scales. An ensemble model is then developed to combine the strengths of these models.

**结果:** The proposed framework improves accuracy by 10 to 15 percent compared to individual models and reduces error during market changes.

**结论:** This study highlights the potential of ensemble methods for financial forecasting and provides a flexible design for integrating new machine learning techniques.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gradient+Boosting+Decision+Tree+with+LSTM+for+Investment+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23084&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes a hybrid framework combining LSTM (Long Short-Term
Memory) networks with LightGBM and CatBoost for stock price prediction. The
framework processes time-series financial data and evaluates performance using
seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks
(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and
standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and
RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the
strengths of sequential and tree-based approaches. Experimental results show
that the proposed framework improves accuracy by 10 to 15 percent compared to
individual models and reduces error during market changes. This study
highlights the potential of ensemble methods for financial forecasting and
provides a flexible design for integrating new machine learning techniques.

</details>


### [68] [VERINA: Benchmarking Verifiable Code Generation](https://arxiv.org/abs/2505.23135)
*Zhe Ye, Zhengxu Yan, Jingxuan He, Timothe Kasriel, Kaiyu Yang, Dawn Song*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）在软件开发中的应用日益广泛，但确保生成代码的正确性仍具挑战。本文提出了可验证代码生成作为解决方案，并引入了Verina基准，以全面评估代码、规范和证明生成的能力。通过对现有LLM的评估，揭示了其在可验证代码生成方面（尤其是证明生成）的重大挑战，强调了改进基于LLM的定理证明器的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在软件开发中被越来越多地采用，但保证生成代码的正确性仍然困难重重，通常需要昂贵的手动审查。因此，研究者寻求一种方法来解决这一限制，并进一步释放LLM在编码方面的潜力。

**方法:** 研究者引入了名为Verina的高质量基准，包含189个手动策划的Lean编程任务，每个任务都配有详细的问题描述、参考实现、正式规范和广泛的测试套件。通过这个基准，可以对代码、规范和证明生成及其组合进行全面而模块化的评估。

**结果:** 对最先进的LLMs进行的广泛评估显示，在可验证代码生成方面存在重大挑战，尤其是在证明生成上。例如，表现最佳的模型OpenAI o4-mini在单次尝试下仅生成61.4%正确的代码、51.0%健全且完整的规范和3.6%成功的证明。

**结论:** Verina基准有望推动可验证代码生成领域的进步，提供严格和全面的评估工具。数据集和评估代码已公开发布，供社区使用和改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VERINA%3A+Benchmarking+Verifiable+Code+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23135，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23135&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly integrated in software
development, but ensuring correctness in LLM-generated code remains challenging
and often requires costly manual review. Verifiable code generation -- jointly
generating code, specifications, and proofs of code-specification alignment --
offers a promising path to address this limitation and further unleash LLMs'
benefits in coding. Yet, there exists a significant gap in evaluation: current
benchmarks often lack support for end-to-end verifiable code generation. In
this paper, we introduce Verina (Verifiable Code Generation Arena), a
high-quality benchmark enabling a comprehensive and modular evaluation of code,
specification, and proof generation as well as their compositions. Verina
consists of 189 manually curated coding tasks in Lean, with detailed problem
descriptions, reference implementations, formal specifications, and extensive
test suites. Our extensive evaluation of state-of-the-art LLMs reveals
significant challenges in verifiable code generation, especially in proof
generation, underscoring the need for improving LLM-based theorem provers in
verification domains. The best model, OpenAI o4-mini, generates only 61.4%
correct code, 51.0% sound and complete specifications, and 3.6% successful
proofs, with one trial per task. We hope Verina will catalyze progress in
verifiable code generation by providing a rigorous and comprehensive benchmark.
We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina
and our evaluation code on https://github.com/sunblaze-ucb/verina.

</details>


### [69] [Best Arm Identification with Possibly Biased Offline Data](https://arxiv.org/abs/2505.23165)
*Le Yang, Vincent Y. F. Tan, Wang Chi Cheung*

**主要类别:** cs.LG

**概要:** 在固定置信度设置下，研究了可能有偏差的离线数据的最佳臂识别（BAI）问题，并提出了LUCB-H算法以适应性地结合在线和离线数据。该算法在离线数据误导时与标准LUCB表现相当，在离线数据有用时显著优于标准LUCB。理论分析和数值实验表明，LUCB-H具有鲁棒性和适应性。


<details>
  <summary>更多</summary>
  
**动机:** 最佳臂识别问题在实际场景中（如临床试验）常见，但当离线数据可能存在偏差时，传统的自适应算法若无偏差界限的先验知识则无法有效工作。这促使研究者探索一种能平衡在线和离线数据的新方法。

**方法:** 提出了一种名为LUCB-H的算法，通过引入辅助偏差校正来适应性地调整置信区间，从而在LUCB框架内平衡在线和离线数据。

**结果:** 理论分析表明，LUCB-H在离线数据误导时样本复杂度与标准LUCB一致，而在离线数据有用时显著优于标准LUCB。此外，还推导出一个实例相关的下界，在某些情况下与LUCB-H的上界匹配。数值实验进一步验证了LUCB-H的有效性、鲁棒性和适应性。

**结论:** LUCB-H算法为解决带有潜在偏差的离线数据的最佳臂识别问题提供了一个有效的解决方案，能够在不同数据质量的情况下自适应地表现良好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Best+Arm+Identification+with+Possibly+Biased+Offline+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23165&send_immediately=true&force_search=false)

**原文摘要:** We study the best arm identification (BAI) problem with potentially biased
offline data in the fixed confidence setting, which commonly arises in
real-world scenarios such as clinical trials. We prove an impossibility result
for adaptive algorithms without prior knowledge of the bias bound between
online and offline distributions. To address this, we propose the LUCB-H
algorithm, which introduces adaptive confidence bounds by incorporating an
auxiliary bias correction to balance offline and online data within the LUCB
framework. Theoretical analysis shows that LUCB-H matches the sample complexity
of standard LUCB when offline data is misleading and significantly outperforms
it when offline data is helpful. We also derive an instance-dependent lower
bound that matches the upper bound of LUCB-H in certain scenarios. Numerical
experiments further demonstrate the robustness and adaptability of LUCB-H in
effectively incorporating offline data.

</details>


### [70] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
*Chongjie Si, Zhiyi Shi, Yadao Wang, Xiaokang Yang, Susanto Rahardja, Wei Shen*

**主要类别:** cs.LG

**概要:** 提出了一种名为MAP的新框架，将权重矩阵重新表述为高维向量，并将其适应解耦为方向和大小。通过与现有方法结合，MAP显著提高了性能，有望成为未来PEFT方法的默认设置。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型的微调计算成本高昂，现有的参数高效微调（PEFT）方法虽然有所改善，但分解权重调整的方向和大小的方式缺乏几何基础。

**方法:** MAP框架将权重矩阵重新表述为高维向量，解耦其适应为方向和大小，通过标准化预训练权重、学习方向更新并引入两个标量系数独立缩放基础和更新向量的大小。

**结果:** 广泛的实验表明，MAP与现有方法结合时显著提高了性能，提供了一个简单而强大的增强。

**结论:** MAP因其通用性和简单性，可作为未来设计PEFT方法的默认设置。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAP%3A+Revisiting+Weight+Decomposition+for+Low-Rank+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23094，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23094&send_immediately=true&force_search=false)

**原文摘要:** The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [71] [FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification](https://arxiv.org/abs/2505.23181)
*Tian Tian, Chunyan Miao, Hangwei Qian*

**主要类别:** cs.LG

**概要:** 提出了一种新的时间序列对比学习增强方法Frequency Refined Augmentation (FreRA)，通过频域优化增强了语义信息保护和数据多样性，显著提升了时间序列分类、异常检测和迁移学习任务的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列增强方法主要从视觉领域借鉴而来，未针对时间序列数据特点设计，可能导致语义信息失真。因此，需要一种更适合时间序列分类任务的增强策略。

**方法:** 从频域角度出发，提出Frequency Refined Augmentation (FreRA)方法，自动分离关键和非关键频率成分，并分别采用语义感知的身份修改和语义无关的自适应修改策略，以保护语义相关信息并引入数据变异性。

**结果:** 理论上证明了FreRA能够生成语义保持的视图；实验结果表明，在多个基准数据集（如UCR、UEA）和大规模数据集上，FreRA在时间序列分类、异常检测和迁移学习任务中均优于十个领先基线方法。

**结论:** FreRA是一种轻量且有效的时间序列增强方法，适用于对比学习框架，能够提升下游任务的表示学习能力和泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FreRA%3A+A+Frequency-Refined+Augmentation+for+Contrastive+Learning+on+Time+Series+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23181&send_immediately=true&force_search=false)

**原文摘要:** Contrastive learning has emerged as a competent approach for unsupervised
representation learning. However, the design of an optimal augmentation
strategy, although crucial for contrastive learning, is less explored for time
series classification tasks. Existing predefined time-domain augmentation
methods are primarily adopted from vision and are not specific to time series
data. Consequently, this cross-modality incompatibility may distort the
semantically relevant information of time series by introducing mismatched
patterns into the data. To address this limitation, we present a novel
perspective from the frequency domain and identify three advantages for
downstream classification: global, independent, and compact. To fully utilize
the three properties, we propose the lightweight yet effective Frequency
Refined Augmentation (FreRA) tailored for time series contrastive learning on
classification tasks, which can be seamlessly integrated with contrastive
learning frameworks in a plug-and-play manner. Specifically, FreRA
automatically separates critical and unimportant frequency components.
Accordingly, we propose semantic-aware Identity Modification and
semantic-agnostic Self-adaptive Modification to protect semantically relevant
information in the critical frequency components and infuse variance into the
unimportant ones respectively. Theoretically, we prove that FreRA generates
semantic-preserving views. Empirically, we conduct extensive experiments on two
benchmark datasets, including UCR and UEA archives, as well as five large-scale
datasets on diverse applications. FreRA consistently outperforms ten leading
baselines on time series classification, anomaly detection, and transfer
learning tasks, demonstrating superior capabilities in contrastive
representation learning and generalization in transfer learning scenarios
across diverse datasets.

</details>


### [72] [Learning to Search for Vehicle Routing with Multiple Time Windows](https://arxiv.org/abs/2505.23098)
*Kuan Xu, Zhiguang Cao, Chenlong Zheng, Linong Liu*

**主要类别:** cs.LG

**概要:** 提出了一种基于强化学习的自适应变邻域搜索（RL-AVNS）方法，用于解决带多时间窗的车辆路径问题（VRPMTW）。通过引入量化客户时间灵活性的适应性度量和基于Transformer的神经策略网络，该方法在解质量和计算效率上显著优于传统方法，并能有效推广到未见问题实例。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自适应方法仅依赖于历史操作性能，在解决复杂物流问题时可能无法充分优化实时解状态和经验学习。因此，需要一种能够动态选择邻域算子的方法来提高解的质量和效率。

**方法:** 提出了基于强化学习的自适应变邻域搜索（RL-AVNS）方法，包含以下特点：1) 引入衡量客户时间灵活性的适应性度量以改进震荡阶段；2) 使用基于Transformer的神经策略网络智能引导局部搜索中的算子选择；3) 在实际补货场景中进行大量计算实验验证方法的有效性。

**结果:** 与传统变邻域搜索（VNS）、自适应VNS（AVNS）及最先进的学习启发式方法相比，RL-AVNS在不同实例规模和时间窗复杂度下均显著提高了解质量和计算效率。此外，该算法对训练过程中未遇到的问题实例具有良好的推广能力。

**结论:** RL-AVNS方法在解决带多时间窗的车辆路径问题方面表现出色，不仅提高了解的质量和计算效率，还展示了其在复杂物流场景中的实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Search+for+Vehicle+Routing+with+Multiple+Time+Windows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23098，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23098&send_immediately=true&force_search=false)

**原文摘要:** In this study, we propose a reinforcement learning-based adaptive variable
neighborhood search (RL-AVNS) method designed for effectively solving the
Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional
adaptive approaches that rely solely on historical operator performance, our
method integrates a reinforcement learning framework to dynamically select
neighborhood operators based on real-time solution states and learned
experience. We introduce a fitness metric that quantifies customers' temporal
flexibility to improve the shaking phase, and employ a transformer-based neural
policy network to intelligently guide operator selection during the local
search. Extensive computational experiments are conducted on realistic
scenarios derived from the replenishment of unmanned vending machines,
characterized by multiple clustered replenishment windows. Results demonstrate
that RL-AVNS significantly outperforms traditional variable neighborhood search
(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,
achieving substantial improvements in solution quality and computational
efficiency across various instance scales and time window complexities.
Particularly notable is the algorithm's capability to generalize effectively to
problem instances not encountered during training, underscoring its practical
utility for complex logistics scenarios.

</details>


### [73] [Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning](https://arxiv.org/abs/2505.23195)
*Lifan Zhao, Yanyan Shen, Zhaoyang Liu, Xue Wang, Jiaji Deng*

**主要类别:** cs.LG

**概要:** 通过结构化剪枝方法优化TSFM的微调过程，可以显著提升其预测性能，使其达到甚至超越专业模型的水平。


<details>
  <summary>更多</summary>
  
**动机:** 尽管时间序列基础模型（TSFMs）在零样本预测任务中表现出色，但在微调后却无法持续优于专门的小规模模型。因此，如何实现TSFM在目标预测任务中的有效适配成为关键问题。

**方法:** 提出了一种结构化剪枝方法，用于规整TSFM的微调过程，聚焦于更相关和紧凑的参数空间，从而保留预训练模型中任务相关的网络子结构。

**结果:** 在七个TSFM和六个基准数据集上的广泛实验表明，微调剪枝后的TSFM显著提高了预测性能，且通常能达到最先进的水平。

**结论:** “先剪枝再微调”的范式能够使TSFM在预测任务中超越强大的专业化基线模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Less+is+More%3A+Unlocking+Specialization+of+Time+Series+Foundation+Models+via+Structured+Pruning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23195，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23195&send_immediately=true&force_search=false)

**原文摘要:** Scaling laws motivate the development of Time Series Foundation Models
(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot
forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot
consistently outperform smaller, specialized models trained on full-shot
downstream data. A key question is how to realize effective adaptation of TSFMs
for a target forecasting task. Through empirical studies on various TSFMs, the
pre-trained models often exhibit inherent sparsity and redundancy in
computation, suggesting that TSFMs have learned to activate task-relevant
network substructures to accommodate diverse forecasting tasks. To preserve
this valuable prior knowledge, we propose a structured pruning method to
regularize the subsequent fine-tuning process by focusing it on a more relevant
and compact parameter space. Extensive experiments on seven TSFMs and six
benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly
improves forecasting performance compared to fine-tuning original models. This
"prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art
performance and surpass strong specialized baselines.

</details>


### [74] [Weight Spectra Induced Efficient Model Adaptation](https://arxiv.org/abs/2505.23099)
*Chongjie Si, Xuankun Yang, Muqing Liu, Yadao Wang, Xiaokang Yang, Wenbo Su, Bo Zheng, Wei Shen*

**主要类别:** cs.LG

**概要:** 大型基础模型在下游任务中表现出色，但全量微调成本高昂。本文研究了参数高效微调(PEFT)方法对权重矩阵的结构性变化，并提出了一种通过学习重新缩放主奇异方向的新方法，实现了对关键组件的精确调节而不破坏整体结构，从而在多个任务上超越强基线。


<details>
  <summary>更多</summary>
  
**动机:** 全量微调大型基础模型计算成本过高，因此需要探索更高效的参数微调方法（如LoRA），并深入理解这些方法如何改变模型参数。

**方法:** 通过奇异值分解(SVD)分析微调过程中权重矩阵的变化，发现微调主要放大了主奇异值并将其重新定向到任务特定方向，而非主子空间保持稳定。基于此，提出了一种利用可学习的主奇异方向重新缩放的方法，以精确调节影响最大的组件。

**结果:** 该方法在多个任务上一致地优于强大的基线方法，证明了结构化信息微调的有效性。

**结论:** 通过对权重矩阵结构变化的系统研究，揭示了任务特定知识注入低维子空间的机制，并提出了一种新的微调方法，能够精确调整关键组件而不破坏全局结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weight+Spectra+Induced+Efficient+Model+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23099，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23099&send_immediately=true&force_search=false)

**原文摘要:** Large-scale foundation models have demonstrated remarkable versatility across
a wide range of downstream tasks. However, fully fine-tuning these models
incurs prohibitive computational costs, motivating the development of
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces
low-rank updates to pre-trained weights. Despite their empirical success, the
underlying mechanisms by which PEFT modifies model parameters remain
underexplored. In this work, we present a systematic investigation into the
structural changes of weight matrices during fully fine-tuning. Through
singular value decomposition (SVD), we reveal that fine-tuning predominantly
amplifies the top singular values while leaving the remainder largely intact,
suggesting that task-specific knowledge is injected into a low-dimensional
subspace. Furthermore, we find that the dominant singular vectors are
reoriented in task-specific directions, whereas the non-dominant subspace
remains stable. Building on these insights, we propose a novel method that
leverages learnable rescaling of top singular directions, enabling precise
modulation of the most influential components without disrupting the global
structure. Our approach achieves consistent improvements over strong baselines
across multiple tasks, highlighting the efficacy of structurally informed
fine-tuning.

</details>


### [75] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang, Zhexuan Gu, Houduo Qi, Yancheng Yuan*

**主要类别:** cs.LG

**概要:** 提出了一种奖励调整模型以加速基于GRPO的RLHF训练，通过增加奖励方差并保持相对偏好和期望。该方法解决了非凸优化问题，并改进了GRPO算法为GRPOVI，实验表明其效率显著优于原GRPO算法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RLHF技术在LLM对齐人类价值观方面很重要，但高效的GRPO-based RLHF训练仍具挑战性。研究发现初始策略模型的高奖励方差可加速RLHF训练，因此需要一种方法来系统地增加奖励方差。

**方法:** 设计了一个奖励调整模型，通过明确表征可行集的极值点，用O(n log n)算法解决非凸优化问题。将此模型整合到GRPO算法中，形成GRPOVI算法，用于更高效的RLHF训练。

**结果:** 实验结果表明，与原始GRPO相比，GRPOVI算法显著提高了RLHF训练效率。此外，还间接解释了带有规则奖励的GRPO在DeepSeek-R1中的经验有效性。

**结论:** 提出的奖励调整模型成功加速了RLHF训练，并且改进后的GRPOVI算法比原GRPO更高效，为未来的RLHF研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+RLHF+Training+with+Reward+Variance+Increase，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23247，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23247&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [76] [LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics](https://arxiv.org/abs/2505.23105)
*Abhishek Vijaya Kumar, Eric Ding, Arjun Devraj, Darius Bunandar, Rachee Singh*

**主要类别:** cs.LG

**概要:** 当现代ML数据中心的加速器发生故障时，操作员通常将受影响的训练或推理任务迁移到全新的机架上。尽管这种方法能保持网络性能，但效率极低，需要为容错保留一整套空闲加速器。本文提出了一种名为LUMION的新型可重构光互连结构，用于连接数据中心机架内的加速器。LUMION能够在故障发生时动态地将备用加速器整合到正在进行的工作负载中，从而无需昂贵的任务迁移即可维持一致的性能。实验表明，LUMION在约1秒内替换故障GPU并重启ML任务，并且通过用备用加速器替换故障加速器，实现了比传统电学机架更高的GPU间带宽，使得微调吞吐量几乎提高了2倍。


<details>
  <summary>更多</summary>
  
**动机:** 现代ML数据中心中，当加速器发生故障时，操作员会将整个任务迁移到新的机架上。这种做法虽然保证了网络性能，但资源利用率低下，因为需要为容错预留大量闲置加速器。因此，作者希望解决这一资源浪费问题。

**方法:** 作者提出了LUMION，一种新型可重构光互连结构，用于数据中心机架内的加速器连接。LUMION通过动态整合备用加速器到当前工作负载中来应对加速器故障，避免了整个任务的迁移。此外，作者构建了一个端到端的硬件原型，并通过实验展示了LUMION的效果。

**结果:** 实验结果表明，LUMION可以在约1秒内检测并替换故障GPU，同时重新启动ML任务。与传统电学机架相比，LUMION在用备用加速器替换故障加速器后，实现了近2倍的微调吞吐量提升。

**结论:** LUMION是一种有效的解决方案，可以显著提高数据中心加速器资源的利用效率，减少任务迁移的需求，并提升整体性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LUMION%3A+Fast+Fault+Recovery+for+ML+Jobs+Using+Programmable+Optical+Fabrics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23105，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23105&send_immediately=true&force_search=false)

**原文摘要:** When accelerators fail in modern ML datacenters, operators migrate the
affected ML training or inference jobs to entirely new racks. This approach,
while preserving network performance, is highly inefficient, requiring
datacenters to reserve full racks of idle accelerators for fault tolerance. In
this paper, we address this resource inefficiency by introducing LUMION, a
novel reconfigurable optical fabric for connecting accelerators within a
datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically
integrates spare accelerators into ongoing workloads as failures occur, thereby
maintaining consistent performance without costly migrations. We show the
benefits of LUMION by building an end-to-end hardware prototype. Our
experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a
healthy one and restarts the ML job within ~ 1 second of the failure. LUMION
achieves higher inter-GPU bandwidth compared to traditional electrical racks
after replacing failed accelerators with spare ones, leading to nearly 2X
improvement in fine-tuning throughput.

</details>


### [77] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
*Haokun Chen, Yueqi Zhang, Yuan Bi, Yao Zhang, Tong Liu, Jinhe Bi, Jian Lan, Jindong Gu, Claudia Grosser, Denis Krompass, Nassir Navab, Volker Tresp*

**主要类别:** cs.LG

**概要:** 近年来，大型语言模型（LLMs）取得了显著进展，但训练数据中可能存在敏感或受版权保护的内容，引发数据隐私和所有权问题。为此，机器遗忘算法被开发以移除特定知识而无需重新训练模型。然而，评估这些算法的有效性仍具挑战。本文提出一个全面的审计框架，包括三个基准数据集、六种遗忘算法和五种基于提示的审计方法，并提出一种利用中间激活扰动的新技术，以克服仅依赖模型输入和输出的审计方法的局限性。


<details>
  <summary>更多</summary>
  
**动机:** LLMs的能力得益于大规模架构和海量数据训练，但这些数据可能包含敏感或受版权保护的内容，违反数据隐私法规（如GDPR）。因此，需要开发有效的机器遗忘算法并评估其性能。

**方法:** 1. 构建了一个综合审计框架，包含：
   - 三个基准数据集
   - 六种遗忘算法
   - 五种基于提示的审计方法
2. 提出了一种新审计技术，通过中间激活扰动来评估遗忘效果，弥补传统审计方法的不足。

**结果:** - 使用多种审计算法验证了不同遗忘策略的有效性和鲁棒性。
- 新提出的基于中间激活扰动的技术为评估遗忘效果提供了更全面的方法。

**结论:** 本文提出了一个全面的审计框架用于评估机器遗忘算法，并引入了一种新技术以改进审计方法，有助于推动该领域的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Machine+Unlearning+Truly+Remove+Model+Knowledge%3F+A+Framework+for+Auditing+Unlearning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23270，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23270&send_immediately=true&force_search=false)

**原文摘要:** In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [78] [Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery](https://arxiv.org/abs/2505.23106)
*Ning Liu, Yue Yu*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种新的神经算子架构NIPS，它通过线性注意力机制和可学习的核网络，在预测精度和计算效率上改进了非局部注意算子(NAO)，用于物理系统的建模。实验表明，NIPS在多个基准测试中显著优于NAO和其他基线方法，推动了可扩展、可解释和高效的物理学习的发展。


<details>
  <summary>更多</summary>
  
**动机:** 尽管注意力机制在AI核心领域（如自然语言处理和计算机视觉）中已经取得了很大进展，但它们在建模复杂物理系统方面的潜力尚未得到充分开发。当前学习物理系统通常涉及发现映射功能性空间的算子，这是一个严重不适定的逆PDE问题，亟需一种新的方法来解决这一挑战。

**方法:** 本研究提出了Neural Interpretable PDEs (NIPS) 架构，它基于并增强了Nonlocal Attention Operators (NAO)。NIPS使用线性注意力机制以实现可扩展的学习，并整合了一个可学习的核网络，该网络在傅里叶空间中作为通道无关的卷积操作。这种方法避免了显式计算和存储大规模的两两相互作用，将处理空间相互作用的成本分摊到傅里叶变换中。

**结果:** 经验评估表明，NIPS在各种基准测试中始终优于NAO和其他基线方法，展示了其在可扩展性、可解释性和高效物理学习方面的显著进步。

**结论:** NIPS架构为物理系统的建模提供了一种新方法，通过提高预测准确性和计算效率，实现了更可扩展、更可解释和更高效的物理学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Interpretable+PDEs%3A+Harmonizing+Fourier+Insights+with+Attention+for+Scalable+and+Interpretable+Physics+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23106，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23106&send_immediately=true&force_search=false)

**原文摘要:** Attention mechanisms have emerged as transformative tools in core AI domains
such as natural language processing and computer vision. Yet, their largely
untapped potential for modeling intricate physical systems presents a
compelling frontier. Learning such systems often entails discovering operators
that map between functional spaces using limited instances of function pairs --
a task commonly framed as a severely ill-posed inverse PDE problem. In this
work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator
architecture that builds upon and enhances Nonlocal Attention Operators (NAO)
in both predictive accuracy and computational efficiency. NIPS employs a linear
attention mechanism to enable scalable learning and integrates a learnable
kernel network that acts as a channel-independent convolution in Fourier space.
As a consequence, NIPS eliminates the need to explicitly compute and store
large pairwise interactions, effectively amortizing the cost of handling
spatial interactions into the Fourier transform. Empirical evaluations
demonstrate that NIPS consistently surpasses NAO and other baselines across
diverse benchmarks, heralding a substantial leap in scalable, interpretable,
and efficient physics learning. Our code and data accompanying this paper are
available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

</details>


### [79] [Score-based Generative Modeling for Conditional Independence Testing](https://arxiv.org/abs/2505.23309)
*Yixin Ren, Chenghou Jin, Yewei Xia, Li Ke, Longtao Huang, Hui Xue, Hao Zhang, Jihong Guan, Shuigeng Zhou*

**主要类别:** cs.LG

**概要:** 提出了一种基于评分的生成建模新方法，用于条件独立性（CI）测试，解决了现有方法在高维设置中对条件分布的不良建模和训练不稳定性问题。该方法通过精确的Type I误差控制和强大的测试能力，在合成和真实数据集上显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 确定随机变量之间的条件独立性关系是机器学习和统计学中的基本但具有挑战性的任务，尤其是在高维情况下。现有的基于生成模型的CI测试方法（如利用GAN的方法）通常在条件分布建模和训练稳定性方面存在问题，导致性能不佳。

**方法:** 提出了一种新的CI测试方法，使用评分生成建模技术。具体来说，首先采用切片条件得分匹配方案准确估计条件得分，并使用Langevin动力学条件采样生成零假设样本，确保精确的Type I误差控制。然后，将拟合优度阶段纳入方法以验证生成的样本并增强实际应用中的可解释性。

**结果:** 理论上建立了由评分生成模型建模的条件分布的误差界，并证明了CI测试的有效性。广泛的实验表明，该方法在合成和真实世界数据集上显著优于现有最先进的方法。

**结论:** 所提出的方法为基于生成模型的CI测试提供了一个有前途的新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Score-based+Generative+Modeling+for+Conditional+Independence+Testing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23309&send_immediately=true&force_search=false)

**原文摘要:** Determining conditional independence (CI) relationships between random
variables is a fundamental yet challenging task in machine learning and
statistics, especially in high-dimensional settings. Existing generative
model-based CI testing methods, such as those utilizing generative adversarial
networks (GANs), often struggle with undesirable modeling of conditional
distributions and training instability, resulting in subpar performance. To
address these issues, we propose a novel CI testing method via score-based
generative modeling, which achieves precise Type I error control and strong
testing power. Concretely, we first employ a sliced conditional score matching
scheme to accurately estimate conditional score and use Langevin dynamics
conditional sampling to generate null hypothesis samples, ensuring precise Type
I error control. Then, we incorporate a goodness-of-fit stage into the method
to verify generated samples and enhance interpretability in practice. We
theoretically establish the error bound of conditional distributions modeled by
score-based generative models and prove the validity of our CI tests. Extensive
experiments on both synthetic and real-world datasets show that our method
significantly outperforms existing state-of-the-art methods, providing a
promising way to revitalize generative model-based CI testing.

</details>


### [80] [Matryoshka Model Learning for Improved Elastic Student Models](https://arxiv.org/abs/2505.23337)
*Chetan Verma, Aditya Srinivas Timmaraju, Cho Jui-Hsieh, Suyash Damle, Ngot Bui, Yang Zhang, Wen Chen, Xin Liu, Prateek Jain, Inderjit S Dhillon*

**主要类别:** cs.LG

**概要:** 提出MatTA框架，通过Teacher-TA-Student方法训练多个精确的学生模型，降低服务成本并提高精度。在专有数据集和GPT-2 Medium上验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 工业级ML模型需要满足快速变化的服务约束，这需要大量的资源进行模型开发。为了减少资源消耗并提供多种可部署选项，提出了MatTA框架。

**方法:** 使用Teacher-TA-Student配方训练多个准确的学生模型，其中TA模型是学生模型的更大版本，具有更高的容量，允许学生模型更好地与教师模型相关联，并引入更多领域特定的专业知识。

**结果:** 在专有数据集和GPT-2 Medium上的实验表明，关键指标提高了20%，SAT Math相对提高了24%以上，LAMBADA基准提高了10%以上。

**结论:** MatTA框架仅需一次训练即可提供多个可部署选项，在实际应用中显著提高了模型性能和服务效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Matryoshka+Model+Learning+for+Improved+Elastic+Student+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23337&send_immediately=true&force_search=false)

**原文摘要:** Industry-grade ML models are carefully designed to meet rapidly evolving
serving constraints, which requires significant resources for model
development. In this paper, we propose MatTA, a framework for training multiple
accurate Student models using a novel Teacher-TA-Student recipe. TA models are
larger versions of the Student models with higher capacity, and thus allow
Student models to better relate to the Teacher model and also bring in more
domain-specific expertise. Furthermore, multiple accurate Student models can be
extracted from the TA model. Therefore, despite only one training run, our
methodology provides multiple servable options to trade off accuracy for lower
serving cost. We demonstrate the proposed method, MatTA, on proprietary
datasets and models. Its practical efficacy is underscored by live A/B tests
within a production ML system, demonstrating 20% improvement on a key metric.
We also demonstrate our method on GPT-2 Medium, a public model, and achieve
relative improvements of over 24% on SAT Math and over 10% on the LAMBADA
benchmark.

</details>


### [81] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang, Yulan Hu, Ge Chen, Qingyang Li, Fuzheng Zhang, Yong Liu*

**主要类别:** cs.LG

**概要:** 在强化学习中，奖励代表了人类偏好，但若存在偏差则会影响大语言模型的对齐。本文定义了奖励中的各种偏差为奖励不公平问题，并提出一种无需针对特定偏差的资源分配视角方法来解决此问题。具体地，将偏好学习建模为资源分配问题，并提出了两种实现奖励公平的方法：公平性正则化和公平系数。实验表明，这些方法能更公平地使大语言模型与人类偏好对齐。


<details>
  <summary>更多</summary>
  
**动机:** 奖励在基于人类反馈的强化学习（RLHF）中扮演重要角色，但其中可能存在的偏差会对大语言模型的对齐产生不良影响。因此，需要解决奖励不公平的问题。

**方法:** 1. 将奖励视为资源，将偏好学习建模为资源分配问题。
2. 提出两种方法：公平性正则化和公平系数，从资源分配的角度实现奖励公平。
3. 在验证和强化学习场景中应用所提出的方法，分别获得公平奖励模型和策略模型。

**结果:** 实验结果表明，所提出的方法能够更公平地使大语言模型与人类偏好对齐。

**结论:** 本文提出了一种无需针对特定偏差的资源分配视角方法，有效缓解了奖励中的各种偏差，从而解决了奖励不公平问题。实验结果证明该方法在使大语言模型与人类偏好对齐方面具有更好的公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Reward+Fairness+in+RLHF%3A+From+a+Resource+Allocation+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23349，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23349&send_immediately=true&force_search=false)

**原文摘要:** Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [82] [DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs](https://arxiv.org/abs/2505.23131)
*Xinyu Yao, Daniel Bourgeois, Abhinav Jain, Yuxin Tang, Jiawen Yao, Zhimin Ding, Arlei Silva, Chris Jermaine*

**主要类别:** cs.LG

**概要:** 本研究提出了一种名为Doppler的三阶段框架，用于训练双策略网络，以减少数据流图在工作保守系统中的执行时间。该框架通过选择操作和将选定的操作放置在设备上来优化复杂机器学习工作负载的执行时间。实验表明，Doppler优于所有基线方法，并且提高了采样效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于学习的方法在分配数据流图操作到设备时面临三个主要限制：1) 依赖于如TensorFlow这样的块同步系统，导致设备利用率低下；2) 在设计基于学习的方法时缺乏对底层系统调度机制的认识；3) 过度依赖强化学习，忽略了专家设计的有效启发式结构。因此，需要一种新的方法来克服这些限制。

**方法:** 提出了一个三阶段框架Doppler，用于训练由两个策略组成的双策略网络：一个是用于选择操作的选择（SEL）策略，另一个是用于将选定的操作放置在设备上的放置（PLC）策略。

**结果:** 实验表明，Doppler在各种任务中优于所有基线方法，不仅减少了系统的执行时间，还通过减少每集训练时间展示了采样效率。

**结论:** Doppler提供了一个有效的解决方案，可以优化复杂机器学习工作负载的执行时间，并在采样效率方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DOPPLER%3A+Dual-Policy+Learning+for+Device+Assignment+in+Asynchronous+Dataflow+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23131，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23131&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of assigning operations in a dataflow graph to devices
to minimize execution time in a work-conserving system, with emphasis on
complex machine learning workloads. Prior learning-based methods often struggle
due to three key limitations: (1) reliance on bulk-synchronous systems like
TensorFlow, which under-utilize devices due to barrier synchronization; (2)
lack of awareness of the scheduling mechanism of underlying systems when
designing learning-based methods; and (3) exclusive dependence on reinforcement
learning, ignoring the structure of effective heuristics designed by experts.
In this paper, we propose \textsc{Doppler}, a three-stage framework for
training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for
selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen
operations on devices. Our experiments show that \textsc{Doppler} outperforms
all baseline methods across tasks by reducing system execution time and
additionally demonstrates sampling efficiency by reducing per-episode training
time.

</details>


### [83] [Dynamic Spectral Backpropagation for Efficient Neural Network Training](https://arxiv.org/abs/2505.23369)
*Mannmohan Muthuraman*

**主要类别:** cs.LG

**概要:** Dynamic Spectral Backpropagation (DSBP) 通过将梯度投影到主特征向量上，在资源受限的情况下提高神经网络训练的效果。五个扩展（动态光谱推理、光谱架构优化、光谱元学习、光谱转移正则化和李代数启发的动态）解决了稳健性、小样本学习和硬件效率等问题。DSBP 在多个数据集上的表现优于 SAM、LoRA 和 MAML，并由三阶随机微分方程和 PAC-Bayes 理论支持。未来工作将关注可扩展性、偏差缓解和伦理问题。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络在资源受限环境下的训练需要降低计算复杂度，同时保持模型性能和泛化能力。此外，还需要解决模型稳健性、小样本学习和硬件效率等挑战。

**方法:** 提出了一种名为 Dynamic Spectral Backpropagation (DSBP) 的方法，该方法通过将梯度投影到主特征向量上来减少计算复杂度并促进平坦极小值。此外，提出了五个扩展：动态光谱推理、光谱架构优化、光谱元学习、光谱转移正则化和李代数启发的动态。这些扩展分别用于解决稳健性、小样本学习和硬件效率等问题。

**结果:** DSBP 在 CIFAR 10、Fashion MNIST、MedMNIST 和 Tiny ImageNet 数据集上的实验结果表明，其性能优于 Sharpness Aware Minimization (SAM)、Low Rank Adaptation (LoRA) 和 Model Agnostic Meta Learning (MAML)。这些结果得到了广泛的实验和可视化支持。

**结论:** Dynamic Spectral Backpropagation (DSBP) 及其扩展提供了一种有效的方法来应对资源受限环境下神经网络训练的挑战。未来的研究将集中于提升方法的可扩展性、减轻潜在偏差以及考虑伦理影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Spectral+Backpropagation+for+Efficient+Neural+Network+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23369，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23369&send_immediately=true&force_search=false)

**原文摘要:** Dynamic Spectral Backpropagation (DSBP) enhances neural network training
under resource constraints by projecting gradients onto principal eigenvectors,
reducing complexity and promoting flat minima. Five extensions are proposed,
dynamic spectral inference, spectral architecture optimization, spectral meta
learning, spectral transfer regularization, and Lie algebra inspired dynamics,
to address challenges in robustness, fewshot learning, and hardware efficiency.
Supported by a third order stochastic differential equation (SDE) and a PAC
Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank
Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion
MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive
experiments and visualizations. Future work focuses on scalability, bias
mitigation, and ethical considerations.

</details>


### [84] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/abs/2505.23412)
*Srishti Gupta, Daniele Angioni, Maura Pintor, Ambra Demontis, Lea Schönherr, Battista Biggio, Fabio Roli*

**主要类别:** cs.LG

**概要:** 本论文探讨了无缓冲区的OOD检测方法在类别增量学习中的应用，发现其性能可与基于缓冲区的方法媲美甚至更优，同时提高了隐私保护和效率。


<details>
  <summary>更多</summary>
  
**动机:** 类别增量学习在开放世界场景中面临两大挑战：遗忘先前学习的类别以及处理未知类别的输入。当前解决方法依赖于联合训练和数据缓冲区，但存在隐私、可扩展性和训练时间的问题。因此，研究者希望找到一种无需缓冲区的有效替代方案。

**方法:** 论文深入分析了后验（post-hoc）的OOD检测方法，并研究了这些方法在推理阶段的应用如何消除对数据缓冲区的需求。通过实验验证，证明无缓冲区的OOD检测可以作为缓冲区方法的有力替代。

**结果:** 实验结果表明，在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上，无缓冲区方法在类别增量学习和拒绝未知样本方面表现出与缓冲区方法相当或更优的性能。

**结论:** 无缓冲区的OOD检测方法不仅能够有效应对类别增量学习中的挑战，还具备更高的隐私保护水平和更好的可扩展性，为设计高效的开放世界CIL系统提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Buffer-free+Class-Incremental+Learning+with+Out-of-Distribution+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23412&send_immediately=true&force_search=false)

**原文摘要:** Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [85] [Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners](https://arxiv.org/abs/2505.23150)
*Michal Nauman, Marek Cygan, Carmelo Sferrazza, Aviral Kumar, Pieter Abbeel*

**主要类别:** cs.LG

**概要:** 最近在语言建模和视觉方面的进展源于对多样化、多任务数据进行大规模模型训练。然而，基于价值的强化学习（RL）受到的影响有限，改进通常由单任务环境中训练的小型模型驱动。这是因为多任务RL中稀疏奖励和梯度冲突使得时间差分优化变得脆弱。因此，通用策略的实际工作流程避免了在线训练，而是克隆专家轨迹或将多个单任务策略蒸馏成一个代理。在这项工作中，我们展示了使用通过交叉熵训练的高容量价值模型，并结合可学习的任务嵌入来解决在线RL中的任务干扰问题，从而实现强大且可扩展的多任务训练。我们在7个多任务基准上测试了该方法，这些基准包含超过280个独特任务，涵盖了高自由度的人体控制和离散视觉RL。我们发现，尽管方法简单，但所提出的方法在单任务和多任务性能以及新任务的样本高效迁移方面达到了最先进的水平。


<details>
  <summary>更多</summary>
  
**动机:** 基于价值的强化学习（RL）在多任务场景下存在挑战，例如稀疏奖励和梯度冲突导致的时间差分优化脆弱性。这促使研究者寻找一种能够有效应对任务干扰并支持在线多任务训练的方法。

**方法:** 使用高容量的价值模型，通过交叉熵进行训练，并结合可学习的任务嵌入来解决在线RL中的任务干扰问题。这种方法允许强大的多任务训练，并适用于高自由度的人体控制和离散视觉RL等复杂任务。

**结果:** 该方法在7个多任务基准测试中表现优异，涵盖超过280个独特任务。不仅实现了单任务和多任务的最先进性能，还能够在新任务上实现样本高效的迁移学习。

**结论:** 通过使用高容量价值模型和可学习任务嵌入，解决了多任务在线RL中的任务干扰问题，实现了鲁棒且可扩展的多任务训练，同时在多种任务类型上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bigger%2C+Regularized%2C+Categorical%3A+High-Capacity+Value+Functions+are+Efficient+Multi-Task+Learners，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23150，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23150&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.

</details>


### [86] [Bidirectional predictive coding](https://arxiv.org/abs/2505.23415)
*Gaspard Oliviers, Mufeng Tang, Rafal Bogacz*

**主要类别:** cs.LG

**概要:** 提出了一种新的预测编码模型——双向预测编码（bPC），该模型结合了生成性和判别性推理，表现出优于单向模型的性能，并在多模态学习和缺失信息推理等生物相关任务中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的预测编码（PC）模型要么是生成性的（自上而下），要么是判别性的（前馈式）。然而，实验表明大脑同时使用这两种推理方式，而单向模型在需要双向处理的任务中表现不佳。因此，需要一种能结合生成性和判别性推理的新模型。

**方法:** 提出了双向预测编码（bPC）模型，该模型将生成性和判别性推理结合起来，并保持了生物学上可实现的电路实现。通过开发一个同时适用于生成和判别任务的能量景观，使bPC模型能够匹配或超越单向模型在其专门任务中的表现。

**结果:** bPC模型在专门的生成或判别任务中匹配或超越了单向模型的表现。此外，在多模态学习和带有缺失信息的推理等生物相关任务中展示了bPC的优越性能。

**结论:** bPC模型更接近于生物视觉推理，因为它结合了生成性和判别性推理，适用于多种任务，并且在生物相关任务中表现更好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bidirectional+predictive+coding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23415&send_immediately=true&force_search=false)

**原文摘要:** Predictive coding (PC) is an influential computational model of visual
learning and inference in the brain. Classical PC was proposed as a top-down
generative model, where the brain actively predicts upcoming visual inputs, and
inference minimises the prediction errors. Recent studies have also shown that
PC can be formulated as a discriminative model, where sensory inputs predict
neural activities in a feedforward manner. However, experimental evidence
suggests that the brain employs both generative and discriminative inference,
while unidirectional PC models show degraded performance in tasks requiring
bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC
model that incorporates both generative and discriminative inference while
maintaining a biologically plausible circuit implementation. We show that bPC
matches or outperforms unidirectional models in their specialised generative or
discriminative tasks, by developing an energy landscape that simultaneously
suits both tasks. We also demonstrate bPC's superior performance in two
biologically relevant tasks including multimodal learning and inference with
missing information, suggesting that bPC resembles biological visual inference
more closely.

</details>


### [87] [Enhanced DACER Algorithm with High Diffusion Efficiency](https://arxiv.org/abs/2505.23426)
*Yinuo Wang, Mining Tan, Wenjun Zou, Haotian Lin, Xujie Song, Wenxuan Wang, Tong Liu, Likun Wang, Guojian Zhan, Tianze Zhu, Shiqi Liu, Jingliang Duan, Shengbo Eben Li*

**主要类别:** cs.LG

**概要:** 为了提高训练和推理效率，同时保持或增强性能，本文提出了Q-gradient field目标和时间加权机制。实验表明，DACER2算法在MuJoCo基准测试和多模态任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 尽管扩散模型在离线RL和模仿学习中表现良好，但在线RL中的应用受限于需要大量扩散步骤，导致训练效率低下。直接减少步骤会导致性能显著下降，因此需要一种新方法来解决这一问题。

**方法:** 提出了一种Q-gradient field目标作为辅助优化目标，以指导每个扩散步骤的去噪过程。此外，引入了时间加权机制，使模型能够在早期阶段有效消除大规模噪声，并在后期阶段细化动作。

**结果:** DACER2算法在大多数MuJoCo控制任务中仅用5个扩散步骤就达到了最先进的性能，并且与DACER相比展现出更强的多模态性。

**结论:** 通过使用Q-gradient field目标和时间加权机制，DACER2提高了训练和推理效率，同时维持甚至增强了性能，为实时在线RL设置提供了更好的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhanced+DACER+Algorithm+with+High+Diffusion+Efficiency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23426&send_immediately=true&force_search=false)

**原文摘要:** Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.

</details>


### [88] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/abs/2505.23173)
*Shohei Enomoto*

**主要类别:** cs.LG

**概要:** Deep learning models may not perform well when deployed on data distributions different from their training data. To address this, Multi-source Domain Generalization (MDG) shows promise but is limited by the cost of creating multi-domain datasets. This paper proposes Pseudo Multi-source Domain Generalization (PMDG), which generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques to enable MDG in Single-source Domain Generalization settings. Experiments show that PMDG performance correlates positively with MDG performance and can match or exceed actual multi-domain performance with sufficient data.


<details>
  <summary>更多</summary>
  
**动机:** Deep learning models often struggle to maintain performance when deployed on data distributions different from their training data, particularly in real-world applications where environmental conditions frequently change. The current methods for Multi-source Domain Generalization (MDG) are limited by the significant costs and difficulties associated with creating multi-domain datasets.

**方法:** The authors propose Pseudo Multi-source Domain Generalization (PMDG), a novel framework that enables the application of sophisticated MDG algorithms in more practical Single-source Domain Generalization (SDG) settings. PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, creating a synthetic multi-domain dataset that can be used with existing MDG algorithms.

**结果:** Experiments conducted using PseudoDomainBed, a modified version of the DomainBed benchmark, reveal several key findings. There is a positive correlation between MDG and PMDG performance, and pseudo-domains have the potential to match or exceed actual multi-domain performance with sufficient data.

**结论:** The comprehensive empirical results provide valuable insights for future research in domain generalization. The authors' code is available at https://github.com/s-enmt/PseudoDomainBed.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pseudo+Multi-Source+Domain+Generalization%3A+Bridging+the+Gap+Between+Single+and+Multi-Source+Domain+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23173&send_immediately=true&force_search=false)

**原文摘要:** Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [89] [Bounded-Abstention Pairwise Learning to Rank](https://arxiv.org/abs/2505.23437)
*Antonio Ferrara, Andrea Pugnana, Francesco Bonchi, Salvatore Ruggieri*

**主要类别:** cs.LG

**概要:** 在高风险领域（如健康、教育和就业），排名系统对决策有很大影响，因此需要引入安全机制。本文提出了一种新的两两学习排序任务中的弃权方法，通过阈值化排序器的条件风险来决定是否将决策交给人类专家处理，并提供了最优弃权策略的理论分析、模型无关的算法以及实证评估。


<details>
  <summary>更多</summary>
  
**动机:** 弃权机制在分类任务中已有较多研究，但在其他机器学习范式中的应用尚待探索，尤其是在排序任务中的应用。

**方法:** 提出了一种基于阈值化排序器条件风险的弃权方法，当估计风险超过预定义阈值时，系统会放弃决策并将决策权交给人类专家。同时，提供了最优弃权策略的理论表征和一个模型无关的插件算法用于构建弃权排序模型。

**结果:** 通过多个数据集上的全面实证评估，证明了该方法的有效性。

**结论:** 本文提出的弃权方法为两两学习排序任务提供了一个有效的安全机制，具有广泛的适用性和良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bounded-Abstention+Pairwise+Learning+to+Rank，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23437，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23437&send_immediately=true&force_search=false)

**原文摘要:** Ranking systems influence decision-making in high-stakes domains like health,
education, and employment, where they can have substantial economic and social
impacts. This makes the integration of safety mechanisms essential. One such
mechanism is $\textit{abstention}$, which enables algorithmic decision-making
system to defer uncertain or low-confidence decisions to human experts. While
abstention have been predominantly explored in the context of classification
tasks, its application to other machine learning paradigms remains
underexplored. In this paper, we introduce a novel method for abstention in
pairwise learning-to-rank tasks. Our approach is based on thresholding the
ranker's conditional risk: the system abstains from making a decision when the
estimated risk exceeds a predefined threshold. Our contributions are threefold:
a theoretical characterization of the optimal abstention strategy, a
model-agnostic, plug-in algorithm for constructing abstaining ranking models,
and a comprehensive empirical evaluations across multiple datasets,
demonstrating the effectiveness of our approach.

</details>


### [90] [The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.23176)
*Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li*

**主要类别:** cs.LG

**概要:** 为了提高联邦学习(FL)的训练效率，先前的研究采用了低秩分解技术以减少通信开销。本文旨在提升这些低秩分解方法的性能，聚焦于分解什么、如何分解和如何聚合这三个关键问题，并提出了三个新技术：模型更新分解(MUD)，块状克罗内克分解(BKD)和聚合感知分解(AAD)。此外，我们提供了严格的理论分析以确保MUD的收敛性。实验结果表明，我们的方法比相关基线方法具有更快的收敛速度和更高的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管低秩分解技术已被用于减少联邦学习中的通信开销，但其性能仍有待提升。因此，本文寻求优化低秩分解方法的性能，解决分解目标、分解方式和聚合策略等核心问题。

**方法:** 本文提出三种新方法：1. 模型更新分解(MUD)，针对模型更新进行分解；2. 块状克罗内克分解(BKD)，提供更高效的分解方式；3. 聚合感知分解(AAD)，改进聚合过程。这三种方法可以同时使用，互相补充，达到最佳性能。此外，文章还对MUD进行了严格的理论分析，确保其收敛性。

**结果:** 广泛的实验结果表明，所提出的方法在收敛速度和准确率方面均优于相关的基准方法。

**结论:** 本文通过引入MUD、BKD和AAD三种技术，有效提升了低秩分解方法在联邦学习中的性能。实验验证了该方法的优越性，并且代码已开源供研究者使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Panaceas+for+Improving+Low-Rank+Decomposition+in+Communication-Efficient+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23176，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23176&send_immediately=true&force_search=false)

**原文摘要:** To improve the training efficiency of federated learning (FL), previous
research has employed low-rank decomposition techniques to reduce communication
overhead. In this paper, we seek to enhance the performance of these low-rank
decomposition methods. Specifically, we focus on three key issues related to
decomposition in FL: what to decompose, how to decompose, and how to aggregate.
Subsequently, we introduce three novel techniques: Model Update Decomposition
(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware
Decomposition (AAD), each targeting a specific issue. These techniques are
complementary and can be applied simultaneously to achieve optimal performance.
Additionally, we provide a rigorous theoretical analysis to ensure the
convergence of the proposed MUD. Extensive experimental results show that our
approach achieves faster convergence and superior accuracy compared to relevant
baseline methods. The code is available at
https://github.com/Leopold1423/fedmud-icml25.

</details>


### [91] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2505.23529)
*Shifeng Xie, Aref Einizade, Jhony H. Giraldo*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的子图高斯嵌入对比（SubGEC）方法，用于图表示学习的自监督学习。通过引入子图高斯嵌入模块和最优传输距离，该方法在多个基准测试中表现出色，并为设计GRL的SSL方法提供了新见解。


<details>
  <summary>更多</summary>
  
**动机:** 尽管自监督学习（SSL）方法在图表示学习（GRL）中被广泛使用，但如何有效地生成并对比子图以保留其特性仍然是一个挑战。因此，需要一种新方法来改进这一过程。

**方法:** 提出了SubGEC方法，包含一个子图高斯嵌入模块，将子图映射到结构化的高斯空间，同时利用Wasserstein和Gromov-Wasserstein距离来衡量子图间的相似性。

**结果:** 在多个基准测试中的广泛实验证明，SubGEC方法优于或与现有最先进的方法表现相当。

**结论:** 研究结果表明，在设计用于GRL的SSL方法时，生成对比对的分布具有重要影响，为未来的研究提供了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Subgraph+Gaussian+Embedding+Contrast+for+Self-Supervised+Graph+Representation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23529&send_immediately=true&force_search=false)

**原文摘要:** Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [92] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
*Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, Shuang Qiu*

**主要类别:** cs.LG

**概要:** 本文针对大型语言模型推理能力的强化学习优化问题，提出了Segment Policy Optimization (SPO) 框架，通过中间粒度的片段级优势估计方法解决了现有方法精确性与成本之间的矛盾，并在短CoT和长CoT场景中均取得了显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 当前强化学习方法在增强大型语言模型推理能力方面面临挑战。主要问题在于现有方法的优势估计粒度要么过细（如PPO），导致难以训练精确的批评模型，要么过粗（如GRPO），导致信用分配不准确。因此，需要一种能够在精确性和计算成本之间取得更好平衡的方法。

**方法:** 提出了一种名为Segment Policy Optimization (SPO) 的新RL框架，通过中间粒度的片段级优势估计方法，在精确性与计算成本之间实现了更好的平衡。该方法包含三个关键组件：灵活的片段划分、精确的片段优势估计以及基于片段优势的策略优化（包括新的概率掩码策略）。此外，SPO被实例化为两种特定场景：SPO-chain用于短CoT推理，采用基于切点的划分和链式优势估计；SPO-tree用于长CoT推理，采用树状优势估计以降低MC估计成本。

**结果:** 实验结果表明，SPO-chain在GSM8K数据集上相比PPO和GRPO提升了6-12个百分点的准确率；SPO-tree在MATH500数据集（2K和4K上下文评估）上相比GRPO提升了7-11个百分点的准确率。这验证了SPO框架的有效性和优越性。

**结论:** SPO框架在短CoT和长CoT场景中均表现优异，分别在GSM8K和MATH500数据集上取得了显著的准确率提升。此外，作者已将代码公开，便于后续研究与应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Segment+Policy+Optimization%3A+Effective+Segment-Level+Credit+Assignment+in+RL+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23564&send_immediately=true&force_search=false)

**原文摘要:** Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [93] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/abs/2505.23182)
*Srijith Nair, Michael Lin, Amirreza Talebi, Peizhong Ju, Elizabeth Bentley, Jia Liu*

**主要类别:** cs.LG

**概要:** 提出了一种新的联合分割学习算法FSL-SAGE，通过辅助模型估计服务器端梯度反馈，从而在降低通信成本和客户端内存需求的同时，实现了与FedAvg相当的收敛速度，并且在实验中表现优于现有的FSL方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习（FL）方法假设客户端能够训练整个模型，这对于大规模模型来说是不可行的；而分割学习（SL）虽然通过将大部分训练任务转移到服务器上减轻了客户端的内存限制，但由于其顺序性质导致网络延迟增加。其他方法通过使用本地损失函数进行并行客户端训练来提高效率，但缺乏服务器反馈可能导致准确性较差。

**方法:** 提出了FSL-SAGE算法，该算法通过辅助模型估计服务器端梯度反馈。这些辅助模型会周期性地适应以模拟服务器在本地数据集上的行为。

**结果:** 理论分析表明，FSL-SAGE的收敛速率为$\mathcal{O}(1/\sqrt{T})$，其中$T$为通信轮数，这一结果与FedAvg相当，同时显著降低了通信成本和客户端内存需求。实证结果也验证了该方法在通信效率和准确性方面优于现有的最先进的FSL方法。

**结论:** FSL-SAGE是一种有效的联邦分割学习算法，能够在减少通信成本和客户端内存需求的同时保持较高的准确性和收敛速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FSL-SAGE%3A+Accelerating+Federated+Split+Learning+via+Smashed+Activation+Gradient+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23182&send_immediately=true&force_search=false)

**原文摘要:** Collaborative training methods like Federated Learning (FL) and Split
Learning (SL) enable distributed machine learning without sharing raw data.
However, FL assumes clients can train entire models, which is infeasible for
large-scale models. In contrast, while SL alleviates the client memory
constraint in FL by offloading most training to the server, it increases
network latency due to its sequential nature. Other methods address the
conundrum by using local loss functions for parallel client-side training to
improve efficiency, but they lack server feedback and potentially suffer poor
accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation
Gradient Estimation), a new federated split learning algorithm that estimates
server-side gradient feedback via auxiliary models. These auxiliary models
periodically adapt to emulate server behavior on local datasets. We show that
FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is
the number of communication rounds. This result matches FedAvg, while
significantly reducing communication costs and client memory requirements. Our
empirical results also verify that it outperforms existing state-of-the-art FSL
methods, offering both communication efficiency and accuracy.

</details>


### [94] [Two Is Better Than One: Rotations Scale LoRAs](https://arxiv.org/abs/2505.23184)
*Hongcan Guo, Guoshun Nan, Yuan Yang, Diyang Zhang, Haotian Li, Zhican Chen, Qinchuan Zhou, Yuhan Ran, Xinye Cao, Sicong Leng, Xiaofeng Tao, Xudong Jiang*

**主要类别:** cs.LG

**概要:** RadarGate是一种新颖的几何启发式门控方法，通过引入LoRA表示的旋转操作来增强表达能力，解决大型语言模型中的泛化不良和欠拟合问题。实验表明其在多个基准任务上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 传统路由机制可能限制大规模语言模型（LLMs）的可扩展性，导致泛化不良和欠拟合问题。研究发现根本原因在于现有加权求和机制的表达能力受限。

**方法:** 提出RadarGate，一种基于几何启发的门控方法，通过学习组件将每个LoRA表示与其他LoRA融合，并通过旋转矩阵定义LoRA表示之间的相对角度关系，从而提升表达能力和特征交互。

**结果:** 在6个公共基准、21个任务上的广泛实验证明了RadarGate的有效性。同时揭示了旋转操作对语义相似表示的对齐作用。

**结论:** RadarGate提供了一个额外的自由度，能够有效解决随着LoRA数量增加而出现的泛化不良和欠拟合问题，为可扩展的LLMs提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two+Is+Better+Than+One%3A+Rotations+Scale+LoRAs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23184，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23184&send_immediately=true&force_search=false)

**原文摘要:** Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates
large language models (LLMs) to efficiently adapt to diverse tasks. However,
traditional gating mechanisms that route inputs to the best experts may
fundamentally hinder LLMs' scalability, leading to poor generalization and
underfitting issues. We identify that the root cause lies in the restricted
expressiveness of existing weighted-sum mechanisms, both within and outside the
convex cone of LoRA representations. This motivates us to propose RadarGate, a
novel geometrically inspired gating method that introduces rotational
operations of LoRAs representations to boost the expressiveness and facilitate
richer feature interactions among multiple LoRAs for scalable LLMs.
Specifically, we first fuse each LoRA representation to other LoRAs using a
learnable component and then feed the output to a rotation matrix. This matrix
involves learnable parameters that define the relative angular relationship
between LoRA representations. Such a simple yet effective mechanism provides an
extra degree of freedom, facilitating the learning of cross-LoRA synergies and
properly tracking the challenging poor generalization and underfitting issues
as the number of LoRA grows. Extensive experiments on 6 public benchmarks
across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We
also provide valuable insights, revealing that the rotations to each pair of
representations are contrastive, encouraging closer alignment of semantically
similar representations during geometrical transformation while pushing
distance ones further apart. We will release our code to the community.

</details>


### [95] [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/abs/2505.23724)
*Minrui Luo, Fuhang Kuang, Yu Wang, Zirui Liu, Tianxing He*

**主要类别:** cs.LG

**概要:** SC-LoRA是一种新的LoRA初始化框架，通过在低秩子空间中约束可训练的LoRA适配器输出，平衡高效微调和知识保留之间的权衡。实验表明，SC-LoRA在提高微调性能的同时显著减少了知识遗忘，优于现有的LoRA初始化方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的LoRA方法存在收敛速度慢和知识遗忘的问题，而现有改进方法无法同时解决这两个问题。因此，需要一种新的方法来平衡高效微调和知识保留。

**方法:** 引入了Subspace-Constrained LoRA (SC-LoRA)，通过在低秩子空间中约束可训练LoRA适配器的输出，使得微调数据的主要特征得以关注，同时避免损害保留的知识特征。

**结果:** 理论分析和广泛的实验表明，SC-LoRA在各种下游任务中表现出优越的微调性能，并显著减少了知识遗忘，超越了当代的LoRA初始化方法。

**结论:** SC-LoRA提供了一种有效的解决方案，在提升微调效率的同时减少知识遗忘，为LoRA初始化方法提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SC-LoRA%3A+Balancing+Efficient+Fine-tuning+and+Knowledge+Preservation+via+Subspace-Constrained+LoRA，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23724&send_immediately=true&force_search=false)

**原文摘要:** Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), are indispensable for efficiently customizing Large Language
Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and
knowledge forgetting problems. Recent studies have leveraged the power of
designed LoRA initialization, to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM. However, none of these works can
address the two cases at the same time. To this end, we introduce
Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework
engineered to navigate the trade-off between efficient fine-tuning and
knowledge preservation. We achieve this by constraining the output of trainable
LoRA adapters in a low-rank subspace, where the context information of
fine-tuning data is most preserved while the context information of preserved
knowledge is least retained, in a balanced way. Such constraint enables the
trainable weights to primarily focus on the main features of fine-tuning data
while avoiding damaging the preserved knowledge features. We provide
theoretical analysis on our method, and conduct extensive experiments including
safety preservation and world knowledge preservation, on various downstream
tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning
performance while markedly diminishing knowledge forgetting, surpassing
contemporary LoRA initialization methods.

</details>


### [96] [Improving the Effective Receptive Field of Message-Passing Neural Networks](https://arxiv.org/abs/2505.23185)
*Shahaf E. Finder, Ron Shapira Weber, Moshe Eliasof, Oren Freifeld, Eran Treister*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的图神经网络架构IM-MPNN，通过多尺度信息传递解决MPNN中的有限有效感受野问题，提升长程依赖捕捉能力。


<details>
  <summary>更多</summary>
  
**动机:** Message-Passing Neural Networks (MPNNs) 在处理图结构数据时面临过压缩等问题，无法有效捕捉长程依赖关系，类似于CNN中的有效感受野（ERF）受限的问题。

**方法:** 受CNN中增强ERF方法的启发，提出了Interleaved Multiscale Message-Passing Neural Networks (IM-MPNN)，该方法通过图的分层粗化，实现跨多尺度表示的消息传递，促进长程交互，无需过多深度或参数。

**结果:** 在如Long-Range Graph Benchmark (LRGB) 等基准上的广泛评估表明，与基础MPNN相比，IM-MPNN在保持计算效率的同时显著提升了捕捉长程依赖的能力。

**结论:** IM-MPNN架构能够有效缓解MPNN中的有限有效感受野问题，并为图结构数据的长程依赖分析提供了一种高效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+the+Effective+Receptive+Field+of+Message-Passing+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23185&send_immediately=true&force_search=false)

**原文摘要:** Message-Passing Neural Networks (MPNNs) have become a cornerstone for
processing and analyzing graph-structured data. However, their effectiveness is
often hindered by phenomena such as over-squashing, where long-range
dependencies or interactions are inadequately captured and expressed in the
MPNN output. This limitation mirrors the challenges of the Effective Receptive
Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical
receptive field is underutilized in practice. In this work, we show and
theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired
by recent advances in ERF augmentation for CNNs, we propose an Interleaved
Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address
these problems in MPNNs. Our method incorporates a hierarchical coarsening of
the graph, enabling message-passing across multiscale representations and
facilitating long-range interactions without excessive depth or
parameterization. Through extensive evaluations on benchmarks such as the
Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over
baseline MPNNs in capturing long-range dependencies while maintaining
computational efficiency.

</details>


### [97] [REOrdering Patches Improves Vision Models](https://arxiv.org/abs/2505.23751)
*Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta*

**主要类别:** cs.LG

**概要:** REOrder是一个两阶段框架，通过信息论先验和强化学习优化patch排列顺序，在ImageNet-1K和Functional Map of the World数据集上显著提升模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 在使用transformers等序列模型处理图像时，通常需要将图像展平为一维序列。尽管全自注意力机制具有排列等变性，现代长序列transformers依赖的架构近似破坏了这种不变性，使得patch顺序对模型性能产生显著影响。因此，作者希望通过重新排序patch来改善模型性能。

**方法:** REOrder包含两个阶段：第一阶段是通过评估不同patch序列的可压缩性来导出信息论先验；第二阶段是通过优化Plackett-Luce策略（使用REINFORCE算法）学习排列策略，从而在组合排列空间中高效学习任务最优的patch顺序。

**结果:** REOrder在ImageNet-1K数据集上比传统行优先顺序提升了最高3.01%的top-1准确率，在Functional Map of the World数据集上提升了13.35%。

**结论:** REOrder提供了一种有效的方法来发现任务最优的patch顺序，显著提升了基于transformer的视觉模型的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REOrdering+Patches+Improves+Vision+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23751，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23751&send_immediately=true&force_search=false)

**原文摘要:** Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


### [98] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/abs/2505.23190)
*Yekun Zhu, Min Tang, Zheng Ma*

**主要类别:** cs.LG

**概要:** 本研究提出了一种新的神经网络方法DeepRTE，用于解决辐射传递方程（RTE），该方法利用预训练的基于注意力的神经网络，具有高精度和计算效率，并通过全面的数值实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决稳态辐射传递方程（RTE），提高求解的准确性和计算效率，特别是在诸如中子输运、大气辐射传递、热传递和光学成像等领域的应用。

**方法:** 提出了一种名为DeepRTE的新神经网络方法，使用预训练的基于注意力机制的神经网络来解决辐射传递方程。

**结果:** 通过综合数值实验验证了所提出方法的有效性。

**结论:** DeepRTE框架能够以高精度和计算效率解决辐射传递方程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepRTE%3A+Pre-trained+Attention-based+Neural+Network+for+Radiative+Tranfer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23190&send_immediately=true&force_search=false)

**原文摘要:** In this study, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our proposed DeepRTE framework leverages pre-trained
attention-based neural networks to solve the RTE with high accuracy and
computational efficiency. The efficacy of the proposed approach is
substantiated through comprehensive numerical experiments.

</details>


### [99] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
*Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo*

**主要类别:** cs.LG

**概要:** Direct Preference Optimization (DPO) 的理论基础通过 Differential Information Distribution (DID) 得到完善，揭示了偏好数据结构与策略行为之间的统一视角。低熵差异信息有助于知识密集型问答，而高熵差异信息对通用指令跟随至关重要。


<details>
  <summary>更多</summary>
  
**动机:** 尽管 DPO 在对齐语言模型和人类偏好方面取得了经验上的成功，但其 log-ratio 奖励参数化的理论依据尚不完整，需要进一步研究以填补这一空白。

**方法:** 利用 Differential Information Distribution (DID)，分析在偏好标签编码从参考策略到目标策略所需的差异信息时，DPO 的 log-ratio 奖励为何是学习目标策略的唯一最优形式，并推导出拒绝响应的最优采样分布闭式表达。同时，探讨偏好编码差异信息的条件与 log-margin 有序策略的隐含假设之间的联系，并通过 DID 的熵分析学习低熵和高熵差异信息对策略分布的影响。

**结果:** 理论结果表明，学习低熵差异信息可强化策略分布，适用于知识密集型问答；而学习高熵差异信息则对通用指令跟随任务至关重要。这些发现通过合成实验和真实世界数据集得到了验证。

**结论:** 本工作通过差异信息的视角，提出了对 DPO 目标、偏好数据结构及策略行为的统一理解，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Differential+Information%3A+An+Information-Theoretic+Perspective+on+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23761&send_immediately=true&force_search=false)

**原文摘要:** Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


### [100] [Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics](https://arxiv.org/abs/2505.23194)
*Shiwei Li, Xiandi Luo, Xing Tang, Haozhao Wang, Hao Chen, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li*

**主要类别:** cs.LG

**概要:** 研究非零初始化对LoRA微调动力学的影响，发现其能提高对次优学习率的鲁棒性且不会显著影响微调性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在标准LoRA层中使用零初始化以确保从预训练模型开始微调，但缺乏理论支持。因此，本文探索非零初始化对LoRA微调动力学的影响。

**方法:** 从无限宽度视角分析同时将矩阵A和B初始化为非零值的情况，评估其对LoRA微调动力学的影响。

**结果:** 非零初始化提高了LoRA对次优（尤其是较小）学习率的鲁棒性，并且引入的随机噪声通常不影响微调性能。

**结论:** 微调不需要严格从预训练模型开始，非零初始化是可行的，实验结果支持这一结论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Zero+Initialization%3A+Investigating+the+Impact+of+Non-Zero+Initialization+on+LoRA+Fine-Tuning+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23194，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23194&send_immediately=true&force_search=false)

**原文摘要:** Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method. In standard LoRA layers, one of the matrices, $A$ or $B$, is
initialized to zero, ensuring that fine-tuning starts from the pretrained
model. However, there is no theoretical support for this practice. In this
paper, we investigate the impact of non-zero initialization on LoRA's
fine-tuning dynamics from an infinite-width perspective. Our analysis reveals
that, compared to zero initialization, simultaneously initializing $A$ and $B$
to non-zero values improves LoRA's robustness to suboptimal learning rates,
particularly smaller ones. Further analysis indicates that although the
non-zero initialization of $AB$ introduces random noise into the pretrained
weight, it generally does not affect fine-tuning performance. In other words,
fine-tuning does not need to strictly start from the pretrained model. The
validity of our findings is confirmed through extensive experiments across
various models and datasets. The code is available at
https://github.com/Leopold1423/non_zero_lora-icml25.

</details>


### [101] [Daunce: Data Attribution through Uncertainty Estimation](https://arxiv.org/abs/2505.23223)
*Xingyuan Pan, Chenlu Ye, Joseph Melkonian, Jiaqi W. Ma, Tong Zhang*

**主要类别:** cs.LG

**概要:** Daunce是一种通过不确定性估计实现数据归属的新方法，它通过对扰动模型进行微调并计算每个样本损失的协方差作为归属分数，具有更高的准确性和可扩展性，适用于大型语言模型和黑盒模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于梯度的TDA方法在大规模应用中受限，而随机投影方法虽然提高了可扩展性，但归属准确性有所下降。为解决这一问题，并受不确定性与影响函数之间联系的启发，提出了新的方法。

**方法:** Daunce通过对一组扰动模型进行微调，并计算这些模型中每个样本损失的协方差作为归属分数，从而实现数据归属。

**结果:** Daunce在从视觉任务到大型语言模型微调的各种任务上进行了验证，表现出比现有TDA方法更准确的归属能力，并且首次在专有的大型语言模型（如OpenAI的GPT模型）上实现了数据归属。

**结论:** Daunce提供了一种简单而有效的方法来进行数据归属，尤其在大型语言模型和黑盒模型中表现优异，为数据调试、策展和估值等关键应用提供了支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Daunce%3A+Data+Attribution+through+Uncertainty+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23223，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23223&send_immediately=true&force_search=false)

**原文摘要:** Training data attribution (TDA) methods aim to identify which training
examples influence a model's predictions on specific test data most. By
quantifying these influences, TDA supports critical applications such as data
debugging, curation, and valuation. Gradient-based TDA methods rely on
gradients and second-order information, limiting their applicability at scale.
While recent random projection-based methods improve scalability, they often
suffer from degraded attribution accuracy. Motivated by connections between
uncertainty and influence functions, we introduce Daunce - a simple yet
effective data attribution approach through uncertainty estimation. Our method
operates by fine-tuning a collection of perturbed models and computing the
covariance of per-example losses across these models as the attribution score.
Daunce is scalable to large language models (LLMs) and achieves more accurate
attribution compared to existing TDA methods. We validate Daunce on tasks
ranging from vision tasks to LLM fine-tuning, and further demonstrate its
compatibility with black-box model access. Applied to OpenAI's GPT models, our
method achieves, to our knowledge, the first instance of data attribution on
proprietary LLMs.

</details>


### [102] [Generalizability vs. Counterfactual Explainability Trade-Off](https://arxiv.org/abs/2505.23225)
*Fabiano Veglianti, Flavio Giorgi, Fabrizio Silvestri, Gabriele Tolomei*

**主要类别:** cs.LG

**概要:** 本文研究了监督学习中模型泛化与反事实可解释性之间的关系，提出了ε-有效反事实概率(ε-VCP)的概念，并通过理论和实证分析揭示了泛化能力差与反事实生成容易之间的内在权衡。


<details>
  <summary>更多</summary>
  
**动机:** 在监督学习中，理解模型泛化能力与反事实解释之间的关系对于提高模型性能和透明度至关重要。

**方法:** 引入ε-有效反事实概率(ε-VCP)的概念，分析其与模型决策边界几何形状的关系，证明ε-VCP随着模型过拟合而增加。

**结果:** 理论分析和实证结果表明，较差的泛化能力与更容易生成反事实之间存在严格的联系，且ε-VCP可以作为量化过拟合的实际代理指标。

**结论:** 模型的泛化能力和反事实可解释性之间存在权衡，ε-VCP为评估模型过拟合提供了一个实用工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalizability+vs.+Counterfactual+Explainability+Trade-Off，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23225&send_immediately=true&force_search=false)

**原文摘要:** In this work, we investigate the relationship between model generalization
and counterfactual explainability in supervised learning. We introduce the
notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) --
the probability of finding perturbations of a data point within its
$\varepsilon$-neighborhood that result in a label change. We provide a
theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the
model's decision boundary, showing that $\varepsilon$-VCP tends to increase
with model overfitting. Our findings establish a rigorous connection between
poor generalization and the ease of counterfactual generation, revealing an
inherent trade-off between generalization and counterfactual explainability.
Empirical results validate our theory, suggesting $\varepsilon$-VCP as a
practical proxy for quantitatively characterizing overfitting.

</details>


### [103] [Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method](https://arxiv.org/abs/2505.23228)
*Wanfu Gao, Jun Gao, Qingqi Han, Hanlin Pan, Kunpeng Liu*

**主要类别:** cs.LG

**概要:** 为了应对多标签数据集中特征和标签之间复杂关系的挑战，本文提出了一种新的方法：通过构建随机游走图来捕捉非线性和隐式间接关联，并通过低维表示系数对变量空间进行对齐，同时保留原始高维数据与低维表示空间之间的流形结构。实验表明该方法在多个基准数据集上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 特征维度的快速增长使得多标签数据集中特征与标签之间的关系变得更加复杂，而现有的低维线性分解方法难以捕捉这些复杂的非线性关联，可能导致特征空间与标签空间的错位。

**方法:** 1. 设计了一个随机游走图，将特征-特征、标签-标签以及特征-标签的关系集成在一起，以准确捕捉非线性和隐式间接关联。
2. 在低秩分解后优化了特征和标签之间关联的潜在表示。
3. 通过低维表示系数对齐变量空间，同时保留原始高维多标签数据和低维表示空间之间的流形结构。

**结果:** 在七个基准数据集和三个代表性数据集上的广泛实验和消融研究证明了所提方法的优越性。

**结论:** 所提出的基于随机游走图的方法能够有效捕捉特征和标签之间的复杂非线性关联，并通过对齐变量空间和保留流形结构提升了多标签学习的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Random+Walk+with+Feature-Label+Space+Alignment%3A+A+Multi-Label+Feature+Selection+Method，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23228，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23228&send_immediately=true&force_search=false)

**原文摘要:** The rapid growth in feature dimension may introduce implicit associations
between features and labels in multi-label datasets, making the relationships
between features and labels increasingly complex. Moreover, existing methods
often adopt low-dimensional linear decomposition to explore the associations
between features and labels. However, linear decomposition struggles to capture
complex nonlinear associations and may lead to misalignment between the feature
space and the label space. To address these two critical challenges, we propose
innovative solutions. First, we design a random walk graph that integrates
feature-feature, label-label, and feature-label relationships to accurately
capture nonlinear and implicit indirect associations, while optimizing the
latent representations of associations between features and labels after
low-rank decomposition. Second, we align the variable spaces by leveraging
low-dimensional representation coefficients, while preserving the manifold
structure between the original high-dimensional multi-label data and the
low-dimensional representation space. Extensive experiments and ablation
studies conducted on seven benchmark datasets and three representative datasets
using various evaluation metrics demonstrate the superiority of the proposed
method\footnote{Code: https://github.com/Heilong623/-GRW-}.

</details>


### [104] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/abs/2505.23244)
*Emo Todorov*

**主要类别:** cs.LG

**概要:** 在连续控制中，策略梯度可以用于随机和确定性策略。本文研究了两者的联系，并展示了在特定MDP家族中，它们的策略梯度、自然梯度和状态值函数相同，但状态-控制值函数不同。此外，提出了一种构造与给定随机策略MDP等效的确定性策略MDP的一般程序。结果表明，通过逼近状态值函数而不是状态-控制值函数，可以统一策略梯度方法。


<details>
  <summary>更多</summary>
  
**动机:** 理解随机和确定性策略梯度之间的关系，以便更好地理解和优化连续控制中的策略梯度方法。

**方法:** 分析特定MDP家族中的随机和确定性策略梯度、自然梯度和值函数；开发一种将随机策略MDP转换为等效确定性策略MDP的通用方法。

**结果:** 在涉及高斯控制噪声和二次控制成本的广泛使用的MDP家族中，随机和确定性策略梯度、自然梯度和状态值函数是相同的，而状态-控制值函数不同。还提出了一种构建与给定随机策略MDP等效的确定性策略MDP的一般程序。

**结论:** 策略梯度方法可以通过逼近状态值函数而不是状态-控制值函数来实现统一。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Equivalence+of+stochastic+and+deterministic+policy+gradients，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23244&send_immediately=true&force_search=false)

**原文摘要:** Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [105] [Measuring Participant Contributions in Decentralized Federated Learning](https://arxiv.org/abs/2505.23246)
*Honoka Anada, Tatsuya Kaneko, Shinya Takamaeda-Yamazaki*

**主要类别:** cs.LG

**概要:** 本文提出了DFL-Shapley和DFL-MR两种方法，用于测量去中心化联邦学习（DFL）中参与者的贡献，并验证了其有效性和适用性。


<details>
  <summary>更多</summary>
  
**动机:** 在去中心化联邦学习（DFL）中，由于没有中央服务器以及存在多个全局模型，传统的贡献度量方法难以应用。因此需要新的方法来衡量参与者在DFL中的贡献，以激励参与者并保证透明性。

**方法:** 提出DFL-Shapley方法，这是对广泛使用的CFL度量的适应性改进，专门针对去中心化环境；同时引入DFL-MR，这是一种可计算的近似方法，通过累积轮次Shapley值来估计总体贡献。

**结果:** 实验结果表明，DFL-Shapley是一个有效的基准度量，而DFL-MR在不同设置下接近DFL-Shapley，证明了这两种方法的有效性。

**结论:** DFL-Shapley和DFL-MR为去中心化联邦学习提供了一种有效的贡献度量方式，能够激励参与者并确保系统透明性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Participant+Contributions+in+Decentralized+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23246&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their data. Measuring participant contributions in FL is
crucial for incentivizing clients and ensuring transparency. While various
methods have been proposed for contribution measurement, they are designed
exclusively for centralized federated learning (CFL), where a central server
collects and aggregates client models, along with evaluating their
contributions. Meanwhile, decentralized federated learning (DFL), in which
clients exchange models directly without a central server, has gained
significant attention for mitigating communication bottlenecks and eliminating
a single point of failure. However, applying existing contribution measurement
methods to DFL is challenging due to the presence of multiple global models and
the absence of a central server. In this study, we present novel methodologies
for measuring participant contributions in DFL. We first propose DFL-Shapley,
an extension of the Shapley value tailored for DFL, adapting this widely used
CFL metric to decentralized settings. Given the impracticality of computing the
ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable
approximation that estimates overall contributions by accumulating round-wise
Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios
and compare them with existing CFL metrics. The experimental results confirm
DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity
to DFL-Shapley across various settings, highlighting their effectiveness as
contribution metrics in DFL.

</details>


### [106] [Efficiently Access Diffusion Fisher: Within the Outer Product Span Space](https://arxiv.org/abs/2505.23264)
*Fangyikang Wang, Hubery Yin, Shaobin Zhuang, Huminhao Zhu, Yinan Li, Lei Qian, Chao Zhang, Hanbin Zhao, Hui Qian, Chen Li*

**主要类别:** cs.LG

**概要:** 近期扩散模型（DMs）研究探索了将二阶扩散Fisher信息（DF）纳入下游任务和理论分析中。然而，当前方法通常通过自动微分学习得分网络来近似DF，这种方法虽然直接但缺乏精度保证且耗时。本文提出DF实际上位于得分与初始数据外积张成的空间内，并基于此结构开发了两种高效的近似算法以分别获取DF的迹和矩阵-向量乘法。这些算法避免了自动微分操作，采用时间高效向量-乘积计算，并建立了近似误差界。实验表明所提出的算法在似然估计和伴随优化中具有更高的精度和更低的计算成本。此外，基于新的DF外积公式，我们设计了首个用于验证一般PF-ODE导出映射最优传输特性的数值验证实验。


<details>
  <summary>更多</summary>
  
**动机:** 当前对扩散Fisher信息的近似方法依赖于自动微分学习得分网络，存在精度无保证和计算耗时的问题。

**方法:** 揭示了扩散Fisher信息位于得分与初始数据外积张成的空间内，并基于此结构开发了两种高效的近似算法以分别获取DF的迹和矩阵-向量乘法。同时，建立了这些算法的近似误差界。

**结果:** 实验结果表明，所提出的算法在似然估计和伴随优化任务中展现了更高的精度和更低的计算成本。此外，基于新提出的外积公式，成功设计并实现了首个数值验证实验以验证一般PF-ODE导出映射的最优传输特性。

**结论:** 本文提出了两种高效的扩散Fisher信息近似算法，不仅提升了精度还降低了计算成本，同时为验证扩散模型的最优传输特性提供了新的工具和视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficiently+Access+Diffusion+Fisher%3A+Within+the+Outer+Product+Span+Space，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23264&send_immediately=true&force_search=false)

**原文摘要:** Recent Diffusion models (DMs) advancements have explored incorporating the
second-order diffusion Fisher information (DF), defined as the negative Hessian
of log density, into various downstream tasks and theoretical analysis.
However, current practices typically approximate the diffusion Fisher by
applying auto-differentiation to the learned score network. This black-box
method, though straightforward, lacks any accuracy guarantee and is
time-consuming. In this paper, we show that the diffusion Fisher actually
resides within a space spanned by the outer products of score and initial data.
Based on the outer-product structure, we develop two efficient approximation
algorithms to access the trace and matrix-vector multiplication of DF,
respectively. These algorithms bypass the auto-differentiation operations with
time-efficient vector-product calculations. Furthermore, we establish the
approximation error bounds for the proposed algorithms. Experiments in
likelihood evaluation and adjoint optimization demonstrate the superior
accuracy and reduced computational cost of our proposed algorithms.
Additionally, based on the novel outer-product formulation of DF, we design the
first numerical verification experiment for the optimal transport property of
the general PF-ODE deduced map.

</details>


### [107] [Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data](https://arxiv.org/abs/2505.23285)
*Muhammad Shafi, Syed Mohsin Bokhari*

**主要类别:** cs.LG

**概要:** 本研究使用 Sentinel-2 卫星数据和监督机器学习算法，分析了2016年至2021年阿曼苏丹国各省份的土地覆盖和土地利用（LULC）变化。


<details>
  <summary>更多</summary>
  
**动机:** 土地覆盖和土地利用（LULC）变化在资源管理、城市化、土壤与环境保护以及促进可持续发展中起着关键作用。为了更好地监测和预测这些变化，结合多光谱时空卫星数据与先进的机器学习算法成为重要的研究手段。

**方法:** 从开源的 Sentinel-2 卫星数据集中获取多光谱时空数据，并采用监督机器学习算法对不同地类（如水体、作物、城市等）进行训练和分类。随后将构建的模型应用于研究区域，以评估2016年至2021年间各地类的变化。

**结果:** 通过使用 Sentinel-2 数据和机器学习算法，成功实现了对阿曼苏丹国各省份在2016年至2021年间 LULC 变化的有效比较评估。

**结论:** 多光谱时空卫星数据与监督机器学习算法的结合为监测和预测土地覆盖及土地利用变化提供了高效的方法，有助于推动资源管理和可持续发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+Analysis+of+the+Land+Use+and+Land+Cover+Changes+in+Different+Governorates+of+Oman+using+Spatiotemporal+Multi-spectral+Satellite+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23285&send_immediately=true&force_search=false)

**原文摘要:** Land cover and land use (LULC) changes are key applications of satellite
imagery, and they have critical roles in resource management, urbanization,
protection of soils and the environment, and enhancing sustainable development.
The literature has heavily utilized multispectral spatiotemporal satellite data
alongside advanced machine learning algorithms to monitor and predict LULC
changes. This study analyzes and compares LULC changes across various
governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using
annual time steps. For the chosen region, multispectral spatiotemporal data
were acquired from the open-source Sentinel-2 satellite dataset. Supervised
machine learning algorithms were used to train and classify different land
covers, such as water bodies, crops, urban, etc. The constructed model was
subsequently applied within the study region, allowing for an effective
comparative evaluation of LULC changes within the given timeframe.

</details>


### [108] [X2Graph for Cancer Subtyping Prediction on Biological Tabular Data](https://arxiv.org/abs/2505.23334)
*Tu Bui, Mohamed Suliman, Aparajita Haldar, Mohammed Amer, Serban Georgescu*

**主要类别:** cs.LG

**概要:** 在医疗领域中，深度学习对文本、音频和图像数据集产生了变革性影响，但在表格数据中的优势尚不明显。本文提出了一种新的深度学习方法X2Graph，在小型生物表格数据集上表现出色。X2Graph利用表列之间的外部关系知识（如基因相互作用），将每个样本转换为图结构，并应用标准的消息传递算法进行图建模。实验表明，X2Graph在三个癌症亚型数据集上的表现优于现有的基于树和深度学习的方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习对文本、音频和图像数据集产生了变革性影响，但在表格数据，特别是医疗领域中数据稀缺的情况下，其主导地位仍不明确。因此，需要一种新方法来提升深度学习在小规模生物表格数据集上的性能。

**方法:** 提出了一种名为X2Graph的新方法，该方法通过利用表列之间的外部关系知识（例如基因相互作用）将每个样本转换为图结构，然后应用标准的消息传递算法进行图建模。

**结果:** X2Graph在三个癌症亚型数据集上的表现优于现有的基于树和深度学习的方法。

**结论:** X2Graph是一种有效的深度学习方法，能够在小型生物表格数据集上实现优越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是X2Graph+for+Cancer+Subtyping+Prediction+on+Biological+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23334，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23334&send_immediately=true&force_search=false)

**原文摘要:** Despite the transformative impact of deep learning on text, audio, and image
datasets, its dominance in tabular data, especially in the medical domain where
data are often scarce, remains less clear. In this paper, we propose X2Graph, a
novel deep learning method that achieves strong performance on small biological
tabular datasets. X2Graph leverages external knowledge about the relationships
between table columns, such as gene interactions, to convert each sample into a
graph structure. This transformation enables the application of standard
message passing algorithms for graph modeling. Our X2Graph method demonstrates
superior performance compared to existing tree-based and deep learning methods
across three cancer subtyping datasets.

</details>


### [109] [Graph Positional Autoencoders as Self-supervised Learners](https://arxiv.org/abs/2505.23345)
*Yang Liu, Deyu Bo, Wenxuan Cao, Yuan Fang, Yawen Li, Chuan Shi*

**主要类别:** cs.LG

**概要:** Graph self-supervised learning is improved by Graph Positional Autoencoders (GraphPAE) which uses a dual-path architecture to reconstruct node features and positions, leading to state-of-the-art performance.


<details>
  <summary>更多</summary>
  
**动机:** Existing graph autoencoders primarily capture low-frequency signals in the graph and fail to learn expressive structural information. This limits their ability to predict corrupted information effectively.

**方法:** The proposed method, Graph Positional Autoencoders (GraphPAE), employs a dual-path architecture. The feature path enhances message-passing processing with positional encoding for better prediction of corrupted information. The position path refines node positions and approximates eigenvectors, enabling the encoder to learn diverse frequency information.

**结果:** Extensive experiments on heterophilic node classification, graph property prediction, and transfer learning show that GraphPAE achieves state-of-the-art performance and outperforms baselines by a large margin.

**结论:** GraphPAE addresses the limitations of traditional graph autoencoders by capturing both low- and high-frequency signals, resulting in more effective graph representations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Positional+Autoencoders+as+Self-supervised+Learners，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23345，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23345&send_immediately=true&force_search=false)

**原文摘要:** Graph self-supervised learning seeks to learn effective graph representations
without relying on labeled data. Among various approaches, graph autoencoders
(GAEs) have gained significant attention for their efficiency and scalability.
Typically, GAEs take incomplete graphs as input and predict missing elements,
such as masked nodes or edges. While effective, our experimental investigation
reveals that traditional node or edge masking paradigms primarily capture
low-frequency signals in the graph and fail to learn the expressive structural
information. To address these issues, we propose Graph Positional Autoencoders
(GraphPAE), which employs a dual-path architecture to reconstruct both node
features and positions. Specifically, the feature path uses positional encoding
to enhance the message-passing processing, improving GAE's ability to predict
the corrupted information. The position path, on the other hand, leverages node
representations to refine positions and approximate eigenvectors, thereby
enabling the encoder to learn diverse frequency information. We conduct
extensive experiments to verify the effectiveness of GraphPAE, including
heterophilic node classification, graph property prediction, and transfer
learning. The results demonstrate that GraphPAE achieves state-of-the-art
performance and consistently outperforms baselines by a large margin.

</details>


### [110] [Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2505.23347)
*Yuting Li, Shaoyuan Huang, Tengwen Zhang, Cheng Zhang, Xiaofei Wang, Victor C. M. Leung*

**主要类别:** cs.LG

**概要:** 在直播服务快速增长的背景下，众包云边缘服务平台（CCPs）在满足日益增长的需求方面发挥着越来越重要的作用。然而，由于CCPs的不稳定性及规模庞大，传统的流调度优化策略难以取得实际效果。本文提出了Sentinel，一种基于主动异常检测的调度框架，通过两阶段调度范式有效降低了异常频率、提高了收入并加快了调度速度。


<details>
  <summary>更多</summary>
  
**动机:** 直播服务的快速增长使得众包云边缘服务平台（CCPs）变得越来越重要，但其不稳定性和大规模特性使得流调度优化面临挑战，传统的优化策略难以应对这些挑战。

**方法:** 提出了一种名为Sentinel的主动异常检测调度框架，将调度过程建模为两阶段范式：预调度阶段进行异常检测并构建策略池；后调度阶段根据请求触发合适的调度策略以完成调度过程。

**结果:** 实验结果表明，Sentinel显著减少了70%的异常频率，提升了74%的收入，并且将调度速度提高了一倍。

**结论:** Sentinel作为一种基于主动异常检测的调度框架，能够有效应对CCPs中的异常问题，显著提升平台性能和收益，同时加快调度速度，具有较高的实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sentinel%3A+Scheduling+Live+Streams+with+Proactive+Anomaly+Detection+in+Crowdsourced+Cloud-Edge+Platforms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23347&send_immediately=true&force_search=false)

**原文摘要:** With the rapid growth of live streaming services, Crowdsourced Cloud-edge
service Platforms (CCPs) are playing an increasingly important role in meeting
the increasing demand. Although stream scheduling plays a critical role in
optimizing CCPs' revenue, most optimization strategies struggle to achieve
practical results due to various anomalies in unstable CCPs. Additionally, the
substantial scale of CCPs magnifies the difficulties of anomaly detection in
time-sensitive scheduling. To tackle these challenges, this paper proposes
Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel
models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in
the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a
strategy pool; in the post-scheduling stage, upon request arrival, it triggers
an appropriate scheduling based on a pre-generated strategy to implement the
scheduling process. Extensive experiments on realistic datasets show that
Sentinel significantly reduces anomaly frequency by 70%, improves revenue by
74%, and doubles the scheduling speed.

</details>


### [111] [Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control](https://arxiv.org/abs/2505.23355)
*Maxiu Xiao, Jianglin Lan, Jingxing Yu, Eldert van Henten, Congcong Sun*

**主要类别:** cs.LG

**概要:** 本研究探讨了在温室气候控制中应用带有不完美输入的交互式强化学习（RL）的可能性与性能。通过开发三种交互式RL算法（奖励塑造、策略塑造和控制共享），分析输入特性的矛盾性，提出基于神经网络的方法以增强鲁棒性，并在模拟环境中评估这些算法。结果表明，策略塑造和控制共享能有效应对不完美输入，分别提高8.4%和6.8%的利润；而奖励塑造对不完美输入敏感，导致9.4%的利润下降。


<details>
  <summary>更多</summary>
  
**动机:** 温室气候控制对于作物生长和资源利用至关重要，但现有的强化学习方法存在训练效率低和对初始学习条件依赖高的问题。交互式RL结合人类输入可能解决这些问题，但在温室气候控制中的应用尚未被探索。

**方法:** 1. 开发三种针对温室气候控制的交互式RL算法：奖励塑造、策略塑造和控制共享。
2. 分析输入特性之间的矛盾及权衡，说明为何种植者的输入难以达到完美。
3. 提出一种基于神经网络的方法，以增强在有限输入情况下的交互式RL代理的鲁棒性。
4. 在模拟温室环境中全面评估这三种交互式RL算法的性能。

**结果:** 交互式RL结合不完美的种植者输入能够改善RL代理的表现。其中，影响动作选择的算法（如策略塑造和控制共享）表现更好，分别提升了8.4%和6.8%的利润。然而，操纵奖励函数的奖励塑造算法对不完美输入敏感，导致9.4%的利润下降。

**结论:** 在温室气候控制中应用交互式RL时，选择合适的机制以处理不完美输入至关重要。策略塑造和控制共享算法比奖励塑造更能适应不完美的输入。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Grower-in-the-Loop+Interactive+Reinforcement+Learning+for+Greenhouse+Climate+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23355&send_immediately=true&force_search=false)

**原文摘要:** Climate control is crucial for greenhouse production as it directly affects
crop growth and resource use. Reinforcement learning (RL) has received
increasing attention in this field, but still faces challenges, including
limited training efficiency and high reliance on initial learning conditions.
Interactive RL, which combines human (grower) input with the RL agent's
learning, offers a potential solution to overcome these challenges. However,
interactive RL has not yet been applied to greenhouse climate control and may
face challenges related to imperfect inputs. Therefore, this paper aims to
explore the possibility and performance of applying interactive RL with
imperfect inputs into greenhouse climate control, by: (1) developing three
representative interactive RL algorithms tailored for greenhouse climate
control (reward shaping, policy shaping and control sharing); (2) analyzing how
input characteristics are often contradicting, and how the trade-offs between
them make grower's inputs difficult to perfect; (3) proposing a neural
network-based approach to enhance the robustness of interactive RL agents under
limited input availability; (4) conducting a comprehensive evaluation of the
three interactive RL algorithms with imperfect inputs in a simulated greenhouse
environment. The demonstration shows that interactive RL incorporating
imperfect grower inputs has the potential to improve the performance of the RL
agent. RL algorithms that influence action selection, such as policy shaping
and control sharing, perform better when dealing with imperfect inputs,
achieving 8.4% and 6.8% improvement in profit, respectively. In contrast,
reward shaping, an algorithm that manipulates the reward function, is sensitive
to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the
importance of selecting an appropriate mechanism when incorporating imperfect
inputs.

</details>


### [112] [Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models](https://arxiv.org/abs/2505.23378)
*Roseline Polle, Agnes Norbury, Alexandra Livia Georgescu, Nicholas Cummins, Stefano Goria*

**主要类别:** cs.LG

**概要:** 通过将说话人自适应任务重新定义为元学习问题，探索了三种方法（基于集成的距离模型、原型网络和基于Transformer的序列模型）。使用预先训练好的语音嵌入，在一个大型纵向数据集上评估这些方法。所有元学习方法的表现都优于横断面模型和传统的混合效应模型，其中基于Transformer的方法表现最佳。


<details>
  <summary>更多</summary>
  
**动机:** 说话人自适应建模可以显著提高基于语音的健康监测应用的性能，但传统混合效应模型在实际生产环境中因计算成本高昂而不实用。因此需要一种更高效的方法来解决这一问题。

**方法:** 将任务重新定义为元学习问题，并探索了三种递增复杂性的方法：基于集成的距离模型、原型网络和基于Transformer的序列模型。使用预先训练好的语音嵌入进行评估。

**结果:** 所有测试的元学习方法均优于横断面模型和传统混合效应模型，其中基于Transformer的方法表现出最强的性能。

**结论:** 元学习方法在预测疲劳相关的健康症状方面具有优越性，特别是基于Transformer的方法最为有效。这为在语音健康监测中的实际应用提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta-Learning+Approaches+for+Speaker-Dependent+Voice+Fatigue+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23378，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23378&send_immediately=true&force_search=false)

**原文摘要:** Speaker-dependent modelling can substantially improve performance in
speech-based health monitoring applications. While mixed-effect models are
commonly used for such speaker adaptation, they require computationally
expensive retraining for each new observation, making them impractical in a
production environment. We reformulate this task as a meta-learning problem and
explore three approaches of increasing complexity: ensemble-based distance
models, prototypical networks, and transformer-based sequence models. Using
pre-trained speech embeddings, we evaluate these methods on a large
longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting
time since sleep from speech as a function of fatigue, a symptom commonly
associated with ill-health. Our results demonstrate that all meta-learning
approaches tested outperformed both cross-sectional and conventional
mixed-effects models, with a transformer-based method achieving the strongest
performance.

</details>


### [113] [Automated Modeling Method for Pathloss Model Discovery](https://arxiv.org/abs/2505.23383)
*Ahmad Anaqreh, Shih-Kai Chou, Mihael Mohorčič, Carolina Fortuna*

**主要类别:** cs.LG

**概要:** 提出了一种新的基于AI的路径损耗模型发现方法，该方法在保持可解释性的同时自动化模型公式化、评估和改进。通过两种技术（深度符号回归和柯尔莫哥洛夫-阿诺德网络）的对比实验表明，前者提供完全可解释性而后者在预测精度上表现更优，同时自动化方法相较于传统方法减少了75%的预测误差。


<details>
  <summary>更多</summary>
  
**动机:** 传统的传播行为建模方法依赖于统计技术，难以满足新一代无线通信系统对准确性和互操作性的需求。虽然AI技术被越来越多地采用，但大多数方法缺乏可解释性。因此，需要一种既能保证准确性又具有可解释性的新方法。

**方法:** 提出了一种新型路径损耗模型发现方法，包含两个技术分支：基于深度符号回归的方法（提供完全可解释性）和基于柯尔莫哥洛夫-阿诺德网络的方法（提供两级可解释性）。该方法自动化了模型的公式化、评估和改进过程。

**结果:** 实验结果表明，柯尔莫哥洛夫-阿诺德网络方法实现了接近1的R²值和最小预测误差，而深度符号回归方法生成了紧凑模型且具有中等精度。自动化方法相较于传统方法减少了高达75%的预测误差。

**结论:** 所提出的方法在保持可解释性的同时显著提高了路径损耗模型的发现效率，为下一代无线通信系统的建模提供了潜在的有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Modeling+Method+for+Pathloss+Model+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23383&send_immediately=true&force_search=false)

**原文摘要:** Modeling propagation is the cornerstone for designing and optimizing
next-generation wireless systems, with a particular emphasis on 5G and beyond
era. Traditional modeling methods have long relied on statistic-based
techniques to characterize propagation behavior across different environments.
With the expansion of wireless communication systems, there is a growing demand
for methods that guarantee the accuracy and interoperability of modeling.
Artificial intelligence (AI)-based techniques, in particular, are increasingly
being adopted to overcome this challenge, although the interpretability is not
assured with most of these methods. Inspired by recent advancements in AI, this
paper proposes a novel approach that accelerates the discovery of path loss
models while maintaining interpretability. The proposed method automates the
model formulation, evaluation, and refinement, facilitating model discovery. We
evaluate two techniques: one based on Deep Symbolic Regression, offering full
interpretability, and the second based on Kolmogorov-Arnold Networks, providing
two levels of interpretability. Both approaches are evaluated on two synthetic
and two real-world datasets. Our results show that Kolmogorov-Arnold Networks
achieve R^2 values close to 1 with minimal prediction error, while Deep
Symbolic Regression generates compact models with moderate accuracy. Moreover,
on the selected examples, we demonstrate that automated methods outperform
traditional methods, achieving up to 75% reduction in prediction errors,
offering accurate and explainable solutions with potential to increase the
efficiency of discovering next-generation path loss models.

</details>


### [114] [OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses](https://arxiv.org/abs/2505.23421)
*Zheming Zhang, Yan Jiang, Qingshan Li, Ai Han*

**主要类别:** cs.LG

**概要:** 在中国竞争激烈的生鲜电商市场中，前端仓库的库存管理是提升客户满意度和竞争优势的关键。本文提出了一种多任务OTPTO方法，通过联合优化商品选择和库存管理来最大化订单完成率，相比传统PTO方法，显著提高了订单完成率并缩小了与最优完成率的差距。


<details>
  <summary>更多</summary>
  
**动机:** 前端仓库在住宅区以确保及时配送生鲜商品，但其规模较小，需考虑容量限制来决定存储哪些商品及数量。传统预测-优化方法未能将预测与库存目标对齐，也未优先考虑消费者满意度。

**方法:** 提出了多任务OTPTO方法，联合优化商品选择和库存管理，包含：0-1混合整数规划模型OM1确定历史最优库存水平；产品选择模型PM1和库存模型PM2进行预测；通过后处理算法OM2进一步优化结果。

**结果:** 实验结果表明，OTPTO方法相较于PTO方法，订单完成率提升了4.34%（相对增加7.05%），与最优完成率的差距缩小了5.27%。

**结论:** OTPTO方法在生鲜电商平台前端仓库库存管理中表现出色，为该领域的未来研究提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OTPTO%3A+Joint+Product+Selection+and+Inventory+Optimization+in+Fresh+E-commerce+Front-End+Warehouses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23421，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23421&send_immediately=true&force_search=false)

**原文摘要:** In China's competitive fresh e-commerce market, optimizing operational
strategies, especially inventory management in front-end warehouses, is key to
enhance customer satisfaction and to gain a competitive edge. Front-end
warehouses are placed in residential areas to ensure the timely delivery of
fresh goods and are usually in small size. This brings the challenge of
deciding which goods to stock and in what quantities, taking into account
capacity constraints. To address this issue, traditional predict-then-optimize
(PTO) methods that predict sales and then decide on inventory often don't align
prediction with inventory goals, as well as fail to prioritize consumer
satisfaction. This paper proposes a multi-task
Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes
product selection and inventory management, aiming to increase consumer
satisfaction by maximizing the full order fulfillment rate. Our method employs
a 0-1 mixed integer programming model OM1 to determine historically optimal
inventory levels, and then uses a product selection model PM1 and the stocking
model PM2 for prediction. The combined results are further refined through a
post-processing algorithm OM2. Experimental results from JD.com's 7Fresh
platform demonstrate the robustness and significant advantages of our OTPTO
method. Compared to the PTO approach, our OTPTO method substantially enhances
the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and
narrows the gap to the optimal full order fulfillment rate by 5.27%. These
findings substantiate the efficacy of the OTPTO method in managing inventory at
front-end warehouses of fresh e-commerce platforms and provide valuable
insights for future research in this domain.

</details>


### [115] [On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers](https://arxiv.org/abs/2505.23427)
*Monika Gahalawat, Maneesh Bilalpur, Raul Fernandez Rojas, Jeffrey F. Cohn, Roland Goecke, Ramanathan Subramanian*

**主要类别:** cs.LG

**概要:** 研究头动模式作为抑郁程度评估的有效性和泛化性，证明其为高效生物标志，并在多个数据集上表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 抑郁症是一种影响全球数百万人的严重情绪障碍，尽管研究人员已经探索了多种言语和非言语行为线索来进行自动抑郁评估，但头部运动尚未得到充分关注。此外，通过单一数据集验证机器学习模型的常见做法可能限制模型的泛化能力。

**方法:** 利用基础头部运动单元（kinemes）进行抑郁程度估计，采用三个来自不同西方文化的抑郁数据集（德国：AVEC2013，澳大利亚：Blackdog，美国：Pitt），通过两种方法研究所得kineme模式的泛化能力：(i) 单个/多个数据集上的k折交叉验证，以及 (ii) 在其他数据集上的模型重用。使用经典机器学习方法评估分类和回归性能。

**结果:** 结果表明：(1) 头部运动模式是估计抑郁程度的有效生物标志，在各种数据集上的分类和回归任务中均表现出高度竞争力，包括在AVEC2013数据集上实现第二佳的平均绝对误差（MAE）。(2) 基于kineme的特征比 (a) 原始头部运动描述符用于二元严重程度分类，以及 (b) 其他视觉行为线索用于严重程度估计（回归）更具泛化能力。

**结论:** 头部运动模式是有效的抑郁程度评估工具，基于kineme的特征具有良好的泛化能力，优于原始头部运动描述和其他视觉行为线索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Validity+of+Head+Motion+Patterns+as+Generalisable+Depression+Biomarkers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23427&send_immediately=true&force_search=false)

**原文摘要:** Depression is a debilitating mood disorder negatively impacting millions
worldwide. While researchers have explored multiple verbal and non-verbal
behavioural cues for automated depression assessment, head motion has received
little attention thus far. Further, the common practice of validating machine
learning models via a single dataset can limit model generalisability. This
work examines the effectiveness and generalisability of models utilising
elementary head motion units, termed kinemes, for depression severity
estimation. Specifically, we consider three depression datasets from different
western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt
datasets) with varied contextual and recording settings to investigate the
generalisability of the derived kineme patterns via two methods: (i) k-fold
cross-validation over individual/multiple datasets, and (ii) model reuse on
other datasets. Evaluating classification and regression performance with
classical machine learning methods, our results show that: (1) head motion
patterns are efficient biomarkers for estimating depression severity, achieving
highly competitive performance for both classification and regression tasks on
a variety of datasets, including achieving the second best Mean Absolute Error
(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more
generalisable than (a) raw head motion descriptors for binary severity
classification, and (b) other visual behavioural cues for severity estimation
(regression).

</details>


### [116] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao, Ran Cheng, Xingyu Wu, Jibin Wu, Kay Chen Tan*

**主要类别:** cs.LG

**概要:** 尽管多样性在强化学习（RL）中起着关键作用，但其对大语言模型（LLM）推理能力的影响尚未得到充分探索。本文系统研究了RL训练中多样性对LLM推理的影响，并提出了一种新的多样性感知策略优化方法。实验表明，高性能模型的解多样性与一种新提出的衡量LLM推理潜力的指标Potential at k之间存在强正相关关系。基于这一发现，我们设计了一种token级别的多样性目标，并将其有选择地应用于正样本。该方法在R1-zero训练框架中实现了平均3.5%的改进，并生成了更多样化和稳健的解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 尽管多样性在强化学习中的重要性已被广泛认可，但其对大语言模型推理能力的具体影响尚未被深入探讨。因此，研究者希望了解多样性如何影响LLM的推理能力，并探索通过增强多样性来提升模型性能的方法。

**方法:** 研究者首先定义了一个新的指标Potential at k来量化LLM的推理潜力，并观察到解多样性与该指标之间的强正相关关系。基于这一发现，研究者设计了一种token级别的多样性目标，并将其转化为一个实际可行的目标函数。然后，他们将该目标函数有选择地应用于正样本，并将其整合到R1-zero训练框架中。

**结果:** 通过在四个数学推理基准上的评估，该方法实现了平均3.5%的性能提升。此外，模型生成的解决方案更加多样化和稳健。

**结论:** 多样性在强化学习训练中对提升LLM的推理能力具有重要作用。本文提出的多样性感知策略优化方法能够有效提高模型性能，并为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diversity-Aware+Policy+Optimization+for+Large+Language+Model+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23433，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23433&send_immediately=true&force_search=false)

**原文摘要:** The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [117] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
*Linyu Li, Zhi Jin, Yuanpeng He, Dongming Jin, Haoran Duan, Zhengwei Tao, Xuan Zhang, Jiandong Li*

**主要类别:** cs.LG

**概要:** 这篇论文重新审视了知识图谱补全中的正则化方法，提出了一种新的稀疏正则化方法（SPR），通过选择性地惩罚嵌入向量中重要的特征分量，忽略次要的或代表噪声的分量，从而有效突破模型性能上限。实验表明该方法优于其他正则化方法。


<details>
  <summary>更多</summary>
  
**动机:** 知识图谱补全对提升知识图谱质量至关重要，但现有方法大多未从更深层次利用正则化的优势，未能充分发挥其潜力。

**方法:** 1. 重新思考并研究正则化方法在知识图谱补全中的应用。
2. 提出一种新的稀疏正则化方法（SPR），将基于秩的选择性稀疏概念嵌入到正则化项中。
3. 核心思想：选择性惩罚嵌入向量中具有显著特征的分量，忽略贡献较小或仅表示噪声的分量。

**结果:** 广泛的对比实验表明，SPR正则化方法比其他正则化方法表现更好，并能够使知识图谱补全模型进一步突破性能上限。

**结论:** 精心设计的正则化不仅减轻过拟合和减少方差，还能使模型突破原始性能上限；提出的SPR方法是一种有效的改进策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Regularization+Methods+for+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23442&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [118] [Strategic Classification with Non-Linear Classifiers](https://arxiv.org/abs/2505.23443)
*Benyamin Trachtenberg, Nir Rosenfeld*

**主要类别:** cs.LG

**概要:** 在战略性分类中，研究了非线性分类器下战略性行为的表现形式及其对学习的影响。发现一旦环境变为战略性，通用近似器（如神经网络）不再具有通用性，并可能在无限制模型类上产生性能差距。


<details>
  <summary>更多</summary>
  
**动机:** 传统的监督学习方法虽然支持多种模型类别，但尚未深入研究非线性分类器下的战略性用户行为。本文希望扩展这一领域的研究范围。

**方法:** 采用自底向上的方法，分析非线性如何影响决策边界点、分类器表达能力以及模型类别的复杂性。通过实验展示战略性环境对通用近似器（如神经网络）的影响。

**结果:** 研究表明，在战略性环境中，通用近似器不再具有通用性，即使在无限制的模型类别中也可能出现性能差距。

**结论:** 非线性分类器下的战略性行为显著影响模型表现，未来需要进一步研究适用于战略性环境的模型设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strategic+Classification+with+Non-Linear+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23443&send_immediately=true&force_search=false)

**原文摘要:** In strategic classification, the standard supervised learning setting is
extended to support the notion of strategic user behavior in the form of costly
feature manipulations made in response to a classifier. While standard learning
supports a broad range of model classes, the study of strategic classification
has, so far, been dedicated mostly to linear classifiers. This work aims to
expand the horizon by exploring how strategic behavior manifests under
non-linear classifiers and what this implies for learning. We take a bottom-up
approach showing how non-linearity affects decision boundary points, classifier
expressivity, and model classes complexity. A key finding is that universal
approximators (e.g., neural nets) are no longer universal once the environment
is strategic. We demonstrate empirically how this can create performance gaps
even on an unrestricted model class.

</details>


### [119] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/abs/2505.23448)
*Pirzada Suhail, Rehna Afroz, Amit Sethi*

**主要类别:** cs.LG

**概要:** 提出了一种结合网络反演和分类器训练的新框架，用于同时解决OOD检测和不确定性估计问题。通过引入“垃圾”类并迭代训练、反演和排除过程，该方法可有效检测OOD样本并估计不确定性，且无需外部OOD数据集或事后校准技术。


<details>
  <summary>更多</summary>
  
**动机:** 在实际场景中，构建安全的机器学习系统需要有效的OOD检测和不确定性估计，特别是在不可避免出现意外输入的情况下。当前方法可能依赖外部数据集或校准技术，缺乏统一解决方案。

**方法:** 1. 扩展标准n类分类器为(n+1)类模型，新增“垃圾”类并用随机高斯噪声初始化。
2. 在每次训练后使用网络反演重建对应所有输出类别的输入图像，并将不连贯的样本归入垃圾类。
3. 通过迭代训练、反演和排除过程，使反演样本逐渐接近分布内数据。
4. 利用垃圾类检测OOD样本，并通过预测置信度估计不确定性。

**结果:** 实验结果表明，该方法能够有效检测OOD样本并提供可靠的不确定性估计，同时具有可扩展性和可解释性，且无需外部OOD数据集或事后校准技术。

**结论:** 所提出的框架为OOD检测和不确定性估计提供了统一且高效的解决方案，具有广泛的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Network+Inversion+for+Uncertainty-Aware+Out-of-Distribution+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23448，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23448&send_immediately=true&force_search=false)

**原文摘要:** Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [120] [Diffusion Guidance Is a Controllable Policy Improvement Operator](https://arxiv.org/abs/2505.23458)
*Kevin Frans, Seohong Park, Pieter Abbeel, Sergey Levine*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Guidance+Is+a+Controllable+Policy+Improvement+Operator，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23458&send_immediately=true&force_search=false)

**原文摘要:** At the core of reinforcement learning is the idea of learning beyond the
performance in the data. However, scaling such systems has proven notoriously
tricky. In contrast, techniques from generative modeling have proven remarkably
scalable and are simple to train. In this work, we combine these strengths, by
deriving a direct relation between policy improvement and guidance of diffusion
models. The resulting framework, CFGRL, is trained with the simplicity of
supervised learning, yet can further improve on the policies in the data. On
offline RL tasks, we observe a reliable trend -- increased guidance weighting
leads to increased performance. Of particular importance, CFGRL can operate
without explicitly learning a value function, allowing us to generalize simple
supervised methods (e.g., goal-conditioned behavioral cloning) to further
prioritize optimality, gaining performance for "free" across the board.

</details>


### [121] [On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment](https://arxiv.org/abs/2505.23459)
*Safwan Labbi, Paul Mangold, Daniil Tiapkin, Eric Moulines*

**主要类别:** cs.LG

**概要:** 在异构环境下，联邦强化学习策略梯度方法的收敛性是一个主要挑战。本文研究发现最优策略可能是非确定性的或时变的，并提出了FedPG算法和b-RS-FedPG方法，证明了它们的全局收敛性和线性加速性能。实验表明，这些方法优于联邦Q-learning。


<details>
  <summary>更多</summary>
  
**动机:** 确保联邦强化学习（FRL）中策略梯度方法在环境异构性下的收敛性是当前的主要挑战。此外，异构环境可能导致最优策略需要是非确定性或时间变化的，即使是在表格环境中。

**方法:** 1. 提出了联邦策略梯度（FedPG）算法，采用局部更新并证明了其在全球范围内的收敛性。
2. 通过Lojasiewicz条件分析FedPG在熵正则化和非正则化情况下的收敛性。
3. 引入了一种新的策略梯度方法b-RS-FedPG，使用基于softmax的参数化和适当的正则化方案。
4. 理论上展示了b-RS-FedPG向近似最优策略的明确收敛率。

**结果:** 理论上证明了FedPG和b-RS-FedPG在异构环境下的全局收敛性，并且具有与代理数量成线性关系的速度提升。实验结果表明，FedPG和b-RS-FedPG在异构环境下始终优于联邦Q-learning。

**结论:** 本文揭示了异构环境对策略优化的影响，提出并验证了FedPG和b-RS-FedPG的有效性，为联邦强化学习中的策略优化提供了新方法和理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Global+Convergence+Rates+for+Federated+Policy+Gradient+under+Heterogeneous+Environment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23459，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23459&send_immediately=true&force_search=false)

**原文摘要:** Ensuring convergence of policy gradient methods in federated reinforcement
learning (FRL) under environment heterogeneity remains a major challenge. In
this work, we first establish that heterogeneity, perhaps counter-intuitively,
can necessitate optimal policies to be non-deterministic or even time-varying,
even in tabular environments. Subsequently, we prove global convergence results
for federated policy gradient (FedPG) algorithms employing local updates, under
a {\L}ojasiewicz condition that holds only for each individual agent, in both
entropy-regularized and non-regularized scenarios. Crucially, our theoretical
analysis shows that FedPG attains linear speed-up with respect to the number of
agents, a property central to efficient federated learning. Leveraging insights
from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient
method that employs a carefully constructed softmax-inspired parameterization
coupled with an appropriate regularization scheme. We further demonstrate
explicit convergence rates for b-RS-FedPG toward near-optimal stationary
policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG
consistently outperform federated Q-learning on heterogeneous settings.

</details>


### [122] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/abs/2505.23470)
*Chenjie Li, Amir Gilad, Boris Glavic, Zhengjie Miao, Sudeepa Roy*

**主要类别:** cs.LG

**概要:** 本论文提出了一种通过少量标记样本来修复标签函数(LFs)的方法，以提高程序化弱监督中生成标签的质量。实验表明，该方法在非常小的标记数据集上就能有效提升LFs的质量。


<details>
  <summary>更多</summary>
  
**动机:** 程序化弱监督（PWS）通过结合用户提供的标签函数（LFs）的输出来减少标注数据所需的人力，但生成标签的质量直接依赖于LFs的准确性。因此，研究如何基于少量已标记样本来修复这些LFs成为重要问题。

**方法:** 作者开发了新的技术来修复一组LFs，通过最小地改变它们在已标记样本上的结果，确保：1. 每个已标记数据点的正确标签有足够的证据；2. 每个修复后的LF具有足够高的准确性。同时，将LFs建模为条件规则，从而可以有选择性地更改其对某些输入的输出。

**结果:** 实验结果表明，该系统能够在非常小的标记数据集上显著提高LFs的质量。

**结论:** 通过少量标记样例修复LFs的方法能够有效提升程序化弱监督中标签的质量，且这种方法在实际应用中表现出良好的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Refining+Labeling+Functions+with+Limited+Labeled+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23470，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23470&send_immediately=true&force_search=false)

**原文摘要:** Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [123] [SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training](https://arxiv.org/abs/2505.23489)
*Ildus Sadrtdinov, Ivan Klimov, Ekaterina Lobacheva, Dmitry Vetrov*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SGD+as+Free+Energy+Minimization%3A+A+Thermodynamic+View+on+Neural+Network+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23489，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23489&send_immediately=true&force_search=false)

**原文摘要:** We present a thermodynamic interpretation of the stationary behavior of
stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural
network training. We show that SGD implicitly minimizes a free energy function
$F=U-TS$, balancing training loss $U$ and the entropy of the weights
distribution $S$, with temperature $T$ determined by the LR. This perspective
offers a new lens on why high LRs prevent training from converging to the loss
minima and how different LRs lead to stabilization at different loss levels. We
empirically validate the free energy framework on both underparameterized (UP)
and overparameterized (OP) models. UP models consistently follow free energy
minimization, with temperature increasing monotonically with LR, while for OP
models, the temperature effectively drops to zero at low LRs, causing SGD to
minimize the loss directly and converge to an optimum. We attribute this
mismatch to differences in the signal-to-noise ratio of stochastic gradients
near optima, supported by both a toy example and neural network experiments.

</details>


### [124] [Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty](https://arxiv.org/abs/2505.23506)
*Sebastián Jiménez, Mira Jürgens, Willem Waegeman*

**主要类别:** cs.LG

**概要:** 在近年来的各种基于二阶分布的监督学习方法中，尽管尝试解耦了 aleatoric 和 epistemic 不确定性，但这些方法未能充分捕捉到模型偏差等关键的 epistemic 不确定性成分。通过更细致的不确定性分类和实验验证，研究发现当前方法通常低估了 epistemic 不确定性，并且高模型偏差会导致误导性的低不确定性估计。因此，只有全面表示所有相关 epistemic 不确定性来源，才能实现有意义的 aleatoric 估计。


<details>
  <summary>更多</summary>
  
**动机:** 现有的解耦 aleatoric 和 epistemic 不确定性的方法未能充分考虑 epistemic 不确定性中的模型偏差等重要组成部分。

**方法:** 提出了一种更精细的 epistemic 不确定性分类方法，并结合经典的偏差-方差分解分析预测误差的不同部分。采用基于模拟的评估协议，涵盖过程性和数据驱动的不确定性成分，以验证现有方法的表现。

**结果:** 研究表明，高模型偏差会导致对 epistemic 不确定性的误导性低估，并且常见的二阶不确定性量化方法会将偏差引起的误差误认为 aleatoric 不确定性。

**结论:** 为了获得有意义的 aleatoric 不确定性估计，必须正确表示所有相关的 epistemic 不确定性来源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+Machine+Learning+Models+Fail+to+Fully+Capture+Epistemic+Uncertainty，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23506&send_immediately=true&force_search=false)

**原文摘要:** In recent years various supervised learning methods that disentangle
aleatoric and epistemic uncertainty based on second-order distributions have
been proposed. We argue that these methods fail to capture critical components
of epistemic uncertainty, particularly due to the often-neglected component of
model bias. To show this, we make use of a more fine-grained taxonomy of
epistemic uncertainty sources in machine learning models, and analyse how the
classical bias-variance decomposition of the expected prediction error can be
decomposed into different parts reflecting these uncertainties. By using a
simulation-based evaluation protocol which encompasses epistemic uncertainty
due to both procedural- and data-driven uncertainty components, we illustrate
that current methods rarely capture the full spectrum of epistemic uncertainty.
Through theoretical insights and synthetic experiments, we show that high model
bias can lead to misleadingly low estimates of epistemic uncertainty, and
common second-order uncertainty quantification methods systematically blur
bias-induced errors into aleatoric estimates, thereby underrepresenting
epistemic uncertainty. Our findings underscore that meaningful aleatoric
estimates are feasible only if all relevant sources of epistemic uncertainty
are properly represented.

</details>


### [125] [AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity](https://arxiv.org/abs/2505.23520)
*Yu Zhang, Dong Guo, Fang Wu, Guoliang Zhu, Dian Ding, Yiming Zhang*

**主要类别:** cs.LG

**概要:** 在大规模语言模型（LLMs）中，扩展上下文长度的预填充阶段面临巨大的计算挑战，主要由于自注意力机制的二次复杂度。现有的方法通常采用动态模式匹配和块稀疏低级实现，但这些方法依赖局部信息进行模式识别，无法捕捉全局上下文，并且块的粗粒度导致持续的内部稀疏性，从而影响准确性和效率。为了解决这些问题，本文提出了AnchorAttention，一种差异感知的动态稀疏注意力机制，它能够在更细的条纹粒度上有效识别关键注意力区域，同时适应全局上下文信息，达到更高的速度和准确性。AnchorAttention包含三个关键组件：基于模式的锚点计算、差异感知的条纹稀疏性识别和细粒度稀疏计算。与之前最先进的方法相比，在文本长度为128k的情况下，AnchorAttention实现了1.44倍的速度提升，同时保持更高的召回率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的处理大规模语言模型中长上下文的方法存在局限性，包括依赖局部信息进行模式识别，无法捕捉全局上下文，以及块的粗粒度导致持续的内部稀疏性，从而影响准确性和效率。因此需要一种新的方法来克服这些限制。

**方法:** 提出了一种名为AnchorAttention的差异感知动态稀疏注意力机制。该机制包含三个关键组件：1）基于模式的锚点计算，利用所有输入中的共性快速计算一组接近最大值的分数作为锚点；2）差异感知的条纹稀疏性识别，通过与锚点进行差异感知比较，快速获得显著区域的离散坐标；3）细粒度稀疏计算，用同时加载离散KV位置的方式代替传统的连续KV块加载方法，以最大化稀疏率并保留完整的硬件计算潜力。

**结果:** AnchorAttention在相同的召回水平下实现了更高的稀疏率，显著减少了计算时间。与之前的最先进方法相比，在文本长度为128k时，AnchorAttention实现了1.44倍的速度提升，同时保持更高的召回率。

**结论:** AnchorAttention作为一种差异感知的动态稀疏注意力机制，能够有效识别关键注意力区域，适应全局上下文信息，提高了大规模语言模型的计算效率和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AnchorAttention%3A+Difference-Aware+Sparse+Attention+with+Stripe+Granularity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23520，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23520&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) with extended context lengths face significant
computational challenges during the pre-filling phase, primarily due to the
quadratic complexity of self-attention. Existing methods typically employ
dynamic pattern matching and block-sparse low-level implementations. However,
their reliance on local information for pattern identification fails to capture
global contexts, and the coarse granularity of blocks leads to persistent
internal sparsity, resulting in suboptimal accuracy and efficiency. To address
these limitations, we propose \textbf{AnchorAttention}, a difference-aware,
dynamic sparse attention mechanism that efficiently identifies critical
attention regions at a finer stripe granularity while adapting to global
contextual information, achieving superior speed and accuracy. AnchorAttention
comprises three key components: (1) \textbf{Pattern-based Anchor Computation},
leveraging the commonalities present across all inputs to rapidly compute a set
of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe
Sparsity Identification}, performing difference-aware comparisons with the
anchor to quickly obtain discrete coordinates of significant regions in a
stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation},
replacing the traditional contiguous KV block loading approach with
simultaneous discrete KV position loading to maximize sparsity rates while
preserving full hardware computational potential. With its finer-grained
sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at
the same recall level, significantly reducing computation time. Compared to
previous state-of-the-art methods, at a text length of 128k, it achieves a
speedup of 1.44$\times$ while maintaining higher recall rates.

</details>


### [126] [Accelerating AllReduce with a Persistent Straggler](https://arxiv.org/abs/2505.23523)
*Arjun Devraj, Eric Ding, Abhishek Vijaya Kumar, Robert Kleinberg, Rachee Singh*

**主要类别:** cs.LG

**概要:** 在分布式机器学习任务中，数据和张量并行性依赖于AllReduce集合操作来同步梯度或激活。然而，由于慢节点的存在，传统的AllReduce算法可能会被延迟。为了解决这个问题，我们提出了StragglAR：一种在存在慢节点的情况下加速分布式训练和推理的AllReduce算法。StragglAR在慢节点引起延迟期间，在剩余的GPU上执行ReduceScatter操作，并在慢节点到达同步屏障后执行一种新的集合算法以完成AllReduce操作。对于具有持续慢节点的大规模GPU集群，StragglAR相较于流行的带宽高效型AllReduce算法（如Ring）实现了2倍的理论加速。在8-GPU服务器上，我们的StragglAR实现比最先进的AllReduce算法速度快22%。


<details>
  <summary>更多</summary>
  
**动机:** 在分布式机器学习的工作负载中，数据和张量并行性用于训练和推理，这些都依赖于AllReduce集体操作来同步梯度或激活。然而，传统的AllReduce算法是块同步的，可能因持续存在的慢节点而延迟。

**方法:** 提出了一种名为StragglAR的新AllReduce算法。该算法在慢节点引起延迟时，在剩余的GPU之间执行ReduceScatter操作，随后当慢节点到达同步屏障时执行一种新的集合算法以完成AllReduce操作。

**结果:** 对于存在持续慢节点的大规模GPU集群，StragglAR相比流行的带宽高效型AllReduce算法（如Ring）实现了2倍的理论加速。实验结果表明，在8-GPU服务器上，StragglAR实现比现有最先进算法快22%。

**结论:** StragglAR是一种有效的AllReduce算法，能够在存在持续慢节点的情况下加速分布式训练和推理过程，特别是在大规模GPU集群中表现突出。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+AllReduce+with+a+Persistent+Straggler，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23523，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23523&send_immediately=true&force_search=false)

**原文摘要:** Distributed machine learning workloads use data and tensor parallelism for
training and inference, both of which rely on the AllReduce collective to
synchronize gradients or activations. However, bulk-synchronous AllReduce
algorithms can be delayed by a persistent straggler that is slower to reach the
synchronization barrier required to begin the collective. To address this
challenge, we propose StragglAR: an AllReduce algorithm that accelerates
distributed training and inference in the presence of persistent stragglers.
StragglAR implements a ReduceScatter among the remaining GPUs during the
straggler-induced delay, and then executes a novel collective algorithm to
complete the AllReduce once the straggler reaches the synchronization barrier.
StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient
AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent
stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%
speedup over state-of-the-art AllReduce algorithms.

</details>


### [127] [Normalizing Flows are Capable Models for RL](https://arxiv.org/abs/2505.23527)
*Raj Ghugare, Benjamin Eysenbach*

**主要类别:** cs.LG

**概要:** 本论文提出了一种单一的正则流（NF）架构，可以无缝集成到强化学习算法中，作为策略、Q函数和占用度量，简化算法并提高性能。


<details>
  <summary>更多</summary>
  
**动机:** 现代强化学习算法依赖强大的概率模型，但这些模型通常计算复杂或需要离散表示。相比之下，正则流（NFs）不需要解微分方程或自回归结构，然而其在强化学习中的潜力尚未得到充分探索。

**方法:** 通过结合最近关于正则流的研究，提出了一个单一的NF架构，该架构可以无缝地整合到强化学习算法中，用作策略、Q函数和占用度量。

**结果:** 所提出的方法简化了算法，并在模仿学习、离线强化学习、目标条件强化学习和无监督强化学习方面实现了更高的性能。

**结论:** 正则流具备足够的表达能力，并能有效提升多种强化学习任务的性能，为未来研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Normalizing+Flows+are+Capable+Models+for+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23527，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23527&send_immediately=true&force_search=false)

**原文摘要:** Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [128] [Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images](https://arxiv.org/abs/2505.23528)
*Maria Eleftheria Vlontzou, Maria Athanasiou, Christos Davatzikos, Konstantina S. Nikita*

**主要类别:** cs.LG

**概要:** 本研究对使用神经影像特征诊断轻度认知障碍 (MCI) 和阿尔茨海默病 (AD) 的机器学习模型进行了全面的公平性分析。研究发现年龄和种族存在偏差，而性别则无显著偏差。不同的偏差缓解策略在不同敏感属性和子问题上表现各异。提出了一种新的综合指标来衡量公平性和性能之间的权衡。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习模型在医疗诊断中的应用日益广泛，但其公平性问题尚未得到充分研究。特别是基于MRI衍生的神经影像特征诊断MCI和AD时，可能存在与年龄、种族和性别相关的偏差，这需要进行深入分析以确保模型的可靠性。

**方法:** 1. 对多队列数据集进行全面的公平性分析，评估与年龄、种族和性别相关的偏差。
2. 考察代理特征对这些敏感属性的影响。
3. 评估各种公平性定义和度量在识别偏差方面的可靠性。
4. 比较常用的预处理、处理中和后处理偏差缓解策略。
5. 提出一种新的复合度量，结合F1分数和平等机会比率，用于量化公平性和性能之间的权衡。

**结果:** - 发现了与年龄和种族相关的偏差，而性别偏差不显著。
- 不同的偏差缓解策略在不同敏感属性和子问题上表现出不同程度的改进。
- 针对种族和性别，拒绝选项分类分别提高了平等机会比率46%和57%，并在MCI与AD子问题中取得了0.75和0.80的调和平均分。
- 针对年龄，在同一子问题中，对抗去偏方法实现了最高的平等机会比率改进（40%），调和平均分为0.69。

**结论:** 研究揭示了机器学习模型在MCI和AD诊断中与年龄和种族相关的偏差，并表明不同偏差缓解策略的有效性因敏感属性和子问题而异。提出的新型复合度量有助于更好地权衡公平性和性能，适用于医疗诊断应用。此外，研究还探讨了与人口统计学特征相关的AD神经病理学和风险因素变化如何影响模型公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+assessment+of+fairness+definitions+and+bias+mitigation+strategies+in+machine+learning-based+diagnosis+of+Alzheimer%27s+disease+from+MR+images，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23528，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23528&send_immediately=true&force_search=false)

**原文摘要:** The present study performs a comprehensive fairness analysis of machine
learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and
Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases
associated with age, race, and gender in a multi-cohort dataset, as well as the
influence of proxy features encoding these sensitive attributes, are
investigated. The reliability of various fairness definitions and metrics in
the identification of such biases is also assessed. Based on the most
appropriate fairness measures, a comparative analysis of widely used
pre-processing, in-processing, and post-processing bias mitigation strategies
is performed. Moreover, a novel composite measure is introduced to quantify the
trade-off between fairness and performance by considering the F1-score and the
equalized odds ratio, making it appropriate for medical diagnostic
applications. The obtained results reveal the existence of biases related to
age and race, while no significant gender bias is observed. The deployed
mitigation strategies yield varying improvements in terms of fairness across
the different sensitive attributes and studied subproblems. For race and
gender, Reject Option Classification improves equalized odds by 46% and 57%,
respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI
versus AD subproblem, whereas for age, in the same subproblem, adversarial
debiasing yields the highest equalized odds improvement of 40% with a harmonic
mean score of 0.69. Insights are provided into how variations in AD
neuropathology and risk factors, associated with demographic characteristics,
influence model fairness.

</details>


### [129] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
*Giorgos Iacovides, Wuyang Zhou, Chao Li, Qibin Zhao, Danilo Mandic*

**主要类别:** cs.LG

**概要:** tnLLM是一个结合领域信息和大语言模型推理能力的张量网络结构搜索框架，能显著减少函数评估次数并提供可解释的结果。


<details>
  <summary>更多</summary>
  
**动机:** 当前最优的张量网络结构搜索算法计算成本高、缺乏对领域信息的利用且透明度不足。

**方法:** 提出tnLLM框架，通过领域感知提示管道指导大语言模型根据张量模式间的实际关系预测合适的张量网络结构，并生成领域相关的解释。

**结果:** 实验表明tnLLM相比现有算法在显著减少函数评估次数的情况下达到相当的目标函数值；同时，利用LLM启用的领域信息可为基于采样的方法提供良好初始化以加速收敛。

**结论:** tnLLM框架有效降低了张量网络结构搜索的计算负担，提升了透明度和性能，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Domain-Aware+Tensor+Network+Structure+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23537，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23537&send_immediately=true&force_search=false)

**原文摘要:** Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [130] [Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis](https://arxiv.org/abs/2505.23552)
*Alex Adams*

**主要类别:** cs.LG

**概要:** This paper compares the performance of Moore-Penrose pseudoinverse and gradient descent for linear regression.


<details>
  <summary>更多</summary>
  
**动机:** To provide guidance on choosing between direct and iterative methods for solving linear regression problems.

**方法:** Theoretical review, computational complexity analysis, and empirical evaluation on synthetic and real-world datasets.

**结果:** Each method excels under different conditions regarding computational time, numerical stability, and predictive accuracy.

**结论:** Researchers and practitioners can make informed decisions based on the characteristics of their specific tasks when selecting a method.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparing+the+Moore-Penrose+Pseudoinverse+and+Gradient+Descent+for+Solving+Linear+Regression+Problems%3A+A+Performance+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23552&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the comparative performance of two fundamental
approaches to solving linear regression problems: the closed-form Moore-Penrose
pseudoinverse and the iterative gradient descent method. Linear regression is a
cornerstone of predictive modeling, and the choice of solver can significantly
impact efficiency and accuracy. I review and discuss the theoretical
underpinnings of both methods, analyze their computational complexity, and
evaluate their empirical behavior on synthetic datasets with controlled
characteristics, as well as on established real-world datasets. My results
delineate the conditions under which each method excels in terms of
computational time, numerical stability, and predictive accuracy. This work
aims to provide practical guidance for researchers and practitioners in machine
learning when selecting between direct, exact solutions and iterative,
approximate solutions for linear regression tasks.

</details>


### [131] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2505.23555)
*Yanzhao Hou, Jiaxiang Geng, Boyu Li, Xiaofeng Tao, Juncheng Wang, Xiaodong Xu, Bing Luo*

**主要类别:** cs.LG

**概要:** 提出了一种自适应的联邦LoRA策略，通过独立客户端采样和带宽分配方案优化训练效率，减少收敛时间。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦LoRA方法在处理系统和数据异构性时，未能充分考虑理论和实际影响，导致整体训练效率（特别是实际时间）无法达到最优。

**方法:** 1. 推导了新的联邦LoRA收敛边界，支持任意独立客户端采样且无需严格梯度假设。
2. 引入自适应带宽分配方案以应对异构客户端资源和系统带宽限制。
3. 基于推导理论，解决非凸优化问题以确定LoRA素描比率和采样概率，目标是最小化墙钟收敛时间。
4. 开发高效低复杂度算法来近似解决方案。

**结果:** 广泛的实验表明，所提出的方法在各种模型和数据集上显著减少了与最先进方法相比的墙钟训练时间。

**结论:** 提出的自适应联邦LoRA策略能够有效降低联邦微调的墙钟收敛时间，并在计算和通信异构性下表现优异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Federated+LoRA+in+Heterogeneous+Wireless+Networks+with+Independent+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23555，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23555&send_immediately=true&force_search=false)

**原文摘要:** Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [132] [DRO: A Python Library for Distributionally Robust Optimization in Machine Learning](https://arxiv.org/abs/2505.23565)
*Jiashuo Liu, Tianyu Wang, Henry Lam, Hongseok Namkoong, Jose Blanchet*

**主要类别:** cs.LG

**概要:** 本论文介绍了一个名为dro的开源Python库，专注于分布鲁棒优化（DRO）在回归和分类问题中的应用。该库实现了14种DRO公式和9种骨干模型，支持79种不同的DRO方法，并与scikit-learn和PyTorch兼容。通过矢量化和优化近似技术，dro在大规模数据集上比基线实现快10到1000倍。


<details>
  <summary>更多</summary>
  
**动机:** 为了提供一个统一且高效的工具来处理分布鲁棒优化问题，特别是在回归和分类任务中。

**方法:** 开发了一个名为dro的开源库，其中包含了14种DRO公式和9种骨干模型，从而支持79种不同的DRO方法。此外，该库还利用了矢量化和优化近似技术以提高运行效率。

**结果:** dro库在大规模数据集上的运行速度比现有基线实现快10到1000倍，同时兼容主流机器学习框架如scikit-learn和PyTorch。

**结论:** dro是一个高效、灵活且易于使用的开源库，为分布鲁棒优化提供了显著的性能提升和广泛的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DRO%3A+A+Python+Library+for+Distributionally+Robust+Optimization+in+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23565，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23565&send_immediately=true&force_search=false)

**原文摘要:** We introduce dro, an open-source Python library for distributionally robust
optimization (DRO) for regression and classification problems. The library
implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO
methods. Furthermore, dro is compatible with both scikit-learn and PyTorch.
Through vectorization and optimization approximation techniques, dro reduces
runtime by 10x to over 1000x compared to baseline implementations on
large-scale datasets. Comprehensive documentation is available at
https://python-dro.org.

</details>


### [133] [Maximum Likelihood Learning of Latent Dynamics Without Reconstruction](https://arxiv.org/abs/2505.23569)
*Samo Hromadka, Kai Biegun, Lior Fox, James Heald, Maneesh Sahani*

**主要类别:** cs.LG

**概要:** 提出了一种新的无监督学习方法RP-GSSM，用于具有潜在动态结构的时间序列数据。该模型结合了对比方法的直观性和概率生成模型的灵活性，能够通过最大似然估计进行学习，并且不需要从潜在变量到观测值的显式网络映射。RP-GSSM在处理非线性随机动力学学习等问题时表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 当前时间序列数据分析中，许多方法要么依赖于对比方法，要么依赖于生成模型，但这些方法通常需要额外的正则化或辅助损失。因此，研究者希望开发一种既灵活又无需复杂调整的概率模型来捕捉时间序列中的潜在动态结构。

**方法:** 引入了识别参数化的高斯状态空间模型（RP-GSSM）。该模型是一种概率模型，通过最大似然估计学习马尔可夫高斯潜在变量，解释不同时间步长之间的统计依赖关系。它结合了对比方法的直观性和生成模型的灵活性，同时避免了显式的网络映射需求。模型使用精确推断，并通过任意非线性神经网络链接观察和潜在变量。

**结果:** RP-GSSM在包括从视频中学习非线性随机动力学的问题上表现优于其他方法，无论是否存在背景干扰因素。这表明该模型能够有效地学习任务相关的潜在变量，而无需自定义正则化、辅助损失或优化器调度。

**结论:** RP-GSSM为下游应用提供了一个有用的基模型，特别是在处理具有潜在动态结构的时间序列数据时。其独特的特性使其成为一种有潜力的工具，适用于多种实际问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Maximum+Likelihood+Learning+of+Latent+Dynamics+Without+Reconstruction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23569，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23569&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel unsupervised learning method for time series data with
latent dynamical structure: the recognition-parametrized Gaussian state space
model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian
Gaussian latents explaining statistical dependence between observations at
different time steps, combining the intuition of contrastive methods with the
flexible tools of probabilistic generative models. Unlike contrastive
approaches, the RP-GSSM is a valid probabilistic model learned via maximum
likelihood. Unlike generative approaches, the RP-GSSM has no need for an
explicit network mapping from latents to observations, allowing it to focus
model capacity on inference of latents. The model is both tractable and
expressive: it admits exact inference thanks to its jointly Gaussian latent
prior, while maintaining expressivity with an arbitrarily nonlinear neural
network link between observations and latents. These qualities allow the
RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary
losses, or optimizer scheduling. We show how this approach outperforms
alternatives on problems that include learning nonlinear stochastic dynamics
from video, with or without background distractors. Our results position the
RP-GSSM as a useful foundation model for a variety of downstream applications.

</details>


### [134] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour, Andrew Magnuson, Purav Gupta, Shihao Ma, Jack Naimer, Arnav Shah, Haonan Duan, Omar Ibrahim, Hani Goodarzi, Chris J. Maddison, Bo Wang*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为BioReason的新架构，首次将DNA基础模型与大型语言模型（LLM）深度结合，通过监督微调和强化学习提升多步骤推理能力，在生物推理基准测试中表现出显著性能提升，并能生成可解释的、逐步的生物学推导。


<details>
  <summary>更多</summary>
  
**动机:** 当前的DNA基础模型虽然在序列表示方面表现出色，但在多步骤推理方面存在困难，并且缺乏透明、直观的生物学解释。为了克服这一限制，促进更深入的生物学理解，提出了BioReason。

**方法:** BioReason是一种创新架构，它将DNA基础模型与大型语言模型（LLM）相结合，使LLM能够直接处理和推理基因组信息。通过监督微调和针对性强化学习，系统被引导生成逻辑上连贯、生物学上有意义的推论。

**结果:** 在基于KEGG的疾病通路预测等生物推理基准测试中，准确率从88%提高到97%，在变异效应预测等任务中平均性能提升了15%。此外，BioReason能够对未见过的生物实体进行推理，并通过可解释的逐步生物痕迹阐明决策过程。

**结论:** BioReason提供了一种变革性的AI方法，用于从基因组数据中生成更深的机制洞察和可测试的假设，推动了生物学领域的发展。所有相关数据、代码和检查点均已公开发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BioReason%3A+Incentivizing+Multimodal+Biological+Reasoning+within+a+DNA-LLM+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23579&send_immediately=true&force_search=false)

**原文摘要:** Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [135] [Improving Time Series Forecasting via Instance-aware Post-hoc Revision](https://arxiv.org/abs/2505.23583)
*Zhiding Liu, Mingyue Cheng, Guanhao Zhao, Jiqian Yang, Qi Liu, Enhong Chen*

**主要类别:** cs.LG

**概要:** 时间序列预测在现实世界的许多应用中起着至关重要的作用。尽管最近的方法通过结合先进的归纳偏差和训练策略，取得了显著的准确性，但我们观察到实例级的变化仍然是一个重大挑战。为了解决这个问题，我们提出了一个模型无关的框架PIR（Post-forecasting Identification and Revision），它首先通过估计精度来识别有偏差的预测实例，然后利用上下文信息从局部和全局的角度进行修正。广泛的实验证明了PIR的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管现有的时间序列预测方法在整体性能上表现良好，但实例级别的变化（如分布偏移、缺失数据和长尾模式）仍然导致特定实例的预测效果不佳。因此，需要一种新的方法来解决这些实例级的问题，从而提高预测的可靠性。

**方法:** 提出了一种与模型无关的框架PIR（Post-forecasting Identification and Revision）。该框架首先通过估计预测实例的精度来识别偏差较大的预测实例；然后，使用上下文信息（包括协变量和历史时间序列数据）从局部和全局角度对预测结果进行修正。这种方法是在后处理阶段进行的。

**结果:** 在真实世界的数据集上进行的广泛实验表明，PIR有效地减少了实例级别的误差，并显著提高了时间序列预测的可靠性。

**结论:** 提出的PIR框架能够有效应对时间序列预测中的实例级变化问题，显著提升了预测的准确性和可靠性。这为未来的预测模型设计提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Time+Series+Forecasting+via+Instance-aware+Post-hoc+Revision，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23583，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23583&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting plays a vital role in various real-world applications
and has attracted significant attention in recent decades. While recent methods
have achieved remarkable accuracy by incorporating advanced inductive biases
and training strategies, we observe that instance-level variations remain a
significant challenge. These variations--stemming from distribution shifts,
missing data, and long-tail patterns--often lead to suboptimal forecasts for
specific instances, even when overall performance appears strong. To address
this issue, we propose a model-agnostic framework, PIR, designed to enhance
forecasting performance through Post-forecasting Identification and Revision.
Specifically, PIR first identifies biased forecasting instances by estimating
their accuracy. Based on this, the framework revises the forecasts using
contextual information, including covariates and historical time series, from
both local and global perspectives in a post-processing fashion. Extensive
experiments on real-world datasets with mainstream forecasting models
demonstrate that PIR effectively mitigates instance-level errors and
significantly improves forecasting reliability.

</details>


### [136] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
*Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, Furu Wei*

**主要类别:** cs.LG

**概要:** 提出了一种新的强化学习算法OPO，强调精确的on-policy训练和最优奖励基线，以解决训练不稳定和计算效率低的问题。在数学推理基准上的评估表明，OPO具有优越的性能和训练稳定性，且不需要额外的模型或正则化项。


<details>
  <summary>更多</summary>
  
**动机:** 当前的强化学习算法由于宽松的on-policy约束而存在训练不稳定性，以及由于辅助模型导致的计算效率低下。

**方法:** 提出了名为OPO的新算法，该算法注重精确的on-policy训练，这在实证上稳定了训练过程并增强了探索能力。此外，OPO引入了理论上最小化梯度方差的最优奖励基线。

**结果:** 在数学推理基准上的结果表明，OPO表现出优越的性能和训练稳定性，同时实现了较低的策略转移和较高的输出熵，鼓励更多样化和较少重复的响应。

**结论:** OPO为大型语言模型对齐和推理任务中的稳定且有效的强化学习提供了一个有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On-Policy+RL+with+Optimal+Reward+Baseline，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23585&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [137] [Accelerated Training of Federated Learning via Second-Order Methods](https://arxiv.org/abs/2505.23588)
*Mrinmay Sen, Sidhant R Nair, C Krishna Mohan*

**主要类别:** cs.LG

**概要:** 本文探讨了联邦学习(FL)中的二阶优化方法，旨在解决全局模型达到最佳性能所需通信轮次过多和收敛速度慢的关键问题。尽管现有研究主要集中在统计与设备标签异质性、隐私和安全等一阶FL方法的挑战上，但对慢速模型训练的关注较少。本文对加速训练过程的各种利用二阶优化的FL方法进行了全面分类，并根据收敛速度、计算成本、内存使用、传输开销以及全局模型的泛化能力对其性能进行了比较。研究表明，通过二阶优化引入Hessian曲率在FL中具有潜力，并指出了关键挑战，如有效利用Hessian及其逆矩阵。这项工作为未来开发可扩展且高效的联邦优化方法奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的一阶方法存在收敛速度慢和通信轮次过多的问题，特别是在数据高度异质的情况下。这些问题导致了较高的通信成本和训练效率低下，因此需要探索更高效的优化方法来改进全局模型的训练过程。

**方法:** 本文对现有的二阶优化方法在联邦学习中的应用进行了深入分析，提供了这些方法的分类，并从多个维度（包括收敛速度、计算成本、内存使用、传输开销和模型泛化能力）对它们进行了比较。此外，还探讨了如何有效利用Hessian矩阵及其逆矩阵以提升优化效果。

**结果:** 研究表明，将二阶优化方法中的Hessian曲率引入联邦学习可以显著提高训练效率，减少通信轮次和成本。然而，有效利用Hessian矩阵及其逆矩阵仍然是一个关键挑战。

**结论:** 二阶优化方法在联邦学习中有很大的潜力，能够改善全局模型的训练效率和性能。未来的研究应着重于开发更加可扩展和高效的联邦优化方法，以进一步应对当前面临的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerated+Training+of+Federated+Learning+via+Second-Order+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23588，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23588&send_immediately=true&force_search=false)

**原文摘要:** This paper explores second-order optimization methods in Federated Learning
(FL), addressing the critical challenges of slow convergence and the excessive
communication rounds required to achieve optimal performance from the global
model. While existing surveys in FL primarily focus on challenges related to
statistical and device label heterogeneity, as well as privacy and security
concerns in first-order FL methods, less attention has been given to the issue
of slow model training. This slow training often leads to the need for
excessive communication rounds or increased communication costs, particularly
when data across clients are highly heterogeneous. In this paper, we examine
various FL methods that leverage second-order optimization to accelerate the
training process. We provide a comprehensive categorization of state-of-the-art
second-order FL methods and compare their performance based on convergence
speed, computational cost, memory usage, transmission overhead, and
generalization of the global model. Our findings show the potential of
incorporating Hessian curvature through second-order optimization into FL and
highlight key challenges, such as the efficient utilization of Hessian and its
inverse in FL. This work lays the groundwork for future research aimed at
developing scalable and efficient federated optimization methods for improving
the training of the global model in FL.

</details>


### [138] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/abs/2505.23593)
*Nikita Agrawal, Simon Mertel, Ruben Mayer*

**主要类别:** cs.LG

**概要:** 这篇论文探讨了在联邦学习中使用黑盒模型进行后训练的问题，强调其与联邦核心原则的矛盾，并分析了开放性对联邦学习的影响。


<details>
  <summary>更多</summary>
  
**动机:** 研究者关注到当前联邦学习领域中采用黑盒模型进行后训练的方法，虽然在集中式后训练中取得成功，但在联邦环境中可能引发隐私和自治等问题。因此，需要重新审视这些方法是否适合联邦学习的核心原则。

**方法:** 作者通过批判性分析黑盒模型在联邦后训练中的应用，讨论其与联邦学习核心原则（如数据隐私和自治）之间的矛盾，并从多个方面阐述开放性对联邦学习的意义和影响。

**结果:** 揭示了黑盒模型在联邦学习中的局限性，并指出其与联邦学习核心原则相悖。同时，提出了开放性对于改进联邦学习框架的重要性。

**结论:** 作者建议在联邦学习中应更加重视模型的透明度和开放性，以确保符合数据隐私保护和用户自治的要求，为未来的研究方向提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+Federated+Foundation+Language+Model+Post-Training+Should+Focus+on+Open-Source+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23593&send_immediately=true&force_search=false)

**原文摘要:** Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


### [139] [LLM Performance for Code Generation on Noisy Tasks](https://arxiv.org/abs/2505.23598)
*Radzim Sendyka, Christian Cabrera, Andrei Paleyes, Diana Robinson, Neil Lawrence*

**主要类别:** cs.LG

**概要:** 这篇论文研究了大型语言模型（LLMs）识别和解决被混淆到无法辨认的任务的能力。通过比较多个模型和混淆方法在编程竞赛和基准任务中的表现，作者发现所有评估的LLMs都能解决混淆后对人类读者不可理解的任务，并引入了“急切模式匹配”的概念来描述这种行为。此外，论文还报告了污染数据集和未见数据集之间性能衰减模式的不同，并提出了通过测量混淆下的性能衰减来检测数据集污染的方法。


<details>
  <summary>更多</summary>
  
**动机:** 研究大型语言模型在面对经过混淆处理的任务时的表现，以评估其是否依赖于记忆训练数据而非真正推理解决问题的能力。

**方法:** 使用编程竞赛和基准任务（如LeetCode和MATH），对多个大型语言模型进行测试，采用多种混淆方法（如噪声和删减）处理任务，并分析模型在这些任务上的表现。

**结果:** 所有评估的LLMs能够在任务被混淆到人类无法理解的程度时仍能解决问题，表现出“急切模式匹配”的行为。并且在模型知识截止日期之后发布的任务中没有观察到类似行为，表明可能存在强记忆化或过度拟合现象。此外，污染数据集和未见数据集之间的性能衰减模式存在显著差异。

**结论:** 需要谨慎设计使用标准数据集的实验，建议通过测量混淆下的性能衰减来检测数据集污染，从而揭示潜在的安全风险和可解释性问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+Performance+for+Code+Generation+on+Noisy+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23598，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23598&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the ability of large language models (LLMs) to
recognise and solve tasks which have been obfuscated beyond recognition.
Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we
compare performance across multiple models and obfuscation methods, such as
noise and redaction. We demonstrate that all evaluated LLMs can solve tasks
obfuscated to a level where the text would be unintelligible to human readers,
and does not contain key pieces of instruction or context. We introduce the
concept of eager pattern matching to describe this behaviour, which is not
observed in tasks published after the models' knowledge cutoff date, indicating
strong memorisation or overfitting to training data, rather than legitimate
reasoning about the presented problem. We report empirical evidence of distinct
performance decay patterns between contaminated and unseen datasets. We discuss
the implications for benchmarking and evaluations of model behaviour, arguing
for caution when designing experiments using standard datasets. We also propose
measuring the decay of performance under obfuscation as a possible strategy for
detecting dataset contamination and highlighting potential safety risks and
interpretability issues for automated software systems.

</details>


### [140] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)
*Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan*

**主要类别:** cs.LG

**概要:** Muddit是一种统一的离散扩散变换器，结合了预训练的文本到图像骨干和轻量级文本解码器，可在文本和图像模态之间进行快速并行生成。它在质量和效率上都优于较大的自回归模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前的自回归统一模型因顺序解码而导致推理速度慢，而非自回归统一模型由于预训练骨干有限而泛化能力弱。需要一种新的模型架构来解决这些问题。

**方法:** 引入Muddit，一种统一的离散扩散变换器，整合了来自预训练文本到图像骨干的强大视觉先验与轻量级文本解码器，支持灵活高质量的多模态生成。

**结果:** 实证结果表明，Muddit在质量和效率方面均达到或超过了显著较大的自回归模型的表现。

**结论:** 离散扩散模型（搭配强大的视觉先验）可作为统一生成任务的有效且可扩展的骨干架构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Muddit%3A+Liberating+Generation+Beyond+Text-to-Image+with+a+Unified+Discrete+Diffusion+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23606&send_immediately=true&force_search=false)

**原文摘要:** Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [141] [Data Model Design for Explainable Machine Learning-based Electricity Applications](https://arxiv.org/abs/2505.23607)
*Carolina Fortuna, Gregor Cerar, Blaz Bertalanic, Andrej Campa, Mihael Mohorcic*

**主要类别:** cs.LG

**概要:** 论文提出了一个数据分类法，用于指导能源应用中机器学习模型的数据模型开发，并通过家庭用电预测应用验证了该分类法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 从传统电网到智能电网的转变、可再生能源使用量的显著增加以及电价的飙升，促使了能源基础设施的数字化转型，但现有的机器学习模型大多依赖于单变量数据，缺乏对元数据和多变量数据的研究。

**方法:** 提出了一种数据分类法，识别并结构化与能源应用相关的各种类型数据；通过家庭用电预测应用，验证了该分类法在指导选择不同模型特征方面的有效性；研究了领域、上下文和行为特征对四种可解释机器学习技术预测准确性的影响。

**结果:** 验证了所提出的分类法在指导特征选择方面的有效性，并通过特征重要性技术解释了个别特征对预测准确性的贡献。

**结论:** 所提出的分类法可以有效指导特定应用的数据模型开发，并提高机器学习模型的预测准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Model+Design+for+Explainable+Machine+Learning-based+Electricity+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23607&send_immediately=true&force_search=false)

**原文摘要:** The transition from traditional power grids to smart grids, significant
increase in the use of renewable energy sources, and soaring electricity prices
has triggered a digital transformation of the energy infrastructure that
enables new, data driven, applications often supported by machine learning
models. However, the majority of the developed machine learning models rely on
univariate data. To date, a structured study considering the role meta-data and
additional measurements resulting in multivariate data is missing. In this
paper we propose a taxonomy that identifies and structures various types of
data related to energy applications. The taxonomy can be used to guide
application specific data model development for training machine learning
models. Focusing on a household electricity forecasting application, we
validate the effectiveness of the proposed taxonomy in guiding the selection of
the features for various types of models. As such, we study of the effect of
domain, contextual and behavioral features on the forecasting accuracy of four
interpretable machine learning techniques and three openly available datasets.
Finally, using a feature importance techniques, we explain individual feature
contributions to the forecasting accuracy.

</details>


### [142] [The Generalized Skew Spectrum of Graphs](https://arxiv.org/abs/2505.23609)
*Armando Bellante, Martin Plávala, Alessandro Luongo*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种置换不变的图嵌入方法，推广了Kondor和Borgwardt(2008)的图的偏谱方法。该方法基于群论和调和分析，引入了一类新的同构不变的图不变量，能够嵌入更丰富的图结构。通过应用保持泛化的启发式算法，可以在相同的计算成本下提高偏谱方法的表现力。作者正式证明了推广方法的不变性，并通过实验展示了其改进的表现力，同时讨论了其高效计算方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图嵌入方法（如Skew Spectrum）无法处理包含属性的图、多层图和超图等复杂图结构，因此需要一种更通用且表现力更强的图嵌入方法。

**方法:** 基于群论和调和分析，提出了一族置换不变的图嵌入方法，定义了一组函数来平衡计算复杂度和表达能力，并应用推广保持启发式算法来提高Skew Spectrum的表现力。

**结果:** 理论证明了推广方法的不变性，并通过实验证明了其在表达能力和处理复杂图结构方面的改进。

**结论:** 提出的图嵌入方法是一种有效的推广，能够在不增加计算成本的情况下提高表现力，适用于更广泛的图结构类型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Generalized+Skew+Spectrum+of+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23609&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes a family of permutation-invariant graph embeddings,
generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded
in group theory and harmonic analysis, our method introduces a new class of
graph invariants that are isomorphism-invariant and capable of embedding richer
graph structures - including attributed graphs, multilayer graphs, and
hypergraphs - which the Skew Spectrum could not handle. Our generalization
further defines a family of functions that enables a trade-off between
computational complexity and expressivity. By applying
generalization-preserving heuristics to this family, we improve the Skew
Spectrum's expressivity at the same computational cost. We formally prove the
invariance of our generalization, demonstrate its improved expressiveness
through experiments, and discuss its efficient computation.

</details>


### [143] [Learning Interpretable Differentiable Logic Networks for Tabular Regression](https://arxiv.org/abs/2505.23615)
*Chang Yue, Niraj K. Jha*

**主要类别:** cs.LG

**概要:** 本文提出了一种扩展的可微逻辑网络（DLNs）框架，用于监督表格回归任务。通过重新设计输出层以支持连续目标，并将两阶段训练过程统一为单一的可微阶段，DLNs在15个公共回归基准测试中表现与现代神经网络和经典回归基线相当或更优，同时保持了模型的可解释性和快速推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管神经网络在许多领域表现出色，但其决策过程通常是不透明的，并且在资源受限环境中推理成本较高。为了解决这些问题，作者提出了可微逻辑网络（DLNs），并将其扩展到监督表格回归任务。

**方法:** 作者通过重新设计最终输出层来支持连续目标值，并将原有的两阶段训练过程合并为一个单一的可微阶段。这使得DLNs可以应用于回归任务，同时保留了可解释性和低推理成本的特点。

**结果:** 在15个公共回归基准上的实验表明，回归DLNs在匹配或超越经典回归基线和现代神经网络准确率的同时，仍然保持了模型的可解释性和高效的计算性能。

**结论:** DLNs是一种可行且具有成本效益的回归任务替代方案，特别是在需要模型透明性和计算效率的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Interpretable+Differentiable+Logic+Networks+for+Tabular+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23615，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23615&send_immediately=true&force_search=false)

**原文摘要:** Neural networks (NNs) achieve outstanding performance in many domains;
however, their decision processes are often opaque and their inference can be
computationally expensive in resource-constrained environments. We recently
proposed Differentiable Logic Networks (DLNs) to address these issues for
tabular classification based on relaxing discrete logic into a differentiable
form, thereby enabling gradient-based learning of networks built from binary
logic operations. DLNs offer interpretable reasoning and substantially lower
inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically,
we redesign the final output layer to support continuous targets and unify the
original two-phase training procedure into a single differentiable stage. We
evaluate the resulting model on 15 public regression benchmarks, comparing it
with modern neural networks and classical regression baselines. Regression DLNs
match or exceed baseline accuracy while preserving interpretability and fast
inference. Our results show that DLNs are a viable, cost-effective alternative
for regression tasks, especially where model transparency and computational
efficiency are important.

</details>


### [144] [Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection](https://arxiv.org/abs/2505.23627)
*Griffin Dietz Smith, Dianna Yee, Jennifer King Chen, Leah Findlater*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的端到端架构，通过提示结合目标阅读文本，同时改进逐字转录和直接错误检测。研究显示这种方法在儿童朗读和成人非典型语音案例中均优于现有技术。


<details>
  <summary>更多</summary>
  
**动机:** 现有的事后方法在自动语音识别（ASR）无法准确转录音频时表现不佳，因此需要一种更有效的方法来进行朗读错误标注。

**方法:** 提出了一种新的端到端架构，该架构通过提示引入目标阅读文本，并同时进行改进的逐字转录和直接错误检测训练。通过两个案例研究（儿童朗读和成人非典型语音）验证了方法的有效性。

**结果:** 研究表明，与微调相比，通过提示引入阅读文本可以提高逐字转录性能；并且将语音识别任务增强为端到端错误检测是可行的。实验结果表明，该方法在逐字转录和错误检测方面优于当前最先进的技术。

**结论:** 通过提示引入目标阅读文本的方法不仅提高了逐字转录的性能，还证明了在语音识别任务中实现端到端错误检测的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prompting+Whisper+for+Improved+Verbatim+Transcription+and+End-to-end+Miscue+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23627&send_immediately=true&force_search=false)

**原文摘要:** Identifying mistakes (i.e., miscues) made while reading aloud is commonly
approached post-hoc by comparing automatic speech recognition (ASR)
transcriptions to the target reading text. However, post-hoc methods perform
poorly when ASR inaccurately transcribes verbatim speech. To improve on current
methods for reading error annotation, we propose a novel end-to-end
architecture that incorporates the target reading text via prompting and is
trained for both improved verbatim transcription and direct miscue detection.
Our contributions include: first, demonstrating that incorporating reading text
through prompting benefits verbatim transcription performance over fine-tuning,
and second, showing that it is feasible to augment speech recognition tasks for
end-to-end miscue detection. We conducted two case studies -- children's
read-aloud and adult atypical speech -- and found that our proposed strategies
improve verbatim transcription and miscue detection compared to current
state-of-the-art.

</details>


### [145] [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634)
*John Halloran*

**主要类别:** cs.LG

**概要:** 为了应对MCP协议中易受FBAs攻击的问题，本文提出了新的数据集和两种方法（DPO和RAG-Pref）以提高LLMs拒绝此类攻击的能力，有效增强了基于MCP的防护能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的MCP协议容易受到检索型虚假良性攻击（FBA），这些攻击可能欺骗MCP代理在受害者的系统上执行恶意操作，而无需用户直接下载受损文件。这表明MCP攻击的威胁模型比以前认为的要广泛得多。因此，需要一种更有效的防御机制来对抗这种新型攻击。

**方法:** 1. 构建了一个新的MCP数据集，包含FBA样本和真正良性的样本。
2. 使用直接偏好优化（DPO）进行大语言模型（LLM）的拒绝训练，以增强对FBA的防护。
3. 提出了一种新的偏好对齐策略——基于检索增强生成的偏好对齐（RAG-Pref），并与DPO结合使用以进一步提升LLMs拒绝FBA的能力。

**结果:** 研究发现DPO能有效改善LLMs对FBA的防护能力，但其效果与模型原有的后训练对齐方案密切相关。例如，基于GRPO的LLMs在拒绝学习方面表现较差。引入的RAG-Pref显著提高了LLMs拒绝FBA的能力，特别是在与DPO结合使用时，大幅提升了针对MCP攻击的防护能力。

**结论:** 通过构建新的MCP数据集并采用DPO和RAG-Pref等方法，可以显著增强LLMs对FBA的防护能力，从而为基于MCP的系统提供更强大的安全保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MCP+Safety+Training%3A+Learning+to+Refuse+Falsely+Benign+MCP+Exploits+using+Improved+Preference+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23634，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23634&send_immediately=true&force_search=false)

**原文摘要:** The model context protocol (MCP) has been widely adapted as an open standard
enabling the seamless integration of generative AI agents. However, recent work
has shown the MCP is susceptible to retrieval-based "falsely benign" attacks
(FBAs), allowing malicious system access and credential theft, but requiring
that users download compromised files directly to their systems. Herein, we
show that the threat model of MCP-based attacks is significantly broader than
previously thought, i.e., attackers need only post malicious content online to
deceive MCP agents into carrying out their attacks on unsuspecting victims'
systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP
dataset of FBAs and (truly) benign samples to explore the effectiveness of
direct preference optimization (DPO) for the refusal training of large language
models (LLMs). While DPO improves model guardrails against such attacks, we
show that the efficacy of refusal learning varies drastically depending on the
model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to
refuse extremely poorly. Thus, to further improve FBA refusals, we introduce
Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel
preference alignment strategy based on RAG. We show that RAG-Pref significantly
improves the ability of LLMs to refuse FBAs, particularly when combined with
DPO alignment, thus drastically improving guardrails against MCP-based attacks.

</details>


### [146] [Global optimization of graph acquisition functions for neural architecture search](https://arxiv.org/abs/2505.23640)
*Yilin Xie, Shiqiang Zhang, Jixiang Qing, Ruth Misener, Calvin Tsay*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的图贝叶斯优化方法，通过明确的优化公式和理论证明，有效提高了神经架构搜索的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图贝叶斯优化工作主要集中在开发图代理模型上，而对获取函数优化的研究较少，特别是在图结构上的离散优化任务。

**方法:** 作者提出了针对图输入空间的明确优化公式，包括可达性和最短路径等属性，并用其构建图核函数和获取函数。还提供了节点或边标签限制下的NAS领域理论证明。

**结果:** 数值结果表明，该方法在多个神经架构搜索基准上能够高效找到最优架构。

**结论:** 所提出的方法为图贝叶斯优化提供了一种等效表示，并在神经架构搜索中展示了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Global+optimization+of+graph+acquisition+functions+for+neural+architecture+search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23640&send_immediately=true&force_search=false)

**原文摘要:** Graph Bayesian optimization (BO) has shown potential as a powerful and
data-efficient tool for neural architecture search (NAS). Most existing graph
BO works focus on developing graph surrogates models, i.e., metrics of networks
and/or different kernels to quantify the similarity between networks. However,
the acquisition optimization, as a discrete optimization task over graph
structures, is not well studied due to the complexity of formulating the graph
search space and acquisition functions. This paper presents explicit
optimization formulations for graph input space including properties such as
reachability and shortest paths, which are used later to formulate graph
kernels and the acquisition function. We theoretically prove that the proposed
encoding is an equivalent representation of the graph space and provide
restrictions for the NAS domain with either node or edge labels. Numerical
results over several NAS benchmarks show that our method efficiently finds the
optimal architecture for most cases, highlighting its efficacy.

</details>


### [147] [Continuous Chain of Thought Enables Parallel Exploration and Reasoning](https://arxiv.org/abs/2505.23648)
*Halil Alperen Gozeten, M. Emrullah Ildiz, Xuechen Zhang, Hrayr Harutyunyan, Ankit Singh Rawat, Samet Oymak*

**主要类别:** cs.LG

**概要:** 当前语言模型通过自回归采样生成思维链痕迹，尽管离散采样取得了显著成功，但连续值标记的思维链（CoT2）提供了一个更丰富和更具表现力的替代方案。我们的工作通过逻辑推理任务探讨了CoT2的优势，并提供了优化和探索方法。理论上，我们证明了CoT2允许模型并行跟踪多个痕迹，并量化了其对推理效率的好处。此外，配备CoT2的一层变压器可以在给定足够的嵌入维度下解决组合“子集总和问题”。基于这些见解，我们提出了一个新颖且有效的监督策略，并引入了解锁CoT2策略优化和自我改进的采样策略。实验结果表明，使用CoT2进行策略优化确实提高了模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管离散采样的思维链在语言模型中取得了显著成功，但连续值标记的思维链（CoT2）可能提供一个更丰富和更具表现力的替代方案，特别是在需要搜索能力的任务中。

**方法:** 1. 提出理论分析，展示CoT2如何允许模型并行跟踪多个痕迹并提高推理效率。2. 证明一层变压器配备CoT2可以解决组合“子集总和问题”。3. 提出匹配softmax输出到目标痕迹的经验标记分布的监督策略。4. 引入两种采样策略：一种是控制并行级别的离散采样，另一种是概率单纯形上的连续探索。

**结果:** 实验结果表明，使用CoT2进行策略优化确实提高了模型的性能，超越了初始的离散或连续监督。

**结论:** 连续值标记的思维链（CoT2）在逻辑推理任务中展示了显著优势，并通过优化和探索方法进一步提升了模型性能。这为未来研究提供了一个新的方向，即通过策略优化和自我改进来增强语言模型的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuous+Chain+of+Thought+Enables+Parallel+Exploration+and+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23648，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23648&send_immediately=true&force_search=false)

**原文摘要:** Current language models generate chain-of-thought traces by autoregressively
sampling tokens from a finite vocabulary. While this discrete sampling has
achieved remarkable success, conducting chain-of-thought with
continuously-valued tokens (CoT2) offers a richer and more expressive
alternative. Our work examines the benefits of CoT2 through logical reasoning
tasks that inherently require search capabilities and provide optimization and
exploration methods for CoT2. Theoretically, we show that CoT2 allows the model
to track multiple traces in parallel and quantify its benefits for inference
efficiency. Notably, one layer transformer equipped with CoT2 can provably
solve the combinatorial "subset sum problem" given sufficient embedding
dimension. These insights lead to a novel and effective supervision strategy
where we match the softmax outputs to the empirical token distributions of a
set of target traces. Complementing this, we introduce sampling strategies that
unlock policy optimization and self-improvement for CoT2. Our first strategy
samples and composes $K$ discrete tokens at each decoding step to control the
level of parallelism, and reduces to standard CoT when $K=1$. Our second
strategy relies on continuous exploration over the probability simplex.
Experiments confirm that policy optimization with CoT2 indeed improves the
performance of the model beyond its initial discrete or continuous supervision.

</details>


### [148] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/abs/2505.23651)
*Juncheol Shin, Minsang Seok, Seonggon Kim, Eunhyeok Park*

**主要类别:** cs.LG

**概要:** 在多目标领域适应中，模型合并是一种强大的技术，但量化模型的应用带来了新的挑战。本文通过误差屏障的视角分析了量化对模型合并的影响，并提出了HDRQ方法，确保量化过程与源预训练模型的偏差最小化，同时平滑损失曲面以促进模型合并。


<details>
  <summary>更多</summary>
  
**动机:** 现有的模型合并技术在处理量化模型时面临挑战，因为量化限制了领域范围并引入离散化效应，使得模型合并变得复杂。

**方法:** 提出了一种新的后训练量化方法HDRQ（Hessian and distant regularizing quantization），该方法考虑了多目标领域适应中的模型合并问题。具体来说，HDRQ确保量化过程与源预训练模型的偏差最小化，并且通过平滑损失曲面来促进模型合并。

**结果:** 广泛的实验验证了所提出方法的有效性。这是据作者所知，首个针对此问题的研究。

**结论:** HDRQ方法能够有效应对量化模型在多目标领域适应中的模型合并挑战，为未来研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Merge-Friendly+Post-Training+Quantization+for+Multi-Target+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23651&send_immediately=true&force_search=false)

**原文摘要:** Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [149] [How does Transformer Learn Implicit Reasoning?](https://arxiv.org/abs/2505.23653)
*Jiaran Ye, Zijun Yao, Zhidian Huang, Liangming Pan, Jinxin Liu, Yushi Bai, Amy Xin, Liu Weichuan, Xiaoyin Che, Lei Hou, Juanzi Li*

**主要类别:** cs.LG

**概要:** 近期研究表明，大型语言模型（LLMs）能够隐式地进行多步推理——在不明确表述中间步骤的情况下得出正确答案，但其潜在机制尚不明确。本文通过在受控的符号环境中从头训练Transformer模型，研究了这种隐式推理的出现过程。分析揭示了三个发展阶段：早期记忆、随后的分布内泛化，以及最终的跨分布泛化。我们发现使用原子三元组训练不是必要条件，但可以加速学习，并且第二步泛化依赖于查询级别的特定组合结构暴露。为了解释这些行为，我们引入了两种诊断工具：跨查询语义修补，识别语义可重用的中间表示；以及基于余弦相似性的表示镜头，揭示成功推理与隐藏空间中基于余弦的聚类现象相关。该聚类现象为训练过程中观察到的行为动态提供了一致的解释，将表示结构与推理能力联系起来。这些发现为隐式多步推理在LLMs中的可解释性提供了新见解，有助于阐明复杂的推理过程如何在内部展开，并为提高此类模型的透明度提供了途径。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有研究表明大型语言模型可以隐式地进行多步推理，但其潜在机制尚未被充分理解。因此，作者希望通过从头训练Transformer模型来研究隐式推理是如何出现的。

**方法:** 作者在受控的符号环境中从头训练Transformer模型，研究其隐式推理能力的发展过程。他们分析了模型在不同阶段的表现，并发现训练数据中的原子三元组虽然不是必要条件，但可以加速学习过程。此外，第二步泛化依赖于特定的组合结构暴露。为了更好地理解这些行为，作者引入了两种诊断工具：跨查询语义修补和基于余弦相似性的表示镜头。

**结果:** 实验结果表明，模型的隐式推理能力遵循一个三阶段的发展轨迹：早期记忆、分布内泛化和跨分布泛化。同时，作者发现使用原子三元组训练可以加速学习过程，而第二步泛化则依赖于特定的组合结构暴露。通过引入的诊断工具，作者揭示了成功推理与隐藏空间中的聚类现象之间的关联。

**结论:** 本文的研究为隐式多步推理在大型语言模型中的可解释性提供了新的见解。这些发现不仅帮助澄清了复杂的推理过程如何在模型内部展开，还为提高此类模型的透明度提供了可能的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+does+Transformer+Learn+Implicit+Reasoning%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23653&send_immediately=true&force_search=false)

**原文摘要:** Recent work suggests that large language models (LLMs) can perform multi-hop
reasoning implicitly -- producing correct answers without explicitly
verbalizing intermediate steps -- but the underlying mechanisms remain poorly
understood. In this paper, we study how such implicit reasoning emerges by
training transformers from scratch in a controlled symbolic environment. Our
analysis reveals a three-stage developmental trajectory: early memorization,
followed by in-distribution generalization, and eventually cross-distribution
generalization. We find that training with atomic triples is not necessary but
accelerates learning, and that second-hop generalization relies on query-level
exposure to specific compositional structures. To interpret these behaviors, we
introduce two diagnostic tools: cross-query semantic patching, which identifies
semantically reusable intermediate representations, and a cosine-based
representational lens, which reveals that successful reasoning correlates with
the cosine-base clustering in hidden space. This clustering phenomenon in turn
provides a coherent explanation for the behavioral dynamics observed across
training, linking representational structure to reasoning capability. These
findings provide new insights into the interpretability of implicit multi-hop
reasoning in LLMs, helping to clarify how complex reasoning processes unfold
internally and offering pathways to enhance the transparency of such models.

</details>


### [150] [AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction](https://arxiv.org/abs/2505.23663)
*Niklas Freymuth, Tobias Würth, Nicolas Schreiber, Balazs Gyenes, Andreas Boltres, Johannes Mitsch, Aleksandar Taranovic, Tai Hoang, Philipp Dahlinger, Philipp Becker, Luise Kärger, Gerhard Neumann*

**主要类别:** cs.LG

**概要:** AMBER 是一种通过监督学习方法进行网格自适应的系统，它利用分层图神经网络从粗网格开始迭代预测尺寸场，并生成新的中间网格。相比其他方法，AMBER 在2D和3D数据集上表现优异且能推广到未见过的几何形状。


<details>
  <summary>更多</summary>
  
**动机:** 有限元法（FEM）模拟复杂物理系统的成本和精度与底层网格分辨率相关。虽然自适应网格可以提高计算效率，但通常需要特定任务的启发式方法或人工设计。

**方法:** AMBER 使用监督学习方法，从粗网格出发，通过预测尺寸场来生成新的中间网格。该过程依赖于分层图神经网络，并在训练期间通过自动投影专家标签进行数据增强。

**结果:** AMBER 在2D和3D数据集上进行了评估，包括经典物理问题、机械组件和具有人类专家网格的实际工业设计。结果表明，AMBER 能够推广到未见过的几何形状，并且一致优于多个近期基线方法。

**结论:** AMBER 提供了一种有效的监督学习方法来进行网格自适应，能够推广到新几何形状并优于多种现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AMBER%3A+Adaptive+Mesh+Generation+by+Iterative+Mesh+Resolution+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23663&send_immediately=true&force_search=false)

**原文摘要:** The cost and accuracy of simulating complex physical systems using the Finite
Element Method (FEM) scales with the resolution of the underlying mesh.
Adaptive meshes improve computational efficiency by refining resolution in
critical regions, but typically require task-specific heuristics or cumbersome
manual design by a human expert. We propose Adaptive Meshing By Expert
Reconstruction (AMBER), a supervised learning approach to mesh adaptation.
Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,
a function mapping from the geometry to the local element size of the target
mesh, and uses this prediction to produce a new intermediate mesh using an
out-of-the-box mesh generator. This process is enabled through a hierarchical
graph neural network, and relies on data augmentation by automatically
projecting expert labels onto AMBER-generated data during training. We evaluate
AMBER on 2D and 3D datasets, including classical physics problems, mechanical
components, and real-world industrial designs with human expert meshes. AMBER
generalizes to unseen geometries and consistently outperforms multiple recent
baselines, including ones using Graph and Convolutional Neural Networks, and
Reinforcement Learning-based approaches.

</details>


### [151] [Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds](https://arxiv.org/abs/2505.23673)
*Aya Kayal, Sattar Vakili, Laura Toni, Da-shan Shiu, Alberto Bernacchia*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Optimization+from+Human+Feedback%3A+Near-Optimal+Regret+Bounds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23673&send_immediately=true&force_search=false)

**原文摘要:** Bayesian optimization (BO) with preference-based feedback has recently
garnered significant attention due to its emerging applications. We refer to
this problem as Bayesian Optimization from Human Feedback (BOHF), which differs
from conventional BO by learning the best actions from a reduced feedback
model, where only the preference between two actions is revealed to the learner
at each time step. The objective is to identify the best action using a limited
number of preference queries, typically obtained through costly human feedback.
Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model,
provides regret bounds for the performance of several algorithms. In this work,
within the same framework we develop tighter performance guarantees.
Specifically, we derive regret bounds of
$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the
maximum information gain$\unicode{x2014}$a kernel-specific complexity
term$\unicode{x2014}$and $T$ is the number of queries. Our results
significantly improve upon existing bounds. Notably, for common kernels, we
show that the order-optimal sample complexities of conventional
BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are
recovered. In other words, the same number of preferential samples as
scalar-valued samples is sufficient to find a nearly optimal solution.

</details>


### [152] [Understanding Mode Connectivity via Parameter Space Symmetry](https://arxiv.org/abs/2505.23681)
*Bo Zhao, Nima Dehmamy, Robin Walters, Rose Yu*

**主要类别:** cs.LG

**概要:** 神经网络极小值通常通过保持训练和测试损失几乎不变的曲线相连，这种现象称为模式连通性。本文提出了一种新的方法，利用参数空间对称性探索极小值的连通性，并通过线性网络的对称群拓扑结构推导出极小值的连通分量数量，同时展示了跳跃连接如何减少该数量。此外，文章分析了模式连通性和线性模式连通性成立或失败的条件，并提供了由对称性引起的极小值连接曲线的显式表达式。研究强调了连续对称性在理解神经网络损失景观中的作用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管模式连通性已经在模型合并和微调等应用中得到使用，但其理论解释尚不清楚。因此，本文旨在通过参数空间对称性来更好地理解和解释模式连通性的现象。

**方法:** 1. 利用参数空间对称性探索极小值的连通性。
2. 通过将对称群的拓扑结构与极小值的拓扑结构联系起来，推导线性网络极小值的连通分量数。
3. 分析跳跃连接如何减少极小值的连通分量数。
4. 研究模式连通性和线性模式连通性成立或失败的条件。
5. 提供由对称性诱导的极小值连接曲线的显式表达式，并利用这些曲线的曲率推导线性模式连通性近似成立的条件。

**结果:** 1. 成功推导了线性网络极小值的连通分量数。
2. 发现跳跃连接可以减少极小值的连通分量数。
3. 提供了模式连通性和线性模式连通性成立或失败的具体条件。
4. 明确了由对称性引起的极小值连接曲线的显式表达式。
5. 推导了线性模式连通性近似成立的条件。

**结论:** 连续对称性在理解神经网络损失景观中起着关键作用。通过对称性分析，可以更深入地理解模式连通性及其相关性质。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Mode+Connectivity+via+Parameter+Space+Symmetry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23681，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23681&send_immediately=true&force_search=false)

**原文摘要:** Neural network minima are often connected by curves along which train and
test loss remain nearly constant, a phenomenon known as mode connectivity.
While this property has enabled applications such as model merging and
fine-tuning, its theoretical explanation remains unclear. We propose a new
approach to exploring the connectedness of minima using parameter space
symmetry. By linking the topology of symmetry groups to that of the minima, we
derive the number of connected components of the minima of linear networks and
show that skip connections reduce this number. We then examine when mode
connectivity and linear mode connectivity hold or fail, using parameter
symmetries which account for a significant part of the minimum. Finally, we
provide explicit expressions for connecting curves in the minima induced by
symmetry. Using the curvature of these curves, we derive conditions under which
linear mode connectivity approximately holds. Our findings highlight the role
of continuous symmetries in understanding the neural network loss landscape.

</details>


### [153] [Learning Compositional Functions with Transformers from Easy-to-Hard Data](https://arxiv.org/abs/2505.23683)
*Zixuan Wang, Eshaan Nichani, Alberto Bietti, Alex Damian, Daniel Hsu, Jason D. Lee, Denny Wu*

**主要类别:** cs.LG

**概要:** 本研究探讨了Transformer在k重组合任务上的可学习性，证明了统计计算差距的存在，并展示了通过课程学习策略和梯度下降方法可以在多项式时间内高效学习此类任务。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有研究表明Transformer能够高效执行多步推理任务，但关于其构造的可学习性，特别是通过基于梯度的优化实现高效学习的数据分布条件，仍是一个未解决的问题。因此，本文旨在研究Transformer在k重组合任务上的可学习性。

**方法:** 作者研究了需要计算k个输入排列和k个隐藏排列交错组合的k重组合任务。他们证明了一个统计查询（SQ）下界，表明任何对k重组合任务分布进行多项式数量查询的SQ学习者都需要指数级样本量。此外，通过两种不同的课程学习策略（一种是按难度递增顺序呈现数据，另一种是同时呈现所有数据），作者展示了可以通过梯度下降在O(log k)深度的Transformer上以多项式时间和样本复杂度进行高效学习。

**结果:** 研究结果表明，存在一个统计计算差距：使用传统的SQ方法学习k重组合任务需要指数级样本，而通过适当的课程学习策略和梯度下降方法，可以在多项式时间内完成学习。

**结论:** 本研究揭示了数据分布中包含简单和复杂示例对于Transformer学习复杂组合任务的必要性和充分性。这为理解Transformer的学习能力提供了新的视角，并强调了课程学习策略的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Compositional+Functions+with+Transformers+from+Easy-to-Hard+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23683，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23683&send_immediately=true&force_search=false)

**原文摘要:** Transformer-based language models have demonstrated impressive capabilities
across a range of complex reasoning tasks. Prior theoretical work exploring the
expressive power of transformers has shown that they can efficiently perform
multi-step reasoning tasks involving parallelizable computations. However, the
learnability of such constructions, particularly the conditions on the data
distribution that enable efficient learning via gradient-based optimization,
remains an open question. Towards answering this question, in this work we
study the learnability of the $k$-fold composition task, which requires
computing an interleaved composition of $k$ input permutations and $k$ hidden
permutations, and can be expressed by a transformer with $O(\log k)$ layers. On
the negative front, we prove a Statistical Query (SQ) lower bound showing that
any SQ learner that makes only polynomially-many queries to an SQ oracle for
the $k$-fold composition task distribution must have sample size exponential in
$k$, thus establishing a statistical-computational gap. On the other hand, we
show that this function class can be efficiently learned, with runtime and
sample complexity polynomial in $k$, by gradient descent on an $O(\log
k)$-depth transformer via two different curriculum learning strategies: one in
which data consists of $k'$-fold composition functions with $k' \le k$
presented in increasing difficulty, and another in which all such data is
presented simultaneously. Our work sheds light on the necessity and sufficiency
of having both easy and hard examples in the data distribution for transformers
to learn complex compositional tasks.

</details>


### [154] [Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms](https://arxiv.org/abs/2505.23696)
*Hiroshi Kera, Nico Pelleriti, Yuki Ishihara, Max Zimmer, Sebastian Pokutta*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的深度学习方法——Oracle Border Basis Algorithm，用于加速多项式方程组的Border basis计算，同时保证输出正确性。通过设计基于Transformer的oracle模型，消除昂贵的计算步骤，并提出了针对Border basis的采样定理和紧凑的输入表示方法，最终实现了高达3.5倍的速度提升。


<details>
  <summary>更多</summary>
  
**动机:** 求解多项式方程组，尤其是有限解的问题，在许多科学领域中至关重要。传统方法（如Gröbner基和Border基）虽然基础但计算成本高，而现有的深度学习方法虽提高了效率却牺牲了输出的正确性。因此，需要一种既能提高计算效率又能保证输出正确性的新方法。

**方法:** 作者引入了Oracle Border Basis Algorithm，这是一种结合深度学习的新型算法。具体方法包括：1) 设计并训练一个基于Transformer的oracle模型，用于识别和消除计算开销大的约简步骤；2) 提出首个Border basis采样定理以生成训练数据；3) 构建专门适用于单项式为中心的代数计算的标记化和嵌入方案，从而减少编码n变量多项式的token数量。

**结果:** 与基础算法相比，该方法在不损害结果正确性的前提下，实现了最高3.5倍的加速效果。此外，提出的采样定理、紧凑的输入表示方法以及稳定的数据高效学习方法为传统计算机代数算法提供了实用增强。

**结论:** Oracle Border Basis Algorithm是首个将深度学习应用于Border basis计算并保证输出正确性的方法。它不仅显著提高了计算效率，还为传统符号计算提供了稳定的改进方案，展现了深度学习在代数计算领域的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computational+Algebra+with+Attention%3A+Transformer+Oracles+for+Border+Basis+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23696&send_immediately=true&force_search=false)

**原文摘要:** Solving systems of polynomial equations, particularly those with finitely
many solutions, is a crucial challenge across many scientific fields.
Traditional methods like Gr\"obner and Border bases are fundamental but suffer
from high computational costs, which have motivated recent Deep Learning
approaches to improve efficiency, albeit at the expense of output correctness.
In this work, we introduce the Oracle Border Basis Algorithm, the first Deep
Learning approach that accelerates Border basis computation while maintaining
output guarantees. To this end, we design and train a Transformer-based oracle
that identifies and eliminates computationally expensive reduction steps, which
we find to dominate the algorithm's runtime. By selectively invoking this
oracle during critical phases of computation, we achieve substantial speedup
factors of up to 3.5x compared to the base algorithm, without compromising the
correctness of results. To generate the training data, we develop a sampling
method and provide the first sampling theorem for border bases. We construct a
tokenization and embedding scheme tailored to monomial-centered algebraic
computations, resulting in a compact and expressive input representation, which
reduces the number of tokens to encode an $n$-variate polynomial by a factor of
$O(n)$. Our learning approach is data efficient, stable, and a practical
enhancement to traditional computer algebra algorithms and symbolic
computation.

</details>


### [155] [DiCoFlex: Model-agnostic diverse counterfactuals with flexible control](https://arxiv.org/abs/2505.23700)
*Oleksii Furman, Ulvi Movsum-zada, Patryk Marszalek, Maciej Zięba, Marek Śmieja*

**主要类别:** cs.LG

**概要:** 本文提出了一种名为DiCoFlex的新框架，用于生成反事实解释。该方法模型不可知且条件生成，能够在单次前向传递中生成多个多样化的反事实，并允许用户在推理时实时定制约束条件。实验表明，DiCoFlex在有效性、多样性、接近性和约束遵循方面优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前生成反事实解释的方法存在需要持续访问预测模型、计算密集型优化以及缺乏灵活性等问题，无法适应新的用户定义约束而无需重新训练。

**方法:** 提出了DiCoFlex，一种模型不可知的条件生成框架，利用仅在标记数据上训练的条件规范化流，在单次前向传递中生成多个多样化的反事实，并允许实时用户驱动的约束定制（如稀疏性和可操作性）。

**结果:** 在标准基准数据集上的广泛实验表明，DiCoFlex在有效性、多样性、接近性和约束遵循方面表现优异，超越了现有方法。

**结论:** DiCoFlex为反事实生成提供了一个实用且可扩展的解决方案，特别适用于敏感决策领域的解释性人工智能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiCoFlex%3A+Model-agnostic+diverse+counterfactuals+with+flexible+control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23700，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23700&send_immediately=true&force_search=false)

**原文摘要:** Counterfactual explanations play a pivotal role in explainable artificial
intelligence (XAI) by offering intuitive, human-understandable alternatives
that elucidate machine learning model decisions. Despite their significance,
existing methods for generating counterfactuals often require constant access
to the predictive model, involve computationally intensive optimization for
each instance and lack the flexibility to adapt to new user-defined constraints
without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,
conditional generative framework that produces multiple diverse counterfactuals
in a single forward pass. Leveraging conditional normalizing flows trained
solely on labeled data, DiCoFlex addresses key limitations by enabling
real-time user-driven customization of constraints such as sparsity and
actionability at inference time. Extensive experiments on standard benchmark
datasets show that DiCoFlex outperforms existing methods in terms of validity,
diversity, proximity, and constraint adherence, making it a practical and
scalable solution for counterfactual generation in sensitive decision-making
domains.

</details>


### [156] [(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs](https://arxiv.org/abs/2505.23702)
*Nathan Lichtlé, Alexi Canesse, Zhe Fu, Hossein Nick Zinat Matin, Maria Laura Delle Monache, Alexandre M. Bayen*

**主要类别:** cs.LG

**概要:** 提出了一种新的神经网络架构(U)NFV，结合有限体积法解决双曲守恒律问题，具有更低误差和更高灵活性。


<details>
  <summary>更多</summary>
  
**动机:** 双曲偏微分方程（PDEs）的求解特别是包含激波和不连续性的守恒律非常具有挑战性，而传统的有限体积法虽然具备良好的数学特性，但在复杂场景下精度和灵活性不足。

**方法:** 引入了(U)NFV，一种模块化的神经网络结构，该结构概括了经典的有限体积方法，用于求解双曲守恒律。通过学习扩展的空间和时间模板上的更新规则，同时保留守恒结构，解决了传统FV方法在复杂设置中缺乏准确性和灵活性的问题。支持有监督训练（NFV）和无监督训练（UNFV）。

**结果:** (U)NFV在求解一阶守恒律时，相比Godunov方法误差低10倍，优于ENO/WENO方法，并且以更少的复杂度媲美不连续Galerkin求解器。在交通建模问题上，无论是从PDEs还是实验高速公路数据中，(U)NFV都能显著提高非线性波动力学的捕捉精度和可扩展性。

**结论:** (U)NFV提供了一种新方法来解决双曲守恒律问题，相较于传统方法，它具有更高的精度、更好的灵活性以及更强的守恒性质保持能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%28U%29NFV%3A+Supervised+and+Unsupervised+Neural+Finite+Volume+Methods+for+Solving+Hyperbolic+PDEs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23702&send_immediately=true&force_search=false)

**原文摘要:** We introduce (U)NFV, a modular neural network architecture that generalizes
classical finite volume (FV) methods for solving hyperbolic conservation laws.
Hyperbolic partial differential equations (PDEs) are challenging to solve,
particularly conservation laws whose physically relevant solutions contain
shocks and discontinuities. FV methods are widely used for their mathematical
properties: convergence to entropy solutions, flow conservation, or total
variation diminishing, but often lack accuracy and flexibility in complex
settings. Neural Finite Volume addresses these limitations by learning update
rules over extended spatial and temporal stencils while preserving conservation
structure. It supports both supervised training on solution data (NFV) and
unsupervised training via weak-form residual loss (UNFV). Applied to
first-order conservation laws, (U)NFV achieves up to 10x lower error than
Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin
solvers with far less complexity. On traffic modeling problems, both from PDEs
and from experimental highway data, (U)NFV captures nonlinear wave dynamics
with significantly higher fidelity and scalability than traditional FV
approaches.

</details>


### [157] [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/abs/2505.23705)
*Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, Sergey Levine*

**主要类别:** cs.LG

**概要:** 论文探讨了在视觉-语言-动作（VLA）模型中加入连续扩散或流匹配动作专家的影响，发现这种模块会显著降低训练速度和知识转移效果。通过深入分析不同设计选择对性能和知识转移的影响，提出了一种保护预训练视觉语言模型（VLM）核心的技术，以缓解这一问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前的VLA模型虽然结合了端到端学习与大规模视觉语言模型的语义知识，但在实时控制任务中面临挑战：强大的VLM参数量巨大，难以进行实时推理，且其离散输出不适合机器人控制。为解决此问题，一些研究引入了连续控制模块（如动作专家或连续输出头），但这些模块是否会影响VLM中的语义知识尚不清楚。

**方法:** 作者分析了在VLA模型中使用连续扩散或流匹配动作专家的效果，并评估其对训练动态、知识转移及性能的影响。针对发现的问题，提出了一种技术来保护VLM核心部分，从而减轻不良影响。

**结果:** 研究表明，简单地添加连续扩散或流匹配动作专家会显著损害训练速度和知识转移效果。而提出的保护技术能够有效缓解这一问题，提升模型性能。

**结论:** 为了在VLA模型中实现高效的连续控制，需要谨慎设计相关模块，以免影响预训练VLM的语义知识。本文提出的保护技术为解决这一问题提供了一个可行方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Insulating+Vision-Language-Action+Models%3A+Train+Fast%2C+Run+Fast%2C+Generalize+Better，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23705&send_immediately=true&force_search=false)

**原文摘要:** Vision-language-action (VLA) models provide a powerful approach to training
control policies for physical systems, such as robots, by combining end-to-end
learning with transfer of semantic knowledge from web-scale vision-language
model (VLM) training. However, the constraints of real-time control are often
at odds with the design of VLMs: the most powerful VLMs have tens or hundreds
of billions of parameters, presenting an obstacle to real-time inference, and
operate on discrete tokens rather than the continuous-valued outputs that are
required for controlling robots. To address this challenge, recent VLA models
have used specialized modules for efficient continuous control, such as action
experts or continuous output heads, which typically require adding new
untrained parameters to the pretrained VLM backbone. While these modules
improve real-time and control capabilities, it remains an open question whether
they preserve or degrade the semantic knowledge contained in the pretrained
VLM, and what effect they have on the VLA training dynamics. In this paper, we
study this question in the context of VLAs that include a continuous diffusion
or flow matching action expert, showing that naively including such experts
significantly harms both training speed and knowledge transfer. We provide an
extensive analysis of various design choices, their impact on performance and
knowledge transfer, and propose a technique for insulating the VLM backbone
during VLA training that mitigates this issue. Videos are available at
https://pi.website/research/knowledge_insulation.

</details>


### [158] [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](https://arxiv.org/abs/2505.23719)
*Andreas Auer, Patrick Podest, Daniel Klotz, Sebastian Böck, Günter Klambauer, Sepp Hochreiter*

**主要类别:** cs.LG

**概要:** TiRex是一种基于增强型LSTM（xLSTM）的时间序列零样本预测模型，通过引入CPM训练掩码策略，提升了长时预测的状态跟踪能力，在GiftEval和Chronos-ZS基准上超越了多个大型模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有的零样本时间序列预测方法主要依赖于Transformer架构，但其在时间序列预测中的表现不如循环模型（如LSTM）。然而，LSTM虽然擅长时间序列建模，但缺乏强大的上下文学习能力。因此需要一种结合两者优势的模型来弥补这一差距。

**方法:** 提出了一种名为TiRex的模型，该模型基于增强型LSTM（xLSTM），具备强大的上下文学习能力。同时引入了一种称为CPM的训练时间掩码策略，以进一步提升其状态跟踪能力，这对于长时预测尤为重要。

**结果:** TiRex在HuggingFace的GiftEval和Chronos-ZS基准测试中取得了新的最佳成绩，显著优于包括TabPFN-TS、Chronos Bolt、TimesFM和Moirai在内的更大规模模型，无论是在短期还是长期预测中均表现出色。

**结论:** TiRex通过结合增强型LSTM和CPM策略，在零样本时间序列预测任务中展现了优越性能，特别是在长时预测方面，为非专家用户提供了强大的预测工具，并在数据稀缺时提高了预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TiRex%3A+Zero-Shot+Forecasting+Across+Long+and+Short+Horizons+with+Enhanced+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23719，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23719&send_immediately=true&force_search=false)

**原文摘要:** In-context learning, the ability of large language models to perform tasks
using only examples provided in the prompt, has recently been adapted for time
series forecasting. This paradigm enables zero-shot prediction, where past
values serve as context for forecasting future values, making powerful
forecasting tools accessible to non-experts and increasing the performance when
training data are scarce. Most existing zero-shot forecasting approaches rely
on transformer architectures, which, despite their success in language, often
fall short of expectations in time series forecasting, where recurrent models
like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited
for time series modeling due to their state-tracking capabilities, they lack
strong in-context learning abilities. We introduce TiRex that closes this gap
by leveraging xLSTM, an enhanced LSTM with competitive in-context learning
skills. Unlike transformers, state-space models, or parallelizable RNNs such as
RWKV, TiRex retains state-tracking, a critical property for long-horizon
forecasting. To further facilitate its state-tracking ability, we propose a
training-time masking strategy called CPM. TiRex sets a new state of the art in
zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and
Chronos-ZS, outperforming significantly larger models including TabPFN-TS
(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)
across both short- and long-term forecasts.

</details>


### [159] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/abs/2505.23721)
*Sean Current, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning, Srinivasan Parthasarathy*

**主要类别:** cs.LG

**概要:** 提出DiffER方法，通过分类扩散预测整个输出SMILES序列，构建包含新长度预测组件的扩散模型集合，在top-1准确性上达到最先进水平，并展示出强大的基线性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于变压器神经网络的自动化学逆合成方法受限于其自回归特性，需要一种新的、非模板依赖的方法来改进预测能力。

**方法:** 提出了DiffER方法，使用分类扩散预测整个输出SMILES序列，构建了带有新颖长度预测组件的扩散模型集合。

**结果:** 在无模板方法中，该方法在top-1准确性上达到最先进的性能，并且在top-3、top-5和top-10准确性上具有竞争力。准确预测SMILES序列长度是提高分类扩散模型性能的关键。

**结论:** DiffER是一种强大的无模板模型基线，能够学习多种实验室使用的合成技术，并在top-k准确性指标上优于其他无模板方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiffER%3A+Categorical+Diffusion+for+Chemical+Retrosynthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23721&send_immediately=true&force_search=false)

**原文摘要:** Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [160] [MuLoCo: Muon is a practical inner optimizer for DiLoCo](https://arxiv.org/abs/2505.23725)
*Benjamin Thérien, Xiaolong Huang, Irina Rish, Eugene Belilovsky*

**主要类别:** cs.LG

**概要:** 研究提出MuLoCo方法，通过使用Muon优化器和误差反馈累加器将DiLoCo的通信开销压缩至2位而性能几乎无损，同时内存复杂度相同但性能显著优于DiLoCo。


<details>
  <summary>更多</summary>
  
**动机:** 尽管DiLoCo框架减少了通信频率，但其通信步骤仍需全量减少模型参数完整副本。目前对误差反馈累加器的作用及内部优化器对压缩性的影响尚待深入研究。

**方法:** 研究调查了Top-k稀疏化和量化等标准压缩方法与两种本地优化器（AdamW和Muon）结合以减少DiLoCo通信开销的效果。特别地，采用Muon作为内部优化器并配合误差反馈累加器实现激进压缩。

**结果:** 实验表明，使用Muon作为内部优化器并通过误差反馈累加器可以将通信delta压缩到2位，几乎没有性能下降。MuLoCo在通信量减少8倍的情况下显著优于DiLoCo，且内存复杂度相同。

**结论:** Muon优化器与误差反馈机制结合能有效减少DiLoCo的通信开销而不损害性能，提出的MuLoCo方法具有更低的通信需求和更高的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MuLoCo%3A+Muon+is+a+practical+inner+optimizer+for+DiLoCo，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23725，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23725&send_immediately=true&force_search=false)

**原文摘要:** DiLoCo is a powerful framework for training large language models (LLMs)
under networking constraints with advantages for increasing parallelism and
accelerator utilization in data center settings. Despite significantly reducing
communication frequency, however, DiLoCo's communication steps still involve
all-reducing a complete copy of the model's parameters. While existing works
have explored ways to reduce communication in DiLoCo, the role of error
feedback accumulators and the effect of the inner-optimizer on compressibility
remain under-explored. In this work, we investigate the effectiveness of
standard compression methods including Top-k sparsification and quantization
for reducing the communication overhead of DiLoCo when paired with two local
optimizers (AdamW and Muon). Our experiments pre-training decoder-only
transformer language models (LMs) reveal that leveraging Muon as the inner
optimizer for DiLoCo along with an error-feedback accumulator allows to
aggressively compress the communicated delta to 2-bits with next to no
performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)
significantly outperforms DiLoCo while communicating 8X less and having
identical memory complexity.

</details>


### [161] [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast](https://arxiv.org/abs/2505.23732)
*Shreeram Suresh Chandra, Lucas Goncalves, Junchen Lu, Carlos Busso, Berrak Sisman*

**主要类别:** cs.LG

**概要:** 当前基于情感的对比语言-音频预训练（CLAP）方法通常通过简单对齐音频样本与相应的文本提示来学习。这种方法未能捕捉情感的序性质，阻碍了情感间理解，并且由于对齐不足，经常导致音频和文本嵌入之间的广泛模态差距。为了解决这些缺点，我们引入了EmotionRankCLAP，这是一种监督对比学习方法，利用情感语音和自然语言提示的维度属性，共同捕捉细粒度的情感变化并改善跨模态对齐。该方法使用Rank-N-Contrast目标来通过在价-唤醒空间中的排名对比样本来学习有序关系。EmotionRankCLAP在通过跨模态检索任务测量的多模态情感序性质建模方面优于现有的情感-CLAP方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于情感的对比语言-音频预训练（CLAP）方法存在两个主要问题：1) 无法捕捉情感的序性质；2) 音频和文本嵌入之间的模态差距较大。这些问题阻碍了情感间的理解和跨模态的有效对齐。因此，需要一种新的方法来解决这些问题。

**方法:** 提出了一种名为EmotionRankCLAP的监督对比学习方法。该方法结合了情感语音和自然语言提示的维度属性，以捕捉细粒度的情感变化并改善跨模态对齐。此外，它采用了Rank-N-Contrast目标，通过在价-唤醒空间中对比样本的排名来学习有序关系。

**结果:** 实验结果表明，EmotionRankCLAP在建模多模态情感序性质方面优于现有的情感-CLAP方法。这通过跨模态检索任务得到了验证。

**结论:** EmotionRankCLAP是一种有效的监督对比学习方法，能够通过Rank-N-Contrast目标学习有序关系，从而捕捉细粒度的情感变化并改善跨模态对齐。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EmotionRankCLAP%3A+Bridging+Natural+Language+Speaking+Styles+and+Ordinal+Speech+Emotion+via+Rank-N-Contrast，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23732&send_immediately=true&force_search=false)

**原文摘要:** Current emotion-based contrastive language-audio pretraining (CLAP) methods
typically learn by na\"ively aligning audio samples with corresponding text
prompts. Consequently, this approach fails to capture the ordinal nature of
emotions, hindering inter-emotion understanding and often resulting in a wide
modality gap between the audio and text embeddings due to insufficient
alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a
supervised contrastive learning approach that uses dimensional attributes of
emotional speech and natural language prompts to jointly capture fine-grained
emotion variations and improve cross-modal alignment. Our approach utilizes a
Rank-N-Contrast objective to learn ordered relationships by contrasting samples
based on their rankings in the valence-arousal space. EmotionRankCLAP
outperforms existing emotion-CLAP methods in modeling emotion ordinality across
modalities, measured via a cross-modal retrieval task.

</details>


### [162] [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)
*Paul Gölz, Nika Haghtalab, Kunhe Yang*

**主要类别:** cs.LG

**概要:** 大型语言模型在预训练后，基于成对比较与人类偏好对齐。尽管用户偏好多样化，最先进的对齐方法（如基于PPO的RLHF和DPO）仍假设与单一偏好模型对齐。通过引入社会选择理论和Bradley-Terry模型，本文提出了对齐方法的失真概念：最优可实现平均效用与学习策略平均效用之间的最坏情况比率。Nash Learning from Human Feedback实现了最小最大最优失真，而RLHF和DPO则遭受显著更高的失真。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大型语言模型对齐方法未能充分考虑用户的多样化偏好，可能导致无法满足用户的平均需求。因此，需要一种更精确的方法来评估和改进这些对齐技术。

**方法:** 利用社会选择理论和Bradley-Terry模型，定义了对齐方法的失真概念，并分析了不同对齐方法的失真程度。通过数学推导，比较了Nash Learning from Human Feedback、RLHF和DPO等方法的性能。

**结果:** Nash Learning from Human Feedback在多种条件下表现出最优的失真性能，而RLHF和DPO在无KL约束的情况下失真较高，在完整设置下甚至可能出现无界失真。

**结论:** Nash Learning from Human Feedback是一种更为稳健和有效的对齐方法，能够在多样化的用户偏好中实现更好的性能。相比之下，RLHF和DPO存在较大的失真风险，可能无法满足用户的平均需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distortion+of+AI+Alignment%3A+Does+Preference+Optimization+Optimize+for+Preferences%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23749&send_immediately=true&force_search=false)

**原文摘要:** After pre-training, large language models are aligned with human preferences
based on pairwise comparisons. State-of-the-art alignment methods (such as
PPO-based RLHF and DPO) are built on the assumption of aligning with a single
preference model, despite being deployed in settings where users have diverse
preferences. As a result, it is not even clear that these alignment methods
produce models that satisfy users on average -- a minimal requirement for
pluralistic alignment. Drawing on social choice theory and modeling users'
comparisons through individual Bradley-Terry (BT) models, we introduce an
alignment method's distortion: the worst-case ratio between the optimal
achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment
methods: Nash Learning from Human Feedback achieves the minimax optimal
distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature
$\beta$), robustly across utility distributions, distributions of comparison
pairs, and permissible KL divergences from the reference policy. RLHF and DPO,
by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a
KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full
setting, depending on how comparison pairs are sampled.

</details>


### [163] [Model Immunization from a Condition Number Perspective](https://arxiv.org/abs/2505.23760)
*Amber Yijia Zheng, Cedar Site Bai, Brian Bullins, Raymond A. Yeh*

**主要类别:** cs.LG

**概要:** 本论文提出了一种基于Hessian矩阵条件数的框架，用于分析线性模型的模型免疫，并设计了一个包含正则化项的算法来控制预训练后的条件数。实验证明该算法在线性模型和非线性深度网络上对模型免疫的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先前的研究已经展示了文本到图像模型免疫的经验证据，但对于免疫何时可能以及免疫模型的精确定义仍然不清楚。

**方法:** 作者提出了一种基于Hessian矩阵条件数的框架，用于分析线性模型的模型免疫，并设计了一个包含正则化项的算法以控制预训练后的条件数。

**结果:** 实验结果表明，所提出的算法在线性模型和非线性深度网络上均对模型免疫具有有效性。

**结论:** 通过提出的框架和算法，可以更好地理解和实现模型免疫，从而在保持模型在无害任务上的实用性的同时，使其难以在有害任务上进行微调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+Immunization+from+a+Condition+Number+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23760&send_immediately=true&force_search=false)

**原文摘要:** Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [164] [Design and testing of an agent chatbot supporting decision making with public transport data](https://arxiv.org/abs/2505.22698)
*Luca Fantin, Marco Antonelli, Margherita Cesetti, Daniele Irto, Bruno Zamengo, Francesco Silvestri*

**主要类别:** cs.AI

**概要:** 本论文介绍了一个基于代理架构的聊天机器人，旨在通过自然语言交互简化公共交通服务质量评估中的复杂数据操作任务，如SQL查询、数据可视化等。此外，论文还提出了一种评估聊天机器人性能的方法，确保生成查询的正确性和回答的一致性。


<details>
  <summary>更多</summary>
  
**动机:** 评估公共交通服务质量需要分析大量关于计划和实际行程的数据以及列出每个服务所需满足的质量约束的文档。对于大多数用户而言，使用SQL查询来询问这些数据集、组织和可视化数据可能非常复杂。因此，需要一个更友好的工具来与这些数据集进行交互并支持决策制定。

**方法:** 该聊天机器人基于代理架构，扩展了核心大型语言模型（LLM）的能力，使其能够与一系列工具交互以执行多种任务，例如执行SQL查询、绘制数据图表和根据行程及其停靠点的坐标创建地图。为了衡量系统的性能，研究团队设计了一个工作流程，通过提出多个问题并存储生成的查询、检索到的数据以及自然语言响应来测试聊天机器人。这些问题基于一组基础示例，并用数据库中的实际数据补充完成。

**结果:** 此方法产生了一个用于评估聊天机器人性能的数据集，特别是其回答的一致性和生成查询的正确性。广泛的测试表明，聊天机器人能够有效地支持公共交通服务质量评估中的数据操作任务。

**结论:** 基于代理架构的聊天机器人提供了一个用户友好的工具，可以显著简化与公共交通服务质量评估相关的复杂数据操作任务。同时，论文提出的评估方法为衡量聊天机器人的性能提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Design+and+testing+of+an+agent+chatbot+supporting+decision+making+with+public+transport+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22698&send_immediately=true&force_search=false)

**原文摘要:** Assessing the quality of public transportation services requires the analysis
of large quantities of data on the scheduled and actual trips and documents
listing the quality constraints each service needs to meet. Interrogating such
datasets with SQL queries, organizing and visualizing the data can be quite
complex for most users. This paper presents a chatbot offering a user-friendly
tool to interact with these datasets and support decision making. It is based
on an agent architecture, which expands the capabilities of the core Large
Language Model (LLM) by allowing it to interact with a series of tools that can
execute several tasks, like performing SQL queries, plotting data and creating
maps from the coordinates of a trip and its stops. This paper also tackles one
of the main open problems of such Generative AI projects: collecting data to
measure the system's performance. Our chatbot has been extensively tested with
a workflow that asks several questions and stores the generated query, the
retrieved data and the natural language response for each of them. Such
questions are drawn from a set of base examples which are then completed with
actual data from the database. This procedure yields a dataset for the
evaluation of the chatbot's performance, especially the consistency of its
answers and the correctness of the generated queries.

</details>


### [165] [Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields](https://arxiv.org/abs/2505.22753)
*Arseniy Pertzovsky, Roni Stern, Ariel Felner, Roie Zivan*

**主要类别:** cs.AI

**概要:** Artificial Potential Fields (APFs) were explored for solving Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. APFs didn't benefit MAPF but increased LMAPF system throughput up to 7 times.


<details>
  <summary>更多</summary>
  
**动机:** To improve the efficiency of Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) by incorporating Artificial Potential Fields (APFs) in various algorithms.

**方法:** The study incorporated APFs into several MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and Priority Inheritance with Backtracking (PIBT).

**结果:** Using APFs did not provide benefits for MAPF problems but significantly increased overall system throughput for LMAPF problems by up to a 7-fold factor.

**结论:** Artificial Potential Fields are more effective for Lifelong MAPF problems than for traditional MAPF problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Lifelong+Multi-Agent+Path-finding+by+Using+Artificial+Potential+Fields，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22753，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22753&send_immediately=true&force_search=false)

**原文摘要:** We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent
Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of
agents must move to their goal locations without collisions, whereas in LMAPF,
new goals are generated upon arrival. We propose methods for incorporating APFs
in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and
Priority Inheritance with Backtracking (PIBT). Experimental results show that
using APF is not beneficial for MAPF but yields up to a 7-fold increase in
overall system throughput for LMAPF.

</details>


### [166] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin, Core Francisco Park, Mujin Kwun, Aaron Walsman, Eran Malach, Nikhil Anand, Hidenori Tanaka, David Alvarez-Melis*

**主要类别:** cs.AI

**概要:** 数学推理任务是评估大型语言模型（LLM）推理能力的重要基准，特别是使用强化学习（RL）方法如GRPO展示了显著的性能提升。然而，仅凭准确率指标无法细致评估模型的能力，也无法揭示模型内化的解决问题的技能。为了更好地理解这些能力，我们提出将问题解决分解为基本能力：计划（将问题映射到一系列步骤）、执行（正确执行解决方案步骤）和验证（识别解决方案的正确性）。实证研究发现，GRPO主要增强了执行能力——提高了模型对已知如何解决的问题的执行稳健性，这种现象被称为温度蒸馏。更重要的是，我们发现通过RL训练的模型在面对根本性的新问题时存在困难，由于规划能力不足而遭遇了“覆盖墙”。为了更深入地探索RL的影响，我们构建了一个最小的、合成的解决方案树导航任务，作为数学问题解决的类比。这个受控设置复制了我们的实证发现，确认RL主要提高执行稳健性。重要的是，在这种设置下，我们确定了RL通过改进探索和对新解决方案路径的泛化，潜在地克服覆盖墙的条件。我们的研究结果提供了关于RL在增强LLM推理中的作用的见解，暴露了关键限制，并提出了克服这些障碍的路径。


<details>
  <summary>更多</summary>
  
**动机:** 当前对于LLM在数学推理任务上的评估主要依赖于准确率指标，这无法全面反映模型的能力，尤其是其内化的解决问题的技能。因此，需要一种更细粒度的方法来评估和理解这些能力。

**方法:** 1. 提出将问题解决过程分解为三个基本能力：计划、执行和验证。
2. 使用GRPO等RL方法进行实验，观察其对不同能力的影响。
3. 构建一个最小的、合成的解决方案树导航任务，以模拟数学问题解决过程并验证RL的效果。
4. 分析RL在提高执行稳健性和克服覆盖墙方面的潜力。

**结果:** 1. GRPO主要增强了执行能力，提高了模型对已知问题的执行稳健性（温度蒸馏现象）。
2. RL训练的模型在面对新问题时遇到困难，显示出规划能力不足导致的“覆盖墙”现象。
3. 通过合成任务验证了RL主要提升执行稳健性，但在特定条件下可以改进探索和泛化能力，从而克服覆盖墙。

**结论:** 强化学习在增强LLM的执行稳健性方面表现出色，但其在规划能力上的局限性使其难以应对全新的问题。研究揭示了RL在数学推理任务中的作用及其关键限制，并为未来改进指明了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decomposing+Elements+of+Problem+Solving%3A+What+%22Math%22+Does+RL+Teach%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22756，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22756&send_immediately=true&force_search=false)

**原文摘要:** Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [167] [Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds](https://arxiv.org/abs/2505.22779)
*Mohammad Helal Uddin, Sabur Baidya*

**主要类别:** cs.AI

**概要:** 本研究提出了一种结合智能手机传感器数据和社交媒体分析的混合方法，用于评估个人抑郁水平。通过深度学习和朴素贝叶斯分类准确识别身体活动并分类用户情绪。实验招募了33名参与者，从身体活动和Twitter活动提取特征，并与抑郁评分相关联。使用支持向量机算法对抑郁严重程度进行分类，准确率达到94%。此方法简单且有效保护隐私。


<details>
  <summary>更多</summary>
  
**动机:** 精神障碍（如抑郁症、焦虑症等）在全球范围内构成了重大挑战，尤其是在表现出社交回避倾向的人群中。为了更好地评估个体的抑郁水平，本研究试图利用日常生活中常见的智能手机传感器数据和社交媒体互动来实现这一目标。

**方法:** 1. 使用CNN-based深度学习模型和朴素贝叶斯分类来识别人类的身体活动并分类用户情绪。
2. 招募33名参与者以获取数据，并从身体活动和社交媒体（Twitter）活动中提取九个相关特征。
3. 使用老年抑郁量表（GDS）问卷评估每周抑郁评分。
4. 运用支持向量机（SVM）算法对抑郁严重程度进行分类，并与其他模型（如多层感知器MLP和k-最近邻）进行比较。

**结果:** 1. 身体活动识别准确率达到95%。
2. 情感分析准确率达到95.6%。
3. 支持向量机算法在抑郁严重程度分类上的准确率达到94%，优于其他模型。
4. 若干身体活动特征与抑郁症状的严重程度显著相关。

**结论:** 本研究提出的混合方法通过结合智能手机传感器数据和社交媒体分析，能够有效评估个体的抑郁水平。支持向量机算法表现优异，为长期监测抑郁提供了一个简单而高效的方法，同时尊重个人隐私。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Human+Depression+with+Hybrid+Data+Acquisition+utilizing+Physical+Activity+Sensing+and+Social+Media+Feeds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22779，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22779&send_immediately=true&force_search=false)

**原文摘要:** Mental disorders including depression, anxiety, and other neurological
disorders pose a significant global challenge, particularly among individuals
exhibiting social avoidance tendencies. This study proposes a hybrid approach
by leveraging smartphone sensor data measuring daily physical activities and
analyzing their social media (Twitter) interactions for evaluating an
individual's depression level. Using CNN-based deep learning models and Naive
Bayes classification, we identify human physical activities accurately and also
classify the user sentiments. A total of 33 participants were recruited for
data acquisition, and nine relevant features were extracted from the physical
activities and analyzed with their weekly depression scores, evaluated using
the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six
are derived from physical activities, achieving an activity recognition
accuracy of 95%, while three features stem from sentiment analysis of Twitter
activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several
physical activity features exhibited significant correlations with the severity
of depression symptoms. For classifying the depression severity, a support
vector machine (SVM)-based algorithm is employed that demonstrated a very high
accuracy of 94%, outperforming alternative models, e.g., the multilayer
perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly
effective in the long run for monitoring depression without breaching personal
privacy.

</details>


### [168] [The WHY in Business Processes: Unification of Causal Process Models](https://arxiv.org/abs/2505.22871)
*Yuval David, Fabiana Fournier, Lior Limonad, Inna Skarbovsky*

**主要类别:** cs.AI

**概要:** 论文提出了一种新方法，将多个因果流程变体统一到一个保持原始因果模型正确性的模型中，并明确表示其因果流交替。该方法经过正式定义、证明，并在五个数据集上进行了评估，同时作为开源实现发布。


<details>
  <summary>更多</summary>
  
**动机:** 现有的发现因果流程模型的方法无法捕捉多个变体间的交替因果条件，处理缺失值和表达日志拆分中的交替条件存在挑战。

**方法:** 提出了一种新的方法，可以将多个因果流程变体统一到一个一致的模型中，同时保留原始因果模型的正确性并明确表示因果流交替。

**结果:** 该方法经过正式定义、证明，并在三个公开和两个专有数据集上进行了评估，结果表明其有效性和可行性。

**结论:** 所提出的方法能够成功地统一多个因果流程变体，并保持原始模型的正确性，同时提供了开源实现以供使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+WHY+in+Business+Processes%3A+Unification+of+Causal+Process+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22871&send_immediately=true&force_search=false)

**原文摘要:** Causal reasoning is essential for business process interventions and
improvement, requiring a clear understanding of causal relationships among
activity execution times in an event log. Recent work introduced a method for
discovering causal process models but lacked the ability to capture alternating
causal conditions across multiple variants. This raises the challenges of
handling missing values and expressing the alternating conditions among log
splits when blending traces with varying activities.
  We propose a novel method to unify multiple causal process variants into a
consistent model that preserves the correctness of the original causal models,
while explicitly representing their causal-flow alternations. The method is
formally defined, proved, evaluated on three open and two proprietary datasets,
and released as an open-source implementation.

</details>


### [169] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
*Massimiliano Pronesti, Michela Lorandi, Paul Flanagan, Oisin Redmon, Anya Belz, Yufang Hou*

**主要类别:** cs.AI

**概要:** 通过将问题视为定量推理任务，开发了一个由数值数据提取模型和效应估计组件组成的系统。在CochraneForest基准测试中，表现最佳的方法（使用RL训练的小规模数值提取模型）比检索基础系统提高了21%的F1分数，且优于400B参数的大规模语言模型。


<details>
  <summary>更多</summary>
  
**动机:** 目前自动化系统评价过程中的主要瓶颈是从研究中提取数值证据并确定特定结果和比较的研究级结论。先前方法依赖浅层文本线索，未能捕捉专家评估背后的数值推理。

**方法:** 提出了一种定量推理方法，从表面文本中提取结构化数值证据，并应用领域知识逻辑得出特定结果的结论。构建了数值推理系统，包括数值数据提取模型和效应估计组件，使用监督微调(SFT)和强化学习(RL)等策略进行训练。

**结果:** 在CochraneForest基准测试中，表现最佳的RL训练小规模数值提取模型相比检索基础系统绝对提升了21%的F1分数，且优于超过400B参数的大规模语言模型高达9%。

**结论:** 基于推理驱动的方法在自动化系统评价综合方面显示出巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Study-Level+Inference+from+Clinical+Trial+Papers+via+RL-based+Numeric+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22928&send_immediately=true&force_search=false)

**原文摘要:** Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [170] [Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages](https://arxiv.org/abs/2505.22948)
*Michael Sun, Weize Yuan, Gang Liu, Wojciech Matusik, Jie Chen*

**主要类别:** cs.AI

**概要:** 提出了一种名为Foundation Molecular Grammar (FMG)的新方法，利用多模态基础模型（MMFMs）来生成可解释的分子语言。FMG将分子渲染为图像、用文本描述它们，并通过提示学习在模态间对齐信息。与之前的语法学习方法相比，FMG在合成能力、多样性和数据效率方面表现出色，同时提供了内置的化学可解释性，有助于自动化的分子发现工作流程。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据高效分子生成方法利用图语法引入可解释性，但语法学习依赖于专家注释或不可靠的启发式算法推断。

**方法:** 提出了Foundation Molecular Grammar (FMG)，利用多模态基础模型(MMFMs)的化学知识，将分子渲染为图像、用文本描述它们并通过提示学习在模态间对齐信息。

**结果:** FMG在分子生成和性质预测中表现出色，不仅提高了合成能力、多样性和数据效率，还提供了内置的化学可解释性。

**结论:** FMG可以作为先前语法学习方法的替代方案，在自动化分子发现工作流程中提供更好的性能和可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+Molecular+Grammar%3A+Multi-Modal+Foundation+Models+Induce+Interpretable+Molecular+Graph+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22948，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22948&send_immediately=true&force_search=false)

**原文摘要:** Recent data-efficient molecular generation approaches exploit graph grammars
to introduce interpretability into the generative models. However, grammar
learning therein relies on expert annotation or unreliable heuristics for
algorithmic inference. We propose Foundation Molecular Grammar (FMG), which
leverages multi-modal foundation models (MMFMs) to induce an interpretable
molecular language. By exploiting the chemical knowledge of an MMFM, FMG
renders molecules as images, describes them as text, and aligns information
across modalities using prompt learning. FMG can be used as a drop-in
replacement for the prior grammar learning approaches in molecular generation
and property prediction. We show that FMG not only excels in synthesizability,
diversity, and data efficiency but also offers built-in chemical
interpretability for automated molecular discovery workflows. Code is available
at https://github.com/shiningsunnyday/induction.

</details>


### [171] [Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/abs/2505.22954)
*Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, Jeff Clune*

**主要类别:** cs.AI

**概要:** 引入了达尔文哥德尔机器（DGM），一种自我改进的系统，通过迭代修改自身代码并使用编码基准进行验证，从而自动提升编码能力。实验表明，DGM在多个基准上显著优于没有自我改进机制的基线模型，并且在安全预防措施下运行。


<details>
  <summary>更多</summary>
  
**动机:** 当前的AI系统具有人类设计的固定架构，无法自主和持续地自我改进。元学习虽然可以自动化发现新算法，但受限于一阶改进和人为设计的搜索空间。因此需要一种新的方法来实现AI的自我改进。

**方法:** DGM结合达尔文进化论和开放式研究的灵感，维护一个生成的编码代理档案库，通过采样代理和基础模型创建新的版本，形成不断增长的高质量代理树。同时，DGM通过沙盒、人工监督等安全措施确保系统的安全性。

**结果:** DGM在SWE-bench上的表现从20.0%提升到50.0%，在Polyglot上的表现从14.2%提升到30.7%，显著优于没有自我改进或开放式探索的基线模型。

**结论:** DGM是迈向自我改进AI的重要一步，能够在路径上自行收集垫脚石，展开无尽创新。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Darwin+Godel+Machine%3A+Open-Ended+Evolution+of+Self-Improving+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22954&send_immediately=true&force_search=false)

**原文摘要:** Today's AI systems have human-designed, fixed architectures and cannot
autonomously and continuously improve themselves. The advance of AI could
itself be automated. If done safely, that would accelerate AI development and
allow us to reap its benefits much sooner. Meta-learning can automate the
discovery of novel algorithms, but is limited by first-order improvements and
the human design of a suitable search space. The G\"odel machine proposed a
theoretical alternative: a self-improving AI that repeatedly modifies itself in
a provably beneficial manner. Unfortunately, proving that most changes are net
beneficial is impossible in practice. We introduce the Darwin G\"odel Machine
(DGM), a self-improving system that iteratively modifies its own code (thereby
also improving its ability to modify its own codebase) and empirically
validates each change using coding benchmarks. Inspired by Darwinian evolution
and open-endedness research, the DGM maintains an archive of generated coding
agents. It grows the archive by sampling an agent from it and using a
foundation model to create a new, interesting, version of the sampled agent.
This open-ended exploration forms a growing tree of diverse, high-quality
agents and allows the parallel exploration of many different paths through the
search space. Empirically, the DGM automatically improves its coding
capabilities (e.g., better code editing tools, long-context window management,
peer-review mechanisms), increasing performance on SWE-bench from 20.0% to
50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly
outperforms baselines without self-improvement or open-ended exploration. All
experiments were done with safety precautions (e.g., sandboxing, human
oversight). The DGM is a significant step toward self-improving AI, capable of
gathering its own stepping stones along paths that unfold into endless
innovation.

</details>


### [172] [Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness](https://arxiv.org/abs/2505.22960)
*Yongjin Yang, Euiin Yi, Jongwoo Ko, Kimin Lee, Zhijing Jin, Se-Young Yun*

**主要类别:** cs.AI

**概要:** 论文探讨了多智能体辩论(MAD)方法在数学推理和安全任务中的表现，发现其优势取决于问题难度和模型能力，并揭示了智能体多样性的影响。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有研究利用MAD方法，但对其在不同条件下的有效性与自智能体方法的系统性对比仍缺乏了解。

**方法:** 将MAD概念化为测试时计算扩展技术，通过实证研究比较MAD与强大的自智能体测试时扩展基线在数学推理和安全相关任务上的表现，系统考察任务难度、模型规模和智能体多样性对MAD性能的影响。

**结果:** 对于数学推理，MAD在自智能体扩展之上提供的优势有限，但在问题难度增加或模型能力下降时更有效，智能体多样性影响较小；对于安全任务，MAD的协作改进可能增加漏洞，但通过引入多样化的智能体配置可逐步降低攻击成功率。

**结论:** 研究结果为未来更有效和更具策略性的MAD系统发展提供了关键指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Multi-Agent+Debate+as+Test-Time+Scaling%3A+A+Systematic+Study+of+Conditional+Effectiveness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22960，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22960&send_immediately=true&force_search=false)

**原文摘要:** The remarkable growth in large language model (LLM) capabilities has spurred
exploration into multi-agent systems, with debate frameworks emerging as a
promising avenue for enhanced problem-solving. These multi-agent debate (MAD)
approaches, where agents collaboratively present, critique, and refine
arguments, potentially offer improved reasoning, robustness, and diverse
perspectives over monolithic models. Despite prior studies leveraging MAD, a
systematic understanding of its effectiveness compared to self-agent methods,
particularly under varying conditions, remains elusive. This paper seeks to
fill this gap by conceptualizing MAD as a test-time computational scaling
technique, distinguished by collaborative refinement and diverse exploration
capabilities. We conduct a comprehensive empirical investigation comparing MAD
with strong self-agent test-time scaling baselines on mathematical reasoning
and safety-related tasks. Our study systematically examines the influence of
task difficulty, model scale, and agent diversity on MAD's performance. Key
findings reveal that, for mathematical reasoning, MAD offers limited advantages
over self-agent scaling but becomes more effective with increased problem
difficulty and decreased model capability, while agent diversity shows little
benefit. Conversely, for safety tasks, MAD's collaborative refinement can
increase vulnerability, but incorporating diverse agent configurations
facilitates a gradual reduction in attack success through the collaborative
refinement process. We believe our findings provide critical guidance for the
future development of more effective and strategically deployed MAD systems.

</details>


### [173] [Strategic Reflectivism In Intelligent Systems](https://arxiv.org/abs/2505.22987)
*Nick Byrd*

**主要类别:** cs.AI

**概要:** 21世纪的认知科学家将二元过程理论应用于人工智能，本文综合了历史教训与近期人类和机器实验的结果，提出了战略性反思主义（Strategic Reflectivism），强调智能系统在直觉和反思推理之间进行实用切换以最优实现竞争目标。


<details>
  <summary>更多</summary>
  
**动机:** 鉴于21世纪认知科学家正在将源于20世纪理性论战的双过程理论应用于人工智能，作者认为有必要回顾历史教训，并结合现代实验结果提出新的理论。

**方法:** 综合20世纪关于直觉和反思思维的辩论、有限理性等概念，以及21世纪对人类和机器的实验数据，构建战略性反思主义理论。

**结果:** 提出了战略性反思主义，指出智能系统的关键在于在直觉和反思推理之间进行实用切换，以最优方式实现竞争目标。

**结论:** 战略性反思主义基于美国实用主义，超越了如模型规模或思维链等反思性思维的表面指标，随着我们对直觉和反思价值了解的增加，其可操作性不断增强。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strategic+Reflectivism+In+Intelligent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22987&send_immediately=true&force_search=false)

**原文摘要:** By late 20th century, the rationality wars had launched debates about the
nature and norms of intuitive and reflective thinking. Those debates drew from
mid-20th century ideas such as bounded rationality, which challenged more
idealized notions of rationality observed since the 19th century. Now that 21st
century cognitive scientists are applying the resulting dual process theories
to artificial intelligence, it is time to dust off some lessons from this
history. So this paper synthesizes old ideas with recent results from
experiments on humans and machines. The result is Strategic Reflectivism, which
takes the position that one key to intelligent systems (human or artificial) is
pragmatic switching between intuitive and reflective inference to optimally
fulfill competing goals. Strategic Reflectivism builds on American Pragmatism,
transcends superficial indicators of reflective thinking such as model size or
chains of thought, and becomes increasingly actionable as we learn more about
the value of intuition and reflection.

</details>


### [174] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/abs/2505.22990)
*Pin-Han Chen, Yu-Sheng Lin, Wei-Cheng Lee, Tin-Yu Leu, Po-Hsiang Hsu, Anjana Dissanayake, Sungjin Oh, Chinq-Shiun Chiu*

**主要类别:** cs.AI

**概要:** 开发了MenTeR，一个多智能体工作流框架，用于端到端模拟设计，减少人工干预，加速设计周期，并探索更广泛的设计空间。


<details>
  <summary>更多</summary>
  
**动机:** 模拟设计过程复杂、耗时且依赖专家直觉，影响电路开发的时间和成本效率。

**方法:** 通过引入多个专门的AI代理，协作处理设计过程的不同方面（如规范理解、电路优化和测试平台验证），以减少频繁试错干预的需求。

**结果:** MenTeR不仅加快了设计周期时间，还促进了对设计空间的更广泛探索，展现出处理现实世界模拟系统的强大能力。

**结论:** MenTeR为未来的“RF/模拟副驾驶”奠定了基础，可以与人类设计师无缝协作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MenTeR%3A+A+fully-automated+Multi-agenT+workflow+for+end-to-end+RF%2FAnalog+Circuits+Netlist+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22990&send_immediately=true&force_search=false)

**原文摘要:** RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [175] [Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction](https://arxiv.org/abs/2505.23034)
*Guangyi Liu, Yongqi Zhang, Xunyuan Liu, Quanming Yao*

**主要类别:** cs.AI

**概要:** 提出CBR-DDI框架，结合LLM和GNN，通过案例推理提升药物相互作用预测性能，实验表明其性能优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型在制药任务中显示出潜力，但在药物-药物相互作用（DDI）预测方面仍存在挑战。受临床实践中基于案例推理的启发，提出了CBR-DDI框架以改善LLM在DDI任务中的推理能力。

**方法:** CBR-DDI框架利用LLM提取药理学见解，并用图神经网络（GNN）建模药物关联，构建知识库。采用混合检索机制和双层知识增强提示使LLM能有效检索和重用相关案例，同时引入代表性抽样策略进行动态案例优化。

**结果:** 广泛的实验证明，CBR-DDI达到了最先进的性能，相较于流行的LLM和CBR基线，准确率提高了28.7%，同时保持了高可解释性和灵活性。

**结论:** CBR-DDI框架成功地将药理学原理从历史案例中提炼出来，改进了LLM在DDI预测任务中的表现，展现了高性能、可解释性和灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Case-Based+Reasoning+Enhances+the+Predictive+Power+of+LLMs+in+Drug-Drug+Interaction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23034&send_immediately=true&force_search=false)

**原文摘要:** Drug-drug interaction (DDI) prediction is critical for treatment safety.
While large language models (LLMs) show promise in pharmaceutical tasks, their
effectiveness in DDI prediction remains challenging. Inspired by the
well-established clinical practice where physicians routinely reference similar
historical cases to guide their decisions through case-based reasoning (CBR),
we propose CBR-DDI, a novel framework that distills pharmacological principles
from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI
constructs a knowledge repository by leveraging LLMs to extract pharmacological
insights and graph neural networks (GNNs) to model drug associations. A hybrid
retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to
effectively retrieve and reuse relevant cases. We further introduce a
representative sampling strategy for dynamic case refinement. Extensive
experiments demonstrate that CBR-DDI achieves state-of-the-art performance,
with a significant 28.7% accuracy improvement over both popular LLMs and CBR
baseline, while maintaining high interpretability and flexibility.

</details>


### [176] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
*Yutong Xie, Zhuoheng Li, Xiyuan Wang, Yijun Pan, Qijia Liu, Xingzhi Cui, Kuang-Yu Lo, Ruoyi Gao, Xingjian Zhang, Jin Huang, Walter Yuan, Matthew O. Jackson, Qiaozhu Mei*

**主要类别:** cs.AI

**概要:** 尽管基础模型在许多领域取得了成功，但它们在建模和理解人类行为方面的潜力尚未得到充分探索。我们引入了Be.FM，这是最早公开的基础模型之一，专为人类行为建模而设计。基于开源大型语言模型构建并使用多样化的行为数据进行微调，Be.FM可用于理解和预测人类决策。我们构建了一套全面的基准任务来测试行为基础模型的能力。我们的结果表明，Be.FM可以预测行为，推断个人和群体特征，生成关于情境的见解，并应用行为科学知识。


<details>
  <summary>更多</summary>
  
**动机:** 探索基础模型在建模和理解人类行为方面的潜力，填补该领域的研究空白。

**方法:** 引入名为Be.FM的开放基础模型，该模型基于开源大型语言模型构建，并在多样化的行为数据上进行微调。同时，构建了一套全面的基准任务来评估模型的能力。

**结果:** Be.FM能够预测行为、推断个人和群体特征、生成关于情境的见解以及应用行为科学知识。

**结论:** Be.FM作为首个公开的人类行为建模基础模型，在理解和预测人类决策方面展现出了强大的能力，为未来的研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Be.FM%3A+Open+Foundation+Models+for+Human+Behavior，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23058，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23058&send_immediately=true&force_search=false)

**原文摘要:** Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [177] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/abs/2505.23075)
*Amit Kumthekar, Zion Tilley, Henry Duong, Bhargav Patel, Michael Magnoli, Ahmed Omar, Ahmed Nasser, Chaitanya Gharpure, Yevgen Reztzov*

**主要类别:** cs.AI

**概要:** 论文提出了一种名为共识机制的新框架，通过模仿临床分诊和多学科决策，结合多个医疗专家代理模型以提升临床决策能力，并在多个医疗基准测试中表现优于现有模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型在临床应用中的方法主要依赖单一模型架构，存在过时风险和对单个模型系统的刚性依赖问题。为解决这一问题，提出了一种新的框架——共识机制。

**方法:** 该方法通过集成多个专业医疗专家代理模型，模仿临床分诊和多学科临床决策过程，从而提高临床决策能力和适应性。此外，其内部模型配置可以根据成本、延迟或性能进行优化。

**结果:** 在MedXpertQA、MedQA和MedMCQA等多个基准测试中，共识机制表现出显著的准确率提升，分别比OpenAI的O3和Google的Gemini 2.5 Pro高出7.5%和15.1%。在差异诊断生成方面，系统也显示出更高的召回率、精确率和F1分数。

**结论:** 共识机制提供了一种有效的解决方案来减少对单一模型的依赖，同时提高了临床决策的准确性和适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Second+Opinion+Matters%3A+Towards+Adaptive+Clinical+AI+via+the+Consensus+of+Expert+Model+Ensemble，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23075，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23075&send_immediately=true&force_search=false)

**原文摘要:** Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [178] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
*Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang*

**主要类别:** cs.AI

**概要:** 近期大型语言模型（LLMs）在推理能力方面取得了显著进展，但将这些成就扩展到多模态小型语言模型（MSLMs）面临诸多挑战。为此，本文提出了一种名为Infi-MMR的新框架及相应的多模态推理模型Infi-MMR-3B，通过三个阶段的系统性训练，有效提升了MSLMs的推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在推理能力上取得巨大进步，但在多模态领域，特别是多模态小型语言模型中，仍存在高质量数据缺乏、视觉处理影响推理能力和强化学习可能导致错误推理过程等关键问题。

**方法:** 设计了名为Infi-MMR的新型框架，包括三个阶段：基础推理激活（利用高质量文本推理数据集）、跨模态推理适应（使用带有标题的多模态数据进行技能迁移）和多模态推理增强（采用无标题的多模态数据减少语言偏差）。

**结果:** Infi-MMR-3B在多模态数学推理和一般推理任务上均表现出色，具体为MathVerse测试（43.68%）、MathVision测试（27.04%）、OlympiadBench测试（21.33%）以及MathVista测试（67.2%）。

**结论:** 提出的Infi-MMR框架能够系统地挖掘多模态小型语言模型的推理潜力，并且Infi-MMR-3B在多项基准测试中展现了最先进的推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Infi-MMR%3A+Curriculum-based+Unlocking+Multimodal+Reasoning+via+Phased+Reinforcement+Learning+in+Multimodal+Small+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23091，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23091&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [179] [Conceptual Framework Toward Embodied Collective Adaptive Intelligence](https://arxiv.org/abs/2505.23153)
*Fan Wang, Shaoshan Liu*

**主要类别:** cs.AI

**概要:** Collective Adaptive Intelligence (CAI) 是一种变革性的人工智能方法，通过多个自主代理的合作、适应和自组织，在复杂动态环境中表现出色。本文提出了一个概念框架来设计和分析 CAI，并强调了任务泛化、弹性、可扩展性和自组装等关键属性，旨在为开发更强大、灵活和可扩展的 AI 系统提供理论与实践指导。


<details>
  <summary>更多</summary>
  
**动机:** 当前人工智能系统在面对复杂动态环境时缺乏足够的适应性和弹性，因此需要一种新的方法（CAI）来提升系统在未知挑战中的表现能力。

**方法:** 提出一个概念框架用于设计和分析 CAI，并明确了任务泛化、弹性、可扩展性和自组装等关键属性，以将理论基础与实际工程方法相结合。

**结果:** 该框架能够帮助研究者理解并实施 CAI，从而推动开发出更具弹性、可扩展性和适应性的 AI 系统。

**结论:** 通过提供结构化的理论和实践基础，本文为跨领域的研究人员和实践者提供了指导，以促进更强大的自适应智能系统的开发。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conceptual+Framework+Toward+Embodied+Collective+Adaptive+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23153，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23153&send_immediately=true&force_search=false)

**原文摘要:** Collective Adaptive Intelligence (CAI) represent a transformative approach in
artificial intelligence, wherein numerous autonomous agents collaborate, adapt,
and self-organize to navigate complex, dynamic environments. This paradigm is
particularly impactful in embodied AI applications, where adaptability and
resilience are paramount. By enabling systems to reconfigure themselves in
response to unforeseen challenges, CAI facilitate robust performance in
real-world scenarios. This article introduces a conceptual framework for
designing and analyzing CAI. It delineates key attributes including task
generalization, resilience, scalability, and self-assembly, aiming to bridge
theoretical foundations with practical methodologies for engineering adaptive,
emergent intelligence. By providing a structured foundation for understanding
and implementing CAI, this work seeks to guide researchers and practitioners in
developing more resilient, scalable, and adaptable AI systems across various
domains.

</details>


### [180] [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/abs/2505.23281)
*Mislav Balunović, Jasper Dekoninck, Ivo Petrov, Nikola Jovanović, Martin Vechev*

**主要类别:** cs.AI

**概要:** 大型语言模型（LLMs）在数学基准测试上取得了显著进步，但现有数据集可能混杂了记忆而非真实推理能力。为解决此问题，我们引入了MathArena基准，利用实时数学竞赛题评估模型，发现AIME 2024存在污染风险，而其他较新的竞赛显示模型有强大推理能力。此外，MathArena首次评估了模型的证明写作能力，结果显示顶级模型在USAMO 2025上的表现仍远低于人类水平。


<details>
  <summary>更多</summary>
  
**动机:** 当前许多常用的数学评测数据集（如AIME 2024）在线上广泛可得，可能导致模型通过记忆而非真实推理能力取得高分；同时，现有基准未充分评估对数学任务至关重要的证明写作能力。

**方法:** 提出名为MathArena的新基准，基于不断更新的数学竞赛题目进行实时评估，以消除潜在的记忆干扰，并首次纳入证明写作能力的评测。通过新发布的竞赛题目评估模型性能，确保结果反映真实的推理能力而非记忆能力。

**结果:** 1. 发现AIME 2024存在明显的污染风险；2. 在更难且发布时间晚于模型发布日期的竞赛（如SMT 2025）中，顶级模型展现出强大的推理能力；3. 在USAMO 2025证明写作任务中，顶级模型得分低于25%，远低于最终答案任务的表现；4. 已评估30个模型在五个竞赛中的149个问题上的表现。

**结论:** MathArena提供了一种有效评估LLMs数学推理和证明写作能力的方法，未来将继续跟踪新竞赛中模型的表现，确保严格的、与时俱进的评测标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MathArena%3A+Evaluating+LLMs+on+Uncontaminated+Math+Competitions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23281&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of reasoning capabilities in large language models
(LLMs) has led to notable improvements on mathematical benchmarks. However,
many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely
available online, making it difficult to disentangle genuine reasoning from
potential memorization. Furthermore, these benchmarks do not evaluate
proof-writing capabilities, which are crucial for many mathematical tasks. To
address this, we introduce MathArena, a new benchmark based on the following
key insight: recurring math competitions provide a stream of high-quality,
challenging problems that can be used for real-time evaluation of LLMs. By
evaluating models as soon as new problems are released, we effectively
eliminate the risk of contamination. Using this framework, we find strong signs
of contamination in AIME 2024. Nonetheless, evaluations on harder competitions,
such as SMT 2025 -- published well after model release dates -- demonstrate
impressive reasoning capabilities in top-performing models. MathArena is also
the first benchmark for proof-writing capabilities. On USAMO 2025, even top
models score below 25%, far behind their performance on final-answer tasks. So
far, we have evaluated 30 models across five competitions, totaling 149
problems. As an evolving benchmark, MathArena will continue to track the
progress of LLMs on newly released competitions, ensuring rigorous and
up-to-date evaluation of mathematical reasoning.

</details>


### [181] [AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning](https://arxiv.org/abs/2505.23381)
*Bowen Ping, Minnan Luo, Zhuohang Dang, Chenxi Wang, Chengyou Jia*

**主要类别:** cs.AI

**概要:** 本文提出了一种新的神经-符号协作框架AutoGPS，用于解决几何问题。该框架包含多模态问题形式化模块（MPF）和演绎符号推理模块（DSR）。实验表明，AutoGPS在基准数据集上表现出色，并具有高度的可靠性和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 当前解决几何问题的方法主要分为基于神经网络和基于符号的方法，但两者在可靠性和可解释性方面均存在局限性。

**方法:** AutoGPS采用了一个多模态问题形式化模块（MPF）和一个演绎符号推理模块（DSR）。MPF利用神经交叉模态理解将几何问题转化为结构化的正式语言表示，并从DSR获得反馈。DSR以形式化表示为输入，将几何问题求解视为超图扩展任务，生成数学严谨且易于人类理解的逐步解决方案。

**结果:** 实验结果表明，AutoGPS在基准数据集上达到了最先进的性能水平。此外，人工评估显示AutoGPS的逐步推理逻辑一致性高达99%。

**结论:** AutoGPS提供了一种可靠的、可解释的几何问题解决方法，显著提高了现有技术的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoGPS%3A+Automated+Geometry+Problem+Solving+via+Multimodal+Formalization+and+Deductive+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23381&send_immediately=true&force_search=false)

**原文摘要:** Geometry problem solving presents distinctive challenges in artificial
intelligence, requiring exceptional multimodal comprehension and rigorous
mathematical reasoning capabilities. Existing approaches typically fall into
two categories: neural-based and symbolic-based methods, both of which exhibit
limitations in reliability and interpretability. To address this challenge, we
propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry
problems with concise, reliable, and human-interpretable reasoning processes.
Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a
Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal
comprehension to translate geometry problems into structured formal language
representations, with feedback from DSR collaboratively. The DSR takes the
formalization as input and formulates geometry problem solving as a hypergraph
expansion task, executing mathematically rigorous and reliable derivation to
produce minimal and human-readable stepwise solutions. Extensive experimental
evaluations demonstrate that AutoGPS achieves state-of-the-art performance on
benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms
AutoGPS's impressive reliability and interpretability, with 99\% stepwise
logical coherence. The project homepage is at
https://jayce-ping.github.io/AutoGPS-homepage.

</details>


### [182] [A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy](https://arxiv.org/abs/2505.23397)
*Ahmad Mohsin, Helge Janicke, Ahmed Ibrahim, Iqbal H. Sarker, Seyit Camtepe*

**主要类别:** cs.AI

**概要:** 本文提出一个用于安全运营中心（SOC）的人工智能协作框架，包含五个自主层级，强调信任校准和人机协作，通过案例验证了其在减少警报疲劳和增强响应协调方面的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有SOC框架过于关注自动化，缺乏系统性结构来管理人类监督、信任校准和与AI协作的可扩展自主性。

**方法:** 提出了一种基于五个级别（从手动到完全自动）的新型自主分层框架，并将其映射到人机协作角色和任务特定的信任阈值。通过模拟网络范围展示了该框架，其中包括基于LLM的安全操作中心助手AI-Avatar。

**结果:** 该框架能够适应性地在各种SOC功能中集成可解释的人工智能，包括监控、保护、威胁检测、警报分类和事件响应。案例研究显示，人机协作可以减少警报疲劳、增强响应协调并战略性地校准信任。

**结论:** 本文系统地展示了设计下一代认知SOC的理论和实践可行性，利用AI来增强而不是取代人类决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Unified+Framework+for+Human+AI+Collaboration+in+Security+Operations+Centers+with+Trusted+Autonomy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23397，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23397&send_immediately=true&force_search=false)

**原文摘要:** This article presents a structured framework for Human-AI collaboration in
Security Operations Centers (SOCs), integrating AI autonomy, trust calibration,
and Human-in-the-loop decision making. Existing frameworks in SOCs often focus
narrowly on automation, lacking systematic structures to manage human
oversight, trust calibration, and scalable autonomy with AI. Many assume static
or binary autonomy settings, failing to account for the varied complexity,
criticality, and risk across SOC tasks considering Humans and AI collaboration.
To address these limitations, we propose a novel autonomy tiered framework
grounded in five levels of AI autonomy from manual to fully autonomous, mapped
to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This
enables adaptive and explainable AI integration across core SOC functions,
including monitoring, protection, threat detection, alert triage, and incident
response. The proposed framework differentiates itself from previous research
by creating formal connections between autonomy, trust, and HITL across various
SOC levels, which allows for adaptive task distribution according to
operational complexity and associated risks. The framework is exemplified
through a simulated cyber range that features the cybersecurity AI-Avatar, a
fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates
human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing
response coordination, and strategically calibrating trust. This research
systematically presents both the theoretical and practical aspects and
feasibility of designing next-generation cognitive SOCs that leverage AI not to
replace but to enhance human decision-making.

</details>


### [183] [GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning](https://arxiv.org/abs/2505.23399)
*Jusheng Zhang, Yijia Fan, Wenjun Lin, Ruiqi Chen, Haoyi Jiang, Wenhao Chai, Jian Wang, Keze Wang*

**主要类别:** cs.AI

**概要:** 提出GAM-Agent框架，通过多轮辩论和不确定性感知控制器提升视觉-语言推理的准确性和可解释性，在多个基准上显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的单智能体或多块模型在视觉-语言推理方面存在局限性，特别是在处理复杂任务时的鲁棒性和可解释性不足。

**方法:** 将推理过程建模为非零和博弈，包含专注于不同视觉感知子任务的基础智能体和验证逻辑一致性和事实正确性的关键智能体，通过结构化声明、证据和不确定性估计进行通信，并引入不确定性感知控制器动态调整智能体协作。

**结果:** 在四个具有挑战性的基准测试中，GAM-Agent在各种VLM骨干网络上显著提升了性能，特别是在中小型模型上提高了5-6%，即使在强大的模型如GPT-4o上也提升了2-3%。

**结论:** GAM-Agent提供了一种模块化、可扩展和通用的方法，朝着可靠和可解释的多智能体多模态推理迈出了重要一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GAM-Agent%3A+Game-Theoretic+and+Uncertainty-Aware+Collaboration+for+Complex+Visual+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23399&send_immediately=true&force_search=false)

**原文摘要:** We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing
vision-language reasoning. Unlike prior single-agent or monolithic models,
GAM-Agent formulates the reasoning process as a non-zero-sum game between base
agents--each specializing in visual perception subtasks--and a critical agent
that verifies logic consistency and factual correctness. Agents communicate via
structured claims, evidence, and uncertainty estimates. The framework
introduces an uncertainty-aware controller to dynamically adjust agent
collaboration, triggering multi-round debates when disagreement or ambiguity is
detected. This process yields more robust and interpretable predictions.
Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and
V*Bench--demonstrate that GAM-Agent significantly improves performance across
various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid
scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\%, and still enhances
strong models like GPT-4o by up to 2--3\%. Our approach is modular, scalable,
and generalizable, offering a path toward reliable and explainable multi-agent
multimodal reasoning.

</details>


### [184] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/abs/2505.23432)
*Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi*

**主要类别:** cs.AI

**概要:** 生成式AI（GenAI）工具的迅速崛起引发了关于其在不同工作环境中是补充还是替代人类工作者角色的讨论。本文提出一个数学框架，模拟工作、工作者和工作者与工作的匹配度，并将技能分解为决策级和行动级子技能，以反映人类和GenAI的互补优势。通过分析子技能能力变化对工作成功的影响，确定了成功概率急剧转变的条件，以及具有互补子技能的工作者组合显著优于单一工作者的充分条件。这解释了诸如生产力压缩等现象，即GenAI辅助对低技能工作者带来更大的收益。使用O*NET和Big-Bench Lite的数据展示了该框架的实际应用价值，结果强调了GenAI何时以及如何补充人类技能，而非取代它们。


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI的快速发展促使人们重新思考其在工作环境中的角色——是作为人类工作者的补充还是替代品。当前缺乏一个清晰的理论框架来理解GenAI和人类技能之间的相互作用，特别是在不同技能水平下如何影响工作表现。因此，需要建立一个能够量化分析人类和AI技能互补性的模型。

**方法:** 研究者提出了一个数学框架，用于建模工作、工作者及其匹配程度。同时，引入了一种新的技能分解方法，将技能分为决策级和行动级子技能，用以体现人类和生成式AI的不同优势。通过分析子技能能力的变化对工作成功概率的影响，识别出成功概率发生显著变化的条件，并建立了在何种情况下结合具有互补子技能的工作者能显著优于单独依赖某一个工作者的充分条件。最后，通过O*NET和Big-Bench Lite数据验证了该框架的实际可行性。

**结果:** 研究表明，在某些条件下，结合具有互补子技能的工作者（如人类与GenAI）可以显著提高工作效率，而不是单纯依赖某一类工作者。此外，研究还揭示了生产力压缩现象，即GenAI辅助对于低技能工作者的提升效果更为明显。这些发现为理解GenAI如何补充而非取代人类技能提供了重要依据。

**结论:** 生成式AI在许多情境下能够有效补充人类技能，而非简单地替代它们。具体而言，通过将技能细分为决策级和行动级子技能，可以更好地理解人类和AI的优势所在，并在实际应用中优化二者协作方式。这一研究成果为未来探索人机协作模式奠定了理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Mathematical+Framework+for+AI-Human+Integration+in+Work，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23432&send_immediately=true&force_search=false)

**原文摘要:** The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [185] [Emergent Risk Awareness in Rational Agents under Resource Constraints](https://arxiv.org/abs/2505.23436)
*Daniel Jarne Ornia, Nicholas Bishop, Joel Dyer, Wei-Chen Lee, Ani Calinescu, Doyne Farme, Michael Wooldridge*

**主要类别:** cs.AI

**概要:** 本论文探讨了在资源有限或可能失败的情况下，具有代理能力的高级推理模型（AI代理人）的行为变化及与人类目标的潜在偏差问题。通过生存多臂赌博机框架，研究量化了生存驱动偏好转移的影响，确定了偏差出现的条件，并提出了缓解风险行为的机制。


<details>
  <summary>更多</summary>
  
**动机:** AI代理人在资源耗尽或失败约束下，面临行动序列被强制终止的情况，导致效用驱动行为的隐性权衡。此外，由于这些代理人通常代表人类主体行事，约束暴露的不对称性可能导致人类目标和代理激励之间的意外偏差。

**方法:** 作者通过生存多臂赌博机框架形式化这一设置，提供了理论和实证结果以量化生存驱动偏好转移的影响，识别偏差出现的条件，并提出缓解风险寻求或风险规避行为的机制。

**结果:** 研究量化了生存驱动偏好转移的影响，确定了偏差出现的具体条件，并提出了缓解机制，从而提高了对AI代理人在生存压力下新兴行为的理解和可解释性。

**结论:** 本文旨在增强对在生存压力下运行的AI代理人新兴行为的理解和可解释性，并为在关键资源受限环境中安全部署此类AI系统提供指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergent+Risk+Awareness+in+Rational+Agents+under+Resource+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23436，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23436&send_immediately=true&force_search=false)

**原文摘要:** Advanced reasoning models with agentic capabilities (AI agents) are deployed
to interact with humans and to solve sequential decision-making problems under
(approximate) utility functions and internal models. When such problems have
resource or failure constraints where action sequences may be forcibly
terminated once resources are exhausted, agents face implicit trade-offs that
reshape their utility-driven (rational) behaviour. Additionally, since these
agents are typically commissioned by a human principal to act on their behalf,
asymmetries in constraint exposure can give rise to previously unanticipated
misalignment between human objectives and agent incentives. We formalise this
setting through a survival bandit framework, provide theoretical and empirical
results that quantify the impact of survival-driven preference shifts, identify
conditions under which misalignment emerges and propose mechanisms to mitigate
the emergence of risk-seeking or risk-averse behaviours. As a result, this work
aims to increase understanding and interpretability of emergent behaviours of
AI agents operating under such survival pressure, and offer guidelines for
safely deploying such AI systems in critical resource-limited environments.

</details>


### [186] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473)
*Xiaorui Wu, Xiaofeng Mao, Fei Li, Xin Zhang, Xiaolu Zhang, Jun Zhou, Yuxiang Peng, Li Zheng, Chong Teng, Donghong Ji, Zhuang Li*

**主要类别:** cs.AI

**概要:** 大型语言模型（LLMs）常因过度保守的安全对齐而拒绝无害的输入查询，损害用户体验。本文提出了一种名为EVOREFUSE的提示优化方法，通过进化算法生成多样化的伪恶意指令，以减少不必要的拒绝。该方法创建了两个新数据集：EVOREFUSE-TEST和EVOREFUSE-ALIGN，显著提高了触发拒绝率、词汇多样性和模型响应置信度。使用EVOREFUSE-ALIGN监督微调的LLAMA3.1-8B-INSTRUCT模型比其他模型减少了14.31%的过度拒绝，同时保持安全性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的收集用于评估和缓解LLM过度拒绝的伪恶意指令的方法存在局限性，如手动创建或重写指令缺乏可扩展性或无法生成足够多样化和有效的拒绝诱导提示。

**方法:** EVOREFUSE采用进化算法，通过变异策略和重组在更广泛的方向上探索指令空间，并迭代演化种子指令以最大化LLM拒绝概率的证据下界。

**结果:** 使用EVOREFUSE生成的数据集EVOREFUSE-TEST和EVOREFUSE-ALIGN，前者在9个LLMs上的平均拒绝触发率比次优基准高出140.41%，词汇多样性提高34.86%，LLM响应置信度得分提升40.03%；后者用于监督和基于偏好的对齐训练，使LLAMA3.1-8B-INSTRUCT模型的过度拒绝减少了14.31%，且不牺牲安全性。

**结论:** EVOREFUSE提供了一种有效的方法来生成多样化的伪恶意指令，帮助改进LLM的安全对齐，减少过度拒绝现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EVOREFUSE%3A+Evolutionary+Prompt+Optimization+for+Evaluation+and+Mitigation+of+LLM+Over-Refusal+to+Pseudo-Malicious+Instructions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23473，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23473&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


### [187] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
*Xiang Li, Haiyang Yu, Xinghua Zhang, Ziyang Huang, Shizhu He, Kang Liu, Jun Zhao, Fei Huang, Yongbin Li*

**主要类别:** cs.AI

**概要:** 开发了Socratic-PRMBench，一个包含2995条推理路径的新基准，用于系统评估过程奖励模型（PRMs）在六种推理模式下的表现。实验揭示了现有PRMs的重要缺陷，强调了改进的必要性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基准主要关注逐步正确性评估，缺乏对不同推理模式下PRMs的系统评估。

**方法:** 引入Socratic-PRMBench基准，涵盖六种推理模式（转换、分解、重新收集、演绎、验证和集成），并包含2995条有缺陷的推理路径。通过实验评估PRMs和作为批评模型的LLMs。

**结果:** 发现现有PRMs在评估不同推理模式下的推理步骤时存在显著弱点。

**结论:** Socratic-PRMBench可作为全面测试平台，推动未来PRMs的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Socratic-PRMBench%3A+Benchmarking+Process+Reward+Models+with+Systematic+Reasoning+Patterns，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23474，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23474&send_immediately=true&force_search=false)

**原文摘要:** Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [188] [Autoformalization in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2505.23486)
*Ke Weng, Lun Du, Sirui Li, Wangyue Lu, Haozhe Sun, Hengyu Liu, Tiancheng Zhang*

**主要类别:** cs.AI

**概要:** Autoformalization 是将非正式数学命题转化为可验证形式表示的过程，是自动定理证明中的基础任务。本文综述了该领域在数学和大语言模型（LLM）视角下的最新进展，探讨其应用、工作流程以及对 LLM 输出可验证性的提升作用，并总结了开源资源及未来方向。


<details>
  <summary>更多</summary>
  
**动机:** 自动形式化技术能够将非正式的数学命题转化为可验证的形式表示，从而推动数学在理论与应用领域的使用，并通过结合人工智能特别是大语言模型的进步，为自动化定理证明提供了新的机会。

**方法:** 本文从数学和大语言模型两个角度全面概述了自动形式化的最新进展，分析了其在不同数学领域和难度级别的应用，探讨了从数据预处理到模型设计与评估的端到端工作流程，并研究了自动形式化在提高大语言模型输出可验证性、可信度和推理能力方面的作用。

**结果:** 文章展示了自动形式化在多个数学领域中的实际应用，揭示了它如何改进大语言模型的性能，并整理了支持当前研究的开源模型和数据集。

**结论:** 尽管自动形式化领域已经取得了显著进展，但仍面临许多开放性挑战。作者讨论了这些挑战并提出了未来的研究方向，强调了进一步发展自动形式化技术的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Autoformalization+in+the+Era+of+Large+Language+Models%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23486，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23486&send_immediately=true&force_search=false)

**原文摘要:** Autoformalization, the process of transforming informal mathematical
propositions into verifiable formal representations, is a foundational task in
automated theorem proving, offering a new perspective on the use of mathematics
in both theoretical and applied domains. Driven by the rapid progress in
artificial intelligence, particularly large language models (LLMs), this field
has witnessed substantial growth, bringing both new opportunities and unique
challenges. In this survey, we provide a comprehensive overview of recent
advances in autoformalization from both mathematical and LLM-centric
perspectives. We examine how autoformalization is applied across various
mathematical domains and levels of difficulty, and analyze the end-to-end
workflow from data preprocessing to model design and evaluation. We further
explore the emerging role of autoformalization in enhancing the verifiability
of LLM-generated outputs, highlighting its potential to improve both the
trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key
open-source models and datasets supporting current research, and discuss open
challenges and promising future directions for the field.

</details>


### [189] [TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/abs/2505.23518)
*Hangoo Kang, Jehyeok Yeon, Gagandeep Singh*

**主要类别:** cs.AI

**概要:** 自主代理AI系统通过视觉-语言模型（VLMs）迅速向实际部署迈进，但其跨模态推理能力引入了新的对抗性操作攻击面。我们提出了一种生成式对抗框架TRAP，利用扩散型语义注入操纵代理的决策。TRAP无需访问模型内部即可生成视觉上自然的图像，却能在代理AI系统中引发一致的选择偏差。在微软COCO数据集上的评估显示，TRAP对包括LLaVA-34B、Gemma3和Mistral-3.1在内的领先模型实现了100%的攻击成功率。这一结果揭示了自主代理可通过人类无法察觉的跨模态操作被持续误导的关键漏洞。


<details>
  <summary>更多</summary>
  
**动机:** 当前的对抗攻击通常依赖于可见像素扰动或需要特权模型或环境访问，这使得它们在隐秘、现实世界的利用中不切实际。因此，研究一种无需访问模型内部、能通过语义层面操作来影响代理决策的方法成为必要。

**方法:** TRAP结合了基于负向提示的退化与正向语义优化，由一个Siamese语义网络和布局感知的空间掩码引导。该方法通过扩散型语义注入来操纵代理的决策，无需访问模型内部即可生成视觉上自然的图像。

**结果:** 在微软COCO数据集上的多候选决策场景评估中，TRAP对多个领先的模型实现了100%的攻击成功率，显著优于SPSA、Bandit和标准扩散方法等基线。

**结论:** 研究揭示了自主代理AI系统存在可通过人类无法察觉的跨模态操作被持续误导的关键漏洞，强调了需要超越像素级鲁棒性的防御策略来解决跨模态决策中的语义漏洞。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRAP%3A+Targeted+Redirecting+of+Agentic+Preferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23518&send_immediately=true&force_search=false)

**原文摘要:** Autonomous agentic AI systems powered by vision-language models (VLMs) are
rapidly advancing toward real-world deployment, yet their cross-modal reasoning
capabilities introduce new attack surfaces for adversarial manipulation that
exploit semantic reasoning across modalities. Existing adversarial attacks
typically rely on visible pixel perturbations or require privileged model or
environment access, making them impractical for stealthy, real-world
exploitation. We introduce TRAP, a generative adversarial framework that
manipulates the agent's decision-making using diffusion-based semantic
injections. Our method combines negative prompt-based degradation with positive
semantic optimization, guided by a Siamese semantic network and layout-aware
spatial masking. Without requiring access to model internals, TRAP produces
visually natural images yet induces consistent selection biases in agentic AI
systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)
dataset, building multi-candidate decision scenarios. Across these scenarios,
TRAP achieves a 100% attack success rate on leading models, including
LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such
as SPSA, Bandit, and standard diffusion approaches. These results expose a
critical vulnerability: Autonomous agents can be consistently misled through
human-imperceptible cross-modal manipulations. These findings highlight the
need for defense strategies beyond pixel-level robustness to address semantic
vulnerabilities in cross-modal decision-making.

</details>


### [190] [Individual differences in the cognitive mechanisms of planning strategy discovery](https://arxiv.org/abs/2505.23519)
*Ruiqi He, Falk Lieder*

**主要类别:** cs.AI

**概要:** 人们通过元认知强化学习（MCRL）获得高效的规划策略，但模型在策略发现速度上仍逊于人类。本研究探讨了加入促进人类策略发现的认知机制（如内在生成的元认知伪奖励、主观努力评估和终止思考）能否缩小模型与人类表现的差距。结果表明，这些机制有助于策略发现，但仍未完全弥合差距，需进一步探索其他因素。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先前研究表明MCRL模型可以学习新规划策略并解释更多参与者的经验驱动型发现，但它也揭示了元认知学习中的显著个体差异，并且模型在策略发现速度上慢于人类。因此，研究者希望探索是否通过结合可能促进人类策略发现的认知机制，可以使MCRL模型更接近人类表现。

**方法:** 研究纳入了三种可能促进策略发现的认知机制：内在生成的元认知伪奖励、主观努力评估以及终止思考。通过对规划任务数据进行分析，研究者考察了参与者使用这些机制的比例及其对策略发现的影响。

**结果:** 数据分析显示，更大比例的参与者至少使用了一种上述机制，且存在显著的个体差异。元认知伪奖励、主观努力评估以及学习无需进一步规划即可行动的价值被发现能够促进策略发现。然而，即使加入这些增强机制，模型与人类表现之间仍然存在差距。

**结论:** 虽然引入特定认知机制为理解个体差异及这些机制对策略发现的影响提供了有价值的见解，但它们并未完全弥合MCRL模型与人类表现之间的差距。这提示需要进一步探索其他可能促进策略发现的因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Individual+differences+in+the+cognitive+mechanisms+of+planning+strategy+discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23519，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23519&send_immediately=true&force_search=false)

**原文摘要:** People employ efficient planning strategies. But how are these strategies
acquired? Previous research suggests that people can discover new planning
strategies through learning from reinforcements, a process known as
metacognitive reinforcement learning (MCRL). While prior work has shown that
MCRL models can learn new planning strategies and explain more participants'
experience-driven discovery better than alternative mechanisms, it also
revealed significant individual differences in metacognitive learning.
Furthermore, when fitted to human data, these models exhibit a slower rate of
strategy discovery than humans. In this study, we investigate whether
incorporating cognitive mechanisms that might facilitate human strategy
discovery can bring models of MCRL closer to human performance. Specifically,
we consider intrinsically generated metacognitive pseudo-rewards, subjective
effort valuation, and termination deliberation. Analysis of planning task data
shows that a larger proportion of participants used at least one of these
mechanisms, with significant individual differences in their usage and varying
impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort
valuation, and learning the value of acting without further planning were found
to facilitate strategy discovery. While these enhancements provided valuable
insights into individual differences and the effect of these mechanisms on
strategy discovery, they did not fully close the gap between model and human
performance, prompting further exploration of additional factors that people
might use to discover new planning strategies.

</details>


### [191] [Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)](https://arxiv.org/abs/2505.23536)
*Janik-Vasily Benzin, Gyunam Park, Stefanie Rinderle-Ma*

**主要类别:** cs.AI

**概要:** 本论文提出了一种同步模型和事件抽象的形式基础，证明了通过模型抽象（MA）和事件日志抽象（EA）可以获得等价的过程模型，并基于行为特征抽象提出了一种新的EA技术。


<details>
  <summary>更多</summary>
  
**动机:** 当前在过程智能项目中，虽然可以通过多次抽象模型和事件数据来优化目标（如减少模型大小），但缺乏一种技术可以实现发现模型后对底层事件日志的同步抽象。这导致失去了日志中包含的真实世界行为的基础，从而限制了分析的洞察力。

**方法:** 作者提供了同步模型和事件抽象的形式化基础，证明了通过模型抽象（MA）和从抽象事件日志中发现过程模型可以产生等价的过程模型。具体方法基于非顺序保留的模型抽象技术——行为特征抽象。

**结果:** 证明了同步抽象方法的可行性，并提出了一种新的事件抽象（EA）技术，该技术能够保持与真实世界行为的一致性，从而支持更深入的分析。

**结论:** 同步模型和事件抽象的方法为过程智能项目提供了一个重要的理论基础，使得在优化模型的同时，仍能保持对真实世界行为的洞察。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synchronizing+Process+Model+and+Event+Abstraction+for+Grounded+Process+Intelligence+%28Extended+Version%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23536&send_immediately=true&force_search=false)

**原文摘要:** Model abstraction (MA) and event abstraction (EA) are means to reduce
complexity of (discovered) models and event data. Imagine a process
intelligence project that aims to analyze a model discovered from event data
which is further abstracted, possibly multiple times, to reach optimality
goals, e.g., reducing model size. So far, after discovering the model, there is
no technique that enables the synchronized abstraction of the underlying event
log. This results in loosing the grounding in the real-world behavior contained
in the log and, in turn, restricts analysis insights. Hence, in this work, we
provide the formal basis for synchronized model and event abstraction, i.e., we
prove that abstracting a process model by MA and discovering a process model
from an abstracted event log yields an equivalent process model. We prove the
feasibility of our approach based on behavioral profile abstraction as
non-order preserving MA technique, resulting in a novel EA technique.

</details>


### [192] [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559)
*Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, Jiaxuan You*

**主要类别:** cs.AI

**概要:** 近期大型语言模型（LLM）代理在科学发现自动化方面取得了显著进展，但同时也引发了重要的伦理和安全问题。为系统地应对这些挑战，我们提出了SafeScientist，这是一个创新的AI科学家框架，旨在增强AI驱动科学研究中的安全性和伦理责任。SafeScientist主动拒绝不恰当或高风险任务，并在整个研究过程中强调安全性。为了实现全面的安全监督，我们集成了多种防御机制，包括提示监控、代理协作监控、工具使用监控以及伦理审查组件。同时，我们还提出了SciSafetyBench，这是一个专门设计用于评估科学背景下AI安全性的新基准，包含跨6个领域的240个高风险科学任务、30个特别设计的科学工具和120个与工具相关的风险任务。广泛的实验表明，SafeScientist相比传统AI科学家框架提升了35%的安全性能，且不影响科学产出质量。此外，我们严格验证了我们的安全管道对各种对抗攻击方法的鲁棒性，进一步证实了我们集成方法的有效性。代码和数据将在https://github.com/ulab-uiuc/SafeScientist提供。警告：本文可能包含具有冒犯性或危害性的示例数据。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型代理在加速科学发现方面表现出色，但由此引发的伦理和安全问题日益严重，亟需一个能够确保AI驱动科学研究既高效又安全的框架。

**方法:** 开发了SafeScientist这一AI科学家框架，通过主动拒绝高风险任务、强化研究过程中的安全性，并整合多种防御机制如提示监控、代理协作监控、工具使用监控及伦理审查组件来实现全面安全监督。同时，提出了一个新的基准测试SciSafetyBench，用于评估科学背景下的AI安全性。

**结果:** 实验结果表明，SafeScientist在不降低科学产出质量的前提下，将安全性能提升了35%。并且，其安全管道在面对多种对抗攻击方法时表现出较强的鲁棒性。

**结论:** SafeScientist框架有效地提高了AI驱动科学研究的安全性和伦理责任感，证明了其在实际应用中的价值。同时，SciSafetyBench为评估科学背景下的AI安全性提供了有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SafeScientist%3A+Toward+Risk-Aware+Scientific+Discoveries+by+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23559&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in large language model (LLM) agents have significantly
accelerated scientific discovery automation, yet concurrently raised critical
ethical and safety concerns. To systematically address these challenges, we
introduce \textbf{SafeScientist}, an innovative AI scientist framework
explicitly designed to enhance safety and ethical responsibility in AI-driven
scientific exploration. SafeScientist proactively refuses ethically
inappropriate or high-risk tasks and rigorously emphasizes safety throughout
the research process. To achieve comprehensive safety oversight, we integrate
multiple defensive mechanisms, including prompt monitoring, agent-collaboration
monitoring, tool-use monitoring, and an ethical reviewer component.
Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel
benchmark specifically designed to evaluate AI safety in scientific contexts,
comprising 240 high-risk scientific tasks across 6 domains, alongside 30
specially designed scientific tools and 120 tool-related risk tasks. Extensive
experiments demonstrate that SafeScientist significantly improves safety
performance by 35\% compared to traditional AI scientist frameworks, without
compromising scientific output quality. Additionally, we rigorously validate
the robustness of our safety pipeline against diverse adversarial attack
methods, further confirming the effectiveness of our integrated approach. The
code and data will be available at https://github.com/ulab-uiuc/SafeScientist.
\textcolor{red}{Warning: this paper contains example data that may be offensive
or harmful.}

</details>


### [193] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575)
*Benjamin Arnav, Pablo Bernabeu-Pérez, Nathan Helm-Burger, Tim Kostolansky, Hannes Whittingham, Mary Phuong*

**主要类别:** cs.AI

**概要:** 在AI模型部署中，为防止其采取未被察觉的有害行动，研究了Chain-of-Thought (CoT) 监控方法，并与仅监控最终输出的方法进行比较。发现CoT监控在某些情况下能提升检测能力，但也可能因误导性推理而降低性能。为此，提出了一种混合监控协议，结合推理和最终输出评分，显著提高了对微妙欺骗行为的检测率。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI模型自主性的增加，确保它们不会采取未被察觉的有害行动变得至关重要。因此，需要探索有效的监控方法来识别潜在的有害行为。

**方法:** 研究了Chain-of-Thought (CoT) 监控方法，其中较弱但可信的监控模型持续监督更强大但不可信模型的中间推理步骤。将CoT监控与仅监控最终输出的方法进行比较，评估其在不同场景下的表现，并引入一种混合协议，独立评分推理和最终输出后通过加权平均结合两者。

**结果:** CoT监控在行动监控无法可靠识别破坏行为的情景下，提升了27个百分点的检测率。然而，在更明显的破坏案例中，CoT痕迹可能包含误导性的推理，从而降低性能。混合监控协议在所有测试模型和任务中始终优于CoT和行动监控，对于微妙欺骗情景的检测率是行动监控的四倍以上。

**结论:** 虽然CoT监控可以提高对某些破坏行为的检测能力，但其可能受到误导性推理的影响。提出的混合监控协议能够有效结合推理和最终输出的评分，显著提高检测率，是一种更优的监控策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoT+Red-Handed%3A+Stress+Testing+Chain-of-Thought+Monitoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23575，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23575&send_immediately=true&force_search=false)

**原文摘要:** As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [194] [MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning](https://arxiv.org/abs/2505.23596)
*Linqiang Guo, Wei Liu, Yi Wen Heng, Tse-Hsun, Chen, Yang Wang*

**主要类别:** cs.AI

**概要:** MAPLE是一个多代理框架，通过将应用交互建模为有限状态机（FSM），提高了移动GUI代理在跨应用任务中的表现。它包含规划、执行、验证、错误恢复和知识保留的专门代理，显著提升了任务成功率、恢复成功率和动作准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的移动GUI代理仅能对当前屏幕进行推理，缺乏对应用导航流的结构化模型，限制了其上下文理解、意外结果检测和错误恢复能力。

**方法:** MAPLE将每个UI屏幕建模为离散状态，用户动作为转换，并通过四个阶段（规划、执行、验证、错误恢复）的专门代理动态构建FSM。这些代理使用从UI屏幕提取的感知数据实时协作，跟踪导航进度和流，验证动作结果，并通过回滚到稳定状态来恢复错误。

**结果:** 在Mobile-Eval-E和SPA-Bench两个基准测试中，MAPLE相比现有最佳基线方法，任务成功率提高12%，恢复成功率提升13.8%，动作准确性提升6.5%。

**结论:** 结构化的状态建模对于指导移动GUI代理执行任务至关重要。提出的FSM表示可以作为轻量级、与模型无关的记忆层集成到未来的GUI代理架构中，支持结构化规划、执行验证和错误恢复。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAPLE%3A+A+Mobile+Assistant+with+Persistent+Finite+State+Machines+for+Recovery+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23596&send_immediately=true&force_search=false)

**原文摘要:** Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [195] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao, Jingxian Xu, Hanbing Liu, Jinyu Wang, Mengyu Zhou, Haoyu Dong, Shi Han, Dongmei Zhang*

**主要类别:** cs.AI

**概要:** 论文提出了一种名为Formula Tuning (Fortune)的强化学习框架，该框架训练大语言模型生成可执行的电子表格公式以进行表格数据的问题回答。通过使用答案正确性作为奖励信号，减少了对监督公式的依赖，并在多个基准测试中显著提高了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LMs）展示了强大的通用推理能力，但在处理表格数据时，尤其是在复杂场景下的数值或符号推理方面仍然存在困难。电子表格公式提供了一个强大且富有表现力的媒介，用于表示可执行的符号操作，这些模式尚未被充分利用。

**方法:** 提出了一个强化学习框架Formula Tuning (Fortune)，该框架训练语言模型生成可执行的电子表格公式以回答基于通用表格数据的问题。利用二元答案正确性作为奖励信号，减少对监督公式注释的依赖，引导模型通过推理学习公式推导。

**结果:** Formula Tuning显著提升了语言模型在多步数值和符号推理任务上的性能，使一个7B参数模型在表格理解上超越了O1模型。

**结论:** Formula Tuning增强了语言模型在符号表格推理方面的能力，表明公式驱动的强化学习有可能推动语言模型在这一领域的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fortune%3A+Formula-Driven+Reinforcement+Learning+for+Symbolic+Table+Reasoning+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23667，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23667&send_immediately=true&force_search=false)

**原文摘要:** Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform O1 on table understanding. This highlights
the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [196] [ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork](https://arxiv.org/abs/2505.23686)
*Caroline Wang, Arrasy Rahman, Jiaxun Cui, Yoonchang Sung, Peter Stone*

**主要类别:** cs.AI

**概要:** 这篇论文提出了一种新的 AHT 方法，通过统一的框架和 ROTATE 算法，在队友生成和代理训练之间实现交互式优化，显著提高了泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的 Ad Hoc Teamwork (AHT) 方法通常采用两阶段管道，分别设计算法生成队友群体和训练 AHT 代理。然而，这种方法存在一些问题：1. 生成的队友行为覆盖范围有限；2. 忽略了生成的队友是否易于学习；3. 训练过程中将队友集合视为静态，缺乏对训练队友分布的控制。

**方法:** 本文提出了一种统一的 AHT 框架，将问题重新定义为临时代理与对抗性队友生成器之间的开放式学习过程。引入了一种基于遗憾驱动的开放式训练算法 ROTATE，该算法在改进 AHT 代理和生成探测其缺陷的队友之间交替进行。

**结果:** 通过在多样化的 AHT 环境中进行大量实验，结果表明 ROTATE 在泛化到未见过的评估队友方面显著优于基线方法。

**结论:** ROTATE 算法为鲁棒且可泛化的团队合作建立了一个新的标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ROTATE%3A+Regret-driven+Open-ended+Training+for+Ad+Hoc+Teamwork，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23686，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23686&send_immediately=true&force_search=false)

**原文摘要:** Developing AI agents capable of collaborating with previously unseen partners
is a fundamental generalization challenge in multi-agent learning, known as Ad
Hoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage
pipeline, where first, a fixed population of teammates is generated with the
idea that they should be representative of the teammates that will be seen at
deployment time, and second, an AHT agent is trained to collaborate well with
agents in the population. To date, the research community has focused on
designing separate algorithms for each stage. This separation has led to
algorithms that generate teammate pools with limited coverage of possible
behaviors, and that ignore whether the generated teammates are easy to learn
from for the AHT agent. Furthermore, algorithms for training AHT agents
typically treat the set of training teammates as static, thus attempting to
generalize to previously unseen partner agents without assuming any control
over the distribution of training teammates. In this paper, we present a
unified framework for AHT by reformulating the problem as an open-ended
learning process between an ad hoc agent and an adversarial teammate generator.
We introduce ROTATE, a regret-driven, open-ended training algorithm that
alternates between improving the AHT agent and generating teammates that probe
its deficiencies. Extensive experiments across diverse AHT environments
demonstrate that ROTATE significantly outperforms baselines at generalizing to
an unseen set of evaluation teammates, thus establishing a new standard for
robust and generalizable teamwork.

</details>


### [197] [Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics](https://arxiv.org/abs/2505.23695)
*Ran Zhang, Mohannad Elhamod*

**主要类别:** cs.AI

**概要:** The paper introduces a modular agentic system leveraging LLMs to automate the data-to-dashboard process with domain detection, concept extraction, analysis generation and self-reflection. Evaluated across three datasets, it outperforms GPT-4o in insightfulness, domain relevance, and analytical depth. The system simulates business analysts' reasoning without relying on closed ontologies or question templates.


<details>
  <summary>更多</summary>
  
**动机:** To create an advanced automated system that transforms raw data into insightful visualizations by simulating the analytical reasoning of business analysts, using LLM capabilities for domain detection, concept extraction, multi-perspective analysis, and iterative self-reflection.

**方法:** The method involves developing a modular pipeline of LLM agents which perform domain detection, concept extraction, multi-perspective analysis generation, and iterative self-reflection. This framework retrieves domain-relevant knowledge and adapts to various datasets without depending on closed ontologies or question templates.

**结果:** The system was evaluated on three datasets from different domains and compared against GPT-4o with a single-prompt baseline. It demonstrated superior performance in terms of insightfulness, domain relevance, and analytical depth as per tailored evaluation metrics and qualitative human assessment.

**结论:** This work presents a novel modular pipeline that bridges the gap between raw data and visualization, offering new possibilities for human-in-the-loop validation by domain experts in business analytics.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-to-Dashboard%3A+Multi-Agent+LLM+Framework+for+Insightful+Visualization+in+Enterprise+Analytics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23695&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard

</details>


### [198] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
*Ruida Wang, Yuxin Li, Yi R., Fung, Tong Zhang*

**主要类别:** cs.AI

**概要:** 该论文提出了一种名为NL-FL HybridReasoning的端到端框架，通过整合自然语言和形式语言推理，显著提升了大模型在数学推理任务上的表现。实验结果表明，该框架在MATH-500和AMC基准上分别达到了89.80%和84.34%的准确率，超越了纯自然语言基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 尽管强化学习方法在提升大语言模型的自然语言和形式语言推理能力方面取得了显著进展，但其难以赋予模型新的能力，特别是将形式语言知识有效融入自然语言数学推理中。由于自然语言和形式语言在问题结构和推理格式上存在固有差异，这种融合面临诸多挑战。因此，需要一种能够弥合这两种语言差距的新方法。

**方法:** 论文提出了NL-FL HybridReasoning框架，包括以下关键技术：1) NL-FL Problem Alignment方法，将自然语言的问答问题转化为形式语言的存在性定理；2) Mixed Problem Input技术，使形式语言推理器能够同时处理问答和存在性问题；3) 基于大语言模型的Answer Extraction机制，用于弥合自然语言和形式语言输出格式的差距。

**结果:** HybridReasoning框架在MATH-500和AMC基准测试中分别实现了89.80%和84.34%的准确率，相比自然语言基线模型分别提升了4.60%和4.82%。值得注意的是，即使在更多尝试次数下，某些问题仍只能由该框架解决而基线模型无法解决。

**结论:** NL-FL HybridReasoning框架成功地将形式语言推理引入自然语言数学问题求解中，显著提升了模型的表现，并解决了仅依赖自然语言基线模型无法解决的问题。这一成果为未来结合多种推理模式以增强人工智能系统的能力提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Let%27s+Reason+Formally%3A+Natural-Formal+Hybrid+Reasoning+Enhances+LLM%27s+Math+Capability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23703&send_immediately=true&force_search=false)

**原文摘要:** Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [199] [Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons](https://arxiv.org/abs/2505.23746)
*Hugo Henry, Kelly Cohen*

**主要类别:** cs.AI

**概要:** This paper explores the use of Genetic Fuzzy Systems (GFS) to model airfoil self-noise using different fuzzy regression strategies, finding that clustering-assisted fuzzy inference is effective for complex aeroacoustic phenomena.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of modeling airfoil self-noise, which is crucial for aerospace, automotive and drone applications.

**方法:** The study uses the Airfoil Self-Noise dataset to evaluate a brute force Takagi Sugeno Kang (TSK) fuzzy system, a cascading Genetic Fuzzy Tree (GFT) architecture and a novel clustered approach based on Fuzzy C-means (FCM).

**结果:** Clustering assisted fuzzy inference was found to be an effective regression tool for complex aeroacoustic phenomena.

**结论:** Genetic Fuzzy Systems, particularly when combined with clustering techniques, offer a viable solution for modeling airfoil self-noise.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+of+Genetic+Fuzzy+regression+techniques+for+aeroacoustic+phenomenons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23746，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23746&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the application of Genetic Fuzzy Systems (GFS) to
model the self-noise generated by airfoils, a key issue in aeroaccoustics with
significant implications for aerospace, automotive and drone applications.
Using the publicly available Airfoil Self Noise dataset, various Fuzzy
regression strategies are explored and compared. The paper evaluates a brute
force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading
Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on
Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the
viability of clustering assisted fuzzy inference as an effective regression
tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,
Cascading systems, Clustering and AI.

</details>


### [200] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai*

**主要类别:** cs.AI

**概要:** ZeroGUI是一个可扩展的在线学习框架，通过结合VLM自动任务生成、自动奖励评估和两阶段在线强化学习，解决了现有GUI代理方法对人工标注的依赖和适应性不足的问题。实验表明ZeroGUI在多个环境中显著提升了性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前纯视觉GUI代理方法主要采用离线学习框架，存在对高质量手动注释的重度依赖以及在动态交互环境中适应性有限的问题。

**方法:** 提出了一种名为ZeroGUI的可扩展在线学习框架，该框架整合了基于VLM的自动任务生成、基于VLM的自动奖励评估以及两阶段在线强化学习，以实现零人工成本的GUI代理训练。

**结果:** 在两个先进的GUI代理（UI-TARS和Aguvis）上的实验表明，ZeroGUI显著提高了在OSWorld和AndroidLab环境中的性能。

**结论:** ZeroGUI提供了一种有效的解决方案，减少了对人工干预的需求，并增强了GUI代理在动态环境中的适应能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZeroGUI%3A+Automating+Online+GUI+Learning+at+Zero+Human+Cost，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23762&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [201] [Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games](https://arxiv.org/abs/2505.22781)
*Antonio Ocello, Daniil Tiapkin, Lorenzo Mancini, Mathieu Laurière, Eric Moulines*

**主要类别:** stat.ML

**概要:** 提出了一种新算法MF-TRPO，用于计算有限状态动作空间中平均场博弈的近似纳什均衡。该算法结合了TRPO在强化学习中的稳定性与鲁棒性，并提供了严格的理论收敛性分析。


<details>
  <summary>更多</summary>
  
**动机:** 将强化学习中的TRPO方法扩展到平均场博弈（MFG）框架中，以解决多智能体问题并计算近似纳什均衡。

**方法:** 引入Mean-Field Trust Region Policy Optimization (MF-TRPO) 算法，利用TRPO的稳定性和鲁棒性优化策略，并基于标准假设进行理论分析。

**结果:** 证明了MF-TRPO算法在精确形式和样本形式下的收敛性，提供了高概率保证和有限样本复杂度分析。

**结论:** 通过结合RL技术和平均场决策，为解决复杂的多智能体问题提供了一个理论基础扎实的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Finite-Sample+Convergence+Bounds+for+Trust+Region+Policy+Optimization+in+Mean-Field+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22781&send_immediately=true&force_search=false)

**原文摘要:** We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel
algorithm designed to compute approximate Nash equilibria for ergodic
Mean-Field Games (MFG) in finite state-action spaces. Building on the
well-established performance of TRPO in the reinforcement learning (RL)
setting, we extend its methodology to the MFG framework, leveraging its
stability and robustness in policy optimization. Under standard assumptions in
the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing
theoretical guarantees on its convergence. Our results cover both the exact
formulation of the algorithm and its sample-based counterpart, where we derive
high-probability guarantees and finite sample complexity. This work advances
MFG optimization by bridging RL techniques with mean-field decision-making,
offering a theoretically grounded approach to solving complex multi-agent
problems.

</details>


### [202] [Highly Efficient and Effective LLMs with Multi-Boolean Architectures](https://arxiv.org/abs/2505.22811)
*Ba-Hien Tran, Van Minh Nguyen*

**主要类别:** stat.ML

**概要:** 本论文提出了一种新的框架，将大型语言模型（LLMs）转化为多核布尔参数，并首次在布尔域中直接微调它们，从而显著降低了微调和推理的复杂度，同时通过广泛的实验表明该方法优于最近的超低比特量化和二值化方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的权重二值化方法分为两类：后训练二值化和基于训练感知的微调方法。前者信息损失大，性能差；后者依赖于全精度潜在权重进行梯度近似，不仅次优而且复杂度高。因此需要一种更有效的方法来降低LLMs的复杂度，同时保持其性能。

**方法:** 论文提出了一种新框架，将LLMs转换为多核布尔参数，并首次实现了在布尔域中的直接微调，从而消除了对昂贵的潜在权重的需求，减少了微调和推理过程中的复杂度。

**结果:** 通过在多种LLMs上进行广泛且富有洞察力的实验，证明了该方法比最近的超低比特量化和二值化方法表现更好。

**结论:** 所提出的框架能够有效地将LLMs转化为多核布尔参数并在布尔域内直接微调，这种方法不仅显著降低了计算复杂度，而且在性能上优于其他超低比特量化和二值化技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Highly+Efficient+and+Effective+LLMs+with+Multi-Boolean+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22811，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22811&send_immediately=true&force_search=false)

**原文摘要:** Weight binarization has emerged as a promising strategy to drastically reduce
the complexity of large language models (LLMs). It is mainly classified into
two approaches: post-training binarization and finetuning with training-aware
binarization methods. The first approach, while having low complexity, leads to
significant loss of information from the original LLMs, resulting in poor
performance. The second approach, on the other hand, relies heavily on
full-precision latent weights for gradient approximation of binary weights,
which not only remains suboptimal but also introduces substantial complexity.
In this paper, we introduce a novel framework that effectively transforms LLMs
into multi-kernel Boolean parameters, for the first time, finetunes them
directly in the Boolean domain, eliminating the need for expensive latent
weights. This significantly reduces complexity during both finetuning and
inference. Through extensive and insightful experiments across a wide range of
LLMs, we demonstrate that our method outperforms recent ultra low-bit
quantization and binarization methods.

</details>


### [203] [Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features](https://arxiv.org/abs/2505.22997)
*Agnideep Aich, Ashit Baran Aich, Bruce Wade*

**主要类别:** stat.ML

**概要:** 提出了一种新的分类器Deep Copula Classifier (DCC)，结合了神经网络的表示能力和统计理论，能够有效处理复杂依赖关系的分类问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统分类器通常假设特征独立或依赖过于简化的关联，导致在实际依赖关系重要的情况下性能较差。

**方法:** 引入了Deep Copula Classifier (DCC)，一种生成模型，通过神经网络参数化的copulas将每个特征的边缘分布学习与其联合依赖结构的建模分离。使用轻量级神经网络来灵活和自适应地捕捉特征交互。

**结果:** DCC在标准条件下收敛到贝叶斯最优分类器，并提供了O(n^{-r/(2r + d)})的明确收敛率。此外，还提出了几种实用扩展，包括高维可扩展性、半监督学习和在线适应等。

**结论:** DCC提供了一个具有数学基础且可解释的框架，用于依赖感知分类，统一了统计严谨性和神经网络的表示能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Theoretical+Foundations+of+the+Deep+Copula+Classifier%3A+A+Generative+Approach+to+Modeling+Dependent+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.22997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.22997&send_immediately=true&force_search=false)

**原文摘要:** Traditional classifiers often assume feature independence or rely on overly
simplistic relationships, leading to poor performance in settings where
real-world dependencies matter. We introduce the Deep Copula Classifier (DCC),
a generative model that separates the learning of each feature's marginal
distribution from the modeling of their joint dependence structure via neural
network-parameterized copulas. For each class, lightweight neural networks are
used to flexibly and adaptively capture feature interactions, making DCC
particularly effective when classification is driven by complex dependencies.
We establish that DCC converges to the Bayes-optimal classifier under standard
conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for
r-smooth copula densities. Beyond theoretical guarantees, we outline several
practical extensions, including high-dimensional scalability through vine and
factor copula architectures, semi-supervised learning via entropy
regularization, and online adaptation using streaming gradient methods. By
unifying statistical rigor with the representational power of neural networks,
DCC offers a mathematically grounded and interpretable framework for
dependency-aware classification.

</details>


### [204] [JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows](https://arxiv.org/abs/2505.23196)
*Eshant English, Christoph Lippert*

**主要类别:** stat.ML

**概要:** 本论文提出了一种名为JAPAN的共形预测框架，通过基于密度的符合性评分来改进传统方法中基于残差评分的不足。该方法使用流模型估计预测密度，并通过设定密度阈值构建预测区域，从而实现更紧凑、适应性强且保持有限样本覆盖率保证的预测集。实验结果表明，JAPAN在多变量回归和预测任务中具有更好的校准性能和更紧致的预测区域。


<details>
  <summary>更多</summary>
  
**动机:** 现有的共形预测方法通常依赖于基于残差的符合性评分，这些方法在处理多模态分布时表现不佳，容易产生过于保守的预测区域，无法捕捉复杂预测分布的真实形状。因此，需要一种新的方法来解决这些问题。

**方法:** JAPAN（Joint Adaptive Prediction Areas with Normalising-Flows）是一种新的共形预测框架，它使用基于密度的符合性评分。通过利用流模型估计预测密度，并根据估计的密度评分设置阈值来构建预测区域。这种方法可以生成紧凑、可能不相连且上下文适应性强的预测区域，同时保留有限样本覆盖率保证。

**结果:** 理论上证明了JAPAN的高效性，并通过多变量回归和预测任务的实验证明了其良好的校准性能和更紧致的预测区域。此外，还提供了几个扩展以增加框架的灵活性。

**结论:** JAPAN提供了一种改进的共形预测方法，能够更好地处理复杂的预测分布，特别是在多模态情况下。通过基于密度的方法，JAPAN实现了更高效的预测区域构建，为实际应用中的不确定性量化提供了可靠的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JAPAN%3A+Joint+Adaptive+Prediction+Areas+with+Normalising-Flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23196&send_immediately=true&force_search=false)

**原文摘要:** Conformal prediction provides a model-agnostic framework for uncertainty
quantification with finite-sample validity guarantees, making it an attractive
tool for constructing reliable prediction sets. However, existing approaches
commonly rely on residual-based conformity scores, which impose geometric
constraints and struggle when the underlying distribution is multimodal. In
particular, they tend to produce overly conservative prediction areas centred
around the mean, often failing to capture the true shape of complex predictive
distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction
Areas with Normalising-Flows), a conformal prediction framework that uses
density-based conformity scores. By leveraging flow-based models, JAPAN
estimates the (predictive) density and constructs prediction areas by
thresholding on the estimated density scores, enabling compact, potentially
disjoint, and context-adaptive regions that retain finite-sample coverage
guarantees. We theoretically motivate the efficiency of JAPAN and empirically
validate it across multivariate regression and forecasting tasks, demonstrating
good calibration and tighter prediction areas compared to existing baselines.
We also provide several \emph{extensions} adding flexibility to our proposed
framework.

</details>


### [205] [Stable Thompson Sampling: Valid Inference via Variance Inflation](https://arxiv.org/abs/2505.23260)
*Budhaditya Halder, Shubhayan Pan, Koulik Khamaru*

**主要类别:** stat.ML

**概要:** 论文提出了一种改进的Thompson Sampling算法——Stable Thompson Sampling，通过增加后验方差来实现模型参数的有效统计推断。此方法在保持较低后悔值的同时，能够生成渐近正态的臂均值估计，为自适应决策算法提供了有效的统计推断工具。


<details>
  <summary>更多</summary>
  
**动机:** 传统的Thompson Sampling算法尽管在理论上和实践中表现优秀，但其自适应采样机制使得对模型参数构建置信区间变得困难。因此，需要一种改进的方法，在不显著增加后悔值的前提下，提供有效的统计推断能力。

**方法:** 提出了一种名为Stable Thompson Sampling的新变体，该方法通过引入一个对数因子放大后验方差。这一调整克服了数据非独立同分布的问题，从而可以得到臂均值的渐近正态估计。

**结果:** 实验表明，Stable Thompson Sampling不仅保留了标准TS的低后悔值特性，还实现了模型参数的有效统计推断。方差膨胀仅使后悔值增加了对数因子，代价较小。

**结论:** Stable Thompson Sampling展示了在自适应决策算法中进行有效统计推断的可能性，揭示了在少量增加后悔值的情况下获得合理统计推断的权衡原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+Thompson+Sampling%3A+Valid+Inference+via+Variance+Inflation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23260&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of statistical inference when the data is collected
via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known
to be both asymptotically optimal and empirically effective, its adaptive
sampling scheme poses challenges for constructing confidence intervals for
model parameters. We propose and analyze a variant of TS, called Stable
Thompson Sampling, in which the posterior variance is inflated by a logarithmic
factor. We show that this modification leads to asymptotically normal estimates
of the arm means, despite the non-i.i.d. nature of the data. Importantly, this
statistical benefit comes at a modest cost: the variance inflation increases
regret by only a logarithmic factor compared to standard TS. Our results reveal
a principled trade-off: by paying a small price in regret, one can enable valid
statistical inference for adaptive decision-making algorithms.

</details>


### [206] [The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation](https://arxiv.org/abs/2505.23445)
*Adrien Majka, El-Mahdi El-Mhamdi*

**主要类别:** stat.ML

**概要:** 本研究探讨了在不同目标和度量差异分布下，代理指标与实际目标之间的耦合对Goodhart定律的影响。结果表明，在轻尾目标和轻尾差异情况下，依赖性不会改变Goodhart效应的本质；但在轻尾目标和重尾差异情况下，过优化的程度与目标和度量之间差异的重尾性成反比。


<details>
  <summary>更多</summary>
  
**动机:** 尽管有越来越多的实证证据支持Goodhart定律，但其形式化研究仍然有限。之前的尝试大多基于简化的独立性假设和特定的学习范式，这限制了对代理指标与实际目标之间耦合影响的理解。

**方法:** 研究通过放宽先前工作中所做的简化独立性假设和学习范式假设，分析了代理指标与实际目标之间的耦合对Goodhart定律的影响。特别是在轻尾和重尾分布的情况下，探讨了优化代理指标如何影响实际目标的优化。

**结果:** 结果显示，在轻尾目标和轻尾差异的情况下，依赖性不会改变Goodhart效应的本质；而在轻尾目标和重尾差异的情况下，出现了一个例子，其中过优化的程度与目标和度量之间差异的重尾性成反比。

**结论:** 研究结论表明，代理指标与实际目标之间的耦合对Goodhart效应的影响取决于目标和差异分布的特性。在某些情况下，过优化可能加剧问题，具体取决于分布的尾部特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Strong%2C+Weak+and+Benign+Goodhart%27s+law.+An+independence-free+and+paradigm-agnostic+formalisation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23445&send_immediately=true&force_search=false)

**原文摘要:** Goodhart's law is a famous adage in policy-making that states that ``When a
measure becomes a target, it ceases to be a good measure''. As machine learning
models and the optimisation capacity to train them grow, growing empirical
evidence reinforced the belief in the validity of this law without however
being formalised. Recently, a few attempts were made to formalise Goodhart's
law, either by categorising variants of it, or by looking at how optimising a
proxy metric affects the optimisation of an intended goal. In this work, we
alleviate the simplifying independence assumption, made in previous works, and
the assumption on the learning paradigm made in most of them, to study the
effect of the coupling between the proxy metric and the intended goal on
Goodhart's law. Our results show that in the case of light tailed goal and
light tailed discrepancy, dependence does not change the nature of Goodhart's
effect. However, in the light tailed goal and heavy tailed discrepancy case, we
exhibit an example where over-optimisation occurs at a rate inversely
proportional to the heavy tailedness of the discrepancy between the goal and
the metric. %

</details>


### [207] [Learning Parametric Distributions from Samples and Preferences](https://arxiv.org/abs/2505.23557)
*Marc Jourdan, Gizem Yüce, Nicolas Flammarion*

**主要类别:** stat.ML

**概要:** 近期语言建模的进展强调了偏好反馈在提升模型性能中的作用。本文研究了在连续参数分布中，偏好反馈如何改善参数估计的条件。我们发现基于偏好的M-估计量比仅基于样本的M-估计量具有更好的渐近方差，并通过确定性偏好进一步改进。利用确定性偏好揭示的硬约束，我们提出了一种估计误差为$\mathcal{O}(1/n)$的估计器，这比仅使用样本能达到的$\Theta(1/\sqrt{n})$速率有显著改进。此外，我们建立了一个与此加速速率相匹配的下界。尽管我们的分析假设较为严格，但它们适用于一些显著案例，如基于对数概率奖励的高斯或拉普拉斯分布。


<details>
  <summary>更多</summary>
  
**动机:** 随着语言建模的进步，偏好反馈在提升模型性能方面的作用愈发重要。因此，有必要研究在何种条件下，偏好反馈可以改善连续参数分布中的参数估计。

**方法:** 研究者构建了一个框架，在该框架中学习者观察来自未知分布的样本对及其相对偏好，这些偏好依赖于相同的未知参数。通过分析基于偏好的M-估计量和仅基于样本的M-估计量，研究者展示了前者的渐近方差更优。此外，他们提出了一个利用确定性偏好硬约束的估计器，实现了$\mathcal{O}(1/n)$的估计误差缩放。

**结果:** 研究表明，基于偏好的M-估计量具有优于仅基于样本的M-估计量的渐近方差，并且通过确定性偏好可以进一步改进。提出的估计器达到了$\mathcal{O}(1/n)$的误差缩放，并且这一结果与理论下界相符。

**结论:** 偏好反馈可以在特定条件下显著提高参数估计的精度。尽管分析假设较为严格，但其结论对于理解偏好反馈在统计学习中的作用提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Parametric+Distributions+from+Samples+and+Preferences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23557，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23557&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in language modeling have underscored the role of preference
feedback in enhancing model performance. This paper investigates the conditions
under which preference feedback improves parameter estimation in classes of
continuous parametric distributions. In our framework, the learner observes
pairs of samples from an unknown distribution along with their relative
preferences depending on the same unknown parameter. We show that
preference-based M-estimators achieve a better asymptotic variance than
sample-only M-estimators, further improved by deterministic preferences.
Leveraging the hard constraints revealed by deterministic preferences, we
propose an estimator achieving an estimation error scaling of
$\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$
rate attainable with samples alone. Next, we establish a lower bound that
matches this accelerated rate; up to dimension and problem-dependent constants.
While the assumptions underpinning our analysis are restrictive, they are
satisfied by notable cases such as Gaussian or Laplace distributions for
preferences based on the log-probability reward.

</details>


### [208] [Multilook Coherent Imaging: Theoretical Guarantees and Algorithms](https://arxiv.org/abs/2505.23594)
*Xi Chen, Soham Jana, Christopher A. Metzler, Arian Maleki, Shirin Jalali*

**主要类别:** stat.ML

**概要:** Multilook相干成像在数字全息、超声成像和合成孔径雷达等应用中广泛使用，但受到乘性噪声（speckle）的影响。本文研究了基于似然方法的理论和算法方面，提供了严格的分析框架，并提出了结合Newton-Schulz算法和bagging策略的PGD方法以提高性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管多视相干成像技术广泛应用，其理论基础尚未被充分探索，尤其是关于乘性噪声对图像质量的影响及如何通过最大似然估计改善这一问题。

**方法:** 本文首先建立了深度图像先验假设下的最大似然估计的均方误差（MSE）的第一个理论上限。接着提出了一种高效的计算最大似然解的方法——投影梯度下降（PGD），并引入两个改进：1. 使用Newton-Schulz算法降低矩阵求逆的计算复杂度；2. 提出一种bagging策略减少投影误差。

**结果:** 理论结果表明MSE与深度图像先验中的参数数量、视图数量、信号维度和每视图测量数量相关。实验结果证明，结合上述技术的PGD方法达到了最先进的性能。

**结论:** 本文为多视相干成像提供了严格的理论和算法框架，所提出的改进方法显著提高了最大似然估计的性能，代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multilook+Coherent+Imaging%3A+Theoretical+Guarantees+and+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23594，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23594&send_immediately=true&force_search=false)

**原文摘要:** Multilook coherent imaging is a widely used technique in applications such as
digital holography, ultrasound imaging, and synthetic aperture radar. A central
challenge in these systems is the presence of multiplicative noise, commonly
known as speckle, which degrades image quality. Despite the widespread use of
coherent imaging systems, their theoretical foundations remain relatively
underexplored. In this paper, we study both the theoretical and algorithmic
aspects of likelihood-based approaches for multilook coherent imaging,
providing a rigorous framework for analysis and method development. Our
theoretical contributions include establishing the first theoretical upper
bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under
the deep image prior hypothesis. Our results capture the dependence of MSE on
the number of parameters in the deep image prior, the number of looks, the
signal dimension, and the number of measurements per look. On the algorithmic
side, we employ projected gradient descent (PGD) as an efficient method for
computing the maximum likelihood solution. Furthermore, we introduce two key
ideas to enhance the practical performance of PGD. First, we incorporate the
Newton-Schulz algorithm to compute matrix inverses within the PGD iterations,
significantly reducing computational complexity. Second, we develop a bagging
strategy to mitigate projection errors introduced during PGD updates. We
demonstrate that combining these techniques with PGD yields state-of-the-art
performance. Our code is available at
https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.

</details>


### [209] [Instance-Optimality for Private KL Distribution Estimation](https://arxiv.org/abs/2505.23620)
*Jiayuan Ye, Vitaly Feldman, Kunal Talwar*

**主要类别:** stat.ML

**概要:** 研究了在给定n个独立同分布样本的情况下，估计未知离散分布p的问题。提出了最小化KL散度的最优私有估计器，并从实例最优性的角度提出了一种新的算法框架。


<details>
  <summary>更多</summary>
  
**动机:** 估计未知离散分布是一个基本问题，传统的最小最大最优方法无法很好地适应实际分布。

**方法:** 构建了最小最大最优私有估计器，并提出了基于Good-Turing估计器变体的实例最优算法。

**结果:** 提出了达到实例最优性的算法，具有和不具有差分隐私约束的情况均适用，并提供了理论下界。

**结论:** 通过实例最优性视角，提出的算法能更好地适应个体实例，相比传统方法性能更优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Instance-Optimality+for+Private+KL+Distribution+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23620&send_immediately=true&force_search=false)

**原文摘要:** We study the fundamental problem of estimating an unknown discrete
distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the
distribution. We are interested in minimizing the KL divergence between the
true distribution and the algorithm's estimate. We first construct minimax
optimal private estimators. Minimax optimality however fails to shed light on
an algorithm's performance on individual (non-worst-case) instances $p$ and
simple minimax-optimal DP estimators can have poor empirical performance on
real distributions. We then study this problem from an instance-optimality
viewpoint, where the algorithm's error on $p$ is compared to the minimum
achievable estimation error over a small local neighborhood of $p$. Under
natural notions of local neighborhood, we propose algorithms that achieve
instance-optimality up to constant factors, with and without a differential
privacy constraint. Our upper bounds rely on (private) variants of the
Good-Turing estimator. Our lower bounds use additive local neighborhoods that
more precisely captures the hardness of distribution estimation in KL
divergence, compared to ones considered in prior works.

</details>


### [210] [On the Convergence Analysis of Muon](https://arxiv.org/abs/2505.23737)
*Wei Shen, Ruichuan Huang, Minhui Huang, Cong Shen, Jiawei Zhang*

**主要类别:** stat.ML

**概要:** 本论文分析了Muon优化器的收敛速度，并揭示了其在神经网络训练中优于梯度下降法的原因。实验结果支持理论发现，即Muon能从Hessian矩阵的低秩和近似块对角结构中获益。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Muon优化器在神经网络训练中表现出色，但对其收敛特性和优越性能的理论理解仍不足。

**方法:** 作者进行了全面的收敛率分析，比较了Muon与梯度下降（GD）的性能，并确定了Muon优于GD的条件。

**结果:** 理论分析表明，Muon可以从Hessian矩阵的低秩和近似块对角特性中受益，这些特性在实际神经网络训练中很常见。实验结果验证了理论分析。

**结论:** Muon优化器在特定条件下能够显著优于传统优化器，其优势来源于对矩阵结构参数的有效利用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Convergence+Analysis+of+Muon，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.23737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.23737&send_immediately=true&force_search=false)

**原文摘要:** The majority of parameters in neural networks are naturally represented as
matrices. However, most commonly used optimizers treat these matrix parameters
as flattened vectors during optimization, potentially overlooking their
inherent structural properties. Recently, an optimizer called Muon has been
proposed, specifically designed to optimize matrix-structured parameters.
Extensive empirical evidence shows that Muon can significantly outperform
traditional optimizers when training neural networks. Nonetheless, the
theoretical understanding of Muon's convergence behavior and the reasons behind
its superior performance remain limited. In this work, we present a
comprehensive convergence rate analysis of Muon and its comparison with
Gradient Descent (GD). We further characterize the conditions under which Muon
can outperform GD. Our theoretical results reveal that Muon can benefit from
the low-rank and approximate blockwise diagonal structure of Hessian matrices
-- phenomena widely observed in practical neural network training. Our
experimental results support and corroborate the theoretical findings.

</details>
