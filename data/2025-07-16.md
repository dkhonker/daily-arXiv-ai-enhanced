<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 19]
- [cs.AI](#cs.AI) [总数: 22]
- [cs.CR](#cs.CR) [总数: 18]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H., Siddhrath Jandial, Shashank S. Agashe, Rajesh Kumar Reddy Moore, Youngkwan Kim*

**主要类别:** cs.LG

**AI概要:** 本文提出新的工具对工具匹配（TTTM）分析流程，旨在解决半导体制造设备中传统方法的不足。通过假设不匹配设备在数据上会表现出更高的方差和/或更多模式，研究人员开发的方法在单变量和多变量情况下均显示了有效性，并对多变量算法的超参数敏感性进行了分析。


<details>
  <summary>更多</summary>
  
**动机:** 传统的TTTM方法依赖于静态配置数据或黄金参考，这些在商业生产线中难以获得。此外，现有方法无法很好地扩展到异构环境，即来自不同供应商、不同型号的设备。

**方法:** 作者提出了新的TTTM分析流程，假设不匹配的设备会在数据中显示出更高的方差和/或更多的模式。基于这一假设，他们开发了单变量和多变量方法来评估设备间的匹配度。

**结果:** 最佳的单变量方法与方差和模式数量分别达到了>0.95和>0.5的相关系数，而最佳的多变量方法与表现最好的单变量方法之间的相关系数超过了0.75。这表明所提出的方法是有效的。

**结论:** 所提出的TTTM分析流程克服了传统方法中存在的问题，在单变量和多变量情况下都展示了良好的效果。此外，还研究了多变量算法对超参数的敏感性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tool-to-Tool+Matching+Analysis+Based+Difference+Score+Computation+Methods+for+Semiconductor+Manufacturing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10564&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [2] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的损失函数，即线性自适应交叉熵损失函数，并在CIFAR-100数据集上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的标准交叉熵损失函数缺乏对真实类别的预测概率的依赖，这可能限制了优化过程的效果。为了改进这一点，作者提出了线性自适应交叉熵损失函数，以增强分类任务中的优化效果。

**方法:** 新提出的损失函数基于信息理论，与标准交叉熵损失函数相比，增加了一个取决于真实类别预测概率的项。这一特性有助于优化使用one-hot编码类别标签的分类任务。

**结果:** 在基于ResNet模型和CIFAR-100数据集上的评估显示，所提出的损失函数在分类准确性方面始终优于标准交叉熵损失函数，同时保持了几乎相同的效率。

**结论:** 线性自适应交叉熵损失函数不仅提高了分类准确性，而且保持了简单性和计算效率，为未来损失函数设计的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Cross+Entropy+with+a+Linearly+Adaptive+Loss+Function+for+Optimized+Classification+Performance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10574&send_immediately=true&force_search=false)

**原文摘要:** We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [3] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的自适应学习率调度器VolSched，它通过计算长期和短期准确率波动率之间的比率来动态调整学习率。在CIFAR-100数据集上的实验表明，VolSched可以提高模型性能，并使模型获得更宽的最小值，从而实现更好的泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的预定义和自适应学习率调度器仍可能导致次优的泛化性能，因此需要一种新的方法来改进这一点。

**方法:** 引入了VolSched，一种受几何布朗运动等随机过程中的波动性概念启发的自适应学习率调度器。它通过计算长期和短期准确率波动率之间的比率来动态调整学习率。

**结果:** 当与ResNet-18和ResNet-34配对时，VolSched分别提高了1.4和1.3个百分点的top-1准确性。Hessian分析显示，VolSched找到的最终解比次佳基线平坦38％。

**结论:** VolSched可以更有效地探索损失景观，提供更广泛的最小值，从而提高模型的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Adaptive+Volatility-based+Learning+Rate+Scheduler，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10575，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10575&send_immediately=true&force_search=false)

**原文摘要:** Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [4] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

**主要类别:** cs.LG

**AI概要:** 本文提供了一个单层Transformer的通用近似定理，证明了它能够以任意精度近似任何连续的序列到序列映射，并通过案例研究展示了该结果的实际意义。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习和Transformer架构在许多领域中取得了成功，但对这些模型的理论理解仍然有限。为了弥补这一差距，本文旨在深入探讨深度学习和Transformer的数学基础，并提出新的理论成果。

**方法:** 作者回顾了线性代数、概率论和优化中的关键概念，详细分析了多头自注意力机制和反向传播算法，并提出了一个关于Transformer的通用近似定理，随后给出了正式陈述和完整证明。

**结果:** 主要贡献是一个通用近似定理，表明单层Transformer可以近似任何连续的序列到序列映射至任意精度。此外，还提供了展示该结果实际影响的案例研究。

**结论:** 本研究加深了对Transformer模型的理论理解，并有助于弥合理论与实践之间的鸿沟。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Approximation+Theorem+for+a+Single-Layer+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10581，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10581&send_immediately=true&force_search=false)

**原文摘要:** Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [5] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha, Diego Kreutz, Gabriel Canto, Hendrio Bragança, Eduardo Feitosa*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了MH-FSF框架，一个全面、模块化和可扩展的平台，用于促进特征选择方法的再现和实施。它提供了17种方法的实现，并在10个公开的Android恶意软件数据集上进行了系统评估。研究结果强调了数据预处理和选择标准的重要性，以及统一平台对比较不同特征选择技术的意义。


<details>
  <summary>更多</summary>
  
**动机:** 当前的研究经常受到有限的基准测试和依赖专有数据集的影响，这严重阻碍了可重复性并可能对整体性能产生负面影响。

**方法:** 研究人员开发了MH-FSF框架，该框架包含了17种方法（11种经典方法和6种特定领域的方法），并在10个公开可用的Android恶意软件数据集上进行了系统评估。

**结果:** 研究结果显示，在平衡和不平衡的数据集上的表现存在差异，突显了考虑这些不对称性的数据预处理和选择标准的关键需求。

**结论:** 通过提供这个框架，研究人员旨在显著拓宽现有的文献，并为特征选择的新研究方向铺平道路，特别是在Android恶意软件检测的背景下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MH-FSF%3A+A+Unified+Framework+for+Overcoming+Benchmarking+and+Reproducibility+Limitations+in+Feature+Selection+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10591，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10591&send_immediately=true&force_search=false)

**原文摘要:** Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [6] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo, Di Wu, Yi He, Shuqiang Huang, Xindong Wu*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了OL-MDISF，一种用于处理混合类型、漂移和不完整流数据的在线学习方法。它通过构建潜在的copula-based表示，利用集成熵和潜在不匹配检测漂移，并进行结构感知的伪标记。


<details>
  <summary>更多</summary>
  
**动机:** 在线学习面对三个主要挑战：现实世界数据流的异质性对传统参数模型提出的挑战；数据流分布随时间变化导致模型性能急剧下降；由于时间和成本限制无法标注所有数据实例。

**方法:** 作者提出了一种称为OL-MDISF的方法，该方法为异构特征构建了潜在的copula-based表示，通过集成熵和潜在不匹配检测漂移，并执行结构感知的伪标记。

**结果:** 实验在14个真实世界的数据集上进行了，包括CER趋势，消融研究，敏感性分析和时间集成动态等方面，提供了复杂，弱监督流数据在线学习的可重复基准。

**结论:** 本论文提供了一个独立的技术参考文献，讨论了相关工作，并希望提供的文档能作为复杂、弱监督流数据在线学习的可重复基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Extension+OL-MDISF%3A+Online+Learning+from+Mix-Typed%2C+Drifted%2C+and+Incomplete+Streaming+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10594，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10594&send_immediately=true&force_search=false)

**原文摘要:** Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [7] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu, Wenxuan Tu, Yue Liu, Miaomiao Li, Wenpeng Lu, Zhigang Luo, Xinwang Liu, Ping Chen*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的方法DTRGC，用于处理属性缺失图的深度图聚类问题。该方法通过动态聚类感知特征传播、层次化邻域感知插补和跳跃式表示增强来提高聚类性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有属性缺失图的填补方法未能充分考虑节点邻域信息量的变化，导致结果不可靠，特别是在节点已知邻域信息不足的情况下。

**方法:** DTRGC方法首先对具有足够已知邻域信息的节点进行处理，并将填补结果作为新知识迭代地填补更复杂的节点，同时利用聚类信息校正填补错误。具体步骤包括：1) 动态聚类感知特征传播（DCFP）初始化缺失节点属性；2) 层次化邻域感知插补（HNAI）根据邻域属性完整性分组并优先处理信息最丰富的节点；3) 跳跃式表示增强（HRE）整合多跳信息以丰富节点表示。

**结果:** 实验结果显示，DTRGC在六个常用的图数据集上显著提高了各种深度图聚类方法在属性缺失图上的聚类性能。

**结论:** DTRGC提供了一种有效的方法来应对属性缺失图的深度图聚类挑战，特别是对于那些邻域信息不完整的节点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Divide-Then-Rule%3A+A+Cluster-Driven+Hierarchical+Interpolator+for+Attribute-Missing+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10595&send_immediately=true&force_search=false)

**原文摘要:** Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [8] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao, Chonggang Lu, Yue Wang, Zheyong Xie, Ziyan Liu, Haofu Qian, JianZhao Huang, Fangcheng Shi, Zijie Meng, Hongcheng Guo, Mingqian He, Xinze Lyu, Yiming Lu, Ziyang Xiang, Zheyu Ye, Chengqiang Lu, Zhe Xu, Yi Wu, Yao Hu, Yan Gao, Jun Fan, Xiaolong Jiang, Weiting Liu, Boyang Wang, Shaosheng Cao*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为RedOne的领域特定大型语言模型，通过三阶段训练策略，在社交网络服务中实现了性能瓶颈的突破和全面的基础建立。实验表明，RedOne在8个主要任务和SNS双语评估基准上平均提升了14.02%和7.56%，并显著改善了有害内容检测和帖子查看搜索的表现。


<details>
  <summary>更多</summary>
  
**动机:** 社交网络服务（SNS）的快速增长为平台内容管理和互动质量改进带来了重大挑战。现有的大型语言模型研究专注于孤立的任务，不仅在个别场景中的数据扩展中遇到效益递减的问题，而且无法灵活适应多样化的现实世界环境。

**方法:** 为了解决上述问题，研究人员引入了RedOne，这是一种专门针对社交网络服务设计的领域特定大型语言模型。该模型采用三阶段训练策略：持续预训练、监督微调和偏好优化，并使用大规模真实世界数据集进行训练。

**结果:** RedOne展示了强大的通用能力，在8个主要SNS任务上平均提高了14.02%，在SNS双语评估基准上提高了7.56%。在线测试还显示，与单任务微调基线模型相比，RedOne将有害内容检测的曝光率降低了11.23%，并将帖子查看搜索的点击页面率提高了14.95%。

**结论:** 这些结果确立了RedOne作为一种稳健的领域特定大型语言模型的地位，适用于社交网络服务，表现出色的泛化能力和在实际应用中的巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RedOne%3A+Revealing+Domain-specific+LLM+Post-Training+in+Social+Networking+Services，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10605&send_immediately=true&force_search=false)

**原文摘要:** As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [9] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu, Vidya A. Chhabria*

**主要类别:** cs.LG

**AI概要:** 提出DALI-PD框架，生成合成布局热图以加速物理设计中的机器学习研究。


<details>
  <summary>更多</summary>
  
**动机:** 现有的高质量大规模训练数据集有限且难以创建，公共数据集静态、生成慢且需要频繁更新。

**方法:** 使用扩散模型生成包括功率、IR压降、拥塞、宏放置和单元密度图在内的多样化的布局热图。

**结果:** 创建了包含超过20,000个布局配置的数据集，这些热图与真实布局非常相似，并提高了下游ML任务的准确性。

**结论:** DALI-PD框架可以生成合成布局热图，从而加速物理设计中的机器学习研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DALI-PD%3A+Diffusion-based+Synthetic+Layout+Heatmap+Generation+for+ML+in+Physical+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10606&send_immediately=true&force_search=false)

**原文摘要:** Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [10] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor, Chioma Nwafor, Amro Zakaria, Nkechi Nwankwo*

**主要类别:** cs.LG

**AI概要:** 阿联酋依赖海水淡化满足90%以上的饮用水需求，但该过程耗能高且受气候因素影响大。本文提出了一种新颖的两阶段预测建模架构，用于预测气溶胶光学深度（AOD）和海水淡化性能效率损失，并开发了基于AOD预测值的灰尘感知规则控制逻辑，以调整海水淡化厂的操作参数。此外，还开发了一个交互式仪表板，为气候适应性规划提供管理决策支持系统。


<details>
  <summary>更多</summary>
  
**动机:** 阿联酋海水淡化过程能耗高，对能源相关CO2排放贡献大，并且面对气候变化如海水温度上升、盐度增加等带来的可持续性挑战。为了应对这些挑战，需要一种方法来预测AOD及其对太阳能淡化系统的影响，并制定相应的控制策略。

**方法:** 提出了一个两阶段的预测建模架构：第一阶段使用卫星时间序列和气象数据预测AOD；第二阶段利用预测的AOD和其他气象因素预测淡化性能效率损失。此外，还提出了一个基于预测值的灰尘感知规则控制逻辑。

**结果:** 所提出的框架实现了98%的准确率，SHAP分析揭示了导致系统退化的关键驱动因素。此外，通过将预测模型和规则控制集成到一个交互式仪表板中，增强了研究结果的实际应用价值。

**结论:** 本研究所提出的预测模型和控制逻辑可以为海水淡化过程中的操作参数调整、维护计划调整和能源源切换提供有效的指导，有助于提高系统的经济性和环境友好性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Feed-Forward+Artificial+Intelligence+Pipeline+for+Sustainable+Desalination+under+Climate+Uncertainties%3A+UAE+Insights，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10609&send_immediately=true&force_search=false)

**原文摘要:** The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [11] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye, Yingzi Huangfu, Shujian Gao, Wei Ren, Weifan Liu, Zekuan Yu*

**主要类别:** cs.LG

**AI概要:** 提出FedGSCA框架，通过全局样本选择器和客户自适应调整机制提高在噪声标签下的联邦学习模型稳定性和性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习方法难以应对医疗数据中的噪声异质性和类别不平衡问题，导致训练不稳定和模型性能下降。

**方法:** 引入了全局样本选择器（Global Sample Selector）来聚合所有客户端的噪声知识，解决噪声异质性问题，并提出客户自适应调整（Client Adaptive Adjustment, CAA）机制，结合自适应阈值伪标签生成和鲁棒信度标签损失，动态调整以适应类别分布并处理噪声标签。

**结果:** 在现实世界结肠切片数据集和两个合成医学数据集上评估，结果显示FedGSCA优于现有最先进方法，尤其在极端和异构噪声场景中表现出色。

**结论:** FedGSCA展示了显著优势，提高了模型稳定性并能有效处理复杂噪声，适合实际医疗联邦学习场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedGSCA%3A+Medical+Federated+Learning+with+Global+Sample+Selector+and+Client+Adaptive+Adjuster+under+Label+Noise，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10611，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10611&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [12] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen, Siqi Wang, Teng Xiao, Yudong Wang, Shiqi Chen, Xunliang Cai, Junxian He, Jingang Wang*

**主要类别:** cs.LG

**AI概要:** 本文重新审视了自然语言处理中的缩放定律，通过实证分析确定了数据质量和训练策略对模型性能的影响，并提出了一个次优的缩放定律。


<details>
  <summary>更多</summary>
  
**动机:** 传统的缩放定律认为增加模型大小和训练数据可以提高性能，但最近的研究发现，在大型语言模型中，性能改进减速，这种现象被称为次缩放。

**方法:** 本文通过广泛的实证分析超过400个模型，研究数据质量与训练策略对模型性能的影响。

**结果:** 高数据密度和非最优资源配置是导致次缩放的主要因素。高数据密度会导致由于冗余信息而产生的收益递减，而最优资源配置对于持续的性能改进至关重要。

**结论:** 本文提出了一种次优缩放定律，该定律更好地预测了次缩放状态下的性能，强调了数据质量和多样性的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sub-Scaling+Laws%3A+On+the+Role+of+Data+Density+and+Training+Strategies+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10613&send_immediately=true&force_search=false)

**原文摘要:** Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [13] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu, Rui Zhang, Xi Lin, Zhichao Lu, Qingfu Zhang*

**主要类别:** cs.LG

**AI概要:** 本文探讨了为算法设计定制大型语言模型（LLMs）的潜力，并通过实验展示了微调后的LLMs在不同任务上的优越表现和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法多依赖于为通用编程任务训练的现成LLMs，对于是否需要特定为算法设计定制的LLMs及其如何获得和跨任务表现的问题尚未解答。

**方法:** 引入了一种多样性感知排序（DAR）采样策略以平衡训练数据的多样性和质量，并使用直接偏好优化高效对齐LLM输出与任务目标。

**结果:** 微调后的LLMs在三项不同的算法设计任务中显著超越了其现成版本，在较小的Llama-3.2-1B-Instruct上甚至匹配了较大的Llama-3.1-8B-Instruct的表现。

**结论:** 研究结果表明，针对具体任务调整LLMs在算法设计中具有重要价值，并为未来的研究开辟了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-tuning+Large+Language+Model+for+Automated+Algorithm+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10614，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10614&send_immediately=true&force_search=false)

**原文摘要:** The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [14] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani, Aryo Pradipta Gema, Seraphina Goldfarb-Tarrant, Ivan Titov*

**主要类别:** cs.LG

**AI概要:** 论文比较了强化学习（RL）和监督微调（SFT）在数学问题上的训练动态，发现RL对数学有小幅提升但对知识密集型基准稍有下降；SFT这两方面变化更明显。模型参数分析表明，SFT对中层MLPs影响更大，可能是导致领域外性能下降的原因。部分模型冻结训练的尝试结果不明确。整体观察表明RL放大现有能力，而SFT用新技能替换旧技能。


<details>
  <summary>更多</summary>
  
**动机:** 了解强化学习（RL）和监督微调（SFT）在大型语言模型（LLM）后训练阶段针对数学推理和代码数据集的训练动态，以解释为什么它们在领域内和领域外的表现不同。

**方法:** 在同一数学问题上，使用相同的模型和相似的超参数，对RL和SFT进行比较分析。检查不同检查点的模型参数，观察算法如何修改查询和键权重。尝试通过在训练期间冻结模型的部分来减轻知识密集型基准上的性能下降。

**结果:** RL在数学问题上有轻微的领域内增益，而在知识密集型基准如MMLU上略有下降；这些趋势在SFT中更为明显。SFT不仅更多地更新了查询和键权重，还影响了中层MLPs，这可能导致了领域外性能的下降。部分模型冻结训练的结果好坏参半。

**结论:** 研究结果初步表明，RL倾向于放大模型现有的能力，而SFT则倾向于用新的技能替换旧的技能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalpel+vs.+Hammer%3A+GRPO+Amplifies+Existing+Capabilities%2C+SFT+Replaces+Them，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10616&send_immediately=true&force_search=false)

**原文摘要:** Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [15] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

**主要类别:** cs.LG

**AI概要:** 本文研究了大型语言模型预训练算法创新所需的计算资源，并分析了计算上限对算法创新的影响。


<details>
  <summary>更多</summary>
  
**动机:** 了解开发算法创新所需的计算资源，以及这些需求随时间的变化情况，评估计算上限是否能显著减缓AI算法的进步。

**方法:** 收集并分析了36种用于Llama 3和DeepSeek-V3的预训练算法创新，估计每种创新在开发中使用的总FLOP和硬件的FLOP/s。

**结果:** 使用大量资源的创新每年的需求翻倍，但即使是有严格限制的计算上限也允许一半以上的记录在案的创新。

**结论:** 计算上限单独存在不太可能显著减缓AI算法的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compute+Requirements+for+Algorithmic+Innovation+in+Frontier+AI+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10618，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10618&send_immediately=true&force_search=false)

**原文摘要:** Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [16] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa, Tobi Awodunmila, Muhammad Ahmed Mohsin, Ahsan Bilal, Muhammad Ali Jamshed*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种元学习框架，以实现5G/6G网络频谱的动态分配。相比传统深度强化学习方法，该框架下的模型在模拟环境中表现出更高的网络吞吐量、更低的SINR和延迟违规，并且具有更好的资源分配公平性。


<details>
  <summary>更多</summary>
  
**动机:** 在5G/6G网络中，动态分配频谱对于高效利用资源至关重要。然而，传统的深度强化学习由于样本复杂度高和无指导探索带来的安全风险，通常难以应用。

**方法:** 研究人员提出了一个元学习框架，使智能体能够学习稳健的初始策略，并快速适应新的无线场景，同时将数据需求降到最低。实验中实现了三种元学习架构：与模型无关的元学习（MAML）、循环神经网络（RNN）以及带有注意力机制增强的RNN。

**结果:** 与非元学习的DRL算法PPO基线相比，基于注意力机制的元学习代理达到了48 Mbps的峰值平均网络吞吐量，而PPO基线则急剧下降至10 Mbps。此外，本方法还使SINR和延迟违规减少了超过50%，并且具有0.7的公平性指数，显示出更佳的资源分配能力。

**结论:** 研究证明，元学习是复杂无线系统中实现智能化控制的有效且更安全的选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta-Reinforcement+Learning+for+Fast+and+Data-Efficient+Spectrum+Allocation+in+Dynamic+Wireless+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10619&send_immediately=true&force_search=false)

**原文摘要:** The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [17] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu, Hao Miao, Cheng Long, Yan Zhao, Ziyue Li, Panos Kalnis*

**主要类别:** cs.LG

**AI概要:** 本文提供了一个关于基于LLM的跨模态时间序列分析的最新概述，包括分类现有方法、讨论其应用和总结开放挑战。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）在时间序列分析方面显示出巨大的潜力，但它们与文本数据之间存在跨模态差距。为了弥合这一差距，并扩展LLMs在解决实际问题中的应用，作者撰写了这篇教程。

**方法:** 作者引入了一个分类法，根据跨模态建模策略（如转换、对齐和融合）将现有方法分为三类，并讨论了这些方法在一系列下游任务中的应用。

**结果:** 参与者将获得对当前进展、方法和未来研究方向的深入了解。

**结论:** 该教程旨在扩大LLM在解决跨模态时间序列分析中实际问题的应用，同时平衡有效性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLMs+Meet+Cross-Modal+Time+Series+Analytics%3A+Overview+and+Directions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10620&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [18] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih, Deyu Cao, Tejas Balaji*

**主要类别:** cs.LG

**AI概要:** 本文通过引入优化动态的结构先验，将梯度下降轨迹建模为轨迹推断问题，并提出了一种统一的框架——梯度流匹配。实验表明该方法在生成分布内权重、改进下游训练初始化和支持微调以提高性能方面表现优异，并且在检测有害协变量偏移方面有实际应用。


<details>
  <summary>更多</summary>
  
**动机:** 扩散和基于流的生成模型已经在图像合成、视频生成和自然语言建模等领域取得了显著成功。然而，这些技术尚未广泛应用于权重空间学习。本研究旨在利用最近的技术，将从优化动态中得出的结构先验纳入权重空间学习中。

**方法:** 作者将由梯度下降引起的轨迹建模为轨迹推断问题，并在此基础上提出了一个统一的框架——梯度流匹配。此外，作者还探索了架构和算法的选择，包括奖励微调、使用自动编码器进行潜在权重表示、基于任务特定上下文数据的条件化以及采用信息源分布（如Kaiming均匀分布）。

**结果:** 实验表明，该方法在生成分布内权重方面与基线持平或超越；改善了下游训练的初始化；支持微调以增强性能；并且在一个实际应用案例中，即在安全关键系统中检测有害协变量偏移方面，该方法的表现优于最接近的可比基线。

**结论:** 该研究表明，将优化路径作为归纳偏差处理的方法是有效的，尤其是在生成分布内权重和改进下游训练初始化方面。此外，该方法在检测有害协变量偏移方面具有潜在的应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flows+and+Diffusions+on+the+Neural+Manifold，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10623，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10623&send_immediately=true&force_search=false)

**原文摘要:** Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [19] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang, Shiwen Xu, Michael Horton, Joachim Gudmundsson, Zhiyong Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于图增强Transformer的深度学习模型HIGFormer，用于足球比赛结果预测。该模型通过多层次交互框架捕捉球员和团队间的异质互动，并在大规模真实世界数据集上显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的足球比赛结果预测方法往往忽视了球员和球队之间交互的异质性，这对准确建模比赛动态至关重要。

**方法:** HIGFormer包含三个部分：(1) 球员交互网络，通过异质交互图编码球员表现；(2) 团队交互网络，从团队视角构建交互图以模拟历史比赛关系；(3) 比赛对比Transformer，共同分析团队和个人层面的信息来预测比赛结果。

**结果:** 在WyScout开放访问数据集上的广泛实验表明，HIGFormer在预测准确性方面显著优于现有方法。此外，该模型还为球员表现评估提供了有价值的见解。

**结论:** HIGFormer提供了一种新视角进行人才选拔和团队策略分析，并在足球比赛结果预测中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Player-Team+Heterogeneous+Interaction+Graph+Transformer+for+Soccer+Outcome+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10626&send_immediately=true&force_search=false)

**原文摘要:** Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

**主要类别:** cs.AI

**AI概要:** SAMEP是一种新的框架，它通过持久的、安全的和语义可搜索的记忆共享来解决当前AI代理架构中的短暂记忆限制问题。实验结果显示其有效减少了冗余计算，并提高了上下文相关性分数，同时完全符合法规要求。


<details>
  <summary>更多</summary>
  
**动机:** 当前的AI代理架构受到短暂记忆的限制，无法在会话和代理边界之间有效地协作和共享知识。

**方法:** SAMEP实现了分布式内存库，具有基于矢量的语义搜索、加密访问控制（AES-256-GCM）和与现有代理通信协议兼容的标准API（MCP，A2A）。

**结果:** 实验结果表明，SAMEP使冗余计算减少了73%，上下文相关性得分提高了89%，并完全符合法规要求，包括审计跟踪生成。

**结论:** SAMEP实现了一种新的持久性、协作性的AI代理生态系统，在保持安全性和隐私保证的同时，解决了三个关键挑战：跨代理会话的持久上下文保留，具有细粒度访问控制的安全多代理协作以及相关历史上下文的有效语义发现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SAMEP%3A+A+Secure+Protocol+for+Persistent+Context+Sharing+Across+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10562，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10562&send_immediately=true&force_search=false)

**原文摘要:** Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [21] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

**主要类别:** cs.AI

**AI概要:** 本研究质疑在去中心化多智能体强化学习中引入人工归纳偏置的必要性，并通过AI母语框架证明了无需外部归纳偏置即可实现有效的符号交流，这为连接主义和符号主义之间的桥梁提供了新的途径。


<details>
  <summary>更多</summary>
  
**动机:** 该研究旨在解决去中心化多智能体强化学习中的“联合探索困境”，并质疑是否需要引入人工归纳偏置来促进通信的出现。

**方法:** 研究采用了基于矢量量化变分自编码器（VQ-VAE）的AI母语框架，使智能体拥有内源符号系统，以观察其神经表示是否会自发地进行语义压缩和语义收敛。

**结果:** 实验结果表明，智能体能够实现有效的符号交流，并展示了更强的一般性和效率。此外，还发现了符号使用呈现出显著的幂律分布。

**结论:** 该研究表明，通过AI母语框架，智能体可以在没有外部归纳偏置的情况下实现有效的符号交流，这为未来的研究开辟了新的方向，特别是在连接主义和符号主义的结合方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Mother+Tongue%3A+Self-Emergent+Communication+in+MARL+via+Endogenous+Symbol+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10566&send_immediately=true&force_search=false)

**原文摘要:** In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [22] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis, Ranjan Sapkota, Manoj Karkee, Nikolaos D. Tselikas*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种新的模块化代理AI视觉分类框架，通过集成多模式代理、非视觉推理协调器和检索增强生成模块，显著提高了零样本设置下的苹果叶病诊断准确率。


<details>
  <summary>更多</summary>
  
**动机:** 当前的多智能体架构虽然能够融合视觉和语言理解，但在零样本设置下如何信任这些智能体仍是一个挑战。为了提高在零样本情况下的可信度和准确性，提出了该研究。

**方法:** 该研究引入了包含一般多模态智能体、非视觉推理协调器和检索增强生成（RAG）模块的框架，并应用于苹果叶病诊断中，通过三种配置进行测试，使用置信度校准指标来调整不同智能体间的信任度。

**结果:** 结果显示，在零样本设置下，使用基于信任的协调和RAG方法可将准确率提高77.94%，总体达到85.63%。GPT-4o表现出更好的校准效果，而Qwen-2.5-VL则显得过于自信。此外，图像-RAG使预测结果能基于视觉相似案例进行迭代重新评估。

**结论:** 提出的系统分离了感知与元推理，使得多智能体AI更加可扩展和可解释，适用于诊断、生物学及其他对信任度要求高的领域。所有模型、提示、结果及系统组件已公开发布以支持可重复性、透明性和社区基准测试。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Orchestrator-Agent+Trust%3A+A+Modular+Agentic+AI+Visual+Classification+System+with+Trust-Aware+Orchestration+and+RAG-Based+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10571，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10571&send_immediately=true&force_search=false)

**原文摘要:** Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [23] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

**主要类别:** cs.AI

**AI概要:** 大型语言模型（LLMs）在需要符号推理、算术准确性和逻辑一致性等任务上系统性失败，这种失败源于计算执行而非知识访问，被称为“计算分裂脑综合症”。


<details>
  <summary>更多</summary>
  
**动机:** 解释为什么大型语言模型在一些特定任务上的表现不佳，并揭示其根本原因。

**方法:** 通过受控实验和架构分析来诊断这些失败的原因，并将LLMs的行为与正确原则的应用进行对比。

**结果:** 发现LLMs经常能表达正确的原则但无法可靠地应用它们，这一现象被描述为计算的“分裂脑综合症”，并指出这限制了模型行为的灵活性。

**结论:** 当前的LLM能力存在界限，未来应开发具有元认知控制、原则提升和结构化执行的新模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comprehension+Without+Competence%3A+Architectural+Limits+of+LLMs+in+Symbolic+Computation+and+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10624&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [24] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang, Xue Xiao, Ping Yin, Taotao Xie*

**主要类别:** cs.AI

**AI概要:** KG2data结合了知识图谱、大语言模型和工具使用技术，针对气象领域开发了一种智能数据获取和查询处理系统。相比其他类似系统，它在API调用准确性上表现更佳，并且通过使用知识图谱作为持久性记忆，提高了复杂查询处理能力，降低了调整LLM的成本。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）在通过API调用有效利用工具方面的能力尚未充分研究，特别是在气象学等知识密集型领域。为了提高LLMs在这些领域的性能，需要解决它们对特定领域知识的有限访问问题。

**方法:** KG2data整合了知识图谱、LLMs、ReAct代理和工具使用技术，以实现智能数据获取和查询处理。该系统使用虚拟API来评估三种指标上的API调用准确性：名称识别失败、幻觉失败和调用正确性。同时，它使用知识图谱作为持久性内存，增强了内容检索、复杂查询处理、特定领域推理、语义关系解析和异构数据集成。

**结果:** 与RAG2data和chat2data相比，KG2data在API调用准确性上表现出色，分别达到了1.43%，0%和88.57%，而其他两个系统的相应指标分别为(16%，10%，72.14%)和(7.14%，8.57%，71.43%)。此外，KG2data还降低了微调LLMs的成本，使得系统能够更好地适应不断变化的领域知识和API结构。

**结论:** KG2data为高知识需求领域提供了新的解决方案，实现了基于知识的智能问答和数据分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+the+Capabilities+of+Large+Language+Models+for+API+calls+through+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10630&send_immediately=true&force_search=false)

**原文摘要:** API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [25] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova, Aleksandr Puzikov, Boris Bliznukov, Radu State*

**主要类别:** cs.AI

**AI概要:** 本文提供Web of Agents (WoA) 的首个全面进化概述，引入四轴分类法系统分析代理架构，并强调新的智能定位范式对现代Agentic AI的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于WoA的研究分散在不同社区，缺乏对这一领域整体发展轨迹的理解。作者希望通过此文为WoA提供一个综合的进化概览，以解决这种碎片化的问题。

**方法:** 文章介绍了一种四轴分类法（语义基础、通信范式、智能位置、发现机制），用于比较各代代理架构，并分析了从早期标准如FIPA标准和基于OWL的语义代理到现代协议如A2A和MCP的演变。

**结果:** 该分析揭示了智能定位从外部数据或平台到嵌入代理核心模型（LLM）的范式转变，这是现代Agentic AI的基础。同时指出新协议虽重要但不足以建立强大的开放可信生态系统。

**结论:** 研究总结认为，未来的研究前沿在于解决持续存在的社会技术挑战，并勾画了一个新的议程，重点是去中心化身份、经济模型、安全性和治理新兴WoA。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Semantic+Web+and+MAS+to+Agentic+AI%3A+A+Unified+Narrative+of+the+Web+of+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10644，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10644&send_immediately=true&force_search=false)

**原文摘要:** The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [26] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani, Sean O Leary, James McDermott*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于规则的音乐生成方法，通过变异现有曲调的语法结构来创造新曲调。


<details>
  <summary>更多</summary>
  
**动机:** 动机是探索一种新颖的方法来生成音乐，该方法不是直接改变曲调，而是通过改变表示曲调重复结构的语法规则。这允许创建与原始曲调有关的新曲调，并研究这些曲调在多次变异过程中的变化。

**方法:** 使用Sequitur算法解析曲调以找到所有重复的结构（Pathway Assembly）。然后对由此产生的语法进行19种不同类型的变异操作。变异后的语法被展开以返回新的曲调。

**结果:** 通过编辑距离、结构复杂性和长度的变化展示了曲调在多次变异后如何变化。分析了每种变异类型的效果大小，并回顾了输出曲调的音乐性。

**结论:** 研究表明，通过对曲调的语法结构进行变异可以产生新的曲调，同时保留与原始曲调的联系。此外，不同的变异类型对新曲调的影响程度也有所不同。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parsing+Musical+Structure+to+Enable+Meaningful+Variations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10740，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10740&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [27] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota, Nicolas Tsesmetzis, Faruk O. Alpak, Apurva Gala, Detlef Hohl*

**主要类别:** cs.AI

**AI概要:** 这篇技术评论文章探讨了数据中心的能源消耗情景及其对温室气体排放的影响，评估了AI在2035年之前对CO2排放的净影响，并讨论了AI在未来优化各行业流程、减少碳足迹的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 随着大量数据的可用性、计算资源和高级算法的发展，AI已经进入了几乎每个领域，特别是在建设支持AI模型开发和运行的数据中心方面引起了大量的投资和兴趣。本文旨在探讨这些发展对能源消耗和温室气体（GHG）排放的影响，以及AI在短期内和长期内对二氧化碳减排的可能作用。

**方法:** 作者通过分析和预测数据中心的能源消耗模式来研究其对温室气体排放的影响。他们考虑了到2030年的近期预测和2035年及以后的长期展望，同时探讨了AI在不同领域的自动化和效率提升能力。

**结果:** 在短期内，由于大型数据中心的电力需求和训练复杂AI模型的需求，AI的增长可能会增加电力消耗和相关的二氧化碳排放。然而，在长期内，AI有潜力通过自动化和优化能源生产、供应和消费等领域的流程，显著减少碳足迹，从而对气候缓解工作提供支持。

**结论:** 尽管AI在初期可能会给环境带来一些增长的痛苦，但从长远来看，它具有支持气候缓解工作的潜力，可以在那些传统解决方案不足的领域为企业和社会创造价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+and+the+Net-Zero+Journey%3A+Energy+Demand%2C+Emissions%2C+and+the+Potential+for+Transition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10750，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10750&send_immediately=true&force_search=false)

**原文摘要:** Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [28] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati, Bimal Karki, Saroj Gopali, Akbar Siami Namin*

**主要类别:** cs.AI

**AI概要:** 本文旨在通过深度学习模型检测物联网恶意攻击，并对深度学习和基于图的模型在恶意网络流量检测方面进行了全面评估。实验结果表明，BERT模型表现最佳，准确率达到99.94%，而多头注意力机制提供了可解释的结果但需要大量处理时间，GraphSAGE模型训练时间最短但准确率最低。


<details>
  <summary>更多</summary>
  
**动机:** 物联网系统中的流量模式既有序列性又多样化，这为深度学习模型提供了丰富的时序模式进行学习，从而能够更有效地检测恶意攻击。

**方法:** 使用了基于GraphSAGE、BERT、TCN以及多头注意力机制、双向长短期记忆（BI-LSTM）等模型来建模时间模式并检测特征重要性。

**结果:** BERT模型表现最佳，准确率为99.94%，具有高精度和召回率，F1-score和AUC-ROC分数为99.99%；多头注意力机制提供了有前途的结果，但需要大量的处理时间；GraphSAGE模型虽然训练时间最短，但准确率最低。

**结论:** 不同类型的深度学习模型在检测物联网恶意攻击方面各有优劣，其中BERT模型在性能上表现最优，而GraphSAGE模型则在效率上有所优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IoT+Malware+Network+Traffic+Detection+using+Deep+Learning+and+GraphSAGE+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10758，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10758&send_immediately=true&force_search=false)

**原文摘要:** This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [29] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King, Nikolos Gurney, John H. Miller, Volkan Ustun*

**主要类别:** cs.AI

**AI概要:** 该研究将AI辅助检测视为分类任务，并通过适当预处理，利用神经网络对抽象任务数据进行有效分类。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能在复杂任务中的广泛应用，如文本生成、医疗诊断和自动驾驶，检测来自AI的辅助变得越来越重要。然而，对于人类来说，尤其是在查看抽象任务数据时，辅助检测具有挑战性。因此，本研究旨在解决这一问题。

**方法:** 研究人员构建了四种不同的神经网络友好型图像公式以及额外的时间序列公式，以明确编码用户的探索/开发情况。然后在三个经典深度学习架构以及一个并行CNN-RNN架构上对每个图像公式的质量进行了基准测试。

**结果:** 研究表明，当数据经过适当的预处理后，常见的模型可以有效地对此类数据进行分类。此外，编码时间和空间量对于在抽象任务中检测AI辅助至关重要。

**结论:** 通过对不同数据表示方法的实验，证明了适当的数据预处理和选择合适的深度学习架构能够提高AI辅助检测的效果。这为未来的研究提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+AI+Assistance+in+Abstract+Complex+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10761&send_immediately=true&force_search=false)

**原文摘要:** Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [30] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi, Bhanu T. Gullapalli, Daiqi Gao, Benjamin M. Marlin, Vivek Shetty, Susan A. Murphy*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种叫做SigmaScheduling的方法，可以根据预测行为时间的不确定性动态安排决策点，提高干预效果。


<details>
  <summary>更多</summary>
  
**动机:** 目前的做法是为所有个体设定固定的时间间隔来安排决策点，这在处理具有不规则日常事务的个体时表现不佳，可能会在目标行为已经发生后才安排决策点，使干预无效。

**方法:** 论文提出的SigmaScheduling方法根据预测行为时间的不确定性动态安排决策点。当行为时间更可预测时，将决策点安排得更接近预测的行为时间；当时间不太确定时，提前安排决策点。

**结果:** 通过使用来自68名参与者的现实世界数据进行评估，SigmaScheduling使得至少70%的情况下的决策点出现在刷牙事件之前，保持了干预和影响行为的机会。

**结论:** SigmaScheduling可以推进精准mHealth的发展，特别是针对像口腔卫生或饮食习惯这样的时间敏感、习惯性行为的JITAIs。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty-Informed+Scheduling+of+Decision+Points+for+Intelligent+Mobile+Health+Interventions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10798，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10798&send_immediately=true&force_search=false)

**原文摘要:** Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [31] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston, Ritvik Ranjan, Sahithi Lakamana, Anthony Spadaro, Selen Bozkurt, Jeanmarie Perrone, Abeed Sarker*

**主要类别:** cs.AI

**AI概要:** 研究发现，使用少量示例提示的大型语言模型（LLM），如GPT-4，在主题分析任务中表现出接近专家水平的性能，这表明LLM可以作为自动化主题分析的有效补充工具。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）在归纳主题分析方面面临挑战，这项任务需要深度解释和领域专业知识。为了评估LLMs是否能够复制由专家驱动的主题分析，特别是针对社交媒体数据进行分析的能力，开展了本研究。

**方法:** 使用两个不同时期的Reddit数据集（n=286 和 n=686，分别用于模型优化和验证），并根据十二个专家衍生的主题，评估了五个LLMs与专家编码的表现。将任务建模为一系列二元分类问题，而不是单一的多标签分类，并通过准确率、精确度、召回率和F1分数来衡量性能。

**结果:** 在验证集上，带有两次示例提示的GPT-4表现最佳（准确率：90.9%；F1分数：0.71）。对于高频率主题，模型得出的主题分布与专家分类非常接近。

**结论:** 研究表明，基于少量示例提示的LLM方法可以实现主题分析的自动化，为定性研究提供了一个可扩展的补充工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Thematic+Analyses+Using+LLMs%3A+Xylazine+Wound+Management+Social+Media+Chatter+Use+Case，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10803&send_immediately=true&force_search=false)

**原文摘要:** Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [32] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia, Heng Zheng, Shawn Bowers, Bertram Ludäscher*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一款名为AF-XRAY的开源工具包，它通过多种方式解析、分析和可视化法律推理中的抽象论证框架（AFs），帮助非专家识别模糊来源并解释论证接受。


<details>
  <summary>更多</summary>
  
**动机:** 目前在使用论证框架（AFs）进行法律推理时，对于非专家来说，确定模糊来源以及解释论证接受仍然具有挑战性。

**方法:** AF-XRAY引入了：(i) 基于博弈论论证长度的分层可视化，揭示了良好的推导结构；(ii) 根据语义角色（主要，次要，失误）对攻击边缘进行分类；(iii) 在模棱两可的三值基础语义上叠加二值解的可视化；(iv) 识别关键攻击集，其暂停可以解决未决定的论证。

**结果:** 通过对关键攻击集的系统生成，AF-XRAY将模糊场景转化为基础解决方案，使用户能够确定模糊的具体原因并探索替代解决方案。此外，该工具还支持目的论法律推理，展示了不同假设如何导致不同的正当结论。

**结论:** AF-XRAY作为一款开源工具包，为探索、分析和可视化法律推理中的抽象论证框架提供了一种新方法，有助于非专家更好地理解法律论证中的复杂性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AF-XRAY%3A+Visual+Explanation+and+Resolution+of+Ambiguity+in+Legal+Argumentation+Frameworks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10831，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10831&send_immediately=true&force_search=false)

**原文摘要:** Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [33] [WhisperKit: On-device Real-time ASR with Billion-Scale Transformers](https://arxiv.org/abs/2507.10860)
*Atila Orhon, Arda Okan, Berkin Durmus, Zach Nagengast, Eduardo Pacheco*

**主要类别:** cs.AI

**AI概要:** WhisperKit 是一个用于实时自动语音识别的优化设备端推理系统，其在延迟和准确性方面优于领先的基于云的系统。


<details>
  <summary>更多</summary>
  
**动机:** 许多商业应用依赖于实时自动语音识别（ASR），其中准确性和延迟是选择部署系统时最重要的因素。作者希望提供一个更优的解决方案，以满足这些需求。

**方法:** 作者开发了 WhisperKit 系统，这是一个优化过的设备端推理系统。它被设计用来实现实时自动语音识别，并且与多种服务器端系统进行了基准测试比较，包括前沿模型、专有模型和开源模型。

**结果:** WhisperKit 实现了最低的延迟 0.46 秒，并达到了最高的准确性 2.2% 的词错误率（WER）。

**结论:** WhisperKit 在实时自动语音识别方面提供了显著改进的性能，超越了当前领先的基于云的服务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WhisperKit%3A+On-device+Real-time+ASR+with+Billion-Scale+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10860，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10860&send_immediately=true&force_search=false)

**原文摘要:** Real-time Automatic Speech Recognition (ASR) is a fundamental building block
for many commercial applications of ML, including live captioning, dictation,
meeting transcriptions, and medical scribes. Accuracy and latency are the most
important factors when companies select a system to deploy. We present
WhisperKit, an optimized on-device inference system for real-time ASR that
significantly outperforms leading cloud-based systems. We benchmark against
server-side systems that deploy a diverse set of models, including a frontier
model (OpenAI gpt-4o-transcribe), a proprietary model (Deepgram nova-3), and an
open-source model (Fireworks large-v3-turbo).Our results show that WhisperKit
matches the lowest latency at 0.46s while achieving the highest accuracy 2.2%
WER. The optimizations behind the WhisperKit system are described in detail in
this paper.

</details>


### [34] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He, Liuyi Wang, Lu Chen, Chengju Liu, Qijun Chen*

**主要类别:** cs.AI

**AI概要:** 提出NavComposer框架，自动产生高质量导航指令，并引入NavInstrCritic系统进行无注释评估。


<details>
  <summary>更多</summary>
  
**动机:** 现有的专家提供或合成的导航指令在数量和质量上不足以支持大规模研究。

**方法:** NavComposer通过分解语义实体并重新组合成自然语言指令，采用模块化结构集成先进技术，并以数据无关的方式运作。NavInstrCritic从对比匹配、语义一致性和语言多样性三个维度对指令进行全面评价。

**结果:** 广泛的实验提供了直接且实用的证据证明该方法的有效性。

**结论:** NavComposer和NavInstrCritic使得指令生成与评估更加可扩展和泛化，促进了更大规模的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NavComposer%3A+Composing+Language+Instructions+for+Navigation+Trajectories+through+Action-Scene-Object+Modularization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10894&send_immediately=true&force_search=false)

**原文摘要:** Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [35] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu, Ting Chen, Irit Hochberg, Zhoujian Sun, Ruth Edry, Zhengxing Huang, Mor Peleg*

**主要类别:** cs.AI

**AI概要:** 研究了基于大型语言模型的多智能体系统在为患有多种慢性病的患者提供更安全的治疗建议方面的可行性和价值。设计并评估了单智能体和多智能体系统的框架，结果显示当前的大型语言模型中，单智能体的表现与多学科团队相当，但建议有时不完整或存在不必要的药物冲突。


<details>
  <summary>更多</summary>
  
**动机:** 由于治疗冲突的风险，为患有多种慢性病的患者推荐治疗方法具有挑战性，现有的决策支持系统存在可扩展性的限制。受到全科医生管理多病症患者方式的启发，本研究旨在探讨使用基于大型语言模型的多智能体系统进行更安全的治疗推荐的可行性和价值。

**方法:** 设计了一个单智能体和一个多智能体系统框架，模拟多学科团队的决策过程，通过让智能体之间进行讨论以解决医疗冲突。该系统在多病症患者的治疗计划任务上进行了评估，并与单智能体方法和真实世界基准进行了比较。

**结果:** 结果显示，在当前的大型语言模型中，单智能体全科医生的表现与多学科团队相当。表现最好的模型能够提供正确地解决所有临床目标的建议，但这些建议有时是不完整的。一些模型还提出了不必要的药物，导致了药物和病情之间或药物与药物之间的不必要冲突。

**结论:** 研究表明，虽然基于大型语言模型的单智能体和多智能体系统可以在治疗规划方面表现出色，但仍然需要进一步完善，以确保建议的全面性和安全性。此外，研究定义了超越技术精度和召回率的评价指标，允许检查所提建议是否符合临床目标及药物负担。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lessons+Learned+from+Evaluation+of+LLM+based+Multi-agents+in+Safer+Therapy+Recommendation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10911，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10911&send_immediately=true&force_search=false)

**原文摘要:** Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [36] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang, Keyan Ding, Kehua Feng, Zeyuan Wang, Ming Qin, Xiaotong Li, Qiang Zhang, Huajun Chen*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的框架KPO，通过整合先验知识和使用强化学习减少生成有害蛋白质的风险。


<details>
  <summary>更多</summary>
  
**动机:** 现有的蛋白质语言模型虽然在序列生成方面表现出色，但也存在生成有害蛋白质序列的风险，如增强病毒传播或逃避免疫反应的序列，这带来了重要的生物安全和伦理挑战。

**方法:** 作者提出了一个名为KPO的知识引导偏好优化框架，该框架通过蛋白质安全知识图谱整合先验知识，采用高效的图修剪策略识别优选序列，并使用强化学习最小化生成有害蛋白质的风险。

**结果:** 实验结果表明，KPO可以有效降低产生危险序列的可能性，同时保持高功能性。

**结论:** KPO提供了一个强大的安全保障框架，可用于生物技术中应用生成模型的安全性保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Safe+and+Controllable+Protein+Generation+via+Knowledge+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10923&send_immediately=true&force_search=false)

**原文摘要:** Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [37] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic, Min-Hong Shih*

**主要类别:** cs.AI

**AI概要:** 结合卷积神经网络和表格数据的方法来预测鸟类在不同气候下的存在，平均准确率为85%


<details>
  <summary>更多</summary>
  
**动机:** 由于气候变化，许多栖息地正在经历范围转移，需要一种准确的模型来预测特定栖息地中是否存在鸟类物种

**方法:** 使用卫星图像和环境特征（例如温度、降水量、海拔）的卷积神经网络以及生态和地理数据的表格方法来预测鸟类的存在

**结果:** 两种系统都以85%的平均准确率预测了鸟类的分布

**结论:** 该方法提供了一种可扩展且可靠的方法来理解鸟类迁徙

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Habitat+Shifts%3A+Integrating+Convolutional+Neural+Networks+and+Tabular+Data+for+Species+Migration+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10993，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10993&send_immediately=true&force_search=false)

**原文摘要:** Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [38] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt, Tunaberk Almaci, Stefan Feuerriegel, Mrinmaya Sachan*

**主要类别:** cs.AI

**AI概要:** ExRec是针对个性化运动推荐的新框架，它通过语义基础的知识追踪来提高学习效果。该方法解决了现有系统中两个关键问题：即问题的语义内容和学生学习的连续结构化进展。在四个真实世界任务中验证了其有效性，并展示了其对新问题的强大适应性和可解释的学生学习轨迹。


<details>
  <summary>更多</summary>
  
**动机:** 现有的练习推荐方法虽然可以通过知识追踪模拟学生成绩，但常常忽略了问题的语义内容以及学生学习的连续、结构化的进程。

**方法:** ExRec提出了一个端到端的流程，包括标注问题的知识成分（KCs），学习它们的语义表示，训练知识追踪模型，并优化几种强化学习方法。此外，还通过一种定制的基于模型的价值评估方法改进了标准的Q学习连续强化学习方法。

**结果:** ExRec在四个具有不同教育目标的真实在线数学学习任务中验证了其有效性，展示了对新问题的强大泛化能力，并产生了可解释的学生学习轨迹。

**结论:** 研究结果表明，以知识追踪为指导的强化学习在教育中的个性化方面具有很大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Exercise+Recommendation+with+Semantically-Grounded+Knowledge+Tracing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11060&send_immediately=true&force_search=false)

**原文摘要:** We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [39] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang, Qizhen Wu, Lei Chen*

**主要类别:** cs.AI

**AI概要:** 提出了一种基于视觉-语言模型的指挥官，用于多无人地面车辆对抗中的自主战术决策，整合场景理解和战略推理。


<details>
  <summary>更多</summary>
  
**动机:** 传统手工规则方法在复杂瞬息万变的战场环境中变得脆弱，而当前强化学习方法主要关注行动操作而非策略决策，且缺乏可解释性。

**方法:** 该方法结合视觉语言模型进行场景理解，以及轻量级大型语言模型进行战略推理，实现统一感知和决策，并具有强适应性和可解释性。

**结果:** 仿真和消融实验验证了该方法与基线模型相比，胜率超过80%。

**结论:** 所提出的视觉-语言模型指挥官能够解决自主对抗中从感知到决策的智能推理问题，模拟人类指挥官的认知过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tactical+Decision+for+Multi-UGV+Confrontation+with+a+Vision-Language+Model-Based+Commander，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11079&send_immediately=true&force_search=false)

**原文摘要:** In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [40] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang, Bin Wang, Jiahao Wang, Xiaofeng Zhao, Min Zhang, Hao Yang, Meishan Zhang, Yu Li, Jing Li, Jun Yu, Min Zhang*

**主要类别:** cs.AI

**AI概要:** 本文提出F2STrans，一种提升大型语言模型代码翻译正确性和可读性的新方法，并引入新的评测基准。实验表明该方法显著提升了代码翻译性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在代码翻译任务上取得了进展，但确保翻译后的代码既正确又易读仍然是一个挑战，这限制了其在实际软件开发中的应用。

**方法:** F2STrans包括两个关键阶段：1）功能学习，通过从在线编程平台挖掘的高质量源目标代码对优化翻译正确性；2）风格学习，通过引入正面和负面的风格示例改进翻译的可读性。

**结果:** 实验显示，该方法不仅在新的评测基准上表现出色，而且使Qwen-1.5B在20个不同的代码翻译场景中平均表现优于增强提示的Qwen-32B和GPT-4。

**结论:** F2STrans范式能够显著改善大型语言模型在代码翻译任务上的性能，为解决代码翻译的正确性和可读性问题提供了一种有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Function-to-Style+Guidance+of+LLMs+for+Code+Translation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11083&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [41] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin, Cong He, Charles CC Lee, Wei Zhou*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为GoldMine OS的研究型架构，通过结合区块链和AI技术实现黄金的去中心化交易。该系统包含四个协作代理和一个协调核心，可以快速发行代币、维持市场流动性和应对风险。实验结果表明，该系统能够满足性能和安全性的严格要求，并具有扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 去中心化交易实物资产（如黄金）需要将实物资产托管与区块链系统连接起来，同时满足严格的合规性、流动性和风险管理要求。现有的方法无法高效地实现这一目标，因此需要一种新的解决方案。

**方法:** 作者提出了一种名为GoldMine OS的架构，该架构使用多个专业化的AI代理来自动和保护物理黄金到基于区块链的稳定币的转换。该方法结合了链上智能合约和链下AI代理进行决策，以结合区块链的透明度和可靠性以及AI驱动自动化的灵活性。

**结果:** 实验结果表明，原型可以在不到1.2秒的时间内按需发行代币，比手动工作流程快100倍以上。Market Making代理在波动条件下也能保持紧密的流动性，价差通常低于0.5％。故障注入测试显示系统的韧性，例如，在攻击被检测并在10秒内得到缓解。该架构在基准测试中可扩展到每秒5000笔交易和10000个并发用户。

**结论:** 研究结果表明，基于AI代理的替代资产去中心化交易所可以满足严格的性能和安全要求。这为传统上不流动资产的民主化访问带来了更广泛的影响。此外，他们的治理模型提供了持续的透明度、适应性和正式的系统完整性保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Agent+Architecture+for+Decentralized+Trading+of+Alternative+Assets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11117&send_immediately=true&force_search=false)

**原文摘要:** Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [42] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski, Mingzhi Lyu, Jiayou Lu, Nupur Kapur, Adams Kong*

**主要类别:** cs.CR

**AI概要:** 论文分析了文本反转中的中毒攻击，并提出了Safe-Zone Training（SZT）的防御机制，该机制通过JPEG压缩、限制高时间步长和损失掩码来增强对所有中毒攻击的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 由于中毒攻击对扩散模型（DMs）的稳健性构成重大挑战，特别是针对文本反转（TI）这一广泛应用的个性化技术，因此需要系统地分析这些攻击的发生时间和位置，以便提出有效的防御措施。

**方法:** 作者引入了语义敏感性图谱以可视化中毒对文本嵌入的影响；识别并实验验证了DMs在不同时间步上的非均匀学习行为，特别是在低噪声样本上；观察到对抗信号会干扰相关概念区域的学习，从而破坏TI过程。基于这些见解，作者提出了由JPEG压缩、高时间步长限制和损失掩码组成的Safe-Zone Training（SZT）防御机制。

**结果:** 广泛的实验表明，SZT大大增强了TI对所有中毒攻击的鲁棒性，改善了生成质量，超越了之前发表的防御方法。

**结论:** Safe-Zone Training（SZT）是一种新颖且有效的防御机制，能够显著提高扩散模型中文本反转技术的抗中毒攻击能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+and+Where+do+Data+Poisons+Attack+Textual+Inversion%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10578&send_immediately=true&force_search=false)

**原文摘要:** Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [43] [Breaking a 5-Bit Elliptic Curve Key using a 133-Qubit Quantum Computer](https://arxiv.org/abs/2507.10592)
*Steve Tippeconnic*

**主要类别:** cs.CR

**AI概要:** 本实验使用Shor风格的量子攻击破解了一个5位椭圆曲线加密密钥，并在IBM的133量子比特ibm_torino上执行，成功地从公钥关系中提取了秘密标量k。


<details>
  <summary>更多</summary>
  
**动机:** 动机在于展示量子计算在破解传统加密技术上的潜力，特别是在椭圆曲线密码学方面。

**方法:** 通过使用Qiskit Runtime 2.0，在IBM的133-qubit ibm_torino上构建和运行一个15-qubit电路，该电路包括10个逻辑量子比特和5个辅助量子比特。该方法不直接将秘密标量编码到oracle中，而是通过量子干涉在一个32阶椭圆曲线子群上操作以提取秘密标量k。

**结果:** 尽管电路深度极端（超过67,000层），量子电路仍产生了有效的干涉图案。经典后处理揭示了前100个可逆(a, b)结果中的k = 7。

**结论:** 实验成功地使用量子计算资源破解了一个5位椭圆曲线加密密钥，证明了量子计算在密码分析中的实际应用能力。所有代码、电路和原始数据都公开可用，以供复制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+a+5-Bit+Elliptic+Curve+Key+using+a+133-Qubit+Quantum+Computer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10592，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10592&send_immediately=true&force_search=false)

**原文摘要:** This experiment breaks a 5-bit elliptic curve cryptographic key using a
Shor-style quantum attack. Executed on IBM's 133-qubit ibm_torino with Qiskit
Runtime 2.0, a 15-qubit circuit, comprised of 10 logical qubits and 5 ancilla,
interferes over an order-32 elliptic curve subgroup to extract the secret
scalar k from the public key relation Q = kP, without ever encoding k directly
into the oracle. From 16,384 shots, the quantum interference reveals a diagonal
ridge in the 32 x 32 QFT outcome space. The quantum circuit, over 67,000 layers
deep, produced valid interference patterns despite extreme circuit depth, and
classical post-processing revealed k = 7 in the top 100 invertible (a, b)
results. All code, circuits, and raw data are publicly available for
replication.

</details>


### [44] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan, Zhuosheng Zhang*

**主要类别:** cs.CR

**AI概要:** 本研究提出了一种基于层次缩放机制(LaSM)的方法，以增强GUI代理对基于弹出式的环境注入攻击的防御能力。该方法通过选择性放大关键层中的注意力和MLP模块，改善模型显著性和任务相关区域之间的对齐，从而提高防御成功率，甚至在强烈的归纳攻击下也能达到98%以上的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于多模态大语言模型（MLLMs）构建的图形用户界面（GUI）代理虽然展示了强大的决策能力，但它们对于基于弹出窗口的环境注入攻击非常脆弱。这种攻击会误导模型的注意力，导致不安全或错误的操作。当前的防御措施要么需要高昂的再训练成本，要么在归纳干扰下表现不佳。

**方法:** 研究人员系统地研究了这些攻击如何改变GUI代理的注意力行为，并发现正确和错误输出之间存在层次注意力分散模式。基于这一见解，他们提出了LaSM（Layer-wise Scaling Mechanism），它选择性地放大关键层中的注意力和MLP模块，以改善模型显著性与任务相关区域之间的对齐，而无需额外的训练。

**结果:** 广泛的实验表明，LaSM在12种类型的弹出式干扰和4种不同的模型骨干上一致提高了防御成功率。当与提示级别的警报结合使用时，即使在强烈的归纳攻击下，LaSM也能实现超过98%的鲁棒性。

**结论:** 本研究表明，注意力错位是MLLM代理的核心漏洞，可以通过选择性的层次调制有效解决。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LaSM%3A+Layer-wise+Scaling+Mechanism+for+Defending+Pop-up+Attack+on+GUI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10610，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10610&send_immediately=true&force_search=false)

**原文摘要:** Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [45] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

**主要类别:** cs.CR

**AI概要:** 本文探讨了博弈论、代理AI和网络安全的交汇点，强调大型语言模型（LLM）在连接理论与实践中的作用。


<details>
  <summary>更多</summary>
  
**动机:** 传统网络安全方法依赖于手动响应和脆弱的启发式方法，需要更主动和智能的防御系统。博弈论为建模对抗行为和设计战略防御提供了严格的理论基础。

**方法:** 结合博弈论框架与大型语言模型（LLM）驱动的代理，以实现抽象策略的具体操作化，并促进复杂工作流中代理间的协调。

**结果:** 这种结合可以实现更丰富的理论基础和新的解决方案概念，同时重塑软件设计，使其从一开始就具有模块化、适应性和信任意识。

**结论:** 通过探索博弈论、代理AI和网络安全的交汇点，可以促进安全、智能和适应性的网络系统的形成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Game+Theory+Meets+LLM+and+Agentic+AI%3A+Reimagining+Cybersecurity+for+the+Age+of+Intelligent+Threats，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10621，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10621&send_immediately=true&force_search=false)

**原文摘要:** Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [46] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee, Muhammad Nadeem, Pavel Tsoi*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的IoT网络流量异常检测方法，该方法结合了可学习的梅尔频率倒谱系数（MFCC）和ResNet-18模型。通过在三个常用数据集上的实验验证，证明了这种方法在异构IoT网络环境中进行稳健和可扩展的异常检测的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 随着物联网（IoT）网络的迅速扩展，安全漏洞激增，迫切需要强大的异常检测和分类技术来应对这一挑战。

**方法:** 该方法使用可学习的梅尔频率倒谱系数（MFCC）将原始信号转换为更高维度的空间，并利用ResNet-18深度学习模型进行特征提取和多类别分类。

**结果:** 在CICIoT2023、NSL-KDD和IoTID20这三个广泛使用的IoT入侵检测数据集上的实验结果表明，整合自适应信号处理技术和深度学习架构可以实现稳健且可扩展的异常检测。

**结论:** 本研究提出的结合MFCC与ResNet-18的方法提供了一个强大的异常检测框架，强调了在异构IoT网络中使用这种集成方法的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectral+Feature+Extraction+for+Robust+Network+Intrusion+Detection+Using+MFCCs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10622&send_immediately=true&force_search=false)

**原文摘要:** The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [47] [Crypto-Assisted Graph Degree Sequence Release under Local Differential Privacy](https://arxiv.org/abs/2507.10627)
*Xiaojian Zhang, Junqing Wang, Kerui Chen, Peiyuan Zhao, Huiyuan Bai*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的框架CADR-LDP，用于发布准确近似实际度数分布的度数序列。该框架结合了加密技术和差分隐私机制，通过优化θ选择和局部投影边缘添加方法来提高效率并减少通信成本。实验结果表明，CADR-LDP在八个图数据集上优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的解决方案主要使用基于边删除过程的图投影技术，这导致了在选择阈值参数θ时存在基本的权衡问题：大θ值引入大量噪音，小θ值移除过多的边，并且θ的选择导致过高的通信成本。

**方法:** CADR-LDP框架首先使用加密辅助的Optimal-θ-Selection方法以低成本选择最优参数，然后采用LPEA-LOW方法对低度节点优先进行投影，从而保留更多边并减少投影误差。

**结果:** 理论分析证明CADR-LDP满足ε-节点本地差分隐私。实验结果表明，在八个图数据集上，该解决方案优于现有方法。

**结论:** CADR-LDP框架提供了一种有效的方法，通过优化θ选择和改进局部投影方法，减少了通信成本，提高了发布的度数序列的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Crypto-Assisted+Graph+Degree+Sequence+Release+under+Local+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10627&send_immediately=true&force_search=false)

**原文摘要:** Given a graph $G$ defined in a domain $\mathcal{G}$, we investigate locally
differentially private mechanisms to release a degree sequence on $\mathcal{G}$
that accurately approximates the actual degree distribution. Existing solutions
for this problem mostly use graph projection techniques based on edge deletion
process, using a threshold parameter $\theta$ to bound node degrees. However,
this approach presents a fundamental trade-off in threshold parameter
selection. While large $\theta$ values introduce substantial noise in the
released degree sequence, small $\theta$ values result in more edges removed
than necessary. Furthermore, $\theta$ selection leads to an excessive
communication cost. To remedy existing solutions' deficiencies, we present
CADR-LDP, an efficient framework incorporating encryption techniques and
differentially private mechanisms to release the degree sequence. In CADR-LDP,
we first use the crypto-assisted Optimal-$\theta$-Selection method to select
the optimal parameter with a low communication cost. Then, we use the LPEA-LOW
method to add some edges for each node with the edge addition process in local
projection. LPEA-LOW prioritizes the projection with low-degree nodes, which
can retain more edges for such nodes and reduce the projection error.
Theoretical analysis shows that CADR-LDP satisfies $\epsilon$-node local
differential privacy. The experimental results on eight graph datasets show
that our solution outperforms existing methods.

</details>


### [48] [Access Control for Information-Theoretically Secure Key-Document Stores](https://arxiv.org/abs/2507.10730)
*Yin Li, Sharad Mehrota, Shantanu Sharma, Komal Kumari*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的基于密钥的访问控制技术，适用于安全外包键值存储。它采用Shamir的秘密分享方法，提供无条件的安全性，支持关键词文档检索，并防止数据、用户访问权限或查询结果大小的泄露。该方法还允许服务器检测并阻止恶意客户端未经授权的数据访问，防止恶意服务器未被察觉的数据篡改，同时确保高效的访问速度。


<details>
  <summary>更多</summary>
  
**动机:** 随着云计算和大数据的发展，数据存储和访问控制变得越来越重要。传统的访问控制方法在面对外包键值存储时，无法有效保护数据隐私和安全性。因此，需要一种新的访问控制技术来解决这些问题。

**方法:** 本文提出的方法采用了Shamir的秘密分享算法，通过将密钥分成多个部分，只有当足够的部分被组合起来时，才能恢复原始密钥。这确保了即使部分信息泄露，也无法获得完整的密钥。此外，该方法还设计了一套机制来检测和阻止恶意行为。

**结果:** 实验结果显示，该方法可以在50万个文件中对5,000个关键词进行高效检索，平均耗时231.5毫秒。更重要的是，它成功地防止了数据泄露和未经授权的访问，保证了数据的安全性和完整性。

**结论:** 本文提出的基于密钥的访问控制技术为安全外包键值存储提供了一个有效的解决方案。它不仅提供了强大的安全性，而且保持了较高的访问效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Access+Control+for+Information-Theoretically+Secure+Key-Document+Stores，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10730，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10730&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a novel key-based access control technique for secure
outsourcing key-value stores where values correspond to documents that are
indexed and accessed using keys. The proposed approach adopts Shamir's
secret-sharing that offers unconditional or information-theoretic security. It
supports keyword-based document retrieval while preventing leakage of the data,
access rights of users, or the size (\textit{i}.\textit{e}., volume of the
output that satisfies a query). The proposed approach allows servers to detect
(and abort) malicious clients from gaining unauthorized access to data, and
prevents malicious servers from altering data undetected while ensuring
efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.

</details>


### [49] [3S-Attack: Spatial, Spectral and Semantic Invisible Backdoor Attack Against DNN Models](https://arxiv.org/abs/2507.10733)
*Jianyao Yin, Luca Arnaboldi, Honglong Chen, Pascal Berrang*

**主要类别:** cs.CR

**AI概要:** 提出了一种新的后门攻击方法，称为3S-attack，在空间、频谱和语义领域中均具有隐蔽性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的后门攻击研究已经探索了在空间、频谱（频率）和语义（特征）领域开发触发器，但很少有结合语义领域的方法。

**方法:** 利用良性样本的语义特征作为触发器，使用Grad-CAM和初步模型进行提取。然后将触发器嵌入频谱域，并在转换回空间域后进行像素级限制。

**结果:** 广泛的实验和理论分析证明了3S-attack的隐蔽性，并强调了需要更强的防御措施来确保AI安全。

**结论:** 3S-attack在各种数据集上展示了其隐蔽性，表明需要更强的防御以确保AI的安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是3S-Attack%3A+Spatial%2C+Spectral+and+Semantic+Invisible+Backdoor+Attack+Against+DNN+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10733&send_immediately=true&force_search=false)

**原文摘要:** Backdoor attacks involve either poisoning the training data or directly
modifying the model in order to implant a hidden behavior, that causes the
model to misclassify inputs when a specific trigger is present. During
inference, the model maintains high accuracy on benign samples but
misclassifies poisoned samples into an attacker-specified target class.
Existing research on backdoor attacks has explored developing triggers in the
spatial, spectral (frequency), and semantic (feature) domains, aiming to make
them stealthy. While some approaches have considered designing triggers that
are imperceptible in both spatial and spectral domains, few have incorporated
the semantic domain. In this paper, we propose a novel backdoor attack, termed
3S-attack, which is stealthy across the spatial, spectral, and semantic
domains. The key idea is to exploit the semantic features of benign samples as
triggers, using Gradient-weighted Class Activation Mapping (Grad-CAM) and a
preliminary model for extraction. The trigger is then embedded in the spectral
domain, followed by pixel-level restrictions after converting the samples back
to the spatial domain. This process minimizes the distance between poisoned and
benign samples, making the attack harder to detect by existing defenses and
human inspection. Extensive experiments on various datasets, along with
theoretical analysis, demonstrate the stealthiness of 3S-attack and highlight
the need for stronger defenses to ensure AI security. Our code is available at:
https://anonymous.4open.science/r/anon-project-3776/

</details>


### [50] [Contrastive-KAN: A Semi-Supervised Intrusion Detection Framework for Cybersecurity with scarce Labeled Data](https://arxiv.org/abs/2507.10808)
*Mohammad Alikhani, Reza Kazemi*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种基于Kolmogorov-Arnold网络的半监督对比学习框架的实时入侵检测系统，利用少量标记数据和大量未标记数据来有效地区分正常和攻击行为，并在多个基准数据集上验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前工业环境中标注的网络攻击数据稀缺，且大多数工业系统通常在正常条件下运行，导致机器学习模型训练困难。此外，关键基础设施中快速检测攻击以防止大规模中断至关重要。

**方法:** 提出了一个基于Kolmogorov-Arnold Network (KAN)的半监督对比学习框架的实时入侵检测系统。该方法使用少量的标注样本和大量的未标注数据进行训练，从而有效地识别出正常行为和攻击行为。

**结果:** 实验结果表明，该方法在三个基准数据集（UNSW-NB15、BoT-IoT和Gas Pipeline）上的表现优于现有的对比学习方法，并且与传统的多层感知器相比，在检测准确性和鲁棒性方面表现出色。

**结论:** 所提出的方法支持多类分类，在安全关键环境中展示了高效性和可靠性，并且KAN能够对复杂关系建模并提供解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Contrastive-KAN%3A+A+Semi-Supervised+Intrusion+Detection+Framework+for+Cybersecurity+with+scarce+Labeled+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10808，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10808&send_immediately=true&force_search=false)

**原文摘要:** In the era of the Fourth Industrial Revolution, cybersecurity and intrusion
detection systems are vital for the secure and reliable operation of IoT and
IIoT environments. A key challenge in this domain is the scarcity of labeled
cyber-attack data, as most industrial systems operate under normal conditions.
This data imbalance, combined with the high cost of annotation, hinders the
effective training of machine learning models. Moreover, rapid detection of
attacks is essential, especially in critical infrastructure, to prevent
large-scale disruptions. To address these challenges, we propose a real-time
intrusion detection system based on a semi-supervised contrastive learning
framework using the Kolmogorov-Arnold Network (KAN). Our method leverages
abundant unlabeled data to distinguish between normal and attack behaviors
effectively. We validate our approach on three benchmark datasets: UNSW-NB15,
BoT-IoT, and Gas Pipeline, using only 2.20 percent, 1.28 percent, and 8 percent
of labeled samples, respectively, to simulate real-world conditions.
Experimental results show that our method outperforms existing contrastive
learning-based approaches. We further compare KAN with a traditional multilayer
perceptron (MLP), demonstrating KAN's superior performance in both detection
accuracy and robustness under limited supervision. KAN's ability to model
complex relationships and its learnable activation functions are also explored
and visualized, offering interpretability and potential for rule extraction.
The method supports multi-class classification and proves effective in
safety-critical environments where reliability is paramount.

</details>


### [51] [Reporte de vulnerabilidades en IIoT. Proyecto DEFENDER](https://arxiv.org/abs/2507.10819)
*Pedro Almansa Jiménez, Lorenzo Fernández Maimó, Ángel Luis Peráles Gómez*

**主要类别:** cs.CR

**AI概要:** 本技术报告的主要目标是对工业物联网（IIoT）环境中的设备进行全面研究，分析其漏洞，并探讨机器学习在提高工业网络安全方面的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 识别和研究IIoT设备的主要类别及其在工业系统中的作用，以理解这些设备如何交互并满足关键工业环境的要求。同时，探索这些设备的操作背景，强调工业场景的独特特征以及设备运行条件。

**方法:** 该报告首先确定并检查了IIoT设备的主要类别，描述它们的特性、功能和角色。然后，它分析了影响IIoT设备的漏洞，概述了攻击的典型阶段，并根据第3节中提出的分类法对选定的真实事件进行了分类。

**结果:** 报告展示了对IIoT设备操作情景的理解，揭示了特定于这些设备的漏洞，并通过实际案例展示了可能的安全威胁。此外，还编制了一些最新和有效的安全对策，特别是强调了机器学习在增强工业网络安全方面的作用。

**结论:** 通过对IIoT设备及其安全挑战的研究，报告提出了针对工业系统的潜在解决方案，特别指出机器学习对于提升工业网络安全的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reporte+de+vulnerabilidades+en+IIoT.+Proyecto+DEFENDER，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10819，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10819&send_immediately=true&force_search=false)

**原文摘要:** The main objective of this technical report is to conduct a comprehensive
study on devices operating within Industrial Internet of Things (IIoT)
environments, describing the scenarios that define this category and analysing
the vulnerabilities that compromise their security. To this end, the report
seeks to identify and examine the main classes of IIoT devices, detailing their
characteristics, functionalities, and roles within industrial systems. This
analysis enables a better understanding of how these devices interact and
fulfil the requirements of critical industrial environments. The report also
explores the specific contexts in which these devices operate, highlighting the
distinctive features of industrial scenarios and the conditions under which the
devices function. Furthermore, it analyses the vulnerabilities affecting IIoT
devices, outlining their vectors, targets, impact, and consequences. The report
then describes the typical phases of an attack, along with a selection of
real-world documented incidents. These cases are classified according to the
taxonomy presented in Section 3, providing a comprehensive view of the
potential threats to security and assessing the impact these vulnerabilities
may have on industrial environments. Finally, the report presents a compilation
of some of the most recent and effective security countermeasures as potential
solutions to the security challenges faced by industrial systems. Special
emphasis is placed on the role of Machine Learning in the development of these
approaches, underscoring its importance in enhancing industrial cybersecurity.

</details>


### [52] [REAL-IoT: Characterizing GNN Intrusion Detection Robustness under Practical Adversarial Attack](https://arxiv.org/abs/2507.10836)
*Zhonghao Zhan, Huichi Zhou, Hamed Haddadi*

**主要类别:** cs.CR

**AI概要:** 本文提出REAL-IoT框架，旨在评估基于GNN的物联网环境入侵检测系统的鲁棒性，通过创建统一数据集和使用大型语言模型过滤可疑流量，发现现有GNN模型对分布偏移和真实攻击敏感，并证明了LLM过滤增强鲁棒性的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 基于GNN的网络入侵检测系统通常在单个数据集上进行评估，导致其泛化能力和对抗鲁棒性被高估。为了更准确地评估这些系统的性能并提高其在实际应用中的可靠性，需要一种新的评估框架。

**方法:** 提出REAL-IoT框架，该框架包括：1) 创建一个统一的数据集来评估模型在分布漂移下的泛化能力；2) 收集来自物理IoT测试平台的真实入侵数据集；3) 使用大型语言模型分析网络数据并过滤可疑流量以减轻对抗样本的影响。

**结果:** 使用REAL-IoT框架的评估显示，与标准基准结果相比，GNN模型在面对分布漂移和现实攻击时性能下降，表明其脆弱性。同时，展示了基于LLM的过滤方法可以提高系统的鲁棒性。

**结论:** 研究强调了为开发具有弹性的物联网入侵检测系统，必须采用现实的威胁建模和严格的测量实践。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REAL-IoT%3A+Characterizing+GNN+Intrusion+Detection+Robustness+under+Practical+Adversarial+Attack，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10836，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10836&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Network (GNN)-based network intrusion detection systems (NIDS)
are often evaluated on single datasets, limiting their ability to generalize
under distribution drift. Furthermore, their adversarial robustness is
typically assessed using synthetic perturbations that lack realism. This
measurement gap leads to an overestimation of GNN-based NIDS resilience. To
address the limitations, we propose \textbf{REAL-IoT}, a comprehensive
framework for robustness evaluation of GNN-based NIDS in IoT environments. Our
framework presents a methodology that creates a unified dataset from canonical
datasets to assess generalization under drift. In addition, it features a novel
intrusion dataset collected from a physical IoT testbed, which captures network
traffic and attack scenarios under real-world settings. Furthermore, using
REAL-IoT, we explore the usage of Large Language Models (LLMs) to analyze
network data and mitigate the impact of adversarial examples by filtering
suspicious flows. Our evaluations using REAL-IoT reveal performance drops in
GNN models compared to results from standard benchmarks, quantifying their
susceptibility to drift and realistic attacks. We also demonstrate the
potential of LLM-based filtering to enhance robustness. These findings
emphasize the necessity of realistic threat modeling and rigorous measurement
practices for developing resilient IoT intrusion detection systems.

</details>


### [53] [BandFuzz: An ML-powered Collaborative Fuzzing Framework](https://arxiv.org/abs/2507.10845)
*Wenxuan Shi, Hongwei Li, Jiahao Yu, Xinqian Sun, Wenbo Guo, Xinyu Xing*

**主要类别:** cs.CR

**AI概要:** 协同模糊测试是结合多个独立模糊测试工具并动态选择适合不同程序组合的一种技术。它在各种程序中提供持续和稳定的性能，但现有框架的有效性受到计算资源需求和资源分配效率低下的限制。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决单个fuzzer在针对不同程序时效果不稳定的问题，研究者们希望找到一种更通用的模糊测试解决方案，从而不需要手动挑选最适合的fuzzer。

**方法:** 协同模糊测试放松了对目标程序的具体假设，通过结合多个fuzzer，并根据不同的程序动态地选择适当的组合来实现。

**结果:** 尽管协同模糊测试理论上是一个更有前途的方向，但是现有的协同模糊测试框架的效果受到了需要额外计算资源和资源分配效率低下等挑战的限制。

**结论:** 协同模糊测试作为一种新兴技术，虽然存在一些局限性，但在追求通用模糊测试解决方案方面展现出了潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BandFuzz%3A+An+ML-powered+Collaborative+Fuzzing+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10845，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10845&send_immediately=true&force_search=false)

**原文摘要:** Collaborative fuzzing has recently emerged as a technique that combines
multiple individual fuzzers and dynamically chooses the appropriate
combinations suited for different programs. Unlike individual fuzzers, which
rely on specific assumptions to maintain their effectiveness, collaborative
fuzzing relaxes the assumptions on target programs, providing constant and
robust performance across various programs. Ideally, collaborative fuzzing
should be a more promising direction toward generic fuzzing solutions, as it
mitigates the need for manual cherry-picking of individual fuzzers. However,
the effectiveness of existing collaborative fuzzing frameworks is limited by
major challenges, such as the need for additional computational resources
compared to individual fuzzers and the inefficient allocation of resources
among the various fuzzers.

</details>


### [54] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton, Hemanth Gowda, Girish Rao, Sachin Pargi, Alireza Hadj Khodabakhshi, Joseph Rombs, Stephan Jou, Manish Marwah*

**主要类别:** cs.CR

**AI概要:** 本文介绍了PhreshPhish，一个大规模、高质量的钓鱼网站数据集，并提出了一套基准数据集，以解决现有数据集存在的问题，推动钓鱼攻击检测领域的进步。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习在实时检测钓鱼攻击方面虽然有效，但进展受到缺乏大型、高质量的数据集和基准的阻碍。现有的数据集存在质量问题、泄漏和不现实的基础率等问题，导致性能结果过于乐观。

**方法:** 引入了PhreshPhish，一个更大规模且质量更高的钓鱼网站数据集，通过最小化泄漏、增加任务难度、增强数据集多样性以及调整更贴近真实世界的基础率，设计了一套专门用于实际模型评估的综合基准数据集。

**结果:** PhreshPhish数据集比现有的公共数据集大得多，提供了显著更高的质量，估计无效或错误标记的数据点率更低。训练并评估了多种解决方案方法，提供了基准集上的基线性能。

**结论:** 该数据集和基准的可用性将实现现实的、标准化的模型比较，并促进钓鱼检测领域的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PhreshPhish%3A+A+Real-World%2C+High-Quality%2C+Large-Scale+Phishing+Website+Dataset+and+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10854&send_immediately=true&force_search=false)

**原文摘要:** Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [55] [From Alerts to Intelligence: A Novel LLM-Aided Framework for Host-based Intrusion Detection](https://arxiv.org/abs/2507.10873)
*Danyu Sun, Jinghuai Zhang, Jiacen Xu, Yu Zheng, Yuan Tian, Zhou Li*

**主要类别:** cs.CR

**AI概要:** 本文介绍了一种新的基于主机的入侵检测系统SHIELD，它利用大型语言模型（LLMs）和多种技术如事件级别的掩码自动编码器、确定性数据增强和多用途提示，来解决传统HIDS存在的问题，并在多个数据集上展示了出色的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前的基于主机的入侵检测系统（HIDS）虽然取得了一些成功，但仍然存在高误报率、不同环境下的结果不一致以及检测结果对人类不友好等问题。而大型语言模型（LLMs）具有丰富的攻击技术知识和通过语义分析检测异常的能力，可能为改进HIDS提供新的途径。

**方法:** 研究人员开发了名为SHIELD的系统，该系统结合了事件级别掩码自动编码器（MAE）、确定性数据增强（DDA）和多用途提示等技术，以克服LLM令牌限制、背景噪音干扰等挑战，并引导LLM进行精确且可解释的攻击调查。

**结果:** 在三个日志数据集（DARPA-E3、NodLink-simulated-data和ATLASv2）上的广泛实验表明，与5个代表性的HIDS相比，SHIELD始终表现出色。

**结论:** 研究结果强调了LLM作为入侵检测强大工具的潜力，并为此领域的未来研究铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Alerts+to+Intelligence%3A+A+Novel+LLM-Aided+Framework+for+Host-based+Intrusion+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10873，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10873&send_immediately=true&force_search=false)

**原文摘要:** Host-based intrusion detection system (HIDS) is a key defense component to
protect the organizations from advanced threats like Advanced Persistent
Threats (APT). By analyzing the fine-grained logs with approaches like data
provenance, HIDS has shown successes in capturing sophisticated attack traces.
Despite the progresses embarked by the research community and industry, HIDS
still frequently encounters backlash from their operators in the deployed
environments, due to issues like high false-positive rate, inconsistent
outcomes across environments and human-unfriendly detection results. Large
Language Models (LLMs) have great potentials to advance the state of HIDS,
given their extensive knowledge of attack techniques and their ability to
detect anomalies through semantic analysis, anchored by recent studies. Yet,
our preliminary analysis indicates that building an HIDS by naively prompting
an LLM is unlikely to succeed. In this work, we explore the direction of
building a customized LLM pipeline for HIDS and develop a system named SHIELD.
SHIELD addresses challenges related to LLM's token limits, confusion of
background noises, etc., by integrating a variety of techniques like
event-level Masked Autoencoder (MAE) for attack window detection, attack
evidence identification and expansion, Deterministic Data Augmentation (DDA)
for profiling normal activities, and multi-purpose prompting that guides the
LLM to conduct precise and interpretable attack investigations. Extensive
experiments on three log datasets (DARPA-E3, NodLink-simulated-data and
ATLASv2) show that SHIELD consistently achieves outstanding performance in
comparison with 5 representative HIDS. These findings highlight the potential
of LLMs as powerful tools for intrusion detection and pave the way for future
research in this domain.

</details>


### [56] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar, Kamalasankari Subramaniakuppusamy, Noha El Kachach*

**主要类别:** cs.CR

**AI概要:** 该论文介绍了一种名为MalCodeAI的多阶段AI流水线，用于自动代码安全分析和修复。它在14种编程语言中提供了可扩展且准确的结果，并在两个阶段中分别实现了0.397和0.199的验证损失。系统在实际开发工作流程中获得了高度评价，特别是在有用性和解释性方面。


<details>
  <summary>更多</summary>
  
**动机:** 网络威胁日益复杂以及传统漏洞检测工具的局限性，促使了寻找新的方法来保障软件系统的安全。

**方法:** MalCodeAI使用了Qwen2.5-Coder-3B-Instruct模型进行代码分解和语义推理，并通过低秩适应（LoRA）在MLX框架内进行了优化。它分为两个阶段：第一阶段是功能分解和代码段总结；第二阶段是漏洞检测和修复。

**结果:** 在定量评估中，MalCodeAI在功能分解和总结代码段时达到了0.397的验证损失，在漏洞检测和修复时达到了0.199的验证损失。在定性评估中，15名开发者对系统的实用性、可解释性和输出的易读性给予了高分。

**结论:** 这项工作标志着向智能、可解释且以开发人员为中心的软件安全解决方案迈进了一大步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MalCodeAI%3A+Autonomous+Vulnerability+Detection+and+Remediation+via+Language+Agnostic+Code+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10898&send_immediately=true&force_search=false)

**原文摘要:** The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [57] [DVFS: A Dynamic Verifiable Fuzzy Search Service for Encrypted Cloud Data](https://arxiv.org/abs/2507.10927)
*Jie Zhang, Xiaohong Li, Man Zheng, Zhe Hou, Guangdong Bai, Ruitao Feng*

**主要类别:** cs.CR

**AI概要:** 本文提出DVFS服务，通过三项核心创新解决了现有模糊多关键字搜索的安全性和效率之间的基本权衡问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的加密数据检索解决方案在安全性和效率之间存在基本的权衡：线性搜索机制虽然提供适应性安全，但在大规模数据下会产生过高的开销；基于树的索引虽提高了性能，但却带来了分支泄漏漏洞。

**方法:** DVFS包含三个核心创新点：1）自适应安全模糊搜索方法，将局部敏感哈希与虚拟二叉树结合，消除分支泄漏并将搜索复杂度从线性降低到对数时间；2）双存储库版本控制机制，支持动态更新并具有前向隐私，防止操作期间的信息泄露；3）基于区块链的验证系统，确保正确性和完整性，实现对数时间验证复杂度。

**结果:** 该方案通过同时解决安全-性能悖论并启用可信赖的动态操作，推进了安全加密检索的发展。

**结论:** DVFS作为一个动态可验证模糊搜索服务，不仅提升了模糊多关键字搜索的安全性和效率，还保证了操作的可信度和动态更新的安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DVFS%3A+A+Dynamic+Verifiable+Fuzzy+Search+Service+for+Encrypted+Cloud+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.10927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.10927&send_immediately=true&force_search=false)

**原文摘要:** Cloud storage introduces critical privacy challenges for encrypted data
retrieval, where fuzzy multi-keyword search enables approximate matching while
preserving data confidentiality. Existing solutions face fundamental trade-offs
between security and efficiency: linear-search mechanisms provide adaptive
security but incur prohibitive overhead for large-scale data, while tree-based
indexes improve performance at the cost of branch leakage vulnerabilities.
  To address these limitations, we propose DVFS - a dynamic verifiable fuzzy
search service with three core innovations: (1) An \textit{adaptive-secure
fuzzy search} method integrating locality-sensitive hashing with virtual binary
trees, eliminating branch leakage while reducing search complexity from linear
to sublinear ($O(\log n)$ time); (2) A \textit{dual-repository version control}
mechanism supporting dynamic updates with forward privacy, preventing
information leakage during operations; (3) A \textit{blockchain-based
verification system} that ensures correctness and completeness via smart
contracts, achieving $O(\log n)$ verification complexity.
  Our solution advances secure encrypted retrieval by simultaneously resolving
the security-performance paradox and enabling trustworthy dynamic operations.

</details>


### [58] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao, Jin Song, Jian Jin*

**主要类别:** cs.CR

**AI概要:** 提出了一种基于散列水印滤波器的神经网络所有权保护方法NeuralMark，增强了对伪造和覆盖攻击的防御。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络作为有价值的数字资产，需要强有力的所有权保护。现有的基于权重的神经网络水印方法虽然简单实用，但容易受到伪造和覆盖攻击。

**方法:** 使用散列函数从密钥生成不可逆的二进制水印，并用作筛选模型参数以嵌入水印的过滤器。设计结合了嵌入参数与散列水印，提供了对伪造和覆盖攻击的强大防御，并加入平均池化以抵抗微调和剪枝攻击。

**结果:** 在13种不同的卷积和Transformer架构中验证了其有效性和鲁棒性，涵盖了五个图像分类任务和一个文本生成任务。

**结论:** NeuralMark可以无缝集成到各种神经网络架构中，确保广泛的适用性，并且源代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hashed+Watermark+as+a+Filter%3A+Defeating+Forging+and+Overwriting+Attacks+in+Weight-based+Neural+Network+Watermarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11137&send_immediately=true&force_search=false)

**原文摘要:** As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


### [59] [FacialMotionID: Identifying Users of Mixed Reality Headsets using Abstract Facial Motion Representations](https://arxiv.org/abs/2507.11138)
*Adriano Castro, Simon Hanisch, Matin Fallahi, Thorsten Strufe*

**主要类别:** cs.CR

**AI概要:** 研究发现面部动作数据在混合现实中可用于用户识别和情感状态推断，存在隐私风险。


<details>
  <summary>更多</summary>
  
**动机:** 随着混合现实系统的普及，理解面部运动数据是否能导致用户识别或敏感属性推断变得越来越重要。

**方法:** 研究者进行了一个包含116名参与者的研究，使用三种类型的头显设备，在三个会话期间收集面部、眼部和头部运动数据，并进行语言和非语言任务。

**结果:** 分析表明，个体可以从这些数据中被重新识别，准确率高达98%，甚至可以在不同设备类型之间进行识别，情感状态的推断准确率也高达86%。

**结论:** 面部动作追踪在混合现实环境中存在潜在的隐私风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FacialMotionID%3A+Identifying+Users+of+Mixed+Reality+Headsets+using+Abstract+Facial+Motion+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.11138，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.11138&send_immediately=true&force_search=false)

**原文摘要:** Facial motion capture in mixed reality headsets enables real-time avatar
animation, allowing users to convey non-verbal cues during virtual
interactions. However, as facial motion data constitutes a behavioral
biometric, its use raises novel privacy concerns. With mixed reality systems
becoming more immersive and widespread, understanding whether face motion data
can lead to user identification or inference of sensitive attributes is
increasingly important.
  To address this, we conducted a study with 116 participants using three types
of headsets across three sessions, collecting facial, eye, and head motion data
during verbal and non-verbal tasks. The data used is not raw video, but rather,
abstract representations that are used to animate digital avatars. Our analysis
shows that individuals can be re-identified from this data with up to 98%
balanced accuracy, are even identifiable across device types, and that
emotional states can be inferred with up to 86% accuracy. These results
underscore the potential privacy risks inherent in face motion tracking in
mixed reality environments.

</details>
