{"id": "2506.09999", "pdf": "https://arxiv.org/pdf/2506.09999", "abs": "https://arxiv.org/abs/2506.09999", "authors": ["Yukun Chen", "Zihuan Qiu", "Fanman Meng", "Hongliang Li", "Linfeng Xu", "Qingbo Wu"], "title": "Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that\nfocus only on vision and text, this paper explores MCIL across vision, audio\nand text modalities, addressing challenges in integrating complementary\ninformation and mitigating catastrophic forgetting. To tackle these issues, we\npropose an MCIL method based on multimodal pre-trained models. Firstly, a\nMultimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts\n(MoE) structure is introduced to achieve effective incremental fine-tuning for\nAudioCLIP. Secondly, to enhance feature discriminability and generalization, we\npropose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking\nthreshold mechanism and a dynamic feature fusion mechanism, along with a\nstrategy to enhance text diversity. Thirdly, a novel multimodal\nclass-incremental contrastive training loss is proposed to optimize cross-modal\nalignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced\nfor comprehensive assessment. Extensive experiments on three multimodal\ndatasets validate the effectiveness of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u6a21\u6001\u7c7b\u589e\u91cf\u5b66\u4e60\uff08MCIL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u6a21\u6001\u589e\u91cf\u7279\u5f81\u63d0\u53d6\u5668\u3001\u81ea\u9002\u5e94\u89c6\u542c\u878d\u5408\u6a21\u5757\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u7c7b\u589e\u91cf\u5bf9\u6bd4\u8bad\u7ec3\u635f\u5931\u6765\u89e3\u51b3\u8de8\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u89c6\u89c9\u548c\u6587\u672c\u4e0a\uff0c\u800c\u5ffd\u7565\u4e86\u97f3\u9891\u4fe1\u606f\u7684\u6574\u5408\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u8fdb\u884c\u6709\u6548\u7684\u4fe1\u606f\u4e92\u8865\uff0c\u5e76\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u7ed3\u6784\uff08MoE\uff09\u7684\u591a\u6a21\u6001\u589e\u91cf\u7279\u5f81\u63d0\u53d6\u5668\uff08MIFE\uff09\uff0c\u4ee5\u5b9e\u73b0AudioCLIP\u7684\u6709\u6548\u589e\u91cf\u5fae\u8c03\u3002\n2. \u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u89c6\u542c\u878d\u5408\u6a21\u5757\uff08AAVFM\uff09\uff0c\u5305\u542b\u63a9\u7801\u9608\u503c\u673a\u5236\u548c\u52a8\u6001\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u540c\u65f6\u589e\u5f3a\u6587\u672c\u591a\u6837\u6027\u3002\n3. \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u7c7b\u589e\u91cf\u5bf9\u6bd4\u8bad\u7ec3\u635f\u5931\uff0c\u4ee5\u4f18\u5316MCIL\u4e2d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\n4. \u5b9a\u4e49\u4e86\u4e24\u4e2a\u9488\u5bf9MCIL\u7684\u5177\u4f53\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u4fbf\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5bf9\u4e09\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684MCIL\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u7684\u4fe1\u606f\u7ed3\u5408\u8d77\u6765\uff0c\u5e76\u4e14\u5728\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u4e00\u5957\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u7528\u4e8e\u8861\u91cfMCIL\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2506.10014", "pdf": "https://arxiv.org/pdf/2506.10014", "abs": "https://arxiv.org/abs/2506.10014", "authors": ["Wei Li", "Mengcheng Lan", "Jiaxing Xu", "Yiping Ke"], "title": "NOCL: Node-Oriented Conceptualization LLM for Graph Tasks without Message Passing", "categories": ["cs.LG"], "comment": "10 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1703.00552, arXiv:1403.2844 by other authors", "summary": "Graphs are essential for modeling complex interactions across domains such as\nsocial networks, biology, and recommendation systems. Traditional Graph Neural\nNetworks, particularly Message Passing Neural Networks (MPNNs), rely heavily on\nsupervised learning, limiting their generalization and applicability in\nlabel-scarce scenarios. Recent self-supervised approaches still require labeled\nfine-tuning, limiting their effectiveness in zero-shot scenarios. Meanwhile,\nLarge Language Models (LLMs) excel in natural language tasks but face\nsignificant challenges when applied to graphs, including preserving reasoning\nabilities, managing extensive token lengths from rich node attributes, and\nbeing limited to textual-attributed graphs (TAGs) and a single level task. To\novercome these limitations, we propose the Node-Oriented Conceptualization LLM\n(NOCL), a novel framework that leverages two core techniques: 1) node\ndescription, which converts heterogeneous node attributes into structured\nnatural language, extending LLM from TAGs to non-TAGs; 2) node concept, which\nencodes node descriptions into compact semantic embeddings using pretrained\nlanguage models, significantly reducing token lengths by up to 93.9% compared\nto directly using node descriptions. Additionally, our NOCL employs graph\nrepresentation descriptors to unify graph tasks at various levels into a\nshared, language-based query format, paving a new direction for Graph\nFoundation Models. Experimental results validate NOCL's competitive supervised\nperformance relative to traditional MPNNs and hybrid LLM-MPNN methods and\ndemonstrate superior generalization in zero-shot settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Node-Oriented Conceptualization LLM (NOCL)\uff0c\u5b83\u901a\u8fc7\u8282\u70b9\u63cf\u8ff0\u548c\u8282\u70b9\u6982\u5ff5\u4e24\u79cd\u6838\u5fc3\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u56fe\u6570\u636e\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08\u5982\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\uff09\u4f9d\u8d56\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u6807\u7b7e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u7528\u6027\uff1b\u800c\u6700\u8fd1\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u4ecd\u7136\u9700\u8981\u6709\u6807\u7b7e\u7684\u5fae\u8c03\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u96f6\u6837\u672c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u540c\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5e94\u7528\u4e8e\u56fe\u6570\u636e\u65f6\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u5305\u62ec\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u3001\u7ba1\u7406\u6765\u81ea\u4e30\u5bcc\u8282\u70b9\u5c5e\u6027\u7684\u5927\u91cf\u6807\u8bb0\u957f\u5ea6\u4ee5\u53ca\u5c40\u9650\u4e8e\u6587\u672c\u5c5e\u6027\u56fe(TAGs)\u548c\u5355\u4e00\u5c42\u6b21\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNode-Oriented Conceptualization LLM (NOCL)\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e24\u9879\u6838\u5fc3\u6280\u672f\uff1a1) \u8282\u70b9\u63cf\u8ff0\uff0c\u5c06\u5f02\u6784\u8282\u70b9\u5c5e\u6027\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u81ea\u7136\u8bed\u8a00\uff0c\u4f7fLLM\u80fd\u591f\u4eceTAGs\u6269\u5c55\u5230\u975eTAGs\uff1b2) \u8282\u70b9\u6982\u5ff5\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5c06\u8282\u70b9\u63cf\u8ff0\u7f16\u7801\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u5d4c\u5165\uff0c\u4e0e\u76f4\u63a5\u4f7f\u7528\u8282\u70b9\u63cf\u8ff0\u76f8\u6bd4\u663e\u8457\u51cf\u5c11\u4e86\u6807\u8bb0\u957f\u5ea6\u9ad8\u8fbe93.9%\u3002\u6b64\u5916\uff0cNOCL\u91c7\u7528\u56fe\u5f62\u8868\u793a\u63cf\u8ff0\u7b26\u6765\u7edf\u4e00\u4e0d\u540c\u7ea7\u522b\u7684\u56fe\u5f62\u4efb\u52a1\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5171\u4eab\u7684\u8bed\u8a00\u67e5\u8be2\u683c\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u76f8\u5bf9\u4e8e\u4f20\u7edfMPNNs\u548c\u6df7\u5408LLM-MPNN\u65b9\u6cd5\uff0cNOCL\u5177\u6709\u7ade\u4e89\u6027\u7684\u76d1\u7763\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "NOCL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u56fe\u6570\u636e\u65b9\u9762\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2506.10044", "pdf": "https://arxiv.org/pdf/2506.10044", "abs": "https://arxiv.org/abs/2506.10044", "authors": ["Uijun Jung", "Deokho Jang", "Sungchul Kim", "Jungho Kim"], "title": "Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks", "categories": ["cs.LG", "cs.CE", "cs.NE", "physics.optics"], "comment": "22 pages, 8 figures, 2 tables, 11 supplementary figures, 7\n  supplementary tables", "summary": "Optical properties of thin film are greatly influenced by the thickness of\neach layer. Accurately predicting these thicknesses and their corresponding\noptical properties is important in the optical inverse design of thin films.\nHowever, traditional inverse design methods usually demand extensive numerical\nsimulations and optimization procedures, which are time-consuming. In this\npaper, we utilize deep learning for the inverse design of the transmission\nspectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural\nnetwork (TNN), which can solve the one-to-many mapping problem that greatly\ndegrades the performance of deep-learning-based inverse designs. In general,\nthe TNN has been implemented by a back-to-back connection of an inverse neural\nnetwork and a pre-trained forward neural network, both of which have been\nimplemented based on multilayer perceptron (MLP) algorithms. In this paper, we\npropose to use not only MLP, but also convolutional neural network (CNN) or\nlong short-term memory (LSTM) algorithms in the configuration of the TNN. We\nshow that an LSTM-LSTM-based TNN yields the highest accuracy but takes the\nlongest training time among nine configurations of TNNs. We also find that a\nCNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed\nbecause it could integrate the strengths of the CNN and LSTM algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e32\u8054\u795e\u7ecf\u7f51\u7edc(TNN)\u6765\u8bbe\u8ba1SiO2/TiO2\u591a\u5c42\u8584\u819c\u7684\u900f\u5c04\u5149\u8c31\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u914d\u7f6e\u7684TNN\u53d1\u73b0\uff0c\u57fa\u4e8eCNN-LSTM\u7684TNN\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u8fbe\u5230\u4e86\u6700\u4f18\u5e73\u8861\u3002", "motivation": "\u4f20\u7edf\u7684\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u6570\u503c\u6a21\u62df\u548c\u4f18\u5316\u8fc7\u7a0b\uff0c\u8017\u65f6\u8f83\u957f\u3002\u4e3a\u4e86\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u5e76\u89e3\u51b3\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9006\u5411\u8bbe\u8ba1\u4e2d\u7684\u4e00\u5bf9\u591a\u6620\u5c04\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u4e32\u8054\u795e\u7ecf\u7f51\u7edc\uff08TNN\uff09\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7531\u9006\u5411\u795e\u7ecf\u7f51\u7edc\u548c\u9884\u8bad\u7ec3\u7684\u524d\u5411\u795e\u7ecf\u7f51\u7edc\u80cc\u9760\u80cc\u8fde\u63a5\u6784\u6210\u7684\u4e32\u8054\u795e\u7ecf\u7f51\u7edc\uff08TNN\uff09\uff0c\u5e76\u4e14\u5728TNN\u7684\u914d\u7f6e\u4e2d\u4e0d\u4ec5\u91c7\u7528\u4e86\u591a\u5c42\u611f\u77e5\u5668(MLP)\uff0c\u8fd8\u5c1d\u8bd5\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u6216\u957f\u77ed\u671f\u8bb0\u5fc6(LSTM)\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLSTM-LSTM\u7684TNN\u5177\u6709\u6700\u9ad8\u7684\u51c6\u786e\u6027\u4f46\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u6700\u957f\uff1b\u800c\u57fa\u4e8eCNN-LSTM\u7684TNN\u5219\u5728\u51c6\u786e\u5ea6\u548c\u901f\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7279\u522b\u662f\u7ed3\u5408CNN\u4e0eLSTM\u7b97\u6cd5\u7684TNN\u7ed3\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347SiO2/TiO2\u591a\u5c42\u8584\u819c\u900f\u5c04\u5149\u8c31\u9006\u5411\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.10101", "pdf": "https://arxiv.org/pdf/2506.10101", "abs": "https://arxiv.org/abs/2506.10101", "authors": ["Seyed Amir Hossein Saberi", "Amir Najafi", "Abolfazl Motahari", "Babak H. khalaj"], "title": "Fundamental Limits of Learning High-dimensional Simplices in Noisy Regimes", "categories": ["stat.ML", "cs.LG"], "comment": "Extension of our ICML 2023 paper, 44 pages", "summary": "In this paper, we establish sample complexity bounds for learning\nhigh-dimensional simplices in $\\mathbb{R}^K$ from noisy data. Specifically, we\nconsider $n$ i.i.d. samples uniformly drawn from an unknown simplex in\n$\\mathbb{R}^K$, each corrupted by additive Gaussian noise of unknown variance.\nWe prove an algorithm exists that, with high probability, outputs a simplex\nwithin $\\ell_2$ or total variation (TV) distance at most $\\varepsilon$ from the\ntrue simplex, provided $n \\ge (K^2/\\varepsilon^2)\ne^{\\mathcal{O}(K/\\mathrm{SNR}^2)}$, where $\\mathrm{SNR}$ is the signal-to-noise\nratio. Extending our prior work~\\citep{saberi2023sample}, we derive new\ninformation-theoretic lower bounds, showing that simplex estimation within TV\ndistance $\\varepsilon$ requires at least $n \\ge \\Omega(K^3\n\\sigma^2/\\varepsilon^2 + K/\\varepsilon)$ samples, where $\\sigma^2$ denotes the\nnoise variance. In the noiseless scenario, our lower bound $n \\ge\n\\Omega(K/\\varepsilon)$ matches known upper bounds up to constant factors. We\nresolve an open question by demonstrating that when $\\mathrm{SNR} \\ge\n\\Omega(K^{1/2})$, noisy-case complexity aligns with the noiseless case. Our\nanalysis leverages sample compression techniques (Ashtiani et al., 2018) and\nintroduces a novel Fourier-based method for recovering distributions from noisy\nobservations, potentially applicable beyond simplex learning.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u9ad8\u7ef4\u5355\u7eaf\u5f62\u4e2d\u5b66\u4e60\u6240\u9700\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0a\u754c\u548c\u4e0b\u754c\u7684\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u5728\u6709\u566a\u58f0\u6570\u636e\u6761\u4ef6\u4e0b\uff0c\u5b66\u4e60\u9ad8\u7ef4\u5355\u7eaf\u5f62\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u7ed9\u5b9a\u4fe1\u566a\u6bd4\u65f6\u8fbe\u5230\u4e00\u5b9a\u7684\u51c6\u786e\u5ea6\u3002", "method": "\u4f7f\u7528\u4e86\u6837\u672c\u538b\u7f29\u6280\u672f\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u65b9\u6cd5\u6765\u4ece\u542b\u566a\u58f0\u7684\u89c2\u6d4b\u4e2d\u6062\u590d\u5206\u5e03\u3002", "result": "\u8bc1\u660e\u4e86\u5f53\u4fe1\u566a\u6bd4\u8db3\u591f\u5927\u65f6\uff0c\u566a\u58f0\u60c5\u51b5\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u65e0\u566a\u58f0\u60c5\u51b5\u76f8\u5339\u914d\u3002\u6b64\u5916\uff0c\u8fd8\u7ed9\u51fa\u4e86\u5728\u603b\u53d8\u5dee\u8ddd\u79bb\u8bef\u5dee\u4e3a\u03b5\u65f6\u4f30\u8ba1\u5355\u7eaf\u5f62\u6240\u9700\u6837\u672c\u6570\u91cf\u7684\u4fe1\u606f\u8bba\u4e0b\u754c\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5bf9\u4e8e\u9ad8\u4fe1\u566a\u6bd4\u7684\u60c5\u51b5\uff0c\u566a\u58f0\u5bf9\u5355\u7eaf\u5f62\u5b66\u4e60\u6837\u672c\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff1b\u5e76\u4e14\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u53ef\u80fd\u9002\u7528\u4e8e\u5355\u7eaf\u5f62\u5b66\u4e60\u4e4b\u5916\u7684\u95ee\u9898\u3002"}}
{"id": "2506.10054", "pdf": "https://arxiv.org/pdf/2506.10054", "abs": "https://arxiv.org/abs/2506.10054", "authors": ["Shangpin Peng", "Weinong Wang", "Zhuotao Tian", "Senqiao Yang", "Xing Wu", "Haotian Xu", "Chengquan Zhang", "Takashi Isobe", "Baotian Hu", "Min Zhang"], "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6Omni-DPO\uff0c\u5b83\u901a\u8fc7\u540c\u65f6\u8003\u8651\u6bcf\u4e2a\u504f\u597d\u5bf9\u7684\u8d28\u91cf\u548c\u6a21\u578b\u5728\u8fd9\u4e9b\u504f\u597d\u5bf9\u4e0a\u7684\u8868\u73b0\u6765\u6539\u8fdb\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5229\u7528\u8bad\u7ec3\u6570\u636e\u5e76\u63d0\u9ad8\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOmni-DPO\u5728\u6587\u672c\u7406\u89e3\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eDPO\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u6240\u6709\u504f\u597d\u5bf9\u89c6\u4e3a\u540c\u7b49\u91cd\u8981\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u5185\u5728\u8d28\u91cf\u548c\u5b66\u4e60\u6548\u7528\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u6570\u636e\u5229\u7528\u7387\u4e0d\u9ad8\u548c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86Omni-DPO\uff0c\u4e00\u4e2a\u53cc\u91cd\u89c6\u89d2\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5b83\u8054\u5408\u8003\u8651\u4e86\u6bcf\u4e2a\u504f\u597d\u5bf9\u7684\u56fa\u6709\u8d28\u91cf\u4ee5\u53ca\u6a21\u578b\u5728\u8fd9\u4e9b\u504f\u597d\u5bf9\u4e0a\u7684\u6f14\u53d8\u8868\u73b0\uff0c\u5e76\u4e14\u6839\u636e\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u7684\u5b66\u4e60\u52a8\u6001\u81ea\u9002\u5e94\u5730\u52a0\u6743\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5404\u79cd\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2dOmni-DPO\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7279\u522b\u662f\u5728\u6587\u672c\u7406\u89e3\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528Omni-DPO\u5fae\u8c03\u7684Gemma-2-9b-it\u6bd4\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578bClaude 3 Opus\u9ad8\u51fa6.7\u5206\uff1b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cOmni-DPO\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Omni-DPO\u901a\u8fc7\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u5229\u7528\u8fbe\u5230\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u76f4\u63a5\u504f\u597d\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u6709\u529b\u7684\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2506.10168", "pdf": "https://arxiv.org/pdf/2506.10168", "abs": "https://arxiv.org/abs/2506.10168", "authors": ["Panagiotis Theodoropoulos", "Augustinos D. Saravanos", "Evangelos A. Theodorou", "Guan-Horng Liu"], "title": "Momentum Multi-Marginal Schr\u00f6dinger Bridge Matching", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Understanding complex systems by inferring trajectories from sparse sample\nsnapshots is a fundamental challenge in a wide range of domains, e.g.,\nsingle-cell biology, meteorology, and economics. Despite advancements in Bridge\nand Flow matching frameworks, current methodologies rely on pairwise\ninterpolation between adjacent snapshots. This hinders their ability to capture\nlong-range temporal dependencies and potentially affects the coherence of the\ninferred trajectories. To address these issues, we introduce \\textbf{Momentum\nMulti-Marginal Schr\\\"odinger Bridge Matching (3MSBM)}, a novel matching\nframework that learns smooth measure-valued splines for stochastic systems that\nsatisfy multiple positional constraints. This is achieved by lifting the\ndynamics to phase space and generalizing stochastic bridges to be conditioned\non several points, forming a multi-marginal conditional stochastic optimal\ncontrol problem. The underlying dynamics are then learned by minimizing a\nvariational objective, having fixed the path induced by the multi-marginal\nconditional bridge. As a matching approach, 3MSBM learns transport maps that\npreserve intermediate marginals throughout training, significantly improving\nconvergence and scalability. Extensive experimentation in a series of\nreal-world applications validates the superior performance of 3MSBM compared to\nexisting methods in capturing complex dynamics with temporal dependencies,\nopening new avenues for training matching frameworks in multi-marginal\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5339\u914d\u6846\u67b63MSBM\uff0c\u5b83\u901a\u8fc7\u5c06\u52a8\u529b\u5b66\u63d0\u5347\u5230\u76f8\u7a7a\u95f4\u5e76\u63a8\u5e7f\u6761\u4ef6\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u6765\u5b66\u4e60\u6ee1\u8db3\u591a\u4e2a\u4f4d\u7f6e\u7ea6\u675f\u7684\u968f\u673a\u7cfb\u7edf\u7684\u5e73\u6ed1\u6d4b\u5ea6\u503c\u6837\u6761\u3002\u8fd9\u4f7f\u5f973MSBM\u80fd\u591f\u6355\u6349\u957f\u8ddd\u79bb\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u4e14\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684Bridge\u548cFlow\u5339\u914d\u6846\u67b6\u4f9d\u8d56\u4e8e\u76f8\u90bb\u5feb\u7167\u4e4b\u95f4\u7684\u6210\u5bf9\u63d2\u503c\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u6355\u83b7\u957f\u8ddd\u79bb\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u80fd\u529b\uff0c\u5e76\u53ef\u80fd\u5f71\u54cd\u63a8\u65ad\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\u3002", "method": "Momentum Multi-Marginal Schr\u00f6dinger Bridge Matching (3MSBM)\uff0c\u4e00\u79cd\u65b0\u7684\u5339\u914d\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5c06\u52a8\u529b\u5b66\u63d0\u5347\u5230\u76f8\u7a7a\u95f4\u5e76\u5f62\u6210\u591a\u8fb9\u9645\u6761\u4ef6\u4e0b\u7684\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u6765\u5b66\u4e60\u6ee1\u8db3\u591a\u4e2a\u4f4d\u7f6e\u7ea6\u675f\u7684\u968f\u673a\u7cfb\u7edf\u5e73\u6ed1\u6d4b\u5ea6\u503c\u6837\u6761\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c3MSBM\u5728\u6355\u6349\u5177\u6709\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u590d\u6742\u52a8\u6001\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u591a\u8fb9\u9645\u8bbe\u7f6e\u4e2d\u7684\u5339\u914d\u6846\u67b6\u8bad\u7ec3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "conclusion": "3MSBM\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u5339\u914d\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4e2d\u95f4\u8fb9\u7f18\u5206\u5e03\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6536\u655b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5904\u7406\u590d\u6742\u7684\u3001\u5177\u6709\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.10130", "pdf": "https://arxiv.org/pdf/2506.10130", "abs": "https://arxiv.org/abs/2506.10130", "authors": ["Luciano Floridi"], "title": "A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI", "categories": ["cs.AI"], "comment": null, "summary": "This article introduces a conjecture that formalises a fundamental trade-off\nbetween provable correctness and broad data-mapping capacity in Artificial\nIntelligence (AI) systems. When an AI system is engineered for deductively\nwatertight guarantees (demonstrable certainty about the error-free nature of\nits outputs) -- as in classical symbolic AI -- its operational domain must be\nnarrowly circumscribed and pre-structured. Conversely, a system that can input\nhigh-dimensional data to produce rich information outputs -- as in contemporary\ngenerative models -- necessarily relinquishes the possibility of zero-error\nperformance, incurring an irreducible risk of errors or misclassification. By\nmaking this previously implicit trade-off explicit and open to rigorous\nverification, the conjecture significantly reframes both engineering ambitions\nand philosophical expectations for AI. After reviewing the historical\nmotivations for this tension, the article states the conjecture in\ninformation-theoretic form and contextualises it within broader debates in\nepistemology, formal verification, and the philosophy of technology. It then\noffers an analysis of its implications and consequences, drawing on notions of\nunderdetermination, prudent epistemic risk, and moral responsibility. The\ndiscussion clarifies how, if correct, the conjecture would help reshape\nevaluation standards, governance frameworks, and hybrid system design. The\nconclusion underscores the importance of eventually proving or refuting the\ninequality for the future of trustworthy AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u731c\u60f3\uff0c\u8be5\u731c\u60f3\u660e\u786e\u6307\u51fa\u4e86AI\u7cfb\u7edf\u4e2d\u53ef\u8bc1\u660e\u6b63\u786e\u6027\u4e0e\u5e7f\u6cdb\u6570\u636e\u6620\u5c04\u80fd\u529b\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002\u5f53AI\u7cfb\u7edf\u88ab\u8bbe\u8ba1\u4e3a\u5177\u6709\u6f14\u7ece\u4e0a\u65e0\u61c8\u53ef\u51fb\u7684\u4fdd\u8bc1\u65f6\uff0c\u5176\u64cd\u4f5c\u9886\u57df\u5fc5\u987b\u88ab\u4e25\u683c\u9650\u5236\uff1b\u53cd\u4e4b\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u5e76\u4ea7\u751f\u4e30\u5bcc\u4fe1\u606f\u8f93\u51fa\u7684\u7cfb\u7edf\u5219\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5e26\u6765\u9519\u8bef\u98ce\u9669\u3002\u6587\u7ae0\u901a\u8fc7\u4fe1\u606f\u8bba\u5f62\u5f0f\u9610\u8ff0\u4e86\u8fd9\u4e00\u731c\u60f3\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u5bf9\u5de5\u7a0b\u76ee\u6807\u3001\u54f2\u5b66\u671f\u671b\u4ee5\u53caAI\u8bc4\u4f30\u6807\u51c6\u3001\u6cbb\u7406\u6846\u67b6\u548c\u6df7\u5408\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u89e3\u51b3\u5728\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u957f\u671f\u5b58\u5728\u7684\u9690\u542b\u77db\u76fe\uff1a\u5373\u7edd\u5bf9\u6b63\u786e\u6027\u548c\u5904\u7406\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002\u8fd9\u79cd\u5f20\u529b\u5728\u5386\u53f2\u4e0a\u4e00\u76f4\u5b58\u5728\uff0c\u4f46\u6ca1\u6709\u5f97\u5230\u5145\u5206\u7684\u8ba4\u8bc6\u548c\u6b63\u5f0f\u5316\u3002", "method": "\u8bba\u6587\u9996\u5148\u56de\u987e\u4e86\u9020\u6210\u8fd9\u79cd\u7d27\u5f20\u5173\u7cfb\u7684\u5386\u53f2\u52a8\u56e0\uff0c\u7136\u540e\u5c06\u731c\u60f3\u8868\u8ff0\u6210\u4fe1\u606f\u8bba\u7684\u5f62\u5f0f\uff0c\u5e76\u5c06\u5176\u7f6e\u4e8e\u66f4\u5e7f\u6cdb\u7684\u8ba4\u77e5\u8bba\u3001\u5f62\u5f0f\u9a8c\u8bc1\u548c\u6280\u672f\u54f2\u5b66\u8fa9\u8bba\u7684\u80cc\u666f\u4e4b\u4e2d\u3002\u63a5\u7740\uff0c\u6587\u7ae0\u5206\u6790\u4e86\u8fd9\u4e00\u731c\u60f3\u53ef\u80fd\u5e26\u6765\u7684\u5f71\u54cd\u548c\u540e\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5982\u679c\u8fd9\u4e2a\u731c\u60f3\u6210\u7acb\uff0c\u5b83\u5c06\u6709\u52a9\u4e8e\u91cd\u5851AI\u7cfb\u7edf\u7684\u8bc4\u4ef7\u6807\u51c6\u3001\u6cbb\u7406\u67b6\u6784\u4ee5\u53ca\u6df7\u5408\u7cfb\u7edf\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u6700\u7ec8\u8bc1\u660e\u6216\u53cd\u9a73\u8fd9\u4e2a\u4e0d\u7b49\u5f0f\u5bf9\u4e8e\u672a\u6765\u53ef\u4fe1AI\u53d1\u5c55\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.10060", "pdf": "https://arxiv.org/pdf/2506.10060", "abs": "https://arxiv.org/abs/2506.10060", "authors": ["Brendan Leigh Ross", "No\u00ebl Vouitsis", "Atiyeh Ashari Ghomi", "Rasa Hosseinzadeh", "Ji Xin", "Zhaoyan Liu", "Yi Sui", "Shiyi Hou", "Kin Kwan Leung", "Gabriel Loaiza-Ganem", "Jesse C. Cresswell"], "title": "Textual Bayes: Quantifying Uncertainty in LLM-Based Systems", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Although large language models (LLMs) are becoming increasingly capable of\nsolving challenging real-world tasks, accurately quantifying their uncertainty\nremains a critical open problem, which limits their applicability in\nhigh-stakes domains. This challenge is further compounded by the closed-source,\nblack-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can\nbe highly sensitive to the prompts that bind them together, which often require\nsignificant manual tuning (i.e., prompt engineering). In this work, we address\nthese challenges by viewing LLM-based systems through a Bayesian lens. We\ninterpret prompts as textual parameters in a statistical model, allowing us to\nuse a small training dataset to perform Bayesian inference over these prompts.\nThis novel perspective enables principled uncertainty quantification over both\nthe model's textual parameters and its downstream predictions, while also\nincorporating prior beliefs about these parameters expressed in free-form text.\nTo perform Bayesian inference, a difficult problem even for well-studied data\nmodalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a\nnovel Markov chain Monte Carlo (MCMC) algorithm that combines prompt\noptimization techniques with standard MCMC methods. MHLP is a turnkey\nmodification to existing LLM pipelines, including those that rely exclusively\non closed-source models. Empirically, we demonstrate that our method yields\nimprovements in both predictive accuracy and uncertainty quantification (UQ) on\na range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a\nviable path for incorporating methods from the rich Bayesian literature into\nthe era of LLMs, paving the way for more reliable and calibrated LLM-based\nsystems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8d1d\u53f6\u65af\u89c6\u89d2\u770b\u5f85\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u5c06\u63d0\u793a\u89c6\u4e3a\u7edf\u8ba1\u6a21\u578b\u4e2d\u7684\u6587\u672c\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u7b97\u6cd5MHLP\u4ee5\u6267\u884c\u8d1d\u53f6\u65af\u63a8\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u90fd\u6709\u6240\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u65b9\u9762\u53d8\u5f97\u8d8a\u6765\u8d8a\u5f3a\u5927\uff0c\u4f46\u51c6\u786e\u91cf\u5316\u5b83\u4eec\u7684\u4e0d\u786e\u5b9a\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u7684\u5f00\u653e\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u6027\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u6700\u5148\u8fdb\u7684LLMs\u662f\u95ed\u6e90\u7684\u9ed1\u76d2\u6027\u8d28\uff0c\u4e14\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5bf9\u63d0\u793a\u975e\u5e38\u654f\u611f\uff0c\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u624b\u52a8\u8c03\u4f18\u3002", "method": "\u672c\u6587\u5c06\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u7f6e\u4e8e\u8d1d\u53f6\u65af\u89c6\u89d2\u4e0b\uff0c\u628a\u63d0\u793a\u89e3\u91ca\u4e3a\u7edf\u8ba1\u6a21\u578b\u4e2d\u7684\u6587\u672c\u53c2\u6570\uff0c\u5e76\u5229\u7528\u5c0f\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u5bf9\u8fd9\u4e9b\u63d0\u793a\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u7406\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u7b97\u6cd5\u2014\u2014Metropolis-Hastings through LLM Proposals (MHLP)\uff0c\u5b83\u7ed3\u5408\u4e86\u63d0\u793a\u4f18\u5316\u6280\u672f\u548c\u6807\u51c6MCMC\u65b9\u6cd5\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217LLM\u57fa\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6548\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9053\u8def\uff0c\u5c06\u4e30\u5bcc\u7684\u8d1d\u53f6\u65af\u6587\u732e\u65b9\u6cd5\u6574\u5408\u5230LLMs\u65f6\u4ee3\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u548c\u6821\u51c6\u826f\u597d\u7684\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.10293", "pdf": "https://arxiv.org/pdf/2506.10293", "abs": "https://arxiv.org/abs/2506.10293", "authors": ["Mo\u00efse Blanchard", "Samory Kpotufe"], "title": "Distributionally-Constrained Adversaries in Online Learning", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "There has been much recent interest in understanding the continuum from\nadversarial to stochastic settings in online learning, with various frameworks\nincluding smoothed settings proposed to bridge this gap. We consider the more\ngeneral and flexible framework of distributionally constrained adversaries in\nwhich instances are drawn from distributions chosen by an adversary within some\nconstrained distribution class [RST11]. Compared to smoothed analysis, we\nconsider general distributional classes which allows for a fine-grained\nunderstanding of learning settings between fully stochastic and fully\nadversarial for which a learner can achieve non-trivial regret. We give a\ncharacterization for which distribution classes are learnable in this context\nagainst both oblivious and adaptive adversaries, providing insights into the\ntypes of interplay between the function class and distributional constraints on\nadversaries that enable learnability. In particular, our results recover and\ngeneralize learnability for known smoothed settings. Further, we show that for\nseveral natural function classes including linear classifiers, learning can be\nachieved without any prior knowledge of the distribution class -- in other\nwords, a learner can simultaneously compete against any constrained adversary\nwithin learnable distribution classes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u5b66\u4e60\u4e2d\u4ece\u5bf9\u6297\u6027\u5230\u968f\u673a\u6027\u8bbe\u7f6e\u7684\u8fde\u7eed\u4f53\uff0c\u63d0\u51fa\u4e86\u53d7\u5206\u5e03\u7ea6\u675f\u7684\u5bf9\u624b\u8fd9\u4e00\u66f4\u901a\u7528\u7075\u6d3b\u7684\u6846\u67b6\u3002\u6587\u7ae0\u7ed9\u51fa\u4e86\u5728\u8be5\u6846\u67b6\u4e0b\u53ef\u5b66\u4e60\u7684\u5206\u5e03\u7c7b\u522b\u7684\u7279\u5f81\uff0c\u5e76\u4e14\u8868\u660e\u5bf9\u4e8e\u4e00\u4e9b\u81ea\u7136\u51fd\u6570\u7c7b\u522b\uff0c\u5b66\u4e60\u53ef\u4ee5\u65e0\u9700\u4e8b\u5148\u77e5\u9053\u5206\u5e03\u7c7b\u522b\u3002", "motivation": "\u7406\u89e3\u5728\u7ebf\u5b66\u4e60\u4e2d\u4ece\u5bf9\u6297\u6027\u5230\u968f\u673a\u6027\u8bbe\u7f6e\u7684\u8fde\u7eed\u4f53\uff0c\u4ee5\u53ca\u5728\u8fd9\u4e24\u4e2a\u6781\u7aef\u4e4b\u95f4\u5982\u4f55\u5b9e\u73b0\u975e\u5e73\u51e1\u9057\u61be\u7684\u5b66\u4e60\u8bbe\u5b9a\u3002", "method": "\u91c7\u7528\u53d7\u5206\u5e03\u7ea6\u675f\u7684\u5bf9\u624b\u6846\u67b6\u6765\u5206\u6790\u54ea\u4e9b\u5206\u5e03\u7c7b\u522b\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u5e76\u4e14\u8003\u8651\u4e86\u65e0\u610f\u8bc6\u548c\u9002\u5e94\u6027\u5bf9\u624b\u4e24\u79cd\u60c5\u51b5\u3002", "result": "\u5f97\u51fa\u4e86\u5728\u7ed9\u5b9a\u6846\u67b6\u4e0b\u53ef\u5b66\u4e60\u7684\u5206\u5e03\u7c7b\u522b\u7684\u7279\u5f81\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u4e8e\u67d0\u4e9b\u81ea\u7136\u51fd\u6570\u7c7b\u522b\uff08\u5982\u7ebf\u6027\u5206\u7c7b\u5668\uff09\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5b66\u4e60\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u5206\u5e03\u7ea6\u675f\u5bf9\u624b\u7684\u7814\u7a76\uff0c\u4e3a\u7406\u89e3\u51fd\u6570\u7c7b\u522b\u4e0e\u5bf9\u624b\u5206\u5e03\u7ea6\u675f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u63a8\u5e7f\u4e86\u5df2\u77e5\u5e73\u6ed1\u8bbe\u7f6e\u4e0b\u7684\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2506.10157", "pdf": "https://arxiv.org/pdf/2506.10157", "abs": "https://arxiv.org/abs/2506.10157", "authors": ["Michelle M. Li", "Ben Y. Reis", "Adam Rodman", "Tianxi Cai", "Noa Dagan", "Ran D. Balicer", "Joseph Loscalzo", "Isaac S. Kohane", "Marinka Zitnik"], "title": "One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Medical foundation models, including language models trained on clinical\nnotes, vision-language models on medical images, and multimodal models on\nelectronic health records, can summarize clinical notes, answer medical\nquestions, and assist in decision-making. Adapting these models to new\npopulations, specialties, or settings typically requires fine-tuning, careful\nprompting, or retrieval from knowledge bases. This can be impractical, and\nlimits their ability to interpret unfamiliar inputs and adjust to clinical\nsituations not represented during training. As a result, models are prone to\ncontextual errors, where predictions appear reasonable but fail to account for\ncritical patient-specific or contextual information. These errors stem from a\nfundamental limitation that current models struggle with: dynamically adjusting\ntheir behavior across evolving contexts of medical care. In this Perspective,\nwe outline a vision for context-switching in medical AI: models that\ndynamically adapt their reasoning without retraining to new specialties,\npopulations, workflows, and clinical roles. We envision context-switching AI to\ndiagnose, manage, and treat a wide range of diseases across specialties and\nregions, and expand access to medical care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u613f\u666f\uff0c\u5373\u5f00\u53d1\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u533b\u7597\u80cc\u666f\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u533b\u5b66\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u8f93\u5165\u548c\u9002\u5e94\u65b0\u4e34\u5e8a\u60c5\u5883\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u57fa\u7840\u6a21\u578b\u5728\u9047\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u672a\u66fe\u51fa\u73b0\u7684\u4e34\u5e8a\u60c5\u51b5\u65f6\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u964c\u751f\u8f93\u5165\u6216\u8fdb\u884c\u76f8\u5e94\u8c03\u6574\uff0c\u5bb9\u6613\u4ea7\u751f\u4e0a\u4e0b\u6587\u9519\u8bef\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u4e0a\u4e0b\u6587\u5207\u6362\u80fd\u529b\u7684AI\u6a21\u578b\u6982\u5ff5\uff0c\u8fd9\u79cd\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u6839\u636e\u4e0d\u540c\u7684\u4e13\u4e1a\u3001\u4eba\u7fa4\u3001\u5de5\u4f5c\u6d41\u7a0b\u4ee5\u53ca\u4e34\u5e8a\u89d2\u8272\u52a8\u6001\u8c03\u6574\u5176\u63a8\u7406\u65b9\u5f0f\u3002", "result": "\u9884\u671f\u8fd9\u6837\u7684\u4e0a\u4e0b\u6587\u5207\u6362AI\u80fd\u591f\u8bca\u65ad\u3001\u7ba1\u7406\u548c\u6cbb\u7597\u8de8\u4e13\u79d1\u548c\u5730\u533a\u5e7f\u6cdb\u75be\u75c5\uff0c\u5e76\u6269\u5927\u533b\u7597\u670d\u52a1\u7684\u53ef\u53ca\u6027\u3002", "conclusion": "\u6587\u7ae0\u8ba4\u4e3a\uff0c\u5177\u5907\u4e0a\u4e0b\u6587\u5207\u6362\u80fd\u529b\u7684\u533b\u5b66AI\u662f\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5b83\u80fd\u591f\u66f4\u597d\u5730\u670d\u52a1\u4e8e\u4e0d\u65ad\u53d8\u5316\u7684\u533b\u7597\u73af\u5883\u3002"}}
{"id": "2506.10089", "pdf": "https://arxiv.org/pdf/2506.10089", "abs": "https://arxiv.org/abs/2506.10089", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "title": "Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection", "categories": ["cs.LG", "I.2.6; I.5.1; G.3"], "comment": "41 pages, 6 figures", "summary": "Out-of-distribution (OOD) detection is a critical task in machine learning,\nparticularly for safety-critical applications where unexpected inputs must be\nreliably flagged. While hierarchical variational autoencoders (HVAEs) offer\nimproved representational capacity over traditional VAEs, their performance is\nhighly sensitive to how latent dimensions are distributed across layers.\nExisting approaches often allocate latent capacity arbitrarily, leading to\nineffective representations or posterior collapse. In this work, we introduce a\ntheoretically grounded framework for optimizing latent dimension allocation in\nHVAEs, drawing on principles from information theory to formalize the trade-off\nbetween information loss and representational attenuation. We prove the\nexistence of an optimal allocation ratio $r^{\\ast}$ under a fixed latent\nbudget, and empirically show that tuning this ratio consistently improves OOD\ndetection performance across datasets and architectures. Our approach\noutperforms baseline HVAE configurations and provides practical guidance for\nprincipled latent structure design, leading to more robust OOD detection with\ndeep generative models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.10433", "pdf": "https://arxiv.org/pdf/2506.10433", "abs": "https://arxiv.org/abs/2506.10433", "authors": ["Florian Handke", "F\u00e9lix Koulischer", "Gabriel Raya", "Luca Ambrogioni"], "title": "Measuring Semantic Information Production in Generative Diffusion Models", "categories": ["stat.ML", "cs.LG"], "comment": "4 pages, 3 figures, an appendix with derivations and implementation\n  details, accepted at ICLR DeLTa 2025", "summary": "It is well known that semantic and structural features of the generated\nimages emerge at different times during the reverse dynamics of diffusion, a\nphenomenon that has been connected to physical phase transitions in magnets and\nother materials. In this paper, we introduce a general information-theoretic\napproach to measure when these class-semantic \"decisions\" are made during the\ngenerative process. By using an online formula for the optimal Bayesian\nclassifier, we estimate the conditional entropy of the class label given the\nnoisy state. We then determine the time intervals corresponding to the highest\ninformation transfer between noisy states and class labels using the time\nderivative of the conditional entropy. We demonstrate our method on\none-dimensional Gaussian mixture models and on DDPM models trained on the\nCIFAR10 dataset. As expected, we find that the semantic information transfer is\nhighest in the intermediate stages of diffusion while vanishing during the\nfinal stages. However, we found sizable differences between the entropy rate\nprofiles of different classes, suggesting that different \"semantic decisions\"\nare located at different intermediate times.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u91cf\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7c7b\u8bed\u4e49'\u51b3\u7b56'\u7684\u65f6\u95f4\u3002\u901a\u8fc7\u4f30\u8ba1\u7ed9\u5b9a\u566a\u58f0\u72b6\u6001\u4e0b\u7684\u7c7b\u522b\u6807\u7b7e\u7684\u6761\u4ef6\u71b5\uff0c\u5e76\u5229\u7528\u6761\u4ef6\u71b5\u7684\u65f6\u95f4\u5bfc\u6570\u6765\u786e\u5b9a\u566a\u58f0\u72b6\u6001\u548c\u7c7b\u522b\u6807\u7b7e\u4e4b\u95f4\u4fe1\u606f\u4f20\u9012\u6700\u5927\u7684\u65f6\u95f4\u95f4\u9694\u3002\u8be5\u65b9\u6cd5\u5728\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548cCIFAR10\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684DDPM\u6a21\u578b\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u7ed3\u679c\u8868\u660e\u8bed\u4e49\u4fe1\u606f\u4f20\u9012\u5728\u6269\u6563\u8fc7\u7a0b\u7684\u4e2d\u95f4\u9636\u6bb5\u6700\u9ad8\uff0c\u800c\u5728\u6700\u540e\u9636\u6bb5\u6d88\u5931\uff0c\u4e14\u4e0d\u540c\u7c7b\u522b\u7684\u71b5\u7387\u66f2\u7ebf\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u5728\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u8bed\u4e49\u548c\u7ed3\u6784\u7279\u5f81\u4f55\u65f6\u51fa\u73b0\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u4e0e\u7269\u7406\u76f8\u53d8\u76f8\u5173\u8054\u3002\u4e3a\u4e86\u66f4\u7cbe\u786e\u5730\u5b9a\u4f4d\u751f\u6210\u8fc7\u7a0b\u4e2d\u7c7b\u8bed\u4e49'\u51b3\u7b56'\u7684\u5177\u4f53\u65f6\u95f4\u70b9\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4fe1\u606f\u8bba\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u516c\u5f0f\u8ba1\u7b97\u6700\u4f18\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\uff0c\u5e76\u636e\u6b64\u4f30\u8ba1\u7ed9\u5b9a\u566a\u58f0\u72b6\u6001\u4e0b\u7684\u7c7b\u522b\u6807\u7b7e\u6761\u4ef6\u71b5\u3002\u4f7f\u7528\u6761\u4ef6\u71b5\u7684\u65f6\u95f4\u5bfc\u6570\u6765\u8bc6\u522b\u566a\u58f0\u72b6\u6001\u548c\u7c7b\u522b\u6807\u7b7e\u95f4\u4fe1\u606f\u4f20\u9012\u6700\u5927\u503c\u7684\u65f6\u95f4\u95f4\u9694\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u7684\u4e2d\u95f4\u9636\u6bb5\uff0c\u8bed\u4e49\u4fe1\u606f\u4f20\u9012\u8fbe\u5230\u5cf0\u503c\uff0c\u800c\u5728\u6700\u7ec8\u9636\u6bb5\u5219\u8d8b\u4e8e\u6d88\u5931\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u7c7b\u522b\u7684\u71b5\u7387\u66f2\u7ebf\u663e\u793a\u51fa\u4e0d\u540c\u7684\u6a21\u5f0f\uff0c\u8868\u660e\u4e0d\u540c'\u8bed\u4e49\u51b3\u7b56'\u53d1\u751f\u5728\u4e0d\u540c\u7684\u4e2d\u95f4\u65f6\u95f4\u70b9\u3002", "conclusion": "\u901a\u8fc7\u65b0\u63d0\u51fa\u7684\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u7cbe\u786e\u5b9a\u4f4d\u751f\u6210\u8fc7\u7a0b\u4e2d\u8bed\u4e49\u4fe1\u606f\u7684\u5f62\u6210\u65f6\u95f4\uff0c\u8fd9\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u6f14\u53d8\u8fc7\u7a0b\u3002"}}
{"id": "2506.10179", "pdf": "https://arxiv.org/pdf/2506.10179", "abs": "https://arxiv.org/abs/2506.10179", "authors": ["Hamzah Dabool", "Raghad Mustafa"], "title": "Correlation vs causation in Alzheimer's disease: an interpretability-driven study", "categories": ["cs.AI", "q-bio.QM", "stat.AP"], "comment": null, "summary": "Understanding the distinction between causation and correlation is critical\nin Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and\nthe identification of true disease drivers. This experiment investigates the\nrelationships among clinical, cognitive, genetic, and biomarker features using\na combination of correlation analysis, machine learning classification, and\nmodel interpretability techniques. Employing the XGBoost algorithm, we\nidentified key features influencing AD classification, including cognitive\nscores and genetic risk factors. Correlation matrices revealed clusters of\ninterrelated variables, while SHAP (SHapley Additive exPlanations) values\nprovided detailed insights into feature contributions across disease stages.\nOur results highlight that strong correlations do not necessarily imply\ncausation, emphasizing the need for careful interpretation of associative data.\nBy integrating feature importance and interpretability with classical\nstatistical analysis, this work lays groundwork for future causal inference\nstudies aimed at uncovering true pathological mechanisms. Ultimately,\ndistinguishing causal factors from correlated markers can lead to improved\nearly diagnosis and targeted interventions for Alzheimer's disease.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u76f8\u5173\u6027\u5206\u6790\u3001\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u4e34\u5e8a\u3001\u8ba4\u77e5\u3001\u9057\u4f20\u53ca\u751f\u7269\u6807\u5fd7\u7269\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f7f\u7528XGBoost\u7b97\u6cd5\u8bc6\u522b\u51fa\u5f71\u54cdAD\u5206\u7c7b\u7684\u5173\u952e\u7279\u5f81\uff0c\u5e76\u5229\u7528SHAP\u503c\u6df1\u5165\u4e86\u89e3\u7279\u5f81\u5728\u4e0d\u540c\u75be\u75c5\u9636\u6bb5\u7684\u8d21\u732e\u3002\u7814\u7a76\u5f3a\u8c03\u5f3a\u76f8\u5173\u5e76\u4e0d\u610f\u5473\u7740\u56e0\u679c\u5173\u7cfb\uff0c\u4e3a\u672a\u6765\u63ed\u793a\u771f\u6b63\u75c5\u7406\u673a\u5236\u7684\u56e0\u679c\u63a8\u65ad\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u7406\u89e3\u56e0\u679c\u5173\u7cfb\u4e0e\u76f8\u5173\u6027\u7684\u533a\u522b\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u4f1a\u5f71\u54cd\u8bca\u65ad\u3001\u6cbb\u7597\u4ee5\u53ca\u771f\u6b63\u75be\u75c5\u9a71\u52a8\u56e0\u7d20\u7684\u8bc6\u522b\u3002", "method": "\u5b9e\u9a8c\u91c7\u7528\u4e86\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\u6765\u63a2\u7a76\u4e34\u5e8a\u3001\u8ba4\u77e5\u3001\u9057\u4f20\u548c\u751f\u7269\u6807\u5fd7\u7269\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5305\u62ec\u4f7f\u7528\u76f8\u5173\u6027\u5206\u6790\u3001\u57fa\u4e8eXGBoost\u7b97\u6cd5\u7684\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u6280\u672f\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5982SHAP\u503c\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86\u5f71\u54cdAD\u5206\u7c7b\u7684\u5173\u952e\u7279\u5f81\uff0c\u4f8b\u5982\u8ba4\u77e5\u5206\u6570\u548c\u9057\u4f20\u98ce\u9669\u56e0\u5b50\uff0c\u5e76\u901a\u8fc7SHAP\u503c\u63d0\u4f9b\u4e86\u5173\u4e8e\u7279\u5f81\u5728\u6574\u4e2a\u75be\u75c5\u8fc7\u7a0b\u4e2d\u8d21\u732e\u5ea6\u7684\u6df1\u5165\u89c1\u89e3\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u5f3a\u70c8\u7684\u76f8\u5173\u6027\u5e76\u4e0d\u603b\u662f\u6697\u793a\u7740\u56e0\u679c\u8054\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u7279\u5f81\u91cd\u8981\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0e\u7ecf\u5178\u7edf\u8ba1\u5206\u6790\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u65e8\u5728\u63ed\u793a\u771f\u5b9e\u75c5\u7406\u673a\u5236\u7684\u672a\u6765\u56e0\u679c\u63a8\u7406\u7814\u7a76\u6253\u4e0b\u4e86\u57fa\u7840\u3002\u6700\u7ec8\uff0c\u533a\u5206\u56e0\u679c\u56e0\u7d20\u4e0e\u5173\u8054\u6807\u8bb0\u53ef\u4ee5\u5bfc\u81f4\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u6539\u8fdb\u548c\u9776\u5411\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2506.10091", "pdf": "https://arxiv.org/pdf/2506.10091", "abs": "https://arxiv.org/abs/2506.10091", "authors": ["Bingshan Hu", "Zheng He", "Danica J. Sutherland"], "title": "Efficient kernelized bandit algorithms via exploration distributions", "categories": ["cs.LG"], "comment": null, "summary": "We consider a kernelized bandit problem with a compact arm set ${X} \\subset\n\\mathbb{R}^d $ and a fixed but unknown reward function $f^*$ with a finite norm\nin some Reproducing Kernel Hilbert Space (RKHS). We propose a class of\ncomputationally efficient kernelized bandit algorithms, which we call\nGP-Generic, based on a novel concept: exploration distributions. This class of\nalgorithms includes Upper Confidence Bound-based approaches as a special case,\nbut also allows for a variety of randomized algorithms. With careful choice of\nexploration distribution, our proposed generic algorithm realizes a wide range\nof concrete algorithms that achieve $\\tilde{O}(\\gamma_T\\sqrt{T})$ regret\nbounds, where $\\gamma_T$ characterizes the RKHS complexity. This matches known\nresults for UCB- and Thompson Sampling-based algorithms; we also show that in\npractice, randomization can yield better practical results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7c7b\u57fa\u4e8e\u63a2\u7d22\u5206\u5e03\u6982\u5ff5\u7684\u9ad8\u6548\u6838\u5316bandit\u7b97\u6cd5GP-Generic\uff0c\u8be5\u7b97\u6cd5\u4e0d\u4ec5\u5305\u62ec\u57fa\u4e8e\u7f6e\u4fe1\u4e0a\u754c\u7684\u65b9\u6cd5\uff0c\u4e5f\u5141\u8bb8\u5404\u79cd\u968f\u673a\u5316\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u9002\u5f53\u9009\u62e9\u63a2\u7d22\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5b9e\u73b0\u4e0e\u73b0\u6709\u7b97\u6cd5\u76f8\u5339\u914d\u7684\u540e\u6094\u754c\u3002", "motivation": "\u7814\u7a76\u8005\u4eec\u8003\u8651\u4e86\u4e00\u4e2a\u5177\u6709\u7d27\u51d1\u81c2\u96c6\u548c\u672a\u77e5\u4f46\u56fa\u5b9a\u7684\u5956\u52b1\u51fd\u6570\u7684\u95ee\u9898\uff0c\u8be5\u5956\u52b1\u51fd\u6570\u5728\u4e00\u4e2a\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4(RKHS)\u4e2d\u5177\u6709\u6709\u9650\u8303\u6570\u3002\u4ed6\u4eec\u7684\u52a8\u673a\u662f\u8bbe\u8ba1\u4e00\u7c7b\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u6838\u5316bandit\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGP-Generic\u7684\u7b97\u6cd5\u7c7b\uff0c\u5b83\u57fa\u4e8e\u65b0\u9896\u7684\u6982\u5ff5\uff1a\u63a2\u7d22\u5206\u5e03\u3002\u8fd9\u7c7b\u7b97\u6cd5\u6db5\u76d6\u4e86\u57fa\u4e8e\u7f6e\u4fe1\u4e0a\u754c\u7684\u7b56\u7565\u4f5c\u4e3a\u7279\u6b8a\u60c5\u51b5\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u968f\u673a\u5316\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u63a2\u7d22\u5206\u5e03\uff0c\u6240\u63d0\u51fa\u7684\u901a\u7528\u7b97\u6cd5\u80fd\u591f\u5b9e\u73b0\u5e7f\u6cdb\u7684\u7279\u5b9a\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u8fbe\u5230$\\tilde{O}(\\gamma_T\\sqrt{T})$\u9057\u61be\u754c\u9650\uff0c\u8fd9\u4e0e\u5df2\u77e5\u7684\u57fa\u4e8eUCB- \u548c Thompson Sampling\u7684\u7b97\u6cd5\u7ed3\u679c\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u5b9e\u8bc1\u663e\u793a\uff0c\u5728\u5b9e\u8df5\u4e2d\u968f\u673a\u5316\u53ef\u4ee5\u4ea7\u751f\u66f4\u597d\u7684\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "GP-Generic\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5141\u8bb8\u7ed3\u5408\u4e0d\u540c\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u4ece\u800c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7684RKHS\u65f6\u3002"}}
{"id": "2506.10572", "pdf": "https://arxiv.org/pdf/2506.10572", "abs": "https://arxiv.org/abs/2506.10572", "authors": ["Kyohei Atarashi", "Satoshi Oyama", "Hiromi Arai", "Hisashi Kashima"], "title": "Box-Constrained Softmax Function and Its Application for Post-Hoc Calibration", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Controlling the output probabilities of softmax-based models is a common\nproblem in modern machine learning. Although the $\\mathrm{Softmax}$ function\nprovides soft control via its temperature parameter, it lacks the ability to\nenforce hard constraints, such as box constraints, on output probabilities,\nwhich can be critical in certain applications requiring reliable and\ntrustworthy models. In this work, we propose the box-constrained softmax\n($\\mathrm{BCSoftmax}$) function, a novel generalization of the\n$\\mathrm{Softmax}$ function that explicitly enforces lower and upper bounds on\noutput probabilities. While $\\mathrm{BCSoftmax}$ is formulated as the solution\nto a box-constrained optimization problem, we develop an exact and efficient\ncomputation algorithm for $\\mathrm{BCSoftmax}$. As a key application, we\nintroduce two post-hoc calibration methods based on $\\mathrm{BCSoftmax}$. The\nproposed methods mitigate underconfidence and overconfidence in predictive\nmodels by learning the lower and upper bounds of the output probabilities or\nlogits after model training, thereby enhancing reliability in downstream\ndecision-making tasks. We demonstrate the effectiveness of our methods\nexperimentally using the TinyImageNet, CIFAR-100, and 20NewsGroups datasets,\nachieving improvements in calibration metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Softmax\u51fd\u6570\u7684\u6cdb\u5316\u7248\u672c\u2014\u2014BCSoftmax\uff0c\u5b83\u80fd\u591f\u5728\u8f93\u51fa\u6982\u7387\u4e0a\u65bd\u52a0\u786c\u6027\u7ea6\u675f\uff0c\u5982\u4e0a\u4e0b\u754c\u9650\u5236\uff0c\u5e76\u4e14\u5f00\u53d1\u4e86\u8be5\u51fd\u6570\u7684\u6709\u6548\u8ba1\u7b97\u7b97\u6cd5\u3002\u57fa\u4e8eBCSoftmax\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u4e24\u79cd\u540e\u6821\u51c6\u65b9\u6cd5\u6765\u7f13\u89e3\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u4e0d\u81ea\u4fe1\u548c\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u51b3\u7b56\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u51e0\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6821\u51c6\u5ea6\u91cf\u65b9\u9762\u6709\u6240\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684Softmax\u51fd\u6570\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u6e29\u5ea6\u53c2\u6570\u8fdb\u884c\u8f6f\u63a7\u5236\uff0c\u4f46\u662f\u65e0\u6cd5\u5bf9\u8f93\u51fa\u6982\u7387\u65bd\u52a0\u5f3a\u5236\u6027\u7684\u786c\u7ea6\u675f\uff08\u4f8b\u5982\u8fb9\u754c\u7ea6\u675f\uff09\uff0c\u8fd9\u5728\u67d0\u4e9b\u9700\u8981\u53ef\u9760\u548c\u53ef\u4fe1\u6a21\u578b\u7684\u5e94\u7528\u4e2d\u662f\u975e\u5e38\u5173\u952e\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBCSoftmax\u7684\u65b0\u51fd\u6570\uff0c\u5b83\u662f\u4f20\u7edfSoftmax\u51fd\u6570\u7684\u4e00\u4e2a\u6cdb\u5316\uff0c\u80fd\u591f\u660e\u786e\u5730\u5bf9\u8f93\u51fa\u6982\u7387\u5b9e\u65bd\u4e0b\u9650\u548c\u4e0a\u9650\u7684\u7ea6\u675f\u3002\u540c\u65f6\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5e26\u6709\u7bb1\u5f0f\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cbe\u786e\u800c\u9ad8\u6548\u7684\u8ba1\u7b97\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u57fa\u4e8eBCSoftmax\u51fd\u6570\uff0c\u7814\u7a76\u8005\u5f15\u5165\u4e86\u4e24\u79cd\u540e\u6821\u51c6\u65b9\u6cd5\u4ee5\u8c03\u6574\u6a21\u578b\u8bad\u7ec3\u540e\u7684\u8f93\u51fa\u6982\u7387\u6216logits\u7684\u754c\u9650\u3002", "result": "\u901a\u8fc7TinyImageNet\u3001CIFAR-100 \u548c 20NewsGroups\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6539\u5584\u6a21\u578b\u6821\u51c6\u5ea6\u91cf\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "BCSoftmax\u51fd\u6570\u53ca\u5176\u76f8\u5173\u7684\u540e\u6821\u51c6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u73b0\u6709Softmax\u51fd\u6570\u65e0\u6cd5\u63d0\u4f9b\u786c\u6027\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u9884\u6d4b\u6a21\u578b\u5728\u6821\u51c6\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5bf9\u4e8e\u589e\u5f3a\u4e0b\u6e38\u51b3\u7b56\u4efb\u52a1\u7684\u53ef\u9760\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.10192", "pdf": "https://arxiv.org/pdf/2506.10192", "abs": "https://arxiv.org/abs/2506.10192", "authors": ["Filip Cano"], "title": "Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems", "categories": ["cs.AI", "I.2"], "comment": "202 pages, 38 figures, PhD Thesis", "summary": "Ensuring responsible use of artificial intelligence (AI) has become\nimperative as autonomous systems increasingly influence critical societal\ndomains. However, the concept of trustworthy AI remains broad and\nmulti-faceted. This thesis advances knowledge in the safety, fairness,\ntransparency, and accountability of AI systems. In safety, we extend classical\ndeterministic shielding techniques to become resilient against delayed\nobservations, enabling practical deployment in real-world conditions. We also\nimplement both deterministic and probabilistic safety shields into simulated\nautonomous vehicles to prevent collisions with road users, validating the use\nof these techniques in realistic driving simulators. We introduce fairness\nshields, a novel post-processing approach to enforce group fairness in\nsequential decision-making settings over finite and periodic time horizons. By\noptimizing intervention costs while strictly ensuring fairness constraints,\nthis method efficiently balances fairness with minimal interference. For\ntransparency and accountability, we propose a formal framework for assessing\nintentional behaviour in probabilistic decision-making agents, introducing\nquantitative metrics of agency and intention quotient. We use these metrics to\npropose a retrospective analysis of intention, useful for determining\nresponsibility when autonomous systems cause unintended harm. Finally, we unify\nthese contributions through the ``reactive decision-making'' framework,\nproviding a general formalization that consolidates previous approaches.\nCollectively, the advancements presented contribute practically to the\nrealization of safer, fairer, and more accountable AI systems, laying the\nfoundations for future research in trustworthy AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a8\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u5b89\u5168\u6027\u3001\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u65b9\u9762\u7684\u77e5\u8bc6\uff0c\u901a\u8fc7\u63d0\u51fa\u65b0\u7684\u9632\u62a4\u63aa\u65bd\u548c\u6280\u672f\u6846\u67b6\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u516c\u5e73\u4e14\u66f4\u6709\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u5bf9\u793e\u4f1a\u5173\u952e\u9886\u57df\u7684\u5f71\u54cd\u8d8a\u6765\u8d8a\u5927\uff0c\u786e\u4fdd\u4eba\u5de5\u667a\u80fd\u7684\u8d1f\u8d23\u4efb\u4f7f\u7528\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f46\u53ef\u4fe1AI\u7684\u6982\u5ff5\u4ecd\u7136\u5bbd\u6cdb\u4e14\u591a\u65b9\u9762\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u5177\u4f53\u5316\u3002", "method": "\u8bba\u6587\u6269\u5c55\u4e86\u7ecf\u5178\u786e\u5b9a\u6027\u5c4f\u853d\u6280\u672f\u4ee5\u5e94\u5bf9\u5ef6\u8fdf\u89c2\u5bdf\uff0c\u5e76\u5c06\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u5b89\u5168\u5c4f\u853d\u5b9e\u65bd\u5230\u6a21\u62df\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\uff1b\u5f15\u5165\u4e86\u516c\u5e73\u5c4f\u853d\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u548c\u5468\u671f\u65f6\u95f4\u8303\u56f4\u5185\u5f3a\u5236\u6267\u884c\u5e8f\u5217\u51b3\u7b56\u8bbe\u7f6e\u4e2d\u7684\u7fa4\u4f53\u516c\u5e73\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u6b63\u5f0f\u6846\u67b6\u6765\u8bc4\u4f30\u6982\u7387\u51b3\u7b56\u4ee3\u7406\u4e2d\u7684\u6545\u610f\u884c\u4e3a\uff0c\u5f15\u5165\u4e86\u80fd\u52a8\u6027\u548c\u610f\u56fe\u5546\u6570\u7684\u5b9a\u91cf\u6307\u6807\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u5e72\u9884\u6210\u672c\u540c\u65f6\u4e25\u683c\u786e\u4fdd\u516c\u5e73\u7ea6\u675f\uff0c\u65b0\u65b9\u6cd5\u6709\u6548\u5730\u5e73\u8861\u4e86\u516c\u5e73\u4e0e\u6700\u5c0f\u5e72\u9884\uff1b\u63d0\u51fa\u7684\u56de\u987e\u6027\u610f\u56fe\u5206\u6790\u6709\u52a9\u4e8e\u786e\u5b9a\u5f53\u81ea\u4e3b\u7cfb\u7edf\u9020\u6210\u610f\u5916\u4f24\u5bb3\u65f6\u7684\u8d23\u4efb\u5f52\u5c5e\uff1b\u901a\u8fc7\u201c\u53cd\u5e94\u5f0f\u51b3\u7b56\u201d\u6846\u67b6\u7edf\u4e00\u4e86\u8fd9\u4e9b\u8d21\u732e\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7efc\u5408\u4ee5\u524d\u65b9\u6cd5\u7684\u4e00\u822c\u5f62\u5f0f\u5316\u3002", "conclusion": "\u672c\u6587\u6240\u4f5c\u7684\u8d21\u732e\u5b9e\u9645\u4e0a\u4fc3\u8fdb\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u516c\u5e73\u3001\u66f4\u6709\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u5b9e\u73b0\uff0c\u5e76\u4e3a\u672a\u6765\u53ef\u4fe1AI\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.10094", "pdf": "https://arxiv.org/pdf/2506.10094", "abs": "https://arxiv.org/abs/2506.10094", "authors": ["Md. Faizul Islam Ansari"], "title": "Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders", "categories": ["cs.LG"], "comment": "6 pages, 6 figures, experimental study on deep clustering with\n  autoencoders", "summary": "This research implements an advanced unsupervised clustering system for MNIST\nhandwritten digits through two-phase deep autoencoder architecture. A deep\nneural autoencoder requires a training process during phase one to develop\nminimal yet interpretive representations of images by minimizing reconstruction\nerrors. During the second phase we unify the reconstruction error with a KMeans\nclustering loss for learned latent embeddings through a joint distance-based\nobjective. Our model contains three elements which include batch normalization\ncombined with dropout and weight decay for achieving generalized and stable\nresults. The framework achieves superior clustering performance during\nextensive tests which used intrinsic measurements including Silhouette Score\nand Davies-Bouldin Index coupled with extrinsic metrics NMI and ARI when\nprocessing image features. The research uses t-SNE visualization to present\nlearned embeddings that show distinct clusters for digits. Our approach reaches\nan optimal combination between data reconstruction accuracy and cluster\nseparation purity when adding the benefit of understandable results and\nscalable implementations. The approach creates a dependable base that helps\ndeploy unsupervised representation learning in different large-scale image\nclustering applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u9636\u6bb5\u6df1\u5ea6\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u805a\u7c7b\u7cfb\u7edf\uff0c\u7528\u4e8eMNIST\u624b\u5199\u6570\u5b57\u3002\u6a21\u578b\u7ed3\u5408\u4e86\u6279\u91cf\u5f52\u4e00\u5316\u3001dropout\u548c\u6743\u91cd\u8870\u51cf\uff0c\u5e76\u5728\u91cd\u6784\u8bef\u5dee\u4e0a\u7edf\u4e00\u4e86KMeans\u805a\u7c7b\u635f\u5931\uff0c\u4ee5\u5b9e\u73b0\u56fe\u50cf\u7279\u5f81\u7684\u5353\u8d8a\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5bf9MNIST\u624b\u5199\u6570\u5b57\u8fdb\u884c\u65e0\u76d1\u7763\u805a\u7c7b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u91cd\u6784\u51c6\u786e\u6027\u548c\u7c07\u5206\u79bb\u7eaf\u5ea6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u81ea\u52a8\u7f16\u7801\u5668\u4ee5\u6700\u5c0f\u5316\u91cd\u6784\u8bef\u5dee\u6765\u83b7\u5f97\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u91cd\u6784\u8bef\u5dee\u4e0eKMeans\u805a\u7c7b\u635f\u5931\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u8054\u5408\u8ddd\u79bb\u76ee\u6807\u51fd\u6570\u4f18\u5316\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u5d4c\u5165\u3002\u6a21\u578b\u8fd8\u5305\u62ec\u6279\u91cf\u5f52\u4e00\u5316\u3001dropout\u548c\u6743\u91cd\u8870\u51cf\u6280\u672f\u3002", "result": "\u6846\u67b6\u5728\u5e7f\u6cdb\u7684\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u7528\u4e86\u8f6e\u5ed3\u7cfb\u6570\u3001Davies-Bouldin\u6307\u6570\u7b49\u5185\u5728\u5ea6\u91cf\u4ee5\u53caNMI\u548cARI\u7b49\u5916\u5728\u5ea6\u91cf\u8bc4\u4f30\u56fe\u50cf\u7279\u5f81\u5904\u7406\u6548\u679c\u3002t-SNE\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u5177\u6709\u660e\u663e\u7684\u6570\u5b57\u7c07\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u636e\u91cd\u6784\u7cbe\u5ea6\u548c\u7c07\u5206\u79bb\u7eaf\u5ea6\u4e4b\u95f4\u8fbe\u5230\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u7ed3\u679c\u6613\u4e8e\u7406\u89e3\u4e14\u5b9e\u73b0\u53ef\u6269\u5c55\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u50cf\u805a\u7c7b\u5e94\u7528\u4e2d\u7684\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002"}}
{"id": "2506.10664", "pdf": "https://arxiv.org/pdf/2506.10664", "abs": "https://arxiv.org/abs/2506.10664", "authors": ["Maxime Haddouche", "Otmane Sakhi"], "title": "Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Off-policy learning serves as the primary framework for learning optimal\npolicies from logged interactions collected under a static behavior policy. In\nthis work, we investigate the more practical and flexible setting of adaptive\noff-policy learning, where policies are iteratively refined and re-deployed to\ncollect higher-quality data. Building on the success of PAC-Bayesian learning\nwith Logarithmic Smoothing (LS) in static settings, we extend this framework to\nthe adaptive scenario using tools from online PAC-Bayesian theory. Furthermore,\nwe demonstrate that a principled adjustment to the LS estimator naturally\naccommodates multiple rounds of deployment and yields faster convergence rates\nunder mild conditions. Our method matches the performance of leading offline\napproaches in static settings, and significantly outperforms them when\nintermediate policy deployments are allowed. Empirical evaluations across\ndiverse scenarios highlight both the advantages of adaptive data collection and\nthe strength of the PAC-Bayesian formulation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u9002\u5e94\u79bb\u7b56\u7565\u5b66\u4e60\uff0c\u6269\u5c55\u4e86PAC-Bayesian\u5e26\u5bf9\u6570\u5e73\u6ed1\u7684\u5b66\u4e60\u6846\u67b6\u5230\u81ea\u9002\u5e94\u573a\u666f\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5141\u8bb8\u4e2d\u95f4\u7b56\u7565\u90e8\u7f72\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u7684\u8868\u73b0\u4f18\u4e8e\u9886\u5148\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u63a2\u8ba8\u4e00\u79cd\u66f4\u5b9e\u7528\u548c\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u5373\u81ea\u9002\u5e94\u79bb\u7b56\u7565\u5b66\u4e60\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u8fed\u4ee3\u5730\u6539\u8fdb\u5e76\u91cd\u65b0\u90e8\u7f72\u7b56\u7565\u4ee5\u6536\u96c6\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u3002", "method": "\u5229\u7528\u5728\u7ebfPAC-Bayesian\u7406\u8bba\u4e2d\u7684\u5de5\u5177\uff0c\u5c06PAC-Bayesian\u5e26\u5bf9\u6570\u5e73\u6ed1\uff08LS\uff09\u7684\u5b66\u4e60\u6846\u67b6\u4ece\u9759\u6001\u8bbe\u7f6e\u6269\u5c55\u5230\u4e86\u81ea\u9002\u5e94\u573a\u666f\u3002", "result": "\u901a\u8fc7\u7ecf\u9a8c\u8bc4\u4f30\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9759\u6001\u8bbe\u7f6e\u4e0b\u4e0e\u9886\u5148\u7684\u79bb\u7ebf\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u800c\u5728\u5141\u8bb8\u4e2d\u95f4\u7b56\u7565\u90e8\u7f72\u7684\u60c5\u51b5\u4e0b\u5219\u663e\u8457\u4f18\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u3002", "conclusion": "\u8c03\u6574\u540e\u7684LS\u4f30\u8ba1\u5668\u81ea\u7136\u9002\u5e94\u591a\u8f6e\u90e8\u7f72\uff0c\u5e76\u4e14\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u63d0\u4f9b\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u8fd9\u8868\u660e\u81ea\u9002\u5e94\u6570\u636e\u6536\u96c6\u7684\u4f18\u52bf\u4ee5\u53caPAC-Bayesian\u516c\u5f0f\u7684\u5f3a\u5927\u4e4b\u5904\u3002"}}
{"id": "2506.10264", "pdf": "https://arxiv.org/pdf/2506.10264", "abs": "https://arxiv.org/abs/2506.10264", "authors": ["Qiyue Yin", "Pei Xu", "Qiaozhe Li", "Shengda Liu", "Shengqi Shen", "Tong Wang", "Yihong Han", "Xiaonan Zhao", "Likun Yang", "Shiyue Cao", "Shiyu Qiu", "Yuxuan Liu", "Shizhao Yu", "Lei Cui", "Chengxin Yan", "Jie Sun", "Xiangquan Tang", "Kaiqi Huang"], "title": "WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models", "categories": ["cs.AI"], "comment": "15 pages, 17 figures", "summary": "Recent breakthroughs in Large Language Models (LLMs) have led to a\nqualitative leap in artificial intelligence' s performance on reasoning tasks,\nparticularly demonstrating remarkable capabilities in mathematical, symbolic,\nand commonsense reasoning. However, as a critical component of advanced human\ncognition, strategic reasoning, i.e., the ability to assess multi-agent\nbehaviors in dynamic environments, formulate action plans, and adapt\nstrategies, has yet to be systematically evaluated or modeled. To address this\ngap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark\nfor LLMs using wargame as its evaluation environment. Wargame, a quintessential\nhigh-complexity strategic scenario, integrates environmental uncertainty,\nadversarial dynamics, and non-unique strategic choices, making it an effective\ntestbed for assessing LLMs' capabilities in multi-agent decision-making, intent\ninference, and counterfactual reasoning. WGSR-Bench designs test samples around\nthree core tasks, i.e., Environmental situation awareness, Opponent risk\nmodeling and Policy generation, which serve as the core S-POE architecture, to\nsystematically assess main abilities of strategic reasoning. Finally, an\nLLM-based wargame agent is designed to integrate these parts for a\ncomprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess\nthe strengths and limitations of state-of-the-art LLMs in game-theoretic\nstrategic reasoning and to advance research in large model-driven strategic\nintelligence.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86WGSR-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528\u6218\u4e89\u6e38\u620f\u4f5c\u4e3a\u8bc4\u4f30\u73af\u5883\u7684\u9996\u4e2a\u7b56\u7565\u63a8\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u3001\u610f\u56fe\u63a8\u65ad\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u3001\u7b26\u53f7\u548c\u5e38\u8bc6\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u663e\u8457\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u6218\u7565\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7684\u8bc4\u4f30\u6216\u5efa\u6a21\u3002\u6218\u7565\u63a8\u7406\u662f\u9ad8\u7ea7\u4eba\u7c7b\u8ba4\u77e5\u7684\u4e00\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5305\u62ec\u8bc4\u4f30\u52a8\u6001\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u3001\u5236\u5b9a\u884c\u52a8\u8ba1\u5212\u548c\u9002\u5e94\u7b56\u7565\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86WGSR-Bench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9LLMs\u7684\u6218\u7565\u63a8\u7406\u57fa\u51c6\uff0c\u5b83\u5229\u7528\u4e86\u6218\u4e89\u6e38\u620f\u8fd9\u4e00\u9ad8\u590d\u6742\u5ea6\u7684\u6218\u7565\u573a\u666f\u6765\u8bc4\u4f30\u6a21\u578b\u3002\u8be5\u57fa\u51c6\u56f4\u7ed5\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\u8bbe\u8ba1\u6d4b\u8bd5\u6837\u672c\uff1a\u73af\u5883\u6001\u52bf\u611f\u77e5\u3001\u5bf9\u624b\u98ce\u9669\u5efa\u6a21\u4ee5\u53ca\u7b56\u7565\u751f\u6210\uff0c\u8fd9\u4e9b\u6784\u6210\u4e86S-POE\u67b6\u6784\u7684\u6838\u5fc3\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6218\u4e89\u6e38\u620f\u4ee3\u7406\uff0c\u4ee5\u8fdb\u884c\u5168\u9762\u7684\u6218\u7565\u63a8\u7406\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7WGSR-Bench\u53ef\u4ee5\u5bf9\u5f53\u524d\u6700\u5148\u8fdb\u7684LLMs\u5728\u535a\u5f08\u8bba\u6218\u7565\u63a8\u7406\u4e2d\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\u8fdb\u884c\u5168\u9762\u8bc4\u4ef7\uff0c\u5e76\u63a8\u52a8\u5927\u578b\u6a21\u578b\u9a71\u52a8\u7684\u6218\u7565\u667a\u80fd\u7814\u7a76\u7684\u53d1\u5c55\u3002", "conclusion": "WGSR-Bench\u4e3a\u8bc4\u4f30\u548c\u4fc3\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6218\u7565\u60c5\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2506.10102", "pdf": "https://arxiv.org/pdf/2506.10102", "abs": "https://arxiv.org/abs/2506.10102", "authors": ["Ahmed Elbakary", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "We present a novel federated multi-task learning method that leverages\ncross-client similarity to enable personalized learning for each client. To\navoid transmitting the entire model to the parameter server, we propose a\ncommunication-efficient scheme that introduces a feature anchor, a compact\nvector representation that summarizes the features learned from the client's\nlocal classes. This feature anchor is shared with the server to account for\nlocal clients' distribution. In addition, the clients share the classification\nheads, a lightweight linear layer, and perform a graph-based regularization to\nenable collaboration among clients. By modeling collaboration between clients\nas a dynamic graph and continuously updating and refining this graph, we can\naccount for any drift from the clients. To ensure beneficial knowledge transfer\nand prevent negative collaboration, we leverage a community detection-based\napproach that partitions this dynamic graph into homogeneous communities,\nmaximizing the sum of task similarities, represented as the graph edges'\nweights, within each community. This mechanism restricts collaboration to\nhighly similar clients within their formed communities, ensuring positive\ninteraction and preserving personalization. Extensive experiments on two\nheterogeneous datasets demonstrate that our method significantly outperforms\nstate-of-the-art baselines. Furthermore, we show that our method exhibits\nsuperior computation and communication efficiency and promotes fairness across\nclients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u6765\u5b9e\u73b0\u4e2a\u6027\u5316\u5b66\u4e60\u3002\u901a\u8fc7\u5f15\u5165\u7279\u5f81\u951a\u70b9\u548c\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5c42\uff0c\u4ee5\u53ca\u57fa\u4e8e\u56fe\u7684\u6b63\u5219\u5316\u548c\u793e\u533a\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u901a\u4fe1\u3001\u534f\u4f5c\u4e0e\u4e2a\u6027\u5316\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u6548\u7387\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u5ba2\u6237\u7aef\u95f4\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u5e76\u907f\u514d\u5c06\u6574\u4e2a\u6a21\u578b\u4f20\u8f93\u5230\u53c2\u6570\u670d\u52a1\u5668\uff0c\u7814\u7a76\u8005\u4eec\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u4e14\u80fd\u591f\u4fc3\u8fdb\u5ba2\u6237\u7aef\u95f4\u79ef\u6781\u534f\u4f5c\u7684\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5f15\u5165\u7279\u5f81\u951a\u70b9\u2014\u2014\u4e00\u4e2a\u7d27\u51d1\u5411\u91cf\u8868\u793a\uff0c\u7528\u4e8e\u603b\u7ed3\u4ece\u5ba2\u6237\u7aef\u672c\u5730\u7c7b\u5b66\u5230\u7684\u7279\u5f81\uff1b2) \u5ba2\u6237\u7aef\u5171\u4eab\u5206\u7c7b\u5934\uff0c\u5373\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5c42\uff1b3) \u6267\u884c\u57fa\u4e8e\u56fe\u7684\u6b63\u5219\u5316\u4ee5\u4fc3\u8fdb\u5ba2\u6237\u7aef\u95f4\u534f\u4f5c\uff1b4) \u901a\u8fc7\u5efa\u6a21\u5ba2\u6237\u7aef\u4e4b\u95f4\u534f\u4f5c\u7684\u52a8\u6001\u56fe\uff0c\u5e76\u6301\u7eed\u66f4\u65b0\u6b64\u56fe\u6765\u9002\u5e94\u53d8\u5316\uff1b5) \u4f7f\u7528\u57fa\u4e8e\u793e\u533a\u68c0\u6d4b\u7684\u65b9\u6cd5\u5212\u5206\u52a8\u6001\u56fe\uff0c\u4ee5\u786e\u4fdd\u4ec5\u5728\u9ad8\u5ea6\u76f8\u4f3c\u7684\u5ba2\u6237\u7aef\u4e4b\u95f4\u8fdb\u884c\u534f\u4f5c\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4e24\u4e2a\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u4fc3\u8fdb\u4e86\u5ba2\u6237\u7aef\u95f4\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u5730\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60\uff0c\u8fd8\u80fd\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\uff0c\u589e\u5f3a\u8de8\u5ba2\u6237\u7aef\u534f\u4f5c\u7684\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2506.10677", "pdf": "https://arxiv.org/pdf/2506.10677", "abs": "https://arxiv.org/abs/2506.10677", "authors": ["Sakhi Otmane", "Gilotte Alexandre", "Rohde David"], "title": "Practical Improvements of A/B Testing with Off-Policy Estimation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We address the problem of A/B testing, a widely used protocol for evaluating\nthe potential improvement achieved by a new decision system compared to a\nbaseline. This protocol segments the population into two subgroups, each\nexposed to a version of the system and estimates the improvement as the\ndifference between the measured effects. In this work, we demonstrate that the\ncommonly used difference-in-means estimator, while unbiased, can be improved.\nWe introduce a family of unbiased off-policy estimators that achieves lower\nvariance than the standard approach. Among this family, we identify the\nestimator with the lowest variance. The resulting estimator is simple, and\noffers substantial variance reduction when the two tested systems exhibit\nsimilarities. Our theoretical analysis and experimental results validate the\neffectiveness and practicality of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b9\u5dee\u66f4\u5c0f\u7684\u65e0\u504f\u79bb\u7b56\u7565\u4f30\u8ba1\u5668\u65cf\uff0c\u7528\u4e8eA/B\u6d4b\u8bd5\u4e2d\u6bd4\u4f20\u7edf\u5747\u503c\u5dee\u5f02\u4f30\u8ba1\u5668\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u7cfb\u7edf\u6539\u8fdb\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u6539\u5584A/B\u6d4b\u8bd5\u4e2d\u5e38\u7528\u7684\u5747\u503c\u5dee\u5f02\u4f30\u8ba1\u5668\uff0c\u8be5\u4f30\u8ba1\u5668\u5c3d\u7ba1\u65e0\u504f\u4f46\u53ef\u4ee5\u88ab\u4f18\u5316\u4ee5\u51cf\u5c11\u65b9\u5dee\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u65cf\u65b0\u7684\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u5e76\u4ece\u4e2d\u786e\u5b9a\u4e86\u5177\u6709\u6700\u4f4e\u65b9\u5dee\u7684\u4f30\u8ba1\u5668\u3002\u65b0\u65b9\u6cd5\u7b80\u5355\u4e14\u5f53\u6d4b\u8bd5\u7684\u4e24\u4e2a\u7cfb\u7edf\u76f8\u4f3c\u65f6\u80fd\u663e\u8457\u964d\u4f4e\u65b9\u5dee\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u90fd\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u5728A/B\u6d4b\u8bd5\u4e2d\u80fd\u591f\u63d0\u4f9b\u66f4\u52a0\u51c6\u786e\u7684\u6548\u679c\u6d4b\u91cf\uff0c\u5c24\u5176\u662f\u5728\u5f85\u6bd4\u8f83\u7cfb\u7edf\u4e4b\u95f4\u5b58\u5728\u76f8\u4f3c\u6027\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.10281", "pdf": "https://arxiv.org/pdf/2506.10281", "abs": "https://arxiv.org/abs/2506.10281", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "title": "Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution", "categories": ["cs.AI"], "comment": "12 pages", "summary": "Artificial Intelligence (AI) is reframed as a cognitive engine driving a\nnovel productivity revolution distinct from the Industrial Revolution's\nphysical thrust. This paper develops a theoretical framing of AI as a cognitive\nrevolution akin to written language - a transformative augmentation of human\nintellect rather than another mechanized tool. We compare AI's emergence to\nhistorical leaps in information technology to show how it amplifies knowledge\nwork. Examples from various domains demonstrate AI's impact as a driver of\nproductivity in cognitive tasks. We adopt a multidisciplinary perspective\ncombining computer science advances with economic insights and sociological\nperspectives on how AI reshapes work and society. Through conceptual\nframeworks, we visualize the shift from manual to cognitive productivity. Our\ncentral argument is that AI functions as an engine of cognition - comparable to\nhow human language revolutionized knowledge - heralding a new productivity\nparadigm. We discuss how this revolution demands rethinking of skills,\norganizations, and policies. This paper, balancing academic rigor with clarity,\nconcludes that AI's promise lies in complementing human cognitive abilities,\nmarking a new chapter in productivity evolution.", "AI": {"tldr": "\u672c\u6587\u5c06\u4eba\u5de5\u667a\u80fd\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u79cd\u8ba4\u77e5\u5f15\u64ce\uff0c\u63a8\u52a8\u7740\u4e00\u573a\u4e0e\u5de5\u4e1a\u9769\u547d\u4e0d\u540c\u7684\u751f\u4ea7\u529b\u9769\u547d\u3002\u901a\u8fc7\u8de8\u5b66\u79d1\u89c6\u89d2\u7ed3\u5408\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u7ecf\u6d4e\u5b66\u548c\u793e\u4f1a\u5b66\u7684\u89c2\u70b9\uff0c\u63a2\u8ba8\u4e86AI\u5982\u4f55\u91cd\u5851\u5de5\u4f5c\u548c\u793e\u4f1a\uff0c\u5e76\u63d0\u51fa\u4e86AI\u4f5c\u4e3a\u8ba4\u77e5\u5de5\u5177\u5bf9\u751f\u4ea7\u529b\u7684\u6df1\u8fdc\u5f71\u54cd\u3002", "motivation": "\u4f5c\u8005\u610f\u56fe\u91cd\u65b0\u6784\u5efa\u5bf9\u4eba\u5de5\u667a\u80fd\u7684\u7406\u89e3\uff0c\u5c06\u5176\u89c6\u4e3a\u4e00\u79cd\u7c7b\u4f3c\u4e8e\u4e66\u5199\u8bed\u8a00\u7684\u8ba4\u77e5\u9769\u547d\uff0c\u65e8\u5728\u653e\u5927\u4eba\u7c7b\u667a\u529b\u800c\u4e0d\u662f\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684\u673a\u68b0\u5316\u5de5\u5177\u3002", "method": "\u91c7\u7528\u591a\u5b66\u79d1\u89c6\u89d2\uff0c\u7ed3\u5408\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u8fdb\u6b65\u3001\u7ecf\u6d4e\u6d1e\u5bdf\u529b\u4ee5\u53ca\u793e\u4f1a\u5b66\u89c6\u89d2\u6765\u5206\u6790AI\u5982\u4f55\u6539\u53d8\u5de5\u4f5c\u548c\u793e\u4f1a\u3002\u901a\u8fc7\u6982\u5ff5\u6846\u67b6\u63cf\u7ed8\u4ece\u624b\u5de5\u5230\u8ba4\u77e5\u751f\u4ea7\u529b\u7684\u8f6c\u53d8\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cAI\u5982\u540c\u4eba\u7c7b\u8bed\u8a00\u4e00\u6837\uff0c\u4f5c\u4e3a\u8ba4\u77e5\u5f15\u64ce\u53d1\u6325\u4f5c\u7528\uff0c\u9884\u793a\u7740\u65b0\u7684\u751f\u4ea7\u529b\u8303\u5f0f\u3002\u5b83\u8981\u6c42\u6211\u4eec\u91cd\u65b0\u601d\u8003\u6280\u80fd\u3001\u7ec4\u7ec7\u7ed3\u6784\u548c\u653f\u7b56\u3002", "conclusion": "\u7ed3\u8bba\u662f\u4eba\u5de5\u667a\u80fd\u7684\u6f5c\u529b\u5728\u4e8e\u8865\u5145\u4eba\u7c7b\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u6807\u5fd7\u7740\u751f\u4ea7\u529b\u6f14\u53d8\u7684\u65b0\u7bc7\u7ae0\u3002"}}
{"id": "2506.10112", "pdf": "https://arxiv.org/pdf/2506.10112", "abs": "https://arxiv.org/abs/2506.10112", "authors": ["Nadav Torem", "Tamar Sde-Chen", "Yoav Y. Schechner"], "title": "NnD: Diffusion-based Generation of Physically-Nonnegative Objects", "categories": ["cs.LG"], "comment": null, "summary": "Most natural objects have inherent complexity and variability. While some\nsimple objects can be modeled from first principles, many real-world phenomena,\nsuch as cloud formation, require computationally expensive simulations that\nlimit scalability. This work focuses on a class of physically meaningful,\nnonnegative objects that are computationally tractable but costly to simulate.\nTo dramatically reduce computational costs, we propose nonnegative diffusion\n(NnD). This is a learned generative model using score based diffusion. It\nadapts annealed Langevin dynamics to enforce, by design, non-negativity\nthroughout iterative scene generation and analysis (inference). NnD trains on\nhigh-quality physically simulated objects. Once trained, it can be used for\ngeneration and inference. We demonstrate generation of 3D volumetric clouds,\ncomprising inherently nonnegative microphysical fields. Our generated clouds\nare consistent with cloud physics trends. They are effectively not\ndistinguished as non-physical by expert perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u8d1f\u6269\u6563\u6a21\u578b(NnD)\uff0c\u5b83\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u6765\u663e\u8457\u964d\u4f4e\u7269\u7406\u4e0a\u6709\u610f\u4e49\u4e14\u975e\u8d1f\u7684\u5bf9\u8c61\u7684\u8ba1\u7b97\u6210\u672c\u3002NnD\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u4e0e\u4e91\u7269\u7406\u8d8b\u52bf\u4e00\u81f4\u7684\u4e09\u7ef4\u4f53\u79ef\u4e91\u3002", "motivation": "\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684\u73b0\u8c61\uff08\u5982\u4e91\u5f62\u6210\uff09\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u6a21\u62df\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u4e3a\u4e86\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u975e\u8d1f\u6269\u6563(NnD)\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u5e76\u9002\u5e94\u9000\u706b\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u4ee5\u4fdd\u8bc1\u8fed\u4ee3\u573a\u666f\u751f\u6210\u548c\u5206\u6790\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u975e\u8d1f\u6027\u3002NnD\u5728\u9ad8\u8d28\u91cf\u7684\u7269\u7406\u6a21\u62df\u5bf9\u8c61\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u4e09\u7ef4\u4f53\u79ef\u4e91\uff0c\u8fd9\u4e9b\u4e91\u7531\u5185\u5728\u7684\u975e\u8d1f\u5fae\u7269\u7406\u573a\u7ec4\u6210\uff0c\u5e76\u4e14\u7b26\u5408\u4e91\u7269\u7406\u5b66\u7684\u8d8b\u52bf\u3002\u4e13\u5bb6\u611f\u77e5\u65e0\u6cd5\u5c06\u5176\u533a\u5206\u6210\u975e\u7269\u7406\u73b0\u8c61\u3002", "conclusion": "NnD\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u975e\u8d1f\u5bf9\u8c61\uff0c\u540c\u65f6\u5927\u5927\u964d\u4f4e\u4e86\u6240\u9700\u7684\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.10899", "pdf": "https://arxiv.org/pdf/2506.10899", "abs": "https://arxiv.org/abs/2506.10899", "authors": ["Dimitri Meunier", "Antoine Moulin", "Jakub Wornbard", "Vladimir R. Kostic", "Arthur Gretton"], "title": "Demystifying Spectral Feature Learning for Instrumental Variable Regression", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "We address the problem of causal effect estimation in the presence of hidden\nconfounders, using nonparametric instrumental variable (IV) regression. A\nleading strategy employs spectral features - that is, learned features spanning\nthe top eigensubspaces of the operator linking treatments to instruments. We\nderive a generalization error bound for a two-stage least squares estimator\nbased on spectral features, and gain insights into the method's performance and\nfailure modes. We show that performance depends on two key factors, leading to\na clear taxonomy of outcomes. In a good scenario, the approach is optimal. This\noccurs with strong spectral alignment, meaning the structural function is\nwell-represented by the top eigenfunctions of the conditional operator, coupled\nwith this operator's slow eigenvalue decay, indicating a strong instrument.\nPerformance degrades in a bad scenario: spectral alignment remains strong, but\nrapid eigenvalue decay (indicating a weaker instrument) demands significantly\nmore samples for effective feature learning. Finally, in the ugly scenario,\nweak spectral alignment causes the method to fail, regardless of the\neigenvalues' characteristics. Our synthetic experiments empirically validate\nthis taxonomy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5b58\u5728\u9690\u85cf\u6df7\u6dc6\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u975e\u53c2\u6570\u5de5\u5177\u53d8\u91cf\u56de\u5f52\u8fdb\u884c\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u95ee\u9898\u3002\u901a\u8fc7\u8c31\u7279\u5f81\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u7ed9\u51fa\u4e86\u57fa\u4e8e\u8c31\u7279\u5f81\u7684\u4e24\u9636\u6bb5\u6700\u5c0f\u4e8cquares\u4f30\u8ba1\u5668\u7684\u4e00\u822c\u5316\u8bef\u5dee\u754c\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u65b9\u6cd5\u7684\u8868\u73b0\u53d6\u51b3\u4e8e\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\u8c31\u5bf9\u9f50\u548c\u7b97\u5b50\u7684\u7279\u5f81\u503c\u8870\u51cf\u901f\u7387\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u4e86\u8868\u73b0\u826f\u597d\u7684\u3001\u4e00\u822c\u7684\u548c\u8f83\u5dee\u7684\u60c5\u666f\u5206\u7c7b\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5f53\u5b58\u5728\u9690\u85cf\u6df7\u6dc6\u53d8\u91cf\u65f6\u5982\u4f55\u51c6\u786e\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u975e\u53c2\u6570\u5de5\u5177\u53d8\u91cf\u56de\u5f52\uff0c\u5e76\u5229\u7528\u8c31\u7279\u5f81\uff0c\u5373\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8de8\u8d8a\u4e86\u5c06\u5904\u7406\u4e0e\u5de5\u5177\u53d8\u91cf\u8054\u7cfb\u8d77\u6765\u7684\u7b97\u5b50\u7684\u9876\u7ea7\u7279\u5f81\u5b50\u7a7a\u95f4\u3002", "result": "\u5f97\u5230\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8c31\u7279\u5f81\u7684\u4e24\u9636\u6bb5\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5668\u7684\u4e00\u822c\u5316\u8bef\u5dee\u754c\uff0c\u5e76\u4e14\u53d1\u73b0\u4e86\u65b9\u6cd5\u6027\u80fd\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\u662f\u8c31\u5bf9\u9f50\u548c\u6761\u4ef6\u7b97\u5b50\u7684\u7279\u5f81\u503c\u8870\u51cf\u901f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f3a\u8c31\u5bf9\u9f50\u548c\u7f13\u6162\u7279\u5f81\u503c\u8870\u51cf\uff08\u5f3a\u5de5\u5177\u53d8\u91cf\uff09\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff1b\u5728\u8c31\u5bf9\u9f50\u5f3a\u4f46\u7279\u5f81\u503c\u5feb\u901f\u8870\u51cf\uff08\u5f31\u5de5\u5177\u53d8\u91cf\uff09\u7684\u60c5\u51b5\u4e0b\u9700\u8981\u66f4\u591a\u6837\u672c\u624d\u80fd\u6709\u6548\u5b66\u4e60\u7279\u5f81\uff1b\u800c\u5728\u8c31\u5bf9\u9f50\u5f31\u7684\u60c5\u51b5\u4e0b\uff0c\u65e0\u8bba\u7279\u5f81\u503c\u7279\u6027\u5982\u4f55\uff0c\u65b9\u6cd5\u90fd\u4f1a\u5931\u8d25\u3002"}}
{"id": "2506.10304", "pdf": "https://arxiv.org/pdf/2506.10304", "abs": "https://arxiv.org/abs/2506.10304", "authors": ["Jasper Yao"], "title": "The Alignment Trap: Complexity Barriers", "categories": ["cs.AI", "cs.CC", "cs.CY", "cs.LG"], "comment": "29 Pages, 4 Figures", "summary": "We establish fundamental computational complexity barriers to verifying AI\nsafety as system capabilities scale. Our main results show that for AI systems\nwith expressiveness EXP$(m)$ above a critical threshold $\\tau$, safety\nverification requires exponential time and is coNP-complete. We formalize the\nCapability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI\ncapability drives societal safety requirements toward perfection, creating an\ninescapable tension with verification complexity. Through four core theorems,\nwe prove that (1) verification complexity grows exponentially with system\nexpressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the\npolicy space, (3) no finite set of alignment techniques can provide universal\ncoverage, and (4) robust safety properties form measure-zero sets for neural\nnetworks. These results characterize an \"intractability gap\" where practical\nsafety requirements fall within the region of computational intractability. We\nconclude by presenting a strategic trilemma: AI development must either\nconstrain system complexity to maintain verifiable safety, accept unverifiable\nrisks while scaling capabilities, or develop fundamentally new safety paradigms\nbeyond verification. Our work provides the first systematic\ncomplexity-theoretic analysis of AI alignment and establishes rigorous bounds\nthat any safety approach must confront. A formal verification of the core\ntheorems in Lean4 is currently in progress.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86AI\u7cfb\u7edf\u80fd\u529b\u6269\u5c55\u65f6\u9a8c\u8bc1AI\u5b89\u5168\u6027\u7684\u57fa\u672c\u8ba1\u7b97\u590d\u6742\u6027\u969c\u788d\uff0c\u8bc1\u660e\u4e86\u5f53AI\u7cfb\u7edf\u7684\u8868\u8fbe\u80fd\u529b\u8d85\u8fc7\u67d0\u4e00\u4e34\u754c\u9608\u503c\u65f6\uff0c\u5b89\u5168\u6027\u9a8c\u8bc1\u9700\u8981\u6307\u6570\u65f6\u95f4\u4e14\u662fcoNP\u5b8c\u5168\u95ee\u9898\u3002\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u5b9a\u7406\uff0c\u6587\u7ae0\u63ed\u793a\u4e86\u968f\u7740AI\u80fd\u529b\u7684\u589e\u52a0\uff0c\u793e\u4f1a\u5bf9\u5b89\u5168\u7684\u8981\u6c42\u8d8b\u5411\u4e8e\u5b8c\u7f8e\uff0c\u4e0e\u9a8c\u8bc1\u590d\u6742\u6027\u4e4b\u95f4\u5f62\u6210\u4e86\u4e0d\u53ef\u907f\u514d\u7684\u7d27\u5f20\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6218\u7565\u4e0a\u7684\u4e09\u96be\u56f0\u5883\uff1a\u9650\u5236\u7cfb\u7edf\u590d\u6742\u6027\u3001\u63a5\u53d7\u4e0d\u53ef\u9a8c\u8bc1\u7684\u98ce\u9669\u6216\u5f00\u53d1\u65b0\u7684\u5b89\u5168\u8303\u5f0f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u968f\u7740AI\u7cfb\u7edf\u80fd\u529b\u7684\u63d0\u5347\uff0c\u9a8c\u8bc1\u8fd9\u4e9b\u7cfb\u7edf\u5b89\u5168\u6027\u7684\u8ba1\u7b97\u590d\u6742\u6027\u6240\u5e26\u6765\u7684\u6311\u6218\u3002\u76ee\u7684\u662f\u8981\u7406\u89e3\u5728AI\u7cfb\u7edf\u80fd\u529b\u589e\u957f\u8fc7\u7a0b\u4e2d\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u8fd0\u884c\u6240\u9762\u4e34\u7684\u6839\u672c\u969c\u788d\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u590d\u6742\u6027\u7406\u8bba\u6765\u5206\u6790AI\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898\uff0c\u5b9a\u4e49\u4e86\u8868\u8fbe\u80fd\u529bEXP(m)\u548c\u4e34\u754c\u9608\u503c\u03c4\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7Capability-Risk Scaling (CRS)\u52a8\u6001\u5f62\u5f0f\u5316\u63cf\u8ff0\u4e86AI\u80fd\u529b\u4e0e\u793e\u4f1a\u5b89\u5168\u9700\u6c42\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u5b9a\u7406\u4ece\u7406\u8bba\u4e0a\u8bba\u8bc1\u4e86\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898\u7684\u590d\u6742\u6027\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a(1) \u9a8c\u8bc1\u590d\u6742\u5ea6\u968f\u7cfb\u7edf\u8868\u8fbe\u80fd\u529b\u5448\u6307\u6570\u589e\u957f\uff1b(2) \u5b89\u5168\u7b56\u7565\u4ec5\u5360\u7b56\u7565\u7a7a\u95f4\u7684\u4e00\u5c0f\u90e8\u5206\uff08\u6700\u591a\u4e3a$2^{-2^m}$\uff09\uff1b(3) \u4efb\u4f55\u6709\u9650\u6570\u91cf\u7684\u5bf9\u9f50\u6280\u672f\u90fd\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u8986\u76d6\uff1b(4) \u5bf9\u4e8e\u795e\u7ecf\u7f51\u7edc\u800c\u8a00\uff0c\u5f3a\u5065\u7684\u5b89\u5168\u5c5e\u6027\u6784\u6210\u4e86\u96f6\u6d4b\u96c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u6307\u51fa\u4e86\u4e00\u4e2a\u201c\u96be\u4ee5\u5904\u7406\u7684\u5dee\u8ddd\u201d\uff0c\u5373\u5b9e\u9645\u5b89\u5168\u8981\u6c42\u5904\u4e8e\u8ba1\u7b97\u4e0a\u96be\u4ee5\u5904\u7406\u7684\u533a\u57df\u5185\u3002\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u6218\u7565\u4e0a\u7684\u4e09\u96be\u56f0\u5883\uff0c\u5f3a\u8c03\u4e86AI\u53d1\u5c55\u5fc5\u987b\u9762\u5bf9\u7684\u9009\u62e9\uff1a\u8981\u4e48\u9650\u5236\u7cfb\u7edf\u590d\u6742\u5ea6\u4ee5\u4fdd\u6301\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u6027\uff0c\u8981\u4e48\u5728\u6269\u5c55\u80fd\u529b\u65f6\u63a5\u53d7\u65e0\u6cd5\u9a8c\u8bc1\u7684\u98ce\u9669\uff0c\u6216\u8005\u5f00\u53d1\u8d85\u51fa\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u7684\u65b0\u5b89\u5168\u8303\u4f8b\u3002"}}
{"id": "2506.10120", "pdf": "https://arxiv.org/pdf/2506.10120", "abs": "https://arxiv.org/abs/2506.10120", "authors": ["Maryam Khalid", "Akane Sano"], "title": "GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": null, "summary": "Graph-based Active Learning (AL) leverages the structure of graphs to\nefficiently prioritize label queries, reducing labeling costs and user burden\nin applications like health monitoring, human behavior analysis, and sensor\nnetworks. By identifying strategically positioned nodes, graph AL minimizes\ndata collection demands while maintaining model performance, making it a\nvaluable tool for dynamic environments. Despite its potential, existing graph\nAL methods are often evaluated on static graph datasets and primarily focus on\nprediction accuracy, neglecting user-centric considerations such as sampling\ndiversity, query fairness, and adaptability to dynamic settings. To bridge this\ngap, we introduce GRAIL, a novel benchmarking framework designed to evaluate\ngraph AL strategies in dynamic, real-world environments. GRAIL introduces novel\nmetrics to assess sustained effectiveness, diversity, and user burden, enabling\na comprehensive evaluation of AL methods under varying conditions. Extensive\nexperiments on datasets featuring dynamic, real-life human sensor data reveal\ntrade-offs between prediction performance and user burden, highlighting\nlimitations in existing AL strategies. GRAIL demonstrates the importance of\nbalancing node importance, query diversity, and network topology, providing an\nevaluation mechanism for graph AL solutions in dynamic environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6GRAIL\uff0c\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u56fe\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u5f15\u5165\u4e86\u65b0\u5ea6\u91cf\u6765\u8bc4\u4f30\u6301\u7eed\u6709\u6548\u6027\u3001\u591a\u6837\u6027\u548c\u7528\u6237\u8d1f\u62c5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5728\u9759\u6001\u56fe\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e14\u4e3b\u8981\u5173\u6ce8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8003\u91cf\uff0c\u4f8b\u5982\u91c7\u6837\u591a\u6837\u6027\u3001\u67e5\u8be2\u516c\u5e73\u6027\u4ee5\u53ca\u5bf9\u52a8\u6001\u73af\u5883\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGRAIL\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u65e8\u5728\u52a8\u6001\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\u56fe\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u65b0\u9896\u7684\u6307\u6807\u6765\u8861\u91cf\u6301\u7eed\u6709\u6548\u6027\u3001\u591a\u6837\u6027\u548c\u7528\u6237\u8d1f\u62c5\u3002", "result": "\u901a\u8fc7\u5728\u5177\u6709\u52a8\u6001\u771f\u5b9e\u4eba\u7c7b\u4f20\u611f\u5668\u6570\u636e\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u9884\u6d4b\u6027\u80fd\u4e0e\u7528\u6237\u8d1f\u62c5\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5f3a\u8c03\u4e86\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "conclusion": "GRAIL\u8bc1\u660e\u4e86\u5e73\u8861\u8282\u70b9\u91cd\u8981\u6027\u3001\u67e5\u8be2\u591a\u6837\u6027\u548c\u7f51\u7edc\u62d3\u6251\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u56fe\u4e3b\u52a8\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bc4\u4f30\u673a\u5236\u3002"}}
{"id": "2506.10908", "pdf": "https://arxiv.org/pdf/2506.10908", "abs": "https://arxiv.org/abs/2506.10908", "authors": ["Emmanuel J. Cand\u00e8s", "Andrew Ilyas", "Tijana Zrnic"], "title": "Probably Approximately Correct Labels", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \"expert\" labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684AI\u9884\u6d4b\u6765\u8865\u5145\u4e13\u5bb6\u6807\u7b7e\uff0c\u4ece\u800c\u66f4\u7ecf\u6d4e\u5730\u6784\u5efa\u5e26\u6709\u9ad8\u8d28\u91cf\u6807\u7b7e\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u83b7\u53d6\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u96c6\u901a\u5e38\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5927\u91cf\u7684\u4eba\u5de5\u6ce8\u91ca\u6216\u6602\u8d35\u7684\u5b9e\u9a8c\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u7684AI\u9884\u6d4b\u548c\u4e13\u5bb6\u6807\u7b7e\u6765\u521b\u5efa\u53ef\u80fd\u662f\u8fd1\u4f3c\u6b63\u786e\u7684\u6807\u7b7e\uff0c\u5e76\u4fdd\u8bc1\u603b\u4f53\u6807\u7b7e\u9519\u8bef\u7387\u8f83\u4f4e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u3001\u56fe\u50cf\u4ee5\u53ca\u86cb\u767d\u8d28\u6298\u53e0\u5206\u6790\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u4f7f\u7528\u73b0\u4ee3AI\u6a21\u578b\u8fdb\u884c\u4e25\u683c\u800c\u6709\u6548\u7684\u6570\u636e\u96c6\u6574\u7406\u3002"}}
{"id": "2506.10326", "pdf": "https://arxiv.org/pdf/2506.10326", "abs": "https://arxiv.org/abs/2506.10326", "authors": ["Cameron Angliss", "Jiaxun Cui", "Jiaheng Hu", "Arrasy Rahman", "Peter Stone"], "title": "A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pok\u00e9mon", "categories": ["cs.AI", "cs.GT", "cs.LG", "cs.MA", "I.2.1; I.2.6"], "comment": "15 pages, 3 figures, 10 tables, submitted to NeurIPS 2025 Datasets &\n  Benchmarks Track", "summary": "Developing AI agents that can robustly adapt to dramatically different\nstrategic landscapes without retraining is a central challenge for multi-agent\nlearning. Pok\\'emon Video Game Championships (VGC) is a domain with an\nextraordinarily large space of possible team configurations of approximately\n$10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete,\ncombinatorial nature of team building in Pok\\'emon VGC causes optimal\nstrategies to shift dramatically depending on both the team being piloted and\nthe opponent's team, making generalization uniquely challenging. To advance\nresearch on this problem, we introduce VGC-Bench: a benchmark that provides\ncritical infrastructure, standardizes evaluation protocols, and supplies\nhuman-play datasets and a range of baselines - from large-language-model agents\nand behavior cloning to reinforcement learning and empirical game-theoretic\nmethods such as self-play, fictitious play, and double oracle. In the\nrestricted setting where an agent is trained and evaluated on a single-team\nconfiguration, our methods are able to win against a professional VGC\ncompetitor. We extensively evaluated all baseline methods over progressively\nlarger team sets and find that even the best-performing algorithm in the\nsingle-team setting struggles at scaling up as team size grows. Thus, policy\ngeneralization across diverse team strategies remains an open challenge for the\ncommunity. Our code is open sourced at\nhttps://github.com/cameronangliss/VGC-Bench.", "AI": {"tldr": "\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aVGC-Bench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u4e3aPok\u00e9mon\u89c6\u9891\u6e38\u620f\u9526\u6807\u8d5b\u4e2d\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u4ee5\u53ca\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5230\u5f3a\u5316\u5b66\u4e60\u7b49\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u9650\u5b9a\u4e8e\u5355\u4e2a\u961f\u4f0d\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6218\u80dc\u4e13\u4e1aVGC\u9009\u624b\uff0c\u4f46\u5f53\u56e2\u961f\u89c4\u6a21\u6269\u5927\u65f6\uff0c\u6700\u4f73\u7b97\u6cd5\u5728\u6269\u5c55\u6027\u4e0a\u9047\u5230\u56f0\u96be\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u9762\u4e34\u7684\u4e00\u4e2a\u4e2d\u5fc3\u6311\u6218\u662f\u5f00\u53d1\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u5b8c\u5168\u4e0d\u540c\u6218\u7565\u73af\u5883\u7684\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u3002Pok\u00e9mon\u89c6\u9891\u6e38\u620f\u9526\u6807\u8d5b\uff08VGC\uff09\u662f\u4e00\u4e2a\u5177\u6709\u975e\u5e38\u5927\u7684\u53ef\u80fd\u961f\u4f0d\u914d\u7f6e\u7a7a\u95f4\u7684\u9886\u57df\uff0c\u5176\u79bb\u6563\u6027\u548c\u7ec4\u5408\u6027\u4f7f\u5f97\u6700\u4f18\u7b56\u7565\u4f1a\u6839\u636e\u4e0d\u540c\u7684\u961f\u4f0d\u548c\u5bf9\u624b\u800c\u5267\u70c8\u53d8\u5316\uff0c\u56e0\u6b64\u901a\u7528\u5316\u5c24\u4e3a\u56f0\u96be\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86VGC-Bench\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u63d0\u4f9b\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u4eba\u7c7b\u73a9\u5bb6\u6570\u636e\u96c6\u4ee5\u53ca\u4e00\u7cfb\u5217\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u3001\u884c\u4e3a\u514b\u9686\u3001\u5f3a\u5316\u5b66\u4e60\u53ca\u7ecf\u9a8c\u535a\u5f08\u8bba\u65b9\u6cd5\u5982\u81ea\u5bf9\u5f08\u3001\u865a\u6784\u5bf9\u5f08\u548c\u53cc\u91cdoracle\u7b49\u3002", "result": "\u5728\u4ec5\u9650\u4e8e\u5355\u4e00\u961f\u4f0d\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51fb\u8d25\u4e13\u4e1aVGC\u7ade\u4e89\u8005\uff1b\u7136\u800c\uff0c\u5728\u9010\u6b65\u6269\u5927\u7684\u961f\u4f0d\u96c6\u5408\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5728\u5355\u961f\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u597d\u7684\u7b97\u6cd5\u4e5f\u96be\u4ee5\u968f\u7740\u961f\u4f0d\u89c4\u6a21\u7684\u589e\u957f\u800c\u826f\u597d\u6269\u5c55\u3002", "conclusion": "\u653f\u7b56\u6cdb\u5316\u81f3\u591a\u6837\u5316\u7684\u961f\u4f0d\u7b56\u7565\u4ecd\u7136\u662f\u793e\u533a\u9762\u4e34\u7684\u5f00\u653e\u6311\u6218\u3002\u4ee3\u7801\u5df2\u7ecf\u5f00\u6e90\uff0c\u53ef\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2506.10127", "pdf": "https://arxiv.org/pdf/2506.10127", "abs": "https://arxiv.org/abs/2506.10127", "authors": ["Xinyi Hu", "Aldo Pacchiano"], "title": "Meet Me at the Arm: The Cooperative Multi-Armed Bandits Problem with Shareable Arms", "categories": ["cs.LG"], "comment": null, "summary": "We study the decentralized multi-player multi-armed bandits (MMAB) problem\nunder a no-sensing setting, where each player receives only their own reward\nand obtains no information about collisions. Each arm has an unknown capacity,\nand if the number of players pulling an arm exceeds its capacity, all players\ninvolved receive zero reward. This setting generalizes the classical\nunit-capacity model and introduces new challenges in coordination and capacity\ndiscovery under severe feedback limitations. We propose A-CAPELLA (Algorithm\nfor Capacity-Aware Parallel Elimination for Learning and Allocation), a\ndecentralized algorithm that achieves logarithmic regret in this generalized\nregime. Our main contribution is a collaborative hypothesis testing protocol\nthat enables synchronized successive elimination and capacity estimation\nthrough carefully structured collision patterns. This represents a provably\nefficient learning result in decentralized no-sensing MMAB with unknown arm\ncapacities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65e0\u611f\u77e5\u8bbe\u7f6e\u4e0b\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u4eba\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA-CAPELLA\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u78b0\u649e\u6a21\u5f0f\u5b9e\u73b0\u4e86\u540c\u6b65\u8fde\u7eed\u6dd8\u6c70\u548c\u5bb9\u91cf\u4f30\u8ba1\uff0c\u4ece\u800c\u5728\u672a\u77e5\u81c2\u5bb9\u91cf\u7684\u60c5\u51b5\u4e0b\u8fbe\u6210\u4e86\u53ef\u8bc1\u660e\u7684\u6709\u6548\u5b66\u4e60\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u4e00\u4e2a\u66f4\u666e\u904d\u5316\u7684\u591a\u4eba\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\u6bcf\u4e2a\u81c2\u90fd\u6709\u672a\u77e5\u7684\u5bb9\u91cf\u9650\u5236\uff0c\u5e76\u4e14\u73a9\u5bb6\u53ea\u80fd\u83b7\u5f97\u81ea\u5df1\u7684\u5956\u52b1\u4fe1\u606f\u800c\u65e0\u6cd5\u5f97\u77e5\u662f\u5426\u4e0e\u5176\u4ed6\u73a9\u5bb6\u53d1\u751f\u78b0\u649e\u3002\u5728\u8fd9\u79cd\u53cd\u9988\u6781\u5176\u6709\u9650\u7684\u6761\u4ef6\u4e0b\uff0c\u534f\u8c03\u73a9\u5bb6\u52a8\u4f5c\u548c\u53d1\u73b0\u81c2\u7684\u5bb9\u91cf\u6210\u4e3a\u65b0\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86A-CAPELLA\uff08\u4e00\u79cd\u7528\u4e8e\u5b66\u4e60\u4e0e\u5206\u914d\u7684\u5bb9\u91cf\u611f\u77e5\u5e76\u884c\u6d88\u9664\u7b97\u6cd5\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u7b97\u6cd5\uff0c\u5b83\u4f7f\u7528\u534f\u4f5c\u5047\u8bbe\u68c0\u9a8c\u534f\u8bae\uff0c\u5141\u8bb8\u901a\u8fc7\u7279\u5b9a\u7ed3\u6784\u7684\u78b0\u649e\u6a21\u5f0f\u8fdb\u884c\u540c\u6b65\u8fde\u7eed\u6d88\u9664\u548c\u5bb9\u91cf\u4f30\u8ba1\u3002", "result": "A-CAPELLA \u7b97\u6cd5\u5728\u672a\u77e5\u81c2\u5bb9\u91cf\u7684\u53bb\u4e2d\u5fc3\u5316\u65e0\u611f\u77e5MMAB\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u6570\u7ea7\u522b\u7684\u9057\u61be\u5ea6\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7A-CAPELLA\u7b97\u6cd5\uff0c\u4f5c\u8005\u4eec\u4e3a\u53bb\u4e2d\u5fc3\u5316\u65e0\u611f\u77e5MMAB\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u672a\u77e5\u5bb9\u91cf\u9650\u5236\u7684\u573a\u666f\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u534f\u8c03\u548c\u5b66\u4e60\u3002"}}
{"id": "2506.10971", "pdf": "https://arxiv.org/pdf/2506.10971", "abs": "https://arxiv.org/abs/2506.10971", "authors": ["He Ye", "Rojas Kevin", "Tao Molei"], "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e26\u6709\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u7684\u63a9\u7801\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u5f15\u5bfc\u53cd\u5411\u52a8\u529b\u5b66\u7684\u663e\u5f0f\u89e3\uff0c\u5e76\u63ed\u793a\u4e86\u57281D\u548c2D\u4e2d\u4e0d\u540c\u7684\u884c\u4e3a\u3002\u7814\u7a76\u8868\u660e\u5f3a\u6307\u5bfc\u4f1a\u52a0\u901f\u603b\u53d8\u5dee\u7684\u53cc\u6307\u6570\u8870\u51cf\uff0c\u5e76\u4e14\u5f71\u54cd\u91c7\u6837\u5206\u5e03\u7684\u534f\u65b9\u5dee\u7ed3\u6784\u3002", "motivation": "\u4e3a\u4e86\u7cbe\u786e\u5730\u63cf\u8ff0\u6307\u5bfc\u5982\u4f55\u5f71\u54cd\u91c7\u6837\u884c\u4e3a\uff0c\u4ee5\u53ca\u7406\u89e3\u5728\u7279\u5b9a\u7c7b\u522b\u7684\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u6307\u5bfc\u5bf9\u6837\u672c\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5047\u8bbe\u6ca1\u6709\u5206\u6570\u8bef\u5dee\u4e5f\u6ca1\u6709\u79bb\u6563\u5316\u8bef\u5dee\uff0c\u7814\u7a76\u8005\u4eec\u4e3a\u5e26\u6307\u5bfc\u7684\u53cd\u5411\u52a8\u529b\u5b66\u5bfc\u51fa\u4e86\u4e00\u4e2a\u663e\u5f0f\u7684\u89e3\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u7ef4\u5ea6\u4e0b\u7684\u91c7\u6837\u884c\u4e3a\u53ca\u603b\u53d8\u5dee\u968f\u6307\u5bfc\u5f3a\u5ea6\u7684\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u5f53\u76ee\u6807\u662f\u91c7\u6837\u81ea\u7279\u5b9a\u7c7b\u522b\u65f6\uff0c\u6307\u5bfc\u589e\u5f3a\u4e86\u7c7b\u7279\u5b9a\u533a\u57df\u540c\u65f6\u6291\u5236\u4e86\u4e0e\u5176\u4ed6\u7c7b\u522b\u5171\u4eab\u7684\u533a\u57df\uff1b\u5bf9\u4e8e\u5927\u7684\u6307\u5bfc\u5f3a\u5ea6w\uff0c\u57281D\u548c2D\u60c5\u51b5\u4e0b\uff0c\u603b\u53d8\u5dee\u6cbf\u53cd\u5411\u52a8\u529b\u5b66\u7684\u8870\u51cf\u901f\u7387\u4e0ew\u5448\u53cc\u6307\u6570\u5173\u7cfb\u3002", "conclusion": "\u6307\u5bfc\u4e0d\u4ec5\u5851\u9020\u4e86\u8f93\u51fa\u5206\u5e03\uff0c\u8fd8\u63a7\u5236\u4e86\u91c7\u6837\u8f68\u8ff9\u7684\u52a8\u529b\u5b66\uff0c\u8fd9\u4e9b\u7406\u8bba\u7ed3\u679c\u5f97\u5230\u4e86\u5b9e\u9a8c\u7684\u652f\u6301\uff0c\u5c55\u793a\u4e86\u6307\u5bfc\u5bf9\u51e0\u4f55\u6548\u5e94\u53ca\u5176\u6536\u655b\u6027\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.10357", "pdf": "https://arxiv.org/pdf/2506.10357", "abs": "https://arxiv.org/abs/2506.10357", "authors": ["Zaijing Li", "Yuquan Xie", "Rui Shao", "Gongwei Chen", "Weili Guan", "Dongmei Jiang", "Liqiang Nie"], "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts", "categories": ["cs.AI"], "comment": "24 pages, 10 figures", "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOptimus-3\u7684\u591a\u529f\u80fd\u4ee3\u7406\uff0c\u5b83\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\u3001\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u591a\u6a21\u6001\u63a8\u7406\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u5982Minecraft\u4e2d\u6784\u5efa\u5177\u6709\u611f\u77e5\u3001\u8ba1\u5212\u3001\u884c\u52a8\u3001\u57fa\u7840\u548c\u53cd\u601d\u80fd\u529b\u7684\u901a\u7528\u4ee3\u7406\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOptimus-3\u5728Minecraft\u73af\u5883\u4e2d\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u4ee3\u7406\u5728\u5404\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\uff08\u5982Minecraft\uff09\u4e2d\u6784\u5efa\u5177\u5907\u611f\u77e5\u3001\u89c4\u5212\u3001\u52a8\u4f5c\u6267\u884c\u3001\u60c5\u5883\u7406\u89e3\u548c\u81ea\u6211\u53cd\u7701\u7b49\u80fd\u529b\u7684\u901a\u7528\u4ee3\u7406\u4ecd\u9762\u4e34\u4ee5\u4e0b\u6311\u6218\uff1a\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0d\u8db3\u3001\u5f02\u6784\u4efb\u52a1\u95f4\u7684\u5e72\u6270\u4ee5\u53ca\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u7684\u89c6\u89c9\u591a\u6837\u6027\u3002", "method": "1) \u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u589e\u5f3a\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u4e3a\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff1b2) \u4e3a\u4e86\u51cf\u5c11\u5f02\u6784\u4efb\u52a1\u4e4b\u95f4\u7684\u5e72\u6270\uff0c\u5f15\u5165\u4e86\u5e26\u6709\u4efb\u52a1\u7ea7\u8def\u7531\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\uff1b3) \u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u63a8\u7406\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u4ee3\u7406\u5bf9Minecraft\u4e2d\u89c6\u89c9\u591a\u6837\u6027\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cOptimus-3\u5728Minecraft\u73af\u5883\u4e2d\u8de8\u8d8a\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u901a\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ca\u73b0\u6709\u6700\u5148\u8fdb\u4ee3\u7406\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u4e00\u7cfb\u5217\u521b\u65b0\uff0c\u7814\u7a76\u8005\u4eec\u5c55\u793a\u4e86Optimus-3\uff0c\u4e00\u79cd\u4e13\u4e3aMinecraft\u8bbe\u8ba1\u7684\u901a\u7528\u4ee3\u7406\uff0c\u5b83\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5f53\u524d\u6784\u5efa\u8de8\u529f\u80fd\u4ee3\u7406\u6240\u9047\u5230\u7684\u4e3b\u8981\u96be\u9898\uff0c\u5e76\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.10133", "pdf": "https://arxiv.org/pdf/2506.10133", "abs": "https://arxiv.org/abs/2506.10133", "authors": ["Arnaud Fickinger", "Abderrahim Bendahi", "Stuart Russell"], "title": "Provable Sim-to-Real Transfer via Offline Domain Randomization", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Reinforcement-learning agents often struggle when deployed from simulation to\nthe real-world. A dominant strategy for reducing the sim-to-real gap is domain\nrandomization (DR) which trains the policy across many simulators produced by\nsampling dynamics parameters, but standard DR ignores offline data already\navailable from the real system. We study offline domain randomization (ODR),\nwhich first fits a distribution over simulator parameters to an offline\ndataset. While a growing body of empirical work reports substantial gains with\nalgorithms such as DROPO, the theoretical foundations of ODR remain largely\nunexplored. In this work, we (i) formalize ODR as a maximum-likelihood\nestimation over a parametric simulator family, (ii) prove consistency of this\nestimator under mild regularity and identifiability conditions, showing it\nconverges to the true dynamics as the dataset grows, (iii) derive gap bounds\ndemonstrating ODRs sim-to-real error is up to an O(M) factor tighter than\nuniform DR in the finite-simulator case (and analogous gains in the continuous\nsetting), and (iv) introduce E-DROPO, a new version of DROPO which adds an\nentropy bonus to prevent variance collapse, yielding broader randomization and\nmore robust zero-shot transfer in practice.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u79bb\u7ebf\u57df\u968f\u673a\u5316(ODR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u62df\u5668\u53c2\u6570\u5206\u5e03\u62df\u5408\u5230\u79bb\u7ebf\u6570\u636e\u96c6\u6765\u51cf\u5c11\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002\u4f5c\u8005\u63d0\u51fa\u4e86ODR\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u6e29\u548c\u7684\u6b63\u5219\u6027\u548c\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\u4e0b\u4f30\u8ba1\u91cf\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63a8\u5bfc\u51fa\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u8bef\u5dee\u7684\u754c\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u7b97\u6cd5E-DROPO\u4ee5\u63d0\u9ad8\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4ece\u4eff\u771f\u73af\u5883\u90e8\u7f72\u5230\u73b0\u5b9e\u4e16\u754c\u65f6\u7ecf\u5e38\u9047\u5230\u56f0\u96be\u3002\u867d\u7136\u9886\u57df\u968f\u673a\u5316(DR)\u662f\u4e00\u79cd\u51cf\u5c11\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u7684\u4e3b\u8981\u7b56\u7565\uff0c\u4f46\u5b83\u5ffd\u7565\u4e86\u5df2\u7ecf\u53ef\u4ee5\u4ece\u771f\u5b9e\u7cfb\u7edf\u4e2d\u83b7\u5f97\u7684\u79bb\u8eab\u6570\u636e\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u5229\u7528\u8fd9\u4e9b\u79bb\u8eab\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5373\u79bb\u7ebf\u9886\u57df\u968f\u673a\u5316(ODR)\uff0c\u5e76\u4e3a\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u6587\u7ae0\u9996\u5148\u5c06\u79bb\u7ebf\u9886\u57df\u968f\u673a\u5316\uff08ODR\uff09\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u53c2\u6570\u5316\u6a21\u62df\u5668\u65cf\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u95ee\u9898\u3002\u7136\u540e\uff0c\u5bf9\u8fd9\u4e2a\u4f30\u8ba1\u5668\u7684\u4e00\u81f4\u6027\u8fdb\u884c\u4e86\u8bc1\u660e\uff0c\u5728\u6e29\u548c\u7684\u6b63\u5219\u6027\u548c\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\u4e0b\u8868\u660e\u968f\u7740\u6570\u636e\u96c6\u7684\u589e\u957f\u5b83\u4f1a\u6536\u655b\u4e8e\u771f\u5b9e\u7684\u52a8\u6001\u3002\u63a5\u7740\uff0c\u5f97\u51fa\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u8bef\u5dee\u7684\u754c\u9650\uff0c\u663e\u793aODR\u76f8\u6bd4\u5747\u5300DR\u5177\u6709\u66f4\u7d27\u81f4\u7684\u8bef\u5dee\u8303\u56f4\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7248\u672cDROPO\u7b97\u6cd5\u2014\u2014E-DROPO\uff0c\u8be5\u7b97\u6cd5\u589e\u52a0\u4e86\u4e00\u4e2a\u71b5\u5956\u52b1\u4ee5\u907f\u514d\u65b9\u5dee\u5d29\u6e83\uff0c\u4ece\u800c\u5728\u5b9e\u8df5\u4e2d\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u968f\u673a\u5316\u548c\u66f4\u5f3a\u5065\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u6a21\u62df\u5668\u65f6\uff0cODR\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8bef\u5dee\u53ef\u4ee5\u6bd4\u4f20\u7edf\u7684\u5747\u5300DR\u6700\u591a\u51cf\u5c11O(M)\u500d\uff08\u5bf9\u4e8e\u8fde\u7eed\u8bbe\u7f6e\u4e5f\u6709\u7c7b\u4f3c\u7684\u4f18\u52bf\uff09\u3002\u540c\u65f6\uff0c\u6240\u63d0\u51fa\u7684E-DROPO\u7b97\u6cd5\u4e0d\u4ec5\u9632\u6b62\u4e86\u65b9\u5dee\u5d29\u6e83\uff0c\u8fd8\u5728\u5b9e\u8df5\u4e2d\u5c55\u793a\u4e86\u66f4\u5e7f\u6cdb\u7684\u968f\u673a\u5316\u4ee5\u53ca\u66f4\u52a0\u7a33\u5065\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u4e3a\u79bb\u7ebf\u57df\u968f\u673a\u5316(ODR)\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u901a\u8fc7\u65b0\u7684E-DROPO\u7b97\u6cd5\u5c55\u793a\u4e86\u5176\u5728\u51cf\u5c11\u4eff\u771f\u81f3\u5b9e\u9645\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u9519\u8bef\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u51fa\u66f4\u80fd\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u53d8\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2506.10384", "pdf": "https://arxiv.org/pdf/2506.10384", "abs": "https://arxiv.org/abs/2506.10384", "authors": ["Jim O'Connor", "Yeonghun Lee", "Gary B Parker"], "title": "NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War", "categories": ["cs.AI"], "comment": "IEEE Conference on Games 2025", "summary": "StarCraft: Brood War remains a challenging benchmark for artificial\nintelligence research, particularly in the domain of macromanagement, where\nlong-term strategic planning is required. Traditional approaches to StarCraft\nAI rely on rule-based systems or supervised deep learning, both of which face\nlimitations in adaptability and computational efficiency. In this work, we\nintroduce NeuroPAL, a neuroevolutionary framework that integrates\nNeuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning\n(PAL) to improve the efficiency of evolutionary training. By alternating\nbetween frequent, low-fidelity training and periodic, high-fidelity\nevaluations, PAL enhances the sample efficiency of NEAT, enabling agents to\ndiscover effective strategies in fewer training iterations. We evaluate\nNeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and\ncompare its performance to standard NEAT-based training. Our results show that\nPAL significantly accelerates the learning process, allowing the agent to reach\ncompetitive levels of play in approximately half the training time required by\nNEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as\nproxy barracks placement and defensive building optimization, strategies\ncommonly used by expert human players. These findings suggest that structured\nevaluation mechanisms like PAL can enhance the scalability and effectiveness of\nneuroevolution in complex real-time strategy environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aNeuroPAL\u7684\u795e\u7ecf\u8fdb\u5316\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u62d3\u6251\u7ed3\u6784\u589e\u5f3a\u7684\u795e\u7ecf\u8fdb\u5316(NEAT)\u4e0e\u95f4\u65ad\u6027\u5373\u65f6\u5b66\u4e60(PAL)\uff0c\u4ee5\u63d0\u9ad8StarCraft: Brood War\u6e38\u620f\u4e2d\u5b8f\u89c2\u7ba1\u7406AI\u8bad\u7ec3\u7684\u6548\u7387\u3002\u7ed3\u679c\u663e\u793a\uff0cNeuroPAL\u76f8\u6bd4\u6807\u51c6NEAT\u5927\u5927\u52a0\u901f\u4e86\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u4e14\u80fd\u591f\u53d1\u5c55\u51fa\u4eba\u7c7b\u73a9\u5bb6\u5e38\u7528\u7684\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u7684\u300a\u661f\u9645\u4e89\u9738\u300b\u4eba\u5de5\u667a\u80fd\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u6216\u6709\u76d1\u7763\u7684\u6df1\u5ea6\u5b66\u4e60\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u3002", "method": "\u7814\u7a76\u8005\u4eec\u5f15\u5165\u4e86NeuroPAL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u62d3\u6251\u7ed3\u6784\u589e\u5f3a\u7684\u795e\u7ecf\u8fdb\u5316\uff08NEAT\uff09\u4e0e\u95f4\u65ad\u6027\u4efb\u610f\u65f6\u95f4\u5b66\u4e60\uff08PAL\uff09\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u9891\u7e41\u7684\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u548c\u5468\u671f\u6027\u7684\u9ad8\u7cbe\u5ea6\u8bc4\u4f30\u4ea4\u66ff\u8fdb\u884c\u6765\u63d0\u5347\u6837\u672c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPAL\u663e\u8457\u52a0\u5feb\u4e86\u5b66\u4e60\u8fdb\u7a0b\uff0c\u4f7f\u5f97\u4ee3\u7406\u80fd\u591f\u5728\u5927\u7ea6\u4e00\u534a\u7684\u65f6\u95f4\u5185\u8fbe\u5230\u5177\u6709\u7ade\u4e89\u529b\u7684\u6e38\u620f\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u8fdb\u5316\u7684\u4ee3\u7406\u5c55\u73b0\u51fa\u4e86\u5982\u4ee3\u7406\u5175\u8425\u653e\u7f6e\u548c\u9632\u5fa1\u5efa\u7b51\u4f18\u5316\u7b49\u9ad8\u7ea7\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u50cfPAL\u8fd9\u6837\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u673a\u5236\u53ef\u4ee5\u63d0\u9ad8\u795e\u7ecf\u8fdb\u5316\u5728\u590d\u6742\u5b9e\u65f6\u7b56\u7565\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.10137", "pdf": "https://arxiv.org/pdf/2506.10137", "abs": "https://arxiv.org/abs/2506.10137", "authors": ["Daniel Lawson", "Adriana Hugessen", "Charlotte Cloutier", "Glen Berseth", "Khimya Khetarpal"], "title": "Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Behavioral cloning (BC) methods trained with supervised learning (SL) are an\neffective way to learn policies from human demonstrations in domains like\nrobotics. Goal-conditioning these policies enables a single generalist policy\nto capture diverse behaviors contained within an offline dataset. While\ngoal-conditioned behavior cloning (GCBC) methods can perform well on\nin-distribution training tasks, they do not necessarily generalize zero-shot to\ntasks that require conditioning on novel state-goal pairs, i.e. combinatorial\ngeneralization. In part, this limitation can be attributed to a lack of\ntemporal consistency in the state representation learned by BC; if temporally\nrelated states are encoded to similar latent representations, then the\nout-of-distribution gap for novel state-goal pairs would be reduced. Hence,\nencouraging this temporal consistency in the representation space should\nfacilitate combinatorial generalization. Successor representations, which\nencode the distribution of future states visited from the current state, nicely\nencapsulate this property. However, previous methods for learning successor\nrepresentations have relied on contrastive samples, temporal-difference (TD)\nlearning, or both. In this work, we propose a simple yet effective\nrepresentation learning objective, $\\text{BYOL-}\\gamma$ augmented GCBC, which\nis not only able to theoretically approximate the successor representation in\nthe finite MDP case without contrastive samples or TD learning, but also,\nresults in competitive empirical performance across a suite of challenging\ntasks requiring combinatorial generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u793a\u5b66\u4e60\u76ee\u6807\uff0c\u79f0\u4e3aBYOL-\u03b3\u589e\u5f3a\u7684GCBC\uff0c\u5b83\u80fd\u591f\u5728\u65e0\u9700\u5bf9\u6bd4\u6837\u672c\u6216\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u7684\u60c5\u51b5\u4e0b\u7406\u8bba\u4e0a\u8fd1\u4f3c\u540e\u7ee7\u8868\u793a\uff0c\u5e76\u5728\u9700\u8981\u7ec4\u5408\u6cdb\u5316\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u76ee\u6807\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u514b\u9686\uff08GCBC\uff09\uff0c\u867d\u7136\u80fd\u591f\u5f88\u597d\u5730\u5b8c\u6210\u8bad\u7ec3\u96c6\u5185\u7684\u4efb\u52a1\uff0c\u4f46\u5728\u9762\u5bf9\u65b0\u72b6\u6001-\u76ee\u6807\u5bf9\u7684\u4efb\u52a1\u65f6\u7f3a\u4e4f\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u90e8\u5206\u5f52\u56e0\u4e8e\u884c\u4e3a\u514b\u9686\u6240\u5b66\u7684\u72b6\u6001\u8868\u5f81\u7f3a\u4e4f\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u4fc3\u8fdb\u7ec4\u5408\u6cdb\u5316\uff0c\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u540e\u7ee7\u8868\u5f81\u6765\u7f16\u7801\u4ece\u5f53\u524d\u72b6\u6001\u51fa\u53d1\u672a\u6765\u53ef\u80fd\u8bbf\u95ee\u5230\u7684\u72b6\u6001\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u662fBYOL-\u03b3\u589e\u5f3a\u7684\u76ee\u6807\u6761\u4ef6\u884c\u4e3a\u514b\u9686\uff08GCBC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u8868\u793a\u5b66\u4e60\u76ee\u6807\uff0c\u5b83\u4e0d\u9700\u8981\u5bf9\u6bd4\u6837\u672c\u6216\u8005\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u5c31\u80fd\u5728\u6709\u9650\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u60c5\u51b5\u4e0b\u8fd1\u4f3c\u540e\u7ee7\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBYOL-\u03b3\u589e\u5f3a\u7684GCBC\u4e0d\u4ec5\u80fd\u5728\u7406\u8bba\u4e0a\u8fd1\u4f3c\u540e\u7ee7\u8868\u793a\uff0c\u800c\u4e14\u5728\u4e00\u7cfb\u5217\u8981\u6c42\u7ec4\u5408\u6cdb\u5316\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5b9e\u9645\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165BYOL-\u03b3\u589e\u5f3a\u7684\u76ee\u6807\u6761\u4ef6\u884c\u4e3a\u514b\u9686\uff0c\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u7ec4\u5408\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6848\uff0c\u4ece\u800c\u6539\u5584\u4e86\u6a21\u578b\u5bf9\u4e8e\u672a\u89c1\u72b6\u6001-\u76ee\u6807\u5bf9\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.10140", "pdf": "https://arxiv.org/pdf/2506.10140", "abs": "https://arxiv.org/abs/2506.10140", "authors": ["Andrei V. Konstantinov", "Vlada A. Efremenko", "Lev V. Utkin"], "title": "Survival Analysis as Imprecise Classification with Trainable Kernels", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Survival analysis is a fundamental tool for modeling time-to-event data in\nhealthcare, engineering, and finance, where censored observations pose\nsignificant challenges. While traditional methods like the Beran estimator\noffer nonparametric solutions, they often struggle with the complex data\nstructures and heavy censoring. This paper introduces three novel survival\nmodels, iSurvM (the imprecise Survival model based on Mean likelihood\nfunctions), iSurvQ (the imprecise Survival model based on the Quantiles of\nlikelihood functions), and iSurvJ (the imprecise Survival model based on the\nJoint learning), that combine imprecise probability theory with attention\nmechanisms to handle censored data without parametric assumptions. The first\nidea behind the models is to represent censored observations by interval-valued\nprobability distributions for each instance over time intervals between events\nmoments. The second idea is to employ the kernel-based Nadaraya-Watson\nregression with trainable attention weights for computing the imprecise\nprobability distribution over time intervals for the entire dataset. The third\nidea is to consider three decision strategies for training, which correspond to\nthe proposed three models. Experiments on synthetic and real datasets\ndemonstrate that the proposed models, especially iSurvJ, consistently\noutperform the Beran estimator from the accuracy and computational complexity\npoints of view. Codes implementing the proposed models are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u751f\u5b58\u6a21\u578biSurvM\u3001iSurvQ\u548ciSurvJ\uff0c\u5b83\u4eec\u7ed3\u5408\u4e86\u4e0d\u7cbe\u786e\u6982\u7387\u8bba\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u6765\u5904\u7406\u5220\u5931\u6570\u636e\u3002\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684Beran\u4f30\u8ba1\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982Beran\u4f30\u8ba1\u91cf\u5728\u5904\u7406\u590d\u6742\u7684\u8d44\u6599\u7ed3\u6784\u548c\u4e25\u91cd\u5220\u5931\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e0d\u9700\u8981\u53c2\u6570\u5047\u8bbe\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5220\u5931\u6570\u636e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e73\u5747\u4f3c\u7136\u51fd\u6570\u7684\u4e0d\u7cbe\u786e\u751f\u5b58\u6a21\u578biSurvM\u3001\u57fa\u4e8e\u4f3c\u7136\u51fd\u6570\u5206\u4f4d\u6570\u7684\u4e0d\u7cbe\u786e\u751f\u5b58\u6a21\u578biSurvQ\u4ee5\u53ca\u57fa\u4e8e\u8054\u5408\u5b66\u4e60\u7684\u4e0d\u7cbe\u786e\u751f\u5b58\u6a21\u578biSurvJ\uff1b\u4f7f\u7528\u6838\u57faNadaraya-Watson\u56de\u5f52\u5e76\u5e26\u6709\u53ef\u8bad\u7ec3\u6ce8\u610f\u529b\u6743\u91cd\u6765\u8ba1\u7b97\u6574\u4e2a\u6570\u636e\u96c6\u7684\u65f6\u95f4\u95f4\u9694\u4e0a\u7684\u4e0d\u7cbe\u786e\u6982\u7387\u5206\u5e03\uff1b\u8003\u8651\u4e86\u5bf9\u5e94\u4e8e\u4e09\u4e2a\u6240\u63d0\u8bae\u6a21\u578b\u7684\u4e09\u79cd\u51b3\u7b56\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u7279\u522b\u662f\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5c24\u5176\u662fiSurvJ\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8eBeran\u4f30\u8ba1\u91cf\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u751f\u5b58\u5206\u6790\u6a21\u578b\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5220\u5931\u89c2\u6d4b\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u975e\u53c2\u6570\u65b9\u6cd5\u3002"}}
{"id": "2506.10387", "pdf": "https://arxiv.org/pdf/2506.10387", "abs": "https://arxiv.org/abs/2506.10387", "authors": ["Yuquan Xie", "Zaijing Li", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Dongmei Jiang", "Liqiang Nie"], "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills", "categories": ["cs.AI"], "comment": "20 pages, 5 figures, 5 tables", "summary": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI\nagents have yielded promising outcomes. However, these agents still struggle\nwith long-horizon tasks in online environments, primarily due to insufficient\nknowledge and the inherent gap between offline and online domains. In this\npaper, inspired by how humans generalize knowledge in open-ended environments,\nwe propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of\ninsufficient knowledge. It progressively abstracts trajectories into execution\nskills, core skills, and ultimately meta-skills, providing a hierarchical\nknowledge structure for long-horizon task planning. To bridge the domain gap,\nwe propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,\nwhich efficiently leverages skills acquired in offline environments to reduce\nthe action search space during online tree exploration. Building on HMS, we\npropose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To\nvalidate the performance of Mirage-1 in real-world long-horizon scenarios, we\nconstructed a new benchmark, AndroidLH. Experimental results show that Mirage-1\noutperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld,\nMobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:\nhttps://cybertronagent.github.io/Mirage-1.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u591a\u6a21\u6001\u6280\u80fd\u6a21\u5757\uff08HMS\uff09\u548c\u6280\u80fd\u589e\u5f3a\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff08SA-MCTS\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u8de8\u5e73\u53f0\u5373\u63d2\u5373\u7528GUI\u4ee3\u7406Mirage-1\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6267\u884c\u957f\u5468\u671f\u4efb\u52a1\u65f6\u7684\u77e5\u8bc6\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u7f29\u5c0f\u79bb\u7ebf\u4e0e\u5728\u7ebf\u73af\u5883\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMirage-1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5148\u524d\u7684\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u56fe\u5f62\u7528\u6237\u754c\u9762\u4ee3\u7406\u5728\u5904\u7406\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u957f\u671f\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u77e5\u8bc6\u4e0d\u591f\u5145\u5206\u4ee5\u53ca\u79bb\u7ebf\u4e0e\u5728\u7ebf\u9886\u57df\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c42\u6b21\u5316\u7684\u591a\u6a21\u6001\u6280\u80fd\uff08HMS\uff09\u6a21\u5757\u6765\u9010\u6b65\u62bd\u8c61\u8f68\u8ff9\u4e3a\u6267\u884c\u6280\u80fd\u3001\u6838\u5fc3\u6280\u80fd\u548c\u5143\u6280\u80fd\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9488\u5bf9\u957f\u671f\u4efb\u52a1\u89c4\u5212\u7684\u5206\u5c42\u77e5\u8bc6\u7ed3\u6784\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u6280\u80fd\u589e\u5f3a\u578b\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08SA-MCTS\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u79bb\u7ebf\u73af\u5883\u4e2d\u83b7\u5f97\u7684\u6280\u80fd\u6765\u51cf\u5c11\u5728\u7ebf\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u7684\u52a8\u4f5c\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6AndroidLH\uff0c\u5b9e\u9a8c\u8bc1\u660eMirage-1\u5728AndroidWorld, MobileMiniWob++, Mind2Web-Live, \u4ee5\u53caAndroidLH\u4e0a\u7684\u6027\u80fd\u5206\u522b\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8632%, 19%, 15%, \u548c79%\u3002", "conclusion": "\u63d0\u51fa\u7684HMS\u6a21\u5757\u548cSA-MCTS\u7b97\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aGUI\u4ee3\u7406\u6267\u884c\u957f\u671f\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u800cMirage-1\u4f5c\u4e3a\u4e00\u4e2a\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e0b\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.10138", "pdf": "https://arxiv.org/pdf/2506.10138", "abs": "https://arxiv.org/abs/2506.10138", "authors": ["Mohammad Taufeeque", "Aaron David Tucker", "Adam Gleave", "Adri\u00e0 Garriga-Alonso"], "title": "Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban", "categories": ["cs.LG", "cs.AI"], "comment": "33 pages, 22 figures", "summary": "We partially reverse-engineer a convolutional recurrent neural network (RNN)\ntrained to play the puzzle game Sokoban with model-free reinforcement learning.\nPrior work found that this network solves more levels with more test-time\ncompute. Our analysis reveals several mechanisms analogous to components of\nclassic bidirectional search. For each square, the RNN represents its plan in\nthe activations of channels associated with specific directions. These\nstate-action activations are analogous to a value function - their magnitudes\ndetermine when to backtrack and which plan branch survives pruning. Specialized\nkernels extend these activations (containing plan and value) forward and\nbackward to create paths, forming a transition model. The algorithm is also\nunlike classical search in some ways. State representation is not unified;\ninstead, the network considers each box separately. Each layer has its own plan\nrepresentation and value function, increasing search depth. Far from being\ninscrutable, the mechanisms leveraging test-time compute learned in this\nnetwork by model-free training can be understood in familiar terms.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u5206\u6790\u4e86\u4e00\u4e2a\u7528\u4e8e\u73a9\u63a8\u7bb1\u5b50\u6e38\u620f\u7684\u5377\u79ef\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u673a\u5236\u4e0e\u53cc\u5411\u641c\u7d22\u7ec4\u4ef6\u7c7b\u4f3c\uff0c\u5e76\u4e14\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5229\u7528\u65b9\u9762\u6709\u4e00\u4e9b\u4e0d\u540c\u4e8e\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u7684\u7279\u70b9\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u4e00\u4e2a\u901a\u8fc7\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6765\u73a9\u63a8\u7bb1\u6e38\u620f\u7684\u5377\u79ef\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u6d4b\u8bd5\u9636\u6bb5\u589e\u52a0\u8ba1\u7b97\u91cf\u80fd\u591f\u89e3\u51b3\u66f4\u591a\u5173\u5361\u7684\u539f\u56e0\u3002", "method": "\u91c7\u7528\u90e8\u5206\u9006\u5411\u5de5\u7a0b\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u8be5\u795e\u7ecf\u7f51\u7edc\u4e2d\u7c7b\u4f3c\u4e8e\u7ecf\u5178\u53cc\u5411\u641c\u7d22\u7ec4\u4ef6\u7684\u5de5\u4f5c\u673a\u5236\u3002", "result": "\u53d1\u73b0\u5bf9\u4e8e\u6bcf\u4e2a\u65b9\u683c\uff0cRNN \u5728\u4e0e\u7279\u5b9a\u65b9\u5411\u76f8\u5173\u7684\u901a\u9053\u6fc0\u6d3b\u4e2d\u8868\u793a\u8ba1\u5212\uff1b\u8fd9\u4e9b\u72b6\u6001-\u52a8\u4f5c\u6fc0\u6d3b\u7c7b\u4f3c\u4e8e\u4ef7\u503c\u51fd\u6570\uff0c\u5b83\u4eec\u7684\u5927\u5c0f\u51b3\u5b9a\u4e86\u4f55\u65f6\u56de\u6eaf\u4ee5\u53ca\u54ea\u4e2a\u8ba1\u5212\u5206\u652f\u5b58\u6d3b\u4e0b\u6765\uff1b\u4e13\u95e8\u7684\u5185\u6838\u5411\u524d\u548c\u5411\u540e\u6269\u5c55\u8fd9\u4e9b\u6fc0\u6d3b\uff08\u5305\u542b\u8ba1\u5212\u548c\u4ef7\u503c\uff09\u4ee5\u521b\u5efa\u8def\u5f84\uff0c\u5f62\u6210\u8f6c\u6362\u6a21\u578b\u3002", "conclusion": "\u8be5\u7f51\u7edc\u867d\u7136\u5728\u67d0\u4e9b\u65b9\u9762\u4e0e\u7ecf\u5178\u641c\u7d22\u4e0d\u540c\uff0c\u4f46\u5176\u901a\u8fc7\u65e0\u6a21\u578b\u8bad\u7ec3\u5b66\u5230\u7684\u673a\u5236\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u719f\u6089\u7684\u672f\u8bed\uff0c\u8fd9\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\u662f\u53ef\u4ee5\u89e3\u91ca\u7684\u3002"}}
{"id": "2506.10159", "pdf": "https://arxiv.org/pdf/2506.10159", "abs": "https://arxiv.org/abs/2506.10159", "authors": ["Minoh Jeong", "Seonho Kim", "Alfred Hero"], "title": "Probabilistic Variational Contrastive Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Deterministic embeddings learned by contrastive learning (CL) methods such as\nSimCLR and SupCon achieve state-of-the-art performance but lack a principled\nmechanism for uncertainty quantification. We propose Variational Contrastive\nLearning (VCL), a decoder-free framework that maximizes the evidence lower\nbound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction\nterm and adding a KL divergence regularizer to a uniform prior on the unit\nhypersphere. We model the approximate posterior $q_\\theta(z|x)$ as a projected\nnormal distribution, enabling the sampling of probabilistic embeddings. Our two\ninstantiations--VSimCLR and VSupCon--replace deterministic embeddings with\nsamples from $q_\\theta(z|x)$ and incorporate a normalized KL term into the\nloss. Experiments on multiple benchmarks demonstrate that VCL mitigates\ndimensional collapse, enhances mutual information with class labels, and\nmatches or outperforms deterministic baselines in classification accuracy, all\nthe while providing meaningful uncertainty estimates through the posterior\nmodel. VCL thus equips contrastive learning with a probabilistic foundation,\nserving as a new basis for contrastive approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u2014\u2014\u53d8\u5206\u5bf9\u6bd4\u5b66\u4e60\uff08VCL\uff09\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06InfoNCE\u635f\u5931\u89e3\u91ca\u4e3a\u66ff\u4ee3\u91cd\u6784\u9879\uff0c\u5e76\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u6dfb\u52a0KL\u6563\u5ea6\u6b63\u5219\u5316\u9879\u5230\u5747\u5300\u5148\u9a8c\uff0c\u6765\u6700\u5927\u5316\u8bc1\u636e\u4e0b\u754c\u3002VCL\u80fd\u591f\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u540c\u65f6\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4e0e\u786e\u5b9a\u6027\u57fa\u7ebf\u76f8\u5339\u914d\u6216\u8d85\u8d8a\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5982SimCLR\u548cSupCon\u867d\u7136\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u4e00\u79cd\u539f\u5219\u6027\u7684\u673a\u5236\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u89e3\u7801\u5668\u7684\u6846\u67b6\u2014\u2014\u53d8\u5206\u5bf9\u6bd4\u5b66\u4e60\uff08VCL\uff09\uff0c\u5b83\u901a\u8fc7\u5c06InfoNCE\u635f\u5931\u89c6\u4e3a\u4ee3\u7406\u91cd\u6784\u9879\uff0c\u5e76\u5411\u5355\u4f4d\u8d85\u7403\u4f53\u4e0a\u7684\u7edf\u4e00\u5148\u9a8c\u6dfb\u52a0KL\u6563\u5ea6\u6b63\u5219\u5316\u9879\uff0c\u6765\u6700\u5927\u5316\u8bc1\u636e\u4e0b\u754c(ELBO)\u3002\u8fd1\u4f3c\u540e\u9a8c$q_\u03b8(z|x)$\u88ab\u5efa\u6a21\u4e3a\u6295\u5f71\u6b63\u6001\u5206\u5e03\uff0c\u4ece\u800c\u53ef\u4ee5\u62bd\u6837\u6982\u7387\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVCL\u51cf\u8f7b\u4e86\u7ef4\u5ea6\u574d\u7f29\u73b0\u8c61\uff0c\u589e\u5f3a\u4e86\u4e0e\u7c7b\u522b\u6807\u7b7e\u7684\u4e92\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u5206\u7c7b\u51c6\u786e\u7387\u65b9\u9762\u4e0e\u786e\u5b9a\u6027\u57fa\u7ebf\u76f8\u5339\u914d\u6216\u4f18\u4e8e\u5b83\u4eec\u3002\u6b64\u5916\uff0cVCL\u8fd8\u63d0\u4f9b\u4e86\u901a\u8fc7\u540e\u9a8c\u6a21\u578b\u5f97\u51fa\u7684\u6709\u610f\u4e49\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "VCL\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u7387\u57fa\u7840\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.10408", "pdf": "https://arxiv.org/pdf/2506.10408", "abs": "https://arxiv.org/abs/2506.10408", "authors": ["Jintao Liang", "Gang Su", "Huifeng Lin", "You Wu", "Rui Zhao", "Ziyue Li"], "title": "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\novercome the knowledge limitations of Large Language Models (LLMs) by\nintegrating external retrieval with language generation. While early RAG\nsystems based on static pipelines have shown effectiveness in well-structured\ntasks, they struggle in real-world scenarios requiring complex reasoning,\ndynamic retrieval, and multi-modal integration. To address these challenges,\nthe field has shifted toward Reasoning Agentic RAG, a paradigm that embeds\ndecision-making and adaptive tool use directly into the retrieval process. In\nthis paper, we present a comprehensive review of Reasoning Agentic RAG methods,\ncategorizing them into two primary systems: predefined reasoning, which follows\nfixed modular pipelines to boost reasoning, and agentic reasoning, where the\nmodel autonomously orchestrates tool interaction during inference. We analyze\nrepresentative techniques under both paradigms, covering architectural design,\nreasoning strategies, and tool coordination. Finally, we discuss key research\nchallenges and propose future directions to advance the flexibility,\nrobustness, and applicability of reasoning agentic RAG systems. Our collection\nof the relevant research has been organized into a\nhttps://github.com/ByebyeMonica/Reasoning-Agentic-RAG.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u63a8\u7406\u4ee3\u7406RAG\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u9884\u5b9a\u4e49\u63a8\u7406\u548c\u4ee3\u7406\u63a8\u7406\u4e24\u5927\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u67b6\u6784\u8bbe\u8ba1\u3001\u63a8\u7406\u7b56\u7565\u53ca\u5de5\u5177\u534f\u8c03\u7b49\u65b9\u9762\u7684\u6280\u672f\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u5173\u952e\u7814\u7a76\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u4ee5\u63d0\u9ad8\u63a8\u7406\u4ee3\u7406RAG\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u57fa\u4e8e\u9759\u6001\u6d41\u6c34\u7ebf\u7684\u65e9\u671fRAG\u7cfb\u7edf\u5728\u9700\u8981\u590d\u6742\u63a8\u7406\u3001\u52a8\u6001\u68c0\u7d22\u548c\u591a\u6a21\u6001\u6574\u5408\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u9886\u57df\u8f6c\u5411\u4e86\u5c06\u51b3\u7b56\u5236\u5b9a\u548c\u81ea\u9002\u5e94\u5de5\u5177\u4f7f\u7528\u76f4\u63a5\u5d4c\u5165\u68c0\u7d22\u8fc7\u7a0b\u7684\u63a8\u7406\u4ee3\u7406RAG\u8303\u5f0f\u3002", "method": "\u5bf9Reasoning Agentic RAG\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u5c06\u5176\u5206\u7c7b\u4e3a\u9884\u5b9a\u4e49\u63a8\u7406\u548c\u4ee3\u7406\u63a8\u7406\u4e24\u79cd\u4e3b\u8981\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u4e86\u4e24\u7c7b\u8303\u5f0f\u4e0b\u7684\u4ee3\u8868\u6027\u6280\u672f\uff0c\u5305\u62ec\u67b6\u6784\u8bbe\u8ba1\u3001\u63a8\u7406\u7b56\u7565\u548c\u5de5\u5177\u534f\u8c03\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u4e8eReasoning Agentic RAG\u9886\u57df\u7684\u6df1\u5165\u7406\u89e3\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u5b58\u5728\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u5bf9\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u63d0\u51fa\u4e86\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\u548c\u5206\u6790\u63a8\u7406\u4ee3\u7406RAG\u7684\u65b9\u6cd5\uff0c\u4e3a\u63a8\u8fdb\u8be5\u9886\u57df\u5185\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u548c\u672a\u6765\u7684\u63a2\u7d22\u8def\u5f84\u3002"}}
{"id": "2506.10177", "pdf": "https://arxiv.org/pdf/2506.10177", "abs": "https://arxiv.org/abs/2506.10177", "authors": ["Defang Chen", "Zhenyu Zhou", "Can Wang", "Siwei Lyu"], "title": "Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "50 pages. The short version appeared in ICML 2024. arXiv admin note:\n  substantial text overlap with arXiv:2405.11326", "summary": "Diffusion-based generative models employ stochastic differential equations\n(SDEs) and their equivalent probability flow ordinary differential equations\n(ODEs) to establish a smooth transformation between complex high-dimensional\ndata distributions and tractable prior distributions. In this paper, we reveal\na striking geometric regularity in the deterministic sampling dynamics: each\nsimulated sampling trajectory lies within an extremely low-dimensional\nsubspace, and all trajectories exhibit an almost identical ''boomerang'' shape,\nregardless of the model architecture, applied conditions, or generated content.\nWe characterize several intriguing properties of these trajectories,\nparticularly under closed-form solutions based on kernel-estimated data\nmodeling. We also demonstrate a practical application of the discovered\ntrajectory regularity by proposing a dynamic programming-based scheme to better\nalign the sampling time schedule with the underlying trajectory structure. This\nsimple strategy requires minimal modification to existing ODE-based numerical\nsolvers, incurs negligible computational overhead, and achieves superior image\ngeneration performance, especially in regions with only $5 \\sim 10$ function\nevaluations.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u4e2d\u786e\u5b9a\u6027\u91c7\u6837\u52a8\u6001\u7684\u51e0\u4f55\u89c4\u5f8b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u5bf9\u9f50\u91c7\u6837\u65f6\u95f4\u8ba1\u5212\u4e0e\u8f68\u8ff9\u7ed3\u6784\uff0c\u4ece\u800c\u4ee5\u6700\u5c0f\u7684\u4fee\u6539\u548c\u8ba1\u7b97\u5f00\u9500\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u8005\u4eec\u89c2\u5bdf\u5230\u5728\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u5c3d\u7ba1\u6a21\u578b\u67b6\u6784\u3001\u5e94\u7528\u6761\u4ef6\u6216\u751f\u6210\u5185\u5bb9\u4e0d\u540c\uff0c\u6a21\u62df\u7684\u6bcf\u4e2a\u91c7\u6837\u8f68\u8ff9\u90fd\u4f4d\u4e8e\u4e00\u4e2a\u6781\u4f4e\u7ef4\u7684\u5b50\u7a7a\u95f4\u5185\uff0c\u5e76\u4e14\u6240\u6709\u8f68\u8ff9\u90fd\u8868\u73b0\u51fa\u51e0\u4e4e\u76f8\u540c\u7684\u201c\u56de\u529b\u9556\u201d\u5f62\u72b6\u3002", "method": "\u901a\u8fc7\u5bf9\u57fa\u4e8eSDEs\u548cODEs\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5206\u6790\uff0c\u7814\u7a76\u8005\u4eec\u53d1\u73b0\u4e86\u91c7\u6837\u8f68\u8ff9\u7684\u4e00\u4e9b\u6709\u8da3\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u7684\u65b9\u6848\u6765\u6539\u8fdb\u91c7\u6837\u65f6\u95f4\u8868\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b80\u5355\u7b56\u7565\u4ec5\u9700\u5bf9\u73b0\u6709\u7684\u57fa\u4e8eODE\u7684\u6570\u503c\u6c42\u89e3\u5668\u505a\u5fae\u5c0f\u6539\u52a8\uff0c\u4e0d\u4f1a\u5e26\u6765\u660e\u663e\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5e76\u4e14\u5c24\u5176\u5728\u4ec5\u67095~10\u6b21\u51fd\u6570\u8bc4\u4f30\u7684\u533a\u57df\u4e2d\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u56fe\u50cf\u751f\u6210\u8868\u73b0\u3002", "conclusion": "\u53d1\u73b0\u7684\u8f68\u8ff9\u89c4\u5f8b\u6027\u4e0d\u4ec5\u52a0\u6df1\u4e86\u5bf9\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7406\u89e3\uff0c\u8fd8\u4e3a\u63d0\u9ad8\u8fd9\u7c7b\u6a21\u578b\u6548\u7387\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2506.10420", "pdf": "https://arxiv.org/pdf/2506.10420", "abs": "https://arxiv.org/abs/2506.10420", "authors": ["Boris Sedlak", "Alireza Furutanpey", "Zihang Wang", "V\u00edctor Casamayor Pujol", "Schahram Dustdar"], "title": "Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.LG"], "comment": null, "summary": "Edge computing breaks with traditional autoscaling due to strict resource\nconstraints, thus, motivating more flexible scaling behaviors using multiple\nelasticity dimensions. This work introduces an agent-based autoscaling\nframework that dynamically adjusts both hardware resources and internal service\nconfigurations to maximize requirements fulfillment in constrained\nenvironments. We compare four types of scaling agents: Active Inference, Deep Q\nNetwork, Analysis of Structural Knowledge, and Deep Active Inference, using two\nreal-world processing services running in parallel: YOLOv8 for visual\nrecognition and OpenCV for QR code detection. Results show all agents achieve\nacceptable SLO performance with varying convergence patterns. While the Deep Q\nNetwork benefits from pre-training, the structural analysis converges quickly,\nand the deep active inference agent combines theoretical foundations with\npractical scalability advantages. Our findings provide evidence for the\nviability of multi-dimensional agent-based autoscaling for edge environments\nand encourage future work in this research direction.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u81ea\u52a8\u6269\u5c55\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u53d7\u9650\u73af\u5883\u4e2d\u52a8\u6001\u8c03\u6574\u786c\u4ef6\u8d44\u6e90\u548c\u5185\u90e8\u670d\u52a1\u914d\u7f6e\uff0c\u4ee5\u6700\u5927\u5316\u9700\u6c42\u6ee1\u8db3\u5ea6\u3002\u901a\u8fc7\u6bd4\u8f83\u56db\u79cd\u7c7b\u578b\u7684\u6269\u5c55\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528YOLOv8\u548cOpenCV\u4e24\u4e2a\u73b0\u5b9e\u4e16\u754c\u5904\u7406\u670d\u52a1\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u6709\u4ee3\u7406\u90fd\u8fbe\u5230\u4e86\u53ef\u63a5\u53d7\u7684\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u6027\u80fd\uff0c\u4f46\u6536\u655b\u6a21\u5f0f\u4e0d\u540c\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u7531\u4e8e\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\u800c\u4e0e\u4f20\u7edf\u7684\u81ea\u52a8\u6269\u5c55\u65b9\u5f0f\u4e0d\u76f8\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u52a0\u7075\u6d3b\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u91c7\u7528\u591a\u4e2a\u5f39\u6027\u7ef4\u5ea6\u6765\u5e94\u5bf9\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u81ea\u52a8\u6269\u5c55\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u52a8\u6001\u5730\u540c\u65f6\u8c03\u6574\u786c\u4ef6\u8d44\u6e90\u548c\u5185\u90e8\u670d\u52a1\u914d\u7f6e\u3002\u7814\u7a76\u4e2d\u6bd4\u8f83\u4e86\u56db\u79cd\u7c7b\u578b\u7684\u6269\u5c55\u4ee3\u7406\uff1a\u4e3b\u52a8\u63a8\u7406\u3001\u6df1\u5ea6Q\u7f51\u7edc\u3001\u7ed3\u6784\u77e5\u8bc6\u5206\u6790\u548c\u6df1\u5ea6\u4e3b\u52a8\u63a8\u7406\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u4e24\u4e2a\u5e76\u884c\u8fd0\u884c\u7684\u5b9e\u9645\u5904\u7406\u670d\u52a1\u2014\u2014\u7528\u4e8e\u89c6\u89c9\u8bc6\u522b\u7684YOLOv8\u548c\u7528\u4e8e\u4e8c\u7ef4\u7801\u68c0\u6d4b\u7684OpenCV\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6240\u6709\u4ee3\u7406\u90fd\u80fd\u8fbe\u5230\u53ef\u63a5\u53d7\u7684\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u6027\u80fd\uff0c\u4f46\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6536\u655b\u6a21\u5f0f\u3002\u6df1\u5ea6Q\u7f51\u7edc\u5f97\u76ca\u4e8e\u9884\u8bad\u7ec3\uff0c\u7ed3\u6784\u5206\u6790\u5219\u5feb\u901f\u6536\u655b\uff0c\u800c\u6df1\u5ea6\u4e3b\u52a8\u63a8\u7406\u4ee3\u7406\u7ed3\u5408\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u6269\u5c55\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u591a\u7ef4\u57fa\u4e8e\u4ee3\u7406\u7684\u8fb9\u7f18\u73af\u5883\u81ea\u52a8\u6269\u5c55\u7684\u53ef\u884c\u6027\u63d0\u4f9b\u4e86\u8bc1\u636e\uff0c\u5e76\u9f13\u52b1\u5728\u8fd9\u4e2a\u7814\u7a76\u65b9\u5411\u4e0a\u8fdb\u4e00\u6b65\u5f00\u5c55\u5de5\u4f5c\u3002"}}
{"id": "2506.10144", "pdf": "https://arxiv.org/pdf/2506.10144", "abs": "https://arxiv.org/abs/2506.10144", "authors": ["Yaowen Zhang", "Libera Fresiello", "Peter H. Veltink", "Dirk W. Donker", "Ying Wang"], "title": "Physiological-Model-Based Neural Network for Heart Rate Estimation during Daily Physical Activities", "categories": ["cs.LG", "physics.med-ph"], "comment": null, "summary": "Heart failure (HF) poses a significant global health challenge, with early\ndetection offering opportunities for improved outcomes. Abnormalities in heart\nrate (HR), particularly during daily activities, may serve as early indicators\nof HF risk. However, existing HR monitoring tools for HF detection are limited\nby their reliability on population-based averages. The estimation of\nindividualized HR serves as a dynamic digital twin, enabling precise tracking\nof cardiac health biomarkers. Current HR estimation methods, categorized into\nphysiologically-driven and purely data-driven models, struggle with efficiency\nand interpretability. This study introduces a novel physiological-model-based\nneural network (PMB-NN) framework for HR estimation based on oxygen uptake\n(VO2) data during daily physical activities. The framework was trained and\ntested on individual datasets from 12 participants engaged in activities\nincluding resting, cycling, and running. By embedding physiological\nconstraints, which were derived from our proposed simplified human movement\nphysiological model (PM), into the neural network training process, the PMB-NN\nmodel adheres to human physiological principles while achieving high estimation\naccuracy, with a median R$^2$ score of 0.8 and an RMSE of 8.3 bpm. Comparative\nstatistical analysis demonstrates that the PMB-NN achieves performance on par\nwith the benchmark neural network model while significantly outperforming\ntraditional physiological model (p=0.002). In addition, our PMB-NN is adept at\nidentifying personalized parameters of the PM, enabling the PM to generate\nreasonable HR estimation. The proposed framework with a precise VO2 estimation\nsystem derived from body movements enables the future possibilities of\npersonalized and real-time cardiac monitoring during daily life physical\nactivities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7406\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc(PMB-NN)\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u65e5\u5e38\u4f53\u529b\u6d3b\u52a8\u4e2d\u7684\u6444\u6c27\u91cf(VO2)\u6570\u636e\u4f30\u8ba1\u5fc3\u7387(HR)\u3002\u8be5\u6846\u67b6\u572812\u540d\u53c2\u4e0e\u8005\u7684\u4e2a\u4f53\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5c06\u751f\u7406\u7ea6\u675f\u5d4c\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cPMB-NN\u6a21\u578b\u9075\u5faa\u4e86\u4eba\u4f53\u751f\u7406\u539f\u5219\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5fc3\u7387\u4f30\u8ba1\u3002\u4e0e\u57fa\u51c6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u76f8\u6bd4\uff0cPMB-NN\u7684\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u751f\u7406\u6a21\u578b\u3002\u6b64\u5916\uff0cPMB-NN\u80fd\u591f\u8bc6\u522bPM\u7684\u4e2a\u6027\u5316\u53c2\u6570\uff0c\u4f7f\u5f97PM\u53ef\u4ee5\u751f\u6210\u5408\u7406\u7684\u5fc3\u7387\u4f30\u8ba1\u3002", "motivation": "\u5fc3\u810f\u8870\u7aed\uff08HF\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5168\u7403\u5065\u5eb7\u6311\u6218\uff0c\u65e9\u671f\u68c0\u6d4b\u80fd\u6539\u5584\u9884\u540e\u3002\u5fc3\u7387\u5f02\u5e38\u53ef\u80fd\u662fHF\u98ce\u9669\u7684\u65e9\u671f\u6307\u6807\uff0c\u4f46\u73b0\u6709\u7684HF\u68c0\u6d4b\u5de5\u5177\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u4eba\u7fa4\u7684\u5e73\u5747\u503c\uff0c\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u3002\u4e3a\u4e86\u66f4\u7cbe\u786e\u5730\u8ffd\u8e2a\u5fc3\u810f\u5065\u5eb7\u751f\u7269\u6807\u5fd7\u7269\uff0c\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u4e2a\u4f53\u5316\u7684\u5fc3\u7387\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5b83\u65e2\u9ad8\u6548\u53c8\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7406\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\uff08PMB-NN\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u65e5\u5e38\u4f53\u529b\u6d3b\u52a8\u671f\u95f4\u7684\u6444\u6c27\u91cf\uff08VO2\uff09\u6570\u636e\u6765\u4f30\u8ba1\u5fc3\u7387\u3002\u8be5\u6846\u67b6\u5728\u8fdb\u884c\u4f11\u606f\u3001\u9a91\u884c\u548c\u8dd1\u6b65\u7b49\u6d3b\u52a8\u768412\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u901a\u8fc7\u5c06\u4ece\u7b80\u5316\u7684\u4eba\u4f53\u8fd0\u52a8\u751f\u7406\u6a21\u578b\u4e2d\u5f97\u51fa\u7684\u751f\u7406\u7ea6\u675f\u5d4c\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cPMB-NN\u6a21\u578b\u80fd\u591f\u5728\u9075\u5faa\u4eba\u4f53\u751f\u7406\u539f\u5219\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u4f30\u6d4b\u51c6\u786e\u6027\u3002", "result": "PMB-NN\u6a21\u578b\u8fbe\u5230\u4e86\u9ad8\u7684\u4f30\u7b97\u51c6\u786e\u5ea6\uff0c\u4e2d\u4f4dR^2\u5206\u6570\u4e3a0.8\uff0cRMSE\u4e3a8.3 bpm\u3002\u7edf\u8ba1\u6bd4\u8f83\u5206\u6790\u8868\u660e\uff0cPMB-NN\u6a21\u578b\u6027\u80fd\u4e0e\u57fa\u51c6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u751f\u7406\u6a21\u578b(p=0.002)\u3002\u53e6\u5916\uff0cPMB-NN\u8fd8\u64c5\u957f\u8bc6\u522bPM\u7684\u4e2a\u6027\u5316\u53c2\u6570\uff0c\u8fd9\u4f7fPM\u80fd\u591f\u4ea7\u751f\u5408\u7406\u7684\u5fc3\u7387\u4f30\u8ba1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PMB-NN\u6846\u67b6\u7ed3\u5408\u4e86\u751f\u7406\u6a21\u578b\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u52bf\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5fc3\u7387\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u800c\u4e14\u4fdd\u6301\u4e86\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002\u8fd9\u79cd\u6846\u67b6\u4e3a\u672a\u6765\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4e2d\u4e2a\u6027\u5316\u548c\u5b9e\u65f6\u5fc3\u810f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.10259", "pdf": "https://arxiv.org/pdf/2506.10259", "abs": "https://arxiv.org/abs/2506.10259", "authors": ["Atsutoshi Kumagai", "Tomoharu Iwata", "Taishi Nishiyama", "Yasutoshi Ida", "Yasuhiro Fujiwara"], "title": "Meta-learning Representations for Learning from Multiple Annotators", "categories": ["cs.LG", "stat.ML"], "comment": "24 pages", "summary": "We propose a meta-learning method for learning from multiple noisy\nannotators. In many applications such as crowdsourcing services, labels for\nsupervised learning are given by multiple annotators. Since the annotators have\ndifferent skills or biases, given labels can be noisy. To learn accurate\nclassifiers, existing methods require many noisy annotated data. However,\nsufficient data might be unavailable in practice. To overcome the lack of data,\nthe proposed method uses labeled data obtained in different but related tasks.\nThe proposed method embeds each example in tasks to a latent space by using a\nneural network and constructs a probabilistic model for learning a\ntask-specific classifier while estimating annotators' abilities on the latent\nspace. This neural network is meta-learned to improve the expected test\nclassification performance when the classifier is adapted to a given small\namount of annotated data. This classifier adaptation is performed by maximizing\nthe posterior probability via the expectation-maximization (EM) algorithm.\nSince each step in the EM algorithm is easily computed as a closed-form and is\ndifferentiable, the proposed method can efficiently backpropagate the loss\nthrough the EM algorithm to meta-learn the neural network. We show the\neffectiveness of our method with real-world datasets with synthetic noise and\nreal-world crowdsourcing datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u591a\u4e2a\u566a\u58f0\u6807\u6ce8\u8005\u4e2d\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5c06\u6bcf\u4e2a\u4efb\u52a1\u793a\u4f8b\u5d4c\u5165\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u6784\u5efa\u6982\u7387\u6a21\u578b\u4ee5\u5728\u4f30\u8ba1\u6807\u6ce8\u8005\u80fd\u529b\u7684\u540c\u65f6\u5b66\u4e60\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5206\u7c7b\u5668\u3002\u795e\u7ecf\u7f51\u7edc\u88ab\u5143\u5b66\u4e60\u4ee5\u63d0\u9ad8\u5f53\u5206\u7c7b\u5668\u9002\u5e94\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u65f6\u7684\u9884\u671f\u6d4b\u8bd5\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u4f17\u5305\u670d\u52a1\u7b49\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u76d1\u7763\u5b66\u4e60\u7684\u6807\u7b7e\u7531\u591a\u4e2a\u6807\u6ce8\u8005\u63d0\u4f9b\u3002\u7531\u4e8e\u6807\u6ce8\u8005\u5177\u6709\u4e0d\u540c\u7684\u6280\u80fd\u6216\u504f\u89c1\uff0c\u7ed9\u51fa\u7684\u6807\u7b7e\u53ef\u80fd\u662f\u6709\u566a\u58f0\u7684\u3002\u4e3a\u4e86\u5b66\u4e60\u51c6\u786e\u7684\u5206\u7c7b\u5668\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u566a\u58f0\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\u8db3\u591f\u7684\u6570\u636e\u3002", "method": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528\u5728\u4e0d\u540c\u4f46\u76f8\u5173\u4efb\u52a1\u4e2d\u83b7\u5f97\u7684\u6807\u6ce8\u6570\u636e\u3002\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5c06\u6bcf\u4e2a\u4efb\u52a1\u4e2d\u7684\u793a\u4f8b\u5d4c\u5165\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u6982\u7387\u6a21\u578b\u6765\u5b66\u4e60\u7279\u5b9a\u4efb\u52a1\u7684\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u4f30\u8ba1\u6807\u6ce8\u8005\u5728\u8fd9\u4e2a\u6f5c\u5728\u7a7a\u95f4\u4e0a\u7684\u80fd\u529b\u3002\u795e\u7ecf\u7f51\u7edc\u88ab\u5143\u5b66\u4e60\u4ee5\u6539\u8fdb\u5f53\u5206\u7c7b\u5668\u9002\u5e94\u7ed9\u5b9a\u7684\u5c0f\u91cf\u6807\u6ce8\u6570\u636e\u65f6\u7684\u671f\u671b\u6d4b\u8bd5\u5206\u7c7b\u6027\u80fd\u3002\u5206\u7c7b\u5668\u7684\u9002\u5e94\u662f\u901a\u8fc7\u6700\u5927\u5316\u540e\u9a8c\u6982\u7387\u7ecf\u7531\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u7b97\u6cd5\u5b8c\u6210\u7684\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u566a\u58f0\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u4f17\u5305\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4ece\u6709\u9650\u4e14\u542b\u566a\u58f0\u7684\u6807\u6ce8\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u76f8\u5173\u7684\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u8fc1\u79fb\u77e5\u8bc6\uff0c\u4ece\u800c\u6539\u5584\u5206\u7c7b\u5668\u7684\u8868\u73b0\u3002"}}
{"id": "2506.10481", "pdf": "https://arxiv.org/pdf/2506.10481", "abs": "https://arxiv.org/abs/2506.10481", "authors": ["Yaoming Zhu", "Junxin Wang", "Yiyang Li", "Lin Qiu", "ZongYu Wang", "Jun Xu", "Xuezhi Cao", "Yuhuai Wei", "Mingshi Wang", "Xunliang Cai", "Rong Ma"], "title": "OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics", "categories": ["cs.AI"], "comment": null, "summary": "As models become increasingly sophisticated, conventional algorithm\nbenchmarks are increasingly saturated, underscoring the need for more\nchallenging benchmarks to guide future improvements in algorithmic reasoning.\nThis paper introduces OIBench, a high-quality, private, and challenging\nolympiad-level informatics dataset comprising 250 carefully curated original\nproblems. We detail the construction methodology of the benchmark, ensuring a\ncomprehensive assessment across various programming paradigms and complexities,\nand we demonstrate its contamination-resistant properties via experiments. We\npropose Time/Space Completion Curves for finer-grained efficiency analysis and\nenable direct human-model comparisons through high-level participant\nevaluations. Our experiments reveal that while open-source models lag behind\nclosed-source counterparts, current SOTA models already outperform most human\nparticipants in both correctness and efficiency, while still being suboptimal\ncompared to the canonical solutions. By releasing OIBench as a fully\nopen-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we\nhope this benchmark will contribute to advancing code reasoning capabilities\nfor future LLMs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aOIBench\u7684\u65b0\u57fa\u51c6\uff0c\u5b83\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u79c1\u5bc6\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u7ea7\u522b\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u7b97\u6cd5\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u4f20\u7edf\u7684\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5\u9010\u6e10\u9971\u548c\uff0c\u8fd9\u7a81\u663e\u4e86\u9700\u8981\u66f4\u52a0\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6765\u6307\u5bfc\u672a\u6765\u7b97\u6cd5\u63a8\u7406\u7684\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b250\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u539f\u521b\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8be5\u57fa\u51c6\u7684\u6784\u5efa\u65b9\u6cd5\uff0c\u786e\u4fdd\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u5404\u79cd\u7f16\u7a0b\u8303\u5f0f\u548c\u590d\u6742\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u6297\u6c61\u67d3\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65f6\u95f4/\u7a7a\u95f4\u5b8c\u6210\u66f2\u7ebf\u6765\u8fdb\u884c\u66f4\u7ec6\u81f4\u7684\u6548\u7387\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f00\u6e90\u6a21\u578b\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u4f46\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u6b63\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5df2\u7ecf\u8d85\u8fc7\u4e86\u5927\u591a\u6570\u4eba\uff0c\u5c3d\u7ba1\u4e0e\u6807\u51c6\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u5b8c\u5168\u5f00\u6e90\u7684OIBench\uff08https://huggingface.co/datasets/AGI-Eval/OIBench\uff09\uff0c\u4f5c\u8005\u5e0c\u671b\u8fd9\u4e2a\u57fa\u51c6\u6709\u52a9\u4e8e\u63a8\u8fdb\u672a\u6765\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.10146", "pdf": "https://arxiv.org/pdf/2506.10146", "abs": "https://arxiv.org/abs/2506.10146", "authors": ["Tejaswi Kasarla", "Max van Spengler", "Pascal Mettes"], "title": "Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Out-of-distribution recognition forms an important and well-studied problem\nin deep learning, with the goal to filter out samples that do not belong to the\ndistribution on which a network has been trained. The conclusion of this paper\nis simple: a good hierarchical hyperbolic embedding is preferred for\ndiscriminating in- and out-of-distribution samples. We introduce Balanced\nHyperbolic Learning. We outline a hyperbolic class embedding algorithm that\njointly optimizes for hierarchical distortion and balancing between shallow and\nwide subhierarchies. We then use the class embeddings as hyperbolic prototypes\nfor classification on in-distribution data. We outline how to generalize\nexisting out-of-distribution scoring functions to operate with hyperbolic\nprototypes. Empirical evaluations across 13 datasets and 13 scoring functions\nshow that our hyperbolic embeddings outperform existing out-of-distribution\napproaches when trained on the same data with the same backbones. We also show\nthat our hyperbolic embeddings outperform other hyperbolic approaches, beat\nstate-of-the-art contrastive methods, and natively enable hierarchical\nout-of-distribution generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e73\u8861\u53cc\u66f2\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5c42\u6b21\u5931\u771f\u548c\u6d45\u5c42\u4e0e\u5bbd\u5b50\u5c42\u6b21\u4e4b\u95f4\u7684\u5e73\u8861\u6765\u5d4c\u5165\u7c7b\u522b\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u5d4c\u5165\u4f5c\u4e3a\u53cc\u66f2\u539f\u578b\u6765\u8fdb\u884c\u5206\u7c7b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u572813\u4e2a\u6570\u636e\u96c6\u548c13\u4e2a\u8bc4\u5206\u51fd\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u79bb\u7fa4\u5206\u5e03\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4e00\u4e2a\u91cd\u8981\u7684\u95ee\u9898\uff0c\u5373\u8bc6\u522b\u51fa\u4e0d\u5c5e\u4e8e\u7f51\u7edc\u8bad\u7ec3\u65f6\u6240\u7528\u5206\u5e03\u7684\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u5e73\u8861\u53cc\u66f2\u5b66\u4e60\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u53cc\u66f2\u7c7b\u5d4c\u5165\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5171\u540c\u4f18\u5316\u4e86\u5c42\u6b21\u5931\u771f\u4ee5\u53ca\u6d45\u5c42\u548c\u5bbd\u5b50\u5c42\u6b21\u95f4\u7684\u5e73\u8861\u3002\u7136\u540e\u5c06\u7c7b\u5d4c\u5165\u7528\u4f5c\u53cc\u66f2\u539f\u578b\u4ee5\u8fdb\u884c\u5206\u5e03\u5185\u6570\u636e\u7684\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u5982\u4f55\u5c06\u73b0\u6709\u7684\u79bb\u7fa4\u5206\u5e03\u8bc4\u5206\u51fd\u6570\u63a8\u5e7f\u5230\u53ef\u4ee5\u4f7f\u7528\u53cc\u66f2\u539f\u578b\u5de5\u4f5c\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u76f8\u540c\u7684\u8bad\u7ec3\u6570\u636e\u548c\u76f8\u540c\u7684\u9aa8\u5e72\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u53cc\u66f2\u5d4c\u5165\u572813\u4e2a\u6570\u636e\u96c6\u548c13\u4e2a\u8bc4\u5206\u51fd\u6570\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u79bb\u7fa4\u5206\u5e03\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5b83\u4eec\u4e5f\u4f18\u4e8e\u5176\u4ed6\u53cc\u66f2\u65b9\u6cd5\u3001\u6700\u65b0\u7684\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u81ea\u7136\u5730\u5b9e\u73b0\u5c42\u6b21\u5316\u7684\u79bb\u7fa4\u5206\u5e03\u6cdb\u5316\u3002", "conclusion": "\u826f\u597d\u7684\u5206\u5c42\u53cc\u66f2\u5d4c\u5165\u5bf9\u4e8e\u533a\u5206\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6837\u672c\u662f\u4f18\u9009\u7684\u3002"}}
{"id": "2506.10313", "pdf": "https://arxiv.org/pdf/2506.10313", "abs": "https://arxiv.org/abs/2506.10313", "authors": ["Mo\u00efse Blanchard", "Vineet Goyal"], "title": "Collaborative Min-Max Regret in Grouped Multi-Armed Bandits", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study the impact of sharing exploration in multi-armed bandits in a\ngrouped setting where a set of groups have overlapping feasible action sets\n[Baek and Farias '24]. In this grouped bandit setting, groups share reward\nobservations, and the objective is to minimize the collaborative regret,\ndefined as the maximum regret across groups. This naturally captures\napplications in which one aims to balance the exploration burden between groups\nor populations -- it is known that standard algorithms can lead to\nsignificantly imbalanced exploration cost between groups. We address this\nproblem by introducing an algorithm Col-UCB that dynamically coordinates\nexploration across groups. We show that Col-UCB achieves both optimal minimax\nand instance-dependent collaborative regret up to logarithmic factors. These\nbounds are adaptive to the structure of shared action sets between groups,\nproviding insights into when collaboration yields significant benefits over\neach group learning their best action independently.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5206\u7ec4\u573a\u666f\u4e0b\u591a\u81c2\u8001\u864e\u673a\u4e2d\u5171\u4eab\u63a2\u7d22\u7684\u5f71\u54cd\uff0c\u5f15\u5165\u4e86Col-UCB\u7b97\u6cd5\u4ee5\u52a8\u6001\u534f\u8c03\u5404\u7ec4\u4e4b\u95f4\u7684\u63a2\u7d22\uff0c\u5e76\u8bc1\u660e\u8be5\u7b97\u6cd5\u8fbe\u5230\u4e86\u6700\u4f18\u7684\u6700\u5c0f\u6700\u5927\u548c\u5b9e\u4f8b\u4f9d\u8d56\u7684\u5408\u4f5c\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u5206\u7ec4\u591a\u81c2\u8001\u864e\u673a\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u5e73\u8861\u4e0d\u540c\u7ec4\u95f4\u6216\u4eba\u7fa4\u95f4\u7684\u63a2\u7d22\u8d1f\u62c5\u95ee\u9898\uff0c\u56e0\u4e3a\u6807\u51c6\u7b97\u6cd5\u53ef\u80fd\u5bfc\u81f4\u7ec4\u95f4\u7684\u63a2\u7d22\u6210\u672c\u663e\u8457\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCol-UCB\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u5730\u8de8\u7ec4\u534f\u8c03\u63a2\u7d22\u3002", "result": "Col-UCB\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u6700\u5c0f\u6700\u5927\u548c\u5b9e\u4f8b\u4f9d\u8d56\u7684\u5408\u4f5c\u9057\u61be\u754c\uff0c\u8fd9\u4e9b\u754c\u9650\u9002\u5e94\u4e8e\u7ec4\u95f4\u5171\u4eab\u52a8\u4f5c\u96c6\u7684\u7ed3\u6784\u3002", "conclusion": "Col-UCB\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u51cf\u5c11\u5408\u4f5c\u9057\u61be\uff0c\u5e76\u4e14\u5f53\u7ec4\u4e4b\u95f4\u5b58\u5728\u5171\u4eab\u7684\u52a8\u4f5c\u96c6\u5408\u65f6\uff0c\u534f\u4f5c\u53ef\u4ee5\u6bd4\u6bcf\u4e2a\u7ec4\u72ec\u7acb\u5b66\u4e60\u6700\u4f73\u884c\u52a8\u5e26\u6765\u663e\u8457\u7684\u597d\u5904\u3002"}}
{"id": "2506.10521", "pdf": "https://arxiv.org/pdf/2506.10521", "abs": "https://arxiv.org/abs/2506.10521", "authors": ["Yuhao Zhou", "Yiheng Wang", "Xuming He", "Ruoyao Xiao", "Zhiwei Li", "Qiantai Feng", "Zijie Guo", "Yuejin Yang", "Hao Wu", "Wenxuan Huang", "Jiaqi Wei", "Dan Si", "Xiuqi Yao", "Jia Bu", "Haiwen Huang", "Tianfan Fu", "Shixiang Tang", "Ben Fei", "Dongzhan Zhou", "Fenghua Ling", "Yan Lu", "Siqi Sun", "Chenhui Li", "Guanjie Zheng", "Jiancheng Lv", "Wenlong Zhang", "Lei Bai"], "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "82 pages", "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Scientists' First Exam (SFE)\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u79d1\u5b66\u8ba4\u77e5\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u5148\u8fdb\u7684GPT-o3\u548cInternVL-3\u6a21\u578b\u5728SFE\u4e0a\u5f97\u5206\u8f83\u4f4e\uff0c\u8868\u660eMLLMs\u5728\u79d1\u5b66\u9886\u57df\u8fd8\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u53d1\u73b0\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u4fe1\u606f\u5bc6\u96c6\u578b\u79d1\u5b66\u6570\u636e\u548c\u7279\u5b9a\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u79d1\u5b66\u57fa\u51c6\u53ef\u4ee5\u8bc4\u4f30MLLM\u7684\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5bf9\u4e8e\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4ef7\u4e0d\u8db3\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6SFE\u6765\u5168\u9762\u8bc4\u4f30MLLMs\u7684\u79d1\u5b66\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86Scientists' First Exam (SFE) \u57fa\u51c6\uff0c\u5b83\u5305\u62ec830\u5bf9\u4e13\u5bb6\u9a8c\u8bc1\u7684VQA\u95ee\u9898\uff0c\u8986\u76d6\u4e09\u4e2a\u95ee\u9898\u7c7b\u578b\uff0c\u5e76\u4e14\u8de8\u4e94\u4e2a\u9ad8\u4ef7\u503c\u5b66\u79d1\u4e2d\u768466\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684GPT-o3\u548cInternVL-3\u6a21\u578b\u5728SFE\u4e0a\u7684\u5f97\u5206\u5206\u522b\u53ea\u670934.08%\u548c26.52%\uff0c\u8fd9\u8868\u660eMLLMs\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u8fd8\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "SFE\u57fa\u51c6\u4e3a\u8bc4\u4f30MLLMs\u5728\u79d1\u5b66\u4fe1\u53f7\u611f\u77e5\u3001\u79d1\u5b66\u5c5e\u6027\u7406\u89e3\u548c\u79d1\u5b66\u6bd4\u8f83\u63a8\u7406\u65b9\u9762\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e9b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e0c\u671bSFE\u80fd\u591f\u4fc3\u8fdbAI\u589e\u5f3a\u79d1\u5b66\u53d1\u73b0\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.10332", "pdf": "https://arxiv.org/pdf/2506.10332", "abs": "https://arxiv.org/abs/2506.10332", "authors": ["Aaryam Sharma"], "title": "Air in Your Neighborhood: Fine-Grained AQI Forecasting Using Mobile Sensor Data", "categories": ["cs.LG", "stat.ML"], "comment": "10 pages, 7 figures. Code available at\n  https://github.com/ASChampOmega/AQI_Forecasting.git", "summary": "Air pollution has become a significant health risk in developing countries.\nWhile governments routinely publish air-quality index (AQI) data to track\npollution, these values fail to capture the local reality, as sensors are often\nvery sparse. In this paper, we address this gap by predicting AQI in 1 km^2\nneighborhoods, using the example of AirDelhi dataset. Using Spatio-temporal\nGNNs we surpass existing works by 71.654 MSE a 79% reduction, even on unseen\ncoordinates. New insights about AQI such as the existence of strong repetitive\nshort-term patterns and changing spatial relations are also discovered. The\ncode is available on GitHub.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08Spatio-temporal GNNs\uff09\u6765\u9884\u6d4b1\u5e73\u65b9\u516c\u91cc\u8303\u56f4\u5185\u7684\u7a7a\u6c14\u8d28\u91cf\u6307\u6570(AQI)\uff0c\u4ee5AirDelhi\u6570\u636e\u96c6\u4e3a\u4f8b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5747\u65b9\u8bef\u5dee\uff0c\u5e76\u63ed\u793a\u4e86AQI\u7684\u65b0\u89c1\u89e3\u3002", "motivation": "\u5728\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff0c\u7a7a\u6c14\u6c61\u67d3\u5df2\u7ecf\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u5065\u5eb7\u98ce\u9669\u3002\u5c3d\u7ba1\u653f\u5e9c\u7ecf\u5e38\u53d1\u5e03\u7a7a\u6c14\u8d28\u91cf\u6307\u6570\uff08AQI\uff09\u6570\u636e\u6765\u8ddf\u8e2a\u6c61\u67d3\u60c5\u51b5\uff0c\u4f46\u8fd9\u4e9b\u6570\u503c\u672a\u80fd\u53cd\u6620\u5c40\u90e8\u5b9e\u9645\u60c5\u51b5\uff0c\u56e0\u4e3a\u4f20\u611f\u5668\u901a\u5e38\u975e\u5e38\u7a00\u758f\u3002", "method": "\u91c7\u7528\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08Spatio-temporal GNNs\uff09\u5bf91\u5e73\u65b9\u516c\u91cc\u8303\u56f4\u7684AQI\u8fdb\u884c\u9884\u6d4b\uff0c\u4ee5AirDelhi\u6570\u636e\u96c6\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u540e\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u5750\u6807\u4e0a\u76f8\u8f83\u4e8e\u5df2\u6709\u5de5\u4f5c\u964d\u4f4e\u4e8671.654 MSE\uff0c\u5373\u51cf\u5c11\u4e8679%\u7684\u5747\u65b9\u8bef\u5dee\uff1b\u540c\u65f6\u53d1\u73b0\u4e86\u5173\u4e8eAQI\u7684\u65b0\u6d1e\u5bdf\uff0c\u6bd4\u5982\u5b58\u5728\u5f3a\u70c8\u7684\u77ed\u671f\u91cd\u590d\u6a21\u5f0f\u548c\u53d8\u5316\u7684\u7a7a\u95f4\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8fd0\u7528\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u5c0f\u533a\u57df\u8303\u56f4\u5185\u7684AQI\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u6709\u5173AQI\u7279\u6027\u7684\u65b0\u7406\u89e3\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4ee3\u7801\u5df2\u516c\u5f00\u4e8eGitHub\u3002"}}
{"id": "2506.10527", "pdf": "https://arxiv.org/pdf/2506.10527", "abs": "https://arxiv.org/abs/2506.10527", "authors": ["Yanan Cai", "Ahmed Salem", "Besmira Nushi", "Mark Russinovich"], "title": "LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs", "categories": ["cs.AI", "cs.PF"], "comment": null, "summary": "We introduce LogiPlan, a novel benchmark designed to evaluate the\ncapabilities of large language models (LLMs) in logical planning and reasoning\nover complex relational structures. Logical relational reasoning is important\nfor applications that may rely on LLMs to generate and query structured graphs\nof relations such as network infrastructure, knowledge bases, or business\nprocess schema. Our framework allows for dynamic variation of task complexity\nby controlling the number of objects, relations, and the minimum depth of\nrelational chains, providing a fine-grained assessment of model performance\nacross difficulty levels. LogiPlan encompasses three complementary tasks: (1)\nPlan Generation, where models must construct valid directed relational graphs\nmeeting specified structural constraints; (2) Consistency Detection, testing\nmodels' ability to identify inconsistencies in relational structures; and (3)\nComparison Question, evaluating models' capacity to determine the validity of\nqueried relationships within a given graph. Additionally, we assess models'\nself-correction capabilities by prompting them to verify and refine their\ninitial solutions. We evaluate state-of-the-art models including DeepSeek R1,\nGemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,\nO3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant\nperformance gaps that correlate with model scale and architecture. Our analysis\ndemonstrates that while recent reasoning-enhanced models show promising results\non simpler instances, they struggle with more complex configurations requiring\ndeeper logical planning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aLogiPlan\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5173\u7cfb\u7ed3\u6784\u4e0a\u7684\u903b\u8f91\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u63a7\u5236\u5bf9\u8c61\u6570\u91cf\u3001\u5173\u7cfb\u6570\u91cf\u4ee5\u53ca\u5173\u7cfb\u94fe\u7684\u6700\u5c0f\u6df1\u5ea6\u6765\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u590d\u6742\u5ea6\uff0c\u5e76\u5305\u542b\u4e86\u4e09\u4e2a\u4e92\u8865\u4efb\u52a1\uff1a\u8ba1\u5212\u751f\u6210\u3001\u4e00\u81f4\u6027\u68c0\u6d4b\u548c\u6bd4\u8f83\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u6d4b\u8bd5\u4e86\u6a21\u578b\u81ea\u6211\u7ea0\u6b63\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6700\u8fd1\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5728\u8f83\u7b80\u5355\u7684\u5b9e\u4f8b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u66f4\u6df1\u5c42\u6b21\u903b\u8f91\u89c4\u5212\u7684\u590d\u6742\u914d\u7f6e\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u50cf\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u3001\u77e5\u8bc6\u5e93\u6216\u4e1a\u52a1\u6d41\u7a0b\u6a21\u5f0f\u7b49\u590d\u6742\u5173\u7cfb\u56fe\u65f6\u7684\u903b\u8f91\u5173\u7cfb\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86LogiPlan\u57fa\u51c6\uff0c\u5b83\u5141\u8bb8\u901a\u8fc7\u6539\u53d8\u5bf9\u8c61\u6570\u91cf\u3001\u5173\u7cfb\u6570\u91cf\u53ca\u5173\u7cfb\u94fe\u6df1\u5ea6\u6765\u8c03\u63a7\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u4e14\u5305\u542b\u4e09\u9879\u4e92\u8865\u4efb\u52a1\u4ee5\u5168\u9762\u8003\u5bdf\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u867d\u7136\u5f53\u524d\u5148\u8fdb\u7684\u589e\u5f3a\u578b\u63a8\u7406\u6a21\u578b\u80fd\u591f\u5728\u7b80\u5355\u60c5\u5f62\u4e0b\u53d6\u5f97\u8f83\u597d\u6210\u7ee9\uff0c\u4f46\u5f53\u9762\u5bf9\u9700\u8981\u6df1\u5c42\u903b\u8f91\u89c4\u5212\u7684\u590d\u6742\u60c5\u51b5\u65f6\uff0c\u5219\u663e\u5f97\u529b\u4e0d\u4ece\u5fc3\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u5904\u7406\u6d89\u53ca\u590d\u6742\u903b\u8f91\u89c4\u5212\u7684\u95ee\u9898\u65f6\uff0c\u5373\u4f7f\u662f\u6700\u65b0\u7684\u5148\u8fdb\u6a21\u578b\u4e5f\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u8868\u660e\u6709\u5fc5\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\u4ee5\u63d0\u9ad8\u5176\u5904\u7406\u6b64\u7c7b\u95ee\u9898\u7684\u80fd\u529b\u3002"}}
{"id": "2506.10165", "pdf": "https://arxiv.org/pdf/2506.10165", "abs": "https://arxiv.org/abs/2506.10165", "authors": ["Gilad Landau", "Miran \u00d6zdogan", "Gereon Elvers", "Francesco Mantegna", "Pratik Somaiya", "Dulhan Jayalath", "Luisa Kurth", "Teyun Kwon", "Brendan Shillingford", "Greg Farquhar", "Minqi Jiang", "Karim Jerbi", "Hamza Abdelhedi", "Yorguin Mantilla Ramos", "Caglar Gulcehre", "Mark Woolrich", "Natalie Voets", "Oiwi Parker Jones"], "title": "The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "The advance of speech decoding from non-invasive brain data holds the\npotential for profound societal impact. Among its most promising applications\nis the restoration of communication to paralysed individuals affected by speech\ndeficits such as dysarthria, without the need for high-risk surgical\ninterventions. The ultimate aim of the 2025 PNPL competition is to produce the\nconditions for an \"ImageNet moment\" or breakthrough in non-invasive neural\ndecoding, by harnessing the collective power of the machine learning community.\n  To facilitate this vision we present the largest within-subject MEG dataset\nrecorded to date (LibriBrain) together with a user-friendly Python library\n(pnpl) for easy data access and integration with deep learning frameworks. For\nthe competition we define two foundational tasks (i.e. Speech Detection and\nPhoneme Classification from brain data), complete with standardised data splits\nand evaluation metrics, illustrative benchmark models, online tutorial code, a\ncommunity discussion board, and public leaderboard for submissions. To promote\naccessibility and participation the competition features a Standard track that\nemphasises algorithmic innovation, as well as an Extended track that is\nexpected to reward larger-scale computing, accelerating progress toward a\nnon-invasive brain-computer interface for speech.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e862025 PNPL\u7ade\u8d5b\u7684\u76ee\u6807\uff0c\u5373\u901a\u8fc7\u5229\u7528\u673a\u5668\u5b66\u4e60\u793e\u533a\u7684\u529b\u91cf\u6765\u63a8\u52a8\u975e\u4fb5\u5165\u6027\u795e\u7ecf\u89e3\u7801\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u4e3a\u4e86\u5e2e\u52a9\u6709\u8a00\u8bed\u969c\u788d\u7684\u762b\u75ea\u8005\u6062\u590d\u6c9f\u901a\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u63d0\u4f9b\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u5355\u4e2a\u53d7\u8bd5\u8005MEG\u6570\u636e\u96c6LibriBrain\u548c\u4e00\u4e2a\u4fbf\u4e8e\u8bbf\u95ee\u8be5\u6570\u636e\u96c6\u7684Python\u5e93pnpl\u3002\u5b9a\u4e49\u4e86\u4e24\u4e2a\u57fa\u7840\u4efb\u52a1\uff08\u8bed\u97f3\u68c0\u6d4b\u548c\u4ece\u8111\u6570\u636e\u4e2d\u8fdb\u884c\u97f3\u7d20\u5206\u7c7b\uff09\uff0c\u5e76\u8bbe\u7f6e\u4e86\u6807\u51c6\u7684\u6570\u636e\u5206\u5272\u3001\u8bc4\u4f30\u6307\u6807\u3001\u57fa\u51c6\u6a21\u578b\u3001\u5728\u7ebf\u6559\u7a0b\u4ee3\u7801\u3001\u8ba8\u8bba\u677f\u548c\u516c\u5f00\u6392\u884c\u699c\u3002\u6bd4\u8d5b\u5206\u4e3a\u5f3a\u8c03\u7b97\u6cd5\u521b\u65b0\u7684\u6807\u51c6\u8d5b\u9053\u548c\u53ef\u80fd\u5956\u52b1\u66f4\u5927\u89c4\u6a21\u8ba1\u7b97\u7684\u6269\u5c55\u8d5b\u9053\u3002", "motivation": "\u4fc3\u8fdb\u975e\u4fb5\u5165\u6027\u8111-\u673a\u63a5\u53e3\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4ee5\u5e2e\u52a9\u56e0\u8a00\u8bed\u7f3a\u9677\u800c\u65e0\u6cd5\u6b63\u5e38\u4ea4\u6d41\u7684\u762b\u75ea\u4e2a\u4f53\u6062\u590d\u6c9f\u901a\u80fd\u529b\uff0c\u907f\u514d\u9ad8\u98ce\u9669\u5916\u79d1\u624b\u672f\u7684\u9700\u6c42\u3002", "method": "\u521b\u5efa\u4e86\u6700\u5927\u7684\u5355\u4e2a\u53d7\u8bd5\u8005MEG\u6570\u636e\u96c6LibriBrain\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684Python\u5e93pnpl\u7528\u4e8e\u6570\u636e\u8bbf\u95ee\uff1b\u5b9a\u4e49\u4e86\u4e24\u4e2a\u57fa\u7840\u4efb\u52a1\uff1a\u4ece\u8111\u6570\u636e\u4e2d\u68c0\u6d4b\u8bed\u97f3\u548c\u97f3\u7d20\u5206\u7c7b\uff1b\u63d0\u4f9b\u6807\u51c6\u5316\u6570\u636e\u5212\u5206\u3001\u8bc4\u4ef7\u6307\u6807\u3001\u793a\u4f8b\u57fa\u51c6\u6a21\u578b\u3001\u5728\u7ebf\u6559\u7a0b\u4ee3\u7801\u3001\u793e\u533a\u8ba8\u8bba\u677f\u5757\u4ee5\u53ca\u516c\u5f00\u63d0\u4ea4\u6392\u884c\u699c\u3002", "result": "\u4e3a2025 PNPL\u7ade\u8d5b\u8bbe\u7acb\u4e86\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u5927\u4efb\u52a1\u3001\u6570\u636e\u96c6\u3001\u5de5\u5177\u5e93\u3001\u57fa\u51c6\u6a21\u578b\u53ca\u5728\u7ebf\u8d44\u6e90\u7b49\uff0c\u65e8\u5728\u9f13\u52b1\u673a\u5668\u5b66\u4e60\u793e\u533a\u53c2\u4e0e\u5e76\u52a0\u901f\u975e\u4fb5\u5165\u6027\u795e\u7ecf\u89e3\u7801\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u7acb2025 PNPL\u7ade\u8d5b\u53ca\u76f8\u5173\u8d44\u6e90\uff0c\u671f\u671b\u80fd\u591f\u6c47\u805a\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u529b\u91cf\uff0c\u5728\u975e\u4fb5\u5165\u6027\u8111-\u673a\u63a5\u53e3\u5c24\u5176\u662f\u8a00\u8bed\u6062\u590d\u65b9\u9762\u5b9e\u73b0\u7a81\u7834\u3002"}}
{"id": "2506.10378", "pdf": "https://arxiv.org/pdf/2506.10378", "abs": "https://arxiv.org/abs/2506.10378", "authors": ["Jikai Jin", "Vasilis Syrgkanis", "Sham Kakade", "Hanlin Zhang"], "title": "Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\u6765\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u63a7\u5236\u57fa\u7840\u6a21\u578b\u7684\u6df7\u6dc6\u56e0\u7d20\u8bc6\u522b\u51fa\u4e09\u4e2a\u6f5c\u5728\u80fd\u529b\u56e0\u5b50\u4e4b\u95f4\u7684\u7ebf\u6027\u56e0\u679c\u7ed3\u6784\uff0c\u5e76\u63ed\u793a\u4e86\u4ece\u4e00\u822c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u5230\u6307\u4ee4\u9075\u5faa\u719f\u7ec3\u5ea6\u518d\u5230\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u6e05\u6670\u56e0\u679c\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5728\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e25\u683c\u56e0\u679c\u8bc4\u4f30\u65f6\u9047\u5230\u7684\u65b9\u6cd5\u8bba\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u7684\u6df7\u6dc6\u6548\u5e94\u548c\u4e0e\u5e7f\u6cdb\u518d\u8bad\u7ec3\u76f8\u5173\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u89c2\u5bdf\u5230\u7684\u57fa\u51c6\u6027\u80fd\u5efa\u6a21\u4e3a\u5c11\u6570\u51e0\u4e2a\u6f5c\u5728\u80fd\u529b\u56e0\u5b50\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6f5c\u5728\u56e0\u5b50\u88ab\u786e\u5b9a\u4e3a\u5728\u9002\u5f53\u63a7\u5236\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u5171\u540c\u6df7\u6742\u56e0\u7d20\u4e4b\u540e\u5b58\u5728\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5e94\u7528\u8fd9\u79cd\u65b9\u6cd5\u4e8e\u5305\u542b\u8d85\u8fc71500\u4e2a\u6a21\u578b\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7814\u7a76\u8005\u53d1\u73b0\u4e86\u4e00\u4e2a\u7b80\u6d01\u7684\u4e09\u8282\u70b9\u7ebf\u6027\u56e0\u679c\u7ed3\u6784\uff0c\u5b83\u80fd\u591f\u53ef\u9760\u5730\u89e3\u91ca\u6240\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u53d8\u5316\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u4ed4\u7ec6\u63a7\u5236\u57fa\u7840\u6a21\u578b\u53d8\u4f53\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u662f\u51c6\u786e\u63ed\u793a\u6f5c\u5728\u6a21\u578b\u80fd\u529b\u4e4b\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2506.10585", "pdf": "https://arxiv.org/pdf/2506.10585", "abs": "https://arxiv.org/abs/2506.10585", "authors": ["Mohd Anwar Jamal Faiz"], "title": "Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning", "categories": ["cs.AI", "cs.HC", "cs.SC"], "comment": "9 pages, 7 figures, 2 tables, 3 codes, oeis sequence A384735", "summary": "This paper introduces the Primender sequence, a novel integer sequence\ndefined by a hybrid rule that combines classical primality with modular\ndigit-based conditions. Specifically, a number n is included in the sequence if\nit is prime or ends with a prime number of unit digit or any length. In other\nwords, numbers which are primes or have at least one prime suffix. The\nresulting sequence exhibits a deterministic yet non-trivial structure, blending\nnumber-theoretic properties with symbolic patterning. We propose the Primender\nsequence as a benchmark for evaluating the symbolic reasoning capabilities of\nLarge Language Models (LLMs). The study is motivated by the need for\ninterpretable, rule-based testbeds that can assess an LLM's ability to infer\nhidden rules, validate mathematical hypotheses, and generalize symbolic logic\nat scale. A key hypothesis explored is: Whenever a number in the Primender\nsequence is exactly one more than the largest prime less than or equal to it,\nthe difference between it and the previous number in the sequence is also 1. We\ndesign a structured prompt and evaluation framework to test this hypothesis\nacross multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,\nGemini, Grok, and LLaMA. The models are tasked with identifying the underlying\nrule, validating the hypothesis, and generating the next 100,000 terms of the\nsequence. Comparative metrics such as rule inference accuracy, hypothesis\nevaluation, sequence validity, and symbolic explanation quality are used to\nassess model performance. This work contributes a novel mathematical construct\nand a reproducible methodology for benchmarking LLMs in symbolic reasoning,\nhypothesis testing, and scalable pattern generalization - bridging the domains\nof number theory, artificial intelligence, and software engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Primender\u5e8f\u5217\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u7ecf\u5178\u7d20\u6570\u89c4\u5219\u4e0e\u57fa\u4e8e\u6a21\u6570\u4f4d\u6761\u4ef6\u7684\u65b0\u578b\u6574\u6570\u5e8f\u5217\u3002\u8be5\u5e8f\u5217\u88ab\u63d0\u8bae\u4f5c\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002\u7814\u7a76\u52a8\u673a\u662f\u9700\u8981\u53ef\u89e3\u91ca\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u8bc4\u4f30LLM\u63a8\u65ad\u9690\u85cf\u89c4\u5219\u3001\u9a8c\u8bc1\u6570\u5b66\u5047\u8bbe\u548c\u5927\u89c4\u6a21\u63a8\u5e7f\u7b26\u53f7\u903b\u8f91\u7684\u80fd\u529b\u3002\u901a\u8fc7\u8bbe\u8ba1\u7ed3\u6784\u5316\u63d0\u793a\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u7684LLM\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u4e00\u7cfb\u5217\u6bd4\u8f83\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u4f9b\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u57fa\u4e8e\u89c4\u5219\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4ee5\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u65ad\u9690\u85cf\u89c4\u5219\u3001\u9a8c\u8bc1\u6570\u5b66\u5047\u8bbe\u4ee5\u53ca\u5927\u89c4\u6a21\u63a8\u5e7f\u7b26\u53f7\u903b\u8f91\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6574\u6570\u5e8f\u5217Primender\u5e8f\u5217\uff0c\u8be5\u5e8f\u5217\u7ed3\u5408\u4e86\u7ecf\u5178\u7684\u7d20\u6027\u68c0\u6d4b\u4e0e\u57fa\u4e8e\u6a21\u6570\u4f4d\u7684\u6761\u4ef6\u3002\u4e3a\u4e86\u63a2\u7a76\u8fd9\u4e00\u5e8f\u5217\u7684\u6027\u8d28\uff0c\u4f5c\u8005\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u63d0\u793a\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u591a\u4e2a\u6700\u5148\u8fdbLLM\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u6a21\u578b\u88ab\u8981\u6c42\u8bc6\u522b\u5e95\u5c42\u89c4\u5219\u3001\u9a8c\u8bc1\u5047\u8bbe\u5e76\u751f\u6210\u5e8f\u5217\u7684\u4e0b10\u4e07\u4e2a\u9879\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540cLLM\u5728\u89c4\u5219\u63a8\u65ad\u51c6\u786e\u6027\u3001\u5047\u8bbe\u8bc4\u4f30\u3001\u5e8f\u5217\u6709\u6548\u6027\u548c\u7b26\u53f7\u89e3\u91ca\u8d28\u91cf\u7b49\u8bc4\u4ef7\u6307\u6807\u4e0a\u7684\u8868\u73b0\u5404\u5f02\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8d21\u732e\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u5b66\u6784\u9020\u4ee5\u53ca\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u5728\u7b26\u53f7\u63a8\u7406\u3001\u5047\u8bbe\u68c0\u9a8c\u548c\u6a21\u5f0f\u63a8\u5e7f\u65b9\u9762\u5bf9LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u800c\u67b6\u8d77\u4e86\u6570\u8bba\u3001\u4eba\u5de5\u667a\u80fd\u548c\u8f6f\u4ef6\u5de5\u7a0b\u4e4b\u95f4\u7684\u6865\u6881\u3002"}}
{"id": "2506.10167", "pdf": "https://arxiv.org/pdf/2506.10167", "abs": "https://arxiv.org/abs/2506.10167", "authors": ["Zahra Shahrooei", "Ali Baheri"], "title": "Wasserstein Barycenter Soft Actor-Critic", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Deep off-policy actor-critic algorithms have emerged as the leading framework\nfor reinforcement learning in continuous control domains. However, most of\nthese algorithms suffer from poor sample efficiency, especially in environments\nwith sparse rewards. In this paper, we take a step towards addressing this\nissue by providing a principled directed exploration strategy. We propose\nWasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from\na pessimistic actor for temporal difference learning and an optimistic actor to\npromote exploration. This is achieved by using the Wasserstein barycenter of\nthe pessimistic and optimistic policies as the exploration policy and adjusting\nthe degree of exploration throughout the learning process. We compare WBSAC\nwith state-of-the-art off-policy actor-critic algorithms and show that WBSAC is\nmore sample-efficient on MuJoCo continuous control tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5WBSAC\uff0c\u901a\u8fc7\u7ed3\u5408\u60b2\u89c2\u548c\u4e50\u89c2\u7b56\u7565\u7684Wasserstein\u91cd\u5fc3\u6765\u6307\u5bfc\u63a2\u7d22\uff0c\u4ece\u800c\u63d0\u9ad8\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u79bb\u7ebf\u7b56\u7565actor-critic\u7b97\u6cd5\u5728\u8fde\u7eed\u63a7\u5236\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86Wasserstein Barycenter Soft Actor-Critic (WBSAC) \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u4e00\u4e2a\u60b2\u89c2\u7684actor\u8fdb\u884c\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\uff0c\u5e76\u4e14\u4f7f\u7528\u4e00\u4e2a\u4e50\u89c2\u7684actor\u4fc3\u8fdb\u63a2\u7d22\u3002\u901a\u8fc7\u5728\u6574\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8c03\u6574\u63a2\u7d22\u7a0b\u5ea6\uff0c\u5229\u7528\u60b2\u89c2\u4e0e\u4e50\u89c2\u7b56\u7565\u7684Wasserstein\u91cd\u5fc3\u4f5c\u4e3a\u63a2\u7d22\u7b56\u7565\u3002", "result": "WBSAC \u4e0e\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u7b56\u7565 actor-critic \u7b97\u6cd5\u76f8\u6bd4\uff0c\u5728MuJoCo\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u4e86\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "WBSAC \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u7684\u5b9a\u5411\u63a2\u7d22\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2506.10586", "pdf": "https://arxiv.org/pdf/2506.10586", "abs": "https://arxiv.org/abs/2506.10586", "authors": ["Antonio Ferrara", "Francesco Cozzi", "Alan Perotti", "Andr\u00e9 Panisson", "Francesco Bonchi"], "title": "Size-adaptive Hypothesis Testing for Fairness", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": null, "summary": "Determining whether an algorithmic decision-making system discriminates\nagainst a specific demographic typically involves comparing a single point\nestimate of a fairness metric against a predefined threshold. This practice is\nstatistically brittle: it ignores sampling error and treats small demographic\nsubgroups the same as large ones. The problem intensifies in intersectional\nanalyses, where multiple sensitive attributes are considered jointly, giving\nrise to a larger number of smaller groups. As these groups become more\ngranular, the data representing them becomes too sparse for reliable\nestimation, and fairness metrics yield excessively wide confidence intervals,\nprecluding meaningful conclusions about potential unfair treatments.\n  In this paper, we introduce a unified, size-adaptive, hypothesis-testing\nframework that turns fairness assessment into an evidence-based statistical\ndecision. Our contribution is twofold. (i) For sufficiently large subgroups, we\nprove a Central-Limit result for the statistical parity difference, leading to\nanalytic confidence intervals and a Wald test whose type-I (false positive)\nerror is guaranteed at level $\\alpha$. (ii) For the long tail of small\nintersectional groups, we derive a fully Bayesian Dirichlet-multinomial\nestimator; Monte-Carlo credible intervals are calibrated for any sample size\nand naturally converge to Wald intervals as more data becomes available. We\nvalidate our approach empirically on benchmark datasets, demonstrating how our\ntests provide interpretable, statistically rigorous decisions under varying\ndegrees of data availability and intersectionality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u9002\u5e94\u89c4\u6a21\u7684\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u516c\u5e73\u6027\u8bc4\u4f30\u8f6c\u5316\u4e3a\u57fa\u4e8e\u8bc1\u636e\u7684\u7edf\u8ba1\u51b3\u7b56\u3002\u5bf9\u4e8e\u8db3\u591f\u5927\u7684\u5b50\u7fa4\u4f53\uff0c\u8bc1\u660e\u4e86\u7edf\u8ba1\u5947\u5076\u5dee\u5f02\u7684\u4e2d\u5fc3\u6781\u9650\u7ed3\u679c\uff0c\u5e76\u4e3a\u5c0f\u4ea4\u53c9\u7fa4\u4f53\u5bfc\u51fa\u4e86\u4e00\u4e2a\u5b8c\u5168\u8d1d\u53f6\u65afDirichlet-\u591a\u9879\u5f0f\u4f30\u8ba1\u5668\u3002\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u53ef\u7528\u6027\u548c\u4ea4\u53c9\u6027\u7a0b\u5ea6\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u4e2d\uff0c\u5bf9\u7279\u5b9a\u4eba\u7fa4\u662f\u5426\u5b58\u5728\u6b67\u89c6\u7684\u5224\u65ad\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u70b9\u4f30\u8ba1\u7684\u516c\u5e73\u5ea6\u91cf\u4e0e\u9884\u5b9a\u4e49\u9608\u503c\u7684\u6bd4\u8f83\uff0c\u8fd9\u79cd\u505a\u6cd5\u5728\u7edf\u8ba1\u4e0a\u4e0d\u591f\u7a33\u5065\uff0c\u5ffd\u7565\u4e86\u62bd\u6837\u8bef\u5dee\uff0c\u4e5f\u6ca1\u6709\u533a\u5206\u5927\u5c0f\u4e0d\u540c\u7684\u7fa4\u4f53\u3002\u5f53\u8003\u8651\u591a\u4e2a\u654f\u611f\u5c5e\u6027\u65f6\uff0c\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u590d\u6742\uff0c\u56e0\u4e3a\u8fd9\u4f1a\u5bfc\u81f4\u66f4\u591a\u7684\u5c0f\u7fa4\u4f53\u51fa\u73b0\uff0c\u800c\u8fd9\u4e9b\u5c0f\u7fa4\u4f53\u7684\u6570\u636e\u8fc7\u4e8e\u7a00\u758f\uff0c\u65e0\u6cd5\u8fdb\u884c\u53ef\u9760\u7684\u4f30\u8ba1\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u9002\u5e94\u89c4\u6a21\u7684\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u5bf9\u4e8e\u8db3\u591f\u5927\u7684\u5b50\u7fa4\u4f53\uff0c\u901a\u8fc7\u8bc1\u660e\u7edf\u8ba1\u5947\u5076\u5dee\u5f02\u7684\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u6765\u83b7\u5f97\u89e3\u6790\u7f6e\u4fe1\u533a\u95f4\u548cWald\u68c0\u9a8c\uff1b\u5bf9\u4e8e\u8f83\u5c0f\u7684\u4ea4\u53c9\u7fa4\u4f53\uff0c\u5219\u4f7f\u7528\u5b8c\u5168\u8d1d\u53f6\u65afDirichlet-\u591a\u9879\u5f0f\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u53ef\u4fe1\u533a\u95f4\u6765\u6821\u51c6\u4efb\u4f55\u6837\u672c\u5927\u5c0f\u7684\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u6d4b\u8bd5\u80fd\u591f\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u6570\u636e\u53ef\u7528\u6027\u548c\u4ea4\u53c9\u6027\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u7edf\u8ba1\u4e0a\u4e25\u683c\u7684\u51b3\u7b56\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6539\u5584\u4f20\u7edf\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u6cd5\u7684\u7edf\u8ba1\u8106\u5f31\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u52a0\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5927\u6570\u636e\u96c6\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u6570\u636e\u8f83\u4e3a\u7a00\u758f\u7684\u5c0f\u578b\u4ea4\u53c9\u7fa4\u4f53\u3002"}}
{"id": "2506.10613", "pdf": "https://arxiv.org/pdf/2506.10613", "abs": "https://arxiv.org/abs/2506.10613", "authors": ["Henrik Sebastian Steude", "Alexander Diedrich", "Ingo Pill", "Lukas Moddemann", "Daniel Vranje\u0161", "Oliver Niggemann"], "title": "Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Diagnostic processes for complex cyber-physical systems often require\nextensive prior knowledge in the form of detailed system models or\ncomprehensive training data. However, obtaining such information poses a\nsignificant challenge. To address this issue, we present a new diagnostic\napproach that operates with minimal prior knowledge, requiring only a basic\nunderstanding of subsystem relationships and data from nominal operations. Our\nmethod combines a neural network-based symptom generator, which employs\nsubsystem-level anomaly detection, with a new graph diagnosis algorithm that\nleverages minimal causal relationship information between\nsubsystems-information that is typically available in practice. Our experiments\nwith fully controllable simulated datasets show that our method includes the\ntrue causal component in its diagnosis set for 82 p.c. of all cases while\neffectively reducing the search space in 73 p.c. of the scenarios. Additional\ntests on the real-world Secure Water Treatment dataset showcase the approach's\npotential for practical scenarios. Our results thus highlight our approach's\npotential for practical applications with large and complex cyber-physical\nsystems where limited prior knowledge is available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bca\u65ad\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u6700\u5c11\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u75c7\u72b6\u751f\u6210\u5668\u548c\u65b0\u7684\u56fe\u8bca\u65ad\u7b97\u6cd5\u6765\u5de5\u4f5c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u51fa\u771f\u5b9e\u7684\u56e0\u679c\u7ec4\u4ef6\uff0c\u5e76\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u3002", "motivation": "\u590d\u6742\u7684\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u8bca\u65ad\u8fc7\u7a0b\u901a\u5e38\u9700\u8981\u8be6\u7ec6\u7684\u7cfb\u7edf\u6a21\u578b\u6216\u5168\u9762\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u8fd9\u4e9b\u4fe1\u606f\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ea\u9700\u8981\u6781\u5c11\u5148\u9a8c\u77e5\u8bc6\u7684\u65b0\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u75c7\u72b6\u751f\u6210\u5668\uff08\u7528\u4e8e\u5b50\u7cfb\u7edf\u7ea7\u522b\u7684\u5f02\u5e38\u68c0\u6d4b\uff09\u4e0e\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u8bca\u65ad\u7b97\u6cd5\uff08\u5229\u7528\u5b50\u7cfb\u7edf\u95f4\u6700\u5c0f\u7684\u56e0\u679c\u5173\u7cfb\u4fe1\u606f\uff09\u3002", "result": "\u4f7f\u7528\u5b8c\u5168\u53ef\u63a7\u7684\u6a21\u62df\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6240\u6709\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u670982%\u7684\u6982\u7387\u5c06\u771f\u5b9e\u56e0\u679c\u7ec4\u4ef6\u5305\u542b\u5728\u5176\u8bca\u65ad\u96c6\u5408\u5185\uff0c\u5e76\u4e14\u572873%\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u51cf\u5c11\u4e86\u641c\u7d22\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u5728\u5b9e\u9645\u7684\u5b89\u5168\u6c34\u5904\u7406\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u4e5f\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u4e8e\u5177\u6709\u6709\u9650\u5148\u9a8c\u77e5\u8bc6\u7684\u5927\u89c4\u6a21\u590d\u6742\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u7684\u5b9e\u7528\u5e94\u7528\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2506.10982", "pdf": "https://arxiv.org/pdf/2506.10982", "abs": "https://arxiv.org/abs/2506.10982", "authors": ["Sebastian Sanokowski", "Lukas Gruber", "Christoph Bartmann", "Sepp Hochreiter", "Sebastian Lehner"], "title": "Rethinking Losses for Diffusion Bridge Samplers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Diffusion bridges are a promising class of deep-learning methods for sampling\nfrom unnormalized distributions. Recent works show that the Log Variance (LV)\nloss consistently outperforms the reverse Kullback-Leibler (rKL) loss when\nusing the reparametrization trick to compute rKL-gradients. While the on-policy\nLV loss yields identical gradients to the rKL loss when combined with the\nlog-derivative trick for diffusion samplers with non-learnable forward\nprocesses, this equivalence does not hold for diffusion bridges or when\ndiffusion coefficients are learned. Based on this insight we argue that for\ndiffusion bridges the LV loss does not represent an optimization objective that\ncan be motivated like the rKL loss via the data processing inequality. Our\nanalysis shows that employing the rKL loss with the log-derivative trick\n(rKL-LD) does not only avoid these conceptual problems but also consistently\noutperforms the LV loss. Experimental results with different types of diffusion\nbridges on challenging benchmarks show that samplers trained with the rKL-LD\nloss achieve better performance. From a practical perspective we find that\nrKL-LD requires significantly less hyperparameter optimization and yields more\nstable training behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u6269\u6563\u6865\u6765\u8bf4\uff0c\u4f7f\u7528\u5e26\u6709\u5bf9\u6570\u5bfc\u6570\u6280\u5de7\u7684\u9006\u5411KL\u635f\u5931\uff08rKL-LD\uff09\u4e0d\u4ec5\u907f\u514d\u4e86\u6982\u5ff5\u4e0a\u7684\u95ee\u9898\uff0c\u800c\u4e14\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5bf9\u6570\u65b9\u5dee\u635f\u5931\uff08LV\uff09\u3002\u6b64\u5916\uff0crKL-LD\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u66f4\u5c11\u7684\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u884c\u4e3a\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u548c\u6bd4\u8f83\u4e24\u79cd\u7528\u4e8e\u4ece\u975e\u6807\u51c6\u5316\u5206\u5e03\u91c7\u6837\u7684\u65b9\u6cd5\uff1a\u5bf9\u6570\u65b9\u5dee\u635f\u5931\uff08LV\uff09\u4e0e\u7ed3\u5408\u4e86\u5bf9\u6570\u5bfc\u6570\u6280\u5de7\u7684\u9006\u5411Kullback-Leibler\u635f\u5931\uff08rKL-LD\uff09\uff0c\u5c24\u5176\u662f\u5728\u6269\u6563\u6865\u6a21\u578b\u4ee5\u53ca\u5b66\u4e60\u6269\u6563\u7cfb\u6570\u7684\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86LV\u635f\u5931\u4e0erKL\u635f\u5931\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u7684\u7b49\u4ef7\u6027\u5e76\u4e0d\u9002\u7528\u4e8e\u6269\u6563\u6865\u6216\u5f53\u6269\u6563\u7cfb\u6570\u662f\u53ef\u5b66\u4e60\u7684\u65f6\u5019\u3002\u8fdb\u4e00\u6b65\u5730\uff0c\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u4f5c\u8005\u8bba\u8bc1\u4e86\u4e3a\u4ec0\u4e48rKL-LD\u662f\u66f4\u4f18\u7684\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u7684\u6269\u6563\u6865\u7c7b\u578b\u4e0a\uff0c\u4f7f\u7528rKL-LD\u635f\u5931\u8bad\u7ec3\u7684\u91c7\u6837\u5668\u6bd4\u4f7f\u7528LV\u635f\u5931\u7684\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002\u53e6\u5916\uff0crKL-LD\u8fd8\u51cf\u5c11\u4e86\u6240\u9700\u7684\u8d85\u53c2\u6570\u8c03\u6574\u5de5\u4f5c\u91cf\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u52a0\u7a33\u5b9a\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "conclusion": "\u5bf9\u4e8e\u6269\u6563\u6865\u800c\u8a00\uff0c\u91c7\u7528\u5e26\u6709\u5bf9\u6570\u5bfc\u6570\u6280\u5de7\u7684\u9006\u5411KL\u635f\u5931\uff08rKL-LD\uff09\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\u66f4\u4e3a\u5408\u9002\uff0c\u56e0\u4e3a\u5b83\u89e3\u51b3\u4e86LV\u635f\u5931\u5b58\u5728\u7684\u6982\u5ff5\u6027\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.10674", "pdf": "https://arxiv.org/pdf/2506.10674", "abs": "https://arxiv.org/abs/2506.10674", "authors": ["Vincenzo Colle", "Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Fadhel Ayed", "Merouane Debbah"], "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving", "categories": ["cs.AI", "cs.CL"], "comment": "6 pages", "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TeleMath\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u7535\u4fe1\u9886\u57df\u6570\u5b66\u95ee\u9898\u65b9\u9762\u8868\u73b0\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u901a\u8fc7500\u4e2a\u95ee\u7b54\u5bf9\u8986\u76d6\u4e86\u7535\u4fe1\u9886\u57df\u7684\u5e7f\u6cdb\u4e3b\u9898\uff0c\u5e76\u4e14\u7814\u7a76\u53d1\u73b0\u4e13\u95e8\u9488\u5bf9\u6570\u5b66\u6216\u903b\u8f91\u63a8\u7406\u8bbe\u8ba1\u7684\u6a21\u578b\u5728TeleMath\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u7684\u8fdb\u6b65\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e00\u822c\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u6027\u80fd\uff0c\u4f46\u5728\u4fe1\u53f7\u5904\u7406\u3001\u7f51\u7edc\u4f18\u5316\u548c\u6027\u80fd\u5206\u6790\u7b49\u4e13\u4e1a\u9886\u57df\u5185\u7684\u6709\u6548\u6027\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u4eec\u63d0\u51fa\u4e86TeleMath\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u6765\u8bc4\u4f30LLMs\u5728\u7535\u4fe1\u9886\u57df\u89e3\u51b3\u5177\u6709\u6570\u503c\u89e3\u7684\u6570\u5b66\u95ee\u9898\u6027\u80fd\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "TeleMath\u6570\u636e\u96c6\u5305\u542b500\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u7535\u4fe1\u9886\u57df\u7684\u5e7f\u6cdb\u8bdd\u9898\u3002\u8bba\u6587\u6982\u8ff0\u4e86\u4ece\u7531\u4e3b\u9898\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\u79cd\u5b50\u5f00\u59cb\u7684QnA\u751f\u6210\u6d41\u7a0b\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u4e00\u7cfb\u5217\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4ee5\u63ed\u793a\u54ea\u4e9b\u6a21\u578b\u80fd\u591f\u6700\u597d\u5730\u89e3\u51b3\u8fd9\u4e9b\u7279\u5b9a\u4e8e\u7535\u4fe1\u9886\u57df\u7684\u6570\u5b66\u95ee\u9898\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u90a3\u4e9b\u4e13\u95e8\u4e3a\u6570\u5b66\u6216\u903b\u8f91\u63a8\u7406\u8bbe\u8ba1\u7684\u6700\u65b0\u6a21\u578b\u5728TeleMath\u4e0a\u7684\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5373\u4fbf\u662f\u53c2\u6570\u6570\u91cf\u5e9e\u5927\u7684\u901a\u7528\u6a21\u578b\u4e5f\u5e38\u5e38\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u7684\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5bf9\u4e8e\u4e13\u4e1a\u9886\u57df\u5185\u590d\u6742\u4efb\u52a1\uff0c\u7279\u522b\u662f\u6570\u5b66\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u9700\u8981\u4e13\u95e8\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u3002TeleMath\u6570\u636e\u96c6\u4ee5\u53ca\u8bc4\u4f30\u4ee3\u7801\u5df2\u7ecf\u88ab\u516c\u5f00\uff0c\u4ee5\u4fc3\u8fdb\u7ed3\u679c\u7684\u53ef\u91cd\u590d\u6027\u548c\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2506.10180", "pdf": "https://arxiv.org/pdf/2506.10180", "abs": "https://arxiv.org/abs/2506.10180", "authors": ["Mowafaq Salem Alzboon", "Mohammad Al-Batah", "Muhyeeddin Alqaraleh", "Ahmad Abuashour", "Ahmad Fuad Bader"], "title": "A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In many nations, diabetes is becoming a significant health problem, and early\nidentification and control are crucial. Using machine learning algorithms to\npredict diabetes has yielded encouraging results. Using the Pima Indians\nDiabetes dataset, this study attempts to evaluate the efficacy of several\nmachine-learning methods for diabetes prediction. The collection includes\ninformation on 768 patients, such as their ages, BMIs, and glucose levels. The\ntechniques assessed are Logistic Regression, Decision Tree, Random Forest,\nk-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting,\nand Neural Network. The findings indicate that the Neural Network algorithm\nperformed the best, with an accuracy of 78.57 percent, followed by the Random\nForest method, with an accuracy of 76.30 percent. The study implies that\nmachine learning algorithms can aid diabetes prediction and be an efficient\nearly detection tool.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Pima Indians Diabetes\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u7cd6\u5c3f\u75c5\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523078.57%\uff0c\u5176\u6b21\u662f\u968f\u673a\u68ee\u6797\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u4e3a76.30%\u3002", "motivation": "\u7531\u4e8e\u7cd6\u5c3f\u75c5\u6b63\u6210\u4e3a\u8bb8\u591a\u56fd\u5bb6\u7684\u91cd\u5927\u5065\u5eb7\u95ee\u9898\uff0c\u65e9\u671f\u8bc6\u522b\u548c\u63a7\u5236\u975e\u5e38\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6765\u9884\u6d4b\u7cd6\u5c3f\u75c5\u5177\u6709\u91cd\u8981\u7684\u610f\u4e49\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86Pima Indians Diabetes\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u903b\u8f91\u56de\u5f52\u3001\u51b3\u7b56\u6811\u3001\u968f\u673a\u68ee\u6797\u3001k-\u6700\u8fd1\u90bb\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u68af\u5ea6\u63d0\u5347\u548c\u795e\u7ecf\u7f51\u7edc\u7b49\u51e0\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u7684\u8868\u73b0\u6700\u597d\uff0c\u5176\u51c6\u786e\u7387\u4e3a78.57%\uff0c\u5176\u6b21\u662f\u968f\u673a\u68ee\u6797\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u4e3a76.30%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u53ef\u4ee5\u8f85\u52a9\u7cd6\u5c3f\u75c5\u7684\u9884\u6d4b\uff0c\u5e76\u4e14\u4f5c\u4e3a\u6709\u6548\u7684\u65e9\u671f\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2506.10678", "pdf": "https://arxiv.org/pdf/2506.10678", "abs": "https://arxiv.org/abs/2506.10678", "authors": ["Tom Westermann", "Aljosha K\u00f6cher", "Felix Gehlhoff"], "title": "Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "AutomationML (AML) enables standardized data exchange in engineering, yet\nexisting recommendations for proper AML modeling are typically formulated as\ninformal and textual constraints. These constraints cannot be validated\nautomatically within AML itself. This work-in-progress paper introduces a\npipeline to formalize and verify such constraints. First, AML models are mapped\nto OWL ontologies via RML and SPARQL. In addition, a Large Language Model\ntranslates textual rules into SHACL constraints, which are then validated\nagainst the previously generated AML ontology. Finally, SHACL validation\nresults are automatically interpreted in natural language. The approach is\ndemonstrated on a sample AML recommendation. Results show that even complex\nmodeling rules can be semi-automatically checked -- without requiring users to\nunderstand formal methods or ontology technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06AutomationML\u6a21\u578b\u6620\u5c04\u5230OWL\u672c\u4f53\u5e76\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u6587\u672c\u89c4\u5219\u8f6c\u5316\u4e3aSHACL\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9AML\u5efa\u6a21\u89c4\u5219\u7684\u534a\u81ea\u52a8\u5316\u68c0\u67e5\u3002", "motivation": "\u73b0\u6709\u7684AutomationML\u5efa\u6a21\u5efa\u8bae\u901a\u5e38\u662f\u4ee5\u975e\u6b63\u5f0f\u7684\u6587\u672c\u5f62\u5f0f\u8868\u8fbe\uff0c\u8fd9\u4e9b\u7ea6\u675f\u65e0\u6cd5\u5728AML\u672c\u8eab\u5185\u81ea\u52a8\u9a8c\u8bc1\u3002", "method": "\u9996\u5148\u901a\u8fc7RML\u548cSPARQL\u5c06AML\u6a21\u578b\u6620\u5c04\u5230OWL\u672c\u4f53\uff0c\u7136\u540e\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u6587\u672c\u89c4\u5219\u8f6c\u6362\u4e3aSHACL\u7ea6\u675f\uff0c\u5e76\u9488\u5bf9\u5148\u524d\u751f\u6210\u7684AML\u672c\u4f53\u8fdb\u884c\u9a8c\u8bc1\u3002\u6700\u540e\uff0c\u81ea\u52a8\u4ee5\u81ea\u7136\u8bed\u8a00\u89e3\u91caSHACL\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e00\u4e2a\u6837\u672cAML\u5efa\u8bae\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u662f\u590d\u6742\u7684\u5efa\u6a21\u89c4\u5219\u4e5f\u53ef\u4ee5\u88ab\u534a\u81ea\u52a8\u5316\u5730\u68c0\u67e5\uff0c\u800c\u4e0d\u9700\u8981\u7528\u6237\u7406\u89e3\u5f62\u5f0f\u5316\u65b9\u6cd5\u6216\u672c\u4f53\u6280\u672f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0AML\u5efa\u6a21\u89c4\u5219\u7684\u5f62\u5f0f\u5316\u548c\u9a8c\u8bc1\uff0c\u7b80\u5316\u4e86\u7528\u6237\u7684\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u6548\u7387\u3002"}}
{"id": "2506.10184", "pdf": "https://arxiv.org/pdf/2506.10184", "abs": "https://arxiv.org/abs/2506.10184", "authors": ["Mohammad Subhi Al-Batah", "Mowafaq Salem Alzboon", "Muhyeeddin Alqaraleh"], "title": "Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study conducts an empirical examination of MLP networks investigated\nthrough a rigorous methodical experimentation process involving three diverse\ndatasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes\nthree key methods: a) a baseline training using the default settings for the\nMulti-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA)\nbased refinement c) Principal Component Analysis (PCA) based dimension\nreduction. The results show important information on how such techniques affect\nperformance. While PCA had showed benefits in low-dimensional and noise-free\ndatasets GA consistently increased accuracy in complex datasets by accurately\nidentifying critical features. Comparison reveals that feature selection and\ndimensionality reduction play interdependent roles in enhancing MLP\nperformance. The study contributes to the literature on feature engineering and\nneural network parameter optimization, offering practical guidelines for a wide\nrange of machine learning tasks", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u63a2\u8ba8\u4e86\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7f51\u7edc\u7684\u6027\u80fd\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u9ed8\u8ba4\u8bbe\u7f6e\u8bad\u7ec3MLP\u3001\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u7684\u7279\u5f81\u9009\u62e9\u548c\u57fa\u4e8e\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u7684\u964d\u7ef4\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u4f4e\u7ef4\u5ea6\u65e0\u566a\u58f0\u7684\u6570\u636e\u96c6\uff0cPCA\u663e\u793a\u4e86\u4f18\u52bf\uff1b\u800c\u5bf9\u4e8e\u590d\u6742\u6570\u636e\u96c6\uff0cGA\u901a\u8fc7\u7cbe\u786e\u8bc6\u522b\u5173\u952e\u7279\u5f81\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e0d\u540c\u7684\u7279\u5f81\u9009\u62e9\u548c\u964d\u7ef4\u6280\u672f\u5982\u4f55\u5f71\u54cd\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7f51\u7edc\u5728\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e3a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1aa) \u4f7f\u7528\u9ed8\u8ba4\u8bbe\u7f6e\u5bf9\u591a\u5c42\u611f\u77e5\u5668(MLP)\u8fdb\u884c\u57fa\u51c6\u8bad\u7ec3\uff1bb) \u91c7\u7528\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5(GA)\u7684\u65b9\u6cd5\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff1bc) \u5229\u7528\u4e3b\u6210\u5206\u5206\u6790(PCA)\u6267\u884c\u57fa\u4e8e\u7ef4\u5ea6\u7f29\u51cf\u7684\u64cd\u4f5c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4f4e\u7ef4\u5ea6\u4e14\u6ca1\u6709\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e2d\uff0cPCA\u8868\u73b0\u51fa\u8272\uff1b\u800cGA\u5219\u5728\u590d\u6742\u6570\u636e\u96c6\u4e2d\u901a\u8fc7\u51c6\u786e\u5730\u8bc6\u522b\u91cd\u8981\u7279\u5f81\u6301\u7eed\u63d0\u5347\u4e86\u7cbe\u5ea6\u3002\u6bd4\u8f83\u63ed\u793a\u51fa\u7279\u5f81\u9009\u62e9\u4e0e\u7ef4\u5ea6\u51cf\u5c11\u5728\u63d0\u9ad8MLP\u8868\u73b0\u65b9\u9762\u8d77\u7740\u76f8\u4e92\u4f9d\u8d56\u7684\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7279\u5f81\u9009\u62e9\u548c\u964d\u7ef4\u5728\u63d0\u5347MLP\u6027\u80fd\u65b9\u9762\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7279\u5f81\u5de5\u7a0b\u548c\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u6587\u732e\u8d21\u732e\uff0c\u5e76\u4e3a\u5404\u79cd\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u63d0\u51fa\u4e86\u5b9e\u8df5\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2506.10708", "pdf": "https://arxiv.org/pdf/2506.10708", "abs": "https://arxiv.org/abs/2506.10708", "authors": ["Michael Bartholomew", "Joohyung Lee"], "title": "System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers", "categories": ["cs.AI", "cs.LO"], "comment": "In Proceedings of the 14th European Conference on Logics in\n  Artificial Intelligence (JELIA 2014)", "summary": "Answer Set Programming Modulo Theories (ASPMT) is an approach to combining\nanswer set programming and satisfiability modulo theories based on the\nfunctional stable model semantics. It is shown that the tight fragment of ASPMT\nprograms can be turned into SMT instances, thereby allowing SMT solvers to\ncompute stable models of ASPMT programs. In this paper we present a compiler\ncalled {\\sc aspsmt2smt}, which implements this translation. The system uses ASP\ngrounder {\\sc gringo} and SMT solver {\\sc z3}. {\\sc gringo} partially grounds\ninput programs while leaving some variables to be processed by {\\sc z3}. We\ndemonstrate that the system can effectively handle real number computations for\nreasoning about continuous changes.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aaspsmt2smt\u7684\u7f16\u8bd1\u5668\uff0c\u5b83\u80fd\u591f\u5c06ASPMT\u7a0b\u5e8f\u8f6c\u5316\u4e3aSMT\u5b9e\u4f8b\uff0c\u5229\u7528gringo\u548cz3\u5de5\u5177\u6765\u5904\u7406\u90e8\u5206\u53d8\u91cf\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u5b9e\u6570\u8ba1\u7b97\u4ee5\u8fdb\u884c\u8fde\u7eed\u53d8\u5316\u7684\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u7ed3\u5408\u56de\u7b54\u96c6\u7f16\u7a0b\uff08ASP\uff09\u4e0e\u7406\u8bba\u4e0a\u7684\u53ef\u6ee1\u8db3\u6027\uff08SMT\uff09\uff0c\u5e76\u57fa\u4e8e\u51fd\u6570\u7a33\u5b9a\u6a21\u578b\u8bed\u4e49\uff0c\u5f00\u53d1\u51fa\u4e00\u79cd\u65b9\u6cd5\u4f7f\u5f97\u7d27\u81f4\u7247\u6bb5\u7684ASPMT\u7a0b\u5e8f\u53ef\u4ee5\u8f6c\u6362\u4e3aSMT\u5b9e\u4f8b\uff0c\u4ece\u800c\u5141\u8bb8\u4f7f\u7528SMT\u6c42\u89e3\u5668\u6765\u8ba1\u7b97ASPMT\u7a0b\u5e8f\u7684\u7a33\u5b9a\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86aspsmt2smt\u7f16\u8bd1\u5668\uff0c\u8be5\u7f16\u8bd1\u5668\u5b9e\u73b0\u4e86\u5c06ASPMT\u7a0b\u5e8f\u7ffb\u8bd1\u6210SMT\u5b9e\u4f8b\u7684\u529f\u80fd\u3002\u7cfb\u7edf\u4e2d\u4f7f\u7528\u4e86ASP grounder gringo\u6765\u90e8\u5206\u5730\u5bf9\u8f93\u5165\u7a0b\u5e8f\u8fdb\u884c\u57fa\u7840\u5316\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u4e00\u4e9b\u53d8\u91cf\u4f9bz3 SMT\u6c42\u89e3\u5668\u5904\u7406\u3002", "result": "\u901a\u8fc7aspsmt2smt\u7f16\u8bd1\u5668\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u6d89\u53ca\u5b9e\u6570\u8fd0\u7b97\u7684\u8fde\u7eed\u53d8\u5316\u63a8\u7406\u95ee\u9898\u3002", "conclusion": "aspsmt2smt\u7f16\u8bd1\u5668\u6210\u529f\u5730\u5c06ASPMT\u7a0b\u5e8f\u8f6c\u6362\u4e3aSMT\u5b9e\u4f8b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u5b9e\u6570\u8ba1\u7b97\u548c\u8fde\u7eed\u53d8\u5316\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.10186", "pdf": "https://arxiv.org/pdf/2506.10186", "abs": "https://arxiv.org/abs/2506.10186", "authors": ["Yuhui Ding", "Thomas Hofmann"], "title": "Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Equivariant diffusion models have achieved impressive performance in 3D\nmolecule generation. These models incorporate Euclidean symmetries of 3D\nmolecules by utilizing an SE(3)-equivariant denoising network. However,\nspecialized equivariant architectures limit the scalability and efficiency of\ndiffusion models. In this paper, we propose an approach that relaxes such\nequivariance constraints. Specifically, our approach learns a sample-dependent\nSO(3) transformation for each molecule to construct an aligned latent space. A\nnon-equivariant diffusion model is then trained over the aligned\nrepresentations. Experimental results demonstrate that our approach performs\nsignificantly better than previously reported non-equivariant models. It yields\nsample quality comparable to state-of-the-art equivariant diffusion models and\noffers improved training and sampling efficiency. Our code is available at\nhttps://github.com/skeletondyh/RADM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u6bcf\u4e2a\u5206\u5b50\u7684\u6837\u672c\u4f9d\u8d56SO(3)\u53d8\u6362\u6765\u6784\u5efa\u5bf9\u9f50\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5728\u8be5\u5bf9\u9f50\u8868\u793a\u4e0a\u8bad\u7ec3\u975e\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u751f\u6210\u6837\u672c\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u8bad\u7ec3\u548c\u91c7\u6837\u6548\u7387\u3002", "motivation": "\u867d\u7136\u7b49\u53d8\u6269\u6563\u6a21\u578b\u57283D\u5206\u5b50\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e13\u95e8\u7684\u7b49\u53d8\u67b6\u6784\u9650\u5236\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u653e\u677e\u4e86\u7b49\u53d8\u6027\u7ea6\u675f\uff0c\u4e3a\u6bcf\u4e2a\u5206\u5b50\u5b66\u4e60\u4e00\u4e2a\u6837\u672c\u4f9d\u8d56\u7684SO(3)\u53d8\u6362\u4ee5\u6784\u5efa\u5bf9\u9f50\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u4e14\u5728\u8fd9\u4e2a\u5bf9\u9f50\u7684\u7a7a\u95f4\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u975e\u7b49\u53d8\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5148\u524d\u62a5\u9053\u7684\u975e\u7b49\u53d8\u6a21\u578b\uff0c\u800c\u4e14\u4ea7\u751f\u7684\u6837\u672c\u8d28\u91cf\u4e0e\u6700\u5148\u8fdb\u7684\u7b49\u53d8\u6269\u6563\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5bf9\u9f50\u7684\u6f5c\u5728\u7a7a\u95f4\u5e76\u91c7\u7528\u975e\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6837\u672c\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad83D\u5206\u5b50\u751f\u6210\u7684\u6548\u7387\u3002"}}
{"id": "2506.10753", "pdf": "https://arxiv.org/pdf/2506.10753", "abs": "https://arxiv.org/abs/2506.10753", "authors": ["Adam Ishay", "Zhun Yang", "Joohyung Lee", "Ilgu Kang", "Dongjae Lim"], "title": "Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering", "categories": ["cs.AI"], "comment": "In Proceedings the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2024)", "summary": "Causal and temporal reasoning about video dynamics is a challenging problem.\nWhile neuro-symbolic models that combine symbolic reasoning with neural-based\nperception and prediction have shown promise, they exhibit limitations,\nespecially in answering counterfactual questions. This paper introduces a\nmethod to enhance a neuro-symbolic model for counterfactual reasoning,\nleveraging symbolic reasoning about causal relations among events. We define\nthe notion of a causal graph to represent such relations and use Answer Set\nProgramming (ASP), a declarative logic programming method, to find how to\ncoordinate perception and simulation modules. We validate the effectiveness of\nour approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves\nstate-of-the-art performance on the CLEVRER challenge, significantly\noutperforming existing models. In the case of the CRAFT benchmark, we leverage\na large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a\ndynamics simulator. Our findings show that this method can further improve its\nperformance on counterfactual questions by providing alternative prompts\ninstructed by symbolic causal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u795e\u7ecf-\u7b26\u53f7\u6a21\u578b\u4ee5\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u56e0\u679c\u56fe\u6765\u8868\u793a\u4e8b\u4ef6\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b(ASP)\u534f\u8c03\u611f\u77e5\u548c\u6a21\u62df\u6a21\u5757\u3002\u5728CLEVRER\u548cCRAFT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728CLEVRER\u6311\u6218\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "motivation": "\u795e\u7ecf-\u7b26\u53f7\u6a21\u578b\u7ed3\u5408\u4e86\u7b26\u53f7\u63a8\u7406\u4e0e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u611f\u77e5\u548c\u9884\u6d4b\uff0c\u5728\u5904\u7406\u89c6\u9891\u52a8\u6001\u4e2d\u7684\u56e0\u679c\u548c\u65f6\u95f4\u63a8\u7406\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u56de\u7b54\u53cd\u4e8b\u5b9e\u95ee\u9898\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u589e\u5f3a\u795e\u7ecf-\u7b26\u53f7\u6a21\u578b\u7528\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4e8b\u4ef6\u4e4b\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u7b26\u53f7\u63a8\u7406\u3002\u5b9a\u4e49\u4e86\u56e0\u679c\u56fe\u7684\u6982\u5ff5\u6765\u8868\u793a\u8fd9\u4e9b\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u58f0\u660e\u6027\u7684\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\u2014\u2014\u7b54\u6848\u96c6\u7f16\u7a0b(ASP)\uff0c\u6765\u5bfb\u627e\u5982\u4f55\u534f\u8c03\u611f\u77e5\u548c\u6a21\u62df\u6a21\u5757\u7684\u65b9\u5f0f\u3002\u5bf9\u4e8eCRAFT\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd8\u5229\u7528\u4e86\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-3.5\u548cGPT-4\uff09\u4f5c\u4e3a\u52a8\u529b\u5b66\u6a21\u62df\u5668\u7684\u4ee3\u7406\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5CLEVRER\u548cCRAFT\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728CLEVRER\u6311\u6218\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u7531\u7b26\u53f7\u56e0\u679c\u63a8\u7406\u6307\u5bfc\u7684\u66ff\u4ee3\u63d0\u793a\uff0c\u8be5\u65b9\u6cd5\u8fd8\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u5728\u53cd\u4e8b\u5b9e\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u795e\u7ecf-\u7b26\u53f7\u6a21\u578b\u5728\u89c6\u9891\u52a8\u6001\u56e0\u679c\u53ca\u53cd\u4e8b\u5b9e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.10189", "pdf": "https://arxiv.org/pdf/2506.10189", "abs": "https://arxiv.org/abs/2506.10189", "authors": ["Mohammad Subhi Al-Batah", "Muhyeeddin Alqaraleh", "Mowafaq Salem Alzboon"], "title": "Improving Oral Cancer Outcomes Through Machine Learning and Dimensionality Reduction", "categories": ["cs.LG"], "comment": null, "summary": "Oral cancer presents a formidable challenge in oncology, necessitating early\ndiagnosis and accurate prognosis to enhance patient survival rates. Recent\nadvancements in machine learning and data mining have revolutionized\ntraditional diagnostic methodologies, providing sophisticated and automated\ntools for differentiating between benign and malignant oral lesions. This study\npresents a comprehensive review of cutting-edge data mining methodologies,\nincluding Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines\n(SVM), and ensemble learning techniques, specifically applied to the diagnosis\nand prognosis of oral cancer. Through a rigorous comparative analysis, our\nfindings reveal that Neural Networks surpass other models, achieving an\nimpressive classification accuracy of 93,6 % in predicting oral cancer.\nFurthermore, we underscore the potential benefits of integrating feature\nselection and dimensionality reduction techniques to enhance model performance.\nThese insights underscore the significant promise of advanced data mining\ntechniques in bolstering early detection, optimizing treatment strategies, and\nultimately improving patient outcomes in the realm of oral oncology.", "AI": {"tldr": "\u7814\u7a76\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u548c\u6570\u636e\u6316\u6398\u65b9\u6cd5\u5728\u53e3\u8154\u764c\u8bca\u65ad\u548c\u9884\u540e\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u9884\u6d4b\u53e3\u8154\u764c\u65b9\u9762\u5177\u6709\u6700\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u738793.6%\uff0c\u5e76\u5f3a\u8c03\u4e86\u7279\u5f81\u9009\u62e9\u548c\u964d\u7ef4\u6280\u672f\u5bf9\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u5728\u76ca\u5904\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u53e3\u8154\u764c\u60a3\u8005\u7684\u751f\u5b58\u7387\uff0c\u9700\u8981\u65e9\u671f\u8bca\u65ad\u548c\u51c6\u786e\u9884\u540e\u3002\u6700\u8fd1\uff0c\u673a\u5668\u5b66\u4e60\u548c\u6570\u636e\u6316\u6398\u7684\u8fdb\u6b65\u4e3a\u533a\u5206\u826f\u6027\u548c\u6076\u6027\u53e3\u8154\u75c5\u53d8\u63d0\u4f9b\u4e86\u66f4\u52a0\u7cbe\u7ec6\u548c\u81ea\u52a8\u5316\u7684\u5de5\u5177\u3002", "method": "\u672c\u7814\u7a76\u56de\u987e\u4e86\u5305\u62ec\u795e\u7ecf\u7f51\u7edc\u3001K-\u6700\u8fd1\u90bb\uff08KNN\uff09\u3001\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u548c\u96c6\u6210\u5b66\u4e60\u6280\u672f\u5728\u5185\u7684\u6700\u65b0\u6570\u636e\u6316\u6398\u65b9\u6cd5\uff0c\u5e76\u4e13\u95e8\u5e94\u7528\u4e8e\u53e3\u8154\u764c\u7684\u8bca\u65ad\u548c\u9884\u540e\u3002\u901a\u8fc7\u4e25\u683c\u7684\u6bd4\u8f83\u5206\u6790\u6765\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u9884\u6d4b\u53e3\u8154\u764c\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230\u4e8693.6%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6307\u51fa\u7ed3\u5408\u7279\u5f81\u9009\u62e9\u548c\u964d\u7ef4\u6280\u672f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5148\u8fdb\u7684\u6570\u636e\u6316\u6398\u6280\u672f\u5728\u52a0\u5f3a\u65e9\u671f\u68c0\u6d4b\u3001\u4f18\u5316\u6cbb\u7597\u7b56\u7565\u4ee5\u53ca\u6700\u7ec8\u6539\u5584\u53e3\u8154\u80bf\u7624\u5b66\u9886\u57df\u7684\u60a3\u8005\u7ed3\u679c\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.10764", "pdf": "https://arxiv.org/pdf/2506.10764", "abs": "https://arxiv.org/abs/2506.10764", "authors": ["Xiaozhe Li", "Jixuan Chen", "Xinyu Fang", "Shengyuan Ding", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities in solving\ndiverse tasks. However, their proficiency in iteratively optimizing complex\nsolutions through learning from previous feedback remains insufficiently\nexplored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark\ndesigned to evaluate LLM agents on large-scale search space optimization\nproblems. OPT-BENCH includes 20 real-world machine learning tasks sourced from\nKaggle and 10 classical NP problems, offering a diverse and challenging\nenvironment for assessing LLM agents on iterative reasoning and solution\nrefinement. To enable rigorous evaluation, we introduce OPT-Agent, an\nend-to-end optimization framework that emulates human reasoning when tackling\ncomplex problems by generating, validating, and iteratively improving solutions\nthrough leveraging historical feedback. Through extensive experiments on 9\nstate-of-the-art LLMs from 6 model families, we analyze the effects of\noptimization iterations, temperature settings, and model architectures on\nsolution quality and convergence. Our results demonstrate that incorporating\nhistorical context significantly enhances optimization performance across both\nML and NP tasks. All datasets, code, and evaluation tools are open-sourced to\npromote further research in advancing LLM-driven optimization and iterative\nreasoning. Project page:\n\\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OPT-BENCH\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u641c\u7d22\u7a7a\u95f4\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u4e0d\u540c\u8fed\u4ee3\u6b21\u6570\u3001\u6e29\u5ea6\u8bbe\u7f6e\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u6536\u655b\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0\u5386\u53f2\u53cd\u9988\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u5404\u79cd\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u901a\u8fc7\u4ece\u524d\u9988\u4e2d\u5b66\u4e60\u6765\u8fed\u4ee3\u4f18\u5316\u590d\u6742\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aOPT-BENCH\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b20\u4e2a\u6765\u81eaKaggle\u7684\u771f\u5b9e\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u548c10\u4e2a\u7ecf\u5178\u7684NP\u95ee\u9898\u3002\u540c\u65f6\u5f00\u53d1\u4e86OPT-Agent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6a21\u4eff\u4eba\u7c7b\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u751f\u6210\u3001\u9a8c\u8bc1\u5e76\u5229\u7528\u5386\u53f2\u53cd\u9988\u9010\u6b65\u6539\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u548cNP\u95ee\u9898\u4e0a\uff0c\u7ed3\u5408\u5386\u53f2\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u9ad8\u4e86\u4f18\u5316\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u4f18\u5316\u8fed\u4ee3\u6b21\u6570\u3001\u6e29\u5ea6\u8bbe\u7f6e\u4ee5\u53ca\u6a21\u578b\u67b6\u6784\u7b49\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u89e3\u7684\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u63a8\u52a8LLM\u9a71\u52a8\u7684\u4f18\u5316\u548c\u8fed\u4ee3\u63a8\u7406\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u6240\u6709\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u8bc4\u4f30\u5de5\u5177\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.10200", "pdf": "https://arxiv.org/pdf/2506.10200", "abs": "https://arxiv.org/abs/2506.10200", "authors": ["Tina Behrouzi", "Sana Tonekaboni", "Rahul G. Krishnan", "Anna Goldenberg"], "title": "DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection", "categories": ["cs.LG"], "comment": null, "summary": "Real-world observational data often contain existing or emerging\nheterogeneous subpopulations that deviate from global patterns. The majority of\nmodels tend to overlook these underrepresented groups, leading to inaccurate or\neven harmful predictions. Existing solutions often rely on detecting these\nsamples as Out-of-domain (OOD) rather than adapting the model to new emerging\npatterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational\nAutoencoder framework that jointly performs representation learning and\nadaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with\nthe data by dynamically updating its latent structure to capture new trends. It\nleverages a novel non-parametric clustering mechanism, inspired by Gaussian\nMixture Models, to discover and model latent subgroups based on embedding\nsimilarity. Extensive experiments show that DynaSubVAE achieves competitive\nperformance in both near-OOD and far-OOD detection, and excels in class-OOD\nscenarios where an entire class is missing during training. We further\nillustrate that our dynamic subgrouping mechanism outperforms standalone\nclustering methods such as GMM and KMeans++ in terms of both OOD accuracy and\nregret precision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDynaSubVAE\u7684\u52a8\u6001\u5b50\u7fa4\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5b83\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u8868\u793a\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7684OOD\u68c0\u6d4b\u3002\u8be5\u6a21\u578b\u80fd\u968f\u7740\u6570\u636e\u7684\u53d8\u5316\u52a8\u6001\u66f4\u65b0\u5176\u6f5c\u5728\u7ed3\u6784\u4ee5\u6355\u6349\u65b0\u8d8b\u52bf\uff0c\u5e76\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u975e\u53c2\u6570\u805a\u7c7b\u673a\u5236\u6765\u53d1\u73b0\u548c\u5efa\u6a21\u57fa\u4e8e\u5d4c\u5165\u76f8\u4f3c\u6027\u7684\u6f5c\u5728\u5b50\u7fa4\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDynaSubVAE\u5728\u8fd1OOD\u548c\u8fdcOOD\u68c0\u6d4b\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u65f6\u6574\u4e2a\u7c7b\u522b\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u3002\u6b64\u5916\uff0c\u5176\u52a8\u6001\u5b50\u7fa4\u673a\u5236\u5728OOD\u51c6\u786e\u6027\u548c\u9057\u61be\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8eGMM\u548cKMeans++\u7b49\u72ec\u7acb\u805a\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c2\u5bdf\u6570\u636e\u5f80\u5f80\u5305\u542b\u5df2\u5b58\u5728\u6216\u6b63\u5728\u51fa\u73b0\u7684\u5f02\u8d28\u5b50\u7fa4\u4f53\uff0c\u8fd9\u4e9b\u5b50\u7fa4\u4f53\u504f\u79bb\u4e86\u5168\u5c40\u6a21\u5f0f\u3002\u5927\u591a\u6570\u6a21\u578b\u503e\u5411\u4e8e\u5ffd\u7565\u8fd9\u4e9b\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7fa4\u4f53\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u751a\u81f3\u6709\u5bb3\u3002\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u662f\u5c06\u8fd9\u4e9b\u6837\u672c\u8bc6\u522b\u4e3a\u57df\u5916\uff08OOD\uff09\u800c\u4e0d\u662f\u8ba9\u6a21\u578b\u9002\u5e94\u65b0\u7684\u51fa\u73b0\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86DynaSubVAE\uff0c\u8fd9\u662f\u4e00\u79cd\u52a8\u6001\u5b50\u7fa4\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u8868\u5f81\u5b66\u4e60\u548c\u81ea\u9002\u5e94OOD\u68c0\u6d4b\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\u7684\u662f\uff0cDynaSubVAE\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u5176\u6f5c\u5728\u7ed3\u6784\u6765\u968f\u6570\u636e\u6f14\u53d8\u5e76\u6355\u83b7\u65b0\u8d8b\u52bf\u3002\u5b83\u91c7\u7528\u4e86\u4e00\u79cd\u53d7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u542f\u53d1\u7684\u65b0\u9896\u975e\u53c2\u6570\u805a\u7c7b\u673a\u5236\uff0c\u6839\u636e\u5d4c\u5165\u76f8\u4f3c\u6027\u6765\u53d1\u73b0\u548c\u5efa\u6a21\u6f5c\u5728\u5b50\u7fa4\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDynaSubVAE\u5728\u8fd1OOD\u548c\u8fdcOOD\u68c0\u6d4b\u65b9\u9762\u90fd\u80fd\u8fbe\u5230\u7ade\u4e89\u6027\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u67d0\u4e2a\u7c7b\u522b\u5b8c\u5168\u7f3a\u5931\u7684\u7c7bOOD\u573a\u666f\u4e2d\u5c24\u4e3a\u51fa\u8272\u3002\u8fdb\u4e00\u6b65\u8bf4\u660e\u4e86\u6211\u4eec\u7684\u52a8\u6001\u5b50\u7fa4\u673a\u5236\u5728OOD\u51c6\u786e\u5ea6\u548c\u9057\u61be\u7cbe\u5ea6\u4e0a\u8d85\u8fc7\u4e86\u50cfGMM\u548cKMeans++\u8fd9\u6837\u7684\u72ec\u7acb\u805a\u7c7b\u65b9\u6cd5\u3002", "conclusion": "DynaSubVAE\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u73af\u5883\u4e2d\u8bc6\u522b\u51fa\u672a\u88ab\u5145\u5206\u4ee3\u8868\u7684\u5b50\u7fa4\u4f53\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u8c03\u6574\u5176\u5185\u90e8\u7ed3\u6784\u6765\u63d0\u9ad8OOD\u68c0\u6d4b\u6027\u80fd\u3002\u8fd9\u79cd\u52a8\u6001\u5b50\u7fa4\u673a\u5236\u8bc1\u660e\u4e86\u6bd4\u4f20\u7edf\u7684\u56fa\u5b9a\u7ed3\u6784\u6a21\u578b\u66f4\u6709\u6548\u3002"}}
{"id": "2506.10853", "pdf": "https://arxiv.org/pdf/2506.10853", "abs": "https://arxiv.org/abs/2506.10853", "authors": ["Yu Zhang", "Yang Hu", "De Wang"], "title": "A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Human spatiotemporal behavior simulation is critical for urban planning\nresearch, yet traditional rule-based and statistical approaches suffer from\nhigh computational costs, limited generalizability, and poor scalability. While\nlarge language models (LLMs) show promise as \"world simulators,\" they face\nchallenges in spatiotemporal reasoning including limited spatial cognition,\nlack of physical constraint understanding, and group homogenization tendencies.\nThis paper introduces a framework integrating chain-of-thought (CoT) reasoning\nwith Model Context Protocol (MCP) to enhance LLMs' capability in simulating\nspatiotemporal behaviors that correspond with validation data patterns. The\nmethodology combines human-like progressive reasoning through a five-stage\ncognitive framework with comprehensive data processing via six specialized MCP\ntool categories: temporal management, spatial navigation, environmental\nperception, personal memory, social collaboration, and experience evaluation.\nExperiments in Shanghai's Lujiazui district validate the framework's\neffectiveness across 1,000 generated samples. Results demonstrate high\nsimilarity with real mobile signaling data, achieving generation quality scores\nof 7.86 to 8.36 across different base models. Parallel processing experiments\nshow efficiency improvements, with generation times decreasing from 1.30 to\n0.17 minutes per sample when scaling from 2 to 12 processes. This work\ncontributes to integrating CoT reasoning with MCP for urban behavior modeling,\nadvancing LLMs applications in urban computing and providing a practical\napproach for synthetic mobility data generation. The framework offers a\nfoundation for smart city planning, transportation forecasting, and\nparticipatory urban design applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u7a7a\u884c\u4e3a\u6a21\u62df\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4e0a\u6d77\u9646\u5bb6\u5634\u5730\u533a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u548c\u7edf\u8ba1\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff1b\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6709\u6f5c\u529b\u4f5c\u4e3a\u201c\u4e16\u754c\u6a21\u62df\u5668\u201d\uff0c\u4f46\u5728\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5982\u7a7a\u95f4\u8ba4\u77e5\u6709\u9650\u3001\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\u7406\u89e3\u4ee5\u53ca\u7fa4\u4f53\u540c\u8d28\u5316\u503e\u5411\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u96c6\u6210\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u4e0e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u6846\u67b6\u6765\u63d0\u9ad8LLMs\u5728\u6a21\u4eff\u7b26\u5408\u9a8c\u8bc1\u6570\u636e\u6a21\u5f0f\u7684\u65f6\u7a7a\u884c\u4e3a\u65b9\u9762\u7684\u80fd\u529b\u3002\u65b9\u6cd5\u8bba\u7ed3\u5408\u4e86\u4e94\u9636\u6bb5\u8ba4\u77e5\u6846\u67b6\u7684\u4eba\u7c7b\u5f0f\u6e10\u8fdb\u63a8\u7406\u4e0e\u901a\u8fc7\u516d\u4e2a\u4e13\u4e1aMCP\u5de5\u5177\u7c7b\u522b\u8fdb\u884c\u7684\u5168\u9762\u6570\u636e\u5904\u7406\uff1a\u65f6\u95f4\u7ba1\u7406\u3001\u7a7a\u95f4\u5bfc\u822a\u3001\u73af\u5883\u611f\u77e5\u3001\u4e2a\u4eba\u8bb0\u5fc6\u3001\u793e\u4f1a\u534f\u4f5c\u548c\u7ecf\u9a8c\u8bc4\u4f30\u3002", "result": "\u5728\u4e0a\u6d77\u9646\u5bb6\u5634\u5730\u533a\u8fdb\u884c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u901a\u8fc7\u5bf91,000\u4e2a\u751f\u6210\u6837\u672c\u7684\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u4ea7\u751f\u4e0e\u771f\u5b9e\u79fb\u52a8\u4fe1\u53f7\u6570\u636e\u9ad8\u5ea6\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u5f97\u5206\u57287.86\u81f38.36\u4e4b\u95f4\u3002\u5e76\u884c\u5904\u7406\u5b9e\u9a8c\u663e\u793a\u6548\u7387\u6709\u6240\u63d0\u9ad8\uff0c\u5f53\u4ece2\u4e2a\u8fdb\u7a0b\u6269\u5c55\u523012\u4e2a\u8fdb\u7a0b\u65f6\uff0c\u6bcf\u4e2a\u6837\u672c\u7684\u751f\u6210\u65f6\u95f4\u4ece1.30\u5206\u949f\u51cf\u5c11\u5230\u4e860.17\u5206\u949f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u57ce\u5e02\u884c\u4e3a\u5efa\u6a21\u6574\u5408\u4e86CoT\u63a8\u7406\u4e0eMCP\uff0c\u63a8\u52a8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e3a\u5408\u6210\u79fb\u52a8\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u4e3a\u667a\u6167\u57ce\u5e02\u5efa\u8bbe\u3001\u4ea4\u901a\u9884\u6d4b\u53ca\u53c2\u4e0e\u5f0f\u57ce\u5e02\u8bbe\u8ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.10205", "pdf": "https://arxiv.org/pdf/2506.10205", "abs": "https://arxiv.org/abs/2506.10205", "authors": ["Jing Liu", "Toshiaki Koike-Akino", "Ye Wang", "Hassan Mansour", "Matthew Brand"], "title": "AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent", "categories": ["cs.LG"], "comment": "ICML 2025 workshop on Efficient Systems for Foundation Models", "summary": "To address the enormous size of Large Language Models (LLMs), model\ncompression methods, such as quantization and pruning, are often deployed,\nespecially on edge devices. In this work, we focus on layer-wise post-training\nquantization and pruning. Drawing connections between activation-aware weight\npruning and sparse approximation problems, and motivated by the success of\nIterative Hard Thresholding (IHT), we propose a unified method for\nActivation-aware Weight pruning and quantization via Projected gradient descent\n(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM\npruning and quantization methods. Theoretical convergence guarantees of the\nproposed method for pruning are also provided.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAWP\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6fc0\u6d3b\u611f\u77e5\u6743\u91cd\u526a\u679d\u548c\u91cf\u5316\uff0c\u901a\u8fc7\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAWP\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u526a\u679d\u548c\u91cf\u5316\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u6027\u4fdd\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c4\u6a21\u5e9e\u5927\u5e26\u6765\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u9700\u8981\u4f7f\u7528\u8bf8\u5982\u91cf\u5316\u548c\u526a\u679d\u7b49\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002\u672c\u6587\u7279\u522b\u5173\u6ce8\u8bad\u7ec3\u540e\u7684\u9010\u5c42\u91cf\u5316\u548c\u526a\u679d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u79f0\u4e3aAWP\uff08Activation-aware Weight pruning\uff09\u7684\u65b9\u6cd5\uff0c\u5b83\u662f\u4e00\u4e2a\u57fa\u4e8e\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u6fc0\u6d3b\u611f\u77e5\u7684\u6743\u91cd\u526a\u679d\u548c\u91cf\u5316\u3002\u8fd9\u4e00\u65b9\u6cd5\u53d7\u5230\u8fed\u4ee3\u786c\u9608\u503c\uff08IHT\uff09\u6210\u529f\u7684\u542f\u53d1\uff0c\u5e76\u4e14\u4e0e\u7a00\u758f\u903c\u8fd1\u95ee\u9898\u76f8\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAWP\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u526a\u679d\u548c\u91cf\u5316\u65b9\u9762\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6240\u63d0\u526a\u679d\u65b9\u6cd5\u7684\u7406\u8bba\u6536\u655b\u6027\u4fdd\u969c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684AWP\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6027\u80fd\u3002\u901a\u8fc7\u7ed3\u5408\u6fc0\u6d3b\u611f\u77e5\u6743\u91cd\u526a\u679d\u548c\u91cf\u5316\uff0cAWP\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6548\u679c\uff0c\u5e76\u4e14\u5177\u5907\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2506.10897", "pdf": "https://arxiv.org/pdf/2506.10897", "abs": "https://arxiv.org/abs/2506.10897", "authors": ["Daniel Borrajo", "Giuseppe Canonaco", "Tom\u00e1s de la Rosa", "Alfredo Garrach\u00f3n", "Sriram Gopalakrishnan", "Simerjot Kaur", "Marianela Morales", "Sunandita Patra", "Alberto Pozanco", "Keshav Ramani", "Charese Smiley", "Pietro Totis", "Manuela Veloso"], "title": "GenPlanX. Generation of Plans and Execution", "categories": ["cs.AI"], "comment": null, "summary": "Classical AI Planning techniques generate sequences of actions for complex\ntasks. However, they lack the ability to understand planning tasks when\nprovided using natural language. The advent of Large Language Models (LLMs) has\nintroduced novel capabilities in human-computer interaction. In the context of\nplanning tasks, LLMs have shown to be particularly good in interpreting human\nintents among other uses. This paper introduces GenPlanX that integrates LLMs\nfor natural language-based description of planning tasks, with a classical AI\nplanning engine, alongside an execution and monitoring framework. We\ndemonstrate the efficacy of GenPlanX in assisting users with office-related\ntasks, highlighting its potential to streamline workflows and enhance\nproductivity through seamless human-AI collaboration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aGenPlanX\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u7406\u89e3\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u89c4\u5212\u4efb\u52a1\uff0c\u5e76\u4e0e\u7ecf\u5178\u7684\u4eba\u5de5\u667a\u80fd\u89c4\u5212\u5f15\u64ce\u53ca\u6267\u884c\u548c\u76d1\u63a7\u6846\u67b6\u96c6\u6210\u3002\u901a\u8fc7\u8f85\u52a9\u529e\u516c\u76f8\u5173\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u63d0\u9ad8\u751f\u4ea7\u529b\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u667a\u80fd\u89c4\u5212\u6280\u672f\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u65e0\u6cd5\u7406\u89e3\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7ed9\u51fa\u7684\u4efb\u52a1\u8bf4\u660e\u3002\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0c\u5b83\u4eec\u5728\u89e3\u8bfb\u4eba\u7c7b\u610f\u56fe\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u7279\u522b\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u5b58\u5728\u4e00\u79cd\u9700\u6c42\uff0c\u5373\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u5904\u7406\u80fd\u529b\u4e0e\u7ecf\u5178AI\u89c4\u5212\u76f8\u7ed3\u5408\u7684\u7cfb\u7edf\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u673a\u4e4b\u95f4\u7684\u65e0\u7f1d\u534f\u4f5c\u3002", "method": "\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aGenPlanX\u7684\u7cfb\u7edf\uff0c\u5b83\u6574\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u5f62\u5f0f\u7684\u89c4\u5212\u4efb\u52a1\uff0c\u5e76\u4e14\u4e0e\u4f20\u7edf\u7684AI\u89c4\u5212\u5f15\u64ce\u76f8\u8fde\u63a5\u3002\u6b64\u5916\uff0cGenPlanX\u8fd8\u5305\u542b\u4e86\u4e00\u4e2a\u6267\u884c\u548c\u76d1\u63a7\u67b6\u6784\u3002", "result": "GenPlanX\u88ab\u8bc1\u660e\u80fd\u591f\u5728\u5e2e\u52a9\u7528\u6237\u5b8c\u6210\u529e\u516c\u76f8\u5173\u4efb\u52a1\u65b9\u9762\u6709\u6548\uff0c\u8fd9\u8868\u660e\u8be5\u7cfb\u7edf\u6709\u6f5c\u529b\u901a\u8fc7\u7b80\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u589e\u5f3a\u7684\u751f\u4ea7\u6548\u7387\u6765\u6539\u5584\u4eba\u4e0eAI\u7684\u5408\u4f5c\u3002", "conclusion": "GenPlanX\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7ecf\u5178AI\u89c4\u5212\u5f15\u64ce\u4ee5\u53ca\u6267\u884c\u76d1\u63a7\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u89c4\u5212\u4efb\u52a1\u3002\u8fd9\u79cd\u96c6\u6210\u65b9\u6cd5\u6709\u671b\u5728\u591a\u79cd\u573a\u666f\u4e0b\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u548c\u4eba\u673a\u534f\u4f5c\u6c34\u5e73\u3002"}}
{"id": "2506.10212", "pdf": "https://arxiv.org/pdf/2506.10212", "abs": "https://arxiv.org/abs/2506.10212", "authors": ["Sajjad Karimi", "Amit J. Shah", "Gari D. Clifford", "Reza Sameni"], "title": "Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a\ncomprehensive, multimodal perspective on cardiac function by capturing the\nheart's electrical and mechanical activities, respectively. However, the\ndistinct and overlapping information content of these signals, as well as their\npotential for mutual reconstruction and biomarker extraction, remains\nincompletely understood, especially under varying physiological conditions and\nacross individuals.\n  In this study, we systematically investigate the common and exclusive\ncharacteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous\nECG-PCG recordings during rest and exercise. We employ a suite of linear and\nnonlinear machine learning models, including non-causal LSTM networks, to\nreconstruct each modality from the other and analyze the influence of\ncausality, physiological state, and cross-subject variability. Our results\ndemonstrate that nonlinear models, particularly non-causal LSTM, provide\nsuperior reconstruction performance, with reconstructing ECG from PCG proving\nmore tractable than the reverse. Exercise and cross-subject scenarios present\nsignificant challenges, but envelope-based modeling that utilizes instantaneous\namplitude features substantially improves cross-subject generalizability for\ncross-modal learning. Furthermore, we demonstrate that clinically relevant ECG\nbiomarkers, such as fiducial points and QT intervals, can be estimated from PCG\nin cross-subject settings.\n  These findings advance our understanding of the relationship between\nelectromechanical cardiac modalities, in terms of both waveform characteristics\nand the timing of cardiac events, with potential applications in novel\nmultimodal cardiac monitoring technologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528EPHNOGRAM\u6570\u636e\u96c6\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u5fc3\u7535\u56fe(ECG)\u548c\u5fc3\u97f3\u56fe(PCG)\u7684\u5171\u540c\u4e0e\u72ec\u7279\u7279\u5f81\uff0c\u901a\u8fc7\u591a\u79cd\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u5b9e\u73b0\u4e24\u8005\u7684\u4e92\u6784\uff0c\u5e76\u4e14\u5c55\u793a\u4e86\u5982\u4f55\u4ecePCG\u4f30\u8ba1ECG\u751f\u7269\u6807\u5fd7\u7269\u3002\u7814\u7a76\u8868\u660e\u975e\u56e0\u679cLSTM\u7f51\u7edc\u5728\u91cd\u5efa\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728\u8de8\u4e2a\u4f53\u60c5\u51b5\u4e0b\u5229\u7528\u77ac\u65f6\u5e45\u5ea6\u7279\u5f81\u7684\u5305\u7edc\u5efa\u6a21\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u540c\u6b65\u8bb0\u5f55\u7684\u5fc3\u7535\u56fe\u548c\u5fc3\u97f3\u56fe\u63d0\u4f9b\u4e86\u5fc3\u810f\u7535\u6d3b\u52a8\u548c\u673a\u68b0\u6d3b\u52a8\u7684\u5168\u9762\u591a\u6a21\u6001\u89c6\u89d2\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u4e2d\u72ec\u7279\u7684\u3001\u91cd\u53e0\u7684\u4fe1\u606f\u5185\u5bb9\u4ee5\u53ca\u5b83\u4eec\u76f8\u4e92\u91cd\u5efa\u548c\u63d0\u53d6\u751f\u7269\u6807\u5fd7\u7269\u7684\u53ef\u80fd\u6027\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u751f\u7406\u6761\u4ef6\u548c\u4e2a\u4f53\u4e4b\u95f4\u3002", "method": "\u7814\u7a76\u8005\u4eec\u4f7f\u7528\u4e86EPHNOGRAM\u6570\u636e\u96c6\u4e2d\u7684\u540c\u65f6\u671fECG-PCG\u8bb0\u5f55\uff0c\u5728\u4f11\u606f\u548c\u8fd0\u52a8\u72b6\u6001\u4e0b\u8fdb\u884c\u3002\u4ed6\u4eec\u8fd0\u7528\u4e86\u4e00\u7cfb\u5217\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u975e\u56e0\u679cLSTM\u7f51\u7edc\uff0c\u6765\u5c1d\u8bd5\u4ece\u4e00\u79cd\u6a21\u5f0f\u91cd\u5efa\u53e6\u4e00\u79cd\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\u56e0\u679c\u5173\u7cfb\u3001\u751f\u7406\u72b6\u6001\u548c\u8de8\u4e2a\u4f53\u53d8\u5f02\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u975e\u7ebf\u6027\u6a21\u578b\u5c24\u5176\u662f\u975e\u56e0\u679cLSTM\u7f51\u7edc\u8868\u73b0\u51fa\u66f4\u597d\u7684\u91cd\u5efa\u6027\u80fd\uff1b\u4ecePCG\u91cd\u5efaECG\u6bd4\u53cd\u5411\u64cd\u4f5c\u66f4\u5bb9\u6613\u5b9e\u73b0\u3002\u8fd0\u52a8\u548c\u8de8\u4e2a\u4f53\u573a\u666f\u63d0\u51fa\u4e86\u663e\u8457\u6311\u6218\uff0c\u4f46\u662f\u57fa\u4e8e\u5305\u7edc\u5efa\u6a21\u5e76\u5229\u7528\u77ac\u65f6\u5e45\u5ea6\u7279\u5f81\u7684\u65b9\u6cd5\u5927\u5927\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u7684\u8de8\u4e2a\u4f53\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u8bf8\u5982\u6807\u8bb0\u70b9\u548cQT\u95f4\u9694\u7b49\u4e34\u5e8a\u4e0a\u76f8\u5173\u7684\u5fc3\u7535\u56fe\u751f\u7269\u6807\u5fd7\u7269\u53ef\u4ee5\u4ecePCG\u5728\u8de8\u4e2a\u4f53\u73af\u5883\u4e2d\u88ab\u4f30\u8ba1\u51fa\u6765\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u589e\u8fdb\u4e86\u6211\u4eec\u5bf9\u5fc3\u810f\u7535\u673a\u68b0\u6a21\u5f0f\u4e4b\u95f4\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u4e0d\u4ec5\u5728\u4e8e\u6ce2\u5f62\u7279\u6027\u65b9\u9762\uff0c\u4e5f\u5305\u62ec\u5fc3\u810f\u4e8b\u4ef6\u7684\u65f6\u95f4\u5b89\u6392\uff0c\u5e76\u53ef\u80fd\u5e94\u7528\u4e8e\u65b0\u578b\u591a\u6a21\u6001\u5fc3\u810f\u76d1\u6d4b\u6280\u672f\u3002"}}
{"id": "2506.10912", "pdf": "https://arxiv.org/pdf/2506.10912", "abs": "https://arxiv.org/abs/2506.10912", "authors": ["Fei Lin", "Ziyang Gong", "Cong Wang", "Yonglin Tian", "Tengchao Zhang", "Xue Yang", "Gen Luo", "Fei-Yue Wang"], "title": "Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Toxicity remains a leading cause of early-stage drug development failure.\nDespite advances in molecular design and property prediction, the task of\nmolecular toxicity repair - generating structurally valid molecular\nalternatives with reduced toxicity - has not yet been systematically defined or\nbenchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task\nfor general-purpose Multimodal Large Language Models (MLLMs) focused on\nmolecular toxicity repair. We construct a standardized dataset covering 11\nprimary tasks and 560 representative toxic molecules spanning diverse\nmechanisms and granularities. We design a prompt annotation pipeline with\nmechanism-aware and task-adaptive capabilities, informed by expert\ntoxicological knowledge. In parallel, we propose an automated evaluation\nframework, ToxiEval, which integrates toxicity endpoint prediction, synthetic\naccessibility, drug-likeness, and structural similarity into a high-throughput\nevaluation chain for repair success. We systematically assess nearly 30\nmainstream general-purpose MLLMs and design multiple ablation studies to\nanalyze key factors such as evaluation criteria, candidate diversity, and\nfailure attribution. Experimental results show that although current MLLMs\nstill face significant challenges on this task, they begin to demonstrate\npromising capabilities in toxicity understanding, semantic constraint\nadherence, and structure-aware molecule editing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ToxiMol\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u5206\u5b50\u6bd2\u6027\u4fee\u590d\u7684\u57fa\u51c6\u4efb\u52a1\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u800c\u8bbe\u8ba1\u3002\u5b83\u5305\u62ec\u4e00\u4e2a\u6807\u51c6\u5316\u6570\u636e\u96c6\u3001\u673a\u5236\u611f\u77e5\u7684\u4efb\u52a1\u9002\u5e94\u6027\u63d0\u793a\u6ce8\u89e3\u6d41\u7a0b\u4ee5\u53ca\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6ToxiEval\u3002\u5c3d\u7ba1\u5f53\u524d\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f46\u5df2\u663e\u793a\u51fa\u5728\u7406\u89e3\u6bd2\u6027\u3001\u9075\u5b88\u8bed\u4e49\u7ea6\u675f\u548c\u7ed3\u6784\u611f\u77e5\u5206\u5b50\u7f16\u8f91\u65b9\u9762\u6709\u5e0c\u671b\u7684\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u5206\u5b50\u6bd2\u6027\u4fee\u590d\u4efb\u52a1\u5c1a\u672a\u88ab\u7cfb\u7edf\u5b9a\u4e49\u6216\u57fa\u51c6\u5316\uff0c\u5bfc\u81f4\u65e9\u671f\u836f\u7269\u5f00\u53d1\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\u662f\u6bd2\u6027\u95ee\u9898\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6\u4efb\u52a1ToxiMol\u3002", "method": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b11\u4e2a\u4e3b\u8981\u4efb\u52a1\u548c560\u4e2a\u4ee3\u8868\u6027\u6709\u6bd2\u5206\u5b50\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u8bbe\u8ba1\u4e86\u7ed3\u5408\u4e13\u5bb6\u6bd2\u7406\u5b66\u77e5\u8bc6\u7684\u63d0\u793a\u6ce8\u89e3\u6d41\u7a0b\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToxiEval\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u53ef\u4ee5\u7efc\u5408\u8bc4\u4ef7\u6bd2\u6027\u7ec8\u70b9\u9884\u6d4b\u3001\u5408\u6210\u53ef\u8fbe\u6027\u3001\u7c7b\u836f\u6027\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u73b0\u6709\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u4ecd\u7136\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u4f46\u5b83\u4eec\u5df2\u7ecf\u5f00\u59cb\u5c55\u73b0\u51fa\u5bf9\u6bd2\u6027\u7406\u89e3\u3001\u8bed\u4e49\u7ea6\u675f\u9075\u5faa\u53ca\u7ed3\u6784\u610f\u8bc6\u5206\u5b50\u7f16\u8f91\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u867d\u7136\u76ee\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5b50\u6bd2\u6027\u4fee\u590d\u4efb\u52a1\u4e2d\u9047\u5230\u4e86\u4e0d\u5c11\u56f0\u96be\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5df2\u7ecf\u5f00\u59cb\u5c55\u793a\u51fa\u5bf9\u4e8e\u6bd2\u6027\u7406\u89e3\u3001\u6ee1\u8db3\u8bed\u4e49\u7ea6\u675f\u6761\u4ef6\u4ee5\u53ca\u8fdb\u884c\u7ed3\u6784\u654f\u611f\u7684\u5206\u5b50\u7f16\u8f91\u4e0a\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.10235", "pdf": "https://arxiv.org/pdf/2506.10235", "abs": "https://arxiv.org/abs/2506.10235", "authors": ["Chen-Chia Chang", "Wan-Hsuan Lin", "Yikang Shen", "Yiran Chen", "Xin Zhang"], "title": "LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": "Accepted at 42nd International Conference on Machine Learning (ICML)\n  2025", "summary": "Automation of analog topology design is crucial due to customized\nrequirements of modern applications with heavily manual engineering efforts.\nThe state-of-the-art work applies a sequence-to-sequence approach and\nsupervised finetuning on language models to generate topologies given user\nspecifications. However, its circuit formulation is inefficient due to O(|V |2)\ntoken length and suffers from low precision sensitivity to numeric inputs. In\nthis work, we introduce LaMAGIC2, a succinct float-input canonical formulation\nwith identifier (SFCI) for language model-based analog topology generation.\nSFCI addresses these challenges by improving component-type recognition through\nidentifier-based representations, reducing token length complexity to O(|V |),\nand enhancing numeric precision sensitivity for better performance under tight\ntolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher\nsuccess rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a\nprior method. LaMAGIC2 also exhibits better transferability for circuits with\nmore vertices with up to 58.5% improvement. These advancements establish\nLaMAGIC2 as a robust framework for analog topology generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u62df\u62d3\u6251\u751f\u6210\u65b9\u6cd5LaMAGIC2\uff0c\u901a\u8fc7\u6539\u8fdb\u7ec4\u4ef6\u7c7b\u578b\u8bc6\u522b\u3001\u51cf\u5c11token\u957f\u5ea6\u590d\u6742\u5ea6\u548c\u63d0\u9ad8\u6570\u503c\u7cbe\u5ea6\u654f\u611f\u6027\u6765\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLaMAGIC2\u5728\u4e25\u683c\u7684\u5bb9\u5dee\u4e0b\u6210\u529f\u7387\u63d0\u9ad8\u4e8634%\uff0c\u5e76\u4e14\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4MSE\u964d\u4f4e\u4e8610\u500d\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u65b9\u6cd5\u548c\u8bed\u8a00\u6a21\u578b\u76d1\u7763\u5fae\u8c03\u7684\u65b9\u6cd5\u5728\u7535\u8def\u516c\u5f0f\u5316\u65b9\u9762\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u5bf9\u6570\u503c\u8f93\u5165\u7684\u7cbe\u5ea6\u654f\u611f\u6027\u4f4e\u3002", "method": "\u5f15\u5165\u4e86LaMAGIC2\uff08SFCI\uff09\uff0c\u4e00\u79cd\u7b80\u6d01\u7684\u6d6e\u70b9\u8f93\u5165\u6807\u51c6\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5e26\u6709\u6807\u8bc6\u7b26\u8868\u793a\u6cd5\u4ee5\u6539\u5584\u7ec4\u4ef6\u7c7b\u578b\u8bc6\u522b\uff0c\u5c06token\u957f\u5ea6\u590d\u6742\u5ea6\u964d\u4f4e\u81f3O(|V|)\uff0c\u5e76\u589e\u5f3a\u4e86\u6570\u503c\u7cbe\u5ea6\u654f\u611f\u6027\u3002", "result": "LaMAGIC2\u5728\u4e25\u683c\u5bb9\u5dee\u6761\u4ef6\u4e0b\u6210\u529f\u7387\u8fbe\u523034%\u7684\u63d0\u5347\uff0c\u5e76\u4e14MSE\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u51cf\u5c11\u4e8610\u500d\u3002\u5bf9\u4e8e\u5177\u6709\u66f4\u591a\u9876\u70b9\u7684\u7535\u8def\uff0cLaMAGIC2\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8fc1\u79fb\u6027\uff0c\u63d0\u5347\u4e86\u9ad8\u8fbe58.5%\u3002", "conclusion": "LaMAGIC2\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6a21\u62df\u62d3\u6251\u751f\u6210\u3002"}}
{"id": "2506.10947", "pdf": "https://arxiv.org/pdf/2506.10947", "abs": "https://arxiv.org/abs/2506.10947", "authors": ["Rulin Shao", "Shuyue Stella Li", "Rui Xin", "Scott Geng", "Yiping Wang", "Sewoong Oh", "Simon Shaolei Du", "Nathan Lambert", "Sewon Min", "Ranjay Krishna", "Yulia Tsvetkov", "Hannaneh Hajishirzi", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "Spurious Rewards: Rethinking Training Signals in RLVR", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit\nstrong mathematical reasoning in certain models even with spurious rewards that\nhave little, no, or even negative correlation with the correct answer. For\nexample, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute\npoints by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect\nlabel), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the\n29.1% gained with ground truth rewards. However, the spurious rewards that work\nfor Qwen often fail to yield gains with other model families like Llama3 or\nOLMo2. In particular, we find code reasoning -- thinking in code without actual\ncode execution -- to be a distinctive Qwen2.5-Math behavior that becomes\nsignificantly more frequent after RLVR, from 65% to over 90%, even with\nspurious rewards. Overall, we hypothesize that, given the lack of useful reward\nsignal, RLVR must somehow be surfacing useful reasoning representations learned\nduring pretraining, although the exact mechanism remains a topic for future\nwork. We suggest that future RLVR research should possibly be validated on\ndiverse models rather than a single de facto choice, as we show that it is easy\nto get significant performance gains on Qwen models even with completely\nspurious reward signals.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u5956\u52b1\u4fe1\u53f7\u4e0e\u6b63\u786e\u7b54\u6848\u51e0\u4e4e\u6ca1\u6709\u6216\u8d1f\u76f8\u5173\u7684\u60c5\u51b5\u4e0b\uff0c\u5e26\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4ecd\u80fd\u6fc0\u53d1\u67d0\u4e9b\u6a21\u578b\u7684\u5f3a\u5927\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u79cd\u73b0\u8c61\u5728Qwen2.5-Math-7B\u6a21\u578b\u4e2d\u5c24\u4e3a\u663e\u8457\uff0c\u4f46\u5728\u5176\u4ed6\u6a21\u578b\u5bb6\u65cf\u5982Llama3\u6216OLMo2\u4e2d\u5e76\u4e0d\u9002\u7528\u3002", "motivation": "\u7814\u7a76\u4eba\u5458\u60f3\u8981\u63a2\u7d22\u5728\u5956\u52b1\u4fe1\u53f7\u4e0d\u53ef\u9760\u751a\u81f3\u8bef\u5bfc\u7684\u60c5\u51b5\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u5426\u4ecd\u7136\u4fc3\u8fdb\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e86\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u865a\u5047\u5956\u52b1\u65f6\uff0cQwen2.5-Math-7B\u4e0e\u5176\u4ed6\u6a21\u578b\u5bb6\u65cf\u5728MATH-500\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u5956\u52b1\u4fe1\u53f7\u4e0d\u53ef\u9760\uff0c\u4f46RLVR\u80fd\u591f\u663e\u8457\u63d0\u9ad8Qwen2.5-Math-7B\u6a21\u578b\u5728MATH-500\u4e0a\u7684\u5f97\u5206\uff1b\u7136\u800c\uff0c\u76f8\u540c\u7684\u65b9\u6cd5\u5bf9\u5176\u4ed6\u6a21\u578b\u5bb6\u65cf\u6548\u679c\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u89c2\u5bdf\u5230Qwen2.5-Math-7B\u8868\u73b0\u51fa\u66f4\u9891\u7e41\u5730\u4ee5\u4ee3\u7801\u5f62\u5f0f\u8fdb\u884c\u63a8\u7406\u7684\u73b0\u8c61\u3002", "conclusion": "\u7814\u7a76\u63a8\u6d4b\uff0c\u5728\u7f3a\u4e4f\u6709\u7528\u5956\u52b1\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\uff0cRLVR\u53ef\u80fd\u6fc0\u6d3b\u4e86\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u5230\u7684\u6709\u6548\u63a8\u7406\u8868\u793a\u3002\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u8003\u8651\u5728\u66f4\u591a\u6837\u5316\u7684\u6a21\u578b\u4e0a\u9a8c\u8bc1RLVR\u7684\u6548\u679c\u3002"}}
{"id": "2506.10244", "pdf": "https://arxiv.org/pdf/2506.10244", "abs": "https://arxiv.org/abs/2506.10244", "authors": ["Yuji Kawamata", "Kaoru Kamijo", "Maki Kihira", "Akihiro Toyoda", "Tomoru Nakayama", "Akira Imakura", "Tetsuya Sakurai", "Yukihiko Okada"], "title": "A new type of federated clustering: A non-model-sharing approach", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, the growing need to leverage sensitive data across\ninstitutions has led to increased attention on federated learning (FL), a\ndecentralized machine learning paradigm that enables model training without\nsharing raw data. However, existing FL-based clustering methods, known as\nfederated clustering, typically assume simple data partitioning scenarios such\nas horizontal or vertical splits, and cannot handle more complex distributed\nstructures. This study proposes data collaboration clustering (DC-Clustering),\na novel federated clustering method that supports clustering over complex data\npartitioning scenarios where horizontal and vertical splits coexist. In\nDC-Clustering, each institution shares only intermediate representations\ninstead of raw data, ensuring privacy preservation while enabling collaborative\nclustering. The method allows flexible selection between k-means and spectral\nclustering, and achieves final results with a single round of communication\nwith the central server. We conducted extensive experiments using synthetic and\nopen benchmark datasets. The results show that our method achieves clustering\nperformance comparable to centralized clustering where all data are pooled.\nDC-Clustering addresses an important gap in current FL research by enabling\neffective knowledge discovery from distributed heterogeneous data. Its\npractical properties -- privacy preservation, communication efficiency, and\nflexibility -- make it a promising tool for privacy-sensitive domains such as\nhealthcare and finance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u805a\u7c7b\u65b9\u6cd5\u2014\u2014\u6570\u636e\u534f\u4f5c\u805a\u7c7b\uff08DC-Clustering\uff09\uff0c\u5b83\u652f\u6301\u5728\u590d\u6742\u7684\u6570\u636e\u5212\u5206\u573a\u666f\u4e0b\u8fdb\u884c\u805a\u7c7b\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u5e76\u5141\u8bb8\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u805a\u7c7b\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u5904\u7406\u7b80\u5355\u7684\u6570\u636e\u5212\u5206\u60c5\u51b5\uff0c\u5982\u6a2a\u5411\u6216\u7eb5\u5411\u5206\u5272\uff0c\u800c\u4e0d\u80fd\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u5206\u5e03\u5f0f\u7ed3\u6784\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u5bf9\u5206\u5e03\u5f0f\u5f02\u6784\u6570\u636e\u7684\u6709\u6548\u77e5\u8bc6\u53d1\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u6570\u636e\u534f\u4f5c\u805a\u7c7b\uff08DC-Clustering\uff09\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u673a\u6784\u95f4\u4ec5\u5171\u4eab\u4e2d\u95f4\u8868\u793a\u800c\u975e\u539f\u59cb\u6570\u636e\uff0c\u4ece\u800c\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\u4fc3\u8fdb\u534f\u4f5c\u805a\u7c7b\u3002\u6b64\u65b9\u6cd5\u53ef\u4ee5\u5728k-means\u548c\u8c31\u805a\u7c7b\u4e4b\u95f4\u7075\u6d3b\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u4e0e\u4e2d\u5fc3\u670d\u52a1\u5668\u7684\u4e00\u8f6e\u901a\u4fe1\u5b8c\u6210\u6700\u7ec8\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u5f00\u653e\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u4e0e\u96c6\u4e2d\u5f0f\u805a\u7c7b\u76f8\u5ab2\u7f8e\u7684\u805a\u7c7b\u6027\u80fd\uff0c\u5176\u4e2d\u6240\u6709\u6570\u636e\u90fd\u88ab\u6c47\u96c6\u8d77\u6765\u3002", "conclusion": "DC-\u805a\u7c7b\u586b\u8865\u4e86\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u7a7a\u767d\uff0c\u5373\u4ece\u5206\u5e03\u5f0f\u5f02\u6784\u6570\u636e\u4e2d\u6709\u6548\u53d1\u73b0\u77e5\u8bc6\u3002\u5176\u5b9e\u9645\u7279\u6027\u2014\u2014\u9690\u79c1\u4fdd\u62a4\u3001\u901a\u4fe1\u6548\u7387\u548c\u7075\u6d3b\u6027\u2014\u2014\u4f7f\u5176\u6210\u4e3a\u533b\u7597\u4fdd\u5065\u548c\u91d1\u878d\u7b49\u654f\u611f\u9886\u57df\u7684\u6709\u524d\u9014\u5de5\u5177\u3002"}}
{"id": "2506.10269", "pdf": "https://arxiv.org/pdf/2506.10269", "abs": "https://arxiv.org/abs/2506.10269", "authors": ["Ryota Ueda", "Takami Sato", "Ken Kobayashi", "Kazuhide Nakata"], "title": "Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification", "categories": ["cs.LG", "math.OC", "90C22, 68T05", "I.2.6; G.1.6"], "comment": "17 pages, 2 figures. Version revised after ICML 2025 reviews", "summary": "Semidefinite programming (SDP) relaxation has emerged as a promising approach\nfor neural network verification, offering tighter bounds than other convex\nrelaxation methods for deep neural networks (DNNs) with ReLU activations.\nHowever, we identify a critical limitation in the SDP relaxation when applied\nto deep networks: interior-point vanishing, which leads to the loss of strict\nfeasibility -- a crucial condition for the numerical stability and optimality\nof SDP. Through rigorous theoretical and empirical analysis, we demonstrate\nthat as the depth of DNNs increases, the strict feasibility is likely to be\nlost, creating a fundamental barrier to scaling SDP-based verification. To\naddress the interior-point vanishing, we design and investigate five solutions\nto enhance the feasibility conditions of the verification problem. Our methods\ncan successfully solve 88% of the problems that could not be solved by existing\nmethods, accounting for 41% of the total. Our analysis also reveals that the\nvalid constraints for the lower and upper bounds for each ReLU unit are\ntraditionally inherited from prior work without solid reasons, but are actually\nnot only unbeneficial but also even harmful to the problem's feasibility. This\nwork provides valuable insights into the fundamental challenges of SDP-based\nDNN verification and offers practical solutions to improve its applicability to\ndeeper neural networks, contributing to the development of more reliable and\nsecure systems with DNNs.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u4e86\u4f7f\u7528\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u677e\u5f1b\u65b9\u6cd5\u9a8c\u8bc1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65f6\u5b58\u5728\u7684\u5185\u90e8\u70b9\u6d88\u5931\u95ee\u9898\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6570\u503c\u7a33\u5b9a\u6027\u548c\u6700\u4f18\u6027\u7684\u91cd\u8981\u6761\u4ef6\u2014\u2014\u4e25\u683c\u53ef\u884c\u6027\u4e22\u5931\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e94\u79cd\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u9a8c\u8bc1\u95ee\u9898\u7684\u53ef\u884c\u6027\uff0c\u5e76\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u7684\u5927\u90e8\u5206\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u63ed\u793a\u4e86\u4f20\u7edf\u4e0a\u4e3aReLU\u5355\u5143\u8bbe\u5b9a\u7684\u4e0a\u4e0b\u754c\u7ea6\u675f\u5bf9\u95ee\u9898\u7684\u53ef\u884c\u6027\u4e0d\u4ec5\u65e0\u76ca\u53cd\u800c\u6709\u5bb3\u3002", "motivation": "\u5c3d\u7ba1\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u677e\u5f1b\u5728\u63d0\u4f9b\u6bd4\u5176\u4ed6\u51f8\u677e\u5f1b\u65b9\u6cd5\u66f4\u7d27\u81f4\u7684\u8fb9\u754c\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5f53\u5e94\u7528\u4e8e\u6df1\u5c42\u795e\u7ecf\u7f51\u7edc\u65f6\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5185\u90e8\u70b9\u6d88\u5931\u73b0\u8c61\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e25\u683c\u53ef\u884c\u6027\u7684\u4e27\u5931\u3002\u8fd9\u662f\u5bf9\u4e8e\u6269\u5c55\u57fa\u4e8eSDP\u9a8c\u8bc1\u7684\u4e00\u4e2a\u6839\u672c\u969c\u788d\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u8005\u4eec\u5c55\u793a\u4e86\u968f\u7740DNN\u6df1\u5ea6\u7684\u589e\u52a0\uff0c\u4e25\u683c\u53ef\u884c\u6027\u8d8a\u6765\u8d8a\u53ef\u80fd\u4e27\u5931\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ed6\u4eec\u8bbe\u8ba1\u5e76\u8c03\u67e5\u4e86\u4e94\u79cd\u65b9\u6848\u6765\u589e\u5f3a\u9a8c\u8bc1\u95ee\u9898\u7684\u53ef\u884c\u6027\u6761\u4ef6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e0d\u80fd\u89e3\u51b3\u7684\u95ee\u9898\u4e2d\u768488%\uff0c\u5360\u603b\u6570\u768441%\u3002\u53e6\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5148\u524d\u5de5\u4f5c\u4e2d\u6cbf\u7528\u4e0b\u6765\u7684\u9488\u5bf9\u6bcf\u4e2aReLU\u5355\u5143\u7684\u6709\u6548\u7ea6\u675f\u5b9e\u9645\u4e0a\u5bf9\u95ee\u9898\u7684\u53ef\u884c\u6027\u4e0d\u4ec5\u6ca1\u6709\u597d\u5904\uff0c\u751a\u81f3\u662f\u6709\u5bb3\u7684\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5173\u4e8e\u57fa\u4e8eSDP\u7684DNN\u9a8c\u8bc1\u7684\u57fa\u672c\u6311\u6218\u7684\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u6539\u5584\u5176\u5bf9\u66f4\u6df1\u795e\u7ecf\u7f51\u7edc\u7684\u9002\u7528\u6027\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u52a0\u53ef\u9760\u548c\u5b89\u5168\u7684DNN\u7cfb\u7edf\u3002"}}
{"id": "2506.10282", "pdf": "https://arxiv.org/pdf/2506.10282", "abs": "https://arxiv.org/abs/2506.10282", "authors": ["Jiajin Liu", "Dongzhe Fan", "Jiacheng Shen", "Chuanhao Ji", "Daochen Zha", "Qiaoyu Tan"], "title": "Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning", "categories": ["cs.LG"], "comment": "16 pages, 4 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in representing and understanding diverse modalities. However,\nthey typically focus on modality alignment in a pairwise manner while\noverlooking structural relationships across data points. Integrating\nmultimodality with structured graph information (i.e., multimodal graphs, MMGs)\nis essential for real-world applications such as social networks, healthcare,\nand recommendation systems. Existing MMG learning methods fall into three\nparadigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.\nMLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via\nmultimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in\nlanguage or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor\ntreats MLLMs as standalone reasoners with in-context learning or fine-tuning.\nDespite their advances, the MMG field lacks a unified benchmark to fairly\nevaluate across these approaches, making it unclear what progress has been\nmade. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for\nmultimodal graph learning by systematically evaluating these three paradigms\nacross six datasets with different domains. Through extensive experiments, we\nobserve that jointly considering the visual and textual attributes of the nodes\nbenefits graph learning, even when using pre-trained text-to-image alignment\nmodels (e.g., CLIP) as encoders. We also find that converting visual attributes\ninto textual descriptions further improves performance compared to directly\nusing visual inputs. Moreover, we observe that fine-tuning MLLMs on specific\nMMGs can achieve state-of-the-art results in most scenarios, even without\nexplicit graph structure information. We hope that our open-sourced library\nwill facilitate rapid, equitable evaluation and inspire further innovative\nresearch in this field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Graph-MLLM\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u56fe\u5b66\u4e60\u7684\u5168\u9762\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5b66\u4e60\u8303\u5f0f\uff1a\u7f16\u7801\u5668\u3001\u5bf9\u9f50\u5668\u548c\u9884\u6d4b\u5668\u3002\u901a\u8fc7\u516d\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u96c6\u5b9e\u9a8c\uff0c\u7814\u7a76\u53d1\u73b0\u8054\u5408\u8003\u8651\u8282\u70b9\u7684\u89c6\u89c9\u548c\u6587\u672c\u5c5e\u6027\u6709\u52a9\u4e8e\u56fe\u5b66\u4e60\uff0c\u5e76\u4e14\u5c06\u89c6\u89c9\u5c5e\u6027\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u7279\u5b9a\u591a\u6a21\u6001\u56fe\u5fae\u8c03MLLMs\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u591f\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u56fe\u7ed3\u6784\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u56fe\uff08MMG\uff09\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6765\u516c\u5e73\u5730\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u7684\u8fdb\u5c55\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u4eec\u5e0c\u671b\u521b\u5efa\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u5feb\u901f\u800c\u516c\u6b63\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u4e2d\u5b9a\u4e49\u5e76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e09\u79cd\u5229\u7528MLLMs\u8fdb\u884cMMG\u5b66\u4e60\u7684\u65b9\u6cd5\uff1a1) MLLM-as-Encoder\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u589e\u5f3a\u56fe\u795e\u7ecf\u7f51\u7edc\uff1b2) MLLM-as-Aligner\uff0c\u5728\u8bed\u8a00\u6216\u9690\u85cf\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u591a\u6a21\u6001\u5c5e\u6027\uff0c\u4ee5\u652f\u6301\u57fa\u4e8eLLM\u7684\u56fe\u63a8\u7406\uff1b3) MLLM-as-Predictor\uff0c\u5c06MLLMs\u89c6\u4e3a\u72ec\u7acb\u7684\u63a8\u7406\u673a\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u6216\u5fae\u8c03\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u8282\u70b9\u7684\u89c6\u89c9\u548c\u6587\u672c\u5c5e\u6027\u6709\u5229\u4e8e\u56fe\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u5f53\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u5bf9\u9f50\u6a21\u578b\u4f5c\u4e3a\u7f16\u7801\u5668\u65f6\u3002\u53e6\u5916\uff0c\u628a\u89c6\u89c9\u5c5e\u6027\u8f6c\u5316\u4e3a\u6587\u672c\u63cf\u8ff0\u6bd4\u76f4\u63a5\u4f7f\u7528\u89c6\u89c9\u8f93\u5165\u6709\u66f4\u597d\u7684\u8868\u73b0\u3002\u800c\u4e14\uff0c\u5bf9\u4e8e\u7279\u5b9a\u7684MMGs\u8fdb\u884cMLLMs\u7684\u5fae\u8c03\uff0c\u901a\u5e38\u80fd\u591f\u5728\u6ca1\u6709\u663e\u5f0f\u56fe\u7ed3\u6784\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "\u4f5c\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u56fe\u5b66\u4e60\u57fa\u51c6Graph-MLLM\uff0c\u5b83\u63d0\u4f9b\u4e86\u5bf9\u5f53\u524d\u591a\u6a21\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8054\u5408\u5904\u7406\u89c6\u89c9\u4e0e\u6587\u672c\u5c5e\u6027\u7684\u6709\u6548\u6027\u4ee5\u53ca\u5fae\u8c03MLLMs\u7684\u5f3a\u5927\u80fd\u529b\u3002\u6b64\u5de5\u4f5c\u671f\u671b\u80fd\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u521b\u65b0\u7814\u7a76\u3002"}}
{"id": "2506.10314", "pdf": "https://arxiv.org/pdf/2506.10314", "abs": "https://arxiv.org/abs/2506.10314", "authors": ["Luc Raszewski", "Christine De Kock"], "title": "Detecting Sockpuppetry on Wikipedia Using Meta-Learning", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Malicious sockpuppet detection on Wikipedia is critical to preserving access\nto reliable information on the internet and preventing the spread of\ndisinformation. Prior machine learning approaches rely on stylistic and\nmeta-data features, but do not prioritise adaptability to author-specific\nbehaviours. As a result, they struggle to effectively model the behaviour of\nspecific sockpuppet-groups, especially when text data is limited. To address\nthis, we propose the application of meta-learning, a machine learning technique\ndesigned to improve performance in data-scarce settings by training models\nacross multiple tasks. Meta-learning optimises a model for rapid adaptation to\nthe writing style of a new sockpuppet-group. Our results show that\nmeta-learning significantly enhances the precision of predictions compared to\npre-trained models, marking an advancement in combating sockpuppetry on open\nediting platforms. We release a new dataset of sockpuppet investigations to\nfoster future research in both sockpuppetry and meta-learning fields.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5143\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u7ef4\u57fa\u767e\u79d1\u4e0a\u7684\u6076\u610f\u5080\u5121\u8d26\u6237\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u7684\u5080\u5121\u7fa4\u7ec4\u7684\u5199\u4f5c\u98ce\u683c\uff0c\u5e76\u4e14\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u6bd4\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002\u6b64\u5916\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u5080\u5121\u8c03\u67e5\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u98ce\u683c\u548c\u5143\u6570\u636e\u7279\u5f81\uff0c\u4f46\u4e0d\u80fd\u5f88\u597d\u5730\u9002\u5e94\u7279\u5b9a\u4f5c\u8005\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u6587\u672c\u6570\u636e\u6709\u9650\u65f6\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u7279\u5b9a\u5080\u5121\u7fa4\u7ec4\u7684\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u6280\u672f\uff0c\u8fd9\u79cd\u6280\u672f\u901a\u8fc7\u8de8\u591a\u4e2a\u4efb\u52a1\u8bad\u7ec3\u6a21\u578b\u6765\u6539\u5584\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u4f18\u5316\u4e86\u5bf9\u65b0\u5080\u5121\u7fa4\u7ec4\u5199\u4f5c\u98ce\u683c\u7684\u5feb\u901f\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5143\u5b66\u4e60\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u5143\u5b66\u4e60\u65b9\u6cd5\u4e3a\u5f00\u653e\u7f16\u8f91\u5e73\u53f0\u6253\u51fb\u5080\u5121\u884c\u4e3a\u5e26\u6765\u4e86\u8fdb\u6b65\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u5080\u5121\u8c03\u67e5\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7684\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2506.10315", "pdf": "https://arxiv.org/pdf/2506.10315", "abs": "https://arxiv.org/abs/2506.10315", "authors": ["Paul Janson", "Benjamin Therien", "Quentin Anthony", "Xiaolong Huang", "Abhinav Moudgil", "Eugene Belilovsky"], "title": "PyLO: Towards Accessible Learned Optimizers in PyTorch", "categories": ["cs.LG"], "comment": "Accepted at ICML CODEML Workshop 2025", "summary": "Learned optimizers have been an active research topic over the past decade,\nwith increasing progress toward practical, general-purpose optimizers that can\nserve as drop-in replacements for widely used methods like Adam. However,\nrecent advances -- such as VeLO, which was meta-trained for 4000 TPU-months --\nremain largely inaccessible to the broader community, in part due to their\nreliance on JAX and the absence of user-friendly packages for applying the\noptimizers after meta-training. To address this gap, we introduce PyLO, a\nPyTorch-based library that brings learned optimizers to the broader machine\nlearning community through familiar, widely adopted workflows. Unlike prior\nwork focused on synthetic or convex tasks, our emphasis is on applying learned\noptimization to real-world large-scale pre-training tasks. Our release includes\na CUDA-accelerated version of the small_fc_lopt learned optimizer architecture\nfrom (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to\n205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO\nalso allows us to easily combine learned optimizers with existing optimization\ntools such as learning rate schedules and weight decay. When doing so, we find\nthat learned optimizers can substantially benefit. Our code is available at\nhttps://github.com/Belilovsky-Lab/pylo", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PyLO\uff0c\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5e93\uff0c\u5b83\u4f7f\u5f97\u5b66\u4e60\u578b\u4f18\u5316\u5668\u80fd\u591f\u88ab\u66f4\u5e7f\u6cdb\u7684\u673a\u5668\u5b66\u4e60\u793e\u533a\u6240\u4f7f\u7528\u3002PyLO\u63d0\u4f9b\u4e86CUDA\u52a0\u901f\u7248\u672c\u7684\u5c0f\u578b\u5168\u8fde\u63a5\u5b66\u4e60\u4f18\u5316\u5668\u67b6\u6784\uff0c\u5e76\u4e14\u53ef\u4ee5\u8f7b\u677e\u5730\u4e0e\u73b0\u6709\u7684\u4f18\u5316\u5de5\u5177\u5982\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u6743\u91cd\u8870\u51cf\u7ed3\u5408\u4f7f\u7528\u3002", "motivation": "\u5c3d\u7ba1\u5b66\u4e60\u578b\u4f18\u5316\u5668\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u662f\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\uff0c\u4f46\u6700\u8fd1\u7684\u4e00\u4e9b\u8fdb\u5c55\uff0c\u6bd4\u5982VeLO\uff0c\u7531\u4e8e\u5176\u4f9d\u8d56JAX\u4ee5\u53ca\u7f3a\u4e4f\u7528\u6237\u53cb\u597d\u7684\u5305\u6765\u5e94\u7528\u7ecf\u8fc7\u5143\u8bad\u7ec3\u7684\u4f18\u5316\u5668\uff0c\u4ecd\u7136\u96be\u4ee5\u88ab\u66f4\u5e7f\u6cdb\u7684\u793e\u533a\u6240\u91c7\u7528\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63a8\u51fa\u4e86PyLO\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5e93\uff0c\u5b83\u901a\u8fc7\u719f\u6089\u4e14\u5e7f\u6cdb\u63a5\u53d7\u7684\u5de5\u4f5c\u6d41\u7a0b\u5c06\u5b66\u4e60\u578b\u4f18\u5316\u5668\u5e26\u7ed9\u66f4\u5e7f\u5927\u7684\u673a\u5668\u5b66\u4e60\u7fa4\u4f53\u3002\u6b64\u5916\uff0cPyLO\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2aCUDA\u52a0\u901f\u7248\u672c\u7684\u5b66\u4e60\u4f18\u5316\u5668\u67b6\u6784\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5c06\u5176\u4e0e\u73b0\u6709\u4f18\u5316\u5de5\u5177\u76f8\u7ed3\u5408\u3002", "result": "PyLO\u7684\u53d1\u5e03\u4f7f\u5f97\u5b66\u4e60\u578b\u4f18\u5316\u5668\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u4e14\u5f53\u4e0e\u73b0\u6709\u7684\u4f18\u5316\u5de5\u5177\u7ed3\u5408\u65f6\uff0c\u5b66\u4e60\u578b\u4f18\u5316\u5668\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u597d\u5904\u3002CUDA\u52a0\u901f\u7248\u672c\u63d0\u9ad8\u4e86\u4ece39.36\u5230205.59\u6837\u672c/\u79d2\u7684\u541e\u5410\u91cf\uff0c\u7528\u4e8e\u6279\u91cf\u5927\u5c0f\u4e3a32\u7684ViT B/16\u8bad\u7ec3\u3002", "conclusion": "PyLO\u65e8\u5728\u586b\u8865\u5b66\u4e60\u578b\u4f18\u5316\u5668\u5b9e\u9645\u5e94\u7528\u4e0a\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u5e93\u6765\u4fc3\u8fdb\u8fd9\u4e9b\u4f18\u5316\u5668\u5728\u66f4\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\u4e0b\u7684\u4f7f\u7528\u3002"}}
{"id": "2506.10341", "pdf": "https://arxiv.org/pdf/2506.10341", "abs": "https://arxiv.org/abs/2506.10341", "authors": ["Wanqiao Xu", "Allen Nie", "Ruijie Zheng", "Aditya Modi", "Adith Swaminathan", "Ching-An Cheng"], "title": "Provably Learning from Language Feedback", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Interactively learning from observation and language feedback is an\nincreasingly studied area driven by the emergence of large language model (LLM)\nagents. While impressive empirical demonstrations have been shown, so far a\nprincipled framing of these decision problems remains lacking. In this paper,\nwe formalize the Learning from Language Feedback (LLF) problem, assert\nsufficient assumptions to enable learning despite latent rewards, and introduce\n$\\textit{transfer eluder dimension}$ as a complexity measure to characterize\nthe hardness of LLF problems. We show that transfer eluder dimension captures\nthe intuition that information in the feedback changes the learning complexity\nof the LLF problem. We demonstrate cases where learning from rich language\nfeedback can be exponentially faster than learning from reward. We develop a\nno-regret algorithm, called $\\texttt{HELiX}$, that provably solves LLF problems\nthrough sequential interactions, with performance guarantees that scale with\nthe transfer eluder dimension of the problem. Across several empirical domains,\nwe show that $\\texttt{HELiX}$ performs well even when repeatedly prompting LLMs\ndoes not work reliably. Our contributions mark a first step towards designing\nprincipled interactive learning algorithms from generic language feedback.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ece\u8bed\u8a00\u53cd\u9988\u4e2d\u5b66\u4e60\uff08LLF\uff09\u7684\u95ee\u9898\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u8f6c\u79fb\u9003\u907f\u7ef4\u5ea6\u4f5c\u4e3a\u8861\u91cfLLF\u95ee\u9898\u96be\u5ea6\u7684\u590d\u6742\u6027\u6307\u6807\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aHELiX\u7684\u65e0\u6094\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4ec5\u4f9d\u8d56\u5956\u52b1\u76f8\u6bd4\uff0c\u4ece\u4e30\u5bcc\u7684\u8bed\u8a00\u53cd\u9988\u4e2d\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u52a0\u5feb\u5b66\u4e60\u901f\u5ea6\uff0c\u4e14HELiX\u5373\u4f7f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53cd\u9988\u4e0d\u53ef\u9760\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u4eba\u7684\u51fa\u73b0\uff0c\u4ece\u89c2\u5bdf\u548c\u8bed\u8a00\u53cd\u9988\u4e2d\u8fdb\u884c\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6210\u4e3a\u4e86\u4e00\u4e2a\u65e5\u76ca\u7814\u7a76\u7684\u9886\u57df\u3002\u5c3d\u7ba1\u5df2\u7ecf\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u5b9e\u8bc1\u6f14\u793a\uff0c\u4f46\u76ee\u524d\u8fd9\u4e9b\u51b3\u7b56\u95ee\u9898\u7f3a\u4e4f\u4e00\u4e2a\u6709\u539f\u5219\u7684\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u4eec\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u4ece\u8bed\u8a00\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u8db3\u591f\u7684\u5047\u8bbe\u4ee5\u5141\u8bb8\u5728\u5b58\u5728\u6f5c\u5728\u56de\u62a5\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5b66\u4e60\u3002\u4ed6\u4eec\u5f15\u5165\u4e86\u201c\u8f6c\u79fb\u9003\u907f\u7ef4\u5ea6\u201d\u8fd9\u4e00\u590d\u6742\u5ea6\u91cf\u6765\u63cf\u8ff0LLF\u95ee\u9898\u7684\u96be\u5ea6\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u79f0\u4e3aHELiX\u7684\u65e0\u6094\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u987a\u5e8f\u4e92\u52a8\u6765\u89e3\u51b3LLF\u95ee\u9898\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u4ece\u4e30\u5bcc\u7684\u8bed\u8a00\u53cd\u9988\u4e2d\u5b66\u4e60\u65f6\uff0c\u5176\u6548\u7387\u53ef\u80fd\u6bd4\u4ece\u5956\u52b1\u4e2d\u5b66\u4e60\u5feb\u5f97\u591a\u3002\u6b64\u5916\uff0cHELiX\u7b97\u6cd5\u5728\u591a\u79cd\u5b9e\u8bc1\u9886\u57df\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5373\u4f7f\u91cd\u590d\u63d0\u793aLLM\u4e0d\u80fd\u53ef\u9760\u5de5\u4f5c\u65f6\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4ece\u901a\u7528\u8bed\u8a00\u53cd\u9988\u8bbe\u8ba1\u6709\u539f\u5219\u7684\u4ea4\u4e92\u5f0f\u5b66\u4e60\u7b97\u6cd5\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u7684\u89c1\u89e3\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.10351", "pdf": "https://arxiv.org/pdf/2506.10351", "abs": "https://arxiv.org/abs/2506.10351", "authors": ["Yanlong Chen", "Mattia Orlandi", "Pierangelo Maria Rapa", "Simone Benatti", "Luca Benini", "Yawei Li"], "title": "PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 8 figures, 9 tables. Submitted to NeurIPS 2025", "summary": "Physiological signals are often corrupted by motion artifacts, baseline\ndrift, and other low-SNR disturbances, which pose significant challenges for\nanalysis. Additionally, these signals exhibit strong non-stationarity, with\nsharp peaks and abrupt changes that evolve continuously, making them difficult\nto represent using traditional time-domain or filtering methods. To address\nthese issues, a novel wavelet-based approach for physiological signal analysis\nis presented, aiming to capture multi-scale time-frequency features in various\nphysiological signals. Leveraging this technique, two large-scale pretrained\nmodels specific to EMG and ECG are introduced for the first time, achieving\nsuperior performance and setting new baselines in downstream tasks.\nAdditionally, a unified multi-modal framework is constructed by integrating\npretrained EEG model, where each modality is guided through its dedicated\nbranch and fused via learnable weighted fusion. This design effectively\naddresses challenges such as low signal-to-noise ratio, high inter-subject\nvariability, and device mismatch, outperforming existing methods on multi-modal\ntasks. The proposed wavelet-based architecture lays a solid foundation for\nanalysis of diverse physiological signals, while the multi-modal design points\nto next-generation physiological signal processing with potential impact on\nwearable health monitoring, clinical diagnostics, and broader biomedical\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u751f\u7406\u4fe1\u53f7\u5206\u6790\u65b0\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u9488\u5bf9EMG\u548cECG\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3EEG\u6a21\u578b\u6784\u5efa\u4e86\u591a\u6a21\u6001\u6846\u67b6\uff0c\u4e3a\u751f\u7406\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002", "motivation": "\u89e3\u51b3\u751f\u7406\u4fe1\u53f7\u4e2d\u8fd0\u52a8\u4f2a\u5f71\u3001\u57fa\u7ebf\u6f02\u79fb\u7b49\u4f4e\u4fe1\u566a\u6bd4\u5e72\u6270\u95ee\u9898\uff0c\u4ee5\u53ca\u975e\u5e73\u7a33\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5c0f\u6ce2\u7684\u65b9\u6cd5\u6765\u6355\u6349\u4e0d\u540c\u751f\u7406\u4fe1\u53f7\u4e2d\u7684\u591a\u5c3a\u5ea6\u65f6\u9891\u7279\u5f81\uff0c\u5e76\u9996\u6b21\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u4e13\u7528\u4e8eEMG\u548cECG\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u7684EEG\u6a21\u578b\u521b\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5176\u4e2d\u6bcf\u79cd\u6a21\u5f0f\u90fd\u901a\u8fc7\u5176\u4e13\u7528\u5206\u652f\u5f15\u5bfc\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u52a0\u6743\u878d\u5408\u8fdb\u884c\u878d\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u4f18\u5f02\u7684\u8868\u73b0\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u8bbe\u8ba1\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8bf8\u5982\u4f4e\u4fe1\u566a\u6bd4\u3001\u9ad8\u4e2a\u4f53\u95f4\u53d8\u5f02\u6027\u4ee5\u53ca\u8bbe\u5907\u4e0d\u5339\u914d\u7b49\u95ee\u9898\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5c0f\u6ce2\u7684\u67b6\u6784\u4e3a\u591a\u79cd\u751f\u7406\u4fe1\u53f7\u5206\u6790\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u800c\u591a\u6a21\u6001\u8bbe\u8ba1\u5219\u6307\u5411\u4e86\u4e0b\u4e00\u4ee3\u751f\u7406\u4fe1\u53f7\u5904\u7406\uff0c\u5bf9\u53ef\u7a7f\u6234\u5065\u5eb7\u76d1\u6d4b\u3001\u4e34\u5e8a\u8bca\u65ad\u53ca\u66f4\u5e7f\u6cdb\u7684\u751f\u7269\u533b\u5b66\u5e94\u7528\u5177\u6709\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2506.10352", "pdf": "https://arxiv.org/pdf/2506.10352", "abs": "https://arxiv.org/abs/2506.10352", "authors": ["Binyao Guo", "Zihan Lin", "QiZhi He"], "title": "History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials", "categories": ["cs.LG"], "comment": null, "summary": "This study presents an end-to-end learning framework for data-driven modeling\nof path-dependent inelastic materials using neural operators. The framework is\nbuilt on the premise that irreversible evolution of material responses,\ngoverned by hidden dynamics, can be inferred from observable data.\n  We develop the History-Aware Neural Operator (HANO), an autoregressive model\nthat predicts path-dependent material responses from short segments of recent\nstrain-stress history without relying on hidden state variables, thereby\novercoming self-consistency issues commonly encountered in recurrent neural\nnetwork (RNN)-based models. Built on a Fourier-based neural operator backbone,\nHANO enables discretization-invariant learning. To enhance its ability to\ncapture both global loading patterns and critical local path dependencies, we\nembed a hierarchical self-attention mechanism that facilitates multiscale\nfeature extraction.\n  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial\nhidden states, a commonly overlooked issue that can lead to instability in\nrecurrent models when applied to generalized loading paths. By modeling\nstress-strain evolution as a continuous operator rather than relying on fixed\ninput-output mappings, HANO naturally accommodates varying path discretizations\nand exhibits robust performance under complex conditions, including irregular\nsampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate\nHANO on two benchmark problems: elastoplasticity with hardening and progressive\nanisotropic damage in brittle solids. Results show that HANO consistently\noutperforms baseline models in predictive accuracy, generalization, and\nrobustness. With its demonstrated capabilities, HANO provides an effective\ndata-driven surrogate for simulating inelastic materials and is well-suited for\nintegration with classical numerical solvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u540d\u4e3a\u5386\u53f2\u611f\u77e5\u795e\u7ecf\u7b97\u5b50\uff08HANO\uff09\uff0c\u7528\u4e8e\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8def\u5f84\u4f9d\u8d56\u975e\u5f39\u6027\u6750\u6599\u5efa\u6a21\u3002HANO \u901a\u8fc7\u4ece\u8fd1\u671f\u5e94\u53d8-\u5e94\u529b\u5386\u53f2\u7684\u5c0f\u6bb5\u4e2d\u9884\u6d4b\u6750\u6599\u54cd\u5e94\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u9690\u85cf\u72b6\u6001\u53d8\u91cf\uff0c\u4ece\u800c\u514b\u670d\u4e86\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5e38\u89c1\u7684\u81ea\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f7f\u7528\u4f20\u7edf\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u8fdb\u884c\u8def\u5f84\u4f9d\u8d56\u6750\u6599\u5efa\u6a21\u65f6\u9047\u5230\u7684\u81ea\u4e00\u81f4\u6027\u95ee\u9898\u4ee5\u53ca\u5bf9\u521d\u59cb\u9690\u85cf\u72b6\u6001\u654f\u611f\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u540c\u8def\u5f84\u79bb\u6563\u5316\u3001\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u566a\u58f0\u6570\u636e\u7b49\u590d\u6742\u60c5\u51b5\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u5386\u53f2\u611f\u77e5\u795e\u7ecf\u7b97\u5b50\uff08HANO\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5b83\u5229\u7528\u5085\u91cc\u53f6\u57fa\u7840\u795e\u7ecf\u7b97\u5b50\u6765\u5b9e\u73b0\u79bb\u6563\u5316\u4e0d\u53d8\u5b66\u4e60\uff0c\u5e76\u5d4c\u5165\u4e86\u5206\u5c42\u81ea\u6211\u6ce8\u610f\u673a\u5236\u4ee5\u4fc3\u8fdb\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3002\u8fd9\u79cd\u65b9\u6cd5\u5c06\u5e94\u529b-\u5e94\u53d8\u6f14\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u7b97\u5b50\u800c\u975e\u56fa\u5b9a\u8f93\u5165-\u8f93\u51fa\u6620\u5c04\uff0c\u4ece\u800c\u81ea\u7136\u5730\u9002\u5e94\u5404\u79cd\u8def\u5f84\u79bb\u6563\u5316\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "HANO \u5728\u4e24\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1a\u786c\u5316\u5f39\u5851\u6027\u548c\u8106\u6027\u56fa\u4f53\u4e2d\u7684\u6e10\u8fdb\u5404\u5411\u5f02\u6027\u635f\u4f24\u3002\u7ed3\u679c\u8868\u660e\uff0cHANO \u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "HANO \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6570\u636e\u9a71\u52a8\u4ee3\u7406\uff0c\u9002\u7528\u4e8e\u6a21\u62df\u975e\u5f39\u6027\u6750\u6599\uff0c\u5e76\u4e14\u975e\u5e38\u9002\u5408\u4e0e\u7ecf\u5178\u6570\u503c\u6c42\u89e3\u5668\u96c6\u6210\u3002"}}
{"id": "2506.10355", "pdf": "https://arxiv.org/pdf/2506.10355", "abs": "https://arxiv.org/abs/2506.10355", "authors": ["Yu-Yang Qian", "Yuan-Ze Xu", "Zhen-Yu Zhang", "Peng Zhao", "Zhi-Hua Zhou"], "title": "TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Many real-world applications collect data in a streaming environment, where\nlearning tasks are encountered sequentially. This necessitates continual\nlearning (CL) to update models online, enabling adaptation to new tasks while\npreserving past knowledge to prevent catastrophic forgetting. Nowadays, with\nthe flourish of large pre-trained models (LPMs), efficiency has become\nincreasingly critical for CL, due to their substantial computational demands\nand growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of\nLow-Rank Adapters), a novel approach that constructs layer-wise adapters by\nleveraging hierarchical gradient similarity to enable efficient CL,\nparticularly for LPMs. To reduce the computational burden of task similarity\nestimation, we employ bandit techniques to develop an algorithm based on lower\nconfidence bounds to efficiently explore the task structure. Furthermore, we\nuse sparse gradient updates to facilitate parameter optimization, making the\napproach better suited for LPMs. Theoretical analysis is provided to justify\nthe rationale behind our approach, and experiments on both vision transformers\n(ViTs) and large language models (LLMs) demonstrate the effectiveness and\nefficiency of our approach across various domains, including vision and natural\nlanguage processing tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5TreeLoRA\uff0c\u901a\u8fc7\u5229\u7528\u5206\u5c42\u68af\u5ea6\u76f8\u4f3c\u6027\u6765\u6784\u5efa\u5c42\u7ea7\u9002\u914d\u5668\uff0c\u5e76\u7ed3\u5408\u7a00\u758f\u68af\u5ea6\u66f4\u65b0\u548c\u57fa\u4e8e\u4e0b\u7f6e\u4fe1\u754c\u7684\u7b97\u6cd5\u6765\u6709\u6548\u63a2\u7d22\u4efb\u52a1\u7ed3\u6784\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u7387\u3002", "motivation": "\u5728\u6570\u636e\u6d41\u73af\u5883\u4e2d\uff0c\u8fde\u7eed\u9047\u5230\u5b66\u4e60\u4efb\u52a1\u9700\u8981\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u5728\u7ebf\u66f4\u65b0\u6a21\u578b\uff0c\u4ee5\u9002\u5e94\u65b0\u4efb\u52a1\u540c\u65f6\u4fdd\u7559\u8fc7\u53bb\u7684\u77e5\u8bc6\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002\u968f\u7740\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08LPMs\uff09\u7684\u5174\u8d77\uff0c\u7531\u4e8e\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u53c2\u6570\u89c4\u6a21\u7684\u589e\u957f\uff0c\u5bf9\u4e8eCL\u6765\u8bf4\u6548\u7387\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "TreeLoRA\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\u548c\u5206\u5c42\u68af\u5ea6\u76f8\u4f3c\u6027\u6765\u6784\u9020\u5c42\u7ea7\u9002\u914d\u5668\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u3002\u4e3a\u4e86\u51cf\u5c11\u4efb\u52a1\u76f8\u4f3c\u6027\u4f30\u8ba1\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u91c7\u7528\u4e86\u57fa\u4e8e\u4e0b\u7f6e\u4fe1\u754c\u7684\u7b97\u6cd5\u6709\u6548\u5730\u63a2\u7d22\u4efb\u52a1\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528\u4e86\u7a00\u758f\u68af\u5ea6\u66f4\u65b0\u6765\u4fc3\u8fdb\u53c2\u6570\u4f18\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u652f\u6301\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u5408\u7406\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8f6c\u6362\u5668\uff08ViTs\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0a\u90fd\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u3002", "conclusion": "TreeLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f97\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\uff0c\u800c\u4e0d\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u7684\u9057\u5fd8\uff0c\u800c\u4e14\u5728\u591a\u4e2a\u9886\u57df\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2506.10364", "pdf": "https://arxiv.org/pdf/2506.10364", "abs": "https://arxiv.org/abs/2506.10364", "authors": ["Penguin Huang", "Chhavi Yadav", "Ruihan Wu", "Kamalika Chaudhuri"], "title": "Can We Infer Confidential Properties of Training Data from LLMs?", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) are increasingly fine-tuned on domain-specific\ndatasets to support applications in fields such as healthcare, finance, and\nlaw. These fine-tuning datasets often have sensitive and confidential\ndataset-level properties -- such as patient demographics or disease prevalence\n-- that are not intended to be revealed. While prior work has studied property\ninference attacks on discriminative models (e.g., image classification models)\nand generative models (e.g., GANs for image data), it remains unclear if such\nattacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark\ntask for evaluating property inference in LLMs under two fine-tuning paradigms:\nquestion-answering and chat-completion. Built on the ChatDoctor dataset, our\nbenchmark includes a range of property types and task configurations. We\nfurther propose two tailored attacks: a prompt-based generation attack and a\nshadow-model attack leveraging word frequency signals. Empirical evaluations\nacross multiple pretrained LLMs show the success of our attacks, revealing a\npreviously unrecognized vulnerability in LLMs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PropInfer\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5728\u4e24\u79cd\u5fae\u8c03\u8303\u5f0f\u4e0b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c5e\u6027\u63a8\u65ad\u7684\u57fa\u51c6\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u6027\u653b\u51fb\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u653b\u51fb\u80fd\u591f\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u672a\u88ab\u8ba4\u8bc6\u5230\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u652f\u6301\u533b\u7597\u3001\u91d1\u878d\u548c\u6cd5\u5f8b\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5982\u4f55\u9632\u6b62\u654f\u611f\u548c\u673a\u5bc6\u6570\u636e\u5c5e\u6027\u6cc4\u9732\u6210\u4e3a\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u63a2\u8ba8\u4e86\u5bf9\u5224\u522b\u6a21\u578b\uff08\u5982\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff09\u548c\u751f\u6210\u6a21\u578b\uff08\u5982\u7528\u4e8e\u56fe\u50cf\u6570\u636e\u7684GANs\uff09\u7684\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\uff0c\u4f46\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u98ce\u9669\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7814\u7a76\u8005\u4eec\u5f15\u5165\u4e86\u540d\u4e3aPropInfer\u7684\u57fa\u51c6\u4efb\u52a1\u6765\u8bc4\u4ef7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u7b54\u548c\u804a\u5929\u5b8c\u6210\u8fd9\u4e24\u79cd\u5fae\u8c03\u6a21\u5f0f\u4e0b\u7684\u5c5e\u6027\u63a8\u65ad\u80fd\u529b\u3002\u4ed6\u4eec\u57fa\u4e8eChatDoctor\u6570\u636e\u96c6\u6784\u5efa\u4e86\u8be5\u57fa\u51c6\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u7c7b\u578b\u7684\u5c5e\u6027\u548c\u4efb\u52a1\u914d\u7f6e\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e24\u79cd\u5b9a\u5236\u5316\u653b\u51fb\uff1a\u4e00\u79cd\u662f\u57fa\u4e8e\u63d0\u793a\u7684\u751f\u6210\u653b\u51fb\uff0c\u53e6\u4e00\u79cd\u5219\u662f\u5229\u7528\u8bcd\u9891\u4fe1\u53f7\u7684\u5f71\u5b50\u6a21\u578b\u653b\u51fb\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u653b\u51fb\u624b\u6bb5\u6210\u529f\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u4e00\u4e2a\u6b64\u524d\u672a\u88ab\u8ba4\u77e5\u5230\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u4e13\u95e8\u4e3a\u5904\u7406\u6587\u672c\u800c\u8bbe\u8ba1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e5f\u53ef\u80fd\u906d\u53d7\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\uff0c\u8fd9\u8868\u660e\u6709\u5fc5\u8981\u5f00\u53d1\u65b0\u7684\u9632\u5fa1\u63aa\u65bd\u6765\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u4e0d\u88ab\u6cc4\u9732\u3002"}}
{"id": "2506.10403", "pdf": "https://arxiv.org/pdf/2506.10403", "abs": "https://arxiv.org/abs/2506.10403", "authors": ["Tzu-Heng Huang", "Harit Vishwakarma", "Frederic Sala"], "title": "Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are widely used to evaluate the quality of LLM\ngenerations and responses, but this leads to significant challenges: high API\ncosts, uncertain reliability, inflexible pipelines, and inherent biases. To\naddress these, we introduce PAJAMA (Program-As-a-Judge for Automated Model\nAssessment), a new alternative that uses LLMs to synthesize executable judging\nprograms instead of directly scoring responses. These synthesized programs can\nbe stored and run locally, costing orders of magnitude less while providing\ninterpretable, and auditable judging logic that can be easily adapted.\nProgram-based judges mitigate biases, improving judgment consistency by 15.83%\nand reducing biased responses by 23.7% on average compared to a\nQwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a\nmodel, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of\nRewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the\nJudgeLM dataset, all at three orders of magnitude lower cost.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u8d28\u91cf\u7684\u65b9\u6cd5PAJAMA\uff0c\u5b83\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u5408\u6210\u53ef\u6267\u884c\u7684\u8bc4\u5224\u7a0b\u5e8f\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u8bc4\u5206\u3002\u8fd9\u79cd\u65b9\u6cd5\u6210\u672c\u66f4\u4f4e\uff0c\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u5224\u65ad\u4e00\u81f4\u6027\u548c\u66f4\u5c11\u7684\u504f\u89c1\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u7684\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u7684\u65b9\u6cd5\u5b58\u5728\u9ad8API\u6210\u672c\u3001\u53ef\u9760\u6027\u4e0d\u786e\u5b9a\u3001\u6d41\u7a0b\u4e0d\u7075\u6d3b\u548c\u5185\u5728\u504f\u89c1\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86PAJAMA\uff08Program-As-a-Judge for Automated Model Assessment\uff09\uff0c\u901a\u8fc7\u8ba9LLM\u751f\u6210\u53ef\u6267\u884c\u7684\u8bc4\u5224\u7a0b\u5e8f\u6765\u66ff\u4ee3\u76f4\u63a5\u6253\u5206\u7684\u65b9\u5f0f\u3002\u8fd9\u4e9b\u7a0b\u5e8f\u53ef\u4ee5\u672c\u5730\u5b58\u50a8\u5e76\u8fd0\u884c\uff0c\u6781\u5927\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u548c\u53ef\u5ba1\u8ba1\u7684\u8bc4\u5224\u903b\u8f91\u3002", "result": "\u4e0e\u57fa\u4e8eQwen2.5-14B\u7684LLM-as-a-judge\u76f8\u6bd4\uff0cPAJAMA\u5728\u4e00\u81f4\u6027\u4e0a\u63d0\u9ad8\u4e8615.83%\uff0c\u51cf\u5c11\u4e86\u5e73\u574723.7%\u7684\u504f\u89c1\u56de\u5e94\u3002\u5728CHAT-HARD\u5b50\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6307\u6807\uff0cPrometheus\u4e0a\u63d0\u9ad82.19%\uff0cJudgeLM\u6570\u636e\u96c6\u4e0a\u63d0\u53478.67%\uff0c\u540c\u65f6\u6210\u672c\u4f4e\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "PAJAMA\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u5316\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u663e\u8457\u964d\u4f4e\u4e86\u8bc4\u4f30\u6210\u672c\uff0c\u8fd8\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u548c\u516c\u6b63\u6027\uff0c\u4e3a\u6a21\u578b\u751f\u6210\u8d28\u91cf\u8bc4\u4ef7\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.10389", "pdf": "https://arxiv.org/pdf/2506.10389", "abs": "https://arxiv.org/abs/2506.10389", "authors": ["Yuhang Chen", "Zhen Tan", "Tianlong Chen"], "title": "EQA-RM: A Generative Embodied Reward Model with Test-time Scaling", "categories": ["cs.LG"], "comment": "preprint", "summary": "Reward Models (RMs), vital for large model alignment, are underexplored for\ncomplex embodied tasks like Embodied Question Answering (EQA) where nuanced\nevaluation of agents' spatial, temporal, and logical understanding is critical\nyet not considered by generic approaches. We introduce EQA-RM, a novel\ngenerative multimodal reward model specifically architected for EQA, trained\nvia our innovative Contrastive Group Relative Policy Optimization (C-GRPO)\nstrategy to learn fine-grained behavioral distinctions. The generative nature\nof EQA-RM provides interpretable, structured reward feedback (beyond simple\nscalars), uniquely enabling test-time scaling to dynamically adjust evaluation\ngranularity, from concise scores to detailed critiques of reasoning and\ngrounding, at inference without retraining. Concurrently, we introduce\nEQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward\nmodel assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning\nQwen2-VL-2B-Instruct) achieves 61.9\\% accuracy on EQA-RM-Bench with only 700\nsamples, outperforming strong proprietary baselines, including\nGemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art\nmodels such as RoVRM and VisualPRM. The code and dataset can be found here\nhttps://github.com/UNITES-Lab/EQA-RM.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578bEQA-RM\uff0c\u4e13\u4e3a\u5177\u8eab\u95ee\u7b54\u4efb\u52a1\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6bd4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08C-GRPO\uff09\u7b56\u7565\u8bad\u7ec3\u3002EQA-RM\u80fd\u591f\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u5956\u52b1\u53cd\u9988\uff0c\u5e76\u4e14\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8c03\u6574\u8bc4\u4f30\u7c92\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86EQARewardBench\u57fa\u51c6\u6765\u6807\u51c6\u5316EQA\u5956\u52b1\u6a21\u578b\u7684\u8bc4\u4f30\u3002EQA-RM\u8868\u73b0\u51fa\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5728\u4ec5\u4f7f\u7528700\u4e2a\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e8661.9%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u591a\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\uff08RMs\uff09\u5bf9\u4e8e\u590d\u6742\u7684\u5177\u8eab\u4efb\u52a1\u5982\u5177\u8eab\u95ee\u7b54\uff08EQA\uff09\uff0c\u5728\u8bc4\u4f30\u4ee3\u7406\u7684\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u903b\u8f91\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u901a\u7528\u7684\u65b9\u6cd5\u672a\u80fd\u8003\u8651\u8fd9\u4e9b\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u5f15\u5165\u4e86EQA-RM\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u95e8\u4e3aEQA\u8bbe\u8ba1\u7684\u65b0\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6bd4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08C-GRPO\uff09\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u7ec6\u7c92\u5ea6\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u8d85\u8d8a\u7b80\u5355\u6807\u91cf\u7684\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u5956\u52b1\u53cd\u9988\u3002", "result": "EQA-RM\u5c55\u73b0\u51fa\u4e86\u9ad8\u7684\u6837\u672c\u6548\u7387\uff0c\u5728\u4ec5\u4f7f\u7528700\u4e2a\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e8661.9%\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u4e00\u8868\u73b0\u8d85\u8fc7\u4e86\u5305\u62ecGemini-2.5-Flash\u3001GPT-4o\u3001Claude-3.5-Haiku\u4ee5\u53ca\u5f00\u6e90\u7684\u6700\u5148\u8fdb\u6a21\u578b\u5982RoVRM\u548cVisualPRM\u5728\u5185\u7684\u5f3a\u5927\u57fa\u7ebf\u3002", "conclusion": "EQA-RM\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u80fd\u591f\u9488\u5bf9\u590d\u6742\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u7ec6\u5fae\u884c\u4e3a\u5dee\u5f02\u63d0\u4f9b\u7ec6\u81f4\u7684\u8bc4\u4ef7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2506.10412", "pdf": "https://arxiv.org/pdf/2506.10412", "abs": "https://arxiv.org/abs/2506.10412", "authors": ["Ching Chang", "Jeehyun Hwang", "Yidan Shi", "Haixin Wang", "Wen-Chih Peng", "Tien-Fu Chen", "Wei Wang"], "title": "Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This paper is currently under review", "summary": "Time series data in real-world applications such as healthcare, climate\nmodeling, and finance are often irregular, multimodal, and messy, with varying\nsampling rates, asynchronous modalities, and pervasive missingness. However,\nexisting benchmarks typically assume clean, regularly sampled, unimodal data,\ncreating a significant gap between research and real-world deployment. We\nintroduce Time-IMM, a dataset specifically designed to capture cause-driven\nirregularity in multimodal multivariate time series. Time-IMM represents nine\ndistinct types of time series irregularity, categorized into trigger-based,\nconstraint-based, and artifact-based mechanisms. Complementing the dataset, we\nintroduce IMM-TSF, a benchmark library for forecasting on irregular multimodal\ntime series, enabling asynchronous integration and realistic evaluation.\nIMM-TSF includes specialized fusion modules, including a timestamp-to-text\nfusion module and a multimodality fusion module, which support both\nrecency-aware averaging and attention-based integration strategies. Empirical\nresults demonstrate that explicitly modeling multimodality on irregular time\nseries data leads to substantial gains in forecasting performance. Time-IMM and\nIMM-TSF provide a foundation for advancing time series analysis under\nreal-world conditions. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the\nbenchmark library can be accessed at\nhttps://anonymous.4open.science/r/IMMTSF_NeurIPS2025.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Time-IMM\u6570\u636e\u96c6\u548cIMM-TSF\u57fa\u51c6\u5e93\uff0c\u7528\u4e8e\u5904\u7406\u771f\u5b9e\u4e16\u754c\u4e2d\u4e0d\u89c4\u5219\u3001\u591a\u6a21\u6001\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u8fd9\u4e9b\u5de5\u5177\u65e8\u5728\u5f25\u8865\u73b0\u6709\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u878d\u5408\u6a21\u5757\u6765\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u57fa\u51c6\u901a\u5e38\u5047\u8bbe\u6570\u636e\u662f\u5e72\u51c0\u7684\u3001\u5b9a\u671f\u91c7\u6837\u7684\u5355\u6a21\u6001\u6570\u636e\uff0c\u8fd9\u4e0e\u73b0\u5b9e\u4e2d\u7ecf\u5e38\u9047\u5230\u7684\u4e0d\u89c4\u5219\u3001\u591a\u6a21\u6001\u4e14\u6709\u7f3a\u5931\u7684\u6570\u636e\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u4fc3\u8fdb\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u8fdb\u5c55\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Time-IMM\u6570\u636e\u96c6\u548cIMM-TSF\u57fa\u51c6\u5e93\u3002", "method": "\u521b\u5efa\u4e86Time-IMM\u6570\u636e\u96c6\u4ee5\u6355\u6349\u7531\u539f\u56e0\u9a71\u52a8\u7684\u4e0d\u89c4\u5219\u6027\u5728\u591a\u53d8\u91cf\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u4e86IMM-TSF\uff0c\u4e00\u4e2a\u9488\u5bf9\u4e0d\u89c4\u5219\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u57fa\u51c6\u5e93\u3002IMM-TSF\u5305\u542b\u4e13\u95e8\u7684\u878d\u5408\u6a21\u5757\uff0c\u5982\u65f6\u95f4\u6233\u5230\u6587\u672c\u878d\u5408\u6a21\u5757\u548c\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u652f\u6301\u57fa\u4e8e\u6700\u8fd1\u6027\u7684\u5e73\u5747\u7b56\u7565\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u96c6\u6210\u7b56\u7565\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u663e\u5f0f\u5efa\u6a21\u591a\u6a21\u6001\u53ef\u4ee5\u5927\u5e45\u5ea6\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "Time-IMM\u6570\u636e\u96c6\u548cIMM-TSF\u57fa\u51c6\u5e93\u4e3a\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u63a8\u8fdb\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2506.10404", "pdf": "https://arxiv.org/pdf/2506.10404", "abs": "https://arxiv.org/abs/2506.10404", "authors": ["Bryan Shaddy", "Brianna Binder", "Agnimitra Dasgupta", "Haitong Qin", "James Haley", "Angel Farguell", "Kyle Hilburn", "Derek V. Mallia", "Adam Kochanski", "Jan Mandel", "Assad Oberai"], "title": "Generative Algorithms for Wildfire Progression Reconstruction from Multi-Modal Satellite Active Fire Measurements and Terrain Height", "categories": ["cs.LG"], "comment": null, "summary": "Increasing wildfire occurrence has spurred growing interest in wildfire\nspread prediction. However, even the most complex wildfire models diverge from\nobserved progression during multi-day simulations, motivating need for data\nassimilation. A useful approach to assimilating measurement data into complex\ncoupled atmosphere-wildfire models is to estimate wildfire progression from\nmeasurements and use this progression to develop a matching atmospheric state.\nIn this study, an approach is developed for estimating fire progression from\nVIIRS active fire measurements, GOES-derived ignition times, and terrain height\ndata. A conditional Generative Adversarial Network is trained with simulations\nof historic wildfires from the atmosphere-wildfire model WRF-SFIRE, thus\nallowing incorporation of WRF-SFIRE physics into estimates. Fire progression is\nsuccinctly represented by fire arrival time, and measurements for training are\nobtained by applying an approximate observation operator to WRF-SFIRE\nsolutions, eliminating need for satellite data during training. The model is\ntrained on tuples of fire arrival times, measurements, and terrain, and once\ntrained leverages measurements of real fires and corresponding terrain data to\ngenerate samples of fire arrival times. The approach is validated on five\nPacific US wildfires, with results compared against high-resolution perimeters\nmeasured via aircraft, finding an average Sorensen-Dice coefficient of 0.81.\nThe influence of terrain height on the arrival time inference is also evaluated\nand it is observed that terrain has minimal influence when the inference is\nconditioned on satellite measurements.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7VIIRS\u6d3b\u52a8\u706b\u70b9\u6d4b\u91cf\u3001GOES\u884d\u751f\u7684\u70b9\u706b\u65f6\u95f4\u548c\u5730\u5f62\u9ad8\u5ea6\u6570\u636e\u6765\u4f30\u8ba1\u706b\u707e\u8fdb\u7a0b\uff0c\u5e76\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7ed3\u5408WRF-SFIRE\u6a21\u578b\u7684\u7269\u7406\u7279\u6027\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u592a\u5e73\u6d0b\u7f8e\u56fd\u91ce\u706b\u6848\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e73\u5747S\u00f8rensen-Dice\u7cfb\u6570\u4e3a0.81\u3002", "motivation": "\u7531\u4e8e\u591a\u65e5\u6a21\u62df\u8fc7\u7a0b\u4e2d\u6700\u590d\u6742\u7684\u91ce\u706b\u6a21\u578b\u4e0e\u89c2\u5bdf\u5230\u7684\u8fdb\u5c55\u51fa\u73b0\u5206\u6b67\uff0c\u56e0\u6b64\u9700\u8981\u6570\u636e\u540c\u5316\u6765\u6539\u8fdb\u9884\u6d4b\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u5e76\u7528\u5386\u53f2\u91ce\u706b\u6a21\u62df\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u5c06WRF-SFIRE\u6a21\u578b\u7684\u7269\u7406\u7279\u6027\u7eb3\u5165\u4f30\u8ba1\u4e4b\u4e2d\u3002\u706b\u707e\u8fdb\u7a0b\u7531\u706b\u5230\u8fbe\u65f6\u95f4\u7b80\u6d01\u8868\u793a\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u901a\u8fc7\u5bf9WRF-SFIRE\u89e3\u51b3\u65b9\u6848\u5e94\u7528\u8fd1\u4f3c\u89c2\u6d4b\u7b97\u5b50\u83b7\u5f97\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u592a\u5e73\u6d0b\u7f8e\u56fd\u91ce\u706b\u6848\u4f8b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u4e0e\u98de\u673a\u6d4b\u91cf\u7684\u9ad8\u5206\u8fa8\u7387\u5468\u957f\u76f8\u6bd4\uff0c\u5e73\u5747S\u00f8rensen-Dice\u7cfb\u6570\u4e3a0.81\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u5730\u5f62\u9ad8\u5ea6\u5bf9\u5230\u8fbe\u65f6\u95f4\u63a8\u65ad\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f53\u63a8\u65ad\u57fa\u4e8e\u536b\u661f\u6d4b\u91cf\u65f6\uff0c\u5730\u5f62\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528cGAN\u548cWRF-SFIRE\u6a21\u578b\u7269\u7406\u5b66\u7279\u6027\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u4ece\u536b\u661f\u6d4b\u91cf\u4e2d\u4f30\u8ba1\u51fa\u706b\u707e\u8fdb\u7a0b\uff0c\u800c\u5730\u5f62\u9ad8\u5ea6\u5bf9\u57fa\u4e8e\u536b\u661f\u6d4b\u91cf\u7684\u5230\u8fbe\u65f6\u95f4\u63a8\u65ad\u5f71\u54cd\u8f83\u5c0f\u3002"}}
{"id": "2506.10617", "pdf": "https://arxiv.org/pdf/2506.10617", "abs": "https://arxiv.org/abs/2506.10617", "authors": ["Reza Karbasi", "Masoud Rahimi", "Abdol-Hossein Vahabie", "Hadi Moradi"], "title": "Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper addresses the persistent challenge of accurately digitizing\npaper-based electrocardiogram (ECG) recordings, with a particular focus on\nrobustly handling single leads compromised by signal overlaps-a common yet\nunder-addressed issue in existing methodologies. We propose a two-stage\npipeline designed to overcome this limitation. The first stage employs a U-Net\nbased segmentation network, trained on a dataset enriched with overlapping\nsignals and fortified with custom data augmentations, to accurately isolate the\nprimary ECG trace. The subsequent stage converts this refined binary mask into\na time-series signal using established digitization techniques, enhanced by an\nadaptive grid detection module for improved versatility across different ECG\nformats and scales. Our experimental results demonstrate the efficacy of our\napproach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained\nsegmentation task. Crucially, our proposed digitization method yields superior\nperformance compared to a well-established baseline technique across both\nnon-overlapping and challenging overlapping ECG samples. For non-overlapping\nsignals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson\nCorrelation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366,\nrespectively, for the baseline. On samples with signal overlap, our method\nachieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the\nbaseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to\nsignificantly enhance digitization accuracy, especially in the presence of\nsignal overlaps, thereby laying a strong foundation for the reliable conversion\nof analog ECG records into analyzable digital data for contemporary research\nand clinical applications. The implementation is publicly available at this\nGitHub repository: https://github.com/masoudrahimi39/ECG-code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u7ba1\u9053\u6765\u89e3\u51b3\u5fc3\u7535\u56fe\u4fe1\u53f7\u91cd\u53e0\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7eb8\u57fa\u5fc3\u7535\u56fe\u8bb0\u5f55\u6570\u5b57\u5316\u7684\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u975e\u91cd\u53e0\u548c\u91cd\u53e0\u7684\u5fc3\u7535\u56fe\u6837\u672c\u65f6\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u5728\u516c\u5f00\u7684GitHub\u5b58\u50a8\u5e93\u4e2d\u63d0\u4f9b\u4e86\u5b9e\u73b0\u4ee3\u7801\u3002", "motivation": "\u9488\u5bf9\u7eb8\u57fa\u5fc3\u7535\u56fe\uff08ECG\uff09\u8bb0\u5f55\u51c6\u786e\u6570\u5b57\u5316\u8fd9\u4e00\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u56e0\u4fe1\u53f7\u91cd\u53e0\u800c\u53d7\u635f\u7684\u5355\u5bfc\u8054\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u5904\u7406\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8eU-Net\u7684\u5206\u5272\u7f51\u7edc\uff0c\u8bad\u7ec3\u6570\u636e\u96c6\u5305\u542b\u4e86\u589e\u5f3a\u7684\u91cd\u53e0\u4fe1\u53f7\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u65e2\u5b9a\u7684\u6570\u5b57\u5316\u6280\u672f\u5c06\u4f18\u5316\u540e\u7684\u4e8c\u8fdb\u5236\u63a9\u6a21\u8f6c\u6362\u4e3a\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7f51\u683c\u68c0\u6d4b\u6a21\u5757\u4ee5\u63d0\u9ad8\u4e0d\u540cECG\u683c\u5f0f\u548c\u5c3a\u5ea6\u4e0b\u7684\u901a\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u3002U-Net\u67b6\u6784\u5bf9\u7cbe\u7ec6\u5206\u5272\u4efb\u52a1\u5b9e\u73b0\u4e860.87\u7684IoU\u503c\uff0c\u5728\u975e\u91cd\u53e0\u548c\u91cd\u53e0\u4fe1\u53f7\u4e0a\u90fd\u8868\u73b0\u51fa\u6bd4\u57fa\u51c6\u66f4\u597d\u7684\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u975e\u91cd\u53e0\u4fe1\u53f7\u4e0aMSE\u4e3a0.0010\u3001\u76f8\u5173\u7cfb\u6570rho\u4e3a0.9644\uff1b\u800c\u5728\u91cd\u53e0\u4fe1\u53f7\u4e0aMSE\u4e3a0.0029\u3001rho\u4e3a0.9641\uff0c\u660e\u663e\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b58\u5728\u4fe1\u53f7\u91cd\u53e0\u60c5\u51b5\u4e0b\u7684\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u7cbe\u5ea6\uff0c\u4e3a\u53ef\u9760\u5730\u5c06\u6a21\u62df\u5fc3\u7535\u56fe\u8bb0\u5f55\u8f6c\u5316\u4e3a\u53ef\u5206\u6790\u7684\u6570\u5b57\u6570\u636e\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.10629", "pdf": "https://arxiv.org/pdf/2506.10629", "abs": "https://arxiv.org/abs/2506.10629", "authors": ["Yucheng Yang", "Tianyi Zhou", "Qiang He", "Lei Han", "Mykola Pechenizkiy", "Meng Fang"], "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "I.2.6; I.2.8; G.3"], "comment": "Spotlight paper at ICLR 2024. This version includes acknowledgments\n  omitted from the ICLR version and indicates the corresponding authors\n  primarily responsible for the work", "summary": "Unsupervised reinforcement learning (URL) aims to learn general skills for\nunseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL\nby maximizing the mutual information between states and skills but lacks\nsufficient theoretical analysis, e.g., how well its learned skills can\ninitialize a downstream task's policy. Our new theoretical analysis in this\npaper shows that the diversity and separability of learned skills are\nfundamentally critical to downstream task adaptation but MISL does not\nnecessarily guarantee these properties. To complement MISL, we propose a novel\ndisentanglement metric LSEPIN. Moreover, we build an information-geometric\nconnection between LSEPIN and downstream task adaptation cost. For better\ngeometric properties, we investigate a new strategy that replaces the KL\ndivergence in information geometry with Wasserstein distance. We extend the\ngeometric analysis to it, which leads to a novel skill-learning objective WSEP.\nIt is theoretically justified to be helpful to downstream task adaptation and\nit is capable of discovering more initial policies for downstream tasks than\nMISL. We finally propose another Wasserstein distance-based algorithm PWSEP\nthat can theoretically discover all optimal initial policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6280\u80fd\u5b66\u4e60\u76ee\u6807WSEP\u4ee5\u53ca\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u7b97\u6cd5PWSEP\uff0c\u65e8\u5728\u6539\u8fdbMISL\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u4e3a\u4e0b\u6e38\u4efb\u52a1\u53d1\u73b0\u66f4\u591a\u7684\u521d\u59cb\u7b56\u7565\u3002", "motivation": "\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff08URL\uff09\u7684\u76ee\u6807\u662f\u5b66\u4e60\u9002\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u4e0b\u6e38\u4efb\u52a1\u7684\u4e00\u822c\u6280\u80fd\u3002\u5c3d\u7ba1\u4e92\u4fe1\u606f\u6280\u80fd\u5b66\u4e60\uff08MISL\uff09\u901a\u8fc7\u6700\u5927\u5316\u72b6\u6001\u4e0e\u6280\u80fd\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u89e3\u51b3URL\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u8db3\u591f\u7684\u7406\u8bba\u5206\u6790\u6765\u8bf4\u660e\u5176\u5b66\u5230\u7684\u6280\u80fd\u5982\u4f55\u521d\u59cb\u5316\u4e0b\u6e38\u4efb\u52a1\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u89e3\u7f20\u5ea6\u91cfLSEPIN\uff0c\u5e76\u5efa\u7acb\u4e86LSEPIN\u548c\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u6210\u672c\u4e4b\u95f4\u7684\u4fe1\u606f\u51e0\u4f55\u8054\u7cfb\u3002\u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u51e0\u4f55\u7279\u6027\uff0c\u7814\u7a76\u4e86\u7528Wasserstein\u8ddd\u79bb\u66ff\u6362\u4fe1\u606f\u51e0\u4f55\u4e2d\u7684KL\u6563\u5ea6\u7684\u65b0\u7b56\u7565\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u51e0\u4f55\u5206\u6790\uff0c\u4ece\u800c\u5f62\u6210\u4e86\u65b0\u7684\u6280\u80fd\u5b66\u4e60\u76ee\u6807WSEP\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u53e6\u4e00\u4e2a\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u7b97\u6cd5PWSEP\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86WSEP\u5bf9\u4e8e\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u662f\u6709\u5e2e\u52a9\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u6bd4MISL\u53d1\u73b0\u66f4\u591a\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u521d\u59cb\u7b56\u7565\u3002PWSEP\u5219\u88ab\u8bbe\u8ba1\u6210\u80fd\u591f\u7406\u8bba\u4e0a\u53d1\u73b0\u6240\u6709\u6700\u4f18\u521d\u59cb\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u6240\u5b66\u6280\u80fd\u7684\u591a\u6837\u6027\u548c\u53ef\u5206\u79bb\u6027\u5bf9\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u800cMISL\u5e76\u4e0d\u4fdd\u8bc1\u8fd9\u4e9b\u6027\u8d28\u3002\u63d0\u51fa\u7684WSEP\u548cPWSEP\u7b97\u6cd5\u6709\u671b\u6539\u5584\u8fd9\u79cd\u60c5\u51b5\uff0c\u5e76\u6709\u52a9\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u7b56\u7565\u521d\u59cb\u5316\u3002"}}
{"id": "2506.10419", "pdf": "https://arxiv.org/pdf/2506.10419", "abs": "https://arxiv.org/abs/2506.10419", "authors": ["Weiying Zhao", "Aleksei Unagaev", "Natalia Efremova"], "title": "Data-Driven Soil Organic Carbon Sampling: Integrating Spectral Clustering with Conditioned Latin Hypercube Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Soil organic carbon (SOC) monitoring often relies on selecting representative\nfield sampling locations based on environmental covariates. We propose a novel\nhybrid methodology that integrates spectral clustering - an unsupervised\nmachine learning technique with conditioned Latin hypercube sampling (cLHS) to\nenhance the representativeness of SOC sampling. In our approach, spectral\nclustering partitions the study area into $K$ homogeneous zones using\nmultivariate covariate data, and cLHS is then applied within each zone to\nselect sampling locations that collectively capture the full diversity of\nenvironmental conditions. This hybrid spectral-cLHS method ensures that even\nminor but important environmental clusters are sampled, addressing a key\nlimitation of vanilla cLHS which can overlook such areas. We demonstrate on a\nreal SOC mapping dataset that spectral-cLHS provides more uniform coverage of\ncovariate feature space and spatial heterogeneity than standard cLHS. This\nimproved sampling design has the potential to yield more accurate SOC\npredictions by providing better-balanced training data for machine learning\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8c31\u805a\u7c7b\u548c\u6761\u4ef6\u62c9\u4e01\u8d85\u7acb\u65b9\u4f53\u91c7\u6837(cLHS)\u7684\u65b0\u6df7\u5408\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u571f\u58e4\u6709\u673a\u78b3(SOC)\u91c7\u6837\u7684\u4ee3\u8868\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u53d8\u91cf\u534f\u53d8\u91cf\u6570\u636e\u5c06\u7814\u7a76\u533a\u57df\u5212\u5206\u4e3aK\u4e2a\u540c\u8d28\u533a\u57df\uff0c\u5e76\u5728\u6bcf\u4e2a\u533a\u57df\u5185\u5e94\u7528cLHS\u6765\u9009\u62e9\u80fd\u591f\u5168\u9762\u6355\u6349\u73af\u5883\u6761\u4ef6\u591a\u6837\u6027\u7684\u91c7\u6837\u70b9\u3002\u5b9e\u9a8c\u8bc1\u660e\u6b64\u65b9\u6cd5\u6bd4\u6807\u51c6cLHS\u63d0\u4f9b\u4e86\u66f4\u5747\u5300\u7684\u534f\u53d8\u91cf\u7279\u5f81\u7a7a\u95f4\u548c\u7a7a\u95f4\u5f02\u8d28\u6027\u8986\u76d6\u3002", "motivation": "\u4f20\u7edf\u7684SOC\u76d1\u6d4b\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u73af\u5883\u534f\u53d8\u91cf\u9009\u53d6\u6709\u4ee3\u8868\u6027\u7684\u91ce\u5916\u91c7\u6837\u4f4d\u7f6e\uff0c\u4f46\u5355\u7eaf\u4f7f\u7528cLHS\u53ef\u80fd\u4f1a\u5ffd\u7565\u4e00\u4e9b\u91cd\u8981\u7684\u5c0f\u73af\u5883\u7c07\u7fa4\u3002\u4e3a\u4e86\u6539\u5584\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u597d\u7684\u6837\u672c\u591a\u6837\u6027\uff0c\u4ece\u800c\u53ef\u80fd\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4bSOC\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u8c31\u805a\u7c7b\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u7528\u4e8e\u6839\u636e\u591a\u53d8\u91cf\u534f\u53d8\u91cf\u6570\u636e\u5c06\u7814\u7a76\u533a\u57df\u5212\u5206\u6210K\u4e2a\u540c\u8d28\u533a\u3002\u7136\u540e\uff0c\u5728\u6bcf\u4e2a\u533a\u5185\u4f7f\u7528\u6761\u4ef6\u62c9\u4e01\u8d85\u7acb\u65b9\u4f53\u91c7\u6837(cLHS)\u6765\u6311\u9009\u51fa\u80fd\u591f\u53cd\u6620\u6574\u4e2a\u73af\u5883\u6761\u4ef6\u591a\u6837\u6027\u7684\u91c7\u6837\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684cLHS\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u5149\u8c31-cLHS\u65b9\u6cd5\u4e3a\u534f\u53d8\u91cf\u7279\u5f81\u7a7a\u95f4\u548c\u7a7a\u95f4\u5f02\u8d28\u6027\u63d0\u4f9b\u4e86\u66f4\u52a0\u5747\u5300\u7684\u8986\u76d6\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8c31\u805a\u7c7b\u5230cLHS\u4e2d\u5f62\u6210\u7684\u6df7\u5408\u65b9\u6cd5\u63d0\u9ad8\u4e86SOC\u91c7\u6837\u7684\u4ee3\u8868\u6027\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u5e73\u8861\u8bad\u7ec3\u6570\u636e\uff0c\u8fdb\u800c\u6709\u53ef\u80fd\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9SOC\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.10630", "pdf": "https://arxiv.org/pdf/2506.10630", "abs": "https://arxiv.org/abs/2506.10630", "authors": ["Yucong Luo", "Yitong Zhou", "Mingyue Cheng", "Jiahao Wang", "Daoyu Wang", "Tingyue Pan", "Jintao Zhang"], "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To advance time series forecasting (TSF), various methods have been proposed\nto improve prediction accuracy, evolving from statistical techniques to\ndata-driven deep learning architectures. Despite their effectiveness, most\nexisting methods still adhere to a fast thinking paradigm-relying on extracting\nhistorical patterns and mapping them to future values as their core modeling\nphilosophy, lacking an explicit thinking process that incorporates intermediate\ntime series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)\nhave shown remarkable multi-step reasoning capabilities, offering an\nalternative way to overcome these issues. However, prompt engineering alone\npresents several limitations - including high computational cost, privacy\nrisks, and limited capacity for in-depth domain-specific time series reasoning.\nTo address these limitations, a more promising approach is to train LLMs to\ndevelop slow thinking capabilities and acquire strong time series reasoning\nskills. For this purpose, we propose Time-R1, a two-stage reinforcement\nfine-tuning framework designed to enhance multi-step reasoning ability of LLMs\nfor time series forecasting. Specifically, the first stage conducts supervised\nfine-tuning for warmup adaptation, while the second stage employs reinforcement\nlearning to improve the model's generalization ability. Particularly, we design\na fine-grained multi-objective reward specifically for time series forecasting,\nand then introduce GRIP (group-based relative importance for policy\noptimization), which leverages non-uniform sampling to further encourage and\noptimize the model's exploration of effective reasoning paths. Experiments\ndemonstrate that Time-R1 significantly improves forecast performance across\ndiverse datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTime-R1\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u7ec6\u7c92\u5ea6\u591a\u76ee\u6807\u5956\u52b1\u673a\u5236\u4ee5\u53ca\u57fa\u4e8e\u7ec4\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff08GRIP\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u5386\u53f2\u6a21\u5f0f\u7684\u63d0\u53d6\u5e76\u6620\u5c04\u5230\u672a\u6765\u503c\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u660e\u786e\u7684\u4e2d\u95f4\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u8fc7\u7a0b\u3002\u867d\u7136\u65b0\u51fa\u73b0\u7684\u6162\u601d\u8003\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u793a\u4e86\u51fa\u8272\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4ec5\u4f9d\u9760\u63d0\u793a\u5de5\u7a0b\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u9690\u79c1\u98ce\u9669\u53ca\u9886\u57df\u7279\u5b9a\u65f6\u95f4\u5e8f\u5217\u6df1\u5165\u63a8\u7406\u80fd\u529b\u6709\u9650\u7b49\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u6162\u601d\u8003\u80fd\u529b\u548c\u5f3a\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6280\u80fd\u6210\u4e3a\u4e00\u79cd\u66f4\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Time-R1\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u6709\u76d1\u7763\u7684\u5fae\u8c03\u4ee5\u9002\u5e94\u521d\u6b65\u8c03\u6574\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7279\u522b\u5730\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u7ec6\u7c92\u5ea6\u591a\u76ee\u6807\u5956\u52b1\uff0c\u5e76\u5f15\u5165\u4e86GRIP\uff08\u57fa\u4e8e\u7fa4\u4f53\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u653f\u7b56\u4f18\u5316\uff09\uff0c\u5229\u7528\u975e\u5747\u5300\u62bd\u6837\u8fdb\u4e00\u6b65\u9f13\u52b1\u548c\u4f18\u5316\u6a21\u578b\u63a2\u7d22\u6709\u6548\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTime-R1\u80fd\u591f\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u663e\u8457\u6539\u5584\u9884\u6d4b\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0cTime-R1\u80fd\u591f\u6709\u6548\u5730\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u60c5\u51b5\u4e0b\u3002\u6240\u63d0\u51fa\u7684GRIP\u65b9\u6cd5\u4e5f\u8bc1\u660e\u4e86\u5176\u5728\u4fc3\u8fdb\u6a21\u578b\u53d1\u73b0\u66f4\u4f18\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.10434", "pdf": "https://arxiv.org/pdf/2506.10434", "abs": "https://arxiv.org/abs/2506.10434", "authors": ["Nart Gashi", "Panagiotis Kakosimos", "George Papafotiou"], "title": "System Identification Using Kolmogorov-Arnold Networks: A Case Study on Buck Converters", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Kolmogorov-Arnold Networks (KANs) are emerging as a powerful framework for\ninterpretable and efficient system identification in dynamic systems. By\nleveraging the Kolmogorov-Arnold representation theorem, KANs enable function\napproximation through learnable activation functions, offering improved\nscalability, accuracy, and interpretability compared to traditional neural\nnetworks. This paper investigates the application of KANs to model and analyze\nthe dynamics of a buck converter system, focusing on state-space parameter\nestimation along with discovering the system equations. Using simulation data,\nthe methodology involves approximating state derivatives with KANs,\nconstructing interpretable state-space representations, and validating these\nmodels through numerical experiments. The results demonstrate the ability of\nKANs to accurately identify system dynamics, verify model consistency, and\ndetect parameter changes, providing valuable insights into their applicability\nfor system identification in modern industrial systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u5728\u964d\u538b\u8f6c\u6362\u5668\u7cfb\u7edf\u52a8\u6001\u5efa\u6a21\u548c\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86KANs\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u9a8c\u8bc1\u6a21\u578b\u4e00\u81f4\u6027\u548c\u68c0\u6d4b\u53c2\u6570\u53d8\u5316\u7684\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22Kolmogorov-Arnold\u7f51\u7edc\u4f5c\u4e3a\u4e00\u79cd\u5f3a\u5927\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u6765\u8bc6\u522b\u52a8\u6001\u7cfb\u7edf\u7684\u72b6\u6001\u7a7a\u95f4\u53c2\u6570\uff0c\u5e76\u53d1\u73b0\u7cfb\u7edf\u65b9\u7a0b\u3002", "method": "\u4f7f\u7528KANs\u8fd1\u4f3c\u72b6\u6001\u5bfc\u6570\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u72b6\u6001\u7a7a\u95f4\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cKANs\u80fd\u591f\u51c6\u786e\u5730\u8bc6\u522b\u7cfb\u7edf\u52a8\u6001\uff0c\u9a8c\u8bc1\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u5e76\u68c0\u6d4b\u53c2\u6570\u53d8\u5316\u3002", "conclusion": "KANs\u4e3a\u73b0\u4ee3\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u7684\u7cfb\u7edf\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u66f4\u4f18\u9009\u62e9\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.10647", "pdf": "https://arxiv.org/pdf/2506.10647", "abs": "https://arxiv.org/abs/2506.10647", "authors": ["Lang Yin", "Debangshu Banerjee", "Gagandeep Singh"], "title": "Data Shifts Hurt CoT: A Theoretical Study", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u4e25\u683c\u63a2\u8ba8\u4e86\u6570\u636e\u504f\u79fb\u5bf9Chain of Thought (CoT)\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3$k$-parity\u95ee\u9898\u65f6\u8054\u5408\u8003\u8651\u5206\u5e03\u504f\u79fb\u548c\u6570\u636e\u4e2d\u6bd2\u4e24\u79cd\u6570\u636e\u504f\u79fb\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u63ed\u793a\u4e86\u4f7f\u7528CoT\u65b9\u6cd5\u5728\u5b66\u4e60\u5947\u5076\u6027\u95ee\u9898\u4e0a\u53ef\u80fd\u6bd4\u76f4\u63a5\u751f\u6210\u9884\u6d4b\u8868\u73b0\u66f4\u5dee\u7684\u73b0\u8c61\uff0c\u5e76\u5bf9\u6b64\u73b0\u8c61\u7684\u673a\u5236\u539f\u56e0\u8fdb\u884c\u4e86\u4e25\u8c28\u4e14\u5168\u9762\u7684\u89e3\u91ca\u3002", "motivation": "\u5c3d\u7ba1\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\u901a\u8fc7Chain of Thought (CoT)\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3atransformers\u5904\u7406\u590d\u6742\u8ba1\u7b97\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u5047\u8bbe\uff1a\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u76f8\u540c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u4e0d\u542b\u9519\u8bef\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u63a2\u7a76\u5f53\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fd9\u4e9b\u5047\u8bbe\u4e0d\u6210\u7acb\u65f6\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u504f\u79fb\uff08\u5982\u5206\u5e03\u504f\u79fb\u3001\u6570\u636e\u4e2d\u6bd2\uff09\u5982\u4f55\u5f71\u54cd\u91c7\u7528CoT\u5206\u89e3\u8bad\u7ec3\u6240\u5f97\u6a21\u578b\u7684\u8d28\u91cf\u3002", "method": "\u7814\u7a76\u96c6\u4e2d\u5728$k$-parity\u95ee\u9898\u4e0a\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u5f53\u51fa\u73b0\u5206\u5e03\u504f\u79fb\u548c\u6570\u636e\u4e2d\u6bd2\u8fd9\u4e24\u79cd\u60c5\u51b5\u65f6\uff0c\u5229\u7528\u6210\u719fCoT\u6280\u672f\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u6027\u80fd\u53d8\u5316\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u9047\u5230\u7279\u5b9a\u7c7b\u578b\u7684\u6570\u636e\u504f\u79fb\u60c5\u51b5\u4e0b\uff0c\u91c7\u7528CoT\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5b66\u4e60$k$-parity\u95ee\u9898\u65f6\u7684\u8868\u73b0\u53cd\u800c\u4e0d\u5982\u76f4\u63a5\u8fdb\u884c\u9884\u6d4b\u7684\u65b9\u6cd5\uff1b\u5e76\u4ece\u673a\u5236\u5c42\u9762\u8be6\u7ec6\u89e3\u91ca\u4e86\u80cc\u540e\u7684\u539f\u56e0\u3002", "conclusion": "\u867d\u7136CoT\u5bf9\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u6709\u6548\uff0c\u4f46\u5728\u9762\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u7684\u6570\u636e\u504f\u79fb\u6311\u6218\u65f6\uff0c\u5176\u6548\u679c\u53ef\u80fd\u4f1a\u53d7\u5230\u8d1f\u9762\u5f71\u54cd\u3002\u6b64\u53d1\u73b0\u63d0\u793a\u672a\u6765\u7814\u7a76\u9700\u66f4\u52a0\u91cd\u89c6\u63d0\u9ad8\u6a21\u578b\u5bf9\u6297\u6b64\u7c7b\u95ee\u9898\u7684\u80fd\u529b\u3002"}}
{"id": "2506.10443", "pdf": "https://arxiv.org/pdf/2506.10443", "abs": "https://arxiv.org/abs/2506.10443", "authors": ["Zhaode Wang", "Jingbang Yang", "Xinyu Qian", "Shiwen Xing", "Xiaotang Jiang", "Chengfei Lv", "Shengyu Zhang"], "title": "MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices", "categories": ["cs.LG"], "comment": "7 pages, 5 figures. Published in the Proceedings of the 6th ACM\n  International Conference on Multimedia in Asia Workshops (MMAsia '24\n  Workshops). The final authenticated version is available at\n  https://dl.acm.org/doi/10.1145/3700410.3702126", "summary": "Large language models (LLMs) have demonstrated exceptional performance across\na variety of tasks. However, their substantial scale leads to significant\ncomputational resource consumption during inference, resulting in high costs.\nConsequently, edge device inference presents a promising solution. The primary\nchallenges of edge inference include memory usage and inference speed. This\npaper introduces MNN-LLM, a framework specifically designed to accelerate the\ndeployment of large language models on mobile devices. MNN-LLM addresses the\nruntime characteristics of LLMs through model quantization and DRAM-Flash\nhybrid storage, effectively reducing memory usage. It rearranges weights and\ninputs based on mobile CPU instruction sets and GPU characteristics while\nemploying strategies such as multicore load balancing, mixed-precision\nfloating-point operations, and geometric computations to enhance performance.\nNotably, MNN-LLM achieves up to a 8.6x speed increase compared to current\nmainstream LLM-specific frameworks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMNN-LLM\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u91cf\u5316\u548cDRAM-Flash\u6df7\u5408\u5b58\u50a8\u6280\u672f\u6765\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u4e14\u901a\u8fc7\u591a\u79cd\u7b56\u7565\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u76f8\u6bd4\u4e8e\u5f53\u524d\u4e3b\u6d41\u7684LLM\u7279\u5b9a\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe8.6\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5de8\u5927\u7684\u89c4\u6a21\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5de8\u5927\uff0c\u6210\u672c\u9ad8\u6602\u3002\u8fb9\u7f18\u8bbe\u5907\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u901f\u5ea6\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "MNN-LLM\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u6a21\u578b\u91cf\u5316\u548cDRAM-Flash\u6df7\u5408\u5b58\u50a8\u6765\u89e3\u51b3LLMs\u8fd0\u884c\u65f6\u7684\u7279\u70b9\uff0c\u4ece\u800c\u6709\u6548\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u3002\u6b64\u5916\uff0c\u6839\u636e\u79fb\u52a8CPU\u6307\u4ee4\u96c6\u548cGPU\u7279\u6027\u91cd\u65b0\u6392\u5217\u6743\u91cd\u548c\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u591a\u6838\u8d1f\u8f7d\u5747\u8861\u3001\u6df7\u5408\u7cbe\u5ea6\u6d6e\u70b9\u8fd0\u7b97\u4ee5\u53ca\u51e0\u4f55\u8ba1\u7b97\u7b49\u7b56\u7565\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "result": "MNN-LLM\u4e0e\u5f53\u524d\u4e3b\u6d41\u7684LLM\u4e13\u7528\u6846\u67b6\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u8fbe8.6\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "MNN-LLM\u6846\u67b6\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u4f18\u5316\u63aa\u65bd\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u3002"}}
{"id": "2506.10680", "pdf": "https://arxiv.org/pdf/2506.10680", "abs": "https://arxiv.org/abs/2506.10680", "authors": ["Igor Urbanik", "Pawe\u0142 Gajewski"], "title": "Saturation Self-Organizing Map", "categories": ["cs.LG", "cs.AI"], "comment": "github repository: https://github.com/Radinyn/satsom", "summary": "Continual learning poses a fundamental challenge for neural systems, which\noften suffer from catastrophic forgetting when exposed to sequential tasks.\nSelf-Organizing Maps (SOMs), despite their interpretability and efficiency, are\nnot immune to this issue. In this paper, we introduce Saturation\nSelf-Organizing Maps (SatSOM)-an extension of SOMs designed to improve\nknowledge retention in continual learning scenarios. SatSOM incorporates a\nnovel saturation mechanism that gradually reduces the learning rate and\nneighborhood radius of neurons as they accumulate information. This effectively\nfreezes well-trained neurons and redirects learning to underutilized areas of\nthe map.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaturation Self-Organizing Maps (SatSOM)\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u9971\u548c\u673a\u5236\u6765\u51cf\u5c11\u795e\u7ecf\u5143\u7684\u5b66\u4e60\u7387\u548c\u90bb\u57df\u534a\u5f84\uff0c\u4ece\u800c\u63d0\u9ad8\u8fde\u7eed\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u5bf9\u795e\u7ecf\u7cfb\u7edf\u6784\u6210\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u6311\u6218\uff0c\u5f53\u9762\u5bf9\u987a\u5e8f\u4efb\u52a1\u65f6\uff0c\u5b83\u4eec\u7ecf\u5e38\u906d\u53d7\u707e\u96be\u6027\u7684\u9057\u5fd8\u3002\u5c3d\u7ba1\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOMs\uff09\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u4f46\u4e5f\u4e0d\u80fd\u5e78\u514d\u4e8e\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Saturation Self-Organizing Maps (SatSOM)\uff0c\u5b83\u662fSOMs\u7684\u4e00\u79cd\u6269\u5c55\uff0c\u8bbe\u8ba1\u7528\u4e8e\u6539\u5584\u6301\u7eed\u5b66\u4e60\u60c5\u666f\u4e0b\u7684\u77e5\u8bc6\u4fdd\u7559\u3002SatSOM\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9971\u548c\u673a\u5236\uff0c\u968f\u7740\u795e\u7ecf\u5143\u7d2f\u79ef\u4fe1\u606f\uff0c\u8be5\u673a\u5236\u9010\u6e10\u51cf\u5c11\u5176\u5b66\u4e60\u7387\u548c\u90bb\u57df\u534a\u5f84\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u51bb\u7ed3\u4e86\u8bad\u7ec3\u826f\u597d\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u5c06\u5b66\u4e60\u91cd\u5b9a\u5411\u5230\u6620\u5c04\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u533a\u57df\u3002", "conclusion": "SatSOM\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u5f0f\u6765\u5bf9\u6297\u5728\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u8c03\u6574\u795e\u7ecf\u5143\u7684\u5b66\u4e60\u901f\u7387\u548c\u90bb\u5c45\u8303\u56f4\u6765\u4fdd\u6301\u5df2\u6709\u77e5\u8bc6\u7684\u540c\u65f6\u5438\u6536\u65b0\u4fe1\u606f\u3002"}}
{"id": "2506.10532", "pdf": "https://arxiv.org/pdf/2506.10532", "abs": "https://arxiv.org/abs/2506.10532", "authors": ["Fran\u00e7ois Cornet", "Grigory Bartosh", "Mikkel N. Schmidt", "Christian A. Naesseth"], "title": "Equivariant Neural Diffusion for Molecule Generation", "categories": ["cs.LG"], "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)", "summary": "We introduce Equivariant Neural Diffusion (END), a novel diffusion model for\nmolecule generation in 3D that is equivariant to Euclidean transformations.\nCompared to current state-of-the-art equivariant diffusion models, the key\ninnovation in END lies in its learnable forward process for enhanced generative\nmodelling. Rather than pre-specified, the forward process is parameterized\nthrough a time- and data-dependent transformation that is equivariant to rigid\ntransformations. Through a series of experiments on standard molecule\ngeneration benchmarks, we demonstrate the competitive performance of END\ncompared to several strong baselines for both unconditional and conditional\ngeneration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u5206\u5b50\u751f\u6210\u6a21\u578bEquivariant Neural Diffusion (END)\uff0c\u8be5\u6a21\u578b\u5bf9\u6b27\u51e0\u91cc\u5f97\u53d8\u6362\u5177\u6709\u7b49\u53d8\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u524d\u5411\u8fc7\u7a0b\u6539\u8fdb\u4e86\u751f\u6210\u5efa\u6a21\u3002\u5b9e\u9a8c\u8868\u660e\uff0cEND\u5728\u6807\u51c6\u5206\u5b50\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u591a\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u76f8\u6bd4\u8868\u73b0\u51fa\u4e86\u7ade\u4e89\u529b\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u7814\u7a76\u8005\u5f15\u5165\u4e86Equivariant Neural Diffuction (END) \u6a21\u578b\uff0c\u5b83\u5bf9\u521a\u6027\u53d8\u6362\u5177\u6709\u7b49\u53d8\u6027\u5e76\u4e14\u901a\u8fc7\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u524d\u5411\u8fc7\u7a0b\u6765\u63d0\u9ad8\u751f\u6210\u5efa\u6a21\u7684\u80fd\u529b\u3002", "method": "END\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5b83\u7684\u53ef\u5b66\u4e60\u524d\u5411\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u901a\u8fc7\u65f6\u95f4\u53ca\u6570\u636e\u76f8\u5173\u7684\u8f6c\u6362\u53c2\u6570\u5316\uff0c\u8fd9\u4e2a\u8f6c\u6362\u5bf9\u4e8e\u521a\u6027\u53d8\u6362\u662f\u7b49\u53d8\u7684\u3002", "result": "\u4e00\u7cfb\u5217\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEND\u5728\u6807\u51c6\u5206\u5b50\u751f\u6210\u57fa\u51c6\u4e0a\u4e0e\u82e5\u5e72\u5f3a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u65e0\u6761\u4ef6\u548c\u6709\u6761\u4ef6\u751f\u6210\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u7684\u6027\u80fd\u3002", "conclusion": "END\u6a21\u578b\u4e3a3D\u5206\u5b50\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6b27\u51e0\u91cc\u5f97\u53d8\u6362\u4fdd\u6301\u7b49\u53d8\u6027\u4ee5\u53ca\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u524d\u5411\u8fc7\u7a0b\uff0c\u5b83\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.10707", "pdf": "https://arxiv.org/pdf/2506.10707", "abs": "https://arxiv.org/abs/2506.10707", "authors": ["Marco Spinaci", "Marek Polewczyk", "Maximilian Schambach", "Sam Thelin"], "title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Tabular in-context learning (ICL) has recently achieved state-of-the-art\n(SOTA) performance on several tabular prediction tasks. Previously restricted\nto classification problems on small tables, recent advances such as TabPFN and\nTabICL have extended its use to larger datasets. While being architecturally\nefficient and well-adapted to tabular data structures, current table-native ICL\narchitectures, being trained exclusively on synthetic data, do not fully\nleverage the rich semantics and world knowledge contained in real-world tabular\ndata. On another end of this spectrum, tabular ICL models based on pretrained\nlarge language models such as TabuLa-8B integrate deep semantic understanding\nand world knowledge but are only able to make use of a small amount of context\ndue to inherent architectural limitations. With the aim to combine the best of\nboth these worlds, we introduce ConTextTab, integrating semantic understanding\nand alignment into a table-native ICL framework. By employing specialized\nembeddings for different data modalities and by training on large-scale\nreal-world tabular data, our model is competitive with SOTA across a broad set\nof benchmarks while setting a new standard on the semantically rich CARTE\nbenchmark.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ConTextTab\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u5bf9\u9f50\u7684\u8868\u683c\u539f\u751fICL\u6846\u67b6\u3002\u901a\u8fc7\u4f7f\u7528\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u4e13\u4e1a\u5d4c\u5165\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u6700\u5148\u8fdb\u6c34\u5e73\u76f8\u5f53\uff0c\u5e76\u5728\u8bed\u4e49\u4e30\u5bcc\u7684CARTE\u57fa\u51c6\u4e0a\u6811\u7acb\u4e86\u65b0\u7684\u6807\u51c6\u3002", "motivation": "\u76ee\u524d\u7684\u8868\u683c\u539f\u751fICL\u67b6\u6784\u867d\u7136\u5728\u5904\u7406\u8868\u683c\u6570\u636e\u65b9\u9762\u6548\u7387\u9ad8\u4e14\u9002\u5e94\u6027\u597d\uff0c\u4f46\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u548c\u4e16\u754c\u77e5\u8bc6\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u683cICL\u6a21\u578b\u80fd\u591f\u6574\u5408\u6df1\u5ea6\u8bed\u4e49\u7406\u89e3\u4e0e\u4e16\u754c\u77e5\u8bc6\uff0c\u4f46\u7531\u4e8e\u67b6\u6784\u9650\u5236\u53ea\u80fd\u5229\u7528\u5c11\u91cf\u4e0a\u4e0b\u6587\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u5c06\u4e24\u8005\u7684\u4f18\u52bf\u7ed3\u5408\u8d77\u6765\u3002", "method": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86ConTextTab\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8868\u683c\u539f\u751fICL\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u6a21\u5f0f\uff08\u5982\u6587\u672c\u3001\u6570\u503c\u7b49\uff09\u5b9a\u5236\u5316\u7684\u5d4c\u5165\u6280\u672f\uff0c\u5e76\u5728\u5927\u91cf\u7684\u5b9e\u9645\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002", "result": "ConTextTab\u5728\u4e00\u7cfb\u5217\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e86\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\u76f8\u5ab2\u7f8e\u7684\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u5728\u9700\u8981\u6df1\u5c42\u6b21\u8bed\u4e49\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\uff0c\u6bd4\u5982CARTE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fbe\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u9ad8\u5ea6\u3002", "conclusion": "ConTextTab\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u548c\u8868\u683c\u539f\u751fICL\u7684\u4f18\u70b9\uff0c\u4ece\u800c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6c34\u5e73\u7684\u8868\u73b0\u3002"}}
{"id": "2506.10536", "pdf": "https://arxiv.org/pdf/2506.10536", "abs": "https://arxiv.org/abs/2506.10536", "authors": ["Vasilis Michalakopoulos", "Christoforos Menos-Aikateriniadis", "Elissaios Sarmas", "Antonis Zakynthinos", "Pavlos S. Georgilakis", "Dimitris Askounis"], "title": "Data-driven Day Ahead Market Prices Forecasting: A Focus on Short Training Set Windows", "categories": ["cs.LG"], "comment": "13 pages, 10 figures", "summary": "This study investigates the performance of machine learning models in\nforecasting electricity Day-Ahead Market (DAM) prices using short historical\ntraining windows, with a focus on detecting seasonal trends and price spikes.\nWe evaluate four models, namely LSTM with Feed Forward Error Correction (FFEC),\nXGBoost, LightGBM, and CatBoost, across three European energy markets (Greece,\nBelgium, Ireland) using feature sets derived from ENTSO-E forecast data.\nTraining window lengths range from 7 to 90 days, allowing assessment of model\nadaptability under constrained data availability. Results indicate that\nLightGBM consistently achieves the highest forecasting accuracy and robustness,\nparticularly with 45 and 60 day training windows, which balance temporal\nrelevance and learning depth. Furthermore, LightGBM demonstrates superior\ndetection of seasonal effects and peak price events compared to LSTM and other\nboosting models. These findings suggest that short-window training approaches,\ncombined with boosting methods, can effectively support DAM forecasting in\nvolatile, data-scarce environments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4f7f\u7528\u77ed\u671f\u5386\u53f2\u8bad\u7ec3\u7a97\u53e3\u9884\u6d4b\u7535\u529b\u65e5\u524d\u5e02\u573a\u4ef7\u683c\u7684\u8868\u73b0\uff0c\u7279\u522b\u5173\u6ce8\u5b63\u8282\u6027\u8d8b\u52bf\u548c\u4ef7\u683c\u5cf0\u503c\u7684\u68c0\u6d4b\u3002LightGBM \u5728\u4e0d\u540c\u8bad\u7ec3\u7a97\u53e3\u957f\u5ea6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u572845\u5929\u548c60\u5929\u65f6\uff0c\u5728\u6ce2\u52a8\u5927\u3001\u6570\u636e\u7a00\u7f3a\u7684\u73af\u5883\u4e0b\u7ed3\u5408\u77ed\u7a97\u53e3\u8bad\u7ec3\u65b9\u6cd5\u548c\u63d0\u5347\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u652f\u6301\u65e5\u524d\u5e02\u573a\u7684\u9884\u6d4b\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5229\u7528\u6709\u9650\u7684\u5386\u53f2\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u65f6\u5bf9\u4e8e\u7535\u529b\u65e5\u524d\u5e02\u573a\u4ef7\u683c\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u7740\u91cd\u4e8e\u8bc6\u522b\u5b63\u8282\u6027\u6a21\u5f0f\u548c\u4ef7\u683c\u9ad8\u5cf0\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u56db\u79cd\u6a21\u578b\uff1a\u5e26\u524d\u9988\u8bef\u5dee\u6821\u6b63\uff08FFEC\uff09\u7684LSTM\u3001XGBoost\u3001LightGBM\u4ee5\u53caCatBoost\uff0c\u5728\u4e09\u4e2a\u6b27\u6d32\u80fd\u6e90\u5e02\u573a\uff08\u5e0c\u814a\u3001\u6bd4\u5229\u65f6\u3001\u7231\u5c14\u5170\uff09\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u4f7f\u7528\u7684\u7279\u5f81\u96c6\u6765\u6e90\u4e8eENTSO-E\u7684\u9884\u6d4b\u6570\u636e\uff0c\u8bad\u7ec3\u7a97\u53e3\u671f\u4ece7\u5929\u523090\u5929\u4e0d\u7b49\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLightGBM\u6301\u7eed\u8fbe\u5230\u6700\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u572845\u5929\u548c60\u5929\u8bad\u7ec3\u7a97\u53e3\u4e0b\u8868\u73b0\u6700\u597d\u3002\u6b64\u5916\uff0c\u76f8\u5bf9\u4e8eLSTM\u548c\u5176\u4ed6\u63d0\u5347\u6a21\u578b\uff0cLightGBM\u663e\u793a\u51fa\u66f4\u597d\u7684\u5b63\u8282\u6548\u5e94\u548c\u4ef7\u683c\u9ad8\u5cf0\u4e8b\u4ef6\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u91c7\u7528\u8f83\u77ed\u7684\u8bad\u7ec3\u7a97\u53e3\u4e0e\u63d0\u5347\u65b9\u6cd5\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u5373\u4f7f\u5728\u6570\u636e\u9ad8\u5ea6\u6ce2\u52a8\u4e14\u76f8\u5bf9\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u6709\u6548\u5730\u8f85\u52a9\u65e5\u524d\u5e02\u573a\u4ef7\u683c\u9884\u6d4b\u3002"}}
{"id": "2506.10831", "pdf": "https://arxiv.org/pdf/2506.10831", "abs": "https://arxiv.org/abs/2506.10831", "authors": ["Ravishka Rathnasuriya", "Tingxi Li", "Zexin Xu", "Zihe Song", "Mirazul Haque", "Simin Chen", "Wei Yang"], "title": "Efficiency Robustness of Dynamic Deep Learning Systems", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to USENIX Security '25", "summary": "Deep Learning Systems (DLSs) are increasingly deployed in real-time\napplications, including those in resourceconstrained environments such as\nmobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning\nSystems (DDLSs) adapt inference computation based on input complexity, reducing\noverhead. While this dynamic behavior improves efficiency, such behavior\nintroduces new attack surfaces. In particular, efficiency adversarial attacks\nexploit these dynamic mechanisms to degrade system performance. This paper\nsystematically explores efficiency robustness of DDLSs, presenting the first\ncomprehensive taxonomy of efficiency attacks. We categorize these attacks based\non three dynamic behaviors: (i) attacks on dynamic computations per inference,\n(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic\noutput production for downstream tasks. Through an in-depth evaluation, we\nanalyze adversarial strategies that target DDLSs efficiency and identify key\nchallenges in securing these systems. In addition, we investigate existing\ndefense mechanisms, demonstrating their limitations against increasingly\npopular efficiency attacks and the necessity for novel mitigation strategies to\nsecure future adaptive DDLSs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86\u52a8\u6001\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u6548\u7387\u9c81\u68d2\u6027\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u6548\u7387\u653b\u51fb\u7684\u5168\u9762\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u9488\u5bf9\u8fd9\u4e9b\u7cfb\u7edf\u6548\u7387\u7684\u5bf9\u6297\u7b56\u7565\u4ee5\u53ca\u73b0\u6709\u9632\u5fa1\u673a\u5236\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u52a8\u6001\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff08DDLSs\uff09\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u5982\u79fb\u52a8\u548c\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\uff0c\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\u800c\u5f15\u5165\u7684\u65b0\u653b\u51fb\u9762\u6210\u4e3a\u5b89\u5168\u95ee\u9898\u3002\u6548\u7387\u5bf9\u6297\u653b\u51fb\u5229\u7528\u8fd9\u4e9b\u52a8\u6001\u673a\u5236\u6765\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u6df1\u5165\u8bc4\u4f30\u5206\u6790\u4e86\u9488\u5bf9DDLSs\u6548\u7387\u7684\u5bf9\u6297\u7b56\u7565\uff0c\u5e76\u57fa\u4e8e\u4e09\u79cd\u52a8\u6001\u884c\u4e3a\u5bf9\u653b\u51fb\u8fdb\u884c\u4e86\u5206\u7c7b\uff1a\u6bcf\u63a8\u7406\u7684\u52a8\u6001\u8ba1\u7b97\u653b\u51fb\u3001\u52a8\u6001\u63a8\u7406\u8fed\u4ee3\u653b\u51fb\u53ca\u9762\u5411\u4e0b\u6e38\u4efb\u52a1\u7684\u52a8\u6001\u8f93\u51fa\u4ea7\u751f\u653b\u51fb\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4fdd\u969c\u8fd9\u4e9b\u7cfb\u7edf\u5b89\u5168\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u8c03\u67e5\u4e86\u73b0\u6709\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u9762\u5bf9\u65e5\u76ca\u6d41\u884c\u7684\u6548\u7387\u653b\u51fb\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u672a\u6765\u9002\u5e94\u6027\u7684DDLSs\u6765\u8bf4\uff0c\u9700\u8981\u65b0\u7684\u7f13\u89e3\u7b56\u7565\u6765\u786e\u4fdd\u5176\u5b89\u5168\u6027\u3002"}}
{"id": "2506.10577", "pdf": "https://arxiv.org/pdf/2506.10577", "abs": "https://arxiv.org/abs/2506.10577", "authors": ["Pascal Plettenberg", "Andr\u00e9 Alcalde", "Bernhard Sick", "Josephine M. Thomas"], "title": "Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics", "categories": ["cs.LG"], "comment": null, "summary": "The design and optimization of Printed Circuit Board (PCB) schematics is\ncrucial for the development of high-quality electronic devices. Thereby, an\nimportant task is to optimize drafts by adding components that improve the\nrobustness and reliability of the circuit, e.g., pull-up resistors or\ndecoupling capacitors. Since there is a shortage of skilled engineers and\nmanual optimizations are very time-consuming, these best practices are often\nneglected. However, this typically leads to higher costs for troubleshooting in\nlater development stages as well as shortened product life cycles, resulting in\nan increased amount of electronic waste that is difficult to recycle. Here, we\npresent an approach for automating the addition of new components into PCB\nschematics by representing them as bipartite graphs and utilizing a node pair\nprediction model based on Graph Neural Networks (GNNs). We apply our approach\nto three highly relevant PCB design optimization tasks and compare the\nperformance of several popular GNN architectures on real-world datasets labeled\nby human experts. We show that GNNs can solve these problems with high accuracy\nand demonstrate that our approach offers the potential to automate PCB design\noptimizations in a time- and cost-efficient manner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8282\u70b9\u5bf9\u9884\u6d4b\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6dfb\u52a0\u65b0\u7684\u7ec4\u4ef6\u5230PCB\u539f\u7406\u56fe\u4e2d\uff0c\u4ee5\u4f18\u5316\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5173\u952e\u7684PCB\u8bbe\u8ba1\u4f18\u5316\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u5c55\u793a\u4e86GNN\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u65f6\u95f4\u548c\u6210\u672c\u6548\u76ca\u7684PCB\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u719f\u7ec3\u5de5\u7a0b\u5e08\u77ed\u7f3a\u548c\u624b\u52a8\u4f18\u5316\u975e\u5e38\u8017\u65f6\uff0c\u6700\u4f73\u5b9e\u8df5\u7ecf\u5e38\u88ab\u5ffd\u89c6\uff0c\u8fd9\u5bfc\u81f4\u4e86\u540e\u671f\u5f00\u53d1\u9636\u6bb5\u66f4\u9ad8\u7684\u6545\u969c\u6392\u9664\u6210\u672c\u3001\u4ea7\u54c1\u751f\u547d\u5468\u671f\u7f29\u77ed\u4ee5\u53ca\u96be\u4ee5\u56de\u6536\u7684\u7535\u5b50\u5e9f\u7269\u589e\u52a0\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u4f18\u5316PCB\u539f\u7406\u56fe\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u7535\u8def\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5c06PCB\u539f\u7406\u56fe\u8868\u793a\u4e3a\u4e8c\u5206\u56fe\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u8282\u70b9\u5bf9\u9884\u6d4b\u6a21\u578b\u6765\u5b9e\u73b0\u81ea\u52a8\u5316\u6dfb\u52a0\u65b0\u7ec4\u4ef6\u3002\u7814\u7a76\u56e2\u961f\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u4e86\u8fd9\u79cd\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4eba\u5de5\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGNNs\u53ef\u4ee5\u5728\u9ad8\u7cbe\u5ea6\u4e0b\u89e3\u51b3PCB\u8bbe\u8ba1\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6f5c\u529b\u4ee5\u65f6\u95f4\u548c\u6210\u672c\u6548\u76ca\u7684\u65b9\u5f0f\u81ea\u52a8\u5316PCB\u8bbe\u8ba1\u4f18\u5316\u3002", "conclusion": "\u57fa\u4e8eGNN\u7684\u65b9\u6cd5\u5bf9\u4e8e\u81ea\u52a8\u5316PCB\u8bbe\u8ba1\u4e2d\u7684\u7ec4\u4ef6\u6dfb\u52a0\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5b83\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u7684\u8d28\u91cf\uff0c\u8fd8\u53ef\u80fd\u51cf\u5c11\u56e0\u8bbe\u8ba1\u4e0d\u4f73\u800c\u5bfc\u81f4\u7684\u989d\u5916\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u3002"}}
{"id": "2506.10892", "pdf": "https://arxiv.org/pdf/2506.10892", "abs": "https://arxiv.org/abs/2506.10892", "authors": ["Subham Sekhar Sahoo", "Justin Deschenaux", "Aaron Gokaslan", "Guanghan Wang", "Justin Chiu", "Volodymyr Kuleshov"], "title": "The Diffusion Duality", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025. We provide the code at: https://github.com/s-sahoo/duo", "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDuo\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u9ad8\u65af\u6269\u6563\u4e2d\u7684\u5f3a\u5927\u6280\u672f\u8f6c\u79fb\u81f3\u5747\u5300\u72b6\u6001\u79bb\u6563\u6269\u6563\u6a21\u578b\u6765\u63d0\u9ad8\u6587\u672c\u751f\u6210\u7684\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5747\u5300\u72b6\u6001\u79bb\u6563\u6269\u6563\u6a21\u578b\u5177\u6709\u81ea\u6211\u7ea0\u6b63\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u6587\u672c\uff0c\u4f46\u5b83\u4eec\u7684\u8868\u73b0\u901a\u5e38\u4e0d\u5982\u81ea\u56de\u5f52\u6a21\u578b\u548c\u906e\u7f69\u6269\u6563\u6a21\u578b\u3002\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u901a\u8fc7\u65b0\u7684\u65b9\u6cd5\u6765\u7f29\u5c0f\u8fd9\u4e00\u8868\u73b0\u5dee\u8ddd\u3002", "method": "Duo\u65b9\u6cd5\u5229\u7528\u4e86\u4ece\u5e95\u5c42\u9ad8\u65af\u6269\u6563\u81ea\u7136\u4ea7\u751f\u7684\u5747\u5300\u72b6\u6001\u6269\u6563\u8fc7\u7a0b\uff0c\u5b83\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u4e00\u662f\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u5f15\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u8fd9\u53ef\u4ee5\u51cf\u5c11\u65b9\u5dee\u5e76\u52a0\u901f\u8bad\u7ec3\uff1b\u4e8c\u662f\u79bb\u6563\u4e00\u81f4\u6027\u84b8\u998f\u7b97\u6cd5\uff0c\u5b83\u5c06\u8fde\u7eed\u73af\u5883\u4e0b\u7684\u8fde\u8d2f\u6027\u84b8\u998f\u9002\u5e94\u5230\u79bb\u6563\u73af\u5883\u4e2d\uff0c\u4ece\u800c\u5927\u5927\u52a0\u5feb\u91c7\u6837\u901f\u5ea6\u3002", "result": "\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u7684\u6a21\u578b\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76843\u4e2a\u8d85\u8fc7\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u96f6\u6837\u672c\u56f0\u60d1\u5ea6\u3002\u6b64\u5916\uff0c\u79bb\u6563\u4e00\u81f4\u6027\u84b8\u998f\u7b97\u6cd5\u4f7f\u6269\u6563\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u51e0\u6b65\u751f\u6210\uff0c\u5e76\u4e14\u91c7\u6837\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "Duo\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u9ad8\u65af\u6269\u6563\u7684\u6280\u672f\u5e94\u7528\u4e8e\u5747\u5300\u72b6\u6001\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u901f\u5ea6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u3002"}}
{"id": "2506.10922", "pdf": "https://arxiv.org/pdf/2506.10922", "abs": "https://arxiv.org/abs/2506.10922", "authors": ["Adam Karvonen", "Samuel Marks"], "title": "Robustly Improving LLM Fairness in Realistic Settings via Interpretability", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in high-stakes hiring\napplications, making decisions that directly impact people's careers and\nlivelihoods. While prior studies suggest simple anti-bias prompts can eliminate\ndemographic biases in controlled evaluations, we find these mitigations fail\nwhen realistic contextual details are introduced. We address these failures\nthrough internal bias mitigation: by identifying and neutralizing sensitive\nattribute directions within model activations, we achieve robust bias reduction\nacross all tested scenarios. Across leading commercial (GPT-4o, Claude 4\nSonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,\nMistral-24B), we find that adding realistic context such as company names,\nculture descriptions from public careers pages, and selective hiring\nconstraints (e.g.,``only accept candidates in the top 10\\%\") induces\nsignificant racial and gender biases (up to 12\\% differences in interview\nrates). When these biases emerge, they consistently favor Black over White\ncandidates and female over male candidates across all tested models and\nscenarios. Moreover, models can infer demographics and become biased from\nsubtle cues like college affiliations, with these biases remaining invisible\neven when inspecting the model's chain-of-thought reasoning. To address these\nlimitations, our internal bias mitigation identifies race and gender-correlated\ndirections and applies affine concept editing at inference time. Despite using\ndirections from a simple synthetic dataset, the intervention generalizes\nrobustly, consistently reducing bias to very low levels (typically under 1\\%,\nalways below 2.5\\%) while largely maintaining model performance. Our findings\nsuggest that practitioners deploying LLMs for hiring should adopt more\nrealistic evaluation methodologies and consider internal mitigation strategies\nfor equitable outcomes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u5f15\u5165\u771f\u5b9e\u4e0a\u4e0b\u6587\u7ec6\u8282\u65f6\uff0c\u7b80\u5355\u7684\u53cd\u504f\u89c1\u63d0\u793a\u65e0\u6cd5\u6d88\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u90e8\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u4e2d\u548c\u6a21\u578b\u6fc0\u6d3b\u5185\u7684\u654f\u611f\u5c5e\u6027\u65b9\u5411\u6765\u5b9e\u73b0\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e0b\u7684\u7a33\u5065\u504f\u89c1\u51cf\u5c11\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u9ad8\u98ce\u9669\u62db\u8058\u573a\u666f\uff0c\u76f4\u63a5\u5f71\u54cd\u4e2a\u4eba\u804c\u4e1a\u548c\u751f\u8ba1\u7684\u51b3\u7b56\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u7b80\u5355\u7684\u53cd\u504f\u89c1\u63d0\u793a\u53ef\u4ee5\u5728\u63a7\u5236\u8bc4\u4f30\u4e2d\u6d88\u9664\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u4f46\u5728\u52a0\u5165\u73b0\u5b9e\u4e16\u754c\u7684\u4e0a\u4e0b\u6587\u7ec6\u8282\u540e\u8fd9\u4e9b\u63aa\u65bd\u5931\u6548\u4e86\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u79cd\u79f0\u4e3a\u5185\u90e8\u504f\u89c1\u7f13\u89e3\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff1a\u5373\u8bc6\u522b\u5e76\u4e2d\u548c\u6a21\u578b\u6fc0\u6d3b\u8fc7\u7a0b\u4e2d\u4e0e\u654f\u611f\u5c5e\u6027\u76f8\u5173\u7684\u65b9\u5411\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5e94\u7528\u4eff\u5c04\u6982\u5ff5\u7f16\u8f91\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u5373\u4f7f\u4f7f\u7528\u6765\u81ea\u7b80\u5355\u5408\u6210\u6570\u636e\u96c6\u7684\u65b9\u5411\u4fe1\u606f\u4e5f\u80fd\u591f\u5e7f\u6cdb\u9002\u7528\uff0c\u4ece\u800c\u5c06\u504f\u89c1\u663e\u8457\u964d\u4f4e\u81f3\u975e\u5e38\u4f4e\u6c34\u5e73\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5305\u62ec\u9876\u5c16\u5546\u4e1a\u7ea7\u53ca\u5f00\u6e90\u6a21\u578b\u5728\u5185\u7684\u591a\u4e2a\u9886\u5148\u6a21\u578b\u4e0a\uff0c\u589e\u52a0\u8bf8\u5982\u516c\u53f8\u540d\u79f0\u3001\u516c\u5171\u804c\u4e1a\u9875\u9762\u7684\u6587\u5316\u63cf\u8ff0\u4ee5\u53ca\u9009\u62e9\u6027\u62db\u8058\u7ea6\u675f\u7b49\u73b0\u5b9e\u60c5\u5883\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\uff08\u9762\u8bd5\u7387\u5dee\u5f02\u53ef\u8fbe12%\uff09\u3002\u91c7\u7528\u5185\u90e8\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u540e\uff0c\u80fd\u591f\u5c06\u504f\u89c1\u7a33\u5b9a\u5730\u964d\u81f3\u6781\u4f4e\u6c34\u5e73\uff08\u901a\u5e38\u4f4e\u4e8e1%\uff0c\u59cb\u7ec8\u4e0d\u8d85\u8fc72.5%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u57fa\u672c\u4e0d\u53d8\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u90e8\u7f72LLM\u8fdb\u884c\u62db\u8058\u5de5\u4f5c\u7684\u5b9e\u8df5\u8005\u5e94\u8be5\u91c7\u53d6\u66f4\u8d34\u8fd1\u5b9e\u9645\u60c5\u51b5\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8003\u8651\u91c7\u7528\u5185\u90e8\u7f13\u89e3\u7b56\u7565\u4ee5\u786e\u4fdd\u516c\u5e73\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.10616", "pdf": "https://arxiv.org/pdf/2506.10616", "abs": "https://arxiv.org/abs/2506.10616", "authors": ["Yu-Jie Zhang", "Peng Zhao", "Masashi Sugiyama"], "title": "Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Non-stationary online learning has drawn much attention in recent years.\nDespite considerable progress, dynamic regret minimization has primarily\nfocused on convex functions, leaving the functions with stronger curvature\n(e.g., squared or logistic loss) underexplored. In this work, we address this\ngap by showing that the regret can be substantially improved by leveraging the\nconcept of mixability, a property that generalizes exp-concavity to effectively\ncapture loss curvature. Let $d$ denote the dimensionality and $P_T$ the path\nlength of comparators that reflects the environmental non-stationarity. We\ndemonstrate that an exponential-weight method with fixed-share updates achieves\nan $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$ dynamic regret for mixable losses,\nimproving upon the best-known $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$\nresult (Baby and Wang, 2021) in $d$. More importantly, this improvement arises\nfrom a simple yet powerful analytical framework that exploits the mixability,\nwhich avoids the Karush-Kuhn-Tucker-based analysis required by existing work.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.10946", "pdf": "https://arxiv.org/pdf/2506.10946", "abs": "https://arxiv.org/abs/2506.10946", "authors": ["Evelyn Ma", "Duo Zhou", "Peizhi Niu", "Huiting Zhou", "Huan Zhang", "Olgica Milenkovic", "S. Rasoul Etesami"], "title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Unlearning in large language models (LLMs) is becoming increasingly important\ndue to regulatory compliance, copyright protection, and privacy concerns.\nHowever, a key challenge in LLM unlearning is unintended forgetting, where the\nremoval of specific data inadvertently impairs the utility of the model and its\nretention of valuable, desired information. While prior work has primarily\nfocused on architectural innovations, the influence of data-level factors on\nunlearning performance remains underexplored. As a result, existing methods\noften suffer from degraded retention when forgetting high-impact data. To\naddress this, we propose GUARD-a novel framework for Guided Unlearning And\nRetention via Data attribution. At its core, GUARD introduces a lightweight\nproxy data attribution metric tailored for LLM unlearning, which quantifies the\n\"alignment\" between the forget and retain sets while remaining computationally\nefficient. Building on this, we design a novel unlearning objective that\nassigns adaptive, nonuniform unlearning weights to samples, inversely\nproportional to their proxy attribution scores. Through such a reallocation of\nunlearning power, GUARD mitigates unintended losses in retention. We provide\nrigorous theoretical guarantees that GUARD significantly enhances retention\nwhile maintaining forgetting metrics comparable to prior methods. Extensive\nexperiments on the TOFU benchmark across multiple LLM architectures demonstrate\nthat GUARD substantially improves utility preservation while ensuring effective\nunlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to\n194.92% in terms of Truth Ratio when forgetting 10% of the training data.", "AI": {"tldr": "\u63d0\u51fa\u4e86GUARD\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u5f52\u56e0\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53bb\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u5bf9\u9ad8\u5f71\u54cd\u6570\u636e\u9057\u5fd8\u65f6\u7684\u975e\u9884\u671f\u635f\u5931\uff0c\u5e76\u5728\u4fdd\u6301\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u4f3c\u7684\u9057\u5fd8\u5ea6\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4fdd\u7559\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6cd5\u89c4\u9075\u4ece\u3001\u7248\u6743\u4fdd\u62a4\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u53bb\u5b66\u4e60\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728LLM\u53bb\u5b66\u4e60\u4e2d\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u610f\u5916\u9057\u5fd8\uff0c\u5373\u5220\u9664\u7279\u5b9a\u6570\u636e\u4f1a\u65e0\u610f\u4e2d\u635f\u5bb3\u6a21\u578b\u7684\u5b9e\u7528\u6027\u53ca\u5176\u5bf9\u6709\u4ef7\u503c\u4fe1\u606f\u7684\u4fdd\u7559\u3002\u5c3d\u7ba1\u4e4b\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u67b6\u6784\u521b\u65b0\u4e0a\uff0c\u4f46\u6570\u636e\u5c42\u9762\u56e0\u7d20\u5bf9\u53bb\u5b66\u4e60\u8868\u73b0\u7684\u5f71\u54cd\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5728\u5fd8\u8bb0\u9ad8\u5f71\u54cd\u529b\u6570\u636e\u65f6\u4fdd\u7559\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGUARD\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6570\u636e\u5f52\u56e0\u6765\u6307\u5bfc\u53bb\u5b66\u4e60\u548c\u4fdd\u7559\u3002GUARD\u5f15\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u5f52\u56e0\u6307\u6807\uff0c\u4e13\u4e3aLLM\u53bb\u5b66\u4e60\u5b9a\u5236\uff0c\u53ef\u4ee5\u91cf\u5316\u201c\u5fd8\u8bb0\u201d\u96c6\u548c\u201c\u4fdd\u7559\u201d\u96c6\u4e4b\u95f4\u7684\u201c\u4e00\u81f4\u6027\u201d\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u57fa\u4e8e\u6b64\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u53bb\u5b66\u4e60\u76ee\u6807\uff0c\u5b83\u6839\u636e\u6837\u672c\u7684\u4ee3\u7406\u5f52\u56e0\u5206\u6570\u53cd\u6bd4\u5730\u5206\u914d\u81ea\u9002\u5e94\u3001\u975e\u5747\u5300\u7684\u53bb\u5b66\u4e60\u6743\u91cd\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGUARD\u5728\u786e\u4fdd\u6709\u6548\u53bb\u5b66\u4e60\u7684\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u4e86\u5b9e\u7528\u6027\u7684\u4fdd\u7559\u3002\u7279\u522b\u662f\u5728\u5fd8\u8bb010%\u7684\u8bad\u7ec3\u6570\u636e\u65f6\uff0cGUARD\u5c06\u4fdd\u7559\u96c6\u4e0a\u7684\u5b9e\u7528\u6027\u727a\u7272\u964d\u4f4e\u4e86\u9ad8\u8fbe194.92%\uff08\u4ee5\u771f\u5b9e\u7387\u8861\u91cf\uff09\u3002", "conclusion": "GUARD\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u53bb\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u5904\u7406\u65b9\u5f0f\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u8db3\u591f\u9057\u5fd8\u6548\u679c\u7684\u540c\u65f6\uff0c\u6781\u5927\u5730\u6539\u5584\u4e86\u6a21\u578b\u5bf9\u4e8e\u9700\u8981\u4fdd\u7559\u7684\u4fe1\u606f\u7684\u7ef4\u6301\u80fd\u529b\u3002"}}
{"id": "2506.10955", "pdf": "https://arxiv.org/pdf/2506.10955", "abs": "https://arxiv.org/abs/2506.10955", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "38 pages, 14 figures", "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.10628", "pdf": "https://arxiv.org/pdf/2506.10628", "abs": "https://arxiv.org/abs/2506.10628", "authors": ["Thu Ha Phi", "Alexandre Hippert-Ferrer", "Florent Bouchard", "Arnaud Breloy"], "title": "Leveraging Low-rank Factorizations of Conditional Correlation Matrices in Graph Learning", "categories": ["cs.LG", "eess.SP"], "comment": "11 pages, 5 figures", "summary": "This paper addresses the problem of learning an undirected graph from data\ngathered at each nodes. Within the graph signal processing framework, the\ntopology of such graph can be linked to the support of the conditional\ncorrelation matrix of the data. The corresponding graph learning problem then\nscales to the squares of the number of variables (nodes), which is usually\nproblematic at large dimension. To tackle this issue, we propose a graph\nlearning framework that leverages a low-rank factorization of the conditional\ncorrelation matrix. In order to solve for the resulting optimization problems,\nwe derive tools required to apply Riemannian optimization techniques for this\nparticular structure. The proposal is then particularized to a low-rank\nconstrained counterpart of the GLasso algorithm, i.e., the penalized maximum\nlikelihood estimation of a Gaussian graphical model. Experiments on synthetic\nand real data evidence that a very efficient dimension-versus-performance\ntrade-off can be achieved with this approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u76f8\u5173\u77e9\u9635\u4f4e\u79e9\u5206\u89e3\u7684\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9ece\u66fc\u4f18\u5316\u6280\u672f\u89e3\u51b3\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eGLasso\u7b97\u6cd5\u7684\u4e00\u4e2a\u4f4e\u79e9\u7ea6\u675f\u7248\u672c\uff0c\u4ece\u800c\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u7ef4\u5ea6-\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u9488\u5bf9\u4ece\u8282\u70b9\u6536\u96c6\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u65e0\u5411\u56fe\u7684\u95ee\u9898\uff0c\u5728\u56fe\u5f62\u4fe1\u53f7\u5904\u7406\u6846\u67b6\u4e0b\uff0c\u56fe\u7684\u62d3\u6251\u7ed3\u6784\u4e0e\u6570\u636e\u7684\u6761\u4ef6\u76f8\u5173\u77e9\u9635\u7684\u652f\u6301\u6709\u5173\u3002\u7136\u800c\uff0c\u5f53\u53d8\u91cf\u6570\u91cf\u5f88\u5927\u65f6\uff0c\u76f8\u5e94\u7684\u56fe\u5b66\u4e60\u95ee\u9898\u89c4\u6a21\u4f1a\u53d8\u5f97\u975e\u5e38\u5927\uff0c\u6210\u4e3a\u4e00\u4e2a\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6761\u4ef6\u76f8\u5173\u77e9\u9635\u7684\u4f4e\u79e9\u5206\u89e3\u6765\u51cf\u5c11\u95ee\u9898\u89c4\u6a21\u3002\u4e3a\u4e86\u89e3\u51b3\u7531\u6b64\u4ea7\u751f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u8fd9\u79cd\u7279\u5b9a\u7ed3\u6784\u7684\u9ece\u66fc\u4f18\u5316\u6280\u672f\u5de5\u5177\u3002\u6b64\u5916\uff0c\u8fd8\u7279\u522b\u5316\u4e86GLasso\u7b97\u6cd5\uff08\u5373\u9ad8\u65af\u56fe\u6a21\u578b\u7684\u60e9\u7f5a\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff09\u7684\u4f4e\u79e9\u7ea6\u675f\u7248\u672c\u3002", "result": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u826f\u597d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7ef4\u5ea6-\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u4f4e\u79e9\u5206\u89e3\u7684\u56fe\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2506.10959", "pdf": "https://arxiv.org/pdf/2506.10959", "abs": "https://arxiv.org/abs/2506.10959", "authors": ["Zhaiming Shen", "Alexander Hsu", "Rongjie Lai", "Wenjing Liao"], "title": "Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.TH"], "comment": null, "summary": "While in-context learning (ICL) has achieved remarkable success in natural\nlanguage and vision domains, its theoretical understanding--particularly in the\ncontext of structured geometric data--remains unexplored. In this work, we\ninitiate a theoretical study of ICL for regression of H\\\"older functions on\nmanifolds. By establishing a novel connection between the attention mechanism\nand classical kernel methods, we derive generalization error bounds in terms of\nthe prompt length and the number of training tasks. When a sufficient number of\ntraining tasks are observed, transformers give rise to the minimax regression\nrate of H\\\"older functions on manifolds, which scales exponentially with the\nintrinsic dimension of the manifold, rather than the ambient space dimension.\nOur result also characterizes how the generalization error scales with the\nnumber of training tasks, shedding light on the complexity of transformers as\nin-context algorithm learners. Our findings provide foundational insights into\nthe role of geometry in ICL and novels tools to study ICL of nonlinear models.", "AI": {"tldr": "\u672c\u6587\u4ece\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u5728\u6d41\u5f62\u4e0a\u7684H\u00f6lder\u51fd\u6570\u56de\u5f52\u4e2d\u60c5\u5883\u5b66\u4e60\uff08ICL\uff09\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u7ecf\u5178\u6838\u65b9\u6cd5\u8054\u7cfb\u8d77\u6765\uff0c\u5f97\u51fa\u4e86\u6cdb\u5316\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u63ed\u793a\u4e86\u51e0\u4f55\u7ed3\u6784\u5728ICL\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1\u60c5\u5883\u5b66\u4e60\uff08ICL\uff09\u5728\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u51e0\u4f55\u6570\u636e\u80cc\u666f\u4e0b\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u5efa\u7acb\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u7ecf\u5178\u6838\u65b9\u6cd5\u4e4b\u95f4\u7684\u65b0\u8054\u7cfb\uff0c\u63a8\u5bfc\u51fa\u57fa\u4e8e\u63d0\u793a\u957f\u5ea6\u548c\u8bad\u7ec3\u4efb\u52a1\u6570\u91cf\u7684\u6cdb\u5316\u8bef\u5dee\u754c\u3002", "result": "\u5f53\u89c2\u5bdf\u5230\u8db3\u591f\u591a\u7684\u8bad\u7ec3\u4efb\u52a1\u65f6\uff0c\u8f6c\u6362\u5668\u80fd\u591f\u8fbe\u5230\u6d41\u5f62\u4e0aH\u00f6lder\u51fd\u6570\u7684\u6700\u5c0f\u6700\u5927\u56de\u5f52\u7387\uff0c\u8be5\u56de\u5f52\u7387\u968f\u6d41\u5f62\u5185\u5728\u7ef4\u5ea6\u800c\u975e\u73af\u5883\u7a7a\u95f4\u7ef4\u5ea6\u5448\u6307\u6570\u589e\u957f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u51e0\u4f55\u5728ICL\u4e2d\u7684\u89d2\u8272\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u7684\u89c1\u89e3\uff0c\u5e76\u4e14\u4e3a\u7814\u7a76\u975e\u7ebf\u6027\u6a21\u578b\u7684\u60c5\u5883\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2506.10972", "pdf": "https://arxiv.org/pdf/2506.10972", "abs": "https://arxiv.org/abs/2506.10972", "authors": ["Houyi Li", "Wenzhen Zheng", "Qiufeng Wang", "Zhenyu Ding", "Haoying Wang", "Zili Wang", "Shijie Xuyang", "Ning Ding", "Shuigeng Zhou", "Xiangyu Zhang", "Daxin Jiang"], "title": "Farseer: A Refined Scaling Law in Large Language Models", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": "34", "summary": "Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u5c55\u6cd5\u5219Farseer\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u7ea61000\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u914d\u7f6e\u7684LLM\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\u6210\u672c\u975e\u5e38\u9ad8\u6602\uff0c\u5bfc\u81f4\u4ece\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u7684\u6d1e\u89c1\u5f80\u5f80\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u8d44\u6e90\u5bc6\u96c6\u578b\u751f\u4ea7\u7cfb\u7edf\uff0c\u963b\u788d\u4e86\u9ad8\u6548\u521b\u65b0\u3002", "method": "\u7814\u7a76\u8005\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aFarseer\u7684\u65b0\u6269\u5c55\u6cd5\u5219\uff0c\u5b83\u901a\u8fc7\u7cfb\u7edf\u5730\u5efa\u7acb\u6a21\u578b\u635f\u5931\u9762$L(N,D)$\u6765\u63d0\u4f9b\u66f4\u597d\u7684\u7ecf\u9a8c\u6570\u636e\u62df\u5408\u5ea6\u3002", "result": "Farseer\u76f8\u6bd4\u4e4b\u524d\u7684\u6cd5\u5219\uff08\u5982Chinchilla\u5b9a\u5f8b\uff09\u63d0\u9ad8\u4e86433%\u7684\u5916\u63a8\u8bef\u5dee\u51cf\u5c11\u7387\uff0c\u5141\u8bb8\u5728\u6240\u6709$(N,D)$\u8bbe\u7f6e\u4e0b\u53ef\u9760\u5730\u8bc4\u4f30\u7ade\u4e89\u6027\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4e3a\u6700\u4f18\u8ba1\u7b97\u5206\u914d\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "conclusion": "Farseer\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u5c0f\u89c4\u6a21\u6d88\u878d\u7814\u7a76\u7ed3\u679c\u5230\u5927\u89c4\u6a21\u6027\u80fd\u7684\u53ef\u4fe1\u5916\u63a8\uff0c\u5e76\u4e14\u5bf9\u4e8e\u73b0\u4ee3LLM\u8bad\u7ec3\u7684\u590d\u6742\u9700\u6c42\u6709\u4e86\u66f4\u597d\u7684\u53cd\u6620\u3002"}}
{"id": "2506.10973", "pdf": "https://arxiv.org/pdf/2506.10973", "abs": "https://arxiv.org/abs/2506.10973", "authors": ["Julius Berner", "Miguel Liu-Schiaffini", "Jean Kossaifi", "Valentin Duruisseaux", "Boris Bonev", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "title": "Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.FA", "math.NA"], "comment": null, "summary": "A wide range of scientific problems, such as those described by\ncontinuous-time dynamical systems and partial differential equations (PDEs),\nare naturally formulated on function spaces. While function spaces are\ntypically infinite-dimensional, deep learning has predominantly advanced\nthrough applications in computer vision and natural language processing that\nfocus on mappings between finite-dimensional spaces. Such fundamental\ndisparities in the nature of the data have limited neural networks from\nachieving a comparable level of success in scientific applications as seen in\nother fields. Neural operators are a principled way to generalize neural\nnetworks to mappings between function spaces, offering a pathway to replicate\ndeep learning's transformative impact on scientific problems. For instance,\nneural operators can learn solution operators for entire classes of PDEs, e.g.,\nphysical systems with different boundary conditions, coefficient functions, and\ngeometries. A key factor in deep learning's success has been the careful\nengineering of neural architectures through extensive empirical testing.\nTranslating these neural architectures into neural operators allows operator\nlearning to enjoy these same empirical optimizations. However, prior neural\noperator architectures have often been introduced as standalone models, not\ndirectly derived as extensions of existing neural network architectures. In\nthis paper, we identify and distill the key principles for constructing\npractical implementations of mappings between infinite-dimensional function\nspaces. Using these principles, we propose a recipe for converting several\npopular neural architectures into neural operators with minimal modifications.\nThis paper aims to guide practitioners through this process and details the\nsteps to make neural operators work in practice. Our code can be found at\nhttps://github.com/neuraloperator/NNs-to-NOs", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u73b0\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8f6c\u5316\u4e3a\u795e\u7ecf\u7b97\u5b50\uff0c\u4ee5\u5904\u7406\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\u95ee\u9898\uff0c\u5e76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u73b0\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u8fd9\u4e9b\u5e94\u7528\u4e3b\u8981\u96c6\u4e2d\u5728\u6709\u9650\u7ef4\u7a7a\u95f4\u7684\u6620\u5c04\u4e0a\u3002\u7136\u800c\u79d1\u5b66\u95ee\u9898\u5f80\u5f80\u6d89\u53ca\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\uff0c\u5982\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u7cfb\u7edf\u548c\u504f\u5fae\u5206\u65b9\u7a0b\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63a8\u5e7f\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u51fd\u6570\u7a7a\u95f4\u95f4\u7684\u6620\u5c04\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u6d41\u884c\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8f6c\u6362\u6210\u795e\u7ecf\u7b97\u5b50\u7684\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u57fa\u4e8e\u5bf9\u6784\u5efa\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u4e4b\u95f4\u5b9e\u9645\u6620\u5c04\u7684\u5173\u952e\u539f\u5219\u7684\u7406\u89e3\u3002", "result": "\u901a\u8fc7\u9075\u5faa\u6240\u63d0\u51fa\u7684\u539f\u5219\uff0c\u53ef\u4ee5\u5c06\u51e0\u79cd\u6d41\u884c\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8f6c\u53d8\u4e3a\u795e\u7ecf\u7b97\u5b50\uff0c\u53ea\u9700\u8fdb\u884c\u6700\u5c0f\u7684\u4fee\u6539\u3002", "conclusion": "\u8bba\u6587\u65e8\u5728\u4e3a\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5c06\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8f6c\u53d8\u4e3a\u795e\u7ecf\u7b97\u5b50\uff0c\u4ee5\u4fbf\u89e3\u51b3\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u6620\u5c04\u95ee\u9898\u3002"}}
{"id": "2506.10632", "pdf": "https://arxiv.org/pdf/2506.10632", "abs": "https://arxiv.org/abs/2506.10632", "authors": ["Alexander Lobashev", "Dmitry Guskov", "Maria Larchenko", "Mikhail Tamm"], "title": "Hessian Geometry of Latent Space in Generative Models", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.CV", "math.DG", "math.ST", "stat.TH"], "comment": "ICML 2025", "summary": "This paper presents a novel method for analyzing the latent space geometry of\ngenerative models, including statistical physics models and diffusion models,\nby reconstructing the Fisher information metric. The method approximates the\nposterior distribution of latent variables given generated samples and uses\nthis to learn the log-partition function, which defines the Fisher metric for\nexponential families. Theoretical convergence guarantees are provided, and the\nmethod is validated on the Ising and TASEP models, outperforming existing\nbaselines in reconstructing thermodynamic quantities. Applied to diffusion\nmodels, the method reveals a fractal structure of phase transitions in the\nlatent space, characterized by abrupt changes in the Fisher metric. We\ndemonstrate that while geodesic interpolations are approximately linear within\nindividual phases, this linearity breaks down at phase boundaries, where the\ndiffusion model exhibits a divergent Lipschitz constant with respect to the\nlatent space. These findings provide new insights into the complex structure of\ndiffusion model latent spaces and their connection to phenomena like phase\ntransitions. Our source code is available at\nhttps://github.com/alobashev/hessian-geometry-of-diffusion-models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u91cd\u5efaFisher\u4fe1\u606f\u5ea6\u91cf\u6765\u5206\u6790\u751f\u6210\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u5f62\u72b6\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u7edf\u8ba1\u7269\u7406\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u76f8\u53d8\u5206\u5f62\u7ed3\u6784\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\uff0c\u7279\u522b\u662f\u4e0e\u7edf\u8ba1\u7269\u7406\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u76f8\u5173\u7684\u7ed3\u6784\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u70ed\u529b\u5b66\u91cf\u7684\u91cd\u6784\u3002", "method": "\u672c\u7814\u7a76\u7684\u65b9\u6cd5\u662f\u57fa\u4e8e\u91cd\u5efaFisher\u4fe1\u606f\u5ea6\u91cf\uff0c\u901a\u8fc7\u5bf9\u7ed9\u5b9a\u751f\u6210\u6837\u672c\u4e0b\u7684\u6f5c\u5728\u53d8\u91cf\u540e\u9a8c\u5206\u5e03\u8fdb\u884c\u8fd1\u4f3c\uff0c\u5e76\u636e\u6b64\u5b66\u4e60\u5bf9\u6570\u914d\u5206\u51fd\u6570\uff0c\u540e\u8005\u5b9a\u4e49\u4e86\u6307\u6570\u65cf\u7684Fisher\u5ea6\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Ising\u6a21\u578b\u548cTASEP\u6a21\u578b\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e14\u5728\u91cd\u6784\u70ed\u529b\u5b66\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u3002\u5bf9\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f8\u53d8\u7684\u5206\u5f62\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7684\u590d\u6742\u7ed3\u6784\u53ca\u5176\u4e0e\u76f8\u53d8\u7b49\u73b0\u8c61\u4e4b\u95f4\u7684\u8054\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.10703", "pdf": "https://arxiv.org/pdf/2506.10703", "abs": "https://arxiv.org/abs/2506.10703", "authors": ["Floris Holstege", "Shauli Ravfogel", "Bram Wouters"], "title": "Preserving Task-Relevant Information Under Linear Concept Removal", "categories": ["cs.LG"], "comment": null, "summary": "Modern neural networks often encode unwanted concepts alongside task-relevant\ninformation, leading to fairness and interpretability concerns. Existing\npost-hoc approaches can remove undesired concepts but often degrade useful\nsignals. We introduce SPLICE-Simultaneous Projection for LInear concept removal\nand Covariance prEservation-which eliminates sensitive concepts from\nrepresentations while exactly preserving their covariance with a target label.\nSPLICE achieves this via an oblique projection that \"splices out\" the unwanted\ndirection yet protects important label correlations. Theoretically, it is the\nunique solution that removes linear concept predictability and maintains target\ncovariance with minimal embedding distortion. Empirically, SPLICE outperforms\nbaselines on benchmarks such as Bias in Bios and Winobias, removing protected\nattributes while minimally damaging main-task information.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSPLICE\u7684\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u5728\u79fb\u9664\u8868\u793a\u4e2d\u7684\u654f\u611f\u6982\u5ff5\u7684\u540c\u65f6\u4fdd\u6301\u4e0e\u76ee\u6807\u6807\u7b7e\u7684\u534f\u65b9\u5dee\u3002\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\u90fd\u8868\u660eSPLICE\u5728\u53bb\u9664\u4e0d\u9700\u8981\u7684\u6982\u5ff5\u65f6\u80fd\u591f\u6700\u5c0f\u5316\u5bf9\u4e3b\u8981\u4efb\u52a1\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7ecf\u5e38\u5c06\u4e0d\u5e0c\u671b\u51fa\u73b0\u7684\u6982\u5ff5\u4e0e\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u4e00\u8d77\u7f16\u7801\uff0c\u5bfc\u81f4\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002\u73b0\u6709\u4e8b\u540e\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u79fb\u9664\u4e0d\u9700\u8981\u7684\u6982\u5ff5\uff0c\u4f46\u5f80\u5f80\u4e5f\u4f1a\u635f\u5bb3\u6709\u7528\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e86SPLICE\uff08\u540c\u65f6\u6295\u5f71\u4ee5\u5b9e\u73b0\u7ebf\u6027\u6982\u5ff5\u79fb\u9664\u548c\u534f\u65b9\u5dee\u4fdd\u7559\uff09\uff0c\u901a\u8fc7\u659c\u6295\u5f71\u7684\u65b9\u5f0f\u201c\u526a\u9664\u201d\u4e0d\u9700\u8981\u7684\u65b9\u5411\uff0c\u540c\u65f6\u4fdd\u62a4\u91cd\u8981\u7684\u6807\u7b7e\u76f8\u5173\u6027\u3002", "result": "SPLICE\u5728\u7406\u8bba\u4e0a\u662f\u552f\u4e00\u4e00\u4e2a\u53ef\u4ee5\u5728\u79fb\u9664\u7ebf\u6027\u6982\u5ff5\u9884\u6d4b\u80fd\u529b\u7684\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u534f\u65b9\u5dee\u5e76\u6700\u5c0f\u5316\u5d4c\u5165\u5931\u771f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5b9e\u9a8c\u4e0a\uff0c\u5728Bias in Bios\u548cWinobias\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPLICE\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u79fb\u9664\u53d7\u4fdd\u62a4\u5c5e\u6027\u7684\u540c\u65f6\u5bf9\u4e3b\u4efb\u52a1\u4fe1\u606f\u7684\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "SPLICE\u4e3a\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u4e2d\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u5728\u79fb\u9664\u654f\u611f\u4fe1\u606f\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.10751", "pdf": "https://arxiv.org/pdf/2506.10751", "abs": "https://arxiv.org/abs/2506.10751", "authors": ["Sai Prasanna Teja Reddy Bogireddy", "Abrar Majeedi", "Viswanatha Reddy Gajjala", "Zhuoyan Xu", "Siddhant Rai", "Vaishnav Potlapalli"], "title": "Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Automated question answering (QA) over electronic health records (EHRs) can\nbridge critical information gaps for clinicians and patients, yet it demands\nboth precise evidence retrieval and faithful answer generation under limited\nsupervision. In this work, we present Neural, the runner-up in the BioNLP 2025\nArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method\ndecouples the task into (1) sentence-level evidence identification and (2)\nanswer synthesis with explicit citations. For each stage, we automatically\nexplore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning\ninstructions and few-shot demonstrations on the development set. A\nself-consistency voting scheme further improves evidence recall without\nsacrificing precision. On the hidden test set, our method attains an overall\nscore of 51.5, placing second stage while outperforming standard zero-shot and\nfew-shot prompting by over 20 and 10 points, respectively. These results\nindicate that data-driven prompt optimization is a cost-effective alternative\nto model fine-tuning for high-stakes clinical QA, advancing the reliability of\nAI assistants in healthcare.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728BioNLP 2025 ArchEHR-QA\u5171\u4eab\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e9a\u519b\u7684Neural\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u63d0\u793a\u4f18\u5316\u6765\u63d0\u9ad8\u4e34\u5e8a\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e34\u5e8a\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u95ee\u7b54\u95ee\u9898\uff0c\u9700\u8981\u7cbe\u51c6\u7684\u8bc1\u636e\u68c0\u7d22\u548c\u5fe0\u5b9e\u7684\u7b54\u6848\u751f\u6210\uff0c\u5e76\u4e14\u5728\u6709\u9650\u7684\u76d1\u7763\u4e0b\u8fdb\u884c\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u53e5\u5b50\u7ea7\u8bc1\u636e\u8bc6\u522b\u548c\u5e26\u6709\u660e\u786e\u5f15\u7528\u7684\u7b54\u6848\u5408\u6210\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u4f7f\u7528DSPy\u7684MIPROv2\u4f18\u5316\u5668\u81ea\u52a8\u63a2\u7d22\u63d0\u793a\u7a7a\u95f4\uff0c\u5728\u5f00\u53d1\u96c6\u4e0a\u8054\u5408\u8c03\u6574\u6307\u4ee4\u548c\u5c11\u91cf\u6837\u672c\u793a\u4f8b\u3002\u6b64\u5916\uff0c\u91c7\u7528\u81ea\u4e00\u81f4\u6027\u6295\u7968\u65b9\u6848\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u8bc1\u636e\u53ec\u56de\u7387\u800c\u4e0d\u727a\u7272\u7cbe\u786e\u5ea6\u3002", "result": "\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e8651.5\u7684\u603b\u5206\uff0c\u4f4d\u5c45\u7b2c\u4e8c\uff0c\u76f8\u6bd4\u6807\u51c6\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u5206\u522b\u9ad8\u51fa\u8d85\u8fc720\u548c10\u4e2a\u70b9\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u9ad8\u98ce\u9669\u7684\u4e34\u5e8a\u95ee\u7b54\u6765\u8bf4\uff0c\u6570\u636e\u9a71\u52a8\u7684\u63d0\u793a\u4f18\u5316\u662f\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u6a21\u578b\u5fae\u8c03\u7684\u65b9\u5f0f\uff0c\u5b83\u80fd\u591f\u63d0\u5347\u533b\u7597\u9886\u57dfAI\u52a9\u624b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.10772", "pdf": "https://arxiv.org/pdf/2506.10772", "abs": "https://arxiv.org/abs/2506.10772", "authors": ["Ferran Alet", "Ilan Price", "Andrew El-Kadi", "Dominic Masters", "Stratis Markou", "Tom R. Andersson", "Jacklynn Stott", "Remi Lam", "Matthew Willson", "Alvaro Sanchez-Gonzalez", "Peter Battaglia"], "title": "Skillful joint probabilistic weather forecasting from marginals", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Machine learning (ML)-based weather models have rapidly risen to prominence\ndue to their greater accuracy and speed than traditional forecasts based on\nnumerical weather prediction (NWP), recently outperforming traditional\nensembles in global probabilistic weather forecasting. This paper presents FGN,\na simple, scalable and flexible modeling approach which significantly\noutperforms the current state-of-the-art models. FGN generates ensembles via\nlearned model-perturbations with an ensemble of appropriately constrained\nmodels. It is trained directly to minimize the continuous rank probability\nscore (CRPS) of per-location forecasts. It produces state-of-the-art ensemble\nforecasts as measured by a range of deterministic and probabilistic metrics,\nmakes skillful ensemble tropical cyclone track predictions, and captures joint\nspatial structure despite being trained only on marginals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGN\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u6a21\u578b\u6270\u52a8\u751f\u6210\u96c6\u6210\u9884\u6d4b\uff0c\u5e76\u76f4\u63a5\u8bad\u7ec3\u4ee5\u6700\u5c0f\u5316\u8fde\u7eed\u6392\u540d\u6982\u7387\u5f97\u5206(CRPS)\uff0c\u4ece\u800c\u5728\u5404\u79cd\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u6307\u6807\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5929\u6c14\u9884\u62a5\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5929\u6c14\u6a21\u578b\u6bd4\u4f20\u7edf\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5(NWP)\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u6700\u8fd1\u751a\u81f3\u5728\u5168\u7403\u6982\u7387\u5929\u6c14\u9884\u62a5\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u96c6\u5408\u9884\u62a5\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u65b0\u7684FGN\u65b9\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5929\u6c14\u9884\u62a5\u7684\u8d28\u91cf\u3002", "method": "FGN\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u5f97\u5230\u7684\u6a21\u578b\u6270\u52a8\u751f\u6210\u96c6\u6210\u9884\u6d4b\uff0c\u5e76\u4e14\u4f7f\u7528\u4e00\u7ec4\u9002\u5f53\u7ea6\u675f\u7684\u6a21\u578b\u3002\u5b83\u76f4\u63a5\u9488\u5bf9\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u9884\u62a5\u6765\u6700\u5c0f\u5316\u8fde\u7eed\u6392\u540d\u6982\u7387\u5f97\u5206(CRPS)\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "FGN\u5728\u4e00\u7cfb\u5217\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u5ea6\u91cf\u6807\u51c6\u4e0a\u4ea7\u751f\u4e86\u6700\u5148\u8fdb\u7684\u96c6\u6210\u9884\u62a5\u7ed3\u679c\uff0c\u80fd\u591f\u505a\u51fa\u6709\u6280\u5de7\u7684\u70ed\u5e26\u6c14\u65cb\u8def\u5f84\u9884\u6d4b\uff0c\u5e76\u6355\u6349\u5230\u8054\u5408\u7a7a\u95f4\u7ed3\u6784\uff0c\u5c3d\u7ba1\u5b83\u4ec5\u5728\u8fb9\u7f18\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002", "conclusion": "FGN\u662f\u4e00\u79cd\u7b80\u5355\u3001\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728\u5929\u6c14\u9884\u62a5\u9886\u57df\u5c55\u73b0\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.10775", "pdf": "https://arxiv.org/pdf/2506.10775", "abs": "https://arxiv.org/abs/2506.10775", "authors": ["Yufei Tao"], "title": "Monotone Classification with Relative Approximations", "categories": ["cs.LG"], "comment": null, "summary": "In monotone classification, the input is a multi-set $P$ of points in\n$\\mathbb{R}^d$, each associated with a hidden label from $\\{-1, 1\\}$. The goal\nis to identify a monotone function $h$, which acts as a classifier, mapping\nfrom $\\mathbb{R}^d$ to $\\{-1, 1\\}$ with a small {\\em error}, measured as the\nnumber of points $p \\in P$ whose labels differ from the function values $h(p)$.\nThe cost of an algorithm is defined as the number of points having their labels\nrevealed. This article presents the first study on the lowest cost required to\nfind a monotone classifier whose error is at most $(1 + \\epsilon) \\cdot k^*$\nwhere $\\epsilon \\ge 0$ and $k^*$ is the minimum error achieved by an optimal\nmonotone classifier -- in other words, the error is allowed to exceed the\noptimal by at most a relative factor. Nearly matching upper and lower bounds\nare presented for the full range of $\\epsilon$. All previous work on the\nproblem can only achieve an error higher than the optimal by an absolute\nfactor.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u5728\u5355\u8c03\u5206\u7c7b\u4e2d\u4ee5\u6700\u4f4e\u6210\u672c\u627e\u5230\u4e00\u4e2a\u8bef\u5dee\u6700\u591a\u4e3a\u6700\u4f18\u5355\u8c03\u5206\u7c7b\u5668\u8bef\u5dee\u7684$(1 + \\epsilon)$\u500d\u7684\u5355\u8c03\u5206\u7c7b\u5668\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u51e0\u4e4e\u5339\u914d\u7684\u4e0a\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\u5728\u7ed9\u5b9a\u76f8\u5bf9\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u627e\u5230\u4e00\u4e2a\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u5355\u8c03\u5206\u7c7b\u5668\u6240\u9700\u7684\u6700\u5c0f\u6807\u7b7e\u67e5\u8be2\u6210\u672c\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u9488\u5bf9\u4e0d\u540c$\\epsilon$\u503c\u8303\u56f4\u7684\u65b0\u7684\u7b97\u6cd5\u548c\u7406\u8bba\u5206\u6790\u6765\u4f30\u8ba1\u6240\u9700\u6210\u672c\u7684\u4e0a\u4e0b\u754c\u3002", "result": "\u5bf9\u4e8e$\\epsilon$\u7684\u5168\u8303\u56f4\u7ed9\u51fa\u4e86\u51e0\u4e4e\u5339\u914d\u7684\u6210\u672c\u4e0a\u4e0b\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u5141\u8bb8\u8bef\u5dee\u8d85\u8fc7\u6700\u4f18\u89e3\u7684\u76f8\u5bf9\u56e0\u5b50\u65f6\uff0c\u786e\u5b9a\u5355\u8c03\u5206\u7c7b\u5668\u6240\u9700\u6700\u4f4e\u6210\u672c\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u76f8\u6bd4\u4e4b\u524d\u7684\u5de5\u4f5c\u53ea\u80fd\u5b9e\u73b0\u7edd\u5bf9\u56e0\u5b50\u7684\u8bef\u5dee\u6539\u8fdb\u800c\u8a00\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8fdb\u5c55\u3002"}}
{"id": "2506.10801", "pdf": "https://arxiv.org/pdf/2506.10801", "abs": "https://arxiv.org/abs/2506.10801", "authors": ["Benjamin Hoover", "Zhaoyang Shi", "Krishnakumar Balasubramanian", "Dmitry Krotov", "Parikshit Ram"], "title": "Dense Associative Memory with Epanechnikov Energy", "categories": ["cs.LG"], "comment": null, "summary": "We propose a novel energy function for Dense Associative Memory (DenseAM)\nnetworks, the log-sum-ReLU (LSR), inspired by optimal kernel density\nestimation. Unlike the common log-sum-exponential (LSE) function, LSR is based\non the Epanechnikov kernel and enables exact memory retrieval with exponential\ncapacity without requiring exponential separation functions. Moreover, it\nintroduces abundant additional \\emph{emergent} local minima while preserving\nperfect pattern recovery -- a characteristic previously unseen in DenseAM\nliterature. Empirical results show that LSR energy has significantly more local\nminima (memories) that have comparable log-likelihood to LSE-based models.\nAnalysis of LSR's emergent memories on image datasets reveals a degree of\ncreativity and novelty, hinting at this method's potential for both large-scale\nmemory storage and generative tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bc6\u96c6\u5173\u8054\u8bb0\u5fc6\u7f51\u7edc\u7684\u80fd\u91cf\u51fd\u6570\u2014\u2014log-sum-ReLU\uff08LSR\uff09\uff0c\u5b83\u57fa\u4e8eEpanechnikov\u6838\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u6307\u6570\u5206\u79bb\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7cbe\u786e\u7684\u8bb0\u5fc6\u68c0\u7d22\uff0c\u5e76\u4e14\u5177\u6709\u6307\u6570\u5bb9\u91cf\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5f15\u5165\u4e86\u5927\u91cf\u989d\u5916\u7684\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b8c\u7f8e\u7684\u6a21\u5f0f\u6062\u590d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8eLSE\u7684\u6a21\u578b\u76f8\u6bd4\uff0cLSR\u80fd\u91cf\u51fd\u6570\u5177\u6709\u66f4\u591a\u7684\u5c40\u90e8\u6781\u5c0f\u503c\uff08\u8bb0\u5fc6\uff09\uff0c\u5e76\u4e14\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5206\u6790\u663e\u793a\u4e86\u8fd9\u79cd\u8bb0\u5fc6\u7684\u521b\u9020\u6027\u548c\u65b0\u9896\u6027\u3002", "motivation": "\u4f5c\u8005\u53d7\u5230\u6700\u4f18\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u542f\u53d1\uff0c\u65e8\u5728\u4e3aDenseAM\u7f51\u7edc\u8bbe\u8ba1\u4e00\u79cd\u65b0\u578b\u7684\u80fd\u91cf\u51fd\u6570\uff0c\u4ee5\u514b\u670d\u73b0\u6709LSE\u51fd\u6570\u9700\u8981\u6307\u6570\u5206\u79bb\u624d\u80fd\u8fbe\u5230\u7cbe\u786e\u8bb0\u5fc6\u68c0\u7d22\u7684\u95ee\u9898\uff0c\u5e76\u5e0c\u671b\u63d0\u5347\u8bb0\u5fc6\u5bb9\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u7684\u6a21\u5f0f\u6062\u590d\u3002", "method": "\u8bba\u6587\u4e2d\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u80fd\u91cf\u51fd\u6570\u2014\u2014log-sum-ReLU (LSR)\uff0c\u5b83\u662f\u57fa\u4e8eEpanechnikov\u6838\u7684\uff0c\u5e76\u4e14\u80fd\u591f\u4e0d\u4f9d\u8d56\u4e8e\u6307\u6570\u5206\u79bb\u51fd\u6570\u5c31\u5b9e\u73b0\u7cbe\u786e\u7684\u8bb0\u5fc6\u68c0\u7d22\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cLSR\u80fd\u91cf\u51fd\u6570\u6bd4\u57fa\u4e8eLSE\u7684\u6a21\u578b\u6709\u663e\u8457\u66f4\u591a\u7684\u5c40\u90e8\u6781\u5c0f\u503c\uff08\u8bb0\u5fc6\uff09\uff0c\u8fd9\u4e9b\u8bb0\u5fc6\u7684\u5bf9\u6570\u4f3c\u7136\u5ea6\u76f8\u5f53\u3002\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5206\u6790LSR\u4ea7\u751f\u7684\u65b0\u5174\u8bb0\u5fc6\u65f6\uff0c\u53d1\u73b0\u5b83\u4eec\u5177\u6709\u4e00\u5b9a\u7a0b\u5ea6\u7684\u521b\u65b0\u548c\u65b0\u9896\u6027\u3002", "conclusion": "LSR\u80fd\u91cf\u51fd\u6570\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6307\u6570\u7ea7\u7684\u8bb0\u5fc6\u5bb9\u91cf\uff0c\u800c\u4e14\u8fd8\u5728\u4fdd\u6301\u5b8c\u7f8e\u6a21\u5f0f\u6062\u590d\u7684\u540c\u65f6\u589e\u52a0\u4e86\u5927\u91cf\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u8fd9\u8868\u660eLSR\u5728\u5927\u89c4\u6a21\u8bb0\u5fc6\u5b58\u50a8\u548c\u751f\u6210\u4efb\u52a1\u65b9\u9762\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.10805", "pdf": "https://arxiv.org/pdf/2506.10805", "abs": "https://arxiv.org/abs/2506.10805", "authors": ["Alex McKenzie", "Urja Pawar", "Phil Blandfort", "William Bankes", "David Krueger", "Ekdeep Singh Lubana", "Dmitrii Krasheninnikov"], "title": "Detecting High-Stakes Interactions with Activation Probes", "categories": ["cs.LG"], "comment": "33 pages", "summary": "Monitoring is an important aspect of safely deploying Large Language Models\n(LLMs). This paper examines activation probes for detecting \"high-stakes\"\ninteractions -- where the text indicates that the interaction might lead to\nsignificant harm -- as a critical, yet underexplored, target for such\nmonitoring. We evaluate several probe architectures trained on synthetic data,\nand find them to exhibit robust generalization to diverse, out-of-distribution,\nreal-world data. Probes' performance is comparable to that of prompted or\nfinetuned medium-sized LLM monitors, while offering computational savings of\nsix orders-of-magnitude. Our experiments also highlight the potential of\nbuilding resource-aware hierarchical monitoring systems, where probes serve as\nan efficient initial filter and flag cases for more expensive downstream\nanalysis. We release our novel synthetic dataset and codebase to encourage\nfurther study.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u4e8e\u68c0\u6d4b\u53ef\u80fd\u9020\u6210\u91cd\u5927\u4f24\u5bb3\u7684\u9ad8\u98ce\u9669\u4ea4\u4e92\u7684\u6fc0\u6d3b\u63a2\u9488\uff0c\u5e76\u53d1\u73b0\u5b83\u4eec\u5728\u591a\u6837\u5316\u7684\u3001\u5206\u5e03\u5916\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u63a2\u9488\u7684\u6027\u80fd\u4e0e\u63d0\u793a\u6216\u5fae\u8c03\u540e\u7684\u4e2d\u578b\u8bed\u8a00\u6a21\u578b\u76d1\u63a7\u5668\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8282\u7701\u4e86\u516d\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4e3a\u4e86\u5b89\u5168\u5730\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u76d1\u6d4b\u662f\u81f3\u5173\u91cd\u8981\u7684\u65b9\u9762\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u7684\u9ad8\u98ce\u9669\u4ea4\u4e92\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u9488\u5bf9\u8fd9\u79cd\u76d1\u6d4b\u76ee\u6807\u7684\u7814\u7a76\u8fd8\u6bd4\u8f83\u5c11\u3002", "method": "\u7814\u7a76\u8005\u4eec\u8bc4\u4f30\u4e86\u51e0\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u63a2\u9488\u67b6\u6784\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5b83\u4eec\u5728\u591a\u6837\u5316\u3001\u5206\u5e03\u5916\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u63a2\u9488\u4e0d\u4ec5\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4e14\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e\u7ecf\u8fc7\u63d0\u793a\u6216\u5fae\u8c03\u7684\u4e2d\u7b49\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u76d1\u63a7\u5668\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u516d\u500d\u6570\u91cf\u7ea7\u7684\u8ba1\u7b97\u8d44\u6e90\u8282\u7701\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6fc0\u6d3b\u63a2\u9488\u4f5c\u4e3a\u9ad8\u6548\u521d\u59cb\u8fc7\u6ee4\u5668\uff0c\u5728\u6784\u5efa\u8d44\u6e90\u610f\u8bc6\u7684\u5c42\u6b21\u5316\u76d1\u63a7\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4ee5\u4e3a\u66f4\u6602\u8d35\u7684\u4e0b\u6e38\u5206\u6790\u6807\u8bb0\u51fa\u9700\u8981\u5173\u6ce8\u7684\u60c5\u51b5\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u53d1\u5e03\u4e86\u4ed6\u4eec\u7684\u65b0\u9896\u5408\u6210\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5e93\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002"}}
{"id": "2506.10842", "pdf": "https://arxiv.org/pdf/2506.10842", "abs": "https://arxiv.org/abs/2506.10842", "authors": ["Nudrat Fariha", "Md Nazmuddin Moin Khan", "Md Iqbal Hossain", "Syed Ali Reza", "Joy Chakra Bortty", "Kazi Sharmin Sultana", "Md Shadidur Islam Jawad", "Saniah Safat", "Md Abdul Ahad", "Maksuda Begum"], "title": "Advanced fraud detection using machine learning models: enhancing financial transaction security", "categories": ["cs.LG"], "comment": null, "summary": "The rise of digital payments has accelerated the need for intelligent and\nscalable systems to detect fraud. This research presents an end-to-end,\nfeature-rich machine learning framework for detecting credit card transaction\nanomalies and fraud using real-world data. The study begins by merging\ntransactional, cardholder, merchant, and merchant category datasets from a\nrelational database to create a unified analytical view. Through the feature\nengineering process, we extract behavioural signals such as average spending,\ndeviation from historical patterns, transaction timing irregularities, and\ncategory frequency metrics. These features are enriched with temporal markers\nsuch as hour, day of week, and weekend indicators to expose all latent patterns\nthat indicate fraudulent behaviours. Exploratory data analysis reveals\ncontextual transaction trends across all the dataset features. Using the\ntransactional data, we train and evaluate a range of unsupervised models:\nIsolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct\nnormal behavior. These models flag the top 1% of reconstruction errors as\noutliers. PCA visualizations illustrate each models ability to separate\nanomalies into a two-dimensional latent space. We further segment the\ntransaction landscape using K-Means clustering and DBSCAN to identify dense\nclusters of normal activity and isolate sparse, suspicious regions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u3001\u7279\u5f81\u4e30\u5bcc\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4fe1\u7528\u5361\u4ea4\u6613\u5f02\u5e38\u548c\u6b3a\u8bc8\u884c\u4e3a\u3002\u901a\u8fc7\u5bf9\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\u5904\u7406\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u65e0\u76d1\u7763\u6a21\u578b\uff08\u5982\u5b64\u7acb\u68ee\u6797\u3001\u5355\u7c7bSVM\u548c\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\uff09\u6765\u8bc6\u522b\u6f5c\u5728\u7684\u6b3a\u8bc8\u6a21\u5f0f\u3002PCA\u53ef\u89c6\u5316\u6709\u52a9\u4e8e\u533a\u5206\u6b63\u5e38\u4e0e\u5f02\u5e38\u4ea4\u6613\uff0c\u5e76\u901a\u8fc7K-Means\u805a\u7c7b\u548cDBSCAN\u8fdb\u4e00\u6b65\u5212\u5206\u4ea4\u6613\u6d3b\u52a8\u533a\u57df\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u652f\u4ed8\u7684\u5174\u8d77\uff0c\u5bf9\u4e8e\u80fd\u591f\u667a\u80fd\u4e14\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u4ee5\u5e94\u5bf9\u6b3a\u8bc8\u68c0\u6d4b\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u9996\u5148\u5c06\u6765\u81ea\u5173\u7cfb\u6570\u636e\u5e93\u7684\u4ea4\u6613\u3001\u6301\u5361\u4eba\u3001\u5546\u5bb6\u4ee5\u53ca\u5546\u5bb6\u7c7b\u522b\u6570\u636e\u96c6\u5408\u5e76\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u6790\u89c6\u56fe\u3002\u63a5\u7740\uff0c\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u63d0\u53d6\u4e86\u8bf8\u5982\u5e73\u5747\u6d88\u8d39\u3001\u504f\u79bb\u5386\u53f2\u6a21\u5f0f\u7684\u7a0b\u5ea6\u3001\u4ea4\u6613\u65f6\u95f4\u4e0d\u89c4\u5219\u6027\u4ee5\u53ca\u7c7b\u522b\u9891\u7387\u6307\u6807\u7b49\u884c\u4e3a\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u4e86\u5c0f\u65f6\u3001\u661f\u671f\u51e0\u53ca\u5468\u672b\u6307\u793a\u7b26\u7b49\u65f6\u95f4\u6807\u8bb0\u6765\u63ed\u793a\u6240\u6709\u53ef\u80fd\u6307\u793a\u6b3a\u8bc8\u884c\u4e3a\u7684\u6f5c\u5728\u6a21\u5f0f\u3002\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u65e0\u76d1\u7763\u6a21\u578b\uff0c\u5305\u62ec\u5b64\u7acb\u68ee\u6797\u3001\u5355\u7c7bSVM\u4ee5\u53ca\u7ecf\u8fc7\u8bad\u7ec3\u7528\u4ee5\u91cd\u6784\u6b63\u5e38\u884c\u4e3a\u7684\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86PCA\u53ef\u89c6\u5316\u6280\u672f\u5c55\u793a\u4e86\u6bcf\u4e2a\u6a21\u578b\u5728\u4e8c\u7ef4\u6f5c\u7a7a\u95f4\u4e2d\u5206\u79bb\u5f02\u5e38\u503c\u7684\u80fd\u529b\uff0c\u5e76\u8fd0\u7528K-Means\u805a\u7c7b\u548cDBSCAN\u7b97\u6cd5\u5bf9\u4ea4\u6613\u573a\u666f\u8fdb\u884c\u4e86\u8fdb\u4e00\u6b65\u7ec6\u5206\uff0c\u4ee5\u4fbf\u8bc6\u522b\u51fa\u5bc6\u96c6\u7684\u6b63\u5e38\u6d3b\u52a8\u7fa4\u7ec4\u548c\u7a00\u758f\u53ef\u7591\u533a\u57df\u3002", "result": "\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u63ed\u793a\u4e86\u6574\u4e2a\u6570\u636e\u96c6\u7279\u5f81\u4e2d\u7684\u4e0a\u4e0b\u6587\u4ea4\u6613\u8d8b\u52bf\u3002\u5404\u65e0\u76d1\u7763\u6a21\u578b\u88ab\u7528\u6765\u6807\u8bb0\u91cd\u5efa\u8bef\u5dee\u6392\u540d\u524d1%\u7684\u6570\u636e\u70b9\u4f5c\u4e3a\u79bb\u7fa4\u503c\u3002PCA\u53ef\u89c6\u5316\u8868\u660e\u4e86\u5404\u6a21\u578b\u533a\u5206\u5f02\u5e38\u8fdb\u5165\u4e8c\u7ef4\u6f5c\u7a7a\u95f4\u7684\u80fd\u529b\u3002K-Means\u805a\u7c7b\u548cDBSCAN\u5e2e\u52a9\u786e\u5b9a\u4e86\u6b63\u5e38\u7684\u5bc6\u96c6\u4ea4\u6613\u6d3b\u52a8\u7c07\u4ee5\u53ca\u7a00\u758f\u53ef\u7591\u533a\u57df\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5730\u4ece\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e2d\u68c0\u6d4b\u4fe1\u7528\u5361\u4ea4\u6613\u4e2d\u7684\u5f02\u5e38\u548c\u6f5c\u5728\u6b3a\u8bc8\u884c\u4e3a\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u7684\u667a\u80fd\u5316\u6c34\u5e73\uff0c\u540c\u65f6\u4e5f\u8bc1\u660e\u4e86\u901a\u8fc7\u7efc\u5408\u591a\u7ef4\u5ea6\u7279\u5f81\u5de5\u7a0b\u4e0e\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6b3a\u8bc8\u8bc6\u522b\u7cbe\u5ea6\u3002"}}
{"id": "2506.10871", "pdf": "https://arxiv.org/pdf/2506.10871", "abs": "https://arxiv.org/abs/2506.10871", "authors": ["Pierre-Fran\u00e7ois Massiani", "Alexander von Rohr", "Lukas Haverbeck", "Sebastian Trimpe"], "title": "Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization", "categories": ["cs.LG"], "comment": "24 pages, 11 figures, 2 tables. Accepted for publication at ECML-PKDD\n  2025", "summary": "Despite the many recent advances in reinforcement learning (RL), the question\nof learning policies that robustly satisfy state constraints under unknown\ndisturbances remains open. In this paper, we offer a new perspective on\nachieving robust safety by analyzing the interplay between two well-established\ntechniques in model-free RL: entropy regularization, and constraints\npenalization. We reveal empirically that entropy regularization in constrained\nRL inherently biases learning toward maximizing the number of future viable\nactions, thereby promoting constraints satisfaction robust to action noise.\nFurthermore, we show that by relaxing strict safety constraints through\npenalties, the constrained RL problem can be approximated arbitrarily closely\nby an unconstrained one and thus solved using standard model-free RL. This\nreformulation preserves both safety and optimality while empirically improving\nresilience to disturbances. Our results indicate that the connection between\nentropy regularization and robustness is a promising avenue for further\nempirical and theoretical investigation, as it enables robust safety in RL\nthrough simple reward shaping.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u672a\u77e5\u5e72\u6270\u4e0b\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u548c\u7ea6\u675f\u60e9\u7f5a\u4e24\u79cd\u6280\u672f\u7684\u76f8\u4e92\u4f5c\u7528\u6765\u5b66\u4e60\u80fd\u591f\u7a33\u5065\u6ee1\u8db3\u72b6\u6001\u7ea6\u675f\u7684\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u53d6\u5f97\u4e86\u8bb8\u591a\u6700\u65b0\u8fdb\u5c55\uff0c\u4f46\u5728\u672a\u77e5\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u80fd\u591f\u7a33\u5065\u5730\u6ee1\u8db3\u72b6\u6001\u7ea6\u675f\u7684\u7b56\u7565\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e24\u79cd\u6210\u719f\u7684\u6280\u672f\u2014\u2014\u71b5\u6b63\u5219\u5316\u548c\u7ea6\u675f\u60e9\u7f5a\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u63ed\u793a\u4e86\u71b5\u6b63\u5219\u5316\u503e\u5411\u4e8e\u6700\u5927\u5316\u672a\u6765\u53ef\u884c\u52a8\u4f5c\u7684\u6570\u91cf\uff0c\u4fc3\u8fdb\u5bf9\u52a8\u4f5c\u566a\u58f0\u7684\u7ea6\u675f\u6ee1\u8db3\uff1b\u540c\u65f6\u8868\u660e\u901a\u8fc7\u60e9\u7f5a\u653e\u677e\u4e25\u683c\u7684\u5b89\u5168\u7ea6\u675f\uff0c\u53ef\u4ee5\u5c06\u53d7\u7ea6\u675f\u7684RL\u95ee\u9898\u8fd1\u4f3c\u4e3a\u4e00\u4e2a\u4e0d\u53d7\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u4ece\u800c\u4f7f\u7528\u6807\u51c6\u7684\u65e0\u6a21\u578bRL\u65b9\u6cd5\u89e3\u51b3\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u71b5\u6b63\u5219\u5316\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u662f\u8fdb\u4e00\u6b65\u5b9e\u8bc1\u548c\u7406\u8bba\u7814\u7a76\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u5956\u52b1\u5851\u5f62\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u5b89\u5168\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u71b5\u6b63\u5219\u5316\u548c\u7ea6\u675f\u60e9\u7f5a\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u5bf9\u5e72\u6270\u7684\u97e7\u6027\uff0c\u8fd9\u4e3a\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2506.10888", "pdf": "https://arxiv.org/pdf/2506.10888", "abs": "https://arxiv.org/abs/2506.10888", "authors": ["Lucas Gnecco-Heredia", "Benjamin Negrevergne", "Yann Chevaleyre"], "title": "Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers", "categories": ["cs.LG"], "comment": "17 pages including bibliography + 13 pages of supplementary material.\n  Extended version of the article accepted at ECML 2025", "summary": "Finite mixtures of classifiers (a.k.a. randomized ensembles) have been\nproposed as a way to improve robustness against adversarial attacks. However,\nexisting attacks have been shown to not suit this kind of classifier. In this\npaper, we discuss the problem of attacking a mixture in a principled way and\nintroduce two desirable properties of attacks based on a geometrical analysis\nof the problem (effectiveness and maximality). We then show that existing\nattacks do not meet both of these properties. Finally, we introduce a new\nattack called {\\em lattice climber attack} with theoretical guarantees in the\nbinary linear setting, and demonstrate its performance by conducting\nexperiments on synthetic and real datasets.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u653b\u51fb\u5206\u7c7b\u5668\u6df7\u5408\u4f53\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u57fa\u4e8e\u51e0\u4f55\u5206\u6790\u7684\u653b\u51fb\u5c5e\u6027\uff08\u6709\u6548\u6027\u548c\u6700\u5927\u6027\uff09\u3002\u73b0\u6709\u653b\u51fb\u672a\u80fd\u6ee1\u8db3\u8fd9\u4e24\u79cd\u5c5e\u6027\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\u2014\u2014\u683c\u5b50\u6500\u722c\u653b\u51fb\uff0c\u5e76\u5728\u4e8c\u5143\u7ebf\u6027\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9488\u5bf9\u5206\u7c7b\u5668\u6df7\u5408\u4f53\u7684\u653b\u51fb\u624b\u6bb5\u5e76\u4e0d\u9002\u7528\u4e8e\u6b64\u7c7b\u5206\u7c7b\u5668\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7406\u60f3\u7684\u653b\u51fb\u7279\u6027\uff1a\u6709\u6548\u6027\u4e0e\u6700\u5927\u6027\uff0c\u5e76\u4e14\u53d1\u73b0\u73b0\u6709\u653b\u51fb\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u7279\u6027\u3002\u968f\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u683c\u5b50\u6500\u722c\u653b\u51fb\u201d\u7684\u65b0\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u4e8c\u8fdb\u5236\u7ebf\u6027\u73af\u5883\u4e0b\u6709\u7406\u8bba\u4fdd\u969c\u3002", "result": "\u683c\u5b50\u6500\u722c\u653b\u51fb\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u5bf9\u6297\u6709\u9650\u6df7\u5408\u5206\u7c7b\u5668\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u73b0\u6709\u653b\u51fb\u66f4\u52a0\u6709\u6548\u3002"}}
{"id": "2506.10911", "pdf": "https://arxiv.org/pdf/2506.10911", "abs": "https://arxiv.org/abs/2506.10911", "authors": ["Jari Kolehmainen", "Nikolay Blagoev", "John Donaghy", "O\u011fuzhan Ersoy", "Christopher Nies"], "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models", "categories": ["cs.LG"], "comment": null, "summary": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of\nmodel sizes and accelerator counts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoLoCo\u7684\u65b0\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u663e\u5f0f\u5730\u540c\u6b65\u6240\u6709\u6a21\u578b\u53c2\u6570\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u4efb\u4f55\u96c6\u4f53\u901a\u4fe1\u3002\u901a\u8fc7\u4e0e\u968f\u673a\u9009\u62e9\u7684\u5176\u4ed6\u6a21\u578b\u90e8\u5206\u5e73\u5747\u6743\u91cd\uff0cNoLoCo\u9690\u542b\u5730\u540c\u6b65\u4e86\u6a21\u578b\u6743\u91cd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6b64\u65b9\u6cd5\u76f8\u8f83\u4e8e\u5b8c\u5168\u5206\u7247\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u548c\u4f4e\u901a\u4fe1\u8bad\u7ec3\u65b9\u6cd5DiLoCo\uff0c\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3001\u52a0\u901f\u540c\u6b65\u6b65\u9aa4\u4ee5\u53ca\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u5728\u5305\u542b\u6570\u4e07\u4e2a\u52a0\u901f\u5668\u7684\u96c6\u7fa4\u4e0a\u8fdb\u884c\uff0c\u5e76\u4f9d\u8d56\u4e8e\u9ad8\u5e26\u5bbd\u4e92\u8fde\u8fdb\u884c\u901a\u4fe1\u3002\u7136\u800c\uff0c\u6269\u5927\u8fd9\u79cd\u96c6\u7fa4\u7684\u6210\u672c\u5f88\u9ad8\u4e14\u53ef\u80fd\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9650\u5236\u4e86\u53ef\u4ee5\u8bad\u7ec3\u7684\u6a21\u578b\u5927\u5c0f\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63d0\u51fa\u4e86\u51e0\u79cd\u964d\u4f4e\u901a\u4fe1\u9700\u6c42\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u7136\u4f1a\u6267\u884c\u4e00\u4e2a\u9488\u5bf9\u6240\u6709\u6a21\u578b\u526f\u672c\u7684\u540c\u6b65\u6b65\u9aa4\uff0c\u8fd9\u5728\u4f4e\u5e26\u5bbd\u7f51\u7edc\u4e0b\u53ef\u80fd\u4f1a\u53d8\u5f97\u975e\u5e38\u6602\u8d35\u3002", "method": "\u63d0\u51fa\u4e86NoLoCo\u8fd9\u4e00\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5b83\u907f\u514d\u4e86\u8bad\u7ec3\u671f\u95f4\u663e\u5f0f\u7684\u6a21\u578b\u53c2\u6570\u540c\u6b65\u8fc7\u7a0b\uff0c\u4ece\u800c\u4e0d\u9700\u8981\u4efb\u4f55\u5f62\u5f0f\u7684\u96c6\u4f53\u901a\u4fe1\u3002NoLoCo\u5229\u7528\u4e00\u79cd\u65b0\u9896\u7684Nesterov\u52a8\u91cf\u4f18\u5316\u5668\u53d8\u4f53\uff0c\u901a\u8fc7\u4e0e\u53e6\u4e00\u4e2a\u968f\u673a\u9009\u53d6\u7684\u6a21\u578b\u90e8\u5206\u5e73\u5747\u6743\u91cd\u6765\u5b9e\u73b0\u6a21\u578b\u6743\u91cd\u7684\u9690\u5f0f\u540c\u6b65\u3002", "result": "NoLoCo\u5728\u4e0d\u540c\u6570\u91cf\u7684\u52a0\u901f\u5668\u548c\u6a21\u578b\u89c4\u6a21\uff08\u4ece1.25\u4ebf\u523068\u4ebf\u53c2\u6570\uff09\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5168\u5206\u7247\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u6216\u5e7f\u6cdb\u4f7f\u7528\u7684\u4f4e\u901a\u4fe1\u8bad\u7ec3\u65b9\u6cd5DiLoCo\uff0cNoLoCo\u6240\u9700\u7684\u901a\u4fe1\u5f00\u9500\u660e\u663e\u66f4\u4f4e\u3002\u5bf9\u4e8e\u6570\u767e\u4e2a\u52a0\u901f\u5668\u901a\u8fc7\u4e92\u8054\u7f51\u8fdb\u884c\u8bad\u7ec3\u7684\u60c5\u51b5\uff0c\u5176\u540c\u6b65\u6b65\u9aa4\u6bd4DiLoCo\u4e2d\u4f7f\u7528\u7684all-reduce\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6ca1\u6709\u5168\u5c40\u963b\u585e\u901a\u4fe1\uff0c\u51cf\u5c11\u4e86\u52a0\u901f\u5668\u7684\u7a7a\u95f2\u65f6\u95f4\uff1b\u4e0eDiLoCo\u76f8\u6bd4\uff0c\u5728\u5e7f\u6cdb\u7684\u6a21\u578b\u89c4\u6a21\u548c\u52a0\u901f\u5668\u6570\u91cf\u8303\u56f4\u5185\u89c2\u5bdf\u5230\u4e86\u9ad8\u8fbe4%\u7684\u66f4\u5feb\u6536\u655b\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u521b\u65b0\u6027\u7684\u4f18\u5316\u65b9\u6cd5NoLoCo\uff0c\u5b83\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\u7684\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u6536\u655b\u6027\u80fd\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53ca\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u66f4\u5927\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.10914", "pdf": "https://arxiv.org/pdf/2506.10914", "abs": "https://arxiv.org/abs/2506.10914", "authors": ["Yuchen Ma", "Dennis Frauen", "Emil Javurek", "Stefan Feuerriegel"], "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "categories": ["cs.LG"], "comment": null, "summary": "Prior-data fitted networks (PFNs) have recently been proposed as a promising\nway to train tabular foundation models. PFNs are transformers that are\npre-trained on synthetic data generated from a prespecified prior distribution\nand that enable Bayesian inference through in-context learning. In this paper,\nwe introduce CausalFM, a comprehensive framework for training PFN-based\nfoundation models in various causal inference settings. First, we formalize the\nconstruction of Bayesian priors for causal inference based on structural causal\nmodels (SCMs) in a principled way and derive necessary criteria for the\nvalidity of such priors. Building on this, we propose a novel family of prior\ndistributions using causality-inspired Bayesian neural networks that enable\nCausalFM to perform Bayesian causal inference in various settings, including\nback-door, front-door, and instrumental variable adjustment. Finally, we\ninstantiate CausalFM and explicitly train a foundation model for estimating\nconditional average treatment effects (CATEs) using back-door adjustment. We\nshow that CausalFM performs competitively for CATE estimation using various\nsynthetic and semi-synthetic benchmarks. In sum, our framework can be used as a\ngeneral recipe to train foundation models for various causal inference\nsettings. In contrast to the current state-of-the-art in causal inference,\nCausalFM offers a novel paradigm with the potential to fundamentally change how\npractitioners perform causal inference in medicine, economics, and other\ndisciplines.", "AI": {"tldr": "\u63d0\u51fa\u4e86CausalFM\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8ePFNs\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u79cd\u56e0\u679c\u63a8\u7406\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002\u901a\u8fc7\u5f62\u5f0f\u5316\u6784\u5efa\u8d1d\u53f6\u65af\u5148\u9a8c\u548c\u63d0\u51fa\u65b0\u7684\u5148\u9a8c\u5206\u5e03\u65cf\uff0cCausalFM\u80fd\u591f\u5728\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u8d1d\u53f6\u65af\u56e0\u679c\u63a8\u7406\uff0c\u5e76\u4e14\u5728CATEs\u4f30\u8ba1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u9002\u7528\u4e8e\u5404\u79cd\u56e0\u679c\u63a8\u7406\u573a\u666f\u7684\u57fa\u7840\u6a21\u578b\uff0c\u540c\u65f6\u6539\u53d8\u4ece\u4e1a\u8005\u5728\u533b\u5b66\u3001\u7ecf\u6d4e\u5b66\u7b49\u9886\u57df\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u7684\u65b9\u5f0f\u3002", "method": "\u9996\u5148\u6b63\u5f0f\u5b9a\u4e49\u4e86\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u7684\u8d1d\u53f6\u65af\u5148\u9a8c\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u51fa\u4fdd\u8bc1\u8fd9\u4e9b\u5148\u9a8c\u6709\u6548\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u7136\u540e\u5f15\u5165\u4e86\u4e00\u7c7b\u53d7\u56e0\u679c\u542f\u53d1\u7684\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u65b0\u5148\u9a8c\u5206\u5e03\u65cf\u3002\u6700\u540e\uff0c\u5b9e\u4f8b\u5316CausalFM\u5e76\u4e13\u95e8\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7528\u4e8e\u4f30\u8ba1\u6761\u4ef6\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08CATEs\uff09\u7684\u57fa\u7840\u6a21\u578b\u3002", "result": "CausalFM\u5728\u4f7f\u7528\u5408\u6210\u6570\u636e\u548c\u534a\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u7684CATEs\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4ee5\u4f5c\u4e3a\u8bad\u7ec3\u4e0d\u540c\u56e0\u679c\u63a8\u7406\u8bbe\u7f6e\u4e0b\u57fa\u7840\u6a21\u578b\u7684\u4e00\u822c\u65b9\u6cd5\uff0c\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u56e0\u679c\u63a8\u7406\u6280\u672f\u76f8\u6bd4\uff0cCausalFM\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u6709\u6f5c\u529b\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u5b9e\u8df5\u8005\u5728\u591a\u4e2a\u5b66\u79d1\u9886\u57df\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u7684\u65b9\u5f0f\u3002"}}
{"id": "2506.10918", "pdf": "https://arxiv.org/pdf/2506.10918", "abs": "https://arxiv.org/abs/2506.10918", "authors": ["Morris Yau", "Sharut Gupta", "Valerie Engelmayer", "Kazuki Irie", "Stefanie Jegelka", "Jacob Andreas"], "title": "Sequential-Parallel Duality in Prefix Scannable Models", "categories": ["cs.LG"], "comment": null, "summary": "Modern neural sequence models are designed to meet the dual mandate of\nparallelizable training and fast sequential inference. Recent developments have\ngiven rise to various models, such as Gated Linear Attention (GLA) and Mamba,\nthat achieve such ``sequential-parallel duality.'' This raises a natural\nquestion: can we characterize the full class of neural sequence models that\nsupport near-constant-time parallel evaluation and linear-time, constant-space\nsequential inference? We begin by describing a broad class of such models --\nstate space models -- as those whose state updates can be computed using the\nclassic parallel prefix scan algorithm with a custom associative aggregation\noperator. We then define a more general class, Prefix-Scannable Models (PSMs),\nby relaxing the state aggregation operator to allow arbitrary (potentially\nnon-associative) functions such as softmax attention. This generalization\nunifies many existing architectures, including element-wise RNNs (e.g., Mamba)\nand linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new\nmodels with softmax-like operators that achieve O(1) amortized compute per\ntoken and log(N) memory for sequence length N. We empirically evaluate such\nmodels on illustrative small-scale language modeling and canonical synthetic\ntasks, including state tracking and associative recall. Empirically, we find\nthat PSMs retain the expressivity of transformer-based architectures while\nmatching the inference efficiency of state space models -- in some cases\nexhibiting better length generalization than either.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u80fd\u591f\u652f\u6301\u8fd1\u5e38\u6570\u65f6\u95f4\u5e76\u884c\u8bc4\u4f30\u548c\u7ebf\u6027\u65f6\u95f4\u3001\u5e38\u6570\u7a7a\u95f4\u987a\u5e8f\u63a8\u7406\u7684\u795e\u7ecf\u5e8f\u5217\u6a21\u578b\u7684\u5b8c\u6574\u7c7b\u522b\u3002\u901a\u8fc7\u5b9a\u4e49\u524d\u7f00\u53ef\u626b\u63cf\u6a21\u578b\uff08PSM\uff09\uff0c\u5b83\u7edf\u4e00\u4e86\u8bb8\u591a\u73b0\u6709\u67b6\u6784\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u5c0f\u578b\u8bed\u8a00\u5efa\u6a21\u548c\u5408\u6210\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5e76\u663e\u793a\u51fa\u4e0e\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u67b6\u6784\u76f8\u5f53\u7684\u8868\u73b0\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e00\u7c7b\u65b0\u578b\u7684\u795e\u7ecf\u5e8f\u5217\u6a21\u578b\uff0c\u8fd9\u7c7b\u6a21\u578b\u53ef\u4ee5\u540c\u65f6\u6ee1\u8db3\u5e76\u884c\u5316\u8bad\u7ec3\u548c\u5feb\u901f\u987a\u5e8f\u63a8\u7406\u7684\u9700\u6c42\u3002\u7279\u522b\u662f\uff0c\u4f5c\u8005\u60f3\u8981\u523b\u753b\u51fa\u6240\u6709\u80fd\u591f\u652f\u6301\u8fd1\u5e38\u6570\u65f6\u95f4\u5e76\u884c\u8bc4\u4f30\u4ee5\u53ca\u7ebf\u6027\u65f6\u95f4\u3001\u5e38\u6570\u7a7a\u95f4\u987a\u5e8f\u63a8\u7406\u7684\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u9996\u5148\u63cf\u8ff0\u4e86\u4e00\u7c7b\u5e7f\u6cdb\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5176\u72b6\u6001\u66f4\u65b0\u53ef\u4ee5\u4f7f\u7528\u7ecf\u5178\u5e76\u884c\u524d\u7f00\u626b\u63cf\u7b97\u6cd5\u6765\u8ba1\u7b97\u3002\u63a5\u7740\u5b9a\u4e49\u4e86\u4e00\u4e2a\u66f4\u4e00\u822c\u7684\u7c7b\u522b\u2014\u2014\u524d\u7f00\u53ef\u626b\u63cf\u6a21\u578b\uff08PSMs\uff09\uff0c\u5b83\u653e\u5bbd\u4e86\u72b6\u6001\u805a\u5408\u64cd\u4f5c\u7b26\u7684\u8981\u6c42\uff0c\u5141\u8bb8\u4efb\u610f\u53ef\u80fd\u975e\u7ed3\u5408\u6027\u7684\u51fd\u6570\uff0c\u5982softmax\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPSMs\u4fdd\u7559\u4e86\u57fa\u4e8etransformer\u67b6\u6784\u7684\u8868\u73b0\u529b\uff0c\u540c\u65f6\u4e5f\u5177\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u957f\u5ea6\u6cdb\u5316\u751a\u81f3\u4f18\u4e8e\u4e24\u8005\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u524d\u7f00\u53ef\u626b\u63cf\u6a21\u578b\uff08PSMs\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u7edf\u4e00\u4e86\u73b0\u6709\u591a\u79cd\u67b6\u6784\uff0c\u8fd8\u5f15\u5165\u4e86\u65b0\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8868\u73b0\u529b\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2506.10930", "pdf": "https://arxiv.org/pdf/2506.10930", "abs": "https://arxiv.org/abs/2506.10930", "authors": ["Thanathai Lertpetchpun", "Tiantian Feng", "Dani Byrd", "Shrikanth Narayanan"], "title": "Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Speech emotion recognition (SER) in naturalistic conditions presents a\nsignificant challenge for the speech processing community. Challenges include\ndisagreement in labeling among annotators and imbalanced data distributions.\nThis paper presents a reproducible framework that achieves superior (top 1)\nperformance in the Emotion Recognition in Naturalistic Conditions Challenge\n(IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system\nis designed to tackle the aforementioned challenges through multimodal\nlearning, multi-task learning, and imbalanced data handling. Specifically, our\nbest system is trained by adding text embeddings, predicting gender, and\nincluding ``Other'' (O) and ``No Agreement'' (X) samples in the training set.\nOur system's results secured both first and second places in the IS25-SER\nChallenge, and the top performance was achieved by a simple two-system\nensemble.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u6765\u89e3\u51b3\u81ea\u7136\u6761\u4ef6\u4e0b\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5728IS25-SER\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u8868\u73b0\u3002", "motivation": "\u5728\u81ea\u7136\u6761\u4ef6\u4e0b\u8fdb\u884c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u5b58\u5728\u8bf8\u591a\u6311\u6218\uff0c\u5305\u62ec\u6807\u6ce8\u8005\u4e4b\u95f4\u7684\u6807\u6ce8\u5206\u6b67\u4ee5\u53ca\u6570\u636e\u5206\u5e03\u4e0d\u5747\u8861\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u5b83\u91c7\u7528\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u3001\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6700\u597d\u7684\u7cfb\u7edf\u662f\u901a\u8fc7\u6dfb\u52a0\u6587\u672c\u5d4c\u5165\u3001\u9884\u6d4b\u6027\u522b\u4ee5\u53ca\u5c06'\u5176\u4ed6'\uff08O\uff09\u548c'\u65e0\u5171\u8bc6'\uff08X\uff09\u6837\u672c\u7eb3\u5165\u8bad\u7ec3\u96c6\u6765\u8bad\u7ec3\u7684\u3002", "result": "\u8be5\u7cfb\u7edf\u5728IS25-SER\u6311\u6218\u8d5b\u7684\u4efb\u52a12\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e00\u7684\u6210\u7ee9\uff0c\u5e76\u4e14\u901a\u8fc7\u7b80\u5355\u7684\u53cc\u7cfb\u7edf\u96c6\u6210\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u5728MSP-Podcast\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u81ea\u7136\u6761\u4ef6\u4e0b\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u5728\u7ade\u8d5b\u4e2d\u8d62\u5f97\u4e86\u9996\u5956\u3002"}}
{"id": "2506.10943", "pdf": "https://arxiv.org/pdf/2506.10943", "abs": "https://arxiv.org/abs/2506.10943", "authors": ["Adam Zweiger", "Jyothish Pari", "Han Guo", "Ekin Aky\u00fcrek", "Yoon Kim", "Pulkit Agrawal"], "title": "Self-Adapting Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Self-Adapting LLMs (SEAL)\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u751f\u6210\u81ea\u5df1\u7684\u5fae\u8c03\u6570\u636e\u548c\u66f4\u65b0\u6307\u4ee4\u6765\u81ea\u6211\u9002\u5e94\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u8bad\u7ec3\u6a21\u578b\u4ee5\u4ea7\u751f\u6709\u6548\u7684\u81ea\u6211\u7f16\u8f91\uff0c\u5e76\u4ee5\u66f4\u65b0\u540e\u7684\u6a21\u578b\u6027\u80fd\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002\u5b9e\u9a8c\u8868\u660eSEAL\u662f\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u5bfc\u5411\u9002\u5e94\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5f3a\u5927\u4f46\u9759\u6001\uff0c\u7f3a\u4e4f\u9488\u5bf9\u65b0\u4efb\u52a1\u3001\u77e5\u8bc6\u6216\u793a\u4f8b\u8c03\u6574\u6743\u91cd\u7684\u673a\u5236\u3002", "method": "SEAL\u6846\u67b6\u5141\u8bb8LLMs\u901a\u8fc7\u751f\u6210\u5b83\u4eec\u81ea\u5df1\u7684\u5fae\u8c03\u6570\u636e\u548c\u66f4\u65b0\u6307\u4ee4\u6765\u9002\u5e94\u3002\u7ed9\u5b9a\u65b0\u7684\u8f93\u5165\u65f6\uff0c\u6a21\u578b\u4f1a\u4ea7\u751f\u4e00\u4e2a\u81ea\u6211\u7f16\u8f91\uff0c\u8fd9\u53ef\u80fd\u5305\u62ec\u4fe1\u606f\u91cd\u6784\u3001\u6307\u5b9a\u4f18\u5316\u8d85\u53c2\u6570\u6216\u8c03\u7528\u5de5\u5177\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u66f4\u65b0\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u8fd9\u4e9b\u81ea\u6211\u7f16\u8f91\u4f1a\u5bfc\u81f4\u6301\u4e45\u7684\u6743\u91cd\u66f4\u65b0\uff0c\u4ece\u800c\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u3002\u4e3a\u4e86\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u6709\u6548\u7684\u81ea\u6211\u7f16\u8f91\uff0c\u91c7\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\uff0c\u4ee5\u4e0b\u6e38\u66f4\u65b0\u540e\u6a21\u578b\u7684\u8868\u73b0\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSEAL\u5728\u77e5\u8bc6\u6574\u5408\u548c\u5c11\u91cf\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u671d\u7740\u80fd\u591f\u81ea\u6211\u5bfc\u5411\u9002\u5e94\u7684\u8bed\u8a00\u6a21\u578b\u8fc8\u51fa\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002", "conclusion": "SEAL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u6211\u9002\u5e94\uff0c\u4e0d\u9700\u8981\u989d\u5916\u7684\u9002\u5e94\u6a21\u5757\u6216\u8f85\u52a9\u7f51\u7edc\uff0c\u800c\u662f\u76f4\u63a5\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u6765\u63a7\u5236\u5176\u9002\u5e94\u8fc7\u7a0b\u3002"}}
{"id": "2506.10948", "pdf": "https://arxiv.org/pdf/2506.10948", "abs": "https://arxiv.org/abs/2506.10948", "authors": ["Boaz Lavon", "Shahar Katz", "Lior Wolf"], "title": "Execution Guided Line-by-Line Code Generation", "categories": ["cs.LG"], "comment": null, "summary": "We present a novel approach to neural code generation that incorporates\nreal-time execution signals into the language model generation process. While\nlarge language models (LLMs) have demonstrated impressive code generation\ncapabilities, they typically do not utilize execution feedback during\ninference, a critical signal that human programmers regularly leverage. Our\nmethod, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically\nincorporates execution signals as the model generates code, providing\nline-by-line feedback that guides the generation process toward executable\nsolutions. EG-CFG employs a multi-stage process: first, we conduct beam search\nto sample candidate program completions for each line; second, we extract\nexecution signals by executing these candidates against test cases; and\nfinally, we incorporate these signals into the prompt during generation. By\nmaintaining consistent signals across tokens within the same line and\nrefreshing signals at line boundaries, our approach provides coherent guidance\nwhile preserving syntactic structure. Moreover, the method naturally supports\nnative parallelism at the task level in which multiple agents operate in\nparallel, exploring diverse reasoning paths and collectively generating a broad\nset of candidate solutions. Our experiments across diverse coding tasks\ndemonstrate that EG-CFG significantly improves code generation performance\ncompared to standard approaches, achieving state-of-the-art results across\nvarious levels of complexity, from foundational problems to challenging\ncompetitive programming tasks. Our code is available at:\nhttps://github.com/boazlavon/eg_cfg", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6267\u884c\u5f15\u5bfc\u7684\u65e0\u5206\u7c7b\u5668\u6307\u5bfc(EG-CFG)\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u4ee3\u7801\u65f6\u52a8\u6001\u5730\u7ed3\u5408\u4e86\u6267\u884c\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u5728\u5404\u79cd\u590d\u6742\u5ea6\u7684\u7f16\u7801\u4efb\u52a1\u4e2d\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u4ee3\u7801\u751f\u6210\u529f\u80fd\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4e0d\u4f1a\u5728\u63a8\u65ad\u8fc7\u7a0b\u4e2d\u5229\u7528\u6267\u884c\u53cd\u9988\uff0c\u800c\u8fd9\u662f\u4eba\u7c7b\u7a0b\u5e8f\u5458\u7ecf\u5e38\u4f7f\u7528\u7684\u5173\u952e\u4fe1\u53f7\u3002", "method": "Execution-Guided Classifier-Free Guidance (EG-CFG) \u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1. \u4f7f\u7528\u675f\u641c\u7d22\u4e3a\u6bcf\u884c\u4ee3\u7801\u91c7\u6837\u5019\u9009\u5b8c\u6210\uff1b2. \u901a\u8fc7\u5bf9\u6d4b\u8bd5\u7528\u4f8b\u6267\u884c\u8fd9\u4e9b\u5019\u9009\u6765\u63d0\u53d6\u6267\u884c\u4fe1\u53f7\uff1b3. \u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5c06\u8fd9\u4e9b\u4fe1\u53f7\u5408\u5e76\u5230\u63d0\u793a\u4e2d\u3002", "result": "EG-CFG \u76f8\u6bd4\u6807\u51c6\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u8868\u73b0\uff0c\u5728\u57fa\u7840\u95ee\u9898\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u7ade\u4e89\u7f16\u7a0b\u4efb\u52a1\u7b49\u5404\u79cd\u590d\u6742\u5ea6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6210\u679c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5b9e\u65f6\u6267\u884c\u4fe1\u53f7\uff0cEG-CFG \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u4ee3\u7801\u751f\u6210\u65b9\u5f0f\uff0c\u80fd\u591f\u63d0\u9ad8\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u5e76\u4fdd\u6301\u8bed\u6cd5\u7ed3\u6784\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.10953", "pdf": "https://arxiv.org/pdf/2506.10953", "abs": "https://arxiv.org/abs/2506.10953", "authors": ["Xing Han L\u00f9", "Gaurav Kamath", "Marius Mosbach", "Siva Reddy"], "title": "Build the web for agents, not agents for the web", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\uff0c\u5373Agentic Web Interface (AWI)\uff0c\u65e8\u5728\u4e3a\u4ee3\u7406\u8bbe\u8ba1\u4e13\u95e8\u7684\u7f51\u7ad9\u5bfc\u822a\u754c\u9762\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7f51\u9875\u8f93\u5165\u65f6\u9047\u5230\u7684\u95ee\u9898\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u516d\u4e2a\u6307\u5bfc\u539f\u5219\u6765\u8bbe\u8ba1AWI\uff0c\u5f3a\u8c03\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u6807\u51c6\u5316\uff0c\u4ee5\u4fbf\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u4e14\u900f\u660e\u5730\u8bbe\u8ba1web agent\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u6a21\u6001\u6280\u672f\u5f00\u53d1\u7684web agents\u5728\u81ea\u4e3b\u6d4f\u89c8\u548c\u5b8c\u6210\u7f51\u7edc\u4efb\u52a1\u65b9\u9762\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u56e0\u4e3a\u4eba\u673a\u754c\u9762\u4e0eLLM\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u5021\u5bfc\u5728\u7f51\u7edc\u4ee3\u7406\u7814\u7a76\u4e2d\u8fdb\u884c\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e0d\u518d\u8ba9\u7f51\u7edc\u4ee3\u7406\u53bb\u9002\u5e94\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\u7684\u754c\u9762\uff0c\u800c\u662f\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\u2014\u2014Agentic Web Interface\uff08AWI\uff09\uff0c\u4e13\u95e8\u4e3a\u4ee3\u7406\u4f18\u5316\u3002\u6587\u4e2d\u63d0\u51fa\u4e86\u516d\u4e2a\u5173\u4e8eAWI\u8bbe\u8ba1\u7684\u6307\u5bfc\u539f\u5219\uff0c\u8fd9\u4e9b\u539f\u5219\u5173\u6ce8\u4e8e\u5b89\u5168\u3001\u6548\u7387\u548c\u6807\u51c6\u5316\u3002", "result": "\u901a\u8fc7\u5f15\u5165Agentic Web Interface\u7684\u6982\u5ff5\u5e76\u5236\u5b9a\u516d\u9879\u8bbe\u8ba1\u539f\u5219\uff0c\u7814\u7a76\u4eba\u5458\u671f\u671b\u80fd\u591f\u89e3\u51b3\u73b0\u6709\u754c\u9762\u7684\u6839\u672c\u9650\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u548c\u900f\u660e\u7684web agent\u8bbe\u8ba1\u3002", "conclusion": "\u4e3a\u4e86\u4f7fweb agents\u80fd\u591f\u66f4\u597d\u5730\u6267\u884c\u590d\u6742\u7684\u7f51\u7edc\u4e92\u52a8\uff0c\u9700\u8981\u8f6c\u5411\u4e00\u79cd\u4e13\u4e3a\u8fd9\u4e9b\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u65b0\u4ea4\u4e92\u6a21\u5f0f\u3002Agentic Web Interface\u53ca\u5176\u516d\u9879\u8bbe\u8ba1\u539f\u5219\u4e3a\u672a\u6765\u7684web agent\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u5411\uff0c\u8fd9\u5c06\u662f\u4e00\u4e2a\u9700\u8981\u673a\u5668\u5b66\u4e60\u793e\u533a\u5171\u540c\u52aa\u529b\u7684\u5de5\u4f5c\u3002"}}
