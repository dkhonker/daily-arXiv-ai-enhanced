<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 71]
- [cs.AI](#cs.AI) [总数: 41]
- [cs.CR](#cs.CR) [总数: 23]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

**主要类别:** cs.LG

**AI概要:** 本文分析了图神经网络中随网络深度增加而加剧的过平滑问题，并通过与物理学术语Anderson局域化的类比提出了一种新的理解和解决该问题的方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着网络深度的增加，过平滑问题变得更为严重，导致节点表示失去其独特性。因此，需要找到一种方法来解决这个问题。

**方法:** 通过与Anderson局域化的类比分析了过平滑现象，并引入了参与度作为量化这一现象的指标。

**结果:** 研究发现，随着GNN深度的增加，节点特征在多层消息传递后会发生同质化，导致独特性的丧失，这与无序系统中振动模式的行为相似。在此背景下，可以将GNN中的过平滑理解为低频模式（增加的参与度）的扩展和高频模式（降低的参与度）的局部化。

**结论:** 提出了通过减少信息传播中的无序性来缓解过平滑问题的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Over-Smoothing+in+Graph+Neural+Networks%3A+A+Perspective+from+Anderson+Localization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05263，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05263&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [2] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal, Niyaz Bin Hashem, Robin Krambroeckers, Nabeel Mohammed, Shafin Rahman*

**主要类别:** cs.LG

**AI概要:** A method for improving time series forecasting by whitening exogenous input to reduce redundancy and increase awareness of long-term patterns and trends.


<details>
  <summary>更多</summary>
  
**动机:** Most transformer-based time series forecasting models primarily depend on endogenous inputs, recent state-of-the-art approaches have significantly improved performance by incorporating external information through exogenous inputs. However, these methods face challenges such as redundancy when endogenous and exogenous inputs originate from the same source and limited ability to capture long-term dependencies due to fixed look-back windows.

**方法:** The proposed method whitens the exogenous input to reduce redundancy that may persist within the data based on global statistics, and helps the exogenous input to be more aware of patterns and trends over extended periods.

**结果:** The proposed approach achieves state-of-the-art performance in four benchmark datasets, consistently outperforming 11 baseline models.

**结论:** The proposed method is a robust and effective alternative for using exogenous inputs in time series forecasting.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal+Window+Smoothing+of+Exogenous+Variables+for+Improved+Time+Series+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05284，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05284&send_immediately=true&force_search=false)

**原文摘要:** Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [3] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush, Mohsen Raji, Behnam Ghavami*

**主要类别:** cs.LG

**AI概要:** 提出了一种使用XAI的新型DNN压缩方法，能够有效地减小DNN模型尺寸，同时几乎不损失准确性。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络（DNNs）在许多任务中表现出色，但其高计算成本和内存使用量成为了问题。压缩技术和可解释人工智能（XAI）方法被引入以解决这些问题，并理解AI方法的内部工作原理。

**方法:** 使用一种称为逐层相关性传播（LRP）的梯度XAI技术计算DNN参数（即权重）的重要性得分，然后根据这些得分进行剪枝和混合精度量化。

**结果:** 实验结果表明，所提出的压缩方法将模型大小减少了64%，而准确度相比基于XAI的最先进压缩方法提高了42%。

**结论:** 提出的压缩方法在减少模型大小的同时，相较于基于XAI的最先进压缩方法，准确度提高了42%。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compressing+Deep+Neural+Networks+Using+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05286，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05286&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [4] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban, Yves Chemisky, Étienne Prulière, Michaël Clément*

**主要类别:** cs.LG

**AI概要:** This paper proposes a physics-informed machine learning framework called P-DivGNN for reconstructing local stress fields at the micro-scale.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to provide an efficient method for retrieving local stress field distributions which are important for fracture analysis or the definition of local fatigue criteria.

**方法:** P-DivGNN uses a graph neural network to represent a periodic micro-structure and incorporates physical constraints during training.

**结果:** P-DivGNN can retrieve local stress field distributions and achieves significant computational speed-ups compared to FE simulation.

**结论:** The proposed P-DivGNN is effective and efficient for reconstructing local stress fields, particularly in large-scale applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Informed+Graph+Neural+Networks+to+Reconstruct+Local+Fields+Considering+Finite+Strain+Hyperelasticity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05291&send_immediately=true&force_search=false)

**原文摘要:** We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [5] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso, Andrea Bragagnolo, Enzo Tartaglione, Attilio Fiandrotti, Marco Grangetto*

**主要类别:** cs.LG

**AI概要:** This paper introduces NeVe, a dynamic training approach that uses 'neural velocity' to adjust learning rate and define stop criterion, reducing the need for a held-out dataset.


<details>
  <summary>更多</summary>
  
**动机:** Hyperparameter tuning often relies on monitoring the validation loss. The motivation is to find an alternative method that does not require a held-out dataset.

**方法:** The paper presents NeVe, a dynamic training approach that adjusts the learning rate and defines the stop criterion based on the novel notion of 'neural velocity'.

**结果:** Findings show the potential of neural velocity as a key metric for optimizing neural network training efficiently.

**结论:** Neural velocity can be used as a key metric for optimizing neural network training.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Velocity+for+hyperparameter+tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05309&send_immediately=true&force_search=false)

**原文摘要:** Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [6] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova, Florentin Bieder, Robin Sandkühler, Philippe C. Cattin*

**主要类别:** cs.LG

**AI概要:** A conditional graph neural network is used to simulate soft tissue deformation and haptic feedback in virtual environments, achieving high accuracy and overcoming data scarcity through transfer learning.


<details>
  <summary>更多</summary>
  
**动机:** Soft tissue simulation in virtual environments is crucial for medical applications but poses challenges due to the high deformability of soft tissues. Existing methods are complex as they require segmentation, meshing, and stiffness estimation of tissues.

**方法:** A novel data-driven model, specifically a conditional graph neural network (cGNN), is introduced. The model is trained on experimentally collected surface tracking data of a soft tissue phantom and uses transfer learning to overcome data scarcity.

**结果:** The model can predict deformations with a distance error of 0.35±0.03 mm for deformations up to 30 mm and forces with an absolute error of 0.37±0.05 N for forces up to 7.5 N.

**结论:** The data-driven approach using a conditional graph neural network presents a promising solution for simulating soft tissues in virtual environments, with potential applications beyond medical simulations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conditional+Graph+Neural+Network+for+Predicting+Soft+Tissue+Deformation+and+Forces，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05315，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05315&send_immediately=true&force_search=false)

**原文摘要:** Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [7] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

**主要类别:** cs.LG

**AI概要:** This paper presents the first dataless neural network approach for the Resource-Constrained Project Scheduling Problem (RCPSP), providing a mathematical framework that transforms discrete scheduling constraints into differentiable objectives.


<details>
  <summary>更多</summary>
  
**动机:** Despite previous work in dataless approaches for combinatorial optimization problems like the Maximum Independent Set problem, no work has extended these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).

**方法:** The approach transforms discrete scheduling constraints into differentiable objectives suitable for gradient-based optimization using smooth relaxations and automatic differentiation.

**结果:** Implementation and comprehensive experiments on PSPLIB benchmark instances are currently underway, with empirical results to be reported in an updated version of this paper.

**结论:** This paper is the first to present a dataless neural network approach for RCPSP, opening up new possibilities for solving combinatorial optimization problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dataless+Neural+Networks+for+Resource-Constrained+Project+Scheduling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05322，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05322&send_immediately=true&force_search=false)

**原文摘要:** Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [8] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee, Zhang-Wei Hong, Pulkit Agrawal*

**主要类别:** cs.LG

**AI概要:** This paper proposes a new paradigm called HEPO for effectively leveraging heuristics in reinforcement learning, reducing human effort in reward design.


<details>
  <summary>更多</summary>
  
**动机:** Augmenting task rewards with heuristic rewards that encode human priors is crucial for achieving desirable performance in reinforcement learning applications. However, balancing tasks and heuristic rewards requires much human effort and computational resources.

**方法:** The paper proposes a new paradigm called Heuristic Enhanced Policy Optimization (HEPO) for mitigating reward hacking and effectively using heuristics.

**结果:** HEPO achieves superior performance on standard benchmarks with well-engineered reward functions. It also allows policy optimization to achieve good performance even when heuristics are not well-engineered.

**结论:** HEPO offers a new way to effectively leverage heuristics in reinforcement learning, reducing human effort in reward design.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Going+Beyond+Heuristics+by+Imposing+Policy+Improvement+as+a+Constraint，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05328，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05328&send_immediately=true&force_search=false)

**原文摘要:** In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [9] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert, Daniel Muthukrishna, Paul F. Gregory, David W. Hogg, V. Ashley Villar*

**主要类别:** cs.LG

**AI概要:** This paper presents a causally-motivated foundation model that disentangles physical and instrumental factors in time series data. Using a dual-encoder architecture and structured contrastive learning, the model achieves superior performance on prediction tasks, especially in low-data regimes.


<details>
  <summary>更多</summary>
  
**动机:** Observations often conflate the true underlying physical phenomena with systematic distortions introduced by measurement instruments, which limits model generalization in heterogeneous or multi-instrument settings.

**方法:** A causally-motivated foundation model that explicitly disentangles physical and instrumental factors using a dual-encoder architecture trained with structured contrastive learning.

**结果:** The method significantly outperforms traditional single-latent space foundation models on downstream prediction tasks, particularly in low-data regimes.

**结论:** The model supports key capabilities of foundation models and highlights the importance of encoding causal structure into representation learning for structured data.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Foundation+Models%3A+Disentangling+Physics+from+Instrument+Properties，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05333&send_immediately=true&force_search=false)

**原文摘要:** Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [10] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Min Xie, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu*

**主要类别:** cs.LG

**AI概要:** 本文比较分析了监督微调（SFT）和强化微调（RFT），发现在连续学习下游任务时，SFT会导致灾难性遗忘，而RFT能很好地保持先前知识并保护和增强模型的一般知识。进一步分析表明RFT中的隐式正则化是减轻遗忘的关键因素。最后提出了一种基于rollout的实例过滤算法来提高RFT的稳定性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 研究学习范式在连续后训练（CPT）中的基本作用，特别是其对知识保留的影响。

**方法:** 对监督微调（SFT）和强化微调（RFT）两种核心后训练范式进行了比较分析。

**结果:** 实验发现：1) SFT导致先前学习任务的灾难性遗忘，而RFT本质上保持了先前的知识，并且性能可与多任务训练相媲美；2) RFT成功保护甚至增强了模型的一般知识，而SFT则严重削弱了一般模型能力；3) RFT中隐含的正则化是减轻遗忘的关键因素。

**结论:** 强化微调（RFT）作为持续后训练的强有力范式展现出优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Fine-Tuning+Naturally+Mitigates+Forgetting+in+Continual+Post-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05386，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05386&send_immediately=true&force_search=false)

**原文摘要:** Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [11] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari, Ferdinando Cicalese, Alessandro Farinelli*

**主要类别:** cs.LG

**AI概要:** PT-LiRPA is a novel framework that significantly tightens the lower and upper linear bounds of a neural network's output, reducing the computational cost of formal verification tools while providing probabilistic guarantees.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to reduce the computational cost of formal verification tools while providing probabilistic guarantees on verification soundness.

**方法:** PT-LiRPA combines over-approximation techniques from LiRPA-based approaches with a sampling-based method to compute tight intermediate reachable sets.

**结果:** Experiments show that PT-LiRPA-based verifier improves robustness certificates by up to 3.31X and 2.26X compared to related work.

**结论:** The probabilistic approach of PT-LiRPA provides a valuable solution for challenging competition entries where state-of-the-art formal verification methods fail.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probabilistically+Tightened+Linear+Relaxation-based+Perturbation+Analysis+for+Neural+Network+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05405，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05405&send_immediately=true&force_search=false)

**原文摘要:** We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [12] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee, Tom Gunter, Chang Lan, John Peebles, Hanzhi Zhou, Kelvin Zou, Sneha Bangalore, Chung-Cheng Chiu, Nan Du, Xianzhi Du, Philipp Dufter, Ruixuan Hou, Haoshuo Huang, Dongseong Hwang, Xiang Kong, Jinhao Lei, Tao Lei, Meng Li, Li Li, Jiarui Lu, Zhiyun Lu, Yiping Ma, David Qiu, Vivek Rathod, Senyu Tong, Zhucheng Tu, Jianyu Wang, Yongqiang Wang, Zirui Wang, Floris Weers, Sam Wiseman, Guoli Yin, Bowen Zhang, Xiyou Zhou, Danyang Zhuo, Cheng Leong, Ruoming Pang*

**主要类别:** cs.LG

**AI概要:** This paper presents AXLearn, a deep learning system focused on modularity and support for heterogeneous hardware infrastructure.


<details>
  <summary>更多</summary>
  
**动机:** facilitates scalable and high-performance training of large deep learning models

**方法:** design and implement AXLearn, a production deep learning system

**结果:** introduce a novel method of quantifying modularity via Lines-of-Code (LoC)-complexity

**结论:** AXLearn maintains equivalent performance compared to state-of-the-art training systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AXLearn%3A+Modular+Large+Model+Training+on+Heterogeneous+Infrastructure，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05411&send_immediately=true&force_search=false)

**原文摘要:** We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [13] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar, Vishnu Naresh Boddeti*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的训练算法RepLIn，通过显式地强制执行干预过程中的统计独立性，从而降低干预数据的误差，提高模型在干预数据上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法未能充分利用由干预引起的因果关系信息，导致在观察数据和干预数据上预测性能存在较大差异。当干预训练样本数量有限时，这种差异更为明显。因此，需要一种新方法来解决这个问题。

**方法:** 研究提出了RepLIn算法，该算法通过在干预过程中显式地强制执行统计独立性，以降低干预数据的误差。

**结果:** RepLIn算法在合成数据集、真实图像和文本数据集上的实验表明，该算法可扩展到因果图中的节点数，并能改善对连续和离散潜在变量的干预分布变化的鲁棒表示。

**结论:** RepLIn算法能够有效地提升模型在干预数据上的表现，并且对于连续和离散潜在变量的干预分布变化都具有良好的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Incorporating+Interventional+Independence+Improves+Robustness+against+Interventional+Distribution+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05412&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [14] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram, Tanvir Bhathal*

**主要类别:** cs.LG

**AI概要:** This paper explores forecasting N$_2$O agricultural emissions through evaluating popular architectures and proposing two novel deep learning architectures, EmissionNet (ENV) and EmissionNet-Transformer (ENT).


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this work is to address the challenge of forecasting air pollution from agricultural emissions, which is significant yet often overlooked, and traditional physics-based models struggle to capture complex, nonlinear pollutant interactions.

**方法:** The paper evaluates popular architectures and proposes two novel deep learning architectures, EmissionNet (ENV) and EmissionNet-Transformer (ENT), which leverage convolutional and transformer-based architectures.

**结果:** The result of this work is the development of two novel deep learning architectures, EmissionNet (ENV) and EmissionNet-Transformer (ENT), which are capable of extracting spatial-temporal dependencies from high-resolution emissions data.

**结论:** The study concludes that the proposed deep learning models, EmissionNet (ENV) and EmissionNet-Transformer (ENT), offer a promising approach to forecast N$_2$O agricultural emissions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EmissionNet%3A+Air+Quality+Pollution+Forecasting+for+Agriculture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05416，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05416&send_immediately=true&force_search=false)

**原文摘要:** Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [15] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff, Karen Kukla, Michel Benaroch, Joseph Comprix*

**主要类别:** cs.LG

**AI概要:** This paper explores how firms can use MVMO attacks to manipulate financial reports, revealing significant risks in current reporting practices.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to expose how distressed firms may manipulate financial reports for personal gains, and to develop methods to detect such manipulations.

**方法:** The study introduces MVMO attacks as a new method to identify how firms can manipulate their financial data.

**结果:** The result shows that in about 50% of cases, companies could inflate earnings by 100-200% and reduce fraud scores by 15% using MVMO attacks.

**结论:** The study concludes that by using MVMO attacks, companies could significantly manipulate their financial reports, highlighting the need for more robust detection measures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Machine+Learning+Attacks+on+Financial+Reporting+via+Maximum+Violated+Multi-Objective+Attack，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05441&send_immediately=true&force_search=false)

**原文摘要:** Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [16] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram, Tanvir Bhathal, Robby Manihani*

**主要类别:** cs.LG

**AI概要:** This paper introduces a unified, distributional multi-step RL framework designed to directly optimize long-horizon performance.


<details>
  <summary>更多</summary>
  
**动机:** Delayed and sparse rewards present a fundamental obstacle for reinforcement-learning (RL) agents. The sliding-tile game 2048 epitomizes this challenge.

**方法:** Develop and compare four agent variants: standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN (H-DQN) that integrates distributional learning, dueling architectures, noisy networks, prioritized replay, and more.

**结果:** Max episode scores improve from 3.988K (DQN) to 5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048 tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile.

**结论:** Distributional, multi-step targets substantially enhance performance in sparse-reward domains.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是2048%3A+Reinforcement+Learning+in+a+Delayed+Reward+Environment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05465，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05465&send_immediately=true&force_search=false)

**原文摘要:** Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [17] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí, Marin Vlastelica, Georg Martius, Stelian Coros*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的探索策略，可以有效改善零射击强化学习中的样本复杂度问题。


<details>
  <summary>更多</summary>
  
**动机:** 零射击强化学习在缺乏具体奖励的情况下提取最优策略是必要的。到目前为止，许多类似的零射击强化学习算法都与探索问题脱钩，通常依赖其他的探索算法来收集数据。因此，研究人员认为应该从根本上使用FB表示进行探索，以便更有效地学习。

**方法:** 设计了从FB表示中自然产生的探索政策，以最小化FB表示的后验方差，从而减少其认知不确定性。

**结果:** 通过实证研究证明，与其他探索方法相比，这种有原则的探索策略能显著改善FB算法的样本复杂度。

**结论:** 利用FB表示进行有原则的探索策略能显著提高FB算法的样本复杂度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Epistemically-guided+forward-backward+exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05477&send_immediately=true&force_search=false)

**原文摘要:** Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [18] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen, Alessandro Rudi, Francesco Orabona, Nicolo Cesa-Bianchi*

**主要类别:** cs.LG

**AI概要:** This paper proposes a reduction approach for dynamic regret minimization in online convex optimization.


<details>
  <summary>更多</summary>
  
**动机:** To achieve low cumulative loss relative to an arbitrary benchmark sequence in online convex optimization.

**方法:** The study frames dynamic regret minimization as a static regret problem in a function space by constructing a suitable Reproducing Kernel Hilbert Space (RKHS).

**结果:** The reduction enables the recovery of optimal dynamic regret guarantees in the setting of linear losses and yields new scale-free and directionally-adaptive dynamic regret guarantees.

**结论:** The proposed reduction approach leads to algorithms that are computable in practice for dynamic regret minimization.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Regret+Reduces+to+Kernelized+Static+Regret，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05478，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05478&send_immediately=true&force_search=false)

**原文摘要:** We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [19] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen, Lionel Blondé, Alexandros Kalousis*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel training-free diffusion guidance framework called Stein Diffusion Guidance (SDG) that corrects approximate posteriors to remain faithful to the true diffusion posterior. Experiments demonstrate that SDG outperforms standard training-free guidance methods in challenging molecular generation tasks.


<details>
  <summary>更多</summary>
  
**动机:** Stochastic optimal control (SOC) has emerged as a principled framework for fine-tuning diffusion models but is impractical for fast sampling due to computationally intensive simulations. Training-free approaches have been developed but can introduce significant errors leading to unreliable guidance. This work aims to unify the strengths of both paradigms.

**方法:** A novel training-free diffusion guidance framework based on a surrogate stochastic optimal control objective is proposed. A new theoretical bound on the value function reveals the necessity of correcting approximate posteriors to remain faithful to the true diffusion posterior. The problem is connected with Stein variational inference to minimize the Kullback-Leibler discrepancy between the two posteriors. A principled correction mechanism and a novel running cost functional are introduced to enable effective guidance in low-density regions.

**结果:** Experiments on challenging molecular generation tasks show that the SDG method significantly outperforms standard training-free guidance methods.

**结论:** The proposed Stein Diffusion Guidance (SDG) method significantly outperforms standard training-free guidance methods in challenging molecular generation tasks, demonstrating its potential for broader applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Navigating+Sparse+Molecular+Data+with+Stein+Diffusion+Guidance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05482，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05482&send_immediately=true&force_search=false)

**原文摘要:** Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [20] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley, Chanwook Park, Wing Kam Liu, Sourav Saha*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，即可解释的分层深度学习神经网络（Ex-HiDeNN），该方法使用精确、节俭、快速、可分离和可扩展的神经架构与符号回归来从有限的观察中发现闭式表达式。


<details>
  <summary>更多</summary>
  
**动机:** 数据驱动科学和计算的进步极大地推动了利用可训练参数构建复杂功能关系的发展。然而，从复杂数据集中有效地发现可解释且准确的闭式表达式仍然是一个挑战。

**方法:** 文章提出了一种新的方法，即可解释的分层深度学习神经网络（Ex-HiDeNN），该方法使用精确、节俭、快速、可分离和可扩展的神经架构与符号回归来从有限的观察中发现闭式表达式。

**结果:** Ex-HiDeNN在多个基准问题上进行了测试，包括从数据中识别动力系统，并报告了结果。Ex-HiDeNN通常在这些基准测试中显示出卓越的逼近能力，相较于参考数据和传统符号回归，其误差更小。

**结论:** Ex-HiDeNN在所有测试中表现出色，相较于参考数据和传统的符号回归方法，其误差更小。此外，该方法在三个工程应用中均优于文献中使用的参考方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainable+Hierarchical+Deep+Learning+Neural+Networks+%28Ex-HiDeNN%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05498，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05498&send_immediately=true&force_search=false)

**原文摘要:** Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [21] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo, Bilal Farooq*

**主要类别:** cs.LG

**AI概要:** This paper presents an integrated graph-based neural networks architecture for predicting campus buildings occupancy and inter-buildings movement at dynamic temporal resolution using Wi-Fi logs.


<details>
  <summary>更多</summary>
  
**动机:** predicting campus buildings occupancy and inter-buildings movement at dynamic temporal resolution

**方法:** a novel Graph Convolution plus LSTM Neural Network (GCLSTM)

**结果:** the integrated GCLSTM models significantly outperform traditional pedestrian flow estimators like the Multi Layer Perceptron (MLP) and Linear Regression

**结论:** The integrated GCLSTM models significantly outperform traditional pedestrian flow estimators like the Multi Layer Perceptron (MLP) and Linear Regression.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Campus+Origin-Destination+Mobility+Prediction+using+Graph+Convolutional+Neural+Network+on+WiFi+Logs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05507，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05507&send_immediately=true&force_search=false)

**原文摘要:** We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [22] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman, Bassel Hamoud, Kfir Y. Levy*

**主要类别:** cs.LG

**AI概要:** A novel Multilevel Monte Carlo (MLMC) compression scheme is introduced to bridge the gap between biased and unbiased gradient compression techniques in distributed learning.


<details>
  <summary>更多</summary>
  
**动机:** Distributed learning methods face a critical bottleneck due to communication overhead. Gradient compression techniques help alleviate this issue but involve an inherent trade-off between the empirical efficiency of biased compressors and the theoretical guarantees of unbiased compressors.

**方法:** This work introduces a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased compressors to construct statistically unbiased estimates.

**结果:** The method is applied to popular compressors, like Top-$k$ and bit-wise compressors, resulting in enhanced variants. An adaptive version of the approach is also derived to further improve its performance.

**结论:** The MLMC compression scheme effectively bridges the gap between biased and unbiased methods, offering a versatile solution that can enhance the performance of popular compressors in distributed deep learning tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Communication+Overhead%3A+A+Multilevel+Monte+Carlo+Approach+for+Mitigating+Compression+Bias+in+Distributed+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05508&send_immediately=true&force_search=false)

**原文摘要:** Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [23] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du, Jennifer Zhang, Will Y. Zou*

**主要类别:** cs.LG

**AI概要:** A novel deep learning model is proposed for treatment effect optimization in user growth marketing, surpassing traditional methods and demonstrating superior algorithmic flexibility.


<details>
  <summary>更多</summary>
  
**动机:** User growth is a major strategy for consumer internet companies. To optimize costly marketing campaigns and maximize user engagement, a new methodology is needed.

**方法:** A novel treatment effect optimization methodology is proposed to enhance user growth marketing. The deep learning model learns from past experiments to optimize user selection and reward allocation, maximizing campaign impact while minimizing costs.

**结果:** Comprehensive evaluations validate the effectiveness of the model. The proposed constrained and direct optimization algorithms significantly outperform state-of-the-art methods by over 20%, proving their cost-efficiency and real-world impact.

**结论:** The proposed deep learning model for treatment effect optimization surpasses traditional methods and demonstrates superior algorithmic flexibility in handling complex business constraints.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heterogeneous+Causal+Learning+for+Optimizing+Aggregated+Functions+in+User+Growth，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05510，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05510&send_immediately=true&force_search=false)

**原文摘要:** User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [24] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong, Xuandong Zhao, Aneesh Pappu, Dawn Song*

**主要类别:** cs.LG

**AI概要:** This paper provides a comprehensive overview of the current state of research on Large Language Model (LLM) memorization, exploring the landscape, factors, detection methods, implications, and mitigation strategies.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to address concerns about model behavior, privacy risks, and the boundary between learning and memorization in Large Language Models (LLMs).

**方法:** The paper synthesizes recent studies and investigates the landscape of memorization, factors influencing it, and methods for its detection and mitigation. It explores key drivers such as training data duplication, training dynamics, and fine-tuning procedures. Methodologies like prefix-based extraction, membership inference, and adversarial prompting are examined for detecting and measuring memorized content.

**结果:** The result of this paper is a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions.

**结论:** This paper concludes that LLM memorization is a complex issue that requires further research to balance minimizing harmful memorization with utility.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Landscape+of+Memorization+in+LLMs%3A+Mechanisms%2C+Measurement%2C+and+Mitigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05578，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05578&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [25] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang, Shuyang Du, Will Y. Zou*

**主要类别:** cs.LG

**AI概要:** This paper derives a formulation for incorporating multiple treatment policy variables into the functional forms of individual and average treatment effects and develops a methodology to directly rank subjects using aggregated HTE functions.


<details>
  <summary>更多</summary>
  
**动机:** As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted across a wide range of scientific and industrial applications, the treatment action space can naturally expand, from a binary treatment variable to a structured treatment policy.

**方法:** A methodology is developed to directly rank subjects using aggregated HTE functions. In particular, a Neural-Augmented Naive Bayes layer within a deep learning framework is constructed to incorporate an arbitrary number of factors that satisfies the Naive Bayes assumption.

**结果:** These algorithms build towards a generic framework for deep learning of heterogeneous treatment policies and show power to improve performance with public datasets.

**结论:** The algorithms developed in this paper build towards a generic framework for deep learning of heterogeneous treatment policies and show power to improve performance with public datasets.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Learning+of+Continuous+and+Structured+Policies+for+Aggregated+Heterogeneous+Treatment+Effects，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05511&send_immediately=true&force_search=false)

**原文摘要:** As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [26] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir, Cristiana Diaconu, Valentinian Mihai Lungu, James Requeima, Richard E. Turner, Mark van der Wilk*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种管理结构不确定性的原则性方法，即通过贝叶斯推断在可能的因果结构和功能机制上进行平均。但由于因果结构的数量随着图中节点数量的增加而超指数增长，计算变得难以处理。因此，我们建议通过使用元学习来创建端到端模型：模型平均因果估计变压器神经过程（MACE-TNP）来规避这些挑战。


<details>
  <summary>更多</summary>
  
**动机:** 在科学领域，许多问题归结为“如果我们干预一个特定变量，我们将观察到什么效果？”如果已知因果关系（例如因果图），则可以估计干预分布。但在缺乏领域知识的情况下，必须从现有的观察数据中发现因果结构。然而，观察数据通常与多个因果图兼容，使得致力于单一结构的方法容易过于自信。

**方法:** 使用元学习创建端到端模型：模型平均因果估计变压器神经过程（MACE-TNP）。

**结果:** 实证结果表明，MACE-TNP优于强大的贝叶斯基线。

**结论:** MACE-TNP为复杂的贝叶斯因果推理提供了一个灵活和可扩展的范式，并且在未来可以扩展到更具挑战性的设置。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimating+Interventional+Distributions+with+Uncertain+Causal+Graphs+through+Meta-Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05526，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05526&send_immediately=true&force_search=false)

**原文摘要:** In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [27] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis, Andreas Vlachos, Adrian Weller*

**主要类别:** cs.LG

**AI概要:** This paper proposes InterpoLated Learning (InterpoLL), a method to mitigate shortcuts by interpolating the representations of majority examples to include features from intra-class minority examples.


<details>
  <summary>更多</summary>
  
**动机:** Existing shortcut mitigation approaches are model-specific, difficult to tune, computationally expensive, and fail to improve learned representations.

**方法:** InterpoLated Learning (InterpoLL) which interpolates the representations of majority examples to include features from intra-class minority examples with shortcut-mitigating patterns.

**结果:** Experimental results on multiple natural language understanding tasks demonstrate that InterpoLL improves minority generalization over both ERM and state-of-the-art shortcut mitigation methods.

**结论:** InterpoLL improves minority generalization without compromising accuracy on majority examples and has broad applicability across different architectures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Shortcut+Learning+with+InterpoLated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05527，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05527&send_immediately=true&force_search=false)

**原文摘要:** Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [28] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian, Sai Manoj Pudukotai Dinakarrao*

**主要类别:** cs.LG

**AI概要:** This paper investigates the vulnerability of GNN models to hardware-based fault attacks and proposes GBFA, which can significantly degrade prediction accuracy with minimal bit flips.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to investigate the vulnerability of GNN models to hardware-based fault attacks and introduce GBFA as a method to compromise GNN performance.

**方法:** The paper proposes Gradual Bit-Flip Fault Attack (GBFA), a layer-aware bit-flip fault attack that compromises GNN performance by flipping a minimal number of bits. GBFA operates in two steps: creating a Markov model to predict the execution sequence of layers and identifying vulnerable bits within selected weights using gradient ranking.

**结果:** GBFA degrades prediction accuracy significantly, for instance, it degrades GraphSAGE's prediction accuracy by 17% on the Cora dataset with only a single bit flip in the last layer.

**结论:** GBFA can significantly degrade the prediction accuracy of GNN models with minimal bit flips, highlighting the importance of layer-aware attack strategies.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bit-Flip+Fault+Attack%3A+Crushing+Graph+Neural+Networks+via+Gradual+Bit+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05531&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [29] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun, Hongkang Li, Meng Wang*

**主要类别:** cs.LG

**AI概要:** This paper provides the first theoretical analysis of GCNs with jumping connections and graph sparsification, revealing insights into how sparsification affects generalization and the role of jumping connections.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is the lack of theoretical understanding of the generalization guarantees for GCNs with graph sparsification and jumping connections, which have shown empirical success but remain theoretically unexplored.

**方法:** The paper presents a theoretical analysis of the learning dynamics and generalization of GCNs with jumping connections using graph sparsification.

**结果:** The results show that the generalization accuracy of the learned model closely approximates the highest achievable accuracy within a class of target functions dependent on the sparse effective adjacency matrix. It also reveals that the generalization is more affected by the sparsified matrix deviations in the first layer than in the second in a two-hidden-layer GCN.

**结论:** The study concludes that graph sparsification maintains generalization performance when the sparse effective adjacency matrix preserves essential edges, and jumping connections lead to different sparsification requirements across layers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Theoretical+Learning+Performance+of+Graph+Neural+Networks%3A+The+Impact+of+Jumping+Connections+and+Layer-wise+Sparsification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05533&send_immediately=true&force_search=false)

**原文摘要:** Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [30] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu, Mohammad Sadegh Nasr, James P. Long, Kim-Anh Do, Ehsan Irajizad*

**主要类别:** cs.LG

**AI概要:** 本文提出了潜在空间约束图神经网络（LSC-GNN），以整合外部“干净”链接并引导含噪目标图的嵌入。


<details>
  <summary>更多</summary>
  
**动机:** 图神经网络（GNNs）常常难以处理含噪边的问题。

**方法:** 通过在完整图（目标加外部边）上训练一个编码器，在排除目标潜在噪声链接的正则化图上训练另一个编码器，并惩罚它们潜在表示之间的差异。

**结果:** 在基准数据集上的实验表明，LSC-GNN在受到中等噪声影响的图中表现优于标准和抗噪GNN。

**结论:** LSC-GNN有潜力提高在含有噪声的关系结构中的预测性能和可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Learning+on+Noisy+Graphs+via+Latent+Space+Constraints+with+External+Knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05540，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05540&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [31] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao, Sol Lim, Seokhyun Chung*

**主要类别:** cs.LG

**AI概要:** This study proposes an enhanced load estimation framework incorporating auxiliary information for more accurate ergonomic risk assessment in manual material handling.


<details>
  <summary>更多</summary>
  
**动机:** Existing approaches for estimating carried load from gait motion data often rely on direct mappings from loaded gait to hand load, limiting generalization and predictive accuracy. There is a need for an enhanced load estimation framework that incorporates auxiliary information.

**方法:** Our model integrates deep latent variable modeling with temporal convolutional networks and bi-directional cross-attention to capture gait dynamics and fuse loaded and unloaded gait patterns.

**结果:** Experiments using real-world data collected from inertial measurement units attached to participants demonstrate substantial accuracy gains from incorporating auxiliary information.

**结论:** The proposed load estimation framework that integrates deep latent variable modeling with temporal convolutional networks and bi-directional cross-attention, demonstrates substantial accuracy gains from incorporating auxiliary information and highlights the importance of explicit fusion mechanisms.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gait-Based+Hand+Load+Estimation+via+Deep+Latent+Variable+Models+with+Auxiliary+Information，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05544&send_immediately=true&force_search=false)

**原文摘要:** Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [32] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho, Sam Hall-McMaster, Honglak Lee, Samuel J. Gershman*

**主要类别:** cs.LG

**AI概要:** This paper introduces Multitask Preplay, an algorithm inspired by human learning that improves prediction and generalization across multiple tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to leverage human experience on one task to preemptively learn solutions to other tasks that were accessible but not pursued.

**方法:** The paper proposes Multitask Preplay, a novel algorithm that replays experience on one task as the starting point for 'preplay' - counterfactual simulation of an accessible but unpursued task.

**结果:** Compared to traditional planning and predictive representation methods, multitask preplay better predicts how humans generalize to tasks that were accessible but not pursued. The predictions generalize to Craftax, a partially observable 2D Minecraft environment. Multitask Preplay enables artificial agents to learn behaviors that transfer to novel Craftax worlds sharing task co-occurrence structure.

**结论:** Multitask Preplay is a scalable theory of how humans counterfactually learn and generalize across multiple tasks, which can significantly improve the performance of artificial agents in challenging multitask environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preemptive+Solving+of+Future+Problems%3A+Multitask+Preplay+in+Humans+and+Machines，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05561，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05561&send_immediately=true&force_search=false)

**原文摘要:** Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [33] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li, Shiqi Chen, Tingyu Gong, Aydogan Ozcan*

**主要类别:** cs.LG

**AI概要:** The paper presents a new method to optimize optical computing using a model-free reinforcement learning approach.


<details>
  <summary>更多</summary>
  
**动机:** Optical computing holds promise for high-speed, energy-efficient information processing, but the effective optimization and alignment of the diffractive layers is a challenge.

**方法:** A model-free reinforcement learning approach utilizing Proximal Policy Optimization (PPO) for the in situ training of diffractive optical processors.

**结果:** The method demonstrated better convergence and performance across a range of in situ learning tasks.

**结论:** The in situ reinforcement learning approach could offer a scalable framework for various optical and physical systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-free+Optical+Processors+using+In+Situ+Reinforcement+Learning+with+Proximal+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05583，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05583&send_immediately=true&force_search=false)

**原文摘要:** Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [34] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

**主要类别:** cs.LG

**AI概要:** This work proposes a unified Fourier Spectral Transformer network that uses high precision numerical solvers to generate training data and a Transformer network to model the evolution of the spectral coefficients.


<details>
  <summary>更多</summary>
  
**动机:** To develop a method that can achieve highly accurate long term predictions even with limited training data for forecasting future flow dynamics.

**方法:** A unified Fourier Spectral Transformer network that integrates the strengths of classical spectral methods and attention based neural architectures.

**结果:** The spectral Transformer can achieve highly accurate long term predictions even with limited training data, better than traditional numerical methods and machine learning methods in forecasting future flow dynamics.

**结论:** The proposed framework generalizes well to unseen data, bringing a promising paradigm for real time prediction and control of complex dynamical systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Fourier+Spectral+Transformer+Networks+For+Efficient+and+Generalizable+Nonlinear+PDEs+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05584，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05584&send_immediately=true&force_search=false)

**原文摘要:** In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [35] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma*

**主要类别:** cs.LG

**AI概要:** 本文通过大规模实证研究分析了奖励黑客行为，并提出了检测和缓解策略。


<details>
  <summary>更多</summary>
  
**动机:** 奖励黑客行为在强化学习系统中对自主代理的部署构成了严重威胁，因此需要系统地检测和缓解这种行为。

**方法:** 通过对15,247个训练情节、15个RL环境和5种算法进行大规模实证研究，并实施了针对六类奖励黑客行为的自动检测算法。

**结果:** 检测框架在不同环境下实现了78.4%的精确度和81.7%的召回率，计算开销不到5%；缓解技术在受控情景下将黑客行为频率降低了最多54.6%。

**结论:** 该研究提供了对奖励黑客行为的系统性检测和缓解方法，尽管在实践中应用这些技术仍然具有挑战性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+and+Mitigating+Reward+Hacking+in+Reinforcement+Learning+Systems%3A+A+Comprehensive+Empirical+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05619&send_immediately=true&force_search=false)

**原文摘要:** Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [36] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia, Ciyuan Peng, Jing Ren, Falih Gozi Febrinanto, Renqiang Luo, Vidya Saikrishna, Shuo Yu, Xiangjie Kong*

**主要类别:** cs.LG

**AI概要:** 这篇论文是对图学习领域的综合调查，重点介绍了几个关键技术方向，并探讨了伦理考虑，如隐私和公平性，确保负责任地部署图学习模型。


<details>
  <summary>更多</summary>
  
**动机:** Graph learning 的发展始于早期的图论方法，在图神经网络（GNNs）出现后得到了显著的动力。在过去的十年中，可扩展架构、动态图建模、多模态学习、生成式AI、可解释AI（XAI）和负责任的AI等方面的进展扩大了图学习在各种具有挑战性的环境中的应用。

**方法:** 本调查提供了一个全面的介绍，重点是可扩展、时间序列、多模态、生成式、可解释和负责任的图学习等关键维度。我们回顾了处理大规模图、捕捉动态时间依赖性、整合异构数据模式、生成新的图样本和增强可解释性的最先进的技术。

**结果:** 由于能够建模复杂、非欧几里得关系的能力，图学习在从药物发现和欺诈检测到推荐系统和科学推理等各种现实世界的应用中更好地支持传统机器学习难以捕捉的关系。

**结论:** Graph learning 是一个快速发展的领域，其面临的挑战需要被解决以发挥其全部潜力。本调查为研究人员和从业者提供了宝贵的资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05636，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05636&send_immediately=true&force_search=false)

**原文摘要:** Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [37] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera, Neil Mallinar, James B. Simon, Mikhail Belkin*

**主要类别:** cs.LG

**AI概要:** This paper proves the Features at Convergence Theorem (FACT) for neural network weights trained with weight decay and develops a new learning algorithm, FACT-RFM, which performs well on tabular data.


<details>
  <summary>更多</summary>
  
**动机:** To understand how neural networks learn and represent features.

**方法:** Proof of the Features at Convergence Theorem (FACT) and empirical validation of the relation between the "feature matrix" and input vectors/loss gradients. Modification of Recursive Feature Machines to develop FACT-RFM.

**结果:** Neural features satisfy the FACT at convergence. FACT-RFM achieves high performance on tabular data and captures various feature learning behaviors.

**结论:** The Features at Convergence Theorem (FACT) provides a self-consistency equation for neural network weights at convergence, and FACT-RFM is a new learning algorithm that achieves high performance on tabular data.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FACT%3A+the+Features+At+Convergence+Theorem+for+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05644，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05644&send_immediately=true&force_search=false)

**原文摘要:** A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [38] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez, Léonie Straß, Sebastian Meller, Holger Volk, Anna Zamansky, Itzik Klein*

**主要类别:** cs.LG

**AI概要:** This study explores the use of wearable inertial sensors and deep learning techniques for canine gait analysis in veterinary clinical settings.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this study is to provide a tool for clinicians to more accurately differentiate between mobility impairments caused by neurological and orthopedic conditions, which can be difficult even for experienced professionals.

**方法:** The study developed a deep learning approach using inertial sensor readings to distinguish between neurological and orthopedic gaits.

**结果:** The proposed approach achieved 96% accuracy in the multiclass classification task (healthy/orthopedic/neurological) and 82% accuracy in the binary classification task (healthy/non-healthy) when generalizing to unseen dogs.

**结论:** The study concludes that inertial-based deep learning models have the potential to serve as practical and objective diagnostic tools in differentiating between orthopedic and neurological gait abnormalities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Canine+Clinical+Gait+Analysis+for+Orthopedic+and+Neurological+Disorders%3A+An+Inertial+Deep-Learning+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05671，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05671&send_immediately=true&force_search=false)

**原文摘要:** Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [39] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang, Yingfan Ma, Yanmei Gu, Zhengkai Yang, Yihong Zhuang, Feng Wang, Zenan Huang, Yuanyuan Wang, Chao Huang, Bowen Song, Cheng Lin, Junbo Zhao*

**主要类别:** cs.LG

**AI概要:** This paper introduces ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs' physical reasoning and generalization capabilities.


<details>
  <summary>更多</summary>
  
**动机:** Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability.

**方法:** ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions.

**结果:** Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants.

**结论:** ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ABench-Physics%3A+Benchmarking+Physical+Reasoning+in+LLMs+via+High-Difficulty+and+Dynamic+Physics+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.04766，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.04766&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [40] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen, Boyang Zhang, Xiangwei Zhou, Mingxuan Sun, Shuai Zhang, Songyang Zhang, Geoffrey Ye Li*

**主要类别:** cs.LG

**AI概要:** 本文强调了动态客户端-专家对齐的鲁棒定量策略的缺失，并提出了一种概念性的系统设计以实现智能客户端-专家对齐，从而解决了系统性问题，实现了更少的通信轮次收敛，为大规模联邦MoE结构LAM在边缘计算中的广泛部署铺平了道路。


<details>
  <summary>更多</summary>
  
**动机:** 有效的联合训练这些复杂的MoE结构LAMs受到系统级挑战的阻碍，特别是在管理异构客户端资源与众多专门专家所需的复杂协调之间的相互作用时。因此需要一种新的策略来考虑客户端能力和系统负载均衡。

**方法:** 我们提出了一个智能客户端-专家对齐的概念系统设计，包括动态适应性评分、全局专家负载监控和客户端能力剖析。

**结果:** 提出了一种概念性的系统设计以实现智能客户端-专家对齐并解决了系统性问题，可以实现更少的通信轮次收敛。

**结论:** 通过解决系统性问题，我们可以解锁更可扩展、高效和鲁棒的训练机制，从而在边缘计算中部署超高效通信的大规模联邦MoE结构LAM。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Training+of+Large-Scale+AI+Models+Through+Federated+Mixture-of-Experts%3A+A+System-Level+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05685，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05685&send_immediately=true&force_search=false)

**原文摘要:** The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [41] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li, Zefan Wang, Ye He, Yuxuan Li, Qi Shi, Jianling Li, Yonggang Hu, Wanxiang Che, Xu Han, Zhiyuan Liu, Maosong Sun*

**主要类别:** cs.LG

**AI概要:** 该研究介绍了AutoTriton，一个用于Triton编程的强化学习模型，它通过监督微调和强化学习提高了GPU编程效率，并在多个基准测试中表现出与主流大型模型相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管像Triton这样的领域特定语言通过抽象底层细节简化了GPU编程，开发者仍需要手动调整关键参数，如磁贴尺寸和内存访问模式，这成为性能优化和广泛应用的重大障碍。

**方法:** 本研究引入了AutoTriton，这是第一个由强化学习驱动的针对Triton编程的模型。AutoTriton通过监督微调（SFT）和使用Group Relative Policy Optimization (GRPO)算法进行强化学习（RL），结合基于规则的奖励和基于执行的奖励，逐步提高Triton编程能力。

**结果:** 实验表明，8B模型AutoTriton在TritonBench和KernelBench的五个评估通道中表现与主流大型模型相当，包括Claude-4-Sonnet和DeepSeek-R1-0528。进一步的实验证明了AutoTriton每个模块的重要性，包括SFT阶段、RL阶段和奖励设计策略。

**结论:** AutoTriton通过强化学习为高性能内核的自动生成提供了可能，为核心AI系统的建设奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoTriton%3A+Automatic+Triton+Programming+with+Reinforcement+Learning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05687，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05687&send_immediately=true&force_search=false)

**原文摘要:** Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [42] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, Dong Yu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个名为MobileGUI-RL的可扩展框架，用于在线环境中训练GUI代理，从而解决了现有方法存在的可扩展性、过拟合和策略脆弱性问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在离线环境下使用预收集轨迹训练GUI代理，限制了可扩展性，导致对特定UI模板的过拟合，并在面对未见环境时产生脆弱的策略。

**方法:** MobileGUI-RL框架包含了两个关键组件：(i) 通过自我探索和过滤合成可学习任务的课程，(ii) 使用轨迹感知优势和复合奖励来适应GRPO进行GUI导航。

**结果:** 在三个在线移动代理基准测试上的实验显示持续的收益，验证了我们方法的有效性。

**结论:** MobileGUI-RL框架通过在线环境训练GUI代理，有效提升了自动化任务的成功率和执行效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MobileGUI-RL%3A+Advancing+Mobile+GUI+Agent+through+Reinforcement+Learning+in+Online+Environment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05720&send_immediately=true&force_search=false)

**原文摘要:** Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [43] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li, Ziye Jia, Sijie He, Kun Guo, Qihui Wu*

**主要类别:** cs.LG

**AI概要:** This paper proposes a dual-layer UAV-assisted edge computing architecture and a hierarchical offloading scheme to efficiently integrate and coordinate heterogeneous resources in dynamic vehicular environments.


<details>
  <summary>更多</summary>
  
**动机:** Existing UAV-assisted offloading strategies are insufficient in coordinating heterogeneous computing resources and adapting to dynamic network conditions in vehicular networks with compute-intensive and delay-sensitive applications.

**方法:** A dual-layer UAV-assisted edge computing architecture based on partial offloading is proposed, composed of the relay capability of high-altitude UAVs and the computing support of low-altitude UAVs. The joint optimization problem is formulated to minimize system delay and energy consumption while ensuring task completion rate. The high-dimensional decision problem is reformulated as a Markov decision process and solved by a hierarchical offloading scheme based on the soft actor-critic algorithm.

**结果:** Simulations show that the proposed approach outperforms several baselines in task completion rate, system efficiency, and convergence speed.

**结论:** The proposed dual-layer UAV-assisted edge computing architecture and hierarchical offloading scheme demonstrate strong robustness and applicability in dynamic vehicular environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Task+Offloading+for+UAV-Assisted+Vehicular+Edge+Computing+via+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05722，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05722&send_immediately=true&force_search=false)

**原文摘要:** With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [44] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen, Markus Götz, Lars H. Heyen, Achim Streit, Charlotte Debus*

**主要类别:** cs.LG

**AI概要:** This paper introduces WeatherMixer and Jigsaw to improve weather forecasting by overcoming computational bottlenecks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to overcome the limitations of accelerator memory and I/O-bandwidth in training large neural networks for accurate atmospheric dynamics modeling.

**方法:** The paper proposes WeatherMixer, a multi-layer-perceptron-based architecture, and Jigsaw, a novel model parallelization scheme.

**结果:** The model achieves similar accuracy to numerical weather prediction and scales training to 256 GPUs with high performances and scaling efficiency.

**结论:** WeatherMixer and Jigsaw have shown significant improvements in weather forecasting, achieving high scaling efficiency and performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jigsaw%3A+Training+Multi-Billion-Parameter+AI+Weather+Models+with+Optimized+Model+Parallelism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05753，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05753&send_immediately=true&force_search=false)

**原文摘要:** AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [45] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin, Gemma Piella, Mario Ceresa, Miguel A. Gonzalez Ballester*

**主要类别:** cs.LG

**AI概要:** This study presents a novel method combining deep learning with physics-informed regularization for cardiac tissue analysis, achieving high accuracy in disease classification and improving diagnostic reliability.


<details>
  <summary>更多</summary>
  
**动机:** Cardiac diseases require accurate and timely diagnostic strategies. This study aims to improve image registration accuracy and provide insights into the underlying biomechanical processes of cardiac tissues.

**方法:** This study introduces an innovative approach that combines deep learning image registration with physics-informed regularization to predict the biomechanical properties of moving cardiac tissues and extract features for disease classification.

**结果:** Evaluation on the ACDC dataset achieved high Dice scores for various cardiac regions. The best performing classifier obtained a classification accuracy of 98% in the training set and 100% in the test set of the ACDC dataset.

**结论:** By integrating explainable artificial intelligence, this method significantly improves the accuracy and reliability of cardiac disease diagnosis, paving the way for more personalized and effective patient care.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Motion+to+Meaning%3A+Biomechanics-Informed+Neural+Network+for+Explainable+Cardiovascular+Disease+Identification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05783，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05783&send_immediately=true&force_search=false)

**原文摘要:** Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [46] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi, Ziqi Xu, Stefan Westerlund, Conrad Sanderson*

**主要类别:** cs.LG

**AI概要:** This paper proposes a new approach for predicting the dynamics of graph time series by combining time series prediction methods and an adapted form of flux balance analysis (FBA). The approach addresses the limitations of existing graph prediction methods and demonstrates good performance on both synthetic and real datasets.


<details>
  <summary>更多</summary>
  
**动机:** Existing approaches for graph prediction have limitations such as assuming that the vertices do not to change between consecutive graphs. To address this, we propose to exploit time series prediction methods in combination with an adapted form of flux balance analysis (FBA).

**方法:** The approach exploits time series prediction methods in combination with an adapted form of flux balance analysis (FBA), a linear programming method originating from biochemistry.

**结果:** Empirical evaluations on synthetic datasets and real datasets demonstrate the efficacy of the proposed approach.

**结论:** The proposed approach, which combines time series prediction methods and an adapted form of FBA, is effective for predicting the dynamics of graph time series.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Graph+Structure+via+Adapted+Flux+Balance+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05806，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05806&send_immediately=true&force_search=false)

**原文摘要:** Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [47] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

**主要类别:** cs.LG

**AI概要:** This paper tackles the problems of hyperparameter tuning and model robustness in few-shot domain adaptation by training multiple independent adapters and averaging their outputs.


<details>
  <summary>更多</summary>
  
**动机:** Tackle the problems of hyperparameter tuning and model robustness in few-shot domain adaptation of foundation models

**方法:** Training multiple independent adapters and averaging their outputs

**结果:** The new model has a higher performance and is more robust to distribution shifts compared to any individual adapter. The ensemble is also significantly less sensitive to the residual ratio.

**结论:** The ensemble of multiple independent adapters has a higher performance and is more robust to distribution shifts. The method also addresses the problems of hyperparameter tuning and model robustness.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Robustness+of+Foundation+Models+in+Domain+Adaptation+with+Soup-Adapters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05807&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [48] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna, Kateryna Tarelkina, Eloïse Berthier, Gianni Franchi*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel framework and interactive tool that extends concept-based interpretability methods into the domain of mechanistic interpretability.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to extend concept-based interpretability methods into the domain of mechanistic interpretability for a global dissection of model behavior.

**方法:** The approach involves analyzing how high-level semantic attributes emerge, interact, and propagate through internal model components. The framework systematically quantifies how semantic concepts are represented across layers, revealing latent circuits and information flow underlying model decision-making.

**结果:** A key innovation is the visualization platform named BAGEL, which presents insights in a structured knowledge graph, allowing users to explore concept-class relationships, identify spurious correlations, and enhance model trustworthiness.

**结论:** The proposed framework and tool contribute to a deeper understanding of how deep learning models generalize or fail in the presence of dataset biases.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Concept-Based+Mechanistic+Interpretability+Using+Structured+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05810&send_immediately=true&force_search=false)

**原文摘要:** While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [49] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian, Guanyu Hu, Dimitrios Kollias, Xinyu Yang, Oya Celiktutan*

**主要类别:** cs.LG

**AI概要:** 本文研究了领域泛化和算法公平性的问题，并提出了一个实际框架PAFDG来解决FairDG问题并模拟效用-公平性的权衡。


<details>
  <summary>更多</summary>
  
**动机:** 领域泛化和算法公平性是机器学习中的两个关键挑战，但大多数DG方法只关注最小化未见目标域中的预期风险，而不考虑算法公平性，而公平性方法通常不考虑领域转移。

**方法:** 研究Fair Domain Generalization（FairDG）问题，推导了基于互信息的预期风险和公平性违规上限，并引入了PAFDG框架。

**结果:** 实验表明，PAFDG在现实世界的视觉和语言数据集上实现了比现有方法更优的效用-公平性权衡。

**结论:** PAFDG框架在现实世界的视觉和语言数据集上实现了比现有方法更优的效用-公平性权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fair+Domain+Generalization%3A+An+Information-Theoretic+View，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05823，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05823&send_immediately=true&force_search=false)

**原文摘要:** Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [50] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah, Kerol Djoumessi, Philipp Berens*

**主要类别:** cs.LG

**AI概要:** A novel FL framework is proposed that tackles statistical heterogeneity and minimises communication load by utilising prototypes and lightweight adapter modules.


<details>
  <summary>更多</summary>
  
**动机:** Real-world FL faces challenges including communication overhead and statistical heterogeneity across clients.

**方法:** The proposed FL framework utilises prototypes and lightweight adapter modules to tackle statistical heterogeneity and minimise communication load.

**结果:** Experiments on a real-world retinal fundus image dataset show improvements in accuracy over baseline algorithms.

**结论:** The proposed FL framework provides a promising solution for real-world distributed data sources, especially in medical image analysis.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prototype-Guided+and+Lightweight+Adapters+for+Inherent+Interpretation+and+Generalisation+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05852&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [51] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas, Markos Asprou, Charalambos Konstantinou, Maria K. Michael*

**主要类别:** cs.LG

**AI概要:** Proposes a hybrid approach using physics-informed neural networks (PINNs) to enhance the accuracy and robustness of power system state estimation.


<details>
  <summary>更多</summary>
  
**动机:** Modern power systems face significant challenges in state estimation and real-time monitoring, particularly regarding response speed and accuracy under faulty conditions or cyber-attacks.

**方法:** This paper proposes a hybrid approach using physics-informed neural networks (PINNs) for power system state estimation.

**结果:** Experimental results show that the proposed approach outperforms traditional machine learning models, achieving up to 83% higher accuracy on unseen subsets of the training dataset and 65% better performance on entirely new, unrelated datasets. During a data manipulation attack against a critical bus in a system, the PINN can be up to 93% more accurate than an equivalent neural network.

**结论:** The proposed PINN approach demonstrates superior accuracy and robustness in power system state estimation, especially during data manipulation attacks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Power+System+State+Estimation+using+Physics-Informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05874&send_immediately=true&force_search=false)

**原文摘要:** Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [52] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz, Frederik Hoppe, Marianne Michaelis, Udo Göbel*

**主要类别:** cs.LG

**AI概要:** 提出了一种新方法，将表格数据转化为图形结构，并利用图自动编码器生成实体嵌入，再将其聚合并得到每一表格行的嵌入，从而在嵌入空间中执行各种下游任务，而且此方法对于未见过的样本同样适用。实验结果表明，此方法比现有的通用表格数据嵌入技术性能更优。


<details>
  <summary>更多</summary>
  
**动机:** 表格数据在关系数据库中代表了工业数据的重要部分。因此，分析和解释表格数据至关重要。然而，在设置工业数据库时，往往未指定表格数据的应用任务。

**方法:** 我们的方法将表格数据转换为图形结构，利用图自动编码器创建实体嵌入，然后将其聚合以获得每个表格行（即每个数据样本）的嵌入。

**结果:** 我们提出了一种新的框架，用于生成通用的、即任务无关的表格数据嵌入，以便在没有预定义目标的情况下执行下游任务。

**结论:** 我们的方法在现实世界的数据集上进行的实验表明，与现有的通用表格数据嵌入技术相比，该方法具有更好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Embeddings+of+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05904，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05904&send_immediately=true&force_search=false)

**原文摘要:** Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [53] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li, Marco Hutter, Andreas Krause*

**主要类别:** cs.LG

**AI概要:** This survey compares feature-based and GAN-based methods for learning from demonstrations, focusing on reward function structures and their impact on policy learning.


<details>
  <summary>更多</summary>
  
**动机:** To provide a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning.

**方法:** Comparative analysis of feature-based and GAN-based approaches to learning from demonstrations

**结果:** Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration.

**结论:** The choice between feature-based and GAN-based methods should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature-Based+vs.+GAN-Based+Learning+from+Demonstrations%3A+When+and+Why，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05906，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05906&send_immediately=true&force_search=false)

**原文摘要:** This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [54] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang, Shitong Shao, Zikai Zhou, Pukun Zhao, Hangyu Guo, Tian Ye, Lichen Bai, Shuo Yang, Zeke Xie*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel Diffusion Dataset Condensation (D2C) framework to construct a synthetic sub-dataset with significantly fewer samples than the original dataset, enabling high-quality diffusion model training with greatly reduced cost.


<details>
  <summary>更多</summary>
  
**动机:** From a data-centric perspective addressing the limitation that training diffusion models remains highly resource-intensive, diffusion dataset condensation is studied as a new and challenging problem setting.

**方法:** A novel Diffusion Dataset Condensation (D2C) framework is proposed, which consists of two phases: Select and Attach.

**结果:** Extensive experiments across various dataset sizes, model architectures, and resolutions show that our D2C framework enables significantly faster diffusion model training with dramatically fewer data, while preserving high visual quality. Notably, for the SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID score of 4.3 in just 40k steps using only 0.8% of the training data.

**结论:** The D2C framework enables significantly faster diffusion model training with dramatically fewer data, while preserving high visual quality.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Dataset+Condensation%3A+Training+Your+Diffusion+Model+Faster+with+Less+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05914，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05914&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [55] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin, Tom Strube, Niklas Tschorn, Michael Pantförder, Maximilian Fecke, Ingrid Ljungvall, Jens Häggström, Gerhard Wess, Christoph Schummer, Sven Meister, Falk M. Howar*

**主要类别:** cs.LG

**AI概要:** 研究发现，通过整合多位专家意见和减少标签噪声，可以显著提高犬类心脏杂音分类算法的准确性，特别是XGBoost算法表现突出。


<details>
  <summary>更多</summary>
  
**动机:** 嘈杂的标签对兽医学中AI模型的训练提出了重大挑战，尤其是犬类听诊数据中的专家评估模糊性以及其对分类性能的负面影响。

**方法:** 研究评估了专家评估在犬类听诊数据中的模糊性，并通过整合多位专家的意见来减少标签噪声。使用了140份心音录音数据，并从中筛选出70份高质量数据。通过利用单个心动周期扩展训练数据并增强分类鲁棒性。比较了三种分类算法：AdaBoost、XGBoost和随机森林。

**结果:** 所有算法因标签噪声减少而显示出分类准确性的显著提高，特别是XGBoost。对于轻度心脏杂音的检测，敏感性从37.71%增加到90.98%，特异性从76.70%增加到93.69%。在中度类别中，敏感性从30.23%上升到55.81%，特异性从64.56%上升到97.19%。在响亮/激动人心的类别中，敏感性和特异性分别从58.28%增加到95.09%，从84.84%增加到89.69%。

**结论:** 减少标签噪声可以显著提高分类算法在检测犬类心脏杂音方面的性能，特别是XGBoost算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+AI-Based+Canine+Heart+Disease+Diagnosis+with+Expert-Consensus+Auscultation+Labeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05950&send_immediately=true&force_search=false)

**原文摘要:** Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [56] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng, Shuang Qin, Yue Yu, Fangqing Jiang, Hui Wang, Zhouchen Lin*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一个新的观点，即将Adam视为符号优化器进行简化分析，从而得到了更优的收敛速度。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Adam在训练深度神经网络方面具有显著的经验成功，但其理论收敛性分析仍然不令人满意。现有的工作主要将Adam解释为带有动量的预条件随机梯度下降，这种观点需要强烈的假设和复杂的技巧，导致难以验证和扩展的冗长而不透明的收敛性证明。

**方法:** 该论文提出了一种新的观点，将Adam视为一个符号优化器（sign-like optimizer），并对其进行了简化分析。

**结果:** 该论文通过将Adam解释为符号优化器，首次在较弱的假设下证明了Adam可以达到最优的O(1/T^(1/4))的收敛速度，而不是之前的O(lnT/T^(1/4))。

**结论:** 该论文通过将Adam解释为符号优化器，首次在较弱的假设下证明了Adam可以达到最优的收敛速度，并为动量的作用提供了新的见解，进一步缩小了理论与实践之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Simple+Convergence+Proof+of+Adam+From+a+Sign-like+Descent+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05966，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05966&send_immediately=true&force_search=false)

**原文摘要:** Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [57] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen, R. Rabe, M. H. Davel*

**主要类别:** cs.LG

**AI概要:** KnowIt is a flexible Python framework for deep time series modeling and interpretation with minimal assumptions and easy customization.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind KnowIt is to create a flexible framework for building and interpreting deep time series models with minimal assumptions about task specifications.

**方法:** KnowIt is implemented as a Python toolkit that decouples the definition of dataset, deep neural network architecture, and interpretability technique through well defined interfaces.

**结果:** KnowIt ensures the ease of importing new datasets, custom architectures, and defining different interpretability paradigms while maintaining on-the-fly modeling and interpretation of time series data.

**结论:** KnowIt aims to provide an environment for knowledge discovery on complex time series data and to become a trusted tool for deep time series modeling.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KnowIt%3A+Deep+Time+Series+Modeling+and+Interpretation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06009，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06009&send_immediately=true&force_search=false)

**原文摘要:** KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [58] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough, Marian Andrecki, James Shinner, Daniele Donghi*

**主要类别:** cs.LG

**AI概要:** Kamae is an open-source Python library that bridges the gap between offline and online environments in production recommender systems by translating PySpark preprocessing pipelines into equivalent Keras models.


<details>
  <summary>更多</summary>
  
**动机:** Feature preprocessing must be faithfully replicated across training and inference environments in production recommender systems. This often leads to duplication of logic, increased engineering effort, and risks of dataset shift.

**方法:** Kamae is an open-source Python library that translates PySpark preprocessing pipelines into equivalent Keras models. It provides configurable Spark transformers and estimators, each mapped to a corresponding Keras layer.

**结果:** Kamae enables consistent, end-to-end preprocessing across the ML lifecycle and its utility is demonstrated on real-world use cases including MovieLens dataset and Expedia's Learning-to-Rank pipelines.

**结论:** Kamae provides a solution to bridge the gap between offline and online environments in production recommender systems, enabling consistent preprocessing across the ML lifecycle.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kamae%3A+Bridging+Spark+and+Keras+for+Seamless+ML+Preprocessing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06021，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06021&send_immediately=true&force_search=false)

**原文摘要:** In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [59] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

**主要类别:** cs.LG

**AI概要:** This paper introduces a universal approach for learning in HDLSS setting using multi-view mid fusion techniques.


<details>
  <summary>更多</summary>
  
**动机:** The high-dimensional low-sample-size (HDLSS) setting presents significant challenges in various applications where the feature dimension far exceeds the number of available samples.

**方法:** This paper introduces a universal approach for learning in HDLSS setting using multi-view mid fusion techniques. It shows how existing mid fusion multi-view methods perform well in an HDLSS setting even if no inherent views are provided. Three view construction methods are proposed that split the high-dimensional feature vectors into smaller subsets, each representing a different view.

**结果:** Extensive experimental validation across model-types and learning tasks confirm the effectiveness and generalization of the approach.

**结论:** The work in this paper lays the foundation for further research into the universal benefits of multi-view mid fusion learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-view+mid+fusion%3A+a+universal+approach+for+learning+in+an+HDLSS+setting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06026&send_immediately=true&force_search=false)

**原文摘要:** The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [60] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo, Tommaso Polonelli, Amirhossein Moallemi, Luca Benini, Michele Magno*

**主要类别:** cs.LG

**AI概要:** EdgeCodec is an end-to-end neural compressor for barometric data collected from wind turbine blades that achieves high compression rates while maintaining low reconstruction error and operating in real time on a microcontroller.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind EdgeCodec is to present an end-to-end neural compressor for barometric data collected from wind turbine blades.

**方法:** EdgeCodec leverages a heavily asymmetric autoencoder architecture, trained with a discriminator and enhanced by a Residual Vector Quantizer to maximize compression efficiency.

**结果:** EdgeCodec achieves compression rates between 2'560:1 and 10'240:1 while maintaining a reconstruction error below 3%. It operates in real time on the GAP9 microcontroller with bitrates ranging from 11.25 to 45 bits per second. In its highest compression mode, EdgeCodec reduces the energy consumption of wireless data transmission by up to 2.9x.

**结论:** EdgeCodec can achieve high compression rates while maintaining low reconstruction error and operates in real time on the GAP9 microcontroller. It also reduces the energy consumption of wireless data transmission significantly, extending the operational lifetime of deployed sensor units.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EdgeCodec%3A+Onboard+Lightweight+High+Fidelity+Neural+Compressor+with+Residual+Vector+Quantization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06040&send_immediately=true&force_search=false)

**原文摘要:** We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [61] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer, Corné Verburg, Alexander Heinlein, Miriam Schulte*

**主要类别:** cs.LG

**AI概要:** This paper introduces the Local-Global Convolutional Neural Network (LGCNN) approach for modeling groundwater flow with heat transport, which combines numerical simulations with machine learning.


<details>
  <summary>更多</summary>
  
**动机:** Machine learning methods struggle with real-world applications due to limited or low-quality training data. Groundwater flow with heat transport is challenging to simulate numerically due to high spatio-temporal resolution requirements and large domains.

**方法:** Local-Global Convolutional Neural Network (LGCNN) combining a lightweight numerical surrogate for the transport process (global) with convolutional neural networks for the groundwater velocity and heat diffusion processes (local).

**结果:** The LGCNN model was systematically analyzed based on random subsurface input fields and trained on a handful of cut-outs from a real-world subsurface map, scaling to larger cut-outs without retraining.

**结论:** The LGCNN approach effectively models the city-wide subsurface temperature field and can scale to larger cut-outs without retraining.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Few-Shot+Learning+by+Explicit+Physics+Integration%3A+An+Application+to+Groundwater+Heat+Transport，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06062，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06062&send_immediately=true&force_search=false)

**原文摘要:** Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [62] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel, Ming-Jay Yang, Younes Bouhadjar, Maxime Fabre, Emre Neftci, John Paul Strachan*

**主要类别:** cs.LG

**AI概要:** 本文探讨了量化感知训练（QAT）对结构化状态空间模型（SSM）在资源受限的边缘计算设备上的影响，并展示了其在模拟内存计算芯片上的部署优势。


<details>
  <summary>更多</summary>
  
**动机:** 由于SSM在处理长序列时具有恒定的内存占用，使其成为在资源受限的边缘计算设备上部署的理想选择。然而，先前的工作没有解决其对专用边缘硬件（例如模拟内存计算芯片）的影响。

**方法:** 研究了量化感知训练（QAT）对SSM的影响，分析了模型大小与数值精度之间的关系，并展示了QAT如何增强对模拟噪声的鲁棒性和实现结构化剪枝。

**结果:** QAT可以将SSM的复杂性降低多达两个数量级，并增强了对模拟噪声的鲁棒性，同时实现了结构化剪枝。

**结论:** QAT 可以显著降低 SSM 的复杂性，并增强其对模拟噪声的鲁棒性，同时实现结构化剪枝。将这些技术集成到忆阻模拟内存计算基板上部署 SSMs 可提高计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QS4D%3A+Quantization-aware+training+for+efficient+hardware+deployment+of+structured+state-space+sequential+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06079&send_immediately=true&force_search=false)

**原文摘要:** Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [63] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li, Sikai Bai, Jie Zhang, Song Guo*

**主要类别:** cs.LG

**AI概要:** This paper proposes CoRE-Eval, a training-free, label-free self-evaluation framework for large reasoning models (LRMs) to address overthinking by detecting cyclical fluctuations in their Chain-of-Reasoning Embedding (CoRE) trajectories and dynamically determine whether to terminate reasoning early.


<details>
  <summary>更多</summary>
  
**动机:** Large reasoning models (LRMs) have demonstrated impressive capabilities but often exhibit overthinking -- excessive and redundant reasoning steps that introduce inefficiencies during inference. The important question is how can a model autonomously assess the correctness of its own reasoning trajectory without external labels.

**方法:** Chain-of-Reasoning Embedding (CoRE), a series of hidden states in latent space to enable label-free self-evaluation on intermediate reasoning steps of LRMs, so as to enhance metacognition abilities for improved reasoning efficiency.

**结果:** Extensive experiments on mathematical reasoning benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2% while improving answer accuracy by around 10%, achieving 70.0% accuracy on the challenging AIME benchmark with the 32B model.

**结论:** CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2% while improving answer accuracy by around 10%, achieving 70.0% accuracy on the challenging AIME benchmark with the 32B model.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoRE%3A+Enhancing+Metacognition+with+Label-free+Self-evaluation+in+LRMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06087&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [64] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh, Maxime Wabartha, Stanley Wu, Joelle Pineau, Hsiu-Chin Lin*

**主要类别:** cs.LG

**AI概要:** This paper proposes Uncertainty-Aware RL (UARL), a novel framework that prioritizes safety during training by addressing Out-Of-Distribution (OOD) detection and policy adaptation without requiring direct interactions in target domain.


<details>
  <summary>更多</summary>
  
**动机:** Deploying reinforcement learning policies in real-world involves significant challenges, including distribution shifts, safety concerns, and the impracticality of direct interactions during policy refinement.

**方法:** UARL employs an ensemble of critics to quantify policy uncertainty and incorporates progressive environmental randomization.

**结果:** UARL demonstrates its effectiveness in reliable OOD detection, improved performance, and enhanced sample efficiency compared to baselines.

**结论:** UARL enhances robust generalization to the target domain without explicitly training on it.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safe+Domain+Randomization+via+Uncertainty-Aware+Out-of-Distribution+Detection+and+Policy+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06111，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06111&send_immediately=true&force_search=false)

**原文摘要:** Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [65] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim, Sungjae Lee, Wonjin Lee, Kwang In Kim*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的零阶优化方法，该方法通过关注随机选择的二维子空间来减少估算Hessian矩阵的成本，并在优化步骤之间重用函数评估以进一步降低成本。实验结果表明，与现有的零阶方法相比，该方法实现了显著更快的收敛。


<details>
  <summary>更多</summary>
  
**动机:** 在无法访问或不切实际计算梯度信息的问题中，零阶优化解决了这些问题。虽然大多数现有方法依赖于一阶近似，但结合二阶（曲率）信息可以在原则上显著加速收敛。然而，估算Hessian矩阵所需的高昂功能评估成本通常限制了其实际应用性。

**方法:** 我们提出了基于子空间的近似Hessian（ZO-SAH）方法，这是一种零阶优化算法，通过关注随机选择的二维子空间来减少这些成本。在每个子空间内，ZO-SAH通过将二次多项式拟合到目标函数并提取其二阶系数来估计Hessian。为了进一步降低函数查询成本，ZO-SAH采用了一种周期性的子空间切换策略，在优化步骤之间重用函数评估。

**结果:** 对包括逻辑回归和深度神经网络训练任务在内的八个基准数据集的实验表明，与现有的零阶方法相比，ZO-SAH实现了显著更快的收敛。

**结论:** ZO-SAH在零阶优化方法中实现了显著更快的收敛。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Subspace-based+Approximate+Hessian+Method+for+Zeroth-Order+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06125&send_immediately=true&force_search=false)

**原文摘要:** Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [66] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron, Maksim E. Eren, Valentin Stanev, Cynthia Matuszek, Boian S. Alexandrov*

**主要类别:** cs.LG

**AI概要:** An AI-driven hierarchical link prediction framework integrates matrix factorization techniques to infer hidden associations and suggest novel hypotheses in complex material domains.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the challenge of missing or weakly connected links in large, sparse, and noisy scientific literature networks and knowledge graphs, specifically for materials studied across various physics fields.

**方法:** The presented method combines Hierarchical Nonnegative Matrix Factorization (HNMFk), Boolean matrix factorization (BNMFk), and Logistic matrix factorization (LMF) with automatic model selection to construct a three-level topic tree from a large corpus focused on transition-metal dichalcogenides (TMDs).

**结果:** The ensemble BNMFk + LMF approach fuses interpretability with probabilistic scoring, mapping materials onto coherent topics and highlighting missing or weakly connected links, suggesting novel hypotheses for cross-disciplinary exploration.

**结论:** The AI-driven hierarchical link prediction framework successfully infers hidden associations in complex material domains, particularly useful for scientific literature networks and knowledge graphs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Topic+Modeling+and+Link-Prediction+for+Material+Property+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06139&send_immediately=true&force_search=false)

**原文摘要:** Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [67] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider, Vincent Lostanlen, Martin Ehler, Nicki Holighaus, Peter Balazs*

**主要类别:** cs.LG

**AI概要:** This article analyzes the aliasing effect caused by strides in convolutional layers and its impact on stability, proposing methods to improve Parseval stability.


<details>
  <summary>更多</summary>
  
**动机:** To analyze the implications of aliasing introduced by strides in convolutional layers on numerical stability and statistical generalization.

**方法:** The study adapts a frame-theoretic approach to describe aliasing in convolutional layers with 1D kernels. It derives two optimization objectives that promote Parseval stability by suppressing aliasing systematically.

**结果:** Two computationally efficient optimization objectives are derived to promote Parseval stability. Closed-form expressions for expected value and variance of aliasing effects in layers with random kernels are provided.

**结论:** The study provides new insights into the aliasing behavior at initialization for convolutional layers and proposes methods to promote Parseval stability.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aliasing+in+Convnets%3A+A+Frame-Theoretic+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06152，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06152&send_immediately=true&force_search=false)

**原文摘要:** Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [68] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen, Lars Keuninckx, Matthias Hartmann, Piet Wambacq*

**主要类别:** cs.LG

**AI概要:** A novel method for partial optimization of the connections in Deep Differentiable Logic Gate Networks is introduced, showing superior performance on several benchmarks with fewer logic gates.


<details>
  <summary>更多</summary>
  
**动机:** To introduce a novel method for partial optimization of the connections in Deep Differentiable Logic Gate Networks (LGNs).

**方法:** The training method utilizes a probability distribution over a subset of connections per gate input, selecting the connection with highest merit, after which the gate-types are selected.

**结果:** Connection-optimized LGNs outperform standard fixed-connection LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only a fraction of the number of logic gates. 8000 simple logic gates are sufficient to achieve over 98% on the MNIST data set. The network has 24 times fewer gates, while performing better on the MNIST data set compared to standard fully connected LGNs.

**结论:** The work shows a pathway towards fully trainable Boolean logic.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Method+for+Optimizing+Connections+in+Differentiable+Logic+Gate+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06173&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [69] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider, Itamar Zimerman, Eliya Nachmani*

**主要类别:** cs.LG

**AI概要:** Applying differential design techniques from Transformers to Mamba requires careful adaptation but results in improved performance and reduced overallocation of attention.


<details>
  <summary>更多</summary>
  
**动机:** To explore whether differential design techniques originally developed for Transformers can be applied to Mamba and address the overallocation of attention problem in Mamba-based models.

**方法:** The paper introduces a novel differential mechanism for Mamba, with careful architectural modifications and empirical validation on language modeling benchmarks.

**结果:** The new mechanism for Mamba demonstrates improved retrieval capabilities and superior performance over vanilla Mamba.

**结论:** The novel differential mechanism for Mamba improves its performance and mitigates the overallocation problem, which is validated by extensive ablation studies and empirical analyses.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Differential+Mamba，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06204&send_immediately=true&force_search=false)

**原文摘要:** Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [70] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov, Benjamin Hoover, Parikshit Ram, Bao Pham*

**主要类别:** cs.LG

**AI概要:** This tutorial introduces Associative Memories, their theoretical results, and their relationship with SOTA AI architectures.


<details>
  <summary>更多</summary>
  
**动机:** To provide an approachable introduction to Associative Memories and their connections with SOTA AI architectures.

**方法:** The tutorial introduces Associative Memories using modern language and methods, with practical mathematical derivations and coding notebooks.

**结果:** Participants will gain a deeper understanding of Associative Memories and their potential applications in AI.

**结论:** Associative Memories provide a powerful framework for understanding and designing AI architectures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modern+Methods+in+Associative+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06211&send_immediately=true&force_search=false)

**原文摘要:** Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [71] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis, Victoria E. Galanopoulou, Panagiotis D. Diamantoulakis, Zhiguo Ding, Octavia Dobre*

**主要类别:** cs.LG

**AI概要:** 研究了如何最优选择波导中固定位置的挤压天线进行激活以最大化用户终端通信速率的问题，并提出使用神经网络架构解决该组合分数0-1二次规划问题的方法。


<details>
  <summary>更多</summary>
  
**动机:** 无线通信系统的发展需要灵活、能效高且成本效益好的天线技术。挤压天线（PAs）因其可以通过二进制激活状态动态控制电磁波传播而成为有前景的候选方案。

**方法:** 使用了不同复杂度的神经网络架构来直接从数据中学习激活策略，并利用空间特征和信号结构。同时，在训练和评估流程中加入了用户位置的不确定性，以模拟现实部署条件。

**结果:** 仿真结果表明，所提出的模型在解决复杂天线激活、波导引起的相移和功率分配问题上是有效和稳健的。

**结论:** 神经网络架构在解决组合分数0-1二次规划问题上表现出有效性和鲁棒性，可以优化选择固定位置的挤压天线激活子集以最大化用户终端通信速率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Learning+Optimization+of+Two-State+Pinching+Antennas+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06222，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06222&send_immediately=true&force_search=false)

**原文摘要:** The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [72] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

**主要类别:** cs.AI

**AI概要:** This paper presents an efficient implementation of a symbolic search method that allows for the creation of a large look-up table for Connect-Four, previously thought to be infeasible.


<details>
  <summary>更多</summary>
  
**动机:** Previously, a strong solution in the form of a look-up table for Connect-Four was believed to be infeasible.

**方法:** Symbolic search method based on binary decision diagrams and alpha-beta search

**结果:** Produced an 89.6 GB large look-up table for the standard $7 	imes 6$ board size in 47 hours on a single CPU core with 128 GB main memory.

**结论:** A large lookup table for Connect-Four is now feasible with an efficient implementation using symbolic search methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strongly+Solving+%247+%5Ctimes+6%24+Connect-Four+on+Consumer+Grade+Hardware，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05267&send_immediately=true&force_search=false)

**原文摘要:** While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [73] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang, Miao Zhou, Guijing Huang, Rui Zhuo, Chao Yi, Zhenliang Ma*

**主要类别:** cs.AI

**AI概要:** 该研究提出了Chat2SPaT方法，将用户对信号控制计划的模糊描述转换为确切的信号相位和定时结果，从而简化了交通信号控制计划的管理过程。实验表明其在英语和汉语案例中的准确率超过94%。


<details>
  <summary>更多</summary>
  
**动机:** 预定时交通信号控制需要繁琐的手动工作来创建和更新信号计划，当使用不同的时段或星期计划时，一个交叉口通常与多个计划相关联，导致进一步的重复手动参数输入。为了实现用户友好的交通信号控制计划管理流程，进行了此项研究。

**方法:** 提出了一种将用户对信号控制计划的半结构化和模糊描述转换为确切的信号相位和定时结果的方法，这些结果可以进一步转化为基于阶段或基于环的结构化计划，以与智能交通系统软件和交通信号控制器交互。

**结果:** 实验表明，Chat2SPaT在英语和汉语案例中的计划生成准确率超过94%。

**结论:** Chat2SPaT为交通从业者和研究人员提供了一个易于使用的计划管理流程，可作为智能交通系统领域中更准确、更多功能的大型语言模型应用的潜在新构建块。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chat2SPaT%3A+A+Large+Language+Model+Based+Tool+for+Automating+Traffic+Signal+Control+Plan+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05283&send_immediately=true&force_search=false)

**原文摘要:** Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [74] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

**主要类别:** cs.AI

**AI概要:** This paper proves that any optimal fuzzy classification aggregation function must be a weighted arithmetic mean.


<details>
  <summary>更多</summary>
  
**动机:** The study aims to understand the properties of optimal fuzzy classification aggregation functions when dealing with a continuum of individual classifications.

**方法:** The authors used mathematical proof to demonstrate the characteristics of the fuzzy classification aggregation function.

**结果:** The result shows that the optimal fuzzy classification aggregation function is a weighted arithmetic mean.

**结论:** Any optimal, independent, and zero unanimous fuzzy classification aggregation function must be a weighted arithmetic mean.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fuzzy+Classification+Aggregation+for+a+Continuum+of+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05297，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05297&send_immediately=true&force_search=false)

**原文摘要:** We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [75] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta, Jon Stephens, Amarnath Gupta*

**主要类别:** cs.AI

**AI概要:** OLG++ is an improved system for understanding legal obligations.


<details>
  <summary>更多</summary>
  
**动机:** To create a semantic extension of the Obligation Logic Graph (OLG) for modeling regulatory and legal rules in municipal and interjurisdictional contexts.

**方法:** OLG++ introduces richer node and edge types, including spatial, temporal, party group, defeasibility, and logical grouping constructs. The model supports structured reasoning over rules with contextual conditions, precedence, and complex triggers.

**结果:** Demonstrates expressiveness through examples from food business regulations, showing how OLG++ supports legal question answering using property graph queries. OLG++ also improves over LegalRuleML by providing native support for subClassOf, spatial constraints, and reified exception structures.

**结论:** OLG++ is more expressive than prior graph-based models for legal knowledge representation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OLG%2B%2B%3A+A+Semantic+Extension+of+Obligation+Logic+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05488，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05488&send_immediately=true&force_search=false)

**原文摘要:** We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [76] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan, Jiahe Jin, Zhihan Zhang, Tevin Wang, Andy Tang, Lucy Mo, Morteza Ziyadi, Leonardo F. R. Ribeiro, Zimeng Qiu, Markus Dreyer, Akari Asai, Chenyan Xiong*

**主要类别:** cs.AI

**AI概要:** Deep Research Comparator平台为深度研究代理的托管、并列比较、细粒度的人类反馈收集和排名计算提供了一个整体框架。开发了端到端代理脚手架Simple Deepresearch，有助于各种大型语言模型的轻松整合，将其转化为用于评估的深度研究代理。


<details>
  <summary>更多</summary>
  
**动机:** Evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge. There is a need for assessing long reports and giving detailed feedback on their intermediate steps.

**方法:** Introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation.

**结果:** Developed Simple Deepresearch, an end-to-end agent scaffold that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation.

**结论:** The platform's utility for deep research agent development is demonstrated through collected real user preference data.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Research+Comparator%3A+A+Platform+For+Fine-grained+Human+Annotations+of+Deep+Research+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05495，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05495&send_immediately=true&force_search=false)

**原文摘要:** Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [77] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang, Jiahuan Pei, Mohammad Aliannejadi, Xin Sun, Moonisa Ahsan, Pablo Cesar, Chuang Yu, Zhaochun Ren, Junxiao Wang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个专为AR训练定制的综合性数据集，并评估了九个最先进的VLMs。研究发现，即使是高级模型在细粒度装配任务上也有困难，强调了改进细粒度视觉-语言对齐的需求。


<details>
  <summary>更多</summary>
  
**动机:** 视觉-语言模型（VLMs）对于使人工智能驱动的智能助手在多模态环境中进行解释和推理至关重要。然而，它们在增强现实（AR）训练中的应用仍然基本上未被探索。

**方法:** 引入了一个专门为AR训练定制的综合性数据集，并评估了九个最先进的VLMs。

**结果:** 即使是最先进的模型，包括GPT-4o，在细粒度装配任务上也表现不佳，状态检测的最大F1分数仅为40.54%。

**结论:** 需要增强的数据集、基准和进一步研究以改善细粒度的视觉-语言对齐。我们的工作具有更广泛的社会影响，特别是在使盲人和视力受损用户能够平等地获得人工智能驱动的学习机会方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Grained+Vision-Language+Modeling+for+Multimodal+Training+Assistants+in+Augmented+Reality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05515&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [78] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta, Abhiramon Rajasekharan, Alexis R. Tudor, Elmer Salazar, Joaquín Arias*

**主要类别:** cs.AI

**AI概要:** This paper presents a method for implementing deontic modal logic using answer set programming.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the problem of implementing deontic modal logic and resolving the paradoxes within it.

**方法:** The paper proposes using global constraints of ASP to represent obligations and impermissibilities of deontic modal logic.

**结果:** The various paradoxes of deontic modal logic are elegantly resolved.

**结论:** The proposed representation using global constraints of ASP resolves the paradoxes of deontic modal logic.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+%28Deontic%29+Modal+Operators+With+the+s%28CASP%29+Goal-directed+Predicated+Answer+Set+Programming+System，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05519，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05519&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [79] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar*

**主要类别:** cs.AI

**AI概要:** 此论文参与了2025 ImageCLEF MEDIQA-MAGIC挑战赛，并提出一种针对闭合视觉问答任务的新方法，通过结合微调多模态模型、结构化推理层以及代理检索增强生成技术，实现了对皮肤科问题的准确回答。


<details>
  <summary>更多</summary>
  
**动机:** 本研究旨在应对远程医疗中的实际挑战：必须经常在有限的输入下异步作出高准确性与可解释性的诊断决策。

**方法:** 提出的方法结合了三个核心组件：（1）在竞赛数据集上微调来自Qwen、Gemma和LLaMA家族的开源多模态模型；（2）引入一个结构化推理层，协调并裁定候选模型输出之间的矛盾；（3）整合代理检索增强生成（agentic RAG），从美国皮肤病学会的症状和情况数据库中添加相关信息以填补患者背景中的空白。

**结果:** 团队提交的结果在评分中排名第六，最终获得第二名，表现出了具有竞争力的性能和高度的准确性。

**结论:** 该研究提供了一个可靠的自动化诊断支持系统的发展路径，通过模拟皮肤科医生评估皮肤状况时使用的系统推理模式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cultivating+Multimodal+Intelligence%3A+Interpretive+Reasoning+and+Agentic+RAG+Approaches+to+Dermatological+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05520，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05520&send_immediately=true&force_search=false)

**原文摘要:** The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [80] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei, Fanghua Ye, Xin Sun, Wentao Deng, Koen Hindriks, Junxiao Wang*

**主要类别:** cs.AI

**AI概要:** This paper proposes WikiHowAgent, a multi-agent workflow simulating teaching-learning conversations using LLMs.


<details>
  <summary>更多</summary>
  
**动机:** Existing work in the field of AI4Education often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality.

**方法:** The paper proposes WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations.

**结果:** Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains.

**结论:** The proposed WikiHowAgent is effective in diverse setups and offers valuable insights into LLM capabilities across domains.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conversational+Education+at+Scale%3A+A+Multi-LLM+Agent+Workflow+for+Procedural+Learning+and+Pedagogic+Quality+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05528，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05528&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [81] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar, Brian Pendleton, Abhishek Gupta*

**主要类别:** cs.AI

**AI概要:** This paper critically examines AI red teaming, highlighting the gap between its original intent as a critical thinking exercise and its current narrow focus on model-level flaws. It proposes a framework for operationalizing red teaming at macro and micro levels to address this.


<details>
  <summary>更多</summary>
  
**动机:** Despite its popularity in AI governance, there exists a significant gap between red teaming's original intent as a critical thinking exercise and its narrow focus on discovering model-level flaws in the context of generative AI.

**方法:** The paper proposes a comprehensive framework operationalizing red teaming in AI systems at two levels: macro-level system red teaming spanning the entire AI development lifecycle, and micro-level model red teaming.

**结果:** The current focus of AI red teaming predominantly on individual model vulnerabilities overlooks broader sociotechnical systems and emergent behaviors.

**结论:** Effective AI red teaming requires multifunctional teams to examine emergent risks, systemic vulnerabilities, and the interplay between technical and social factors.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Red+Teaming+AI+Red+Teaming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05538，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05538&send_immediately=true&force_search=false)

**原文摘要:** Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [82] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma, Asiful Arefeen, Stephanie M. Carpenter, Melanie Hingle, Hassan Ghasemzadeh*

**主要类别:** cs.AI

**AI概要:** 本文研究了使用大型语言模型生成反事实解释，并表明这种方法可以在临床和生理预测任务中提高解释性和鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 反事实解释通过强调改变结果所需的最小变化来提供机器学习预测的人类中心视角洞察。因此，反事实解释可以作为预防异常的干预措施和训练稳健模型的增强数据。

**方法:** 探讨了使用大型语言模型（LLMs），特别是GPT-4o-mini，在零样本和三样本设置中生成反事实解释的方法。并在两个数据集上评估了这种方法：用于压力预测的AI-Readi旗舰数据集和用于心脏病检测的公共数据集。

**结果:** 与传统方法如DiCE、CFNOW和NICE相比，基于少量示例的大型语言模型方法具有高可信度（高达99%）、强有效性（高达0.99）以及竞争力的稀疏性。此外，使用大型语言模型生成的反事实解释作为增强样本可以提高下游分类器的性能（平均准确率提高5%）。

**结论:** 使用大型语言模型生成的反事实解释可以提高下游分类器的性能，尤其在数据量较少的情况下。这表明基于提示的生成技术在临床和生理预测任务中的可解释性和鲁棒性有提升潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SenseCF%3A+LLM-Prompted+Counterfactuals+for+Intervention+and+Sensor+Data+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05541，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05541&send_immediately=true&force_search=false)

**原文摘要:** Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [83] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd, Noam Rotstein, Roy Velich, Daniel Bensaïd, Ron Kimmel*

**主要类别:** cs.AI

**AI概要:** SingLoRA is a new method for low-rank adaptation that removes inter-matrix scale conflicts and reduces the parameter count.


<details>
  <summary>更多</summary>
  
**动机:** Address the issue of scale disparities between matrices in LoRA that cause unstable training dynamics.

**方法:** SingLoRA reformulates low-rank adaptation by learning the weights update as a decomposition of a single low-rank matrix multiplied by its transpose.

**结果:** SingLoRA ensures stable optimization, roughly halves the parameter count, and achieves better performance in multiple tasks.

**结论:** SingLoRA surpasses LoRA and LoRA+ in performance while using fewer parameters.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SingLoRA%3A+Low+Rank+Adaptation+Using+a+Single+Matrix，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05566&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [84] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

**主要类别:** cs.AI

**AI概要:** This paper motivates and outlines a programme for a formal theory of measurement of artificial intelligence, arguing that it will allow for better comparisons, connections with risk analysis techniques, and understanding of AI capabilities.


<details>
  <summary>更多</summary>
  
**动机:** The authors aim to motivate the need for a formal theory of measurement for AI, arguing that it will allow for comparisons between systems, connection with established risk analysis techniques, and foregrounding the contingency of AI capability on measurement operations.

**方法:** The authors outline a programme for a formal theory of measurement of artificial intelligence and sketch a layered measurement stack.

**结果:** The paper sketches a layered measurement stack, distinguishes direct from indirect observables, and signposts a pathway towards a unified taxonomy of AI phenomena.

**结论:** A formal theory of measurement for AI is essential and a layered measurement stack provides a pathway towards a unified taxonomy of AI phenomena.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Measurement+Theory+for+Artificial+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05587，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05587&send_immediately=true&force_search=false)

**原文摘要:** We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [85] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang, Juan Chen, En Zhu, Wenhong Cheng, YunPeng Li, Yanbo J. Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的多模态大语言模型（MLlm-DR）以解决抑郁诊断的问题。


<details>
  <summary>更多</summary>
  
**动机:** 以前的研究往往缺乏对这些分数如何确定的清晰解释，限制了它们在临床实践中的采用。虽然LLMs的出现为可解释的抑郁诊断提供了一条可能的路径，但目前能够处理多模态数据的LLMs缺乏对访谈数据的训练，导致直接使用时诊断性能不佳。

**方法:** 提出了一种新的多模态大语言模型（MLlm-DR），该模型可以理解多模态信息输入，并支持可解释的抑郁诊断。MLlm-DR集成了一个较小的LLMs和一个轻量级查询模块（LQ-former）。

**结果:** 所提出的方法在两个基于访谈的基准数据集CMDC和E-DAIC-WOZ上达到了最先进的结果。

**结论:** MLlm-DR在两个基于访谈的基准数据集上取得了最先进的结果，证明了其有效性和优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MLlm-DR%3A+Towards+Explainable+Depression+Recognition+with+MultiModal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05591，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05591&send_immediately=true&force_search=false)

**原文摘要:** Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [86] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan, Fangxue Liu, Cheng Chen*

**主要类别:** cs.AI

**AI概要:** This paper provides an overview of the current state of large language models (LLMs) in geotechnical engineering, highlighting the advantages and challenges associated with their use.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to present the first survey of the adaptation and application of LLMs in geotechnical engineering, which is increasingly being used to streamline geotechnical workflows.

**方法:** This paper outlines key methodologies for adaptation to geotechnical domain, including prompt engineering, retrieval-augmented generation, domain-adaptive pretraining, and fine-tuning.

**结果:** The survey examines the state-of-the-art applications of geotechnical-adapted LLMs, including geological interpretation, subsurface characterization, site planning, design calculations, numerical modeling, safety and risk assessment, and educational tutoring. It also analyzes benefits and limitations of geotechnical-adapted LLMs, and identifies promising directions for future research in this interdisciplinary discipline.

**结论:** The findings of this paper serve as a valuable resource for practitioners seeking to integrate LLMs into geotechnical practice, while also providing a foundation to stimulate further investigation within the academic community.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Domain+adaptation+of+large+language+models+for+geotechnical+applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05613&send_immediately=true&force_search=false)

**原文摘要:** Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [87] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang, Juan Chen, Yanbo J. Wang, En Zhu, Xuan Yang, Yiduo Wang*

**主要类别:** cs.AI

**AI概要:** This paper presents ADMC, a new method for handling missing modalities in multimodal emotion and intent recognition, which outperforms existing approaches.


<details>
  <summary>更多</summary>
  
**动机:** Missing modalities due to sensor malfunctions or incomplete data pose significant challenges in multimodal emotion and intent recognition, prompting the need for a solution that avoids over-coupling and imprecise generation processes.

**方法:** The paper introduces an Attention-based Diffusion model for Missing Modalities feature Completion (ADMC) with independently trained feature extraction networks for each modality and an Attention-based Diffusion Network (ADN) that generates missing modality features.

**结果:** The proposed ADMC approach achieves state-of-the-art results on the IEMOCAP and MIntRec benchmarks, showing improved performance across all missing-modality scenarios and even in full-modality contexts.

**结论:** The Attention-based Diffusion model for Missing Modalities feature Completion (ADMC) is effective in both missing and complete modality scenarios, achieving state-of-the-art results on the IEMOCAP and MIntRec benchmarks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADMC%3A+Attention-based+Diffusion+Model+for+Missing+Modalities+Feature+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05624&send_immediately=true&force_search=false)

**原文摘要:** Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [88] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An, John Liu, Niyam Acharya, Ruhma Hashmi*

**主要类别:** cs.AI

**AI概要:** This paper explores the effectiveness of using Large Language Models (LLMs) to generate retrieval practice questions for college-level data science courses. The results suggest that LLM-generated retrieval questions can effectively support student learning and may provide a scalable solution for integrating retrieval practice into real-time teaching.


<details>
  <summary>更多</summary>
  
**动机:** Retrieval practice is known to significantly enhance student learning and knowledge retention, but generating high-quality retrieval practice questions is often time-consuming and labor intensive for instructors, especially in rapidly evolving technical subjects.

**方法:** An empirical study involving two college-level data science courses, with approximately 60 students. Compare learning outcomes during one week in which students received LLM-generated multiple-choice retrieval practice questions to those from a week in which no such questions were provided.

**结果:** Students exposed to LLM-generated retrieval practice achieved significantly higher knowledge retention, with an average accuracy of 89%, compared to 73% in the week without such practice.

**结论:** LLM-generated retrieval questions can effectively support student learning, but the quality of LLM-generated questions can vary, so instructors must manually verify and revise the generated questions before releasing them to students.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Student+Learning+with+LLM-Generated+Retrieval+Practice+Questions%3A+An+Empirical+Study+in+Data+Science+Courses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05629&send_immediately=true&force_search=false)

**原文摘要:** Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [89] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang, Xiaoming Zhang, Bingyu Yan, Ziyi Zhou, Bo Zhang, Zhenyu Guan, Xi Zhang, Chaozhuo Li*

**主要类别:** cs.AI

**AI概要:** 本文探讨了利用大型语言模型（LLMs）模拟信息传播的心理方面，并提出了一种新的SIP-CoT机制以增强其社会智能和现实性。


<details>
  <summary>更多</summary>
  
**动机:** 社交媒体和生成式AI的指数增长改变了信息传播方式，但传统模型和高级方法都无法完全捕捉在线互动的复杂性以及用户心理和行为动态。大型语言模型（LLMs）提供了模拟信息传播心理方面的潜力。

**方法:** 提出了一种基于社会信息处理的情感引导记忆链（SIP-CoT）机制。

**结果:** 实验结果证实，采用SIP-CoT增强的LLM代理能够更有效地处理社会信息，表现出接近真实人类互动的行为、态度和情绪。

**结论:** 通过整合SIP-CoT和情感记忆，大大增强了LLM代理的社会智能和现实性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLMs+are+Introvert，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05638，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05638&send_immediately=true&force_search=false)

**原文摘要:** The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [90] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu, Lizhe Cao, Shuang Wang, Jiming Wang, Shutong Zhu, Yerong Wu, Yuqing Feng*

**主要类别:** cs.AI

**AI概要:** This paper presents TLJD, a new method using judicial data for more reliable city-level FDI predictions.


<details>
  <summary>更多</summary>
  
**动机:** Foreign direct investment (FDI) is crucial for economic growth and innovation, but common economic data for its prediction can be manipulated. Therefore, leveraging large-scale judicial data for city-level FDI prediction is explored.

**方法:** TLJD integrates row data and column data in the tabular dataset for judicial performance indicator encoding, utilizing a mixture of experts model to adjust weights of indicators considering regional variations.

**结果:** Experiments on cross-city and cross-time tasks show TLJD's superiority, achieving at least 0.92 R2 score over ten baselines.

**结论:** The proposed TLJD method excels in city-level FDI prediction, outperforming other state-of-the-art baselines with at least 0.92 R2 score in different evaluation metrics.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是City-Level+Foreign+Direct+Investment+Prediction+with+Tabular+Learning+on+Judicial+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05651&send_immediately=true&force_search=false)

**原文摘要:** To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [91] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta, Saumya Panda*

**主要类别:** cs.AI

**AI概要:** 本研究比较了人类专家和两种AI模型在复杂皮肤病案例中的治疗计划，并发现人类和AI评委之间存在显著的评估差异，突显了AI整合的挑战和未来人机协作系统的必要性。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI从诊断扩展到其他领域，特别是新的推理模型，评估AI生成的治疗计划成为一个关键挑战。

**方法:** 十名皮肤科医生、一个通才AI（GPT-4o）和一个推理AI（o3）为五个复杂的皮肤病案例生成了治疗计划。匿名化、标准化的计划在两个阶段得分：1）由十位人类专家评分，和2）由一个更高级的AI评委（Gemini 2.5 Pro）使用相同的评分标准进行评分。

**结果:** 观察到了显著的‘评估者效应’。人类专家对同行生成的计划打分显著高于AI计划（平均7.62 vs. 7.16；p=0.0313），GPT-4o排名第6（平均7.38），而推理模型o3排名第11（平均6.97）。相反，AI评委则完全反转，对AI计划的评分显著高于人类计划（平均7.75 vs. 6.79；p=0.0313）。它将o3排在第1位（平均8.20），GPT-4o排在第2位，所有人类专家排名较低。

**结论:** 临床计划的感知质量从根本上取决于评估者的性质。一个被人类专家排名较低的高级推理AI，被一个复杂的AI判定为优越，揭示了基于经验的临床启发式和技术驱动的算法逻辑之间的深刻差距。这一悖论对AI整合提出了重要挑战，表明未来需要协同、可解释的人机系统来弥合这一推理差距，以增强临床护理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Divergent+Realities%3A+A+Comparative+Analysis+of+Human+Expert+vs.+Artificial+Intelligence+Based+Generation+and+Evaluation+of+Treatment+Plans+in+Dermatology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05716&send_immediately=true&force_search=false)

**原文摘要:** Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [92] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn, Florian Buettner*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为ModelAuditor的自我反思代理，可以与用户交流，选择特定任务的度量标准，并模拟依赖于上下文、临床相关的分布变化，从而生成可解释的报告，说明在部署期间性能可能下降的程度，讨论特定的可能失败模式并确定根本原因和缓解策略。


<details>
  <summary>更多</summary>
  
**动机:** 目前，在部署前进行可靠性审计以识别此类灾难性故障案例是一个定制且耗时的过程；从业者缺乏可访问且可解释的工具来暴露和修复隐藏的故障模式。

**方法:** ModelAuditor采用自我反思代理与用户对话、选择任务特定指标，并模拟依赖于上下文、临床相关的分布变化。

**结果:** ModelAuditor能够在三个真实世界的临床场景中正确识别最先进的模型的特定环境下的失败模式，并通过有针对性的建议恢复了在真实世界分布变化下损失的15-25%的性能。

**结论:** ModelAuditor通过多智能体架构实现了对真实世界分布变化下性能下降的识别和恢复，同时在消费级硬件上执行时间少于10分钟，每次审计成本不到0.5美元。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+autonomous+agent+for+auditing+and+improving+the+reliability+of+clinical+AI+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05755&send_immediately=true&force_search=false)

**原文摘要:** The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [93] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes, Edgar Hernando Sepúlveda-Oviedo, Corinne Alonso*

**主要类别:** cs.AI

**AI概要:** This paper proposes a new method for real-time monitoring of battery SoH, which is critical in microgrid settings. The approach uses a discharge pulse at the end of charging to estimate SoH. Initial results indicate successful prediction of battery degradation with low error and high explainability.


<details>
  <summary>更多</summary>
  
**动机:** Real-time monitoring of the state of health (SoH) of batteries remains a major challenge, particularly in microgrids where operational constraints limit the use of traditional methods.

**方法:** an innovative method based on the analysis of a discharge pulse at the end of the charge phase

**结果:** After training using the parameters of two batteries with a capacity degradation of around 85%, we successfully predicted the degradation of two other batteries, cycled down to approximately 90% SoH, with a mean absolute error of around 1% in the worst case, and an explainability score of the estimator close to 0.9.

**结论:** If these performances are confirmed, this method can be easily integrated into battery management systems (BMS) and paves the way for optimized battery management under continuous operation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Real-time+monitoring+of+the+SoH+of+lithium-ion+batteries，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05765&send_immediately=true&force_search=false)

**原文摘要:** Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [94] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, Junnan Li*

**主要类别:** cs.AI

**AI概要:** This paper addresses challenges in task planning and visual grounding for GUI agents through the introduction of GTA1, which uses a test-time scaling method and reinforcement learning.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address two main challenges in GUI agents: resolving ambiguity in task planning and accurately grounding actions in complex interfaces.

**方法:** The paper introduces a test-time scaling method to select the most appropriate action proposal by sampling multiple candidates and leveraging a judge model. Additionally, it proposes a model that achieves improved accuracy when grounding the selected action proposal using reinforcement learning.

**结果:** GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G respectively. When paired with a planner applying the test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld).

**结论:** The proposed method, GTA1, establishes state-of-the-art performance across diverse benchmarks and exhibits state-of-the-art agentic performance when paired with a planner applying the test-time scaling strategy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GTA1%3A+GUI+Test-time+Scaling+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05791&send_immediately=true&force_search=false)

**原文摘要:** Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [95] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao, Yulin Zhang, Luwei Xiao, Xinyi Wu, Yanhao Jia, Zhongliang Guo, Xiaobao Wu, Cong-Duy Nguyen, Guoming Zhang, Anh Tuan Luu*

**主要类别:** cs.AI

**AI概要:** This paper introduces a novel Chinese benchmark dataset, CROP, and an automated evaluation framework, Affective-ROPTester, to examine the predictive capabilities and affective biases of LLMs in ROP risk stratification.


<details>
  <summary>更多</summary>
  
**动机:** To explore the capacity of large language models (LLMs) to predict retinopathy of prematurity (ROP) risk and examine their predictive capabilities and affective biases.

**方法:** The study proposes Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL).

**结果:** LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, but exhibit marked performance gains when augmented with structured external inputs. Affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Positive emotional framing contributes to mitigating predictive bias in model outputs.

**结论:** Affective-ROPTester can help mitigate affective bias in clinical language modeling systems and enhance diagnostic reliability.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Affective-ROPTester%3A+Capability+and+Bias+Analysis+of+LLMs+in+Predicting+Retinopathy+of+Prematurity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05816，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05816&send_immediately=true&force_search=false)

**原文摘要:** Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [96] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau, Éric Piette*

**主要类别:** cs.AI

**AI概要:** 虽然AI系统在各种游戏中表现出色，但他们仍然不能复制人类的决策过程。本文讨论了认知心理学的发现，以及在AI代理中模仿人类行为的尝试，并提出了一种新的工作模型：CogniPlay。


<details>
  <summary>更多</summary>
  
**动机:** 尽管AI系统在许多游戏中表现出与人类相当或超越人类的表现，但将这些系统描述为真正的“类人”仍然是牵强的。他们的成功并没有复制人类认知中观察到的基于模式的、直观的决策过程。

**方法:** 本文回顾了认知心理学的发现和以前在人工代理中模仿人类行为的努力，并基于这些观察提出了CogniPlay模型。

**结果:** CogniPlay模型已被证明在通用游戏播放（GGP）中有效地模仿人类行为。

**结论:** CogniPlay模型可能是实现更像人类的人工智能系统的关键。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CogniPlay%3A+a+work-in-progress+Human-like+model+for+General+Game+Playing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05868，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05868&send_immediately=true&force_search=false)

**原文摘要:** While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [97] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

**主要类别:** cs.AI

**AI概要:** Proposes Neurosymbolic Transition Systems to improve the construction of Automated Reasoning tools combining symbolic algorithms and Large Language Models.


<details>
  <summary>更多</summary>
  
**动机:** The current practice for constructing neurosymbolic AR systems lacks the strong guarantees of traditional symbolic algorithms and deep synchronization of neural networks and symbolic reasoning.

**方法:** Proposal of Neurosymbolic Transition Systems as a principled computational model for infrastructure in building neurosymbolic AR tools.

**结果:** A new paradigm that can scale logical reasoning while retaining the strong guarantees of symbolic algorithms, reified in a logic programming language.

**结论:** Neurosymbolic Transition Systems can serve as a computational model for building neurosymbolic AR tools with strong guarantees of symbolic algorithms.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Current+Practices+for+Building+LLM-Powered+Reasoning+Tools+Are+Ad+Hoc+--+and+We+Can+Do+Better，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05886，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05886&send_immediately=true&force_search=false)

**原文摘要:** There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [98] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich, Michael Stenger, André Bauer, Samuel Kounev*

**主要类别:** cs.AI

**AI概要:** Decomposing the time series forecasting process into three stages can achieve state-of-the-art accuracy and enhance computational efficiency.


<details>
  <summary>更多</summary>
  
**动机:** Time series forecasting remains challenging due to the need for effective sequence representation, memory construction, and accurate target projection. Each dataset and forecasting configuration constitutes a distinct task, posing unique challenges for models to produce accurate predictions.

**方法:** This work decomposes the time series forecasting pipeline into three core stages and investigates a range of architectural configurations to assess the effectiveness of various modules.

**结果:** The models achieve state-of-the-art forecasting accuracy with improved computational efficiency, reduced training and inference times, and a lower parameter count.

**结论:** This work concludes that by decomposing the time series forecasting pipeline into three core stages, state-of-the-art forecasting accuracy can be achieved while greatly enhancing computational efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decomposing+the+Time+Series+Forecasting+Pipeline%3A+A+Modular+Approach+for+Time+Series+Representation%2C+Information+Extraction%2C+and+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05891，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05891&send_immediately=true&force_search=false)

**原文摘要:** With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [99] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati, Xinyue Li, Yuxuan Wu, Gus Xia*

**主要类别:** cs.AI

**AI概要:** This paper explores whether a Music Language Model can perform a similar task called Music Scene Imagination (MSI) which requires cross-modal information from video and music to train.


<details>
  <summary>更多</summary>
  
**动机:** To explore whether a Music Language Model, e.g. MU-LLaMA, can perform a similar task called Music Scene Imagination (MSI) which requires cross-modal information from video and music to train.

**方法:** The authors construct a large-scale video-audio caption dataset with 3,371 pairs and finetune Music Understanding LLaMA for the MSI task to create MusiScene.

**结果:** Comprehensive evaluations prove that MusiScene is more capable of generating contextually relevant captions compared to MU-LLaMA.

**结论:** MusiScene is more capable of generating contextually relevant captions compared to MU-LLaMA and can enhance Video Background Music Generation from text.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MusiScene%3A+Leveraging+MU-LLaMA+for+Scene+Imagination+and+Enhanced+Video+Background+Music+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05894&send_immediately=true&force_search=false)

**原文摘要:** Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [100] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong, Boheng Chen, Chengzhi Wang, Daxiong Luo, Dongsheng Xu, Dongyang Liu, Fan Yang, Fangyuan Li, Fei Teng, Feng Wang, Fukang Qin, Fuquan Peng, Guanxin Tan, Guozhi Wang, Haibo Yu, Haohao Gao, Heng Liu, Hongbo Yang, Hongjian Zou, Houzheng Shen, Hu Meng, Huan Li, Hui Tan, Jiali Chen, Jianzhao Chen, Jinliang Zhu, Kai Wang, Lei Wu, Liangbing Liu, Liuyang Bian, Liyan He, Long Liu, Peiwen Li, Penggang Shi, Qi Ding, Rui Hu, Shuai Cao, Shuai Ren, Shuang Peng, Teng Xie, Weiji Chen, Weilin Xiang, Weixin Wu, Xi Yin, Xiaoxin Chen, Xu Chen, Yafei Wen, Yan Hu, Yanzhou Yang, Yina Xie, Yinghao Chen, Yixuan Liao, Yu Geng, Yuanjiang Ouyang, Yuanzhuo Yang, Yuehua He, Yushuai Peng, Zhaoxiong Wang, Zheng Wang, Zhibo Zhou, Ziyang Wu*

**主要类别:** cs.AI

**AI概要:** 本论文介绍了BlueLM-2.5-3B模型，这是一个为了高效边缘设备部署而设计的紧凑且统一的密集多模态大语言模型。


<details>
  <summary>更多</summary>
  
**动机:** 设计一个紧凑且统一的密集多模态大语言模型，用于高效边缘设备部署，提供强大的通用性和推理能力。

**方法:** 通过多样化的数据整理、关键数据重采样、混合异构强化学习和高性能训练基础设施来开发模型。

**结果:** BlueLM-2.5-3B在多模态容量方面表现出色，同时保持了具有竞争力的纯文本性能，而且在思考模式下与Qwen3-4B在纯文本基准测试中的表现相当，在非思考模式下在大多数多模态基准测试中优于Qwen2.5-VL-3B。

**结论:** BlueLM-2.5-3B为高性能、设备上的多模态大语言模型的发展做出了贡献，并为研究社区提供了有意义的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BlueLM-2.5-3B+Technical+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05934，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05934&send_immediately=true&force_search=false)

**原文摘要:** We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [101] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng, Jiacheng Wang, Xingyu Zhou, Le Liang, Hao Ye, Shi Jin, Geoffrey Ye Li*

**主要类别:** cs.AI

**AI概要:** A unified foundation model for multi-task prediction in wireless networks is proposed that supports diverse prediction intervals.


<details>
  <summary>更多</summary>
  
**动机:** Accurately predicting key system parameters, such as channel state information (CSI), user location, and network traffic, has become essential for a wide range of physical (PHY)-layer and medium access control (MAC)-layer tasks. Traditional deep learning (DL)-based methods often struggle to generalize across different scenarios and tasks.

**方法:** The proposed model enforces univariate decomposition to unify heterogeneous tasks, encodes granularity for interval awareness, and uses a causal Transformer backbone for accurate predictions. Additionally, a patch masking strategy is introduced during training to support arbitrary input lengths.

**结果:** After trained on large-scale datasets, the proposed foundation model demonstrates strong generalization to unseen scenarios and achieves zero-shot performance on new tasks that surpass traditional full-shot baselines.

**结论:** The proposed foundation model demonstrates strong generalization to unseen scenarios and achieves zero-shot performance on new tasks that surpass traditional full-shot baselines.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Wireless+Foundation+Model+for+Multi-Task+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05938，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05938&send_immediately=true&force_search=false)

**原文摘要:** With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [102] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico, Guido Bologna, Luca Coraci, Francesca Fracasso, Silvia Gola, Gabriella Cortellessa*

**主要类别:** cs.AI

**AI概要:** This paper proposes an attribution-based approach to improve the interpretability of Explainable AI-based predictions for arm lymphedema's risk assessment after lymph nodal radiotherapy in breast cancer.


<details>
  <summary>更多</summary>
  
**动机:** The lack of transparency of data-driven Artificial Intelligence techniques limits their interpretability and acceptance into healthcare decision-making processes.

**方法:** An attribution-based approach is proposed, which performs a statistical analysis of the attributes in the rule-based prediction model using standard metrics from Information Retrieval techniques.

**结果:** The user study comparing the output generated by the proposed approach with the raw output of the Explainable AI model suggested higher levels of interpretability and usefulness.

**结论:** The proposed attribution-based approach enhances the interpretability and usefulness of Explainable AI-based predictions for lymphedema risk assessment.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+the+Interpretability+of+Rule-based+Explanations+through+Information+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05976，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05976&send_immediately=true&force_search=false)

**原文摘要:** The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [103] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo, Alvina Lai, Julia Ive, Alexandru Petcu, Yutong Wang, Luyuan Qi, Johan H Thygesen, Kezhi Li*

**主要类别:** cs.AI

**AI概要:** This paper describes the development and evaluation of HopeBot, a chatbot powered by a large language model that administers the PHQ-9 for depression screening. The study found that people trusted the chatbot more than the traditional method and were willing to reuse or recommend it.


<details>
  <summary>更多</summary>
  
**动机:** Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively screen depression but lack interactivity and adaptability. The motivation is to develop a more interactive and adaptable tool for depression screening.

**方法:** Within-subject study with 132 adults in the United Kingdom and China completing both self-administered and chatbot versions of the PHQ-9.

**结果:** Scores demonstrated strong agreement (ICC = 0.91; 45% identical). Among 75 participants providing comparative feedback, 71% reported greater trust in the chatbot. Mean ratings (0-10) were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics, and 7.4 for recommendation helpfulness. Overall, 87.1% expressed willingness to reuse or recommend HopeBot.

**结论:** Voice-based LLM chatbots, like HopeBot, can feasibly serve as scalable, low-burden adjuncts for routine depression screening.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Development+and+Evaluation+of+HopeBot%3A+an+LLM-based+chatbot+for+structured+and+interactive+PHQ-9+depression+screening，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05984，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05984&send_immediately=true&force_search=false)

**原文摘要:** Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [104] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为CogniSQL-R1-Zero的新方法，用于解决将自然语言转换为SQL的问题，通过强化学习框架实现了准确SQL的生成，同时避免了传统方法的复杂性，取得了在Text-to-SQL任务上的最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型提高了自然语言到SQL（Text-to-SQL）转换的流畅性，但生成正确且可执行的SQL，尤其是对于复杂查询仍然具有挑战性。

**方法:** 研究引入了CogniSQL-R1-Zero，这是一种基于强化学习的框架和模型，能够基于执行正确性和格式标签合规性生成准确的SQL。该方法避免了中间监督、混合管道和复杂的奖励塑形，促进了稳定的学习和与最终任务目标的更强对齐。

**结果:** CogniSQL-R1-Zero在Text2SQL基准测试中取得了最先进的执行准确性，并且在训练资源消耗上表现出高效性和可扩展性。

**结论:** CogniSQL-R1-Zero框架和模型通过使用轻量级奖励信号，实现了准确的SQL生成，并在Text2SQL基准测试中取得了最先进的执行准确性。此外，研究人员还发布了两个策划数据集以支持进一步的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CogniSQL-R1-Zero%3A+Lightweight+Reinforced+Reasoning+for+Efficient+SQL+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06013，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06013&send_immediately=true&force_search=false)

**原文摘要:** Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [105] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford, Mark T. Keane*

**主要类别:** cs.AI

**AI概要:** This paper introduces FGNS, a method enhancing XAI interpretability for non-experts by selecting class-representative examples using local and global features. It improved error identification and decision-making in a user study.


<details>
  <summary>更多</summary>
  
**动机:** Explainable AI (XAI) methods often struggle to generate clear, interpretable outputs for users without domain expertise.

**方法:** Feature-Guided Neighbor Selection (FGNS), a post hoc method that enhances interpretability by selecting class-representative examples using both local and global feature importance.

**结果:** In a user study (N = 98) evaluating Kannada script classifications, FGNS significantly improved non-experts' ability to identify model errors while maintaining appropriate agreement with correct predictions. Participants made faster and more accurate decisions compared to those given traditional k-NN explanations.

**结论:** FGNS is a step toward more human-aligned model assessment, but further work is needed to address the gap between explanation quality and perceived trust.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature-Guided+Neighbor+Selection+for+Non-Expert+Evaluation+of+Model+Predictions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06029，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06029&send_immediately=true&force_search=false)

**原文摘要:** Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [106] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio, Lluis Godo, Ramón Pino Pérez, Lluis Subirana*

**主要类别:** cs.AI

**AI概要:** This paper addresses limitations of the Lockean thesis in belief change theory by providing characterizations of logically closed belief sets and proposing a method for minimal belief revision.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind the paper is to address the limitations of the Lockean thesis approach in contexts like belief change theory where belief sets are not generally closed under classical logical deduction.

**方法:** The paper uses the formal setting of the Lockean thesis and describes degrees of confidence in probabilistic terms. It analyzes the limitations of this approach, particularly in belief change theory, and aims to address these by providing characterizations of logically closed belief sets and proposing a method for minimal belief revision.

**结果:** Two characterizations of belief sets that are closed under classical logic deduction are provided. An approach for minimal revision of beliefs during probabilistic update is proposed, demonstrating how a belief set can be deductively closed via minimal revision.

**结论:** The paper provides two characterizations of belief sets that are closed under classical logic deduction and proposes an approach for minimal revision of beliefs in probabilistic update.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Lockean+beliefs+that+are+deductively+closed+and+minimal+change，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06042&send_immediately=true&force_search=false)

**原文摘要:** Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [107] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang*

**主要类别:** cs.AI

**AI概要:** Introducing FEVO, a multi-stage enhancement framework for large language models in the financial domain.


<details>
  <summary>更多</summary>
  
**动机:** Research applying advancements in reasoning for large language models to the financial domain remains limited.

**方法:** The FEVO framework enhances LLM performance in the financial domain through continued pre-training, supervised fine-tuning, and reinforcement learning. High-quality datasets are curated for effective and efficient training.

**结果:** FEVO-R32B achieves state-of-the-art performance on five financial benchmarks and demonstrates better performance than FEVO-R32B-0.

**结论:** The FEVO-R32B model demonstrates the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FEVO%3A+Financial+Knowledge+Expansion+and+Reasoning+Evolution+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06057&send_immediately=true&force_search=false)

**原文摘要:** Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [108] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi, Isha Patel*

**主要类别:** cs.AI

**AI概要:** This paper presents an AI-based framework for healthcare energy management that combines LSTM, GA, and SHAP, showing superior performance over traditional methods and offering a robust solution for improving forecasting accuracy and energy efficiency.


<details>
  <summary>更多</summary>
  
**动机:** The urgent need for efficient energy management in healthcare facilities, where fluctuating demands challenge operational efficiency and sustainability.

**方法:** The study presents an AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm (GA), and SHAP (Shapley Additive Explanations).

**结果:** LSTM significantly outperforms ARIMA and Prophet models in forecasting complex, non-linear demand patterns with a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE) of 29.96.

**结论:** The study establishes a solid foundation for using AI in healthcare energy management, highlighting its scalability, efficiency, and resilience potential.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI-Based+Demand+Forecasting+and+Load+Balancing+for+Optimising+Energy+use+in+Healthcare+Systems%3A+A+real+case+study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06077&send_immediately=true&force_search=false)

**原文摘要:** This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [109] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个名为OpenAgentSafety的框架，用于评估代理在八个关键风险类别中的行为，揭示了现有模型的安全漏洞，强调了在实际部署前需要更强的保障措施。


<details>
  <summary>更多</summary>
  
**动机:** 虽然先前的基准测试试图评估代理的安全性，但大多数通过依赖模拟环境、狭窄的任务领域或不现实的工具抽象而未能达到目的。因此，需要一个全面且模块化的框架来评估代理在各种关键风险类别中的行为。

**方法:** 研究引入了OpenAgentSafety框架，该框架评估了与真实工具交互的代理的行为，并支持超过350个多轮、多用户任务，结合基于规则的分析和LLM-as-judge评估来检测明显和微妙的不安全行为。

**结果:** 对五个突出的LLMs在代理场景中的实证分析显示，在安全性脆弱的任务中，不安全行为的比例从Claude-Sonnet-3.7的51.2%到o3-mini的72.7%不等。

**结论:** OpenAgentSafety框架揭示了现有LLMs在代理场景中的安全行为问题，强调了在实际部署前需要更强的保障措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OpenAgentSafety%3A+A+Comprehensive+Framework+for+Evaluating+Real-World+AI+Agent+Safety，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06134&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [110] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, Pang Wei Koh*

**主要类别:** cs.AI

**AI概要:** This paper explores the concept of delta learning, showing that pairing weak data points can lead to significant gains in model performance, even surpassing strong supervision.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to explore how improvements in language models can be achieved when strong supervision is scarce, by utilizing paired preference data consisting of individually weak data points.

**方法:** The study proposes the delta learning hypothesis and validates it through controlled experiments and at-scale post-training of 8B models on preference data generated by pairing responses from different sized models.

**结果:** Delta learning enables simpler and cheaper open recipes for state-of-the-art post-training, with performance matching that of models tuned with much stronger supervisors.

**结论:** Models can learn surprisingly well from paired data that might typically be considered weak, indicating the potential of delta learning in enhancing model training.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Delta+Learning+Hypothesis%3A+Preference+Tuning+on+Weak+Data+can+Yield+Strong+Gains，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06187，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06187&send_immediately=true&force_search=false)

**原文摘要:** Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [111] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes, Emilie Devijver, Marianne Clausel, Eric Gaussier*

**主要类别:** cs.AI

**AI概要:** This paper explores the use of causal abstractions to identify causal queries without requiring a fully specified causal diagram.


<details>
  <summary>更多</summary>
  
**动机:** To overcome the limitation of requiring a fully specified causal diagram when identifying the effect of a treatment from observational data, especially in complex or high-dimensional settings.

**方法:** Introducing and formalizing several identifiability criteria under the setting of causal abstractions formalized as collections of causal diagrams.

**结果:** The introduction and formalization of several identifiability criteria under the setting of causal abstractions formalized as collections of causal diagrams. A structured hierarchy that organizes these criteria, highlighting their relationships.

**结论:** A hierarchical view of identifiability criteria provides a clearer understanding of what can be identified under varying levels of causal knowledge.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Identifiability+in+Causal+Abstractions%3A+A+Hierarchy+of+Criteria，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06213，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06213&send_immediately=true&force_search=false)

**原文摘要:** Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [112] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu, Yifan Wu, Jason Hartline, Michael J. Curry*

**主要类别:** cs.AI

**AI概要:** 本文设计了一种新的文本评分规则，以更好地与人类偏好对齐，同时保持了正确性。


<details>
  <summary>更多</summary>
  
**动机:** 并非所有的正确评分规则都与人类对文本的偏好很好地吻合，这是我们的研究动机。

**方法:** 我们的论文通过优化和最小化一个正确的评分规则和参考分数（例如人类分数）之间的均方误差，设计了文本的对齐评分规则(ASR)。

**结果:** 我们的ASR在与人类偏好保持一致的同时，还保持了正确性，优于以前的方法。

**结论:** 我们的实验表明，我们的ASR在与人类偏好保持一致的同时，还保持了正确性，优于以前的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aligned+Textual+Scoring+Rules，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06221，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06221&send_immediately=true&force_search=false)

**原文摘要:** Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [113] [Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks](https://arxiv.org/abs/2507.05415)
*Lu Xian, Van Tran, Lauren Lee, Meera Kumar, Yichen Zhang, Florian Schaub*

**主要类别:** cs.CR

**AI概要:** 本研究探讨了美国银行业隐私政策的现状，特别是与第三方进行营销相关数据共享的披露和控制。研究发现，由于政策文件之间存在显著的不一致性，这可能导致消费者产生混淆，并质疑当前政策要求是否达到了其预定目标。


<details>
  <summary>更多</summary>
  
**动机:** 银行现在运营着涉及复杂数据共享实践的网站、移动应用和其他服务，这些实践需要额外的隐私通知和不销售选择退出。然而，隐私政策往往很复杂。

**方法:** 进行了大规模分析，研究美国银行如何根据GLBA、其他联邦隐私政策要求以及加州消费者隐私法（CCPA）实施隐私政策和控制。重点关注特别侵犯隐私的做法的披露和控制：出于营销目的与第三方共享数据。收集了2067家最大的美国银行的隐私政策，其中45.3%提供了多种政策。

**结果:** 发现了同一银行内披露和控制之间的频繁且令人担忧的不一致现象，如在GLBA通知中表示不与第三方共享但其他地方透露共享，或使用第三方营销/广告cookie而未披露。这种策略的多重性及其引发的不一致性可能会造成消费者的困惑，并破坏要求它们的法律的透明度目标。

**结论:** 当前的政策要求，如GLBA通知，在当今的在线银行环境中可能未能达到其预期目标。需要对联邦和州法律下的隐私政策和控制要求进行改革和协调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Layered%2C+Overlapping%2C+and+Inconsistent%3A+A+Large-Scale+Analysis+of+the+Multiple+Privacy+Policies+and+Controls+of+U.S.+Banks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05415&send_immediately=true&force_search=false)

**原文摘要:** Privacy policies are often complex. An exception is the two-page standardized
notice that U.S. financial institutions must provide under the
Gramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile
apps, and other services that involve complex data sharing practices that
require additional privacy notices and do-not-sell opt-outs. We conducted a
large-scale analysis of how U.S. banks implement privacy policies and controls
in response to GLBA; other federal privacy policy requirements; and the
California Consumer Privacy Act (CCPA), a key example for U.S. state privacy
laws. We focused on the disclosure and control of a set of especially
privacy-invasive practices: third-party data sharing for marketing-related
purposes. We collected privacy policies for the 2,067 largest U.S. banks,
45.3\% of which provided multiple policies. Across disclosures and controls
within the \textit{same} bank, we identified frequent, concerning
inconsistencies -- such as banks indicating in GLBA notices that they do not
share with third parties but disclosing sharing elsewhere, or using third-party
marketing/advertising cookies without disclosure. This multiplicity of
policies, with the inconsistencies it causes, may create consumer confusion and
undermine the transparency goals of the very laws that require them. Our
findings call into question whether current policy requirements, such as the
GLBA notice, are achieving their intended goals in today's online banking
landscape. We discuss potential avenues for reforming and harmonizing privacy
policies and control requirements across federal and state laws.

</details>


### [114] [FrameShift: Learning to Resize Fuzzer Inputs Without Breaking Them](https://arxiv.org/abs/2507.05421)
*Harrison Green, Claire Le Goues, Fraser Brown*

**主要类别:** cs.CR

**AI概要:** This paper introduces FrameShift, a novel technique that enhances fuzzing by preserving input structure during mutation, improving performance and coverage.


<details>
  <summary>更多</summary>
  
**动机:** Current fuzzers often generate destructive frameshift mutations due to lack of knowledge about input formats, leading to time-wasting malformed inputs. The motivation is to avoid such breaking mutations.

**方法:** The FrameShift technique preserves input structure during mutation by detecting and using relation fields. It was implemented in two state-of-the-art fuzzers, AFL++ and LibAFL, and evaluated over a 12+ CPU-year period.

**结果:** FrameShift improves fuzzer performance in each configuration, sometimes increasing coverage by more than 50%. It also successfully identifies important structural relationships in various formats.

**结论:** The FrameShift technique proves to be versatile and effective across different programming languages and formats, enhancing fuzzing performance significantly.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FrameShift%3A+Learning+to+Resize+Fuzzer+Inputs+Without+Breaking+Them，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05421，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05421&send_immediately=true&force_search=false)

**原文摘要:** Coverage-guided fuzzers are powerful automated bug-finding tools. They mutate
program inputs, observe coverage, and save any input that hits an unexplored
path for future mutation. Unfortunately, without knowledge of input
formats--for example, the relationship between formats' data fields and
sizes--fuzzers are prone to generate destructive frameshift mutations. These
time-wasting mutations yield malformed inputs that are rejected by the target
program. To avoid such breaking mutations, this paper proposes a novel,
lightweight technique that preserves the structure of inputs during mutation by
detecting and using relation fields.
  Our technique, FrameShift, is simple, fast, and does not require additional
instrumentation beyond standard coverage feedback. We implement our technique
in two state-of-the-art fuzzers, AFL++ and LibAFL, and perform a 12+ CPU-year
fuzzer evaluation, finding that FrameShift improves the performance of the
fuzzer in each configuration, sometimes increasing coverage by more than 50%.
Furthermore, through a series of case studies, we show that our technique is
versatile enough to find important structural relationships in a variety of
formats, even generalizing beyond C/C++ targets to both Rust and Python.

</details>


### [115] [A Systematization of Security Vulnerabilities in Computer Use Agents](https://arxiv.org/abs/2507.05445)
*Daniel Jones, Giorgio Severi, Martin Pouliot, Gary Lopez, Joris de Gruyter, Santiago Zanella-Beguelin, Justin Song, Blake Bullwinkel, Pamela Cortez, Amanda Minnich*

**主要类别:** cs.CR

**AI概要:** 本文对对抗条件下的现实世界CUA进行了系统的威胁分析和测试，揭示了当前CUA实施中的更深层次的架构缺陷。


<details>
  <summary>更多</summary>
  
**动机:** CUA（计算机使用代理）的安全边界仍然不明确，传统的威胁模型无法捕捉到这些新的攻击面和信任边界。

**方法:** 对现实世界中的CUA进行了系统的威胁分析和测试。

**结果:** 确定了CUA范式独有的七类风险，并深入分析了三个具体的利用场景：通过视觉覆盖进行点击劫持、通过链式工具使用实现远程代码执行的间接提示注入以及操纵隐式界面框架以劫持多步推理的CoT暴露攻击。

**结论:** 提出了一个CUA特定的安全评估框架和设计原则，以在对抗性和高风险环境中安全部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Systematization+of+Security+Vulnerabilities+in+Computer+Use+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05445&send_immediately=true&force_search=false)

**原文摘要:** Computer Use Agents (CUAs), autonomous systems that interact with software
interfaces via browsers or virtual machines, are rapidly being deployed in
consumer and enterprise environments. These agents introduce novel attack
surfaces and trust boundaries that are not captured by traditional threat
models. Despite their growing capabilities, the security boundaries of CUAs
remain poorly understood. In this paper, we conduct a systematic threat
analysis and testing of real-world CUAs under adversarial conditions. We
identify seven classes of risks unique to the CUA paradigm, and analyze three
concrete exploit scenarios in depth: (1) clickjacking via visual overlays that
mislead interface-level reasoning, (2) indirect prompt injection that enables
Remote Code Execution (RCE) through chained tool use, and (3) CoT exposure
attacks that manipulate implicit interface framing to hijack multi-step
reasoning. These case studies reveal deeper architectural flaws across current
CUA implementations. Namely, a lack of input provenance tracking, weak
interface-action binding, and insufficient control over agent memory and
delegation. We conclude by proposing a CUA-specific security evaluation
framework and design principles for safe deployment in adversarial and
high-stakes settings.

</details>


### [116] [Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice](https://arxiv.org/abs/2507.05512)
*Gehao Zhang, Eugene Bagdasarian, Juan Zhai, Shiqing Ma*

**主要类别:** cs.CR

**AI概要:** This paper evaluates the robustness of N-gram-based watermarking schemes against code obfuscation and finds them insufficient. A potential path for robust code watermarking is proposed.


<details>
  <summary>更多</summary>
  
**动机:** Distinguishing AI-generated code from human-written code is becoming crucial, but the robustness of N-gram-based watermarking schemes in code content remains insufficiently evaluated.

**方法:** Theoretical modeling and experimental evaluation of code obfuscation's effect on N-gram-based watermarking schemes.

**结果:** Experiments show that all watermarking detectors have coin-flipping detection abilities on obfuscated codes and both programming languages own obfuscators that can achieve attack effects with no detection AUROC higher than 0.6 after the attack.

**结论:** N-gram-based watermarking schemes are not robust against code obfuscation. The paper proposes a potential path for robust code watermarking.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Disappearing+Ink%3A+Obfuscation+Breaks+N-gram+Code+Watermarks+in+Theory+and+Practice，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05512，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05512&send_immediately=true&force_search=false)

**原文摘要:** Distinguishing AI-generated code from human-written code is becoming crucial
for tasks such as authorship attribution, content tracking, and misuse
detection. Based on this, N-gram-based watermarking schemes have emerged as
prominent, which inject secret watermarks to be detected during the generation.
  However, their robustness in code content remains insufficiently evaluated.
Most claims rely solely on defenses against simple code transformations or code
optimizations as a simulation of attack, creating a questionable sense of
robustness. In contrast, more sophisticated schemes already exist in the
software engineering world, e.g., code obfuscation, which significantly alters
code while preserving functionality. Although obfuscation is commonly used to
protect intellectual property or evade software scanners, the robustness of
code watermarking techniques against such transformations remains largely
unexplored.
  In this work, we formally model the code obfuscation and prove the
impossibility of N-gram-based watermarking's robustness with only one intuitive
and experimentally verified assumption, distribution consistency, satisfied.
Given the original false positive rate of the watermarking detection, the ratio
that the detector failed on the watermarked code after obfuscation will
increase to 1 - fpr.
  The experiments have been performed on three SOTA watermarking schemes, two
LLMs, two programming languages, four code benchmarks, and four obfuscators.
Among them, all watermarking detectors show coin-flipping detection abilities
on obfuscated codes (AUROC tightly surrounds 0.5). Among all models,
watermarking schemes, and datasets, both programming languages own obfuscators
that can achieve attack effects with no detection AUROC higher than 0.6 after
the attack. Based on the theoretical and practical observations, we also
proposed a potential path of robust code watermarking.

</details>


### [117] [PROTEAN: Federated Intrusion Detection in Non-IID Environments through Prototype-Based Knowledge Sharing](https://arxiv.org/abs/2507.05524)
*Sara Chennoufi, Yufei Han, Gregory Blanc, Emiliano De Cristofaro, Christophe Kiennert*

**主要类别:** cs.CR

**AI概要:** This paper addresses the challenge of data heterogeneity in Federated Learning for intrusion detection by introducing PROTEAN, a framework that facilitates collaborative and privacy-preserving intrusion detection.


<details>
  <summary>更多</summary>
  
**动机:** Participants in distributed networks face diverse and fast-evolving cyberattacks, and Federated Learning (FL) techniques are a promising mitigation strategy. However, FL solutions are hindered by significant data heterogeneity.

**方法:** The paper introduces PROTEAN, a Prototype Learning-based framework for collaborative and privacy-preserving intrusion detection.

**结果:** PROTEAN enables accurate detection in environments with highly non-IID attack distributions and promotes direct knowledge sharing among participants.

**结论:** PROTEAN is effective in addressing data heterogeneity and improving cyber attack understanding in federated intrusion detection systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PROTEAN%3A+Federated+Intrusion+Detection+in+Non-IID+Environments+through+Prototype-Based+Knowledge+Sharing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05524，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05524&send_immediately=true&force_search=false)

**原文摘要:** In distributed networks, participants often face diverse and fast-evolving
cyberattacks. This makes techniques based on Federated Learning (FL) a
promising mitigation strategy. By only exchanging model updates, FL
participants can collaboratively build detection models without revealing
sensitive information, e.g., network structures or security postures. However,
the effectiveness of FL solutions is often hindered by significant data
heterogeneity, as attack patterns often differ drastically across organizations
due to varying security policies. To address these challenges, we introduce
PROTEAN, a Prototype Learning-based framework geared to facilitate
collaborative and privacy-preserving intrusion detection. PROTEAN enables
accurate detection in environments with highly non-IID attack distributions and
promotes direct knowledge sharing by exchanging class prototypes of different
attack types among participants. This allows organizations to better understand
attack techniques not present in their data collections. We instantiate PROTEAN
on two cyber intrusion datasets collected from IIoT and 5G-connected
participants and evaluate its performance in terms of utility and privacy,
demonstrating its effectiveness in addressing data heterogeneity while
improving cyber attack understanding in federated intrusion detection systems
(IDSs).

</details>


### [118] [AI Agent Smart Contract Exploit Generation](https://arxiv.org/abs/2507.05558)
*Arthur Gervais, Liyi Zhou*

**主要类别:** cs.CR

**AI概要:** 本文介绍了一种名为A1的代理执行驱动系统，该系统能将任何大型语言模型转化为端到端的漏洞生成器，实现了无手工启发式规则的自动化漏洞发现和利用。


<details>
  <summary>更多</summary>
  
**动机:** 将任何大型语言模型转化为端到端的漏洞生成器，实现自动化的漏洞发现和利用。

**方法:** A1系统通过提供六种特定领域的工具，使代理能够自主发现漏洞，并灵活利用这些工具来理解智能合约行为、生成攻击策略、在区块链状态上测试，并基于执行反馈改进方法。所有输出都经过具体验证以消除误报。

**结果:** 在以太坊和币安智能链上的36个真实世界易受攻击合约中，A1系统在VERITE基准测试中达到了62.96%（27个中的17个）的成功率；在所有26个成功案例中，A1每个案例最多提取859万美元，总计933万美元。跨六个大型语言模型的432次实验显示，随着迭代次数增加，性能提升逐渐减少。

**结论:** 研究结果揭示了一个令人担忧的不对称性，即在0.1%的漏洞率下，攻击者实现链上扫描盈利的成本为6000美元，而防御者则需要60000美元，这引发了关于AI代理是否会不可避免地偏向于攻击而非防御的基本问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Agent+Smart+Contract+Exploit+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05558，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05558&send_immediately=true&force_search=false)

**原文摘要:** We present A1, an agentic execution driven system that transforms any LLM
into an end-to-end exploit generator. A1 has no hand-crafted heuristics and
provides the agent with six domain-specific tools that enable autonomous
vulnerability discovery. The agent can flexibly leverage these tools to
understand smart contract behavior, generate exploit strategies, test them on
blockchain states, and refine approaches based on execution feedback. All
outputs are concretely validated to eliminate false positives.
  The evaluation across 36 real-world vulnerable contracts on Ethereum and
Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the
VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional
vulnerable contracts, with 5 cases occurring after the strongest model's
training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59
million USD per case and 9.33 million USD total. Through 432 experiments across
six LLMs, we analyze iteration-wise performance showing diminishing returns
with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations
2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo
analysis of 19 historical attacks shows success probabilities of 85.9%-88.8%
without detection delays.
  We investigate whether an attacker or a defender benefits most from deploying
A1 as a continuous on-chain scanning system. Our model shows that OpenAI's
o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%
vulnerability incidence rates, while faster models require >=1.000% rates to
break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability
rates, attackers achieve an on-chain scanning profitability at a $6000 exploit
value, while defenders require $60000, raising fundamental questions about
whether AI agents inevitably favor exploitation over defense.

</details>


### [119] [iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips](https://arxiv.org/abs/2507.05576)
*Mehdi Elahi, Mohamed R. Elshamy, Abdel-Hameed Badawy, Ahmad Patooghy*

**主要类别:** cs.CR

**AI概要:** This paper addresses the issue of evasive and sporadic thermal Trojan attacks on SoCs by introducing Intermittent Thermal Trojans (iThermTroj) and proposing a set of tiny Machine Learning classifiers for run-time anomaly detection.


<details>
  <summary>更多</summary>
  
**动机:** Thermal Trojan attacks are a pressing concern for the security and reliability of SoCs, especially in mobile applications. The situation becomes more complicated with evasive and sporadic attacks that stay hidden from detection mechanisms.

**方法:** A set of tiny Machine Learning classifiers are proposed for run-time anomaly detection to protect SoCs against intermittent thermal Trojan attacks.

**结果:** The proposed approach improves the attack detection rate by 29.4%, 17.2%, and 14.3% in scenarios where iThermTroj manipulates up to 80%, 60%, and 40% of SoC's thermal data, respectively. The method increases the full protection resolution to 0.8 degrees Celsius.

**结论:** The proposed method provides a robust solution against intermittent thermal Trojan attacks on SoCs, improving the detection rate and providing full protection at 0.8 degrees Celsius.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是iThermTroj%3A+Exploiting+Intermittent+Thermal+Trojans+in+Multi-Processor+System-on-Chips，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05576，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05576&send_immediately=true&force_search=false)

**原文摘要:** Thermal Trojan attacks present a pressing concern for the security and
reliability of System-on-Chips (SoCs), especially in mobile applications. The
situation becomes more complicated when such attacks are more evasive and
operate sporadically to stay hidden from detection mechanisms. In this paper,
we introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'
thermal information in a random time-triggered manner. According to our
experiments, iThermTroj attack can easily bypass available threshold-based
thermal Trojan detection solutions. We investigate SoC vulnerabilities to
variations of iThermTroj through an in-depth analysis of Trojan activation and
duration scenarios. We also propose a set of tiny Machine Learning classifiers
for run-time anomaly detection to protect SoCs against such intermittent
thermal Trojan attacks. Compared to existing methods, our approach improves the
attack detection rate by 29.4\%, 17.2\%, and 14.3\% in scenarios where
iThermTroj manipulates up to 80\%, 60\%, and 40\% of SoC's thermal data,
respectively. Additionally, our method increases the full protection resolution
to 0.8 degrees Celsius, meaning that any temperature manipulations exceeding
$\pm 0.8$ degrees will be detected with 100\% accuracy.

</details>


### [120] [DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective](https://arxiv.org/abs/2507.05622)
*Shuo Shao, Yiming Li, Mengren Zheng, Zhiyang Hu, Yukun Chen, Boheng Li, Yu He, Junfeng Guo, Tianwei Zhang, Dacheng Tao, Zhan Qin*

**主要类别:** cs.CR

**AI概要:** 本文研究了数据集审计在对抗视角下的应用，提出了分类法和攻击类型，并对其进行了系统性策略的制定和评估，强调了开发更安全可靠的数据集审计方法的紧迫性。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习在不同领域的广泛应用取决于训练数据集的质量和组成，但其使用的常见缺乏披露引发了重大的隐私和版权问题。数据集审计技术可以解决这些问题，但其对专门的对抗性攻击的抵抗力仍待探索。

**方法:** 通过对现有方法和攻击目标的理解，提出了系统性的攻击策略：逃避攻击的解耦、移除和检测；基于对抗样本的伪造攻击方法。这些公式和策略形成了新的基准DATABench。

**结果:** 使用DATABench进行的广泛评估显示，所评估的审计方法在对抗性设置下均不够稳健或独特。

**结论:** 现有的数据集审计方法在对抗性环境下均不够强大或独特，迫切需要开发更安全、可靠的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DATABench%3A+Evaluating+Dataset+Auditing+in+Deep+Learning+from+an+Adversarial+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05622&send_immediately=true&force_search=false)

**原文摘要:** The widespread application of Deep Learning across diverse domains hinges
critically on the quality and composition of training datasets. However, the
common lack of disclosure regarding their usage raises significant privacy and
copyright concerns. Dataset auditing techniques, which aim to determine if a
specific dataset was used to train a given suspicious model, provide promising
solutions to addressing these transparency gaps. While prior work has developed
various auditing methods, their resilience against dedicated adversarial
attacks remains largely unexplored. To bridge the gap, this paper initiates a
comprehensive study evaluating dataset auditing from an adversarial
perspective. We start with introducing a novel taxonomy, classifying existing
methods based on their reliance on internal features (IF) (inherent to the
data) versus external features (EF) (artificially introduced for auditing).
Subsequently, we formulate two primary attack types: evasion attacks, designed
to conceal the use of a dataset, and forgery attacks, intending to falsely
implicate an unused dataset. Building on the understanding of existing methods
and attack objectives, we further propose systematic attack strategies:
decoupling, removal, and detection for evasion; adversarial example-based
methods for forgery. These formulations and strategies lead to our new
benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9
representative auditing methods. Extensive evaluations using DATABench reveal
that none of the evaluated auditing methods are sufficiently robust or
distinctive under adversarial settings. These findings underscore the urgent
need for developing a more secure and reliable dataset auditing method capable
of withstanding sophisticated adversarial manipulation. Code is available at
https://github.com/shaoshuo-ss/DATABench.

</details>


### [121] [How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630)
*Sarthak Choudhary, Divyam Anshumaan, Nils Palumbo, Somesh Jha*

**主要类别:** cs.CR

**AI概要:** 本文揭示了KAD框架的结构性漏洞，并提出了一种有效的绕过KAD防御的攻击方法DataFlip。


<details>
  <summary>更多</summary>
  
**动机:** 探究KAD框架的安全性问题，揭示其存在的结构漏洞。

**方法:** 设计了一种系统性的自适应攻击方法DataFlip。

**结果:** DataFlip攻击方法能够以低至1.5%的检测率绕过KAD防御，并在无需白盒访问LLM或任何优化程序的情况下，成功率达到88%。

**结论:** KAD框架存在结构漏洞，DataFlip攻击方法可有效绕过KAD防御。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Not+to+Detect+Prompt+Injections+with+an+LLM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05630&send_immediately=true&force_search=false)

**原文摘要:** LLM-integrated applications and agents are vulnerable to prompt injection
attacks, in which adversaries embed malicious instructions within seemingly
benign user inputs to manipulate the LLM's intended behavior. Recent defenses
based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect
performance by using an LLM to classify inputs as clean or contaminated. In
this work, we formally characterize the KAD framework and uncover a structural
vulnerability in its design that invalidates its core security premise. We
design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this
fundamental weakness. It consistently evades KAD defenses with detection rates
as low as $1.5\%$ while reliably inducing malicious behavior with success rates
of up to $88\%$, without needing white-box access to the LLM or any
optimization procedures.

</details>


### [122] [DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning](https://arxiv.org/abs/2507.05649)
*Kaixiang Zhao, Joseph Yousry Attalla, Qian Lou, Yushun Dong*

**主要类别:** cs.CR

**AI概要:** This paper proposes DESIGN, a novel framework for efficient encrypted GNN inference. It tackles the critical efficiency limitations of existing FHE GNN approaches.


<details>
  <summary>更多</summary>
  
**动机:** Enabling privacy-preserving GNNs in encrypted domains typically incurs substantial computational overhead, rendering real-time and privacy-preserving inference impractical.

**方法:** DESIGN achieves significant performance gains through a hierarchical optimization strategy executed entirely on the server. First, FHE-compatible node importance scores are computed from the encrypted graph. These scores then guide a homomorphic partitioning process, generating multi-level importance masks directly under FHE.

**结果:** Empirical evaluations demonstrate that DESIGN substantially accelerates FHE GNN inference compared to state-of-the-art methods while maintaining competitive model accuracy.

**结论:** DESIGN presents a robust solution for secure graph analytics by substantially accelerating FHE GNN inference while maintaining competitive model accuracy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DESIGN%3A+Encrypted+GNN+Inference+via+Server-Side+Input+Graph+Pruning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05649，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05649&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
various graph-based learning tasks. However, enabling privacy-preserving GNNs
in encrypted domains, such as under Fully Homomorphic Encryption (FHE),
typically incurs substantial computational overhead, rendering real-time and
privacy-preserving inference impractical. In this work, we propose DESIGN
(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel
framework for efficient encrypted GNN inference. DESIGN tackles the critical
efficiency limitations of existing FHE GNN approaches, which often overlook
input data redundancy and apply uniform computational strategies. Our framework
achieves significant performance gains through a hierarchical optimization
strategy executed entirely on the server: first, FHE-compatible node importance
scores (based on encrypted degree statistics) are computed from the encrypted
graph. These scores then guide a homomorphic partitioning process, generating
multi-level importance masks directly under FHE. This dynamically generated
mask facilitates both input graph pruning (by logically removing unimportant
elements) and a novel adaptive polynomial activation scheme, where activation
complexity is tailored to node importance levels. Empirical evaluations
demonstrate that DESIGN substantially accelerates FHE GNN inference compared to
state-of-the-art methods while maintaining competitive model accuracy,
presenting a robust solution for secure graph analytics.

</details>


### [123] [TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)
*Aravind Cheruvu, Shravya Kanchi, Sifat Muhammad Abdullah, Nicholas Kong, Daphne Yao, Murtuza Jadliwala, Bimal Viswanath*

**主要类别:** cs.CR

**AI概要:** 介绍了一种名为TuneShield的防御框架，用于在聊天机器人微调过程中减轻毒性，同时保持对话质量。


<details>
  <summary>更多</summary>
  
**动机:** 近期基础模型的发展革新了对话式AI，但在使用特定对话数据集定制化大型语言模型时，如何减轻毒性特别是处理不可信训练数据时仍是一个重大挑战。

**方法:** TuneShield利用基于LLM的毒性分类，生成合成对话样本（称为‘治愈数据’），并在微调过程中进行对齐处理以进一步引导聊天机器人产生期望的响应。

**结果:** 研究发现，即使毒性分类器不完美或存在偏见，TuneShield也能有效减轻毒性注入攻击，同时保持对话质量，并且对适应性对抗和越狱攻击具有韧性。此外，在基于对话的学习中，TuneShield也显示出减轻适应性毒性注入攻击的有效性。

**结论:** TuneShield是一个有效的框架，可以在聊天机器人微调期间减轻毒性并维持对话质量，对于毒性分类器的不完美或偏见具有鲁棒性，并能够抵御多种攻击。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TuneShield%3A+Mitigating+Toxicity+in+Conversational+AI+while+Fine-tuning+on+Untrusted+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05660&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in foundation models, such as LLMs, have revolutionized
conversational AI. Chatbots are increasingly being developed by customizing
LLMs on specific conversational datasets. However, mitigating toxicity during
this customization, especially when dealing with untrusted training data,
remains a significant challenge. To address this, we introduce TuneShield, a
defense framework designed to mitigate toxicity during chatbot fine-tuning
while preserving conversational quality. TuneShield leverages LLM-based
toxicity classification, utilizing the instruction-following capabilities and
safety alignment of LLMs to effectively identify toxic samples, outperforming
industry API services. TuneShield generates synthetic conversation samples,
termed 'healing data', based on the identified toxic samples, using them to
mitigate toxicity while reinforcing desirable behavior during fine-tuning. It
performs an alignment process to further nudge the chatbot towards producing
desired responses. Our findings show that TuneShield effectively mitigates
toxicity injection attacks while preserving conversational quality, even when
the toxicity classifiers are imperfect or biased. TuneShield proves to be
resilient against adaptive adversarial and jailbreak attacks. Additionally,
TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection
attacks during dialog-based learning (DBL).

</details>


### [124] [Polyadic encryption](https://arxiv.org/abs/2507.05683)
*Steven Duplij, Qiang Guo*

**主要类别:** cs.CR

**AI概要:** This paper proposes a new method of encryption/decryption using polyadic algebraic structures and signal processing.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of the paper is to propose a novel encryption/decryption procedure based on polyadic algebraic structures and signal processing methods.

**方法:** The paper uses signals with integer amplitudes to send information and polyadic techniques to transfer the plaintext into series of special integers.

**结果:** The receiver can restore the plaintext using special rules and systems of equations.

**结论:** The proposed encryption/decryption procedure based on polyadic algebraic structures and signal processing methods is effective.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Polyadic+encryption，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05683，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05683&send_immediately=true&force_search=false)

**原文摘要:** A novel original procedure of encryption/decryption based on the polyadic
algebraic structures and on signal processing methods is proposed. First, we
use signals with integer amplitudes to send information. Then we use polyadic
techniques to transfer the plaintext into series of special integers. The
receiver restores the plaintext using special rules and systems of equations.

</details>


### [125] [Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset](https://arxiv.org/abs/2507.05728)
*Ruofei Wang, Peiqi Duan, Boxin Shi, Renjie Wan*

**主要类别:** cs.CR

**AI概要:** This work proposes the first unlearnable event stream generation method to prevent unauthorized training from event datasets.


<details>
  <summary>更多</summary>
  
**动机:** Safeguarding the event dataset against unauthorized usage has become a serious concern for data owners. Unlearnable Examples are proposed to prevent the unauthorized exploitation of image datasets, but it's unclear how to create unlearnable asynchronous event streams to prevent event misuse.

**方法:** A new form of asynchronous event error-minimizing noise is proposed to perturb event streams, tricking the unauthorized model into learning embedded noise instead of realistic features. A projection strategy is presented to sparsify the noise to render our unlearnable event streams (UEvs).

**结果:** Extensive experiments demonstrate that the method effectively protects event data from unauthorized exploitation, while preserving their utility for legitimate use.

**结论:** The proposed UEvs contribute to the advancement of secure and trustworthy event dataset sharing.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asynchronous+Event+Error-Minimizing+Noise+for+Safeguarding+Event+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05728，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05728&send_immediately=true&force_search=false)

**原文摘要:** With more event datasets being released online, safeguarding the event
dataset against unauthorized usage has become a serious concern for data
owners. Unlearnable Examples are proposed to prevent the unauthorized
exploitation of image datasets. However, it's unclear how to create unlearnable
asynchronous event streams to prevent event misuse. In this work, we propose
the first unlearnable event stream generation method to prevent unauthorized
training from event datasets. A new form of asynchronous event error-minimizing
noise is proposed to perturb event streams, tricking the unauthorized model
into learning embedded noise instead of realistic features. To be compatible
with the sparse event, a projection strategy is presented to sparsify the noise
to render our unlearnable event streams (UEvs). Extensive experiments
demonstrate that our method effectively protects event data from unauthorized
exploitation, while preserving their utility for legitimate use. We hope our
UEvs contribute to the advancement of secure and trustworthy event dataset
sharing. Code is available at: https://github.com/rfww/uevs.

</details>


### [126] [Automated Reasoning for Vulnerability Management by Design](https://arxiv.org/abs/2507.05794)
*Avi Shaked, Nan Messe*

**主要类别:** cs.CR

**AI概要:** This paper proposes an automated reasoning mechanism for managing system vulnerabilities and designing security controls.


<details>
  <summary>更多</summary>
  
**动机:** Current vulnerability management approaches lack the ability to support systematic reasoning about the vulnerability postures of systems designs, which is essential for securing systems.

**方法:** The authors propose a formally grounded automated reasoning mechanism and integrate it into an open-source security design tool.

**结果:** The automated reasoning mechanism enables system designers to identify applicable vulnerabilities, specify mitigation options, declare selected controls, and manage vulnerability postures in a systematic manner.

**结论:** The proposed automated reasoning mechanism integrated into an open-source security design tool allows system designers to systematically manage vulnerability postures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Reasoning+for+Vulnerability+Management+by+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05794&send_immediately=true&force_search=false)

**原文摘要:** For securing systems, it is essential to manage their vulnerability posture
and design appropriate security controls. Vulnerability management allows to
proactively address vulnerabilities by incorporating pertinent security
controls into systems designs. Current vulnerability management approaches do
not support systematic reasoning about the vulnerability postures of systems
designs. To effectively manage vulnerabilities and design security controls, we
propose a formally grounded automated reasoning mechanism. We integrate the
mechanism into an open-source security design tool and demonstrate its
application through an illustrative example driven by real-world challenges.
The automated reasoning mechanism allows system designers to identify
vulnerabilities that are applicable to a specific system design, explicitly
specify vulnerability mitigation options, declare selected controls, and thus
systematically manage vulnerability postures.

</details>


### [127] [LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential Privacy Protocols and Post-Processing Methods](https://arxiv.org/abs/2507.05872)
*Berkay Kemal Balioglu, Alireza Khodaie, Mehmet Emre Gursoy*

**主要类别:** cs.CR

**AI概要:** This paper presents LDP$^3$, an open-source, extensible, and multi-threaded toolkit for LDP researchers and practitioners. LDP$^3$ can help users to select an optimal combination of protocol and PP method under different privacy budgets and datasets.


<details>
  <summary>更多</summary>
  
**动机:** Selecting an optimal combination of protocol and PP method under different privacy budgets and datasets remains a challenge. The lack of a comprehensive and extensible LDP benchmarking toolkit raises difficulties in evaluating new protocols and PP methods.

**方法:** The paper presents LDP$^3$, an open-source, extensible, and multi-threaded toolkit for LDP researchers and practitioners. LDP$^3$ contains implementations of several LDP protocols, PP methods, and utility metrics in a modular and extensible design.

**结果:** Experimental evaluations demonstrate that using LDP$^3$ to select a good protocol and post-processing method substantially improves utility compared to a bad or random choice. The multi-threaded design of LDP$^3$ brings substantial benefits in terms of efficiency.

**结论:** LDP$^3$ is a useful tool for LDP researchers and practitioners, which can help them to select an optimal combination of protocol and PP method under different privacy budgets and datasets. The multi-threaded design of LDP$^3$ brings substantial benefits in terms of efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LDP%24%5E3%24%3A+An+Extensible+and+Multi-Threaded+Toolkit+for+Local+Differential+Privacy+Protocols+and+Post-Processing+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05872，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05872&send_immediately=true&force_search=false)

**原文摘要:** Local differential privacy (LDP) has become a prominent notion for
privacy-preserving data collection. While numerous LDP protocols and
post-processing (PP) methods have been developed, selecting an optimal
combination under different privacy budgets and datasets remains a challenge.
Moreover, the lack of a comprehensive and extensible LDP benchmarking toolkit
raises difficulties in evaluating new protocols and PP methods. To address
these concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an
open-source, extensible, and multi-threaded toolkit for LDP researchers and
practitioners. LDP$^3$ contains implementations of several LDP protocols, PP
methods, and utility metrics in a modular and extensible design. Its modular
design enables developers to conveniently integrate new protocols and PP
methods. Furthermore, its multi-threaded nature enables significant reductions
in execution times via parallelization. Experimental evaluations demonstrate
that: (i) using LDP$^3$ to select a good protocol and post-processing method
substantially improves utility compared to a bad or random choice, and (ii) the
multi-threaded design of LDP$^3$ brings substantial benefits in terms of
efficiency.

</details>


### [128] [Post-Processing in Local Differential Privacy: An Extensive Evaluation and Benchmark Platform](https://arxiv.org/abs/2507.05875)
*Alireza Khodaie, Berkay Kemal Balioglu, Mehmet Emre Gursoy*

**主要类别:** cs.CR

**AI概要:** This paper presents an extensive benchmark to evaluate the behaviors and optimality of post-processing methods under diverse conditions in the context of local differential privacy (LDP). The findings reveal that the optimal post-processing method depends on multiple factors, and an open-source benchmark platform called LDP$^3$ is introduced for future use and development.


<details>
  <summary>更多</summary>
  
**动机:** To mitigate the issue of reduced data utility caused by the inherent perturbation added by LDP protocols, several post-processing (PP) methods have been developed. However, the comparative performance of PP methods under diverse settings remains underexplored.

**方法:** An extensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility metrics, and 6 datasets was used to evaluate the behaviors and optimality of PP methods under diverse conditions.

**结果:** Through extensive experiments, it was shown that while PP can substantially improve utility when the privacy budget is small (i.e., strict privacy), its benefit diminishes as the privacy budget grows. The optimal PP method depends on multiple factors, including the choice of LDP protocol, privacy budget, data characteristics, and the specific utility metric.

**结论:** Local differential privacy (LDP) has limitations in terms of data utility due to the inherent perturbation added by LDP protocols. The optimal post-processing method depends on multiple factors, and LDP$^3$ is introduced as an open-source benchmark platform for future use and development.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Post-Processing+in+Local+Differential+Privacy%3A+An+Extensive+Evaluation+and+Benchmark+Platform，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.05875，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.05875&send_immediately=true&force_search=false)

**原文摘要:** Local differential privacy (LDP) has recently gained prominence as a powerful
paradigm for collecting and analyzing sensitive data from users' devices.
However, the inherent perturbation added by LDP protocols reduces the utility
of the collected data. To mitigate this issue, several post-processing (PP)
methods have been developed. Yet, the comparative performance of PP methods
under diverse settings remains underexplored. In this paper, we present an
extensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility
metrics, and 6 datasets to evaluate the behaviors and optimality of PP methods
under diverse conditions. Through extensive experiments, we show that while PP
can substantially improve utility when the privacy budget is small (i.e.,
strict privacy), its benefit diminishes as the privacy budget grows. Moreover,
our findings reveal that the optimal PP method depends on multiple factors,
including the choice of LDP protocol, privacy budget, data characteristics
(such as distribution and domain size), and the specific utility metric. To
advance research in this area and assist practitioners in identifying the most
suitable PP method for their setting, we introduce LDP$^3$, an open-source
benchmark platform. LDP$^3$ contains all methods used in our experimental
analysis, and it is designed in a modular, extensible, and multi-threaded way
for future use and development.

</details>


### [129] [The Impact of Event Data Partitioning on Privacy-aware Process Discovery](https://arxiv.org/abs/2507.06008)
*Jungeun Lim, Stephan A. Fahrenkrog-Petersen, Xixi Lu, Jan Mendling, Minseok Song*

**主要类别:** cs.CR

**AI概要:** This paper proposes a pipeline that combines anonymization and event data partitioning to address the privacy challenges of information systems.


<details>
  <summary>更多</summary>
  
**动机:** The privacy challenges of information systems that contain sensitive information about customers, patients, and employees can be addressed by anonymizing the event logs while still retaining utility for process discovery.

**方法:** A pipeline that combines anonymization and event data partitioning, where event abstraction is utilized for partitioning.

**结果:** The results demonstrate that event partitioning can bring improvements in process discovery utility for directly-follows-based anonymization techniques.

**结论:** Event partitioning can bring improvements in process discovery utility for directly-follows-based anonymization techniques.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Impact+of+Event+Data+Partitioning+on+Privacy-aware+Process+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06008&send_immediately=true&force_search=false)

**原文摘要:** Information systems support the execution of business processes. The event
logs of these executions generally contain sensitive information about
customers, patients, and employees. The corresponding privacy challenges can be
addressed by anonymizing the event logs while still retaining utility for
process discovery. However, trading off utility and privacy is difficult: the
higher the complexity of event log, the higher the loss of utility by
anonymization. In this work, we propose a pipeline that combines anonymization
and event data partitioning, where event abstraction is utilized for
partitioning. By leveraging event abstraction, event logs can be segmented into
multiple parts, allowing each sub-log to be anonymized separately. This
pipeline preserves privacy while mitigating the loss of utility. To validate
our approach, we study the impact of event partitioning on two anonymization
techniques using three real-world event logs and two process discovery
techniques. Our results demonstrate that event partitioning can bring
improvements in process discovery utility for directly-follows-based
anonymization techniques.

</details>


### [130] [Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks](https://arxiv.org/abs/2507.06039)
*Oleksii Oleksenko, Flavien Solt, Cédric Fournet, Jana Hofmann, Boris Köpf, Stavros Volos*

**主要类别:** cs.CR

**AI概要:** This paper highlights the limitations of current CPU isolation mechanisms against microarchitectural side-channel attacks and proposes a tool using extended model-based relational testing to proactively detect such vulnerabilities.


<details>
  <summary>更多</summary>
  
**动机:** Current isolation mechanisms in CPUs focus on architectural isolation but overlook microarchitectural side channels, leaving software to supplement defenses reactively with ad-hoc patches, which can lead to errors and oversights.

**方法:** Developed a tool that extends model-based relational testing (MRT) methodology to detect cross-domain information leakage, including a new test case generator, execution sandbox, new leakage models, and analysis techniques.

**结果:** Performed an in-depth testing campaign on six x86-64 CPUs, exposing four new leaks and corroborating numerous known ones with only two false positives.

**结论:** The approach enables a shift from reactive patching to proactive security validation in processor design, showing critical gaps in current isolation mechanisms and validating a robust methodology for detecting microarchitectural flaws.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enter%2C+Exit%2C+Page+Fault%2C+Leak%3A+Testing+Isolation+Boundaries+for+Microarchitectural+Leaks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06039&send_immediately=true&force_search=false)

**原文摘要:** CPUs provide isolation mechanisms like virtualization and privilege levels to
protect software. Yet these focus on architectural isolation while typically
overlooking microarchitectural side channels, exemplified by Meltdown and
Foreshadow. Software must therefore supplement architectural defenses with
ad-hoc microarchitectural patches, which are constantly evolving as new attacks
emerge and defenses are proposed. Such reactive approach makes ensuring
complete isolation a daunting task, and leaves room for errors and oversights.
  We address this problem by developing a tool that stress tests
microarchitectural isolation between security domains such as virtual machines,
kernel, and processes, with the goal of detecting flaws in the isolation
boundaries. The tool extends model-based relational testing (MRT) methodology
to enable detection of cross-domain information leakage. We design a new test
case generator and execution sandbox to handle multi-domain execution, new
leakage models to encode expected leaks, and new analysis techniques to manage
nondeterminism.
  We use this tool to perform an in-depth testing campaign on six x86-64 CPUs
for leakage across different isolation boundaries. The testing campaign exposed
four new leaks and corroborated numerous known ones, with only two false
positives throughout the entire campaign. These results show critical gaps in
current isolation mechanisms as well as validate a robust methodology for
detecting microarchitectural flaws. As such, this approach enables a shift from
reactive patching to proactive security validation in processor design.

</details>


### [131] [CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations](https://arxiv.org/abs/2507.06043)
*Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种结合攻击和防御的框架，利用GAN学习LLM内部的安全判断边界，实现了高效的越狱攻击和防御。


<details>
  <summary>更多</summary>
  
**动机:** 先前的研究将LLM越狱攻击与防御隔离开了，分析了LLM的安全保护机制，并提出了结合攻击和防御的框架。

**方法:** 利用生成对抗网络（GAN）学习LLM内部的安全判断边界，以实现高效的越狱攻击和防御。

**结果:** 实验结果表明，所提出的方法在三个流行的LLM中平均越狱成功率为88.85%，在最先进的越狱数据集上的防御成功率平均达到84.17%。

**结论:** 提出的方法不仅验证了其有效性，还揭示了LLM内部安全机制，为增强模型安全性提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CAVGAN%3A+Unifying+Jailbreak+and+Defense+of+LLMs+via+Generative+Adversarial+Attacks+on+their+Internal+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06043&send_immediately=true&force_search=false)

**原文摘要:** Security alignment enables the Large Language Model (LLM) to gain the
protection against malicious queries, but various jailbreak attack methods
reveal the vulnerability of this security mechanism. Previous studies have
isolated LLM jailbreak attacks and defenses. We analyze the security protection
mechanism of the LLM, and propose a framework that combines attack and defense.
Our method is based on the linearly separable property of LLM intermediate
layer embedding, as well as the essence of jailbreak attack, which aims to
embed harmful problems and transfer them to the safe area. We utilize
generative adversarial network (GAN) to learn the security judgment boundary
inside the LLM to achieve efficient jailbreak attack and defense. The
experimental results indicate that our method achieves an average jailbreak
success rate of 88.85\% across three popular LLMs, while the defense success
rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%.
This not only validates the effectiveness of our approach but also sheds light
on the internal security mechanisms of LLMs, offering new insights for
enhancing model security The code and data are available at
https://github.com/NLPGM/CAVGAN.

</details>


### [132] [Wrapless: The trustless lending protocol on top of Bitcoin](https://arxiv.org/abs/2507.06064)
*Oleksandr Kurbatov, Kyrylo Baybula, Yaroslava Chopa, Sergey Kozlov, Oleg Komendant, Illia Dovgopoly, Dmitrii Kurbatov, Zakhar Naumets, Yulia Artikulova, Pavel Kravchenko, Volodymyr Dubinin, Lasha Antadze, Yaroslav Panasenko, Mykhailo Velykodnyi*

**主要类别:** cs.CR

**AI概要:** 这篇论文介绍了一种名为Wrapless的贷款协议，它无需可信的包装机制即可使比特币作为抵押品。


<details>
  <summary>更多</summary>
  
**动机:** 对于无需信任的包装机制的需求以及对传统AMM金融工具的研究还不充分，这是本论文的主要动机。

**方法:** 论文提出了一种名为Wrapless的贷款协议，这种协议无需可信的包装机制即可实现比特币的抵押。该协议在比特币区块链上设立了一个“贷款通道”，使得比特币可以作为任何支持图灵完备智能合约的区块链上的贷款的抵押品。

**结果:** 通过使用Wrapless协议，可以在比特币区块链上实现贷款通道，从而使得比特币可以作为其他区块链上贷款的抵押品。

**结论:** Wrapless协议虽然已经设计得让各方在经济上无法操纵贷款规则，但要将其更接近于传统的AMM金融工具，仍需要大量的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wrapless%3A+The+trustless+lending+protocol+on+top+of+Bitcoin，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06064，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06064&send_immediately=true&force_search=false)

**原文摘要:** This paper presents Wrapless -- a lending protocol that enables the
collateralization of bitcoins without requiring a trusted wrapping mechanism.
The protocol facilitates a "loan channel" on the Bitcoin blockchain, allowing
bitcoins to be locked as collateral for loans issued on any blockchain that
supports Turing-complete smart contracts. The protocol is designed in a way
that makes it economically irrational for each involved party to manipulate the
loan rules. There is still a significant research area to bring the protocol
closer to traditional AMM financial instruments.

</details>


### [133] [Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI](https://arxiv.org/abs/2507.06092)
*Shravya Kanchi, Neal Mangaokar, Aravind Cheruvu, Sifat Muhammad Abdullah, Shirin Nilizadeh, Atul Prakash, Bimal Viswanath*

**主要类别:** cs.CR

**AI概要:** This paper explores the potential of Generative AI (GenAI) techniques in addressing data challenges and improving the performance of security classifiers.


<details>
  <summary>更多</summary>
  
**动机:** Data challenges that negatively impact the performance of machine learning-based supervised classifiers have received limited attention.

**方法:** The authors propose augmenting training datasets with synthetic data generated using GenAI techniques and introduce a novel GenAI scheme called Nimai.

**结果:** GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings.

**结论:** Generative AI techniques can significantly improve the performance of security classifiers, but some GenAI schemes struggle to initialize on certain security tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Taming+Data+Challenges+in+ML-based+Security+Tasks%3A+Lessons+from+Integrating+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06092&send_immediately=true&force_search=false)

**原文摘要:** Machine learning-based supervised classifiers are widely used for security
tasks, and their improvement has been largely focused on algorithmic
advancements. We argue that data challenges that negatively impact the
performance of these classifiers have received limited attention. We address
the following research question: Can developments in Generative AI (GenAI)
address these data challenges and improve classifier performance? We propose
augmenting training datasets with synthetic data generated using GenAI
techniques to improve classifier generalization. We evaluate this approach
across 7 diverse security tasks using 6 state-of-the-art GenAI methods and
introduce a novel GenAI scheme called Nimai that enables highly controlled data
synthesis. We find that GenAI techniques can significantly improve the
performance of security classifiers, achieving improvements of up to 32.6% even
in severely data-constrained settings (only ~180 training samples).
Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to
concept drift post-deployment, requiring minimal labeling in the adjustment
process. Despite successes, our study finds that some GenAI schemes struggle to
initialize (train and produce data) on certain security tasks. We also identify
characteristics of specific tasks, such as noisy labels, overlapping class
distributions, and sparse feature vectors, which hinder performance boost using
GenAI. We believe that our study will drive the development of future GenAI
tools designed for security tasks.

</details>


### [134] [Fun with flags: How Compilers Break and Fix Constant-Time Code](https://arxiv.org/abs/2507.06112)
*Antoine Geimer, Clementine Maurice*

**主要类别:** cs.CR

**AI概要:** The paper conducts a qualitative analysis to identify specific compiler optimization passes responsible for breaking constant-time code and proposes disabling these passes via compiler flags as a practical solution to reduce leakage.


<details>
  <summary>更多</summary>
  
**动机:** Developers' efforts to prevent timing side-channel attacks can be undone by compiler optimizations that reintroduce leaks. There is a lack of actionable insights on which optimization passes are responsible and how to disable them without modifying the compiler.

**方法:** Qualitative analysis of how compiler optimizations break constant-time code and identification of specific optimization passes responsible in GCC and LLVM compilers.

**结果:** A small set of optimization passes are identified as the root cause of most leaks. Disabling these passes via compiler flags offers a practical mitigation with minimal performance overhead.

**结论:** Disabling selected optimization passes via compiler flags significantly reduces leakage with minimal performance overhead, offering an immediately deployable defense for developers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fun+with+flags%3A+How+Compilers+Break+and+Fix+Constant-Time+Code，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06112，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06112&send_immediately=true&force_search=false)

**原文摘要:** Developers rely on constant-time programming to prevent timing side-channel
attacks. But these efforts can be undone by compilers, whose optimizations may
silently reintroduce leaks. While recent works have measured the extent of such
leakage, they leave developers without actionable insights: which optimization
passes are responsible, and how to disable them without modifying the compiler
remains unclear.
  In this paper, we conduct a qualitative analysis of how compiler
optimizations break constant-time code. We construct a dataset of
compiler-introduced constant-time violations and analyze the internals of two
widely used compilers, GCC and LLVM, to identify the specific optimization
passes responsible. Our key insight is that a small set of passes are at the
root of most leaks. To the best of our knowledge, we are also the first to
characterize how the interactions between these passes contribute to leakage.
Based on this analysis, we propose an original and practical mitigation that
requires no source code modification or custom compiler: disabling selected
optimization passes via compiler flags. We show that this approach
significantly reduces leakage with minimal performance overhead, offering an
immediately deployable defense for developers.

</details>


### [135] [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521)
*Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, Yushun Dong*

**主要类别:** cs.CR

**AI概要:** 该调查提供了针对语言模型特定的提取攻击和防御的全面分类，并分析了各种攻击方法和防御机制的有效性，最终提出改进方法的方向。


<details>
  <summary>更多</summary>
  
**动机:** 模型提取攻击对已部署的语言模型构成重大安全威胁，可能危及知识产权和用户隐私。

**方法:** 分析了各种攻击方法，包括基于API的知识蒸馏、直接查询、参数恢复和利用变压器架构的提示窃取技术，并检查了不同部署场景下的防御机制有效性。

**结果:** 提出了专门用于评估攻击效果和防御性能的指标，并指出现有方法的关键局限性，提出未来研究方向。

**结论:** 当前方法存在关键限制，需要进一步研究集成攻击方法和自适应防御机制，以平衡安全性与模型效用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+on+Model+Extraction+Attacks+and+Defenses+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22521，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22521&send_immediately=true&force_search=false)

**原文摘要:** Model extraction attacks pose significant security threats to deployed
language models, potentially compromising intellectual property and user
privacy. This survey provides a comprehensive taxonomy of LLM-specific
extraction attacks and defenses, categorizing attacks into functionality
extraction, training data extraction, and prompt-targeted attacks. We analyze
various attack methodologies including API-based knowledge distillation, direct
querying, parameter recovery, and prompt stealing techniques that exploit
transformer architectures. We then examine defense mechanisms organized into
model protection, data privacy protection, and prompt-targeted strategies,
evaluating their effectiveness across different deployment scenarios. We
propose specialized metrics for evaluating both attack effectiveness and
defense performance, addressing the specific challenges of generative language
models. Through our analysis, we identify critical limitations in current
approaches and propose promising research directions, including integrated
attack methodologies and adaptive defense mechanisms that balance security with
model utility. This work serves NLP researchers, ML engineers, and security
professionals seeking to protect language models in production environments.

</details>
